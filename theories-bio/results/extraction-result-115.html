<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-115 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-115</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-115</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-6336feec273c21816e9cef44151c90ca4a2cd0fb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6336feec273c21816e9cef44151c90ca4a2cd0fb" target="_blank">Bayesian reconstruction of memories stored in neural networks from their connectivity</a></p>
                <p><strong>Paper Venue:</strong> PLoS Comput. Biol.</p>
                <p><strong>Paper TL;DR:</strong> This work addresses the question of whether it is possible to reconstruct the information stored in a recurrent network of neurons, given its synaptic connectivity matrix by determining when solving such an inference problem is theoretically possible in specific attractor network models and by providing a practical algorithm to do so.</p>
                <p><strong>Paper Abstract:</strong> The advent of comprehensive synaptic wiring diagrams of large neural circuits has created the field of connectomics and given rise to a number of open research questions. One such question is whether it is possible to reconstruct the information stored in a recurrent network of neurons, given its synaptic connectivity matrix. Here, we address this question by determining when solving such an inference problem is theoretically possible in specific attractor network models and by providing a practical algorithm to do so. The algorithm builds on ideas from statistical physics to perform approximate Bayesian inference and is amenable to exact analysis. We study its performance on three different models, compare the algorithm to standard algorithms such as PCA, and explore the limitations of reconstructing stored patterns from synaptic connectivity.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e115.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e115.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hebbian (covariance) rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hebbian learning / covariance synaptic plasticity rule</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synaptic weights are set proportional to the empirical correlations of stored activity patterns (covariance form), yielding a low-rank weight matrix W = (1/sqrt(N)) X X^T after mean subtraction; used as the canonical learning rule underpinning Hopfield-type associative memory in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Hebbian learning (covariance rule)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Weights are updated proportional to pairwise correlations of pattern components; in the paper this is written as W = (1/√N) X* (X*)^T, assuming zero-mean (covariance) patterns; the 1/√N scaling is chosen to place the problem in the non-trivial inference regime. Mean activity subtraction (covariance form) is assumed to be performed by the plasticity mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Not explicitly specified in the paper. Implicitly treated as a long-term (consolidated) storage mechanism (i.e., changes that persist to store memories); the manuscript does not give explicit millisecond/second/day times.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic recurrent excitatory networks (Hopfield-type); discussed in context of cortical recurrent networks and hippocampal CA3 as candidate substrates.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Recurrent, symmetric low-rank connectivity (W = X X^T), often considered fully connected in the classic Hopfield form; the paper also studies rectified / sparse, non-negative variants to model excitatory-only synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory / attractor storage and pattern completion</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (used as the generative learning rule for simulations and analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper does not explicitly model interactions between multiple plasticity timescales for Hebbian learning; it treats the Hebbian-embedded weights as the (relatively) fixed object from which inference is performed. The authors discuss, as possible extensions, palimpsest models where embedding strengths evolve (i.e., adding slower forgetting dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Hebbian/covariance storage produces a low-rank weight matrix from which stored patterns can be (in principle) reconstructed; the ability to reconstruct depends on an effective noise parameter Δ (combining learning noise and thresholding) and exhibits a sharp phase transition (recoverable vs impossible) in the model; the classical Hopfield capacity α_c ≈ 0.14 (patterns per neuron) is cited for retrieval in the standard Hopfield model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Model performance is reported with measures tied to inference: mean-squared error (mse) of pattern reconstruction; critical effective noise thresholds (e.g., Δ_c = 1 in the simple symmetric binary case); classical retrieval capacity cited as P < α_c N with α_c ≈ 0.14 for the binary Hopfield model.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation mechanism is modelled for Hebbian weights in this work. The authors note extensions (e.g., palimpsest models or differential embedding strengths) as possible future directions to represent age-dependent memory weakening/forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reconstruction of memories stored in neural networks from their connectivity', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e115.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e115.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rectified Hopfield rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rectified Hopfield learning rule (non-negative Hebbian with threshold and noise)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A minimal model used in the paper that applies a threshold and rectification to the Hebbian correlation matrix to produce non-negative (excitatory) synaptic weights: J_ij = Φ(W_ij - τ + ζ_ij) with Φ(x)=max(0,x), threshold τ and additive noise ζ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Rectified Hebbian rule (thresholded, noisy)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Start from a Hebbian correlation W_ij = (1/√N) Σ X_i X_j; apply additive symmetric Gaussian noise ζ_ij (variance ν^2) and a threshold τ, then rectify to non-negative values: J_ij = max(0, W_ij - τ + ζ_ij). Parameters: threshold τ governs sparsity/connection probability; ν controls noise level. This yields a sparse, non-negative low-rank connectivity matrix intended to model excitatory-to-excitatory synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Not specified in the paper; treated as a synthesized (learned) connectivity structure representing stored memories (i.e., long-term changes). No explicit millisecond/second/day times are given.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Model intended to capture excitatory recurrent subnetworks (e.g., cortical excitatory populations; consistent with ideas about excitatory-to-excitatory memory storage, and discussed in relation to hippocampus and cortex).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Recurrent, excitatory-only (non-negative) sparse connectivity; low-rank structure (few stored patterns => few large eigenvalues); sparsity and connection probability controlled by τ and noise ν; analysis often assumes symmetrised J for tractability.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory / storage of fixed-point attractors</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (defined and used throughout the paper for analysis and simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>The paper analyses how the noise ν and threshold τ interact to determine detectability of stored patterns and reports a stochastic-resonance-like effect where moderate noise can enable recovery that is impossible with too little noise; these are interactions across noise/threshold parameters rather than explicit temporal scales. No explicit modelling of multiple synaptic timescales (fast vs slow) is included.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Rectification and noise produce a sparse, non-negative low-rank matrix from which stored patterns can be inferred under conditions summarised by an effective noise parameter Δ (channel universality). There is a sharp recoverability transition: for the simplest case Δ_c = 1. The model exhibits stochastic-resonance behavior with respect to additive noise ν and threshold τ, and critical variances ν* and connection-probability p_C expressions are derived. AMP inference outperforms PCA in reconstruction error across noise regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative metrics used include the effective noise Δ (derived from Fisher score), Δ_c (critical recoverability threshold; Δ_c = 1 for symmetric binary case), mean-squared error (mse) of reconstruction, connection probability p_C(ν,τ) (Eq. 4), and scaling of the maximal reconstructable pattern count P_max ∼ N^γ with γ ≈ 0.5–0.7 depending on model.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation across timescales is modelled. The authors note that if memories had different embedding strengths (e.g., palimpsest models), the method would tend to recover the most strongly embedded (recent) patterns and not older weakened ones.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reconstruction of memories stored in neural networks from their connectivity', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e115.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e115.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP / temporal asymmetry</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporally asymmetric synaptic plasticity (spike-timing-dependent plasticity, STDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper mentions that in cortex synaptic plasticity is temporally asymmetric as a function of pre- vs post-synaptic spike timing differences, which would produce asymmetric learned components in connectivity and is required for storing temporal sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Spike-timing-dependent plasticity (temporally asymmetric plasticity)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Described qualitatively: plasticity depends on the relative timing of pre- and post-synaptic spikes (an asymmetry with respect to spike timing); the paper does not provide STDP functional kernels or parameters, only the conceptual statement that cortical plasticity has temporal asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Implied in the text by reference to spike-timing dependence: operates at fast timescales associated with spike timing differences (milliseconds). The paper does not provide explicit numerical timescales but explicitly links 'timing difference between spikes' to asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortex (general cortical plasticity is discussed in the Discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Temporal asymmetry in plasticity would create asymmetric learned connectivity (directional weights) rather than the symmetric Hebbian low-rank matrices analysed in the paper; such asymmetry is necessary for sequence storage.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Storage/learning of temporal sequences (sequence learning) and asymmetric attractor dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mention of experimental plasticity phenomenology; not used in the model (the paper focuses on symmetric or symmetrised connectivity for analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>The authors discuss that plasticity depends on both spike timing and firing rate; if rate dependence dominates over spike timing, the learned component can be largely symmetric, indicating an interaction where fast spike-timing effects (ms) may be subdominant to slower rate-based effects (longer timescales), but the paper does not quantify these timescales or the dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper notes that temporally asymmetric plasticity in cortex would produce asymmetric learned connectivity (needed for sequence storage), and contrasts this with temporally symmetric plasticity observed in some areas (e.g., CA3); the paper does not model asymmetric learning but discusses implications for extending methods to asymmetric matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No consolidation mechanism described specifically for STDP in the paper; the paper only notes conceptual consequences (asymmetry → sequences) and that rate-dependent plasticity could dominate, producing symmetric components.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reconstruction of memories stored in neural networks from their connectivity', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e115.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e115.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Temporally symmetric plasticity (CA3)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Temporally symmetric synaptic plasticity (observed in hippocampal area CA3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites experimental reports that plasticity in hippocampal CA3 can be temporally symmetric, which would favor symmetric (bidirectional) Hebbian connectivity supporting attractor dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Temporally symmetric plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Qualitative mention only: plasticity that does not show a strong dependence on spike timing asymmetry, leading to a symmetric (bidirectional) learned connectivity component; the paper does not provide a parametrisation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Not specified in the paper (the term 'temporally symmetric' refers to symmetry with respect to spike timing dependence rather than an explicit timescale).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Hippocampus, area CA3 (cited as showing temporally symmetric plasticity in vitro).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Supports symmetric, bidirectional recurrent connectivity which the paper assumes and analyzes; such symmetry motivates using symmetric/symmetrised J matrices in the analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Associative memory / attractor dynamics (persistent activity consistent with symmetric connectivity).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mention of experimental observations motivating the modelling assumption of symmetric learned components.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Not discussed in detail; no explicit multi-timescale plasticity interactions provided for CA3 in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Used as an empirical argument supporting the focus on symmetric learned connectivity in the model; symmetric plasticity is consistent with attractor dynamics and persistent activity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reconstruction of memories stored in neural networks from their connectivity', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e115.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e115.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inhibitory plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Plasticity of inhibitory synapses (inhibitory-to-excitatory and inhibitory interneuron plasticity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper acknowledges that inhibitory synapses can exhibit plasticity and that such plasticity could expand storage capacity, but the core model assumes inhibition provides a uniform threshold-like control rather than being a substrate for stored information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Inhibitory synaptic plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Qualitative mention: connections involving inhibitory interneurons are known to be plastic in experiments (references given), and plasticity of inhibitory synapses could alter storage properties; paper does not provide a formal rule or parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortex and hippocampus (general discussion); specific interneuron types (PV-positive) noted for high connectivity to pyramidal cells.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Inclusion of plastic inhibitory connections would generally break the symmetric/excitatory-only assumptions and introduce asymmetry; inhibition in the base model is implemented as a global threshold controlling excitatory activity.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Potentially expands associative memory storage capacity and network control, but not modelled explicitly here.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mentioned as experimental fact and modelling extension possibility; current analyses do not include inhibitory plasticity.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Not discussed in detail; the paper notes practical issues (measurement difficulty for inhibitory synapses) and that adding inhibitory plasticity creates asymmetry in connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Paper does not model inhibitory plasticity but highlights it as an experimental reality and a potentially significant factor for storage capacity that would complicate reconstruction from connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not described.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reconstruction of memories stored in neural networks from their connectivity', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e115.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e115.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Palimpsest / variable-strength memories</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Palimpsest memory models (memories stored with variable embedding strength and gradual overwriting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced theoretical class of models in which newly stored patterns progressively overwrite older ones (gradual forgetting), producing memories with different embedding strengths over time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Palimpsest-like plasticity / differential embedding strength</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Qualitative description: memories are stored with varying strengths such that recent patterns are strongly embedded while older patterns are progressively erased; this can be the result of continual plasticity dynamics (no explicit parametrisation in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Not explicitly specified; concept implies slower processes (long-term change and decay of embedding strength over hours/days to longer), but the paper does not quantify timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic recurrent associative memory networks (Hopfield-type) and theoretical models of memory storage/forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Leads to a low-rank connectivity where rank components have different amplitudes (embedding strengths) reflecting recency; older components may have reduced amplitude and be unrecoverable by reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Long-term memory storage with forgetting/overwriting dynamics (long-term consolidation/decay)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mentioned as a theoretical extension/alternative class of models; not implemented in the paper's simulations but discussed as an implication for reconstruction (that only strongly embedded memories are likely to be recovered).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Yes — conceptually represents interaction between fast storage of a new memory and slow decay/overwriting of older memories; the paper references these models as an important extension to represent timescale-dependent memory dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Authors note that their reconstruction method would likely infer only the most strongly embedded (recent) memories in such palimpsest models, and older/weakly embedded memories would be difficult or impossible to reconstruct from connectivity alone.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Implicitly described as gradual overwriting (no explicit consolidation algorithm); the paper suggests modelling memory embedding strengths and their dynamics would be required to study consolidation/forgetting explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reconstruction of memories stored in neural networks from their connectivity', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e115.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e115.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spine-volume ↔ synaptic strength</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Correlation between dendritic spine volume and synaptic strength (structural synaptic plasticity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites experimental evidence that dendritic spine volume correlates with EPSP amplitude and uses that fact to justify modelling measurement noise (ζ_ij) on inferred synaptic strengths from EM-derived spine volumes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Structural synaptic plasticity (spine volume changes correlated with efficacy)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Qualitative: changes in dendritic spine volume are correlated with changes in functional synaptic strength (EPSP amplitude). The paper models measurement noise and imperfect correlation via an additive noise matrix ζ_ij in the learning/observational model.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Not quantified in the paper; structural changes like spine-volume modifications are discussed as a source of measured synaptic strength and imply slower, longer-term plasticity (hours to days) although the manuscript does not specify exact timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Excitatory cortical/hippocampal synapses on dendritic spines (pyramidal cells).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Structural measurements (spine volumes) used as proxies for excitatory synaptic weights; paper notes inhibitory synapses are formed on shafts (harder to measure), which affects which synapses' strengths are recoverable from EM data.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Long-term memory encoding/storage (structural consolidation of synaptic efficacy)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Referenced experimental correlation used to motivate inclusion of measurement noise in the generative model; paper itself is computational/theoretical.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper remarks that measurement noise and imperfect volume–efficacy correlation justify modelling an additive noise term; implicitly acknowledges structural (slow) plasticity as the observable correlate of longer-term learning, contrasted with faster spike-timing-dependent mechanisms, but does not model explicit interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Empirical correlation between spine volume and EPSP amplitude supports the assumption that connectomic measures can give noisy estimates of synaptic strength; this motivates including ζ_ij noise in the model and affects inferability limits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not modelled explicitly; spine-volume correlation is used to motivate that connectomic snapshots reflect (noisy) long-term synaptic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bayesian reconstruction of memories stored in neural networks from their connectivity', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-13 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-13</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-13</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-1.html">extraction-schema-1</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <p><strong>Paper ID:</strong> paper-586385704d3577dcc767c09b5bb87ee1abd1215c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/586385704d3577dcc767c09b5bb87ee1abd1215c" target="_blank">Machine learning for protein folding and dynamics.</a></p>
                <p><strong>Paper Venue:</strong> Current Opinion in Structural Biology</p>
                <p><strong>Paper TL;DR:</strong> The recent advances on all these fronts and the questions that need to be addressed for machine learning approaches to become mainstream in protein simulation are discussed.</p>
                <p><strong>Cost:</strong> 0.026</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e13.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e13.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rosetta (threading + relaxation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rosetta structure prediction and energy-based relaxation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used structure prediction/optimization workflow that stitches sequence fragments onto structural templates (threading) and refines models using an all-atom scoring/relaxation protocol (Rosetta). Historically used as the optimization/refinement step in many CASP pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structure prediction for CASP8 with all-atom refinement using rosetta.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Rosetta threading + all-atom relaxation</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Selects template fragments via threading to build an initial 3D model, then refines the model by minimizing a Rosetta energy function (knowledge- and physics-inspired scoring) using local minimization and stochastic sampling (fragment insertion, Monte Carlo/gradient-based relax). The pipeline often integrates co-evolutionary/contact information and ends with all-atom relaxation to a converged structure.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>fragment assembly / knowledge-based refinement / energy minimization</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Use of template structural information (threading), energy-based refinement (minimization/stochastic sampling) and statistical potentials; rooted in the thermodynamic hypothesis that native-like conformations score best under the Rosetta function.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Rosetta scoring function (knowledge-based + physics-inspired terms) as used in all-atom refinement</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Not specified in paper; characterized by multi-stage fragment assembly and iterative relaxation—computational cost depends on number of decoys and refinement steps (practical runtime varies widely).</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Historically used on a wide range of CASP targets; in the paper Rosetta is referenced as part of previous top-ranked CASP workflows (general protein sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not quantified in this paper; described as part of historically top-ranked CASP pipelines prior to deep-learning advancements.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>CASP targets / PDB-derived templates (as used historically in CASP workflows).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Useful templates exist for threading; Rosetta energy function can guide refinement toward native-like local minima; fragment-based moves can explore conformational space sufficiently.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Workflows are complex and rely on availability of templates and expert tuning; performance can be limited when co-evolutionary information is available that supersedes templates; computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Described as part of the historically typical top predictors in CASP; contrasted with AlphaFold which used a simplified, ML-heavy workflow that replaced threading and still outperformed prior pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e13.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold (CASP13 approach)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold deep-learning distance histogram + autoencoder and potential minimization approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep-learning based structure predictor that predicts inter-residue distance distributions from co-evolutionary data and generates 3D structures via an autoencoder-based model or by minimizing a knowledge-based potential derived from predicted distance histograms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>De novo structure prediction with deep-learning based scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>AlphaFold (distance-histogram prediction + autoencoder / knowledge-based potential minimization)</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>A very deep residual convolutional network is trained on co-evolutionary features to predict probability histograms of inter-residue distances; an autoencoder-like generative module maps sequence + distance histograms to 3D coordinates (replacing threading), producing structure models; alternatively, a knowledge-based potential built from predicted distance histograms is minimized to convergence to obtain structures.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>deep learning-based structure prediction; knowledge-based energy minimization</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Exploits co-evolutionary signals to infer distance constraints, then uses learned generative mapping or energy minimization under a histogram-derived potential; implicitly relies on the thermodynamic hypothesis and use of evolutionary information.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Knowledge-based potential derived from predicted inter-residue distance histograms (protein-specific statistical potential)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Not specified; involves deep-residual-network inference plus generative mapping or iterative potential minimization—practical runtime depends on network size and optimization iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>CASP13 blind targets across diverse proteins where sufficient multiple-sequence alignment depth existed (performance highlighted in CASP13).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ranked first in CASP13 (described as winning by a margin); no explicit numeric RMSD/GDT scores provided in this paper; noted to outperform previous methods in CASP13.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>CASP13 blind targets / PDB for training and template knowledge encoding via autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Sufficient co-evolutionary sequence depth (e.g., >=32-64 sequences) provides informative distance constraints; predicted distance distributions are informative enough to reconstruct fold; autoencoder can implicitly encode structural priors from PDB.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance degrades when co-evolutionary information is sparse; potential minimization approach yields protein-specific potentials similar to structure-based models and may not generalize to atomic-detail accuracy required for drug design.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Contrasted with prior complex threading + Rosetta pipelines; achieved superior performance in CASP13 using a much simpler ML-centric workflow, replacing threading with an autoencoder and using learned distance histograms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e13.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>End-to-end differentiable structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>End-to-end differentiable learning of protein structure (sequence → angles → coordinates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that maps sequence directly to backbone angles and 3D coordinates with a differentiable model trained end-to-end using RMSD-based losses against known structures, enabling direct gradient-based learning of structure prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>End-to-end differentiable learning of protein structure.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>End-to-end differentiable protein structure learning</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>A single differentiable neural network transforms sequence information through intermediate representations to predict backbone torsion angles and 3D coordinates; the model is trained end-to-end with a loss computed on coordinates (e.g., RMSD) against known structures, enabling backpropagation through coordinate reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>end-to-end deep learning / differentiable simulator</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Direct supervised learning to map sequence to 3D structure using differentiable geometry and coordinate loss; relies on available structural examples in the PDB.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>No explicit force field; loss function defined on coordinate RMSD against training structures</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Not specified; involves neural network inference and batched gradient computations for training; complexity scales with network size and sequence length.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Applied to datasets of proteins with known structures (as in the referenced work); performance lower than co-evolution-based methods when evolutionary information is available.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not specified numerically in this review; stated that performance does not yet match co-evolution-based methods when co-evolutionary information is abundant.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>PDB structural datasets used for supervised training (as per referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Sufficient training data allows learning direct sequence-to-structure mapping; structural losses (RMSD) are appropriate supervision for learning coordinates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Currently underperforms co-evolutionary deep models when deep MSAs are available; relies on sufficient labeled structural data for training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Described as offering wider applicability (e.g., protein design) but not yet matching co-evolution-based methods' performance in cases with rich evolutionary information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e13.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sequence-conditioned energy + Langevin</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-conditioned neural energy function with Langevin sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where a deep neural network parameterizes a sequence-dependent energy function and Langevin dynamics is used to generate structural samples from that learned energy landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning protein structure with a differentiable simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Neural sequence-conditioned energy model + Langevin dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Parameterize an energy function conditioned on sequence using a deep neural network; perform Langevin dynamics sampling under this learned energy to generate structure ensembles, leveraging stochastic dynamics to explore conformational space according to the learned energy.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>energy-based model + stochastic sampling (Langevin dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Energy-based modeling of conformational probabilities; Boltzmann-like sampling via Langevin (stochastic differential equation) dynamics to draw samples from the learned energy distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Sequence-conditioned neural network energy (learned statistical/energy model), not a classical force field</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Not specified; involves repeated gradient evaluations of the neural energy during Langevin integration—computational cost depends on network complexity and number of sampling steps.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Described conceptually in the review referencing the original work; specific test proteins not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not provided in this review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>Not specified in this review summary.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Learned energy approximates relevant features of true protein energy landscape; Langevin sampling can adequately explore the learned landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on fidelity of learned energy and sampling efficiency; training such an energy requires substantial data and may suffer from poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Mentioned as an alternative to contact/distance-prediction-based pipelines; these energy-based generative approaches are still developing relative to co-evolutionary distance predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e13.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GAN C-alpha generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative adversarial model for C-alpha distance matrices</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GAN-based generative model that produces realistic C-alpha distance matrices for protein blocks (up to 128 residues), which can be converted to backbone and sidechain structures using standard reconstruction methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative modeling for protein structures.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>GAN-based C-alpha distance matrix generator</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Train a generative adversarial network to produce realistic C-alpha distance matrices for protein fragments; post-process generated distance matrices with standard reconstruction tools to rebuild backbone and side chains; a variational autoencoder was used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>generative modeling / adversarial learning</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Generative modeling to learn distribution of C-alpha distance matrices; adversarial training enforces realism of generated matrices.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>No explicit physical force field; generation is data-driven, followed by structure reconstruction using standard geometric/physical procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Not specified; GAN training complexity depends on network sizes and dataset; reconstruction further adds computational steps.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Blocks up to 128 residues; model not conditioned on sequence for the VAE baseline, but GAN conditioned details vary in the referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not given here; reported in the referenced paper as producing realistic distance matrices comparable to VAE baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>PDB-derived structural blocks (as per referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Distance matrix patterns for protein blocks can be learned and generated by adversarial training; subsequent reconstruction can yield plausible 3D geometry.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Block-size limitation (up to 128 residues), not sequence-conditioned in some variants (limiting design applicability); quality depends on reconstruction pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Compared to variational autoencoder baseline, GAN produced realistic matrices; overall approach differs from co-evolutionary prediction pipelines by being generative rather than predictive from MSAs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e13.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Machine-learned force fields (NN potentials)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural-network / ML representations of potential-energy surfaces (e.g., Behler–Parrinello, SchNet, ANI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of machine learning (neural networks and related models) to represent high-dimensional potential energy surfaces and atomic forces, trained on quantum mechanical calculations or other reference data to act as classical force fields.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalized neural-network representation of high-dimensional potential-energy surfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Machine-learned potential energy surfaces (NN potentials, e.g., Behler–Parrinello, SchNet, ANI)</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Train neural networks (or kernel/ML models) to map atomic configurations (with symmetry-preserving descriptors) to energies and forces, enabling MD simulations under the learned potential; variants include molecule-specific and transferable potentials trained on datasets of QM calculations (e.g., ANI), and architectures designed for molecules/materials (e.g., SchNet).</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>learned force field / ML potential for molecular dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Universal function approximation of the potential-energy surface; training on QM energies/forces to reproduce first-principles behavior; enforcing physical symmetries (permutation, rotation, translation) in descriptors or architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Neural-network learned energy function (examples: Behler–Parrinello NN potential, SchNet, ANI family)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Not given in asymptotic notation; described as several orders of magnitude faster than QM but slower than classical empirical force fields; training requires large QM datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Mostly small organic molecules, water, amino acids, small peptides; one example of a 50 ns MD of a cellulose-binding domain (1EXG) in folded state; recently tested on polypeptides; not yet used for full protein folding simulations or robust thermodynamic/kinetic predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported accuracy on QM energies and forces in referenced works (not numerically reproduced in this review); in current practice, accurate for small molecules but not yet validated for folding thermodynamics/kinetics of proteins within this review.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>QM datasets of small molecules, water, amino acids, small peptides; ANI trained on large QM datasets of small molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Locality/transferability of learned local energy contributions approximates larger-scale interactions; training data is sufficiently representative of relevant configurational space.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Potentially poor extrapolation to regions of configuration space not in training set; challenges modeling long-range electrostatics/van der Waals (locality); computational overhead relative to classical force fields; transferability to large proteins and folding remains an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Positioned between expensive QM and fast empirical force fields: more accurate than empirical FFs in QM-fitted regimes, much faster than QM, but currently limited in system size and sampling compared to established classical force-field MD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e13.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CGnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CGnet neural network coarse-grained force field</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network based coarse-grained force field that directly represents an effective energy function between coarse-grained beads and is trained to reproduce atomistic reference data; applied to simulate folding/unfolding of a small protein (Chignolin).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Machine learning of coarse-grained molecular dynamics force fields.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>CGnet (neural-network coarse-grained force field)</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Maps atomic configurations to a coarse-grained representation, uses a neural network to represent the effective potential energy of the coarse-grained beads including multi-body interactions, trains the network on atomistic MD data (forces/energies) to reproduce thermodynamics and dynamics, and uses the learned energy in coarse-grained MD to obtain free-energy landscapes and folding/unfolding dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>coarse-grained molecular dynamics with learned potential (energy-based ML model)</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Renormalization of atomistic degrees of freedom into effective beads yields multi-body terms; neural networks capture non-linear, many-body effective interactions; aims to reproduce free-energy landscapes and kinetics of the reference system.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Neural-network learned coarse-grained effective energy (CGnet)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Training requires atomistic MD data and network optimization; inference (energy/force eval) is slower than simple pairwise coarse-grained potentials but significantly faster than QM; exact runtimes not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Small protein Chignolin (folding free energy landscape and folding/unfolding dynamics shown); applications so far are system-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Free-energy landscape and identification of metastable minima for Chignolin shown (no numerical RMSD reported in this review); CGnet reproduced folding/unfolding behavior consistent with reference atomistic data as per ref [5].</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>Atomistic MD reference trajectories (as used to train the coarse-grained model for Chignolin).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Coarse-graining mapping is appropriate; atomistic training data sufficiently samples relevant configurations; learned neural energy can capture emergent multi-body terms.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>So far system-specific (transferability is an outstanding issue); defining general coarse-grained mappings and training datasets for transferability remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Presented as an advance over hand-crafted coarse-grained potentials by automatically capturing multi-body non-linearities; transferability trade-offs compared to classical coarse-grained models remain to be systematically studied.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e13.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gō models / structure-based models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structure-based (Gō) models and AWSEM-MD</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simplified models that bias interactions to favor native contacts (Gō models) or combine physical coarse-grained potentials with bioinformatics-based local biases (AWSEM-MD), grounded in the minimal frustration principle and energy landscape theory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Topological and energetic factors: what determines the structural details of the transition state ensemble and “en-route” intermediates for protein folding?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Structure-based (Gō) models and AWSEM-MD</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Gō models assign attractive interactions only between residues that are in contact in the native structure to create a funnel-shaped energy landscape; AWSEM-MD uses coarse-grained physical potentials augmented with bioinformatically-derived local structure biases to predict folding behavior; both are used in folding theory and prediction by exploiting native-centric energy terms.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>coarse-grained, knowledge-based / structure-based modeling</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Energy landscape theory; minimal frustration principle asserting that native interactions guide folding toward the native basin; native-centric modeling simplifies landscape.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Native-contact-based potentials (Gō), AWSEM coarse-grained potential with bioinformatically based local biases</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Typically low computational cost due to simplified representations; exact complexity depends on model specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Small globular proteins and model systems (classically used in theoretical folding studies).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used historically to reproduce qualitative folding pathways, transition states, and en-route intermediates; specific numerical metrics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>Model proteins and small globular proteins with known native structures.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>The native state is known (or approximated) and is the global minimum; non-native interactions are less important (minimal frustration assumption).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Not generally predictive when the native structure is unknown; oversimplifies energetics and may miss non-native-driven behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Compared conceptually to the knowledge-based potential derived from AlphaFold distance histograms (noted to be similar to structure-based models).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e13.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Markov State Models (MSMs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Markov state models of molecular kinetics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Shallow, data-driven models that discretize conformational space and estimate a Markovian transition matrix to model long-timescale kinetics and equilibrium properties from ensembles of short MD trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Markov models of molecular kinetics: Generation and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Markov state model (MSM) construction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Select molecular features, reduce dimensionality, cluster conformations, estimate transition probabilities/rates between clusters at a chosen lag time to build a Markov transition matrix that predicts equilibrium distributions and slow kinetics; can be coarse-grained to metastable states and validated (e.g., Chapman–Kolmogorov test).</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>statistical kinetics model / Markov model</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Markovian approximation of slow dynamics in appropriately chosen collective variables; ergodicity and time-scale separation assumptions enable extrapolation from short trajectories to long-time kinetics.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>N/A (MSMs are analysis models built on trajectories generated under some force field)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Complexity depends on feature dimensionality, clustering algorithm, and matrix estimation (matrix operations scale with number of states); no explicit notation provided.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Applied broadly to peptide and protein folding problems; specific example in paper includes NTL9 folding analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Performance validated via Chapman–Kolmogorov tests and ability to predict long-time kinetics from short trajectories; specific numeric values not reproduced in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>MD trajectory datasets (short trajectories aggregated), NTL9 protein datasets, and other simulation benchmarks referenced in literature.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>If built in true slow CVs, Markovian approximation error is small; sufficient sampling of relevant transitions by short trajectories; choice of features/clustering impacts quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pipeline is error-prone and sensitive to feature selection, dimensionality reduction, and clustering; requires expertise to construct robust MSMs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Described as a cornerstone for extracting slow CVs and kinetics; VAMP/VAMPnets and variational approaches are presented as principled improvements that replace handcrafted pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e13.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VAMPnets (VAC/VAMP approach)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VAMPnets: Deep learning of molecular kinetics using the variational approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end neural-network approach that learns slow collective variables and few-state Markov models by optimizing a variational score (VAMP), replacing the manual MSM pipeline with a single trainable encoder and optional classification output.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Vampnets: Deep learning of molecular kinetics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>VAMPnets</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>A neural network encoder maps molecular configurations x_t to latent slow coordinates y_t; trained on paired time-lagged samples (y_t, y_{t+tau}) to maximize the VAMP score (variational objective for Markov processes), producing either continuous reaction coordinates or discrete metastable state assignments and an associated Markov propagator/transition matrix.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>deep learning-based variational Markov model / end-to-end kinetics learning</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Variational approach to conformational dynamics (VAC/VAMP) which provides an objective function whose maximizer approximates the slowest dynamical modes; combines with deep learning for representation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>N/A (analysis/learning method applied to MD trajectory data)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Training complexity scales with network architecture, dataset size, and lag-time sampling; not specified in asymptotic form.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Demonstrated on benchmark problems including protein folding; example in paper: NTL9 protein folding and hierarchical decompositions shown.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Shown to learn high-quality MSMs and pass Chapman–Kolmogorov tests for NTL9 at tau = 320 ns in the referenced experiments; specific numeric errors not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>MD trajectories for proteins including NTL9 and other benchmark systems described in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Appropriate choice of lag time and sufficient trajectory data; VAMP variational principle applicable to the system dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As with neural models, requires training data and hyperparameter choices; interpretability depends on architecture and whether final layer does classification into discrete states.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Presented as replacing the full MSM pipeline, reducing need for manual feature selection and clustering while producing comparable or superior kinetic models when trained appropriately.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e13.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TICA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Time-lagged Independent Component Analysis (TICA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A linear dimensionality reduction technique for time series that identifies slow collective coordinates by maximizing autocovariance at a given lag time; used to project MD data onto slow modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Identification of slow molecular order parameters for Markov model construction.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Time-lagged Independent Component Analysis (TICA)</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Compute time-lagged covariance matrices from trajectory features and solve a generalized eigenvalue problem to obtain linear projections (TICs) that maximize time-autocorrelation, providing low-dimensional reaction coordinates representing slow dynamical processes.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>linear dimensionality reduction / time-series analysis</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Maximization of autocovariance (slowness) at a specified lag time; related to variational approaches for slow dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>N/A (analysis tool)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Dominated by covariance matrix estimation and eigenvalue decomposition; scales with number of features (practical complexity O(n_features^3) for dense eigenproblems but often manageable with sparse/optimized solvers).</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Used broadly in MD analysis; in this paper TICA projections used for CGnet Chignolin free-energy landscape visualization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Provides TICs used to visualize and identify metastable basins; numeric eigenvalues/timescales not listed in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>MD simulation datasets (e.g., Chignolin trajectories in this paper's figure).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Slow modes are approximately linear combinations of chosen input features; lag time chosen is appropriate to separate timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Linear method may not capture strongly nonlinear slow modes; depends on choice of input features.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>TICA often used upstream of MSM construction and as an explicit comparison/alternative to nonlinear encoders (e.g., VAMPnets/autoencoders).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e13.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Adaptive sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adaptive sampling (MD rounds with model-guided restart selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative protocol that runs multiple short MD trajectories in rounds, selecting starting states for subsequent rounds based on models (MSMs, learned CVs) to efficiently explore rare-event transitions and reconstruct equilibrium kinetics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Quantitative comparison of adaptive sampling methods for protein dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Adaptive sampling of MD</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Run batches of short unbiased MD trajectories, analyze accumulated data to build/update a model of slow CVs or metastable states (e.g., MSM or neural CVs), choose under-sampled or informative starting states for next round to accelerate discovery of rare transitions while retaining ability to reconstruct equilibrium kinetics via MSM/VAMP analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>enhanced sampling / adaptive simulation</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Exploit model-based selection to focus sampling on kinetically relevant regions; unbiased short trajectories allow re-weighting into equilibrium statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Runs with the underlying MD force field (classical or ML-based) used to generate trajectories; adaptive strategy is orthogonal to force field choice.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Operationally scales with number of rounds and trajectories; overall wall-clock efficiency often improved versus single long trajectories but dependent on model update costs.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Applied to ligand binding, protein-protein association/dissociation, and folding problems; referenced example: sampling protein–protein association with equilibrium timescales of hours (ref [65]).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Enables reversible sampling of association/dissociation and access to long timescales through parallel short trajectories; quantitative speedups vary by system (not numerically specified in the review).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>MD trajectory collections generated during adaptive campaigns; prior work compared methods across benchmark systems (see referenced comparative study).</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Model used to select starting states is sufficiently informative to guide sampling; short unbiased trajectories can be combined to reconstruct equilibrium kinetics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Quality depends on model accuracy used for selection; requires iterative coordination between simulation and analysis; may miss rare events if model guidance is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Compared favorably in enabling equilibrium sampling and kinetics reconstruction using MSMs/VAMPnets; alternative to biasing methods like metadynamics which change thermodynamic state.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e13.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Enhanced sampling methods (metadynamics, replica-exchange, umbrella, VES)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Enhanced sampling techniques: metadynamics, replica-exchange, umbrella sampling, variationally enhanced sampling (VES)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of algorithms that accelerate sampling of rare events by biasing the dynamics (metadynamics), using multiple thermodynamic replicas (replica-exchange), restraining collective variables (umbrella sampling), or optimizing a bias variationally (VES); their effectiveness depends on choice of collective variables.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Variational approach to enhanced sampling and free energy calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Enhanced sampling (metadynamics, replica-exchange, umbrella sampling, VES)</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Introduce bias potentials (history-dependent in metadynamics), run multiple replicas at different conditions (replica-exchange), or apply umbrella restraints to sample rare transitions; VES frames bias as a variational optimization problem to accelerate sampling; machine learning is used to iteratively discover or adapt collective variables to improve these methods.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>enhanced sampling / biased simulation</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Modify dynamics to overcome free-energy barriers and sample rare transitions; reweighting techniques recover equilibrium properties; variational principles can optimize biases.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Underlying MD force field (classical or ML-derived) used; added bias potentials are algorithm-specific (history-dependent or variationally optimized).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Increases cost due to additional replicas or bias evaluation; effectiveness depends on CV dimensionality and bias update overhead.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Widely applied across proteins, peptides, and biomolecular systems; specific examples cited in review for enhanced sampling of folding landscapes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to accelerate sampling of folding/unfolding and other rare events; specific numeric speedups or convergence metrics not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>MD simulations and free-energy landscapes reconstructed via reweighting; referenced methodological studies and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Existence of low-dimensional CVs that capture relevant slow transitions; ability to reweight biased samples to target thermodynamic states.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Performance sensitive to poor choice of CVs which can lead to ineffective or counterproductive sampling; typically loses direct equilibrium kinetic information unless corrected by reweighting or multi-ensemble models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Machine learning methods to identify/adapt CVs (e.g., autoencoders, adversarial learning, VAMP-based CVs) can improve enhanced sampling performance; alternatives like adaptive sampling preserve unbiased dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e13.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e13.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of algorithmic and deterministic approaches to protein folding, including physics-based methods, energy minimization algorithms, force field approaches, and rule-based folding methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boltzmann Generator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Boltzmann generators - sampling equilibrium states with deep learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deep generative modeling approach that trains networks to directly sample from the equilibrium (Boltzmann) distribution of many-body systems, enabling direct generation of equilibrium configurations without running MD.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Boltzmann generators - sampling equilibrium states of many-body systems with deep learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_name</strong></td>
                            <td>Boltzmann Generator (deep generative sampling of Boltzmann distribution)</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_description</strong></td>
                            <td>Train a deep generative model (normalizing flow / related architectures) to map simple latent distributions to configuration space while minimizing KL divergence to the Boltzmann distribution defined by an energy function; once trained, the generator produces independent equilibrium samples that can be reweighted to recover exact statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>algorithm_type</strong></td>
                            <td>generative sampling / ML-based equilibrium sampling</td>
                        </tr>
                        <tr>
                            <td><strong>is_deterministic</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>underlying_principles</strong></td>
                            <td>Generative modeling with importance reweighting to match Boltzmann probabilities; leverages deep networks to learn complex multi-dimensional distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>force_field_or_energy_function</strong></td>
                            <td>Samples from equilibrium distribution defined by a given energy function (classical or ML-based); training uses the target energy in the loss.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_complexity</strong></td>
                            <td>Training can be substantial; sampling after training is efficient, producing independent samples; exact costs depend on architecture and dimensionality.</td>
                        </tr>
                        <tr>
                            <td><strong>protein_types_tested</strong></td>
                            <td>Demonstrated on model many-body systems; potential applicability to molecular systems and proteins discussed (see referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not numerically detailed in this review; referenced work demonstrates sampling efficiency gains in example systems.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_datasets</strong></td>
                            <td>Energy-defined many-body systems and MD reference data for training/validation in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_templates</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>is_interpretable</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_assumptions</strong></td>
                            <td>Generative model capacity sufficient to represent Boltzmann distribution; training can converge in high-dimensional molecular spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Challenging to scale to very high-dimensional biomolecular systems; training complexity and representational requirements may be large.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_methods</strong></td>
                            <td>Offered as an alternative to MD-based sampling and enhanced-sampling methods by producing direct equilibrium samples; integration with reweighting allows recovery of exact statistics under the target energy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine learning for protein folding and dynamics.', 'publication_date_yy_mm': '2019-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Machine learning of coarse-grained molecular dynamics force fields. <em>(Rating: 2)</em></li>
                <li>Vampnets: Deep learning of molecular kinetics. <em>(Rating: 2)</em></li>
                <li>Generalized neural-network representation of high-dimensional potential-energy surfaces. <em>(Rating: 2)</em></li>
                <li>SchNet - a deep learning architecture for molecules and materials. <em>(Rating: 2)</em></li>
                <li>ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. <em>(Rating: 2)</em></li>
                <li>De novo structure prediction with deep-learning based scoring. <em>(Rating: 2)</em></li>
                <li>End-to-end differentiable learning of protein structure. <em>(Rating: 2)</em></li>
                <li>Learning protein structure with a differentiable simulator. <em>(Rating: 2)</em></li>
                <li>Generative modeling for protein structures. <em>(Rating: 2)</em></li>
                <li>Markov models of molecular kinetics: Generation and validation. <em>(Rating: 2)</em></li>
                <li>Identification of slow molecular order parameters for Markov model construction. <em>(Rating: 1)</em></li>
                <li>Variational approach to enhanced sampling and free energy calculations. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-97 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-97</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-97</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-6212c08e593aa2a0b767b81d73e5f925b2dd5597</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6212c08e593aa2a0b767b81d73e5f925b2dd5597" target="_blank">Spike-Based Bayesian-Hebbian Learning of Temporal Sequences</a></p>
                <p><strong>Paper Venue:</strong> PLoS Comput. Biol.</p>
                <p><strong>Paper TL;DR:</strong> A modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network (BCPNN) learning rule, finding that the formation of distributed memories can be acquired through plasticity.</p>
                <p><strong>Paper Abstract:</strong> Many cognitive and motor functions are enabled by the temporal representation and processing of stimuli, but it remains an open issue how neocortical microcircuits can reliably encode and replay such sequences of information. To better understand this, a modular attractor memory network is proposed in which meta-stable sequential attractor transitions are learned through changes to synaptic weights and intrinsic excitabilities via the spike-based Bayesian Confidence Propagation Neural Network (BCPNN) learning rule. We find that the formation of distributed memories, embodied by increased periods of firing in pools of excitatory neurons, together with asymmetrical associations between these distinct network states, can be acquired through plasticity. The model’s feasibility is demonstrated using simulations of adaptive exponential integrate-and-fire model neurons (AdEx). We show that the learning and speed of sequence replay depends on a confluence of biophysically relevant parameters including stimulus duration, level of background noise, ratio of synaptic currents, and strengths of short-term depression and adaptation. Moreover, sequence elements are shown to flexibly participate multiple times in the sequence, suggesting that spiking attractor networks of this type can support an efficient combinatorial code. The model provides a principled approach towards understanding how multiple interacting plasticity mechanisms can coordinate hetero-associative learning in unison.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e97.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e97.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BCPNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-based Bayesian Confidence Propagation Neural Network learning rule</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A spike-based, probabilistic Hebbian learning rule that estimates pre/post firing probabilities via local exponentially filtered traces (Z and P traces), computes log-probability intrinsic biases and synaptic weights, and multiplexes positive (excitatory) and negative (interpreted as disynaptic inhibitory) weight components; plasticity is gated by a global third-factor κ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spike-Based Bayesian-Hebbian Learning of Temporal Sequences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Spike-based BCPNN (Bayesian-Hebbian)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>BCPNN uses local spike trains S_i and S_j filtered into fast Z traces (exponentially weighted moving averages) and slower P traces that estimate firing probabilities P_i, P_j and co-activation P_ij. Synaptic weights are computed as w_ij = w_gain * log(P_ij / (P_i P_j)) and intrinsic excitability as β_j = β_gain * log(P_j). Z-trace time constants are specified per receptor (e.g., τ_z^{AMPA} = 5 ms presynaptic, τ_z^{NMDA} = 150 ms presynaptic; postsynaptic τ_z = 5 ms), while P traces integrate over a much slower τ_p (set to 5000 ms in simulations). A global modulatory gating factor κ ∈ [0,1] (interpreted as a neuromodulator-like third factor) enables/disables plasticity by modulating P-trace updates; the rule yields both positive weights (monosynaptic excitation) and negative weights interpreted as disynaptic inhibition.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Multi-scale: fast Z traces operate on milliseconds (AMPA τ_z ≈ 5 ms; NMDA presynaptic τ_z ≈ 150 ms), P-traces integrate over seconds (τ_p set to 5000 ms in simulations; authors note τ_p can range seconds→months biologically), plasticity gating κ can act phasically (sub-second to seconds), learning epochs and behavioral training intervals span 100 ms → seconds.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Neocortical microcircuit model with pyramidal cells and local basket interneurons organized into minicolumns and hypercolumns (modelled cortical columnar architecture).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Recurrent excitatory connections among pyramidal cells across the network (BCPNN-plastic AMPA and NMDA synapses, connection probability 0.25, distance-dependent delays), local recurrent inhibitory feedback via basket cells within hypercolumns (static AMPA pyramidal→basket and GABA basket→pyramidal with p=0.7), modular organization into hypercolumns (soft WTA) and minicolumns.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Hetero-associative temporal sequence learning and auto-associative attractor formation (encoding and recall of ordered sequences; sequence completion and disambiguation).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (spiking AdEx neuron simulations implementing BCPNN and synaptic dynamics).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast receptor dynamics and Z-traces (ms) capture immediate spike timing and stabilize within-attractor activity via AMPA; slower presynaptic NMDA Z-traces (≈150 ms) create temporal windows for forming asymmetric hetero-associations between sequential patterns; P-traces (seconds in simulation) accumulate co-activation statistics to produce longer-lasting synaptic/intrinsic changes (weights and β) and represent memory that decays on a slower palimpsest timescale. The global κ gating coordinates when fast events are 'printed' into the slow P memory, allowing interplay between fast stimulus-driven events and slower consolidation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BCPNN with receptor-specific Z-trace time constants enables learning of attractor states (AMPA auto-association) and asymmetric feedforward associations (NMDA hetero-association) that depend on temporal statistics of training (short inter-pulse intervals yield forward-directed NMDA weights and deterministic lag-1 transitions). NMDA-mediated asymmetry is critical for directionality of recall (switching NMDA pre/post time constants reverses replay direction). The P-trace time constant controls learning speed and memory persistence; gating κ (neuromodulatory-like) prevents interference during interleaved quiescence. Negative log-weight components functionally implement disynaptic inhibition, supporting competition and pattern separation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative metrics reported include: Z and P time constants (τ_z^{AMPA}=5 ms, τ_z^{NMDA}=150 ms, τ_p=5000 ms in sims), connection probabilities (plastic inter-pyramidal p=0.25; local p=0.7), recall speeds (3–7 attractors/s across parameter variations), compression factor < 1 (recall often compressed relative to training), CRP (conditional response probability) curves to quantify transition statistics (chance-level transitions reached ~IPI = 1500 ms), average dwell time μ_dwell as function of IPI, average NMDA synaptic strength vs IPI, ISI CV ≈ 1, and learning converged within ~10–50 training epochs in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Yes — slower P-traces act as a consolidation/eligibility-like memory that accumulates co-activation statistics (potentially implemented biologically by AMPA phosphorylation, mobilization, gene expression and protein synthesis). The κ gate functions as a neuromodulatory print-now signal (e.g., dopamine-like) that determines when fast events (Z-trace evidence) are integrated into the slower P memory; P traces decay in a palimpsest manner, providing slow forgetting and stabilization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spike-Based Bayesian-Hebbian Learning of Temporal Sequences', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e97.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e97.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Timing-Dependent Plasticity (phenomenological STDP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A timing-dependent Hebbian rule in which the sign and magnitude of synaptic change depend on the millisecond-scale timing difference between pre- and postsynaptic spikes; commonly used in sequence learning models but noted here to be unstable without complementary mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spike-Based Bayesian-Hebbian Learning of Temporal Sequences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>STDP</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Phenomenological STDP modifies synaptic strength based on relative spike timing (pre-before-post produces potentiation; post-before-pre produces depression) with a timing window on the order of tens of milliseconds. Authors note STDP's temporal causality benefits feed-forward development but also its inherent instability in isolation and thus the need for additional stabilizing mechanisms (normalization, neuromodulation, intrinsic plasticity) for reliable temporal sequence learning.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Milliseconds (typical STDP windows tens of ms), with longer-term stability issues requiring seconds→minutes mechanisms for stabilization.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Discussed broadly in context of cortex/sequence learning (not directly implemented in this model).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Often used to form feed-forward chains / asymmetric connections; here contrasted with BCPNN which implements hetero-association via receptor time constants.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Temporal sequence learning (discussed as prior work), causal feedforward structural development.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mention / related-work (experimental and modelling literature referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Paper emphasizes that STDP typically needs to be combined with slower processes (weight normalization, neuromodulation, intrinsic plasticity) to avoid instability; i.e., fast STDP (ms) interacts with slower homeostatic/metaplastic processes (s→min→hours).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>STDP is acknowledged as commonly used for sequence learning but insufficient alone due to instability; complementary mechanisms are typically required (and BCPNN provides an integrated probabilistic framework that subsumes some of these aspects).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>No direct quantitative measures in this paper for STDP (it is cited in discussion of related approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Implicitly required: STDP models generally need slower stabilizing processes (normalization, intrinsic excitability changes, neuromodulatory gates) to consolidate fast timing-dependent changes; paper argues BCPNN naturally integrates multiple timescales.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spike-Based Bayesian-Hebbian Learning of Temporal Sequences', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e97.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e97.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Short-term depression (STD)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Short-term synaptic depression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dynamic, use-dependent decrease of available synaptic resources that reduces synaptic efficacy transiently during sustained activity and recovers with a characteristic recovery time, contributing to attractor termination and timing of transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spike-Based Bayesian-Hebbian Learning of Temporal Sequences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Short-term depression (Tsodyks–Markram style)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Modeled as depletion of synaptic resources that multiply peak conductance (dynamic scaling of g_ij^{syn}) during repeated presynaptic activation; characterized by a utilization parameter U (magnitude) and a recovery time constant τ_rec. In the model STD reduces effective AMPA-mediated recurrent drive over hundreds of milliseconds→seconds, accelerating attractor decay.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Fast-to-intermediate: depression onset during sustained spiking (tens to hundreds of ms) with recovery time constants tested on the order of hundreds of ms to seconds (τ_rec variable in parameter sweeps).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Neocortical microcircuit (pyramidal→pyramidal synapses across hypercolumns/minicolumns).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Acts on recurrent excitatory connections (BCPNN-plastic synapses) and shapes dynamics of attractors and transitions by transiently weakening recurrent loops; cooperates with inhibition and adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports temporal segmentation of activity (helps determine dwell times and sequence speed) rather than long-term associative storage; facilitates sequence replay and temporal ordering by imposing fatigue.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Used in computational model (implemented in simulations with parameters varied).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>STD works on short timescales (ms→s) and interacts with AMPA/NMDA-mediated fast and slower synaptic currents and with slow P-trace consolidation: STD causes transient decreases in efficacy that enable attractor termination on a timescale shorter than the P-trace consolidation, thereby permitting temporal ordering while long-term weights remain intact.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>STD contributes to termination of attractor states and hence to dwell times and replay speed; increasing STD magnitude (U) or τ_rec speeds up recall by making attractor lifetimes shorter; STD interacts with adaptation and NMDA strength to set replay speed and compression.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Parameter sweeps reported: variations in utilization U and recovery τ_rec altered recall speed (plots show recall speeds and standard deviations), and compression factor analyses (for trained speed = 2 patterns/s) quantify allowable compression ranges under STD manipulation; exact numeric τ_rec values varied in sweeps (figures), and increasing τ_rec modestly increased recall speed.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No direct consolidation role; STD is a transient resource-based mechanism that shapes short-term dynamics but does not store long-term changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spike-Based Bayesian-Hebbian Learning of Temporal Sequences', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e97.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e97.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Columnar/WTA Connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Modular columnar connectivity with soft winner-take-all hypercolumns and minicolumns</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular recurrent architecture: networks of minicolumns grouped into hypercolumns implementing soft WTA competition via local basket-cell mediated inhibition, with sparse long-range recurrent excitatory connectivity among pyramidal cells mediating attractor states and sequence transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Spike-Based Bayesian-Hebbian Learning of Temporal Sequences</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>N/A (connectivity pattern rather than a synaptic plasticity rule)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Structure: nine hypercolumns, each with ten minicolumns; each minicolumn contains a population of pyramidal neurons that project to local basket cells (pyramidal→basket AMPA, basket→pyramidal GABA, p=0.7). Long-range recurrent pyramidal→pyramidal BCPNN-plastic connections are formed randomly (p=0.25) across minicolumns with distance-dependent delays. The soft WTA dynamics implemented by local inhibition produce sparse, competitive activations (attractors) and irregular spiking.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Topology shapes dynamics across ms→s: WTA competition and inhibition operate on ms timescales (GABA/AMPA currents), while recurrent long-range NMDA-mediated interactions influence transitions on 100 ms timescales and P-traces integrate statistics over seconds.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Neocortical microcircuit (cortical columnar organization; minicolumns and hypercolumns; pyramidal cells and basket interneurons).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Modular recurrent architecture: dense local inhibition implementing WTA within hypercolumns, sparse recurrent excitatory inter-minicolumn connectivity, both AMPA and NMDA receptor-mediated long-range connections, and disynaptic inhibition instantiated via negative BCPNN weights.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports associative memory attractor formation, temporal sequence learning, sequence completion and disambiguation via modular, competitive dynamics and learned asymmetric recurrent weights.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Used in computational model (architecture explicitly implemented in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Modular connectivity enables fast local competition (ms) that stabilizes sparse representations and prevents runaway activity, while slower long-range NMDA-mediated recurrent connectivity (≈150 ms) and P-trace consolidation (seconds) support sequential associations across time; local adaptation and STD (ms→s) set attractor lifetimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Columnar/WTA modularity reconciles irregular cortical spiking with attractor dynamics; soft WTA hypercolumns produce sparse attractors and competition necessary for sequential transitions; learned asymmetric NMDA connections across modules determine sequence order; negative weights implement functional inhibition across modules enabling pattern separation and sequence disambiguation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Network composition used in simulations: 2700 pyramidal + 270 basket cells; N_HC = 9 hypercolumns; N_MC = 10 minicolumns per hypercolumn; within-hypercolumn connections static with p=0.7; long-range plastic p=0.25. Performance metrics (as above) include recall speeds (3–7 atr/s), CRP curves for transition statistics, and success in overlapping sequence completion/disambiguation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>The connectivity pattern itself does not consolidate, but its interaction with BCPNN's slow P-traces provides consolidation of associations formed across modules; gating κ prevents interference during training to allow selective imprinting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Spike-Based Bayesian-Hebbian Learning of Temporal Sequences', 'publication_date_yy_mm': '2016-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-84 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-84</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-84</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-2b7b4e43c642847d97821435151eb649d50415e9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2b7b4e43c642847d97821435151eb649d50415e9" target="_blank">Networks that learn the precise timing of event sequences</a></p>
                <p><strong>Paper Venue:</strong> bioRxiv</p>
                <p><strong>Paper TL;DR:</strong> This work proposes a mechanism for learning both the order and precise timing of event sequences, and chooses short term facilitation as a time-tracking process, and demonstrates that other mechanisms, such as spike rate adaptation, can fulfill this role.</p>
                <p><strong>Paper Abstract:</strong> Neuronal circuits can learn and replay firing patterns evoked by sequences of sensory stimuli. After training, a brief cue can trigger a spatiotemporal pattern of neural activity similar to that evoked by a learned stimulus sequence. Network models show that such sequence learning can occur through the shaping of feedforward excitatory connectivity via long term plasticity. Previous models describe how event order can be learned, but they typically do not explain how precise timing can be recalled. We propose a mechanism for learning both the order and precise timing of event sequences. In our recurrent network model, long term plasticity leads to the learning of the sequence, while short term facilitation enables temporally precise replay of events. Learned synaptic weights between populations determine the time necessary for one population to activate another. Long term plasticity adjusts these weights so that the trained event times are matched during playback. While we chose short term facilitation as a time-tracking process, we also demonstrate that other mechanisms, such as spike rate adaptation, can fulfill this role. We also analyze the impact of trial-to-trial variability, showing how observational errors as well as neuronal noise result in variability in learned event times. The dynamics of the playback process determines how stochasticity is inherited in learned sequence timings. Future experiments that characterize such variability can therefore shed light on the neural mechanisms of sequence learning.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e84.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e84.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rate-based LTP/LTD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rate-based long-term potentiation and depression (LTP/LTD) with soft bounds</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Hebbian, rate-based plasticity rule in which synaptic strength increases when pre- and post-synaptic populations are coactive (LTP) and decays when presynaptic activity is high but the postsynaptic response is weak (LTD); includes temporal asymmetry via a presynaptic delay and soft upper/lower bounds on weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Rate-based LTP/LTD (Hebbian; rate-correlation rule)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Implemented by Eq. (2): œÑ_w dw/dt = -Œ≥_d w u_pre(t-D) (M - u_post(t)) + Œ≥_p (w_max - w) u_pre(t-D) u_post(t). Key mechanisms: (1) LTD term proportional to w and active when pre is active and post is inactive, causing exponential decay toward 0; (2) LTP term proportional to (w_max - w) when pre and post are coactive, pushing w toward w_max; (3) pre‚Üípost temporal asymmetry via delay D (causal dependence on presynaptic activity); (4) soft bounds via factors (w_max - w) and multiplicative w in LTD. Parameters given: œÑ_w (learning timescale, default 150 s), Œ≥_d (LTD strength, default 150), Œ≥_p (LTP strength, default 3614.5), w_max (e.g., 0.4852), M (LTD threshold, default 1), D (delay, default 30 ms).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Multiple: presynaptic delay D ~30 ms (millisecond-scale causal asymmetry); plasticity proto-weight dynamics operate on seconds (rate-correlation detection during/after presentations); primary long-term update timescale œÑ_w = 150 s (minutes-scale); in extended model actual weight consolidation occurs on much longer œÑ_I (minutes to hours) as described in the proto‚Üíactual weight formulation.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortical-like recurrent excitatory population network with a global inhibitory population (model is generic cortex-like; motivated by cortical circuits and clustered pyramidal populations).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Recurrent excitatory populations (self-excitation w_jj) plus excitatory-to-excitatory inter-population connections w_jk that are plastic; global inhibitory population provides competition; plasticity selectively strengthens feedforward links between consecutively activated populations (w_{j+1,j}) while weakening non-consecutive links.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Sequence learning encoding both order and precise timing of events (long-term memory of event order and duration; associative Hebbian learning of sequential transitions).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (rate-based population model) using biologically-motivated plasticity equations and analytic derivations, with numerical simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Long-term plasticity (minutes-scale œÑ_w and longer consolidation œÑ_I) shapes baseline synaptic weights that encode desired inter-event activation thresholds; short/medium timescale processes (see other entries: short-term facilitation œÑ_f ~1 s or adaptation œÑ_a ~400 ms) act during replay to produce temporally precise activation; the learned long-term weights determine when those slower, transient processes cross thresholds to trigger subsequent population activations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Rate-based LTP/LTD during repeated presentations drives weights to fixed points that are monotonic functions of the presynaptic activation duration (closed-form expression Eq. (6)); training selectively strengthens w_{j+1,j} and drives other w_jk to zero if not consecutively activated; matching analytic expressions for learned weight (Eq. 6) and required replay weight (Eq. 9) allows one to select plasticity parameters so that replay timing matches training timing across durations (constraints in Eq. 12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Analytic convergence to fixed point w_{jk}^‚àû given by Eq. (6); convergence rate per trial ‚àù exp(-T Œ≥_d / œÑ_w) (so numerical speed of convergence depends on stimulus duration and Œ≥_d/œÑ_w); example parameter values used: œÑ_w = 150 s, Œ≥_d = 150, Œ≥_p = 3614.5, w_max = 0.4852; number of training trials m = 10 used in simulations; RMSE of replay times computed under noise (root-mean-square error defined in Methods).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Yes ‚Äî two-stage plasticity: (1) proto-weights (rate-correlation detection) evolve on seconds and converge to analytical fixed points (Eq. 5/6); (2) actual synaptic weights W_jk exponentially approach proto-weights on a slower timescale œÑ_I (minutes to hours) with a proto‚Üíactual delay D_p, modeling consolidation of transient learning into long-term weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Networks that learn the precise timing of event sequences', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e84.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e84.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Short-term facilitation (STF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Short-term synaptic facilitation (Tsodyks‚ÄìMarkram style)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A short-term plasticity mechanism that transiently increases the effective synaptic efficacy of an active population over seconds, implemented as a facilitation variable p_j(t) that multiplies baseline weights so effective weight = w_jk p_k.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Short-term facilitation (facilitation dynamics of synaptic efficacy)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Modeled by œÑ_f dp_j/dt = 1 - p_j + (p_max - 1) u_j, with baseline p_j ‚àà [1, p_max] (here p_max = 2). When a presynaptic population u_j is active, p_j rises toward p_max on timescale œÑ_f, increasing the effective synaptic strength w_jk p_j; when presynaptic activity ceases, p_j decays back to 1. Parameters: œÑ_f default 1 s, p_max default 2.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Seconds-range: œÑ_f = 1 s (facilitation evolves over ‚àº1 second, much slower than neuronal firing œÑ = 10 ms but faster than long-term weight consolidation œÑ_w = 150 s).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Modeled for cortical pyramidal-cluster networks (motivation from cortical synapses); general to excitatory inter-population synapses.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Acts multiplicatively on inter-population synapses w_jk so that effective feedforward coupling w_{j+1,j} p_j grows during activation; used in recurrent architecture where short-term facilitation of an active cluster gradually increases its drive to the next cluster.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Temporal encoding for sequence replay ‚Äî supports temporally precise replay by serving as a slow, transient 'clock' that determines when an active population will recruit the next one.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical implementation based on established STP phenomenology (Tsodyks et al. 1998 style); used in simulations and analytic derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>STF (seconds) provides the time-tracking process during replay: long-term weights set baseline w and STF p(t) evolves so that w p(t) reaches activation threshold at a time T determined by w (Eq. 9), enabling precise replay; long-term plasticity (minutes+) learns w so that replay times produced by STF match training durations ‚Äî interaction implemented by equating learned weight w^‚àû(T) to required weight ùí≤(T) (Eq. 11/12).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Short-term facilitation can act as the time-tracker: replay activation time T is given analytically by ùíØ(w) = œÑ_f ln((p_max - 1)/(p_max - Œ∏/w)) (Eq. 10); by adjusting long-term weights via LTP/LTD, network learns w such that facilitation-driven replay times match training times for a continuous range of T; STF causes effective synaptic strength to increase from w to up to 2w (with p_max=2), controlling when threshold Œ∏ is reached.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Analytic mapping between baseline weight and activation time (Eqs. 9 and 10); parameter values: œÑ_f = 1 s, p_max = 2, Œ∏ = 0.5; requirement for activation: Œ∏/p_max < w < Œ∏ (otherwise activation never occurs or is immediate); used to compute replay times and RMSE under noise.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Indirect ‚Äî STF itself is transient (seconds) and does not consolidate; long-term plasticity must set baseline weights that map to desired STF-driven times; proto‚Üíactual consolidation (see LTP/LTD entry) ensures long-term storage of those baseline weights.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Networks that learn the precise timing of event sequences', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e84.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e84.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spike-rate adaptation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-rate adaptation as a slow interneuronal process</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative slow process (to STF) in which an activity-dependent adaptation variable a_j accumulates during firing and reduces effective input, causing the active population to self-suppress over hundreds of milliseconds.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Spike-rate adaptation (activity-dependent negative feedback)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Modeled by œÑ_a da_j/dt = -a_j + b u_j (with œÑ_a default 400 ms and b adaptation strength default 1) and included in the input to the population as subtractive term a_j; adaptation grows while a population is active and decays when inactive, thereby producing a slow decrease in effective self-input and eventually allowing transition to the next population.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Hundreds of milliseconds: œÑ_a = 400 ms (intermediate between fast firing œÑ = 10 ms and STF œÑ_f = 1 s; slower than synaptic input œÑ_s = 50 ms but faster than long-term plasticity œÑ_w = 150 s).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Same cortical-like recurrent population architecture (clusters) with global inhibition; adaptation is intrinsic to each excitatory population (e.g., spike-frequency adaptation mechanisms).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Self-excitation weights w_jj encode duration (timing) via interaction with adaptation; inter-population connections retain the role of encoding order (w_{j+1,j} strengthened by LTP/LTD).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Sequence timing learning where timing is stored in within-population self-excitation (w_jj) rather than in inter-population baseline weights; order still encoded in inter-population weights.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model; authors demonstrate analytically and via simulation that adaptation can replace STF as the time-tracking mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Slow adaptation (œÑ_a ‚àº 400 ms) provides the time-tracker during replay by reducing effective self-excitation until threshold crossing triggers deactivation; long-term plasticity learns self-excitation strengths w_jj so that the adaptation-driven deactivation occurs at the trained duration (equation w_jj = Œ∏ + L + b(1 - e^{-T/œÑ_a})). Thus, long-term learning and intermediate adaptation interact analogous to LTP/LTD+STF case but with opposite sign dynamics (adaptation decreases drive over time rather than facilitation increasing it).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Adaptation can serve as an alternative slow process to enable precise timing of replay; timing is stored as w_jj (self-excitation) whose learned fixed point depends on stimulus duration (closed-form expression for w_jj^‚àû provided), and replay time satisfies w_jj - a_j(T) = Œ∏ + L so that w_jj maps to T via ùí≤(T) = Œ∏ + L + b(1 - e^{-T/œÑ_a}).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Example parameter values: œÑ_a = 400 ms, b = 1; analytic fixed-point for w_jj^‚àû derived (see text) and numerical parameter fitting via least-squares (Eq. 13) used to match learned weights to desired replay timings; simulations used m = 10 training trials and event durations such as 0.6 s, 0.4 s, 1 s, 0.5 s.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Adaptation is transient (hundreds of ms) and does not store memory itself; consolidation is handled by long-term plasticity (learning w_jj values) which are slowly adjusted across training trials and (optionally) consolidated via proto‚Üíactual weight dynamics as in the plasticity entry.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Networks that learn the precise timing of event sequences', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e84.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e84.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent feedforward motif with global inhibition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent excitatory population network with learned feedforward connectivity and global inhibition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Network architecture of N excitatory populations (clusters) with recurrent self-excitation and plastic excitatory-to-excitatory inter-population connections, plus a single inhibitory population providing global inhibition; training sculpts excitatory inter-population weights such that feedforward links between consecutive populations are strengthened to encode order and timing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Architectural motif rather than a synaptic rule (recurrent excitatory clusters with plastic inter-cluster weights and global inhibition)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Populations u_j (j=1..N) with self-excitation w_jj, plastic inter-population weights w_jk subject to LTP/LTD, short-term facilitation p_k on outgoing synapses so effective weight w_jk p_k, and an inhibitory population v receiving excitation from all u_k (weights Z_k) and exerting global inhibitory tone L on all excitatory populations; global inhibition ensures serial activation and deactivation dynamics during replay.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Neuronal firing œÑ = 10 ms (fast), synaptic input time constant œÑ_s = 50 ms, short-term facilitation œÑ_f ‚àº 1 s or adaptation œÑ_a ‚àº 400 ms (intermediate), long-term plasticity œÑ_w = 150 s and consolidation œÑ_I minutes‚Äìhours (slow).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Modeled after cortex: clustered pyramidal cell populations (cell assemblies) with dense intra-cluster connectivity and sparser inter-cluster connections, plus a single global inhibitory population approximating long-range inhibition.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Key motif: training produces a predominantly feedforward chain (w_{j+1,j} strong, other w_jk weak/zero) embedded in a recurrent network where self-excitation maintains active states; effective feedforward coupling during replay is transiently modulated by STF or adaptation to produce sequential activation with trained inter-event intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Serial sequence learning (order + precise timing), associative chaining of population activations into a temporally structured replayable sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical network model; motivated by experimental observations of clustered cortical connectivity (Song et al. 2005; Perin et al. 2011; Ko et al. 2011) and replay of trained sequences in cortex/hippocampus.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Architectural role: fast dynamics (œÑ) produce bistable on/off states sustained by self-excitation; intermediate timescale processes (STF or adaptation) control transitions between states; slow plasticity sculpts the feedforward chain so that intermediate processes produce transitions at the trained times; thus replay emerges from the interaction of architecture (connectivity) and processes operating from ms to minutes/hours.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training repeatedly presented sequences sculpts a feedforward chain embedded in a recurrent network: only connections from k‚Üíj where j immediately follows k in the training are strengthened to a nonzero fixed point (others decay to zero); the learned feedforward weights determine the time necessary for one population to activate the next in conjunction with a slow process (STF or adaptation), enabling precise replay of event timings; analytic expressions link stimulus durations to learned weights (Eqs. 6 and 9) and provide conditions for exact timing matching (Eq. 11/12).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Convergence per trial characterized by multiplicative factor exp(-T Œ≥_d / œÑ_w) e^{-(Œ≥_p - Œ≥_d) D/œÑ_w}; numerical examples used m = 10 training trials, specific event duration examples (0.6s, 0.4s, 1s, 0.5s); robustness to noise quantified by analytic formulas for mean and variance of learned weight under variable stimulus durations and additive neural noise, with RMSE used to quantify timing error.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Yes ‚Äî the paper models proto-weights (seconds-scale detection) and actual weights (consolidation on œÑ_I minutes/hours) allowing learning to continue to be robust while replay occurs, i.e., slow consolidation of the trained feedforward architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Networks that learn the precise timing of event sequences', 'publication_date_yy_mm': '2014-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Triplets of spikes in a model of spike-timing-dependent plasticity <em>(Rating: 2)</em></li>
                <li>Neural networks with dynamic synapses <em>(Rating: 2)</em></li>
                <li>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity <em>(Rating: 2)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
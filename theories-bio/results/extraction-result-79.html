<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-79 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-79</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-79</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-0f57d4fd233378b1cb0b6002fe8ef964431fe3ad</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0f57d4fd233378b1cb0b6002fe8ef964431fe3ad" target="_blank">Hebbian Wiring Plasticity Generates Efficient Network Structures for Robust Inference with Synaptic Weight Plasticity</a></p>
                <p><strong>Paper Venue:</strong> bioRxiv</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that a robustly beneficial network structure naturally emerges by combining Hebbian-type synaptic weight plasticity and wiring plasticity, and the proposed rule reproduces experimental observed correlation between spine dynamics and task performance.</p>
                <p><strong>Paper Abstract:</strong> In the adult mammalian cortex, a small fraction of spines are created and eliminated every day, and the resultant synaptic connection structure is highly nonrandom, even in local circuits. However, it remains unknown whether a particular synaptic connection structure is functionally advantageous in local circuits, and why creation and elimination of synaptic connections is necessary in addition to rich synaptic weight plasticity. To answer these questions, we studied an inference task model through theoretical and numerical analyses. We demonstrate that a robustly beneficial network structure naturally emerges by combining Hebbian-type synaptic weight plasticity and wiring plasticity. Especially in a sparsely connected network, wiring plasticity achieves reliable computation by enabling efficient information transmission. Furthermore, the proposed rule reproduces experimental observed correlation between spine dynamics and task performance.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e79.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e79.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual Hebbian rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dual Hebbian learning rule (synaptic weight plasticity + wiring plasticity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local, unsupervised Hebbian-form learning rule that jointly updates synaptic weights and connection probabilities (spine creation/elimination) so that synaptic weights encode fast/variable components and connection structure encodes slower/constant components of inputs, improving inference and robustness in sparse networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Dual Hebbian learning</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Two coupled Hebbian updates: (1) synaptic weight update: Δw_ij = (η_X/γ)[ r_Y,i^t (r_X,j^t - σ_X^2 ˜ρ w_ij) + b_h (r_Y^o/N - r_Y,i^t) ], a stochastic-gradient-descent-type Hebbian rule with a homeostatic term; (2) connection-probability (wiring) update: Δρ_ij = η_ρ r_Y,i^t (r_X,j^t - σ_X^2 ρ_ij w_o), which adjusts the probability of forming/eliminating a synapse (implemented stochastically with creation prob = ρ_ij/τ_c and elimination prob = (1-ρ_ij)/τ_c). Key parameters: η_X, η_ρ (learning rates), γ (sparsity factor), b_h (homeostatic strength), τ_c (rewiring timescale), w_o (mean weight), σ_X (input noise).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Multiple: fast synaptic weight plasticity operates on fast/online timescales (model updates every 10-100 ms for neural activity; effective weight changes across trials/seconds-minutes in simulations), while wiring plasticity (changes in ρ and actual spine creation/elimination) operates on much slower timescales (τ_c in simulations = 1e5--1e6 time steps; authors map 1e5 steps ≈ 1 day, so wiring operates on hours-to-days).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic feedforward input→output cortical microcircuit in the model; explicitly mapped to cortex and to motor-cortex layers (layer 2/3 inputs → layer 5 outputs) for motor-learning interpretations; CA3→CA1 connectivity referred to as biological example of sparse connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Feedforward architecture with sparse excitatory connections (c_ij ∈ {0,1}) and global inhibition; connection probabilities ρ_ij are learned (connectivity coding), enabling structured, non-random sparse wiring (diagonal/clustered connectivity for specific stimulus representations).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Probabilistic inference / supervised-free estimation of hidden states (interpretable as sensory decoding) and application to motor learning (readout) and rapid retraining; supports both robust representation and skill (task) learning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (analytical derivations + numerical simulations) calibrated and compared to experimental spine dynamics literature.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Explicit separation: connection structure (ρ_ij) learns slowly and tends to encode the constant/slowly varying component of input structure, whereas synaptic weights (w_ij) change faster and track the variable/fast component; this complementary encoding allows rapid adaptation after environmental shifts (fast weight change) while preserving stable priors in wiring (slow). The model shows best performance when τ_c (rewiring timescale) is intermediate and comparable to the timescale of slow environmental changes (T2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dual Hebbian learning yields connection matrices with denser, larger-weight connections between neurons representing the same external state (emergent structured nonrandom connectivity), improves inference accuracy relative to weight-plasticity-only networks in sparse regimes, raises estimated transfer entropy, and reproduces experimentally observed spine survival/age relationships; it enables faster recovery after partial input-model changes because slow connectivity stores invariant components.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative metrics used: accuracy of estimation (bootstrap classification accuracy of hidden state; reported improvement of dual-rule vs weight-only particularly at sparse connectivity), estimated transfer entropy T_E (higher after dual Hebbian learning), coefficient of variation (CV) of output firing (lower in connectivity coding), KL-divergence based model error and per-connection information gain ΔI_ij; biological numeric anchors: spine turnover rates referenced (sensory cortex up to 15%/day; motor cortex ~5%/day); CA3→CA1 connection prob ≈ 6% and synaptic weight information ≈ 4.7 bits (from referenced data). Exact numeric accuracy values vary by simulation parameter (figures), with best rewiring τ_c in simulations around hours–days (mapping 1e5 steps ≈ 1 day).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Yes — slow structural plasticity (connection probabilities / stable spines) acts as a consolidation substrate encoding time-invariant (constant) components of the input model; fast synaptic weight plasticity encodes quickly changing/variable components and can be rapidly updated. Over time, repeated coactivation biases ρ_ij upward so that repeatedly used weight patterns consolidate into stable connections (ρ→1 for some synapses), thereby transferring information from fast to slow substrate.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hebbian Wiring Plasticity Generates Efficient Network Structures for Robust Inference with Synaptic Weight Plasticity', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e79.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e79.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Approximated dual Hebbian rule</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Approximated dual Hebbian rule (weight-dependent approximation with stochastic spine creation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A biologically plausible approximation of the dual Hebbian rule in which spine creation is modeled as largely random but connection probability ρ is made a simple function of synaptic weight, yielding similar functional outcomes and reproducing spine dynamics data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Approximated dual Hebbian rule (weight→ρ coupling + weight Hebbian update)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Instead of directly Hebbian updating ρ via pre/post activity, ρ is made a (slow) function of the synaptic weight (e.g., ρ_ij ← f(w_ij)) and new spines are created randomly; weight updates still follow the Hebbian weight rule (Equation 2). This yields an implicit activity dependence of wiring because w_ij itself is activity-dependent. Parameters: same weight-update params (η_X, b_h), plus mapping function and τ_c for stochastic creation/elimination.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Fast weight plasticity: trial-to-trial / seconds–minutes (model activity updates at 10–100 ms). Wiring effectively changes on slow timescales (τ_c mapped to hours–days), because ρ depends slowly on averaged weights and spine lifetime dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Same modeled feedforward cortical microcircuit; applied to interpret motor cortex spine dynamics (layer 2/3 → layer 5) and general cortical circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Sparse feedforward connectivity with stochastic spine turnover; connection probability correlates with synaptic weight leading to clustered/diagonal connectivity after learning.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised inference + motor retraining; used to model task-related spine stabilization dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model and simulation compared with experimental spine data.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Because ρ is a slow function of w, fast Hebbian changes in w eventually bias wiring over long timescales: repeated weight potentiation makes newly created spines more likely to stabilize (increase ρ), implementing transfer from fast to slow storage indirectly.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The approximated rule (random creation + weight-dependent stabilization) closely approximates the performance of the full dual Hebbian rule, reproduces experimentally observed relationships between spine age/size and survival, and matches empirical training effects on spine stability (e.g., enhanced survival of training-era new spines).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Compared to dual Hebbian and weight-only models: similar accuracy curves and transfer-entropy trends; model reproduces observed 5-day survival statistics and age-dependent survival increases (simulated with mapping 1e5 steps ≈ 1 day).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Indirect consolidation: repeated fast weight potentiation increases the slow stabilization probability (ρ) of corresponding synapses, causing long-term retention (spine survival) without explicit activity-dependent creation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hebbian Wiring Plasticity Generates Efficient Network Structures for Robust Inference with Synaptic Weight Plasticity', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e79.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e79.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Weight coding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weight coding strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation strategy in which inference is implemented primarily by graded synaptic weights on (possibly random) feedforward connections—connection pattern is random and information is carried by w_ij values.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Hebbian synaptic weight plasticity (stochastic-gradient Hebbian rule with homeostasis)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Synaptic weights are set/learned proportional to evidence q_{jμ} (e.g., w_ij ≈ q_{jμ}/ρ), using the Hebbian weight update (Equation 2) that correlates pre- and postsynaptic activity and includes a homeostatic term to regulate output rates; relies on high-resolution weights to encode stimulus selectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Weight plasticity operates on fast to intermediate timescales (trial-to-trial to hours in simulations; neural updates occur at 10–100 ms resolution; behavior change (accuracy) can be seen within many thousands of time steps but faster than wiring).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Generic feedforward cortical microcircuit (input→output); used as baseline coding scheme in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Random sparse feedforward connectivity (constant connection probability ρ across synapse pairs) with representation stored mostly in weights.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised/statistical inference of hidden states (population decoding); supports fast adaptation via weight updates but is vulnerable to sparse connectivity and input heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (analytical comparisons and simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Weight coding alone provides fast plasticity but lacks a slow substrate to capture invariant components; therefore it can relearn but is more sensitive to input variability and sparse connectivity—no explicit slow timescale in pure weight coding.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Weight coding can approach optimal inference under dense connectivity but degrades in sparse regimes: firing variability (CV) of selective output neurons is higher in weight coding than connectivity coding under sparse connections, causing worse performance; weight-only learning is slower to recover after structured shifts when no wiring plasticity is present.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Accuracy of estimation (bootstrap), CV of output firing rates (analytically and in simulation), and KL-divergence based model error; degrades substantially at low connection probabilities (example parameter γ values in figures).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation mechanism beyond weight stabilization; slow persistent changes would require slow weight dynamics, but no structural consolidation substrate is present in this scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hebbian Wiring Plasticity Generates Efficient Network Structures for Robust Inference with Synaptic Weight Plasticity', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e79.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e79.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Connectivity coding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Connectivity coding strategy</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation strategy where synaptic weights are uniform (or coarse) and information is encoded in structured connection probabilities (ρ_ij) so that relevant input neurons connect preferentially to the appropriate outputs, improving information transfer under sparse connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Wiring plasticity (Hebbian-form update of connection probability)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Connection probabilities ρ_ij are adjusted according to a Hebbian-like rule Δρ_ij = η_ρ r_Y,i^t (r_X,j^t - σ_X^2 ρ_ij w_o), which increases ρ for coactive pre/post pairs and decreases for noncoactive pairs; actual connectivity implemented stochastically with creation/elimination probabilities based on ρ and τ_c.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Slow: wiring plasticity implemented with τ_c controlling turnover (hours to days in simulation mapping; τ_c values used include 1e5–1e6 steps where 1e5 ≈ 1 day).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Feedforward cortical microcircuit; used conceptually for sparse cortico-cortical projections (e.g., CA3→CA1 example cited).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Sparse, non-random/structured connectivity with higher inbound density from input neurons that provide more evidence for a given external state (diagonal or clustered connectivity matrices).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised/statistical inference and long-term structural adaptation; supports robustness and low-variance signal transmission under sparse connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model and analytical derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Connectivity coding provides a slow, relatively stable substrate that reduces variance of output signals; when combined with faster weight plasticity (dual coding) the system attains both stability (via connectivity) and flexibility (via weights).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Connectivity coding yields lower firing-rate variability (lower CV) of selective outputs in sparse networks and higher transfer entropy than weight coding under sparse regimes; connectivity tends to have higher relative information capacity than weights at low connection probabilities (information-theoretic analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Maximum transfer entropy T_E, coefficient of variation (CV) of output firing, analytic KL-divergence approximations and accuracy metrics; information-capacity comparison I_C(ρ) ≈ MN·H(ρ) vs I_w(ρ) = ρMN log b, and example biological anchor: CA3→CA1 ρ≈6% making connectivity important.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Connectivity provides a slow consolidated representation of invariant features of inputs; in dual schemes it acts as slow memory.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hebbian Wiring Plasticity Generates Efficient Network Structures for Robust Inference with Synaptic Weight Plasticity', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e79.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e79.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cut-off coding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cut-off coding (pruning-by-weight threshold)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A heuristic wiring strategy in which synapses with weights below a threshold are eliminated (pruned) and remaining synapses carry the representation; previously proposed in developmental pruning models and used here as a comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Cut-off (threshold) pruning for connectivity</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Given a set of synaptic weights ordered by magnitude, retain only the top M·ρ_o incoming weights per output neuron and eliminate others (equivalently, prune connections with weight below a cutoff); no explicit wiring Hebbian update—pruning is weight-driven.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Applied as a restructuring step (could be developmental/slow), but in the paper evaluated as a static/pruning strategy rather than an explicit online temporal rule; when used dynamically it would be on a longer timescale than rapid weight changes.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Evaluated in the same feedforward model as a comparison to dual coding strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Produces highly selective connectivity concentrated on strongest-weight inputs; tends to compress distribution of surviving weights around a non-zero value.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Memory/information storage via pruning; assessed for inference performance under input variability and sparsity.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Used as a computational baseline/comparison in simulations (not proposed as primary mechanism in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Cut-off pruning lacks an explicit multi-timescale interaction in this paper; it can be static or slow but does not provide the complementary fast/slow encoding benefits of the dual Hebbian rule.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cut-off coding can approach near-optimal performance in homogeneous input settings but fails under heterogeneous input variability and becomes fragile in sparse networks; dual coding is more robust to input heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Accuracy of estimation and KL-divergence-based model error under homogeneous vs inhomogeneous input variability; dramatic performance drop observed for cut-off strategy in heterogeneous/inexact input cases (figures 3C–3E).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Implicit (pruned structure is long-lasting), but not an explicit consolidation mechanism studied here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hebbian Wiring Plasticity Generates Efficient Network Structures for Robust Inference with Synaptic Weight Plasticity', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e79.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e79.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Homeostatic plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Homeostatic firing-rate plasticity (rate normalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A homeostatic term added to the Hebbian weight update to constrain average output firing rates and avoid representational collapse, implemented as a subtractive term driving mean output activity toward a target.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Homeostatic plasticity (rate normalization term)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>An additive term to the weight update: + (η_X/γ)·b_h·[ r_Y^o/N - r_Y,i^t ], where b_h is the homeostatic strength and r_Y^o/N is the target mean rate; this term reduces the weights of hyperactive outputs and increases those of hypoactive outputs, stabilizing population activity.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Intermediate/slow relative to Hebbian weight fluctuations: operates over many weight-update steps (simulations show dependence on b_h for population selectivity), typically minutes-to-hours in modeled dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Applied to the model output neuron population (generic cortical output layer) to maintain distributed selectivity across neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Acts on synaptic weights regardless of connectivity; supports balanced representation across output neurons even when connectivity is sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Stabilization/unsupervised learning support for Hebbian learning; enables self-organization of selectivity across the output population.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Model component implemented in simulations (motivated by Turrigiano & Nelson 2004 experimental concept).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Acts as a slower regulatory process complementing fast Hebbian potentiation/depression; helps maintain stable firing distributions while weights and connections adapt on faster/slower timescales respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Appropriate strength of homeostatic plasticity (b_h) produces narrow unimodal output firing-rate distributions and helps most output neurons acquire selectivity; alters learning dynamics and final performance in simulations (Figure 4B-C).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Selectivity metric and accuracy of estimation as a function of b_h; qualitatively, stronger homeostasis yields better-distributed selectivity and improved inference when tuned correctly.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Homeostasis does not consolidate memories per se but provides a stabilizing background that permits separation of fast weight changes and slow wiring adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Hebbian Wiring Plasticity Generates Efficient Network Structures for Robust Inference with Synaptic Weight Plasticity', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Rapid formation and selective stabilization of synapses for enduring motor memories <em>(Rating: 2)</em></li>
                <li>Stably maintained dendritic spines are associated with lifelong memories <em>(Rating: 2)</em></li>
                <li>Principles of long-term dynamics of dendritic spines <em>(Rating: 2)</em></li>
                <li>Experience-dependent structural synaptic plasticity in the mammalian brain <em>(Rating: 2)</em></li>
                <li>A neural circuit model of flexible sensorimotor mapping: learning and forgetting on multiple timescales <em>(Rating: 2)</em></li>
                <li>Optimal information storage in noisy synapses under resource constraints <em>(Rating: 1)</em></li>
                <li>Synaptic pruning in development: a computational account <em>(Rating: 1)</em></li>
                <li>Impact of active dendrites and structural plasticity on the memory capacity of neural tissue <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
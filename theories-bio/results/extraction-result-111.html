<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-111 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-111</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-111</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-5.html">extraction-schema-5</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <p><strong>Paper ID:</strong> paper-8d3c6d2277acd690f417ee5f9843e19670c7d5c9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/8d3c6d2277acd690f417ee5f9843e19670c7d5c9" target="_blank">Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition</a></p>
                <p><strong>Paper Venue:</strong> PLoS ONE</p>
                <p><strong>Paper TL;DR:</strong> This study shows how the spiking dynamics of a recurrent network with lateral excitation and local inhibition in response to distributed spiking input, can be understood as sampling from a variational posterior distribution of a well-defined implicit probabilistic model, which permits a rigorous analytical treatment of experience-dependent plasticity on the network level.</p>
                <p><strong>Paper Abstract:</strong> During the last decade, Bayesian probability theory has emerged as a framework in cognitive science and neuroscience for describing perception, reasoning and learning of mammals. However, our understanding of how probabilistic computations could be organized in the brain, and how the observed connectivity structure of cortical microcircuits supports these calculations, is rudimentary at best. In this study, we investigate statistical inference and self-organized learning in a spatially extended spiking network model, that accommodates both local competitive and large-scale associative aspects of neural information processing, under a unified Bayesian account. Specifically, we show how the spiking dynamics of a recurrent network with lateral excitation and local inhibition in response to distributed spiking input, can be understood as sampling from a variational posterior distribution of a well-defined implicit probabilistic model. This interpretation further permits a rigorous analytical treatment of experience-dependent plasticity on the network level. Using machine learning theory, we derive update rules for neuron and synapse parameters which equate with Hebbian synaptic and homeostatic intrinsic plasticity rules in a neural implementation. In computer simulations, we demonstrate that the interplay of these plasticity rules leads to the emergence of probabilistic local experts that form distributed assemblies of similarly tuned cells communicating through lateral excitatory connections. The resulting sparse distributed spike code of a well-adapted network carries compressed information on salient input features combined with prior experience on correlations among them. Our theory predicts that the emergence of such efficient representations benefits from network architectures in which the range of local inhibition matches the spatial extent of pyramidal cells that share common afferent input.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e111.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e111.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Afferent Hebbian/STDP-like plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Afferent synaptic Hebbian (STDP-like) plasticity (V_{ki} update)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Locally computed, spike-triggered plasticity rule for bottom-up (afferent) weights that implements a gradient direction of a maximum-likelihood objective; weight updates are Hebbian (pre × post) with a weight-dependent postsynaptic expectation term (logistic).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Hebbian / STDP-like plasticity for afferent synapses (V_{ki})</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Derived update (Eq.12): dV_{ki}/dt = η_V · z_k(t) · ( y_i(t) - σ(V_{ki} + V_{0i}) ). Updates are triggered when the postsynaptic network neuron is active (z_k=1) and depend on presynaptic activity y_i(t) and a sigmoidal (logistic) expectation σ(·) of the synapse; V_{0i}=log(π_{0i}/(1-π_{0i})) is a constant offset. Mechanistically: an LTP-like term (proportional to pre y_i when post z_k spikes) and an LTD-like weight-dependent term via the logistic function. Learning rate parameter η_V controls update speed.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Two interacting scales: (fast) updates are event-triggered by spikes (post spike window z_k active for τ = 10 ms; PSP/spike timing relevant on the order of τ ≈ 10 ms). (Slow) accumulated weight change governed by small η_V over behavioral simulation durations (examples: learning runs reported up to T = 10,000 s and 25,000 s).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortex-like two-dimensional neural sheet / cortical microcircuit motif with local receptive fields (modelled as overlapping 6×6 input fields in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Bottom-up (feedforward) afferent convergence from a local subset of input neurons to each network neuron; each input variable y_i has parents among nearby z_k; learning assumes local lateral inhibition (WTA) so that at most one parent explains y_i at a time.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Unsupervised statistical learning / maximum-likelihood learning of local feature detectors (probabilistic 'local experts').</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (derivation from generalized online Expectation-Maximization; validated in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast spiking events (ms scale; τ ≈ 10 ms) produce samples from the posterior; these spike events drive local Hebbian updates that accumulate slowly (seconds to hours) via small η_V, thereby reshaping the generative model (likelihood) over long timescales. The rule is local because local lateral inhibition eliminates non-local terms that would otherwise require knowledge of other neurons' activities.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The derived local Hebbian/STDP-like rule implements the expected gradient direction of a tractable lower bound on log-likelihood (Generalized EM) and turns neurons into probabilistic local experts. Applied in simulations, these updates produce specialization of neurons onto salient local input patterns, increase the model log-likelihood, and produce weight vectors that closely match analytically computed optimal weights (small systematic deviations due to variational approximation and finite learning rates).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Quantitative measures reported: increase of log-likelihood ℒ(θ) during learning (plots provided); final comparison of learned V_{ki} vs analytically optimal weights at T = 10,000 s (good correspondence). Simulation parameters example: τ = 10 ms; target average firing m_k = 6.5%; learning run durations: 10,000 s (≈2.78 h) and 25,000 s (≈6.94 h). No single scalar accuracy metric (e.g., % classification) but explicit log-likelihood and one-to-one weight comparisons are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit biological consolidation described for afferent weights; slow consolidation-like accumulation occurs via repeated spike-driven updates over long durations. The paper uses an online EM perspective rather than a dedicated fast-to-slow consolidation mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e111.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e111.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Homeostatic intrinsic plasticity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Homeostatic intrinsic excitability plasticity (b_k update)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A local homeostatic rule that adjusts each neuron's intrinsic excitability to maintain a long-term target firing rate, stabilizing learning and preventing specialists of weak patterns from being silenced.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Homeostatic intrinsic plasticity (intrinsic excitability b_k)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Derived update (Eq.13): db_k/dt = η_b · ( m_k - z_k(t) ). Here m_k is a prescribed long-term target mean activity for neuron k, and η_b is a small learning rate. When the instantaneous post-synaptic activity z_k(t) is below (above) the target, b_k is increased (decreased), producing a homeostatic compensation.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Operates on slow timescales relative to spikes: updates integrate over many spike events (learning rates η_b are small); target m_k is a long-term average; simulation examples show learning and regulation across thousands of seconds. It is triggered by spikes (ms) but converges over seconds–hours.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Applied within the cortex-like neural sheet model (microcircuit-level intrinsic regulation).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Acts per neuron regardless of recurrent or feedforward connectivity; crucially interacts with local lateral inhibition/WTA circuits to maintain diversity of active feature detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Stabilizing/homeostatic learning that supports unsupervised feature learning by maintaining target firing rates across neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (derived analytically and used in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Provides a slow-timescale target (m_k) that constrains the faster spike-driven plasticity: fast events determine instantaneous updates, while homeostatic changes accumulate slowly to keep neurons active at desired rates, enabling robust long-term specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Homeostatic intrinsic plasticity is necessary for balanced specialization: it ensures neurons that specialize on weak-intensity patterns are not disadvantaged and supports stable learning across inputs with differing intensities. The paper provides an illustration showing homeostasis equalizes opportunities for learning across neurons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Target firing m_k example: 6.5% average activation; convergence/stability of per-neuron firing rates and improved log-likelihood during learning (plots furnished).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit memory consolidation role is described; homeostasis functions as a slow regulatory process rather than a consolidation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e111.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e111.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recurrent excitatory plasticity (wake-sleep and local approx)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent excitatory synaptic plasticity: optimal wake-sleep rule and a local approximate rule (W_{kj}^{exc})</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Plasticity rules for recurrent excitatory connections that enable the network to learn priors (correlations among latent variables); includes a theoretically optimal (non-local) wake-sleep-like rule and a practically usable local approximation with a weight-dependent LTD term.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Wake–sleep (model-based) rule: ∂W_{kj}^{exc} = η_W ( z_k z_j - φ_kj^{opt} ); Local approximate Hebbian rule: ∂W_{kj}^{exc} = η_W ( z_k z_j - φ_θ(W_{kj}^{exc}) )</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Optimal rule (Eq.16): weight change equals η_W times difference between instantaneous coactivation z_k z_j and φ_kj^{opt}, where φ_kj^{opt} = ⟨z_k z_j⟩_{p(z|θ)} (a model expectation) — this term is non-local and depends on the full prior parameters; it is computable via a sleep-phase (wake–sleep) procedure. Local approximation (Eq.17 & Eq.18): choose φ_θ(W) that depends only on the weight itself; the paper fits an example φ: φ(W) = m_k m_j + (1/γ) tan( (π/2) · W / W^{max} ) with fitted parameters W^{max}=1.41 and γ=31.6. LTP is local coactivation-driven; LTD is implemented via φ (model expectation or fitted weight-dependent function).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Two scales: (fast) LTP component is triggered by near-coincident activations (coactivations occurring on the timescale of spikes/PSPs, τ ≈ 10 ms); (slow) weights change slowly via small η_W over long learning periods (examples: learning continued for 25,000 s in simulations). The computation of φ^{opt} requires offline/model-based estimation (a 'sleep' phase) which is conceptually slower/offline relative to online inference.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortex-like neural sheet; recurrent excitatory connections beyond the range of local lateral inhibition (intermediate-range excitatory connectivity forming assemblies).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Sparse, reciprocal excitatory connections at intermediate distances (25% reciprocal connectivity probability in large sheet simulation), symmetric weights in the model; recurrent excitation links distant local feature detectors and encodes statistical co-occurrence (prior).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Learning of structural correlations / associative learning of co-occurrence statistics (shaping the prior distribution p(z|θ)); supports global coherent interpretations and inference under incomplete/ambiguous input.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model with both ideal (wake–sleep) algorithmic rule and local heuristic implemented and compared in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Recurrent weights encode slow priors (learned over long durations) that bias fast sampling dynamics (ms spikes) to produce coherent network states; wake–sleep learning explicitly separates an online (wake) phase collecting data-driven coactivations and an offline (sleep) phase to compute model expectations φ^{opt}, representing an offline consolidation-like process that integrates information across time.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The theoretically optimal (non-local) wake–sleep rule improves log-likelihood by encoding input correlations in recurrent weights. A carefully designed local approximation (weight-dependent LTD φ_θ fitted to wake–sleep outcomes) reproduces similar performance and yields formation of excitatory subnetworks (assemblies of similarly tuned neurons). Learned recurrent excitation enables correct inference from incomplete external input (e.g., cues produce correct filling-in of inner features) and supports stochastic switching between consistent multimodal interpretations for ambiguous inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Comparative metric: log-likelihood ℒ(θ) over learning time; simulations show local approximate rule's ℒ(θ) nearly indistinguishable from wake–sleep (plots provided). Learning durations reported: up to 25,000 s. Fitted local LTD parameters: W^{max}=1.41, γ=31.6. Recurrent connectivity statistics: example of 4198 recurrent excitatory synapses analyzed for strength vs tuning/distance.</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Yes — the wake–sleep algorithm is used to compute the model expectation φ^{opt} in a sleep phase (offline), which plays a role analogous to an offline consolidation or model-based refinement; the authors explicitly mention a sleep/wake separation to obtain the non-local term. The paper does not claim a biological implementation but suggests such offline phases could be plausible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e111.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e111.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>STDP (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spike-Timing-Dependent Plasticity (STDP) — referenced compatibility</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experimental STDP is referenced as being compatible with the theoretically derived afferent plasticity rules; the authors note the derived rule appears compatible with experimental STDP observations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>STDP (referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>STDP is mentioned qualitatively: the authors note that maximum-likelihood-derived afferent synaptic update rules resemble experimental STDP-type plasticity (timing-dependent potentiation/depression), but no explicit STDP timing window or parameters are used in their model equations.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Canonical STDP operates on spike timing differences in the millisecond range (tens of ms); the paper uses PSP duration τ ≈ 10 ms and relates the derived event-based weight updates to spike-timing phenomena, but does not implement full biophysical STDP timing kernels.</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>General cortical microcircuits (cited as experimental context), not specifically measured in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Mentioned in context of local lateral inhibition WTA architectures and afferent plasticity; no new connectivity data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Referenced as a biological correlate of Hebbian learning / unsupervised feature learning.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Mention only (cited experimental literature), not used in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Not elaborated in the paper beyond the qualitative compatibility statement; STDP is implicitly a fast-timescale mechanism that could drive slower synaptic change when integrated over time.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The main claim is compatibility: the theoretically derived afferent plasticity rules align qualitatively with experimental STDP observations reported elsewhere (no quantitative STDP results are reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>None in this paper (STDP only referenced qualitatively).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>Not described for STDP in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e111.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e111.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Local lateral inhibition (WTA motif)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local lateral inhibition / Winner-Take-All (WTA) network motif</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Strong local inhibitory connectivity among nearby network neurons enforces competition (local WTA), ensures that at most one parent explains a given input variable, and makes learning local and tractable.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Lateral inhibition (implemented as direct negative recurrent connections W_{kj}^{inh})</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Inhibition is modeled as symmetric negative connections W^{inh} among nearby neurons (effectively implementing local WTA competition). In the generative model, strong negative coupling among parents of a given input enforces 'explaining-away', so each input y_i is assumed to be generated by at most one nearby z_k. This eliminates the non-local normalization term in afferent plasticity making learning local.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Inhibitory PSPs and competitive interactions operate on fast timescales (PSP duration τ ≈ 10 ms; instantaneous firing probabilities respond on that timescale). The role of inhibition in shaping plasticity unfolds over longer times as it alters which neurons spike and thereby which plasticity updates occur (seconds to hours).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortex-like layer/sheet with local interneuron-mediated inhibition (modeled implicitly as direct negative weights rather than explicit interneurons).</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Local, strong inhibitory reciprocal connections among neurons with overlapping afferent input; lateral inhibition range matched to spatial extent of shared afferent convergences (architecture argument: matching range aids learning).</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Supports unsupervised competitive learning and categorical segmentation of local input patterns (formation of local feature detectors / 'local experts').</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (lateral inhibition fixed in simulations; theoretical role highlighted in derivations).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast inhibitory dynamics enable local competition during each inference/sampling time window (ms), which in turn determines which spike-driven plasticity events occur; thus fast inhibition gates slow synaptic learning and permits a local (rather than non-local) learning rule to be valid.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Lateral inhibition is central to obtaining local plasticity rules: it prevents multiple parents jointly explaining a single input, removing non-local dependencies from the afferent learning rule and facilitating biologically plausible local updates. The authors predict better learning when lateral inhibition range matches the spatial extent of excitatory cells sharing afferent input.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Qualitative and theoretical: comparisons between derivations with W^{inh}=0 vs W^{inh}≠0 show non-local terms vanish with inhibition; simulation evidence shows learning proceeds locally and produces expected clustering of inputs. No single numeric inhibitory strength universally prescribed (but simulated architectures use a confined inhibitory radius).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No explicit consolidation; lateral inhibition functions as an onlinefast-timescale gating mechanism that shapes slow plastic changes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e111.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e111.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of synaptic plasticity rules, brain connectivity patterns, and how they support learning at different temporal scales (e.g., milliseconds to days).</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse recurrent excitation / excitatory subnetworks</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse long-range recurrent excitatory connectivity forming assemblies/subnetworks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sparse reciprocal excitatory connections at intermediate distances link similarly tuned local feature detectors, forming excitatory subnetworks that store statistical correlations (the prior) and support global coherent interpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_name</strong></td>
                            <td>Recurrent excitation (plastic W_{kj}^{exc}) forming assemblies; plasticity via wake–sleep or local Hebbian + weight-dependent LTD</td>
                        </tr>
                        <tr>
                            <td><strong>synaptic_rule_description</strong></td>
                            <td>Sparse reciprocal excitatory links are instantiated beyond the local inhibition radius (in simulations: 25% chance of reciprocal connection). Plasticity (see recurrent excitatory plasticity entry) potentiates coactive pairs (LTP) and depresses via a model expectation or weight-dependent LTD, resulting in assemblies of similarly tuned neurons (strong mutual excitatory weights).</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_scale</strong></td>
                            <td>Fast excitatory interactions occur on PSP/spike timescale (τ ≈ 10 ms) and produce coordinated coactivation; plastic formation of subnetworks evolves over long learning durations (thousands to tens of thousands of seconds in simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>brain_region_circuit</strong></td>
                            <td>Cortex-like 2D neural sheet; intermediate-range excitatory connections among pyramidal-like units.</td>
                        </tr>
                        <tr>
                            <td><strong>connectivity_pattern</strong></td>
                            <td>Sparse, symmetric reciprocal excitatory connections beyond the inhibition radius, forming recurrent subnetworks/assemblies of similarly tuned neurons; nearby similarly tuned neurons have on average stronger excitatory connections than differently tuned or distant pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>learning_type</strong></td>
                            <td>Learning of statistical correlations / associative learning across spatially distributed features; forms priors that support inference under incomplete or ambiguous sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_or_model</strong></td>
                            <td>Computational/theoretical model (plastic recurrent connections are learned in simulation; empirical statistics of learned connectivity are reported).</td>
                        </tr>
                        <tr>
                            <td><strong>multi_timescale_interaction</strong></td>
                            <td>Fast sampling dynamics (ms spikes) produce coactivations that drive slow plastic changes in recurrent excitation (seconds–hours), resulting in a slow prior that biases fast inference; assemblies formed are then used online by fast dynamics to maintain coherent global network states.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recurrent plasticity yields excitatory subnetworks linking neurons tuned to similar features; these subnetworks store correlation structure and allow the network to 'fill in' or infer missing inputs from cues, and to maintain synchronized switching between consistent interpretations in ambiguous cases. Empirically, learned excitatory weights are stronger between similarly tuned neurons and decay with Euclidean distance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_measure</strong></td>
                            <td>Statistics of learned connectivity: analysis over 4198 recurrent excitatory synapses showed stronger weights among similarly tuned neurons and stronger weights for nearby pairs; functional benefit evidenced by improved log-likelihood ℒ(θ) when recurrent plasticity is active versus disabled (plots provided).</td>
                        </tr>
                        <tr>
                            <td><strong>slow_vs_fast_learning</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>consolidation_mechanism</strong></td>
                            <td>No biological consolidation mechanism is specified beyond the slow learning of recurrent weights; wake–sleep/offline computation of model expectations is used as an algorithmic consolidation-like step in the theoretically optimal learning approach.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Distributed Bayesian Computation and Self-Organized Learning in Sheets of Spiking Neurons with Local Lateral Inhibition', 'publication_date_yy_mm': '2015-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Bayesian computation emerges in generic cortical microcircuits through spike-timing-dependent plasticity <em>(Rating: 2)</em></li>
                <li>The wake-sleep algorithm for unsupervised neural networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-432 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-432</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-432</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-252519203</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2209.11326v2.pdf" target="_blank">Towards Faithful Model Explanation in NLP: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity-based methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.</p>
                <p><strong>Cost:</strong> 0.028</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e432.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e432.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attention-as-explanation mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interpreting Attention Weights as Model Explanations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Many papers and introductions describe attention weights (in Transformers/RNN attention) in natural language as token-level importance scores; empirical analyses show attention distributions can be decorrelated from causal contribution to predictions and can be adversarially manipulated without changing outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Attention is not Explanation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Transformer / attention-based classifier</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Neural NLP models (e.g., BERT) that use self-attention; explanations are produced by reporting attention weights from particular heads or averaged heads.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>research paper methods section / intuitive description (attention=importance)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>trained Transformer model implementation (attention weights computed in model forward pass)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>ambiguous description / misattributed interpretive claim</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language claims present attention weights as direct indicators of input-feature importance to final predictions; however implementations compute attention on intermediate hidden states which mix contextual information, and adversarially constructed (or trained) attention distributions can be very different while yielding nearly identical model outputs — demonstrating that attention weights do not reliably reflect causal contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model-internal structure / interpretation layer (attention computation and reporting)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical correlation analyses and adversarial construction of attention distributions; literature critiques and counterexamples (correlational tests vs. gradient/saliency measures)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>comparisons between attention weights and other feature-importance measures (gradients, perturbation results); adversarial objective that maximizes distance between attention distributions while constraining output change; pruning/ablation experiments measuring downstream performance change when attention heads removed</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can mislead users into believing tokens with high attention are causal drivers; adversarially (or intentionally) learned attention can hide bias (Pruthi et al.); in some MT sequence-to-sequence settings attention pruning affects output substantially, but in many single-sequence settings attention changes have little impact — effect varies by task (qualitative and task-dependent impacts reported).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widely observed across attention-based NLP papers; the survey reports this as a central contested issue in many studies (common)</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>ambiguous natural-language framing equating attention weights with importance, conflation of plausibility and faithfulness, and interpreting attention on hidden states rather than input-level contributions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>regularization tying hidden states to input representations, auxiliary tasks (e.g., MLM) to force hidden states to reflect inputs, combine attention with attribution methods (e.g., Attention Attribution), head pruning plus causal intervention to validate contributions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reported partial success: Tutek & Snajder's weight-tying and MLM auxiliary tasks increase attention's influence on prediction; Attention Attribution strengthens meaningful connections and enables more reliable pruning (qualitative/improved causal signals reported but not universally sufficient).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning / NLP (Transformer models)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Faithful Model Explanation in NLP: A Survey', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e432.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e432.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Erasure / surrogate sampling OOD vulnerability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Out-of-distribution and adversarial vulnerability of erasure-based surrogate explanation methods (LIME/SHAP/Anchors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Surrogate/explanation methods that generate nearby examples by erasing or masking features (LIME/SHAP/Anchors) rely on a neighborhood sampling described in papers, but implementations that sample masked inputs produce OOD distributions that can be exploited or give misleading explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Local surrogate explanation pipeline (LIME/SHAP/Anchors)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Black-box explanation pipeline that samples perturbations (often by masking features) in the neighborhood of an example, queries model predictions, weights samples by proximity, and fits a simple surrogate (e.g., sparse linear model) used as the explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method description / algorithm specification in papers (neighborhood sampling by masking)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>explanation tool implementations / sampling code (LIME, SHAP approximations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete specification / distribution-shift (OOD) during neighborhood sampling</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>The natural-language description of neighborhood sampling often omits that masking or erasure produces inputs far from model training distribution; implementations sample these OOD instances which can trigger different model behaviors and allow crafted models to behave benignly on masked inputs while still encoding biases on in-distribution inputs — producing misleading 'explanations' that claim absence of bias.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>input perturbation / explanation sampling stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>adversarially constructed pipelined model and empirical attack (Slack et al.) that behaves differently on masked (OOD) vs in-distribution inputs; analysis of neighborhood choice sensitivity (Laugel et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>attack success (ability to hide biased behavior from explanation methods); human predictive power or fidelity on surrogate predictions; white-box recovery tests (LIME recovers features on transparent models in controlled tests)</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can cause explanation methods to report false negatives (e.g., claim model not using protected attribute) and thus under-detect bias; Slack et al. demonstrate successful attacks where LIME/SHAP report innocuous reasoning though model encodes sensitive information — practical security and trust consequences (qualitative attack demonstrations; no single global numeric loss metric universally reported).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common concern for erasure-based explanation approaches; many papers rely on masking-based neighborhoods and thus vulnerable (widespread in practice).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>masking/erasure produces unrealistic inputs and explanation descriptions rarely emphasize distributional mismatch or how to sample appropriately; implicit assumptions about neighborhood locality not stated</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>use perturbations that produce realistic counterfactuals, adversarially evaluate explanation tools, perform white-box tests on transparent models, expand neighborhood definitions, use model-aware generative sampling (e.g., language-model-based counterfactual generation like Polyjuice), or retrain/evaluate with ROAR-style procedures</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Mixed: ROAR (retraining after removals) addresses OOD sampling by aligning train/test distributions but changes the model (so measures different question); generative counterfactuals produce more realistic neighbors but add complexity; Slack et al. show many current defenses insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning explainability / NLP and CV explainers</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Faithful Model Explanation in NLP: A Survey', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e432.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e432.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Backprop saliency input-recovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Saliency / backpropagation-based visualizations that reflect input recovery rather than model decision</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods like Guided BackPropagation and DeconvNet are often presented as faithful attribution methods in natural-language descriptions, but empirical analyses show they can produce human-readable visualizations even for random networks, indicating they reconstruct input features rather than reflect causal model reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Backpropagation-based saliency methods (Guided BackProp, DeconvNet)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Attribution implementations that modify the backward pass to produce per-input relevance maps or saliency visualizations used to explain classifier decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method description in papers / visualization claim (backprop visualizations indicate important input regions)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>modified backward() implementations in model code / explanation libraries</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>misleading implementation behavior / overclaim of faithfulness</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language framing often implies these visualizations reveal the model's decision mechanism; implemented variants are shown empirically to produce similar human-readable visualizations regardless of the model parameters (including random weights), suggesting they perform (partial) input reconstruction and are not causally tied to the prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>explanation algorithm (custom backward pass)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>sanity checks: randomizing model weights or labels and observing if saliency maps change (Adebayo et al.); theoretical analysis showing input-recovery behavior (Nie et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>sanity-check experiments (e.g., measure similarity of saliency maps before and after weight randomization), pixel-flipping/word-deletion tests checking prediction sensitivity to removed high-relevance regions</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Generates deceptive interpretability: humans perceive explanations as meaningful though they are independent of model reasoning; can cause false confidence in model behavior and misdirect debugging efforts (empirical sanity-check failures reported).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across several backprop-based visualization papers and widely used toolkits; a notable and repeatedly reported pathology.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>the backward-rule heuristics primarily recover features correlated with inputs rather than redistributing relevance consistent with the model's decision; over-ambitious natural-language characterization conflating visually pleasing maps with faithful explanations</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>apply sanity checks (randomization tests), prefer reference-based methods (DeepLift, Integrated Gradients), use perturbation-based faithfulness tests (ROAR), and combine multiple evaluation axes (axiomatic + perturbation + white-box tests)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Sanity checks effectively reveal the problem; DeepLift and some propagation methods perform better on pixel-flipping tests but still have limitations (qualitative improvements reported in cited evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>computer vision explainability / adapted to NLP token saliency</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Faithful Model Explanation in NLP: A Survey', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e432.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e432.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gradient-method manipulation & sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Adversarial manipulability and sensitivity of gradient-based attributions (Vanilla / Integrated Gradients / SmoothGrad)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gradient-based attribution methods are often described as faithful in papers, but implementations are sensitive to saturation, constant shifts, and can be adversarially manipulated to produce divergent attributions with minimal change in predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Axiomatic attribution for deep networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gradient-based attribution pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Implementations that compute gradients (or integrated gradients and smoothing variants) from model outputs to inputs to produce token/pixel-level importance scores.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method specification in research papers (gradient as local importance)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>automatic backward() gradient computations and integration over baselines (captum/implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete claim / sensitivity to implementation and baseline choices</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language descriptions claim gradients reflect feature importance; implemented gradient methods can fail when activations saturate, depend on baseline choice, be insensitive to model randomization, be sensitive to meaningless constant shifts, or be intentionally manipulated to yield misleading attribution distributions that do not change model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>attribution computation stage / baseline selection / numerical stability</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>axiomatic tests (Sensitivity, Implementation Invariance), adversarial manipulation experiments (construct alternative gradients with little output change), randomization/sanity checks</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>sanity-check metrics (change in attributions after weight or label randomization), adversarial objective optimization, input-deletion/pixel-flipping evaluations comparing prediction degradation when removing high-attribution features</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Potentially large: attributions that appear plausible but are unconnected to model decision can mislead debugging and fairness auditing; some methods fail to change when model weights are randomized, indicating unfaithfulness (qualitative and measured test failures reported).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Common across gradient-based explanation implementations; several papers report these weaknesses as recurring issues</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>mathematical properties (saturation), dependence on arbitrary baseline choices, numerical instability, and under-specified method descriptions in prose</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>use axiomatic evaluations to select methods satisfying sensitivity properties, prefer reference-based propagation methods (DeepLift), use SmoothGrad to reduce visual noise, combine with perturbation-based causal checks, and report assumptions and baselines explicitly</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Some methods (DeepLift, Integrated Gradients with careful baselines) improve sensitivity in tests; SmoothGrad reduces visual noise but does not guarantee faithfulness; overall partial mitigation with careful evaluation recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>deep learning explainability (CV and NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Faithful Model Explanation in NLP: A Survey', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e432.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e432.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Influence-function fragility</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mismatch between influence-function theoretical claims and practical approximations in deep networks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Influence functions are described as an inherently faithful way to estimate how training examples affect test predictions, but implementations rely on convexity approximations and can be fragile in deep, non-convex models, producing unreliable attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding Black-box Predictions via Influence Functions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Influence-function based training-example influence estimator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tooling that approximates the effect of removing or modifying a training example on model parameters and predictions via influence function computations rather than retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>method theoretical specification / reproducibility claim (influence≈retraining)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Hessian-vector product based approximation code often applied to deep models</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>assumption-violation in implementation / approximation breakdown</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Papers state influence functions approximate retraining effects; implemented approximations assume convex (or locally well-behaved) loss landscapes — an assumption violated by deep non-convex models leading to variable and fragile approximation quality across architectures, depths, regularization, and test points.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>training-example influence estimation / approximation layer</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical sanity checks comparing approximated influence to actual retraining or qualitative tests; analysis of approximation error under different model conditions (Basu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>approximation error statistics vs ground-truth retraining (where feasible), sensitivity analyses across model architectures and hyperparameters, qualitative case studies</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Can give misleading identification of influential training examples; survey reports that approximation accuracy varies widely and can be poor in modern large models, undermining trust in 'inherently faithful' claim.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed in follow-up studies; influence functions are used but their reliability is often conditional and thus not universally dependable</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>natural-language presentation glosses over convexity/locality assumptions; practical implementations apply approximations beyond theoretically-justified regimes</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>where possible, verify approximations against retraining on smaller models/tasks, provide uncertainty estimates, restrict use to settings where assumptions hold or use alternative example-based methods (kNN/influence with retraining)</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Verification and restricted application can detect failure cases; Basu et al. report that reliability improves in some controlled settings but remains fragile overall.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>machine learning interpretability / NLP and vision</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Faithful Model Explanation in NLP: A Survey', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e432.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e432.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Perturbation evaluation / ROAR distribution-shift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Perturbation-based evaluation producing OOD inputs and changing the model when retraining (ROAR findings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Perturbation-based evaluations (e.g., token/pixel deletion) are described as a faithfulness test in papers, but implementations that remove features cause out-of-distribution inputs and retraining-based fixes (ROAR) show prior evaluation drops can be due to distribution shift rather than identifying true important features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Benchmark for Interpretability Methods in Deep Neural Networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Perturbation-based explanation evaluation (input deletion and ROAR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation pipelines that remove features deemed important by explanations and measure model performance drop, with ROAR variant removing features from both train and test and retraining the model.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>evaluation protocol specification in papers (perturb-and-measure)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>dataset manipulation + model retraining scripts (ROAR) or single-run deletion scripts</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incomplete evaluation assumption / distribution-shift oversight</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Commonly-described evaluation assumes that deleting high-importance features from inputs directly measures attribution fidelity; implemented deletion often yields inputs outside the training distribution, confounding attribution with distribution shift. ROAR retraining was proposed, but retraining changes the model itself, so the test no longer measures the original explanation's faithfulness to the original model.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>evaluation stage (input perturbation and retraining)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical benchmarking (Hooker et al.) showing models retain surprising accuracy even after removing large fractions of input features; contrasting non-retrain deletion with ROAR retrain outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>performance drop on test set after deletion without retraining vs after retraining (ROAR), fraction of features removed (e.g., up to 90%) and resulting accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Shows that simple deletion-based evaluations can overestimate explanation fidelity or conflate fidelity with distributional artifacts; Hooker et al. found image models still achieve decent accuracy after removing up to 90% of pixels, implying previous deletion-based results may reflect OOD effects rather than faithful attributions.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Widely relevant to perturbation-based evaluations across CV and NLP; caution advised in many works</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>assumption that perturbed inputs remain in-distribution and that model remains comparable after retraining; omission of distribution-shift effects in natural-language descriptions of evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>use ROAR to control distribution shift by retraining (while noting the model changes), design more realistic counterfactuals, measure attribution robustness across multiple perturbation styles, and explicitly state assumptions</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>ROAR addresses the train/test distribution mismatch but at the cost of changing the model; it reveals that many prior evaluations conflated distribution shift with importance, demonstrating mixed effectiveness depending on the research question.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>explainability evaluation methodology (CV and NLP)</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Faithful Model Explanation in NLP: A Survey', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e432.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e432.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Implementation Invariance assumption tension</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tension between Implementation Invariance axiom and differing implementations with identical I/O</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Axiomatic evaluations sometimes assert Implementation Invariance (functionally equivalent models must have identical explanations), but the survey highlights that different implementations that are functionally equivalent can use different internal reasoning, and the axiom's application can lead to false negatives when judging faithfulness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Axiomatic attribution for deep networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Axiomatic faithfulness evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Evaluation frameworks that use axioms (e.g., Implementation Invariance, Sensitivity) described in papers as necessary conditions for faithful explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>theoretical principle / axiomatic claim in papers</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>evaluation test implementations (sanity checks applying axioms to methods)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>conceptual/assumption mismatch between theory and implementation</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Natural-language statements of Implementation Invariance treat functionally equivalent I/O as implying identical internal reasoning and thus identical explanations; authors argue this is not guaranteed and that different implementations (e.g., different sorting algorithms) may legitimately have different internal reasoning despite identical I/O, making the axiom unsuitable as a universal necessary condition in implemented evaluation pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>axiomatic evaluation stage / interpretation of theoretical criteria vs implemented models</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>conceptual analysis and counterexamples discussed in the survey (Jacovi & Goldberg critique), empirical sanity checks showing axioms fail to capture all faithfulness aspects</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>axiomatic pass/fail tests applied to explanation methods (e.g., Sundararajan et al.'s tests) and observing counterexamples</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Blind application of Implementation Invariance can incorrectly mark explanation methods as unfaithful or dismiss valid differences in internal reasoning; this affects interpretation of evaluation outcomes and cross-method comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Debated in the literature; widely used as a test but contested by authors of the survey</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>overly strong or underspecified natural-language articulation of axioms without clarifying assumptions about what constitutes equivalent reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>be explicit about assumptions when using axioms, use multiple complementary evaluations (axiomatic + perturbation + white-box), and avoid treating Implementation Invariance as universally necessary</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Conceptual clarifications and combined evaluation strategies reduce misinterpretation risk; survey recommends not relying solely on Implementation Invariance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>explainability theory and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Towards Faithful Model Explanation in NLP: A Survey', 'publication_date_yy_mm': '2022-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Attention is not Explanation <em>(Rating: 2)</em></li>
                <li>Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods <em>(Rating: 2)</em></li>
                <li>Sanity Checks for Saliency Maps <em>(Rating: 2)</em></li>
                <li>A Benchmark for Interpretability Methods in Deep Neural Networks <em>(Rating: 2)</em></li>
                <li>A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations <em>(Rating: 2)</em></li>
                <li>Understanding Black-box Predictions via Influence Functions <em>(Rating: 2)</em></li>
                <li>Learning to Deceive with Attention-Based Explanations <em>(Rating: 1)</em></li>
                <li>Anchors: High-Precision Model-Agnostic Explanations <em>(Rating: 1)</em></li>
                <li>Axiomatic attribution for deep networks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-432",
    "paper_id": "paper-252519203",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "Attention-as-explanation mismatch",
            "name_full": "Interpreting Attention Weights as Model Explanations",
            "brief_description": "Many papers and introductions describe attention weights (in Transformers/RNN attention) in natural language as token-level importance scores; empirical analyses show attention distributions can be decorrelated from causal contribution to predictions and can be adversarially manipulated without changing outputs.",
            "citation_title": "Attention is not Explanation",
            "mention_or_use": "mention",
            "system_name": "Transformer / attention-based classifier",
            "system_description": "Neural NLP models (e.g., BERT) that use self-attention; explanations are produced by reporting attention weights from particular heads or averaged heads.",
            "nl_description_type": "research paper methods section / intuitive description (attention=importance)",
            "code_implementation_type": "trained Transformer model implementation (attention weights computed in model forward pass)",
            "gap_type": "ambiguous description / misattributed interpretive claim",
            "gap_description": "Natural-language claims present attention weights as direct indicators of input-feature importance to final predictions; however implementations compute attention on intermediate hidden states which mix contextual information, and adversarially constructed (or trained) attention distributions can be very different while yielding nearly identical model outputs — demonstrating that attention weights do not reliably reflect causal contribution.",
            "gap_location": "model-internal structure / interpretation layer (attention computation and reporting)",
            "detection_method": "empirical correlation analyses and adversarial construction of attention distributions; literature critiques and counterexamples (correlational tests vs. gradient/saliency measures)",
            "measurement_method": "comparisons between attention weights and other feature-importance measures (gradients, perturbation results); adversarial objective that maximizes distance between attention distributions while constraining output change; pruning/ablation experiments measuring downstream performance change when attention heads removed",
            "impact_on_results": "Can mislead users into believing tokens with high attention are causal drivers; adversarially (or intentionally) learned attention can hide bias (Pruthi et al.); in some MT sequence-to-sequence settings attention pruning affects output substantially, but in many single-sequence settings attention changes have little impact — effect varies by task (qualitative and task-dependent impacts reported).",
            "frequency_or_prevalence": "Widely observed across attention-based NLP papers; the survey reports this as a central contested issue in many studies (common)",
            "root_cause": "ambiguous natural-language framing equating attention weights with importance, conflation of plausibility and faithfulness, and interpreting attention on hidden states rather than input-level contributions",
            "mitigation_approach": "regularization tying hidden states to input representations, auxiliary tasks (e.g., MLM) to force hidden states to reflect inputs, combine attention with attribution methods (e.g., Attention Attribution), head pruning plus causal intervention to validate contributions",
            "mitigation_effectiveness": "Reported partial success: Tutek & Snajder's weight-tying and MLM auxiliary tasks increase attention's influence on prediction; Attention Attribution strengthens meaningful connections and enables more reliable pruning (qualitative/improved causal signals reported but not universally sufficient).",
            "domain_or_field": "deep learning / NLP (Transformer models)",
            "reproducibility_impact": true,
            "uuid": "e432.0",
            "source_info": {
                "paper_title": "Towards Faithful Model Explanation in NLP: A Survey",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Erasure / surrogate sampling OOD vulnerability",
            "name_full": "Out-of-distribution and adversarial vulnerability of erasure-based surrogate explanation methods (LIME/SHAP/Anchors)",
            "brief_description": "Surrogate/explanation methods that generate nearby examples by erasing or masking features (LIME/SHAP/Anchors) rely on a neighborhood sampling described in papers, but implementations that sample masked inputs produce OOD distributions that can be exploited or give misleading explanations.",
            "citation_title": "Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods",
            "mention_or_use": "mention",
            "system_name": "Local surrogate explanation pipeline (LIME/SHAP/Anchors)",
            "system_description": "Black-box explanation pipeline that samples perturbations (often by masking features) in the neighborhood of an example, queries model predictions, weights samples by proximity, and fits a simple surrogate (e.g., sparse linear model) used as the explanation.",
            "nl_description_type": "method description / algorithm specification in papers (neighborhood sampling by masking)",
            "code_implementation_type": "explanation tool implementations / sampling code (LIME, SHAP approximations)",
            "gap_type": "incomplete specification / distribution-shift (OOD) during neighborhood sampling",
            "gap_description": "The natural-language description of neighborhood sampling often omits that masking or erasure produces inputs far from model training distribution; implementations sample these OOD instances which can trigger different model behaviors and allow crafted models to behave benignly on masked inputs while still encoding biases on in-distribution inputs — producing misleading 'explanations' that claim absence of bias.",
            "gap_location": "input perturbation / explanation sampling stage",
            "detection_method": "adversarially constructed pipelined model and empirical attack (Slack et al.) that behaves differently on masked (OOD) vs in-distribution inputs; analysis of neighborhood choice sensitivity (Laugel et al.)",
            "measurement_method": "attack success (ability to hide biased behavior from explanation methods); human predictive power or fidelity on surrogate predictions; white-box recovery tests (LIME recovers features on transparent models in controlled tests)",
            "impact_on_results": "Can cause explanation methods to report false negatives (e.g., claim model not using protected attribute) and thus under-detect bias; Slack et al. demonstrate successful attacks where LIME/SHAP report innocuous reasoning though model encodes sensitive information — practical security and trust consequences (qualitative attack demonstrations; no single global numeric loss metric universally reported).",
            "frequency_or_prevalence": "Common concern for erasure-based explanation approaches; many papers rely on masking-based neighborhoods and thus vulnerable (widespread in practice).",
            "root_cause": "masking/erasure produces unrealistic inputs and explanation descriptions rarely emphasize distributional mismatch or how to sample appropriately; implicit assumptions about neighborhood locality not stated",
            "mitigation_approach": "use perturbations that produce realistic counterfactuals, adversarially evaluate explanation tools, perform white-box tests on transparent models, expand neighborhood definitions, use model-aware generative sampling (e.g., language-model-based counterfactual generation like Polyjuice), or retrain/evaluate with ROAR-style procedures",
            "mitigation_effectiveness": "Mixed: ROAR (retraining after removals) addresses OOD sampling by aligning train/test distributions but changes the model (so measures different question); generative counterfactuals produce more realistic neighbors but add complexity; Slack et al. show many current defenses insufficient.",
            "domain_or_field": "machine learning explainability / NLP and CV explainers",
            "reproducibility_impact": true,
            "uuid": "e432.1",
            "source_info": {
                "paper_title": "Towards Faithful Model Explanation in NLP: A Survey",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Backprop saliency input-recovery",
            "name_full": "Saliency / backpropagation-based visualizations that reflect input recovery rather than model decision",
            "brief_description": "Methods like Guided BackPropagation and DeconvNet are often presented as faithful attribution methods in natural-language descriptions, but empirical analyses show they can produce human-readable visualizations even for random networks, indicating they reconstruct input features rather than reflect causal model reasoning.",
            "citation_title": "A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations",
            "mention_or_use": "mention",
            "system_name": "Backpropagation-based saliency methods (Guided BackProp, DeconvNet)",
            "system_description": "Attribution implementations that modify the backward pass to produce per-input relevance maps or saliency visualizations used to explain classifier decisions.",
            "nl_description_type": "method description in papers / visualization claim (backprop visualizations indicate important input regions)",
            "code_implementation_type": "modified backward() implementations in model code / explanation libraries",
            "gap_type": "misleading implementation behavior / overclaim of faithfulness",
            "gap_description": "Natural-language framing often implies these visualizations reveal the model's decision mechanism; implemented variants are shown empirically to produce similar human-readable visualizations regardless of the model parameters (including random weights), suggesting they perform (partial) input reconstruction and are not causally tied to the prediction.",
            "gap_location": "explanation algorithm (custom backward pass)",
            "detection_method": "sanity checks: randomizing model weights or labels and observing if saliency maps change (Adebayo et al.); theoretical analysis showing input-recovery behavior (Nie et al.)",
            "measurement_method": "sanity-check experiments (e.g., measure similarity of saliency maps before and after weight randomization), pixel-flipping/word-deletion tests checking prediction sensitivity to removed high-relevance regions",
            "impact_on_results": "Generates deceptive interpretability: humans perceive explanations as meaningful though they are independent of model reasoning; can cause false confidence in model behavior and misdirect debugging efforts (empirical sanity-check failures reported).",
            "frequency_or_prevalence": "Observed across several backprop-based visualization papers and widely used toolkits; a notable and repeatedly reported pathology.",
            "root_cause": "the backward-rule heuristics primarily recover features correlated with inputs rather than redistributing relevance consistent with the model's decision; over-ambitious natural-language characterization conflating visually pleasing maps with faithful explanations",
            "mitigation_approach": "apply sanity checks (randomization tests), prefer reference-based methods (DeepLift, Integrated Gradients), use perturbation-based faithfulness tests (ROAR), and combine multiple evaluation axes (axiomatic + perturbation + white-box tests)",
            "mitigation_effectiveness": "Sanity checks effectively reveal the problem; DeepLift and some propagation methods perform better on pixel-flipping tests but still have limitations (qualitative improvements reported in cited evaluations).",
            "domain_or_field": "computer vision explainability / adapted to NLP token saliency",
            "reproducibility_impact": true,
            "uuid": "e432.2",
            "source_info": {
                "paper_title": "Towards Faithful Model Explanation in NLP: A Survey",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Gradient-method manipulation & sensitivity",
            "name_full": "Adversarial manipulability and sensitivity of gradient-based attributions (Vanilla / Integrated Gradients / SmoothGrad)",
            "brief_description": "Gradient-based attribution methods are often described as faithful in papers, but implementations are sensitive to saturation, constant shifts, and can be adversarially manipulated to produce divergent attributions with minimal change in predictions.",
            "citation_title": "Axiomatic attribution for deep networks",
            "mention_or_use": "mention",
            "system_name": "Gradient-based attribution pipeline",
            "system_description": "Implementations that compute gradients (or integrated gradients and smoothing variants) from model outputs to inputs to produce token/pixel-level importance scores.",
            "nl_description_type": "method specification in research papers (gradient as local importance)",
            "code_implementation_type": "automatic backward() gradient computations and integration over baselines (captum/implementations)",
            "gap_type": "incomplete claim / sensitivity to implementation and baseline choices",
            "gap_description": "Natural-language descriptions claim gradients reflect feature importance; implemented gradient methods can fail when activations saturate, depend on baseline choice, be insensitive to model randomization, be sensitive to meaningless constant shifts, or be intentionally manipulated to yield misleading attribution distributions that do not change model outputs.",
            "gap_location": "attribution computation stage / baseline selection / numerical stability",
            "detection_method": "axiomatic tests (Sensitivity, Implementation Invariance), adversarial manipulation experiments (construct alternative gradients with little output change), randomization/sanity checks",
            "measurement_method": "sanity-check metrics (change in attributions after weight or label randomization), adversarial objective optimization, input-deletion/pixel-flipping evaluations comparing prediction degradation when removing high-attribution features",
            "impact_on_results": "Potentially large: attributions that appear plausible but are unconnected to model decision can mislead debugging and fairness auditing; some methods fail to change when model weights are randomized, indicating unfaithfulness (qualitative and measured test failures reported).",
            "frequency_or_prevalence": "Common across gradient-based explanation implementations; several papers report these weaknesses as recurring issues",
            "root_cause": "mathematical properties (saturation), dependence on arbitrary baseline choices, numerical instability, and under-specified method descriptions in prose",
            "mitigation_approach": "use axiomatic evaluations to select methods satisfying sensitivity properties, prefer reference-based propagation methods (DeepLift), use SmoothGrad to reduce visual noise, combine with perturbation-based causal checks, and report assumptions and baselines explicitly",
            "mitigation_effectiveness": "Some methods (DeepLift, Integrated Gradients with careful baselines) improve sensitivity in tests; SmoothGrad reduces visual noise but does not guarantee faithfulness; overall partial mitigation with careful evaluation recommended.",
            "domain_or_field": "deep learning explainability (CV and NLP)",
            "reproducibility_impact": true,
            "uuid": "e432.3",
            "source_info": {
                "paper_title": "Towards Faithful Model Explanation in NLP: A Survey",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Influence-function fragility",
            "name_full": "Mismatch between influence-function theoretical claims and practical approximations in deep networks",
            "brief_description": "Influence functions are described as an inherently faithful way to estimate how training examples affect test predictions, but implementations rely on convexity approximations and can be fragile in deep, non-convex models, producing unreliable attributions.",
            "citation_title": "Understanding Black-box Predictions via Influence Functions",
            "mention_or_use": "mention",
            "system_name": "Influence-function based training-example influence estimator",
            "system_description": "Tooling that approximates the effect of removing or modifying a training example on model parameters and predictions via influence function computations rather than retraining.",
            "nl_description_type": "method theoretical specification / reproducibility claim (influence≈retraining)",
            "code_implementation_type": "Hessian-vector product based approximation code often applied to deep models",
            "gap_type": "assumption-violation in implementation / approximation breakdown",
            "gap_description": "Papers state influence functions approximate retraining effects; implemented approximations assume convex (or locally well-behaved) loss landscapes — an assumption violated by deep non-convex models leading to variable and fragile approximation quality across architectures, depths, regularization, and test points.",
            "gap_location": "training-example influence estimation / approximation layer",
            "detection_method": "empirical sanity checks comparing approximated influence to actual retraining or qualitative tests; analysis of approximation error under different model conditions (Basu et al.)",
            "measurement_method": "approximation error statistics vs ground-truth retraining (where feasible), sensitivity analyses across model architectures and hyperparameters, qualitative case studies",
            "impact_on_results": "Can give misleading identification of influential training examples; survey reports that approximation accuracy varies widely and can be poor in modern large models, undermining trust in 'inherently faithful' claim.",
            "frequency_or_prevalence": "Observed in follow-up studies; influence functions are used but their reliability is often conditional and thus not universally dependable",
            "root_cause": "natural-language presentation glosses over convexity/locality assumptions; practical implementations apply approximations beyond theoretically-justified regimes",
            "mitigation_approach": "where possible, verify approximations against retraining on smaller models/tasks, provide uncertainty estimates, restrict use to settings where assumptions hold or use alternative example-based methods (kNN/influence with retraining)",
            "mitigation_effectiveness": "Verification and restricted application can detect failure cases; Basu et al. report that reliability improves in some controlled settings but remains fragile overall.",
            "domain_or_field": "machine learning interpretability / NLP and vision",
            "reproducibility_impact": true,
            "uuid": "e432.4",
            "source_info": {
                "paper_title": "Towards Faithful Model Explanation in NLP: A Survey",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Perturbation evaluation / ROAR distribution-shift",
            "name_full": "Perturbation-based evaluation producing OOD inputs and changing the model when retraining (ROAR findings)",
            "brief_description": "Perturbation-based evaluations (e.g., token/pixel deletion) are described as a faithfulness test in papers, but implementations that remove features cause out-of-distribution inputs and retraining-based fixes (ROAR) show prior evaluation drops can be due to distribution shift rather than identifying true important features.",
            "citation_title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
            "mention_or_use": "mention",
            "system_name": "Perturbation-based explanation evaluation (input deletion and ROAR)",
            "system_description": "Evaluation pipelines that remove features deemed important by explanations and measure model performance drop, with ROAR variant removing features from both train and test and retraining the model.",
            "nl_description_type": "evaluation protocol specification in papers (perturb-and-measure)",
            "code_implementation_type": "dataset manipulation + model retraining scripts (ROAR) or single-run deletion scripts",
            "gap_type": "incomplete evaluation assumption / distribution-shift oversight",
            "gap_description": "Commonly-described evaluation assumes that deleting high-importance features from inputs directly measures attribution fidelity; implemented deletion often yields inputs outside the training distribution, confounding attribution with distribution shift. ROAR retraining was proposed, but retraining changes the model itself, so the test no longer measures the original explanation's faithfulness to the original model.",
            "gap_location": "evaluation stage (input perturbation and retraining)",
            "detection_method": "empirical benchmarking (Hooker et al.) showing models retain surprising accuracy even after removing large fractions of input features; contrasting non-retrain deletion with ROAR retrain outcomes",
            "measurement_method": "performance drop on test set after deletion without retraining vs after retraining (ROAR), fraction of features removed (e.g., up to 90%) and resulting accuracy",
            "impact_on_results": "Shows that simple deletion-based evaluations can overestimate explanation fidelity or conflate fidelity with distributional artifacts; Hooker et al. found image models still achieve decent accuracy after removing up to 90% of pixels, implying previous deletion-based results may reflect OOD effects rather than faithful attributions.",
            "frequency_or_prevalence": "Widely relevant to perturbation-based evaluations across CV and NLP; caution advised in many works",
            "root_cause": "assumption that perturbed inputs remain in-distribution and that model remains comparable after retraining; omission of distribution-shift effects in natural-language descriptions of evaluation",
            "mitigation_approach": "use ROAR to control distribution shift by retraining (while noting the model changes), design more realistic counterfactuals, measure attribution robustness across multiple perturbation styles, and explicitly state assumptions",
            "mitigation_effectiveness": "ROAR addresses the train/test distribution mismatch but at the cost of changing the model; it reveals that many prior evaluations conflated distribution shift with importance, demonstrating mixed effectiveness depending on the research question.",
            "domain_or_field": "explainability evaluation methodology (CV and NLP)",
            "reproducibility_impact": true,
            "uuid": "e432.5",
            "source_info": {
                "paper_title": "Towards Faithful Model Explanation in NLP: A Survey",
                "publication_date_yy_mm": "2022-09"
            }
        },
        {
            "name_short": "Implementation Invariance assumption tension",
            "name_full": "Tension between Implementation Invariance axiom and differing implementations with identical I/O",
            "brief_description": "Axiomatic evaluations sometimes assert Implementation Invariance (functionally equivalent models must have identical explanations), but the survey highlights that different implementations that are functionally equivalent can use different internal reasoning, and the axiom's application can lead to false negatives when judging faithfulness.",
            "citation_title": "Axiomatic attribution for deep networks",
            "mention_or_use": "mention",
            "system_name": "Axiomatic faithfulness evaluation framework",
            "system_description": "Evaluation frameworks that use axioms (e.g., Implementation Invariance, Sensitivity) described in papers as necessary conditions for faithful explanations.",
            "nl_description_type": "theoretical principle / axiomatic claim in papers",
            "code_implementation_type": "evaluation test implementations (sanity checks applying axioms to methods)",
            "gap_type": "conceptual/assumption mismatch between theory and implementation",
            "gap_description": "Natural-language statements of Implementation Invariance treat functionally equivalent I/O as implying identical internal reasoning and thus identical explanations; authors argue this is not guaranteed and that different implementations (e.g., different sorting algorithms) may legitimately have different internal reasoning despite identical I/O, making the axiom unsuitable as a universal necessary condition in implemented evaluation pipelines.",
            "gap_location": "axiomatic evaluation stage / interpretation of theoretical criteria vs implemented models",
            "detection_method": "conceptual analysis and counterexamples discussed in the survey (Jacovi & Goldberg critique), empirical sanity checks showing axioms fail to capture all faithfulness aspects",
            "measurement_method": "axiomatic pass/fail tests applied to explanation methods (e.g., Sundararajan et al.'s tests) and observing counterexamples",
            "impact_on_results": "Blind application of Implementation Invariance can incorrectly mark explanation methods as unfaithful or dismiss valid differences in internal reasoning; this affects interpretation of evaluation outcomes and cross-method comparisons.",
            "frequency_or_prevalence": "Debated in the literature; widely used as a test but contested by authors of the survey",
            "root_cause": "overly strong or underspecified natural-language articulation of axioms without clarifying assumptions about what constitutes equivalent reasoning",
            "mitigation_approach": "be explicit about assumptions when using axioms, use multiple complementary evaluations (axiomatic + perturbation + white-box), and avoid treating Implementation Invariance as universally necessary",
            "mitigation_effectiveness": "Conceptual clarifications and combined evaluation strategies reduce misinterpretation risk; survey recommends not relying solely on Implementation Invariance.",
            "domain_or_field": "explainability theory and evaluation",
            "reproducibility_impact": true,
            "uuid": "e432.6",
            "source_info": {
                "paper_title": "Towards Faithful Model Explanation in NLP: A Survey",
                "publication_date_yy_mm": "2022-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Attention is not Explanation",
            "rating": 2,
            "sanitized_title": "attention_is_not_explanation"
        },
        {
            "paper_title": "Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods",
            "rating": 2,
            "sanitized_title": "fooling_lime_and_shap_adversarial_attacks_on_post_hoc_explanation_methods"
        },
        {
            "paper_title": "Sanity Checks for Saliency Maps",
            "rating": 2,
            "sanitized_title": "sanity_checks_for_saliency_maps"
        },
        {
            "paper_title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
            "rating": 2,
            "sanitized_title": "a_benchmark_for_interpretability_methods_in_deep_neural_networks"
        },
        {
            "paper_title": "A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations",
            "rating": 2,
            "sanitized_title": "a_theoretical_explanation_for_perplexing_behaviors_of_backpropagationbased_visualizations"
        },
        {
            "paper_title": "Understanding Black-box Predictions via Influence Functions",
            "rating": 2,
            "sanitized_title": "understanding_blackbox_predictions_via_influence_functions"
        },
        {
            "paper_title": "Learning to Deceive with Attention-Based Explanations",
            "rating": 1,
            "sanitized_title": "learning_to_deceive_with_attentionbased_explanations"
        },
        {
            "paper_title": "Anchors: High-Precision Model-Agnostic Explanations",
            "rating": 1,
            "sanitized_title": "anchors_highprecision_modelagnostic_explanations"
        },
        {
            "paper_title": "Axiomatic attribution for deep networks",
            "rating": 1,
            "sanitized_title": "axiomatic_attribution_for_deep_networks"
        }
    ],
    "cost": 0.027860749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Faithful Model Explanation in NLP: A Survey</p>
<p>Qing Lyu 
University of Pennsylvania
University of Pennsylvania
University of Pennsylvania</p>
<p>Marianna Apidianaki 
University of Pennsylvania
University of Pennsylvania
University of Pennsylvania</p>
<p>Chris Callison-Burch 
University of Pennsylvania
University of Pennsylvania
University of Pennsylvania</p>
<p>Towards Faithful Model Explanation in NLP: A Survey</p>
<p>End-to-end neural NLP architectures are notoriously difficult to understand, which gives rise to numerous efforts towards model explainability in recent years. An essential principle of model explanation is Faithfulness, i.e., an explanation should accurately represent the reasoning process behind the model's prediction. This survey first discusses the definition and evaluation of Faithfulness, as well as its significance for explainability. We then introduce the recent advances in faithful explanation by grouping approaches into five categories: similarity methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. Each category will be illustrated with its representative studies, advantages, and shortcomings. Finally, we discuss all the above methods in terms of their common virtues and limitations, and reflect on future work directions towards faithful explainability. For researchers interested in studying interpretability, this survey will offer an accessible and comprehensive overview of the area, laying the basis for further exploration. For users hoping to better understand their own models, this survey will be an introductory manual helping with choosing the most suitable explanation method(s).</p>
<p>Introduction</p>
<p>Since the birth of deep learning, end-to-end Neural Networks (NNs) have achieved enormous success on a wide range of NLP tasks (Wang et al. 2018. However, they largely remain a black-box to humans, i.e., lacking explainability. Recently, numerous methods have been developed to explain how these models work, but there is not yet any unified framework to assess their quality.</p>
<p>In this survey, we review existing model explanation methods through the lens of Faithfulness, the extent to which an explanation accurately reflects a model's reasoning process.</p>
<p>The survey is structured as follows:</p>
<p>• Section 1 introduces the general notion of explainability in NLP and Faithfulness as a central principle of model explanations.</p>
<p>• Section 2 synthesizes existing model explanation methods in pursuit of Faithfulness, by grouping them into five categories: similarity methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models.</p>
<p>• Section 3 discusses their common virtues and challenges and identifies future work directions towards improving Faithfulness.</p>
<p>• Section 4 concludes the survey.</p>
<p>Explainability in NLP</p>
<p>We start by introducing the notion of explainability in NLP, discussing its definition and importance. To prepare for our analysis of model explanation methods in subsequent sections, we will also present a set of properties to categorize different methods, as well as several common design principles.</p>
<p>What Is Explainability.</p>
<p>According to the Cambridge Dictionary, an explanation is:</p>
<p>The details or reasons that someone gives to make something clear or easy to understand (Walter 2008).</p>
<p>In the context of Machine Learning, explainability (also referred to as interpretability 1 ) is the extent to which the internal mechanics of a model can be presented in understandable terms to a human (Lipton 2017;Murdoch et al. 2019; Barredo Arrieta et al. 2020). Despite this intuitive notion, explainability has no established technical definition in the community, which results in numerous papers "wielding the term in a quasiscientific way", essentially referring to different concepts (Lipton 2017;Doshi-Velez and Kim 2017;Miller 2018;Murdoch et al. 2019). We argue that the confusion mainly lies in the interpretation of two key concepts in the above definition: (a) what are the "internal mechanics"? and (b) who is the "human"? Internal mechanics. This can refer to either (i) what knowledge a model encodes or (ii) why a model makes certain predictions. Past work in NLP explainability can be dissected along this dividing line:</p>
<p>(i) The "what" type of work aims to accurately characterize the capacity and deficiency of a model M , in terms of some target knowledge K, which can be linguistic, commonsense, world knowledge, etc. For example, given a Machine Translation (MT) model, does it implicitly capture linguistic knowledge such as the syntactic tree of the input and output sentences? Previous research answers such questions with methods including:</p>
<p>• probing classifiers (or auxiliary/diagnostic classifiers): (Veldhoen, Hupkes, and Zuidema 2016;Adi et al. 2017;Conneau et al. 2018),</p>
<p>• information-theoretic measurement (Voita and Titov 2020;Lovering et al. 2020);</p>
<p>• behavioral tests: including challenge sets (Levesque, Davis, and Morgenstern 2012;Gardner et al. 2020), adversarial attacks (Ebrahimi et al. 2018; Wallace et al. 2019a), and prompting (Petroni et al. 2019;Choenni, Shutova, and van Rooij 2021));</p>
<p>• visualization (Coenen et al. 2019;Ethayarajh 2019;Karidi et al. 2021).</p>
<p>Orthogonal to the choice of method, previous research also spans various domains of the target knowledge K:</p>
<p>• Syntax: Agreement (Linzen, Dupoux, and Goldberg 2016), Negative Polarity Items (Marvin and Linzen 2018), Anaphora (Marvin and Linzen 2018), Dependency Trees (Shi, Padhi, and Knight 2016;Hewitt and Manning 2019;Chi, Hewitt, and Manning 2020);</p>
<p>• Semantics: Lexical Semantics (Shwartz and Waterson 2018;Garí Soler and Apidianaki 2021), Semantic Roles (Ettinger, Elgohary, and Resnik 2016), Logic (e.g., Negation (Kassner and Schütze 2020;Dobreva and Keller 2021), Conjunction and Disjunction (Saha, Nie, and Bansal 2020), Quantifiers (Haruta, Mineshima, and Bekki 2020)), Monotonicity (Yanaka et al. 2020), Systematicity (Goodwin, Sinha, and O'Donnell 2020);</p>
<p>• Pragmatics: Implicatures and Presupposition (Jeretic et al. 2020;Schuster, Chen, and Degen 2020);</p>
<p>• World and Commonsense knowledge: Factual knowledge (Petroni et al. 2019;Kassner and Schütze 2020), Numeracy (Wallace et al. 2019c), Time (Zhou et al. 2019;Thukral, Kukreja, and Kavouras 2021), Space (Mirzaee et al. 2021;Masis and Anderson 2021), Factuality (Rudinger, White, and Van Durme 2018);</p>
<p>• Combination: (Conneau et al. 2018;.</p>
<p>It should be noted that "what is known" does not imply "what is used". For example, even if the previous MT model does encode the linguistic knowledge about syntax trees, this does not mean that it has necessarily used it in translation. Instead, it is possible that the model relies on spurious cues, i.e., still not "doing the right thing". In fact, researchers have found such evidence for some tasks (Ravichander, Belinkov, and Hovy 2021). This leads us to the "why" question.</p>
<p>(ii) The "why" type of work, by contrast, addresses the causal question of what factors (input features, model weights, decision rules, etc.) have led the model M to certain predictions Y . It does not assume that "what is present" equals "what is used", but tries to establish causality between possible factors and the prediction through various approaches. These approaches will be the focus of this survey, so we omit further discussion here. makes certain predictions can be presented in understandable terms to some target audience.</p>
<p>Why Is Explainability Important.</p>
<p>Compared to classic Machine Learning models, end-to-end NNs are intrinsically more complex in terms of the training scheme, reasoning mechanism, and capabilities (Bommasani et al. 2021). For example, it is easy to interpret a Decision Tree, since every node denotes a decision rule, e.g., does the word "advertisement" appear more than twice? By contrast, it is much more opaque what a node/layer in an NN represents and how it contributes to the prediction. Nevertheless, NNs have become the predominant paradigm in NLP research and are increasingly adopted in real-life applications, thus driving the demand to better understand their behavior.</p>
<p>Concretely, explainability can allow us to: (a) Discover artifacts in datasets. Solving the dataset does not mean learning the task, since there can often be unexpected shortcuts (e.g., statistical cues) in data creation. Models are surprisingly good at exploiting them (as shown by Kaushik and Lipton (2018); McCoy, Pavlick, and Linzen (2019); Geva, Goldberg, and Berant (2019), among many others). For example, only using the hypothesis 2 allows the model to achieve a decent performance on Textual Entailment (Poliak et al. 2018). Explaining what features are used in prediction will help us discover such artifacts and create more reliable datasets.</p>
<p>(b) Diagnose a model's strengths and weaknesses, and debug it. Explainability allows us to find where a model succeeds or fails, and fix it before adversaries could exploit them in real use. For example, a model can secretly rely on unwanted biases about gender, race, or age, but we can diagnose and eliminate them through explanations (Ravfogel et al. 2020). Also, if a model is susceptible to subtle perturbations in the data, it is better to discover and guard this in development prior to deployment (Wallace et al. 2019a).</p>
<p>(c) Enhance user trust in high-stake applications. In domains like health, law, finance, and human resources, an end-user may not trust a model if it only provides a prediction but no explanation. For example, if a healthcare center's chatbot tells the patient that they might have COVID based on their description, it is more convincing if there is supporting evidence, e.g., what symptoms are relevant. In such high-stake scenarios, it is crucial to ensure that models are making safe decisions for the right reasons (Janizek, Sturmfels, and Lee 2021).</p>
<p>Properties of Explanations.</p>
<p>Model explanation methods in NLP can be categorized with regard to the following properties:</p>
<p>(a) Time: when the explanation happens. An explanation can be post-hoc, i.e., it is produced after the prediction. Any opaque model is given, and then an external method explains it. Or, an explanation can be built-in, i.e., it is produced at the same time with the prediction. This means that the model is self-explanatory.  (b) Model accessibility: what parts of the model the explanation method has access to. A black-box explanation can only see the model's input and output, while a whitebox explanation can additionally access the model weights.</p>
<p>Method Time</p>
<p>Model accessibility Scope</p>
<p>Unit of explanation</p>
<p>Form of explanation</p>
<p>(c) Scope: where the explanation applies in the dataset. A local explanation only explains why a model makes a single prediction (or a small number of predictions in its local vicinity), whereas a global explanation provides insights into the general reasoning mechanisms for the entire data distribution.</p>
<p>(d) Unit of explanation: what the explanation is in terms of. A prediction can be explained in terms of input features , examples , concepts 3 (Rajagopal et al. 2021), feature interactions (Hao et al. 2021), or a combination of them .</p>
<p>(e) Form of explanation: how the explanation is presented. Typical forms include visualization (Li et al. 2016), importance scores (Arras et al. 2016), natural language (Kumar and Talukdar 2020), or causal graphs ).</p>
<p>(f) Target audience: who is the explanation provided for. As mentioned in Section 1.1.1, different audience groups can have distinct goals when requiring model explanations. Model developers may want to debug the model; fellow researchers may want to find how the model can be extended/improved; industry practitioners may want to assess if the model complies with practical regulations, e.g., it cannot use gender bias; and end-users may want to verify that they can safely rely on the model's decisions.</p>
<p>As a preview, Table 1 compares the model explanation approaches to be discussed in terms of these properties. 4 Note that many existing taxonomies do not explicitly state which properties are taken into account, thus often producing confusing terms. For instance, certain taxonomies juxtapose example-based, local, and global as three classes of explanations, but they are not even concerning the same property. As another example, the term saliency methods has been used to refer to Backpropagation-based methods in our taxonomy. In fact, saliency only describes the form and the unit of explanationimportance scores of features. Then, technically speaking, all methods in Table 1 have instances that can be called a saliency method. Therefore, we aim to outline all properties for comparison and specify their values for each method in our taxonomy.</p>
<p>Principles of Explanations.</p>
<p>To motivate principled design and evaluation of model explanations, previous work has identified a set of principles that a good explanation should satisfy. Here is a nonexhaustive synthesis:</p>
<p>(a) Faithfulness (also referred to as fidelity or reliability): an explanation should accurately reflect the reasoning process behind the model's prediction (Harrington et al. 1985;Ribeiro, Singh, and Guestrin 2016;Jacovi and Goldberg 2020). This is at the heart of this survey. In our opinion, Faithfulness is the most fundamental requirement for any explanation -after all, what is an explanation if it lies about what the model does under the hood? An unfaithful explanation can sound intuitive to humans, but has little to do with how the model makes the prediction. For example, by looking at the attention weights 5 of a sentiment classification model, it may be intuitive to interpret tokens with higher weights as "more important" to the prediction, whereas empirically it is questionable if such causal relation exists (Jain and Wallace 2019).</p>
<p>(b) Plausibility (also referred to as persuasiveness or understandability): an explanation should be understandable and convincing to the target audience (Herman 2019;Jacovi and Goldberg 2020). This implies that Plausibility depends on who the target audience is. For example, a relevance propagation graph across NN layers may be a perfectly plausible explanation for model developers, but not at all plausible to non-expert end-users.</p>
<p>(c) Input Sensitivity: the term is used by many papers in different senses, but the commonality is that an explanation should be sensitive (resp. insensitive) to changes in the input that influence (resp. do not influence) the prediction. Specifically, it has at least the following sub-principles:</p>
<p>(i) Sundararajan, Taly, and Yan (2017): (for explanations whose unit is features and form is importance scores), if two inputs differ only at one feature and lead to different model predictions, then the explanation should assign non-zero importance to the feature.</p>
<p>(ii) Adebayo et al. (2018): if the input data labels are randomized, the explanation should also change. This is because label randomization does change model predictions, so the explanation should be sensitive.</p>
<p>(iii) Kindermans et al. (2019): if a constant shift is added to every input example in the training data, the explanation should not change. This is because the model is invariant to the constant shift, and thus the explanation should not be sensitive.</p>
<p>(d) Model Sensitivity: likewise, the term is used in different senses, but in general, it means that an explanation should be sensitive (resp. insensitive) to changes in the model that influence (resp. do not influence) the prediction. Specifically, it requires at least that:</p>
<p>(i) Sundararajan, Taly, and Yan (2017): an explanation should be insensitive to model implementation details that do not affect the prediction. More specifically, two models are functionally equivalent if their outputs are equal for all possible inputs, although their implementations might be different. The explanation should always be identical for functionally equivalent models. This is called Implementation Invariance in the paper.</p>
<p>(ii) Adebayo et al. (2018): if the model weights are randomized, the explanation should also change. Similar to (c)(ii), weight randomization does change model predictions, so the explanation should be sensitive.</p>
<p>(e) Completeness: an explanation should comprehensively cover all relevant factors to the prediction (Sundararajan, Taly, and Yan 2017). More formally, for explanations in the form of importance scores, the importance of all features should sum up to some kind of "total importance" of the model output. 6 (f) Minimality (also referred to as compactness): an explanation should only include the smallest number of necessary factors (Halpern and Pearl 2005;Miller 2018). Intuitively, this is analogous to the Occam's razor principle, which prefers the simplest theory among all competing ones.</p>
<p>These principles have been proposed and entertained by the community, but not all of them have established technical definition, evaluation, or even a consensus on whether they are necessary as a principle. In fact, we do not agree with all of them, as will be discussed in Secion 1.2.4. Therefore, in this survey, we mainly focus on Faithfulness as it is generally considered one of the most central requirements for explanations (Jain and Wallace 2019; Bastings and Filippova 2020; Jacovi and Goldberg 2020, inter alia), but we will still refer to the other principles when discussing relevant work throughout subsequent sections.</p>
<p>Faithfulness as a Principle</p>
<p>Now, we will zoom in on Faithfulness as one of the above principles, analyzing what it means, its relation to other principles, why it is important, and how it can be measured.</p>
<p>Definition.</p>
<p>As mentioned before, a faithful explanation should accurately reflect the reasoning process behind the model's prediction (Harrington et al. 1985;Ribeiro, Singh, and Guestrin 2016;Jacovi and Goldberg 2020). This is only a loose description though. In fact, there is not yet a consistent and formal definition of Faithfulness in the community.</p>
<p>Instead, people often define Faithfulness on an ad-hoc basis, in terms of different evaluation metrics. We will detail it in Section 1.2.4.</p>
<p>Relation between Faithfulness and Other Principles.</p>
<p>We elucidate the relation between Faithfulness and several other principles introduced in Section 1.1.4, since they are often implicitly conflated in the literature. Faithfulness vs. Plausibility. There is an intrinsic tension between Faithfulness and Plausibility. Think about an extreme case: if an "explanation" were just a copy of all model weights, it would be perfectly faithful but not at all plausible to any target audience. Now consider the other extreme: we could first ask the target audience which features they consider the most important when they themselves were predicting the label of the example. We could then simply copy their response as our "explanation" of how our target model works. This "explanation" would be perfectly plausible to the audience since it fully matches the human reasoning process, but not at all faithful since it has nothing to do with how the model works. Therefore, Plausibility does not guarantee Faithfulness, and vice versa.</p>
<p>Moreover, when we observe that an explanation is implausible in human terms, there can be two possibilities: (a) the model itself is not reasoning in the same way as humans do, and (b) the explanation is unfaithful. For instance, if an explanation says that the model is mainly relying on function words like the, a instead of content words like awesome, great in sentiment classification, it could be that (a) the model is truly relying on those uninformative words to make predictions, potentially because of spurious correlations in the dataset, or that (b) the model is actually relying on content words, but the explanation does not faithfully reflect the fact. Thus, while a lot of prior work evaluates explanations via user studies, this only touches on Plausibility, but does not tell us anything about Faithfulness. Faithfulness vs. Sensitivity, Implementation Invariance, Input Invariance, and Completeness. These four principles are sometimes seen as necessary conditions for Faithfulness, though not always explicitly (Sundararajan, Taly, and Yan 2017;Kindermans et al. 2019;Yeh et al. 2019). Practically, they are often used to prove that an explanation is not faithful via counterexamples. For example, given an explanation method, researchers run it on a dataset and see if there exists any example(s) where these principles are violated. This is called sanity checks in the literature (Adebayo et al. 2018). Faithfulness vs. Minimality. There are no known relations between them, either from the literature or from the authors' point of view.</p>
<p>Importance.</p>
<p>We believe that Faithfulness is one of the most fundamental principles for explainability in any AI system. In NLP specifically, there are two additional pieces of empirical evidence in support of Faithfulness:</p>
<p>(a) Faithfulness establishes causality. As mentioned in Section 1.1.1, there are two types of work in NLP explainability: what knowledge a model encodes and why a model makes certain predictions. The what type of work tells us "what is known by the model", but this is oftentimes implicitly assumed to be also "what is used by the model in making predictions". However, this assumption is not sound. For example, Ravichander, Belinkov, and Hovy (2021) show that language models encode linguistic features like tense and number, although they are irrelevant to the end task labels. This means that a model can encode more than what eventually gets used. Therefore, findings from the what type of work are correlational but not causal. To establish causality, we need faithful explanations of how the model makes predictions.</p>
<p>(b) An unfaithful explanation can be dangerous. Consider an explanation that is not faithful but extremely plausible. It will look very appealing to end-users. Thus, even if the model makes wrong predictions in the future, users may still trust them simply because the explanation sounds plausible. For example, Pruthi et al. (2020) show that it is possible to attention weights can be a deceiving explanation to end-users. They train the model to attend minimally to gender-related tokens (e.g. he and she), therefore hiding the fact that the model is relying on gender bias in prediction. Users may still find the model trustworthy from the explanation since it seems free from bias.</p>
<p>Evaluation.</p>
<p>As mentioned in Section 1.2.1, Faithfulness does not have an established formal definition, but is usually defined ad-hoc during evaluation. However, the evaluation metrics are often inconsistent and incomparable with each other, making it difficult to objectively assess progress in this field.</p>
<p>In their seminal opinion piece, Jacovi and Goldberg (2020) outline several design principles of Faithfulness evaluation metrics, three of which are the most important (and most ignored) in our view: (a) "Be explicit in what you evaluate". Especially, do not conflate Plausibility and Faithfulness. (b) Faithfulness evaluation should not involve human judgment on explanation quality. Humans do not know whether an explanation is faithful; if they did, the explanation would be necessary. (c) Faithfulness evaluation should not involve human-provided gold labels (for the examples to be explained). A faithful explanation method should be able to explain any prediction of the model, whether it is correct or not.</p>
<p>With the above principles in mind, we review existing Faithfulness evaluation methods, which broadly fall into six categories: axiomatic evaluation, predictive power evaluation, robustness evaluation, perturbation-based evaluation, white-box evaluation and human perception evaluation.</p>
<p>(a) Axiomatic evaluation treats certain principles (also called axioms) from Section 1.1.4 as necessary conditions for Faithfulness, and test if an explanation method satisfies them. If it fails any test, then it is unfaithful. However, passing all tests does not guarantee that it is faithful. Thus, axiomatic evaluation is mostly used to disprove Faithfulness via counterexamples.</p>
<p>As mentioned in Section 1.2.2, principles that have been viewed as necessary conditions for Faithfulness include Model Sensitivity, Input Sensitivity, and Completeness.</p>
<p>In particular, under Model Sensitivity, Sundararajan, Taly, and Yan (2017) tests Implementation Invariance, which means that two functionally equivalent models (i.e., they have the same outputs for all inputs) should have the same explanation. An assumption of this test is that two models are functionally equivalent only if they have the same reasoning process (Jacovi and Goldberg 2020). If this assumption holds, when a method provides different explanations for functionally equivalent models, it is unfaithful. However, we do not agree with this assumption. First, it treats the model as a black-box, only considering its input and output. But the reason why we need explanations is that models should not be black-boxes. Second, there exist functionally equivalent models that rely on entirely different reasoning mechanisms, such as various sorting algorithms. It is counter-intuitive if all of them have the same explanation. Therefore, we do not believe that Implementation Invariance is a necessary condition for Faithfulness.</p>
<p>Other axiomatic tests under Model Sensitivity (Adebayo et al. 2018), Input sensitivity (Sundararajan, Taly, and Yan 2017;Adebayo et al. 2018;Kindermans et al. 2019), and Completeness (Yeh et al. 2019), are considered more sensible as necessary conditions for Faithfulness by us. 7 (b) Predictive power evaluation uses the explanation to predict model decisions on unseen examples, and considers a higher accuracy as indicating higher Faithfulness.</p>
<p>The assumption is that an explanation is unfaithful it results in different decisions than the model it explains (Jacovi and Goldberg 2020). In practice, there are two ways to derive predictions from the explanation: with (other) models or with humans. In the former case, either the explanation is an executable model itself, e.g., rule lists (Sushil et al. 2018), or another proxy model is trained with the explanation as input . In the latter case, humans are considered the "proxy model". They are asked to simulate the model's decision on new input examples with only access to the explanation (Doshi-Velez and Kim 2017; Ribeiro, Singh, and Guestrin 2018;Chen et al. 2018;Ye, Nair, and Durrett 2021).</p>
<p>In our opinion, the first case (with models) is theoretically reasonable. Nevertheless, in practice, it is still questionable how expressive the proxy model should be. If too expressive, the proxy model can learn the label itself, regardless of the quality of the explanation.</p>
<p>The second case (with humans) is less reasonable from our view. First, it mingles Plausibility with Faithfulness. If humans fail to simulate model predictions, then it could either that the explanation is not plausible (to them) or that the explanation is unfaithful. Moreover, when simulating the model's prediction, it is difficult to ensure that humans can eliminate their own judgments of what the gold label should be.</p>
<p>Therefore, we believe that the predictive power evaluation is a sensible test for Faithfulness, but we should be cautious with human involvement.</p>
<p>(c) Robustness evaluation measures if the explanation is stable against subtle changes in the input examples, e.g., images that are indistinguishable from each other.</p>
<p>In its earliest version, robustness requires that similar inputs should have similar explanations (Alvarez-Melis and Jaakkola 2018). However, this does not rule out the possibility that the model itself can be brittle to subtle input perturbations.</p>
<p>Later work remedies this flaw by imposing constraints on model predictions. Now, robustness means that for similar inputs that have similar outputs, the explanations should be similar (Ghorbani, Abid, and Zou 2019;Yeh et al. 2019). The underlying assumption is that on similar inputs, the model makes similar predictions only if the reasoning process is similar (Jacovi and Goldberg 2020).</p>
<p>We identify two problems with this evaluation. First, though the notion of "indistinguishable inputs" makes sense in vision, it is hardly applicable to NLP since the input space is discrete. Second, the above assumption is questionable. Even though the inputs and outputs are similar, the model's reasoning mechanism can still differ. For example, consider two similar cat images, which differ only in the length of the cat's fur. We observe that the model predicts cat for both images with similar confidence. Now, it is still possible that the model is mainly relying on different features (e.g., body shape in the first image, while fur color in the second) in the two predictions. There is theoretically nothing preventing the model from doing so. Thus, if we observe that an explanation is not robust, we cannot conclude if it is because the explanation is unfaithful or the model is truly relying on different features. Therefore, we do not recommend robustness as a good evaluation metric for Faithfulness.</p>
<p>(d) Perturbation-based evaluation perturbs parts of the input according to the explanation, and observes the change in the output. It differs from robustness evaluation in that robustness considers extremely similar inputs and expects that the explanation is similar; but now, we consider inputs that are not necessarily similar, and the expectation of the explanation depends on which parts of the input are perturbed.</p>
<p>Concretely, consider an explanation in the form of feature importance scores. We now remove a fixed portion of features from the input, based on the explanation. If the most important features are first removed, the model prediction is expected to change drastically. Conversely, removing the least important features should result in a smaller change. This type of evaluation has been widely adopted in both vision (Bach et al. 2015;Chen et al. 2018) and language (Arras et al. 2016;Chen et al. 2018;Serrano and Smith 2019;Jain and Wallace 2019;DeYoung et al. 2020;Atanasova et al. 2020).</p>
<p>One underlying assumption is that different parts of the input are independent in their contribution to the output (Jacovi and Goldberg 2020). However, features can be correlated. When one feature is removed, we cannot guarantee that other features stay untouched.</p>
<p>Another assumption is that the observed performance change does not come from nonsensical inputs. When certain features are perturbed, the resulting input becomes out-of-distribution (OOD). Compared to CV, this has more serious consequences in NLP, since removing a word can make the sentence ungrammatical or nonsensical, but removing a pixel almost does not change the semantics of an image. Hooker et al. (2019) addresses the issue in CV by proposing the RemOve And Retrain (ROAR) benchmark. According to a given explanation method, the set of most important features is removed from both the training and the testing data. The model is then trained and tested again on the new data, and a larger performance drop indicates higher Faithfulness. In their experiments, image classification models are found to still achieve decent accuracy even after most input features (90%) are removed. This indicates that the performance drop observed in previous evaluation approaches without retraining might indeed come from the distribution shift instead of the lack of important information. However, though ROAR ensures that the training and testing data come from the same distribution, it brings about a new problem -the model itself is not the same.</p>
<p>In short, while perturbation-based evaluation has been widely used, we should be cautious about the assumptions and their consequences, especially since there is still no good fix in NLP.</p>
<p>(e) White-box evaluation rely on known ground-truth explanations, against which a candidate explanation can be compared. The ground-truth explanations come from transparent tasks or transparent models. Transparent tasks are typically synthetic tasks where the set of informative features is controlled. For example, reconstructing a simple function (Chen et al. 2018;Hooker et al. 2019), counting and comparing the number of digits (De Cao et al. 2020), or text classification on synthetic hybrid documents . Since the informative features are controlled, any model that performs well on the tasks must have relied on these features. Therefore, the ground-truth explanation is known.</p>
<p>Transparent models are inherently interpretable models, e.g., Logistic Regression or Decision Tree. The ground-truth explanation of important features can be directly obtained through their weights or prediction structure (Ribeiro, Singh, and Guestrin 2016;Natesan Ramamurthy et al. 2020).</p>
<p>Thus, one can run an explanation method with the transparent task or the transparent model, and compare the resulting explanation with the ground truth. If they are clearly different, then the explanation method is unfaithful.</p>
<p>However, this test is still only a sanity check, constituting a necessary instead of sufficient condition for Faithfulness. Since the synthetic setups are simplified, passing the white-box test still does not guarantee that the explanation method can generalize to real-world scenarios.</p>
<p>(f) Human perception evaluation assesses if the explanation matches human perception. For example, if the explanation is a set of feature importance scores, to what extent does it align with human-annotated importance scores?</p>
<p>A lot of previous work report this type of user studies DeYoung et al. 2020), without clarifying what principle is evaluated. Essentially, such tests only evaluate Plausibility. For them to also touch on Faithfulness, we need to make the assumption that models reason in the same way as humans do. Obviously, this does not always hold; otherwise, we will not need explanations at all. As said at the beginning of Section 1.2.4, Faithfulness evaluation should not involve human judgment on the explanation quality (Jacovi and Goldberg 2020).</p>
<p>Summary.</p>
<p>Among existing evaluation approaches, we recommend axiomatic evaluation, predictive power evaluation (with models), perturbation-based evaluation, and white-box evaluation, with caveats specified before. More ideally, more than one of the above evaluations should be done, since some of them only test a necessary condition of Faithfulness.</p>
<p>To complement the list of design principles provided by Jacovi and Goldberg (2020), we additionally propose a few more towards a better evaluation of Faithfulness:</p>
<p>(a) Define Faithfulness in advance rather than ad-hoc. Instead of using the same term to refer to different things, a clear definition at the beginning of the evaluation will greatly benefit comparability.</p>
<p>(b) State the assumptions of the evaluation, where they do not hold, and its implications. Especially, do not make assumptions about how the model reasons, e.g., "they reason in the same way as humans do", as this conflates plausibility with Faithfulness.</p>
<p>(c) Disentangle the capacity of the model and the quality of the explanation. For example, a non-robust explanation can result from either the model relying on inconsistent features or the explanation being unfaithful.</p>
<p>Attempts at Faithful Explanation</p>
<p>Overview with Motivating Example</p>
<p>We summarize recent advances in developing faithful explanation methods into five categories: similarity methods, analysis of model-internal structures, backpropagationbased methods, counterfactual intervention, and self-explanatory models. To give the reader a quick overview, we briefly explain the intuition behind each method with a motivating example.</p>
<p>Consider the Sentiment Analysis task, where a model should determine if the sentiment of a given piece of text (e.g., product/movie review) is positive, negative, or neutral. An example input can be:</p>
<p>This movie is great. I love it.</p>
<p>Suppose the model prediction is positive, which matches the ground truth. Our goal, now, is to explain why the model makes such a prediction. Here is how each method answers the question on a high level:</p>
<p>Similarity methods provide explanations in terms of previously seen examples, similar to how humans justify their actions by analogy. Specifically, they identify training instances or concepts 8 that are similar to the current test example in the model's induced representation space (e.g. The movie is awesome, The TV show is great) as an explanation, assuming that the reasoning mechanism for similar examples is intrinsically similar.</p>
<p>Analysis of model-internal structures examines the activation patterns of nodes, layers, or other model-specific mechanisms (e.g., attention (Bahdanau, Cho, and Bengio 2015)), and derives an explanation with techniques like visualization, clustering, correlation analysis, etc. For example, on a visualized heat-map of attention weights, great and love may have the highest weight among all token positions. This can be interpreted as that these two tokens are contributing the most to the prediction.</p>
<p>Backpropagation-based methods compute the gradient (or some variation of the gradient) of the model prediction with respect to each feature (e.g., input token). Features with the largest absolute gradient value (say, great and love) are then considered most important to the prediction. The intuition behind is that theoretically, even a slight change in those features 9 could have resulted in a large change in the model output. For example, if great becomes good and love becomes like, the model's confidence of positive will probably not be as high.</p>
<p>Counterfactual intervention perturbs a specific feature (e.g., input token) while controlling for other features and observes the resulting influence in the model prediction.</p>
<p>For example, to test if the word great is important for the model prediction, we can mask it out or replace it with another word, e.g. OK, and see how the model prediction changes. If the probability of the positive class decreases dramatically, then the word great has been an important feature.</p>
<p>Self-explanatory models do not rely on a post-hoc explanation but provide explanations as a byproduct of the inference. For example, a self-explanatory model can be trained to predict the sentiment label (e.g., positive) and justify its prediction at the same time, by producing a natural language explanation (e.g., "The words great and love indicate that the person adores the movie").</p>
<p>Similarity Methods 2.2.1 Overview.</p>
<p>Similarity methods provide explanations in terms of training examples. Specifically, to explain the model prediction on a test example, they find its most similar 10 training examples in the learned representation space, as support for the current prediction. This is akin to how humans explain their actions by analogy, e.g., doctors make diagnoses based on past cases and courts make judgments based on precedents. Caruana et al. (1999) formalize the earliest approach of this kind, named "casebased explanation". Based on the learned hidden activations of the trained model, it finds the test example's k-Nearest Neighbors (kNN) in the training set as an explanation. Note that the similarity is defined in terms of the model's learned space but not the input feature space, since otherwise the "explanation" would be model-independent. The authors analyze the theoretical applicability of this approach to Decision Trees and Neural Networks, but did not experiment or evaluate it in practice. Wallace, Feng, and Boyd-Graber (2018) also use the kNN search algorithm; but instead of a post-hoc explanation, they replace the model's final softmax 11 classifier with a kNN classifier at test time. Concretely, during training, the model architecture is unmodified. Then, each training example is passed through the trained model, and their representations are saved. Now, the inference is done with a modified architecture: a test example is passed through the model and then classified based on the labels of its kNNs in the learned representation space. This approach does not degrade the model performance, according to experimental results with LSTM and CNN models on six text classification tasks. Moreover, it can be combined with other explanation methods of feature importance. However, the resulting explanations are only evaluated on whether the identified features align with human perception of importance on qualitative examples, which is irrelevant to Faithfulness. Rajagopal et al. (2021) introduce a self-explanatory classification model where one component is called a global interpretable layer. This layer essentially identifies the most similar concepts (phrases in this case) in the training data for a given test example. Their approach is mainly evaluated in terms of Plausibility to humans, e.g., how adequate/understandable/trust an explanation is based on their subjective judgment. Only one metric touches on Faithfulness -whether humans can predict model prediction based on the explanation. However, they only report the relative difference in the metric with and without the explanation, instead of absolute scores, which makes it hard to determine how faithful the approach is.</p>
<p>Past Work.</p>
<p>Advantages.</p>
<p>(a) Similarity methods are intuitive to humans since the justification by analogy paradigm has long been established.</p>
<p>(b) They are also easy to implement, as no re-training or data manipulation is needed. The similarity scores are available by simply passing examples through the trained model to obtain the model's representation of them. Figure 1: A neuron that "turns on" inside quotes (figure from Karpathy, Johnson, and Fei-Fei (2015)). Blue/red indicates positive/negative activations respectively, and a darker shade indicates larger magnitude.</p>
<p>(c) In addition, they are highly model-agnostic, since all kinds of neural networks have a representation space. Thus, any similarity metric (like cosine similarity) can be easily applied.</p>
<p>(d) Finally, human subjects rate similarity-based explanations (Rajagopal et al. 2021) as more understandable, adequate and trustworthy to several other baselines in the family of backpropagation-based methods and counterfactual intervention (Simonyan, Vedaldi, and Zisserman 2014;Han, Wallace, and Tsvetkov 2020).</p>
<p>Disadvantages.</p>
<p>(a) Most similarity methods only provide the user with the outcome of the model's reasoning process (i.e., which examples are similar in the learned space), but do not shed light on how the model reasons (i.e., how the space is learned) (Caruana et al. 1999).</p>
<p>(b) Existing work mostly evaluates the resulting explanations with Plausibilityrelated metrics, including adequacy, relevance, understandability, usefulness, etc., through human judgments. But as mentioned in Section 1.2, Plausibility is not equal to Faithfulness. These evaluations only measure whether the explanation aligns with how humans reason about an example, but not how models do it. Thus, it is questionable whether similarity methods can truly establish causality between the model prediction and the explanation.</p>
<p>Analysis of Model-Internal Structures 2.3.1 Overview.</p>
<p>The analysis of model-internal structures, e.g., neurons, layers, and specific mechanisms like convolution or attention, is believed to shed light on the inner workings of NLP models. Common analysis techniques include visualization (e.g., activation heatmaps, information flow), clustering (e.g., neurons with similar functions, inputs with similar activation patterns), and correlation analysis (e.g., between neuron activations and linguistic properties).</p>
<p>Past Work.</p>
<p>We categorize research into this area historically, breaking it into work that happened before and after the advent of the attention mechanism (Bahdanau, Cho, and Bengio 2015). The reason is that attention has become one of the most widely-adopted architectures in NLP systems nowadays and has significantly reshaped this line of work ever since. The pre-attention era. The initial success of neural models in NLP sparked interest in finding interpretable functions of individual neurons. (Karpathy, Johnson, and Fei-Fei 2015) examine the activation patterns of neurons in a character-level Long-Short Term Memory (LSTM) language model. They found neurons with specific purposes, e.g., one that activates within quotes, inside if-statements, or toward the end of a line, respectively ( Figure 1). Li et al. (2016) visualize LSTM's representation of compositional structures in language, including negation (e.g., "not + adjective") and intensification (e.g., "very + adjective"), as shown in Figure 2. Neurons are found to capture basic semantic properties of compositionality, for instance, negated positive adjectives (e.g. "not nice", "not good") are clustered together with negative adjectives (e.g. "bad") in the latent representation space. Also using visualization methods, Strobelt et al. (2018) identifies neurons in LSTM with specific functions, e.g., those that turn on for "a + adjective + noun".  and Hiebert et al. (2018) take the reverse direction: instead of analyzing which neurons fire for a given input pattern, they look for inputs that have similar neuron activations. Preliminary observations show that Gated Recurrent Units (GRU) and LSTM language models can capture certain orthographic and grammatical patterns.</p>
<p>In the meantime, a variety of visualization tools are developed, including RNNvis 12 (Ming et al. 2017), LSTMVis 13 (Strobelt et al. 2018), Seq2Seq-Vis 14 (Strobelt et al. 2019), etc., allowing researchers to inspect and interact with their own models.</p>
<p>The post-attention era. Since its birth, Transformers (Vaswani et al. 2017) has become the foundation of State-of-the-Art (SOTA) systems on many NLP tasks (Devlin et al. 2019;Liu et al. 2019;Clark, Luong, and Le 2020;Brown et al. 2020;Raffel et al. 2020).</p>
<p>12 https://www.myaooo.com/projects/rnnvis/ 13 http://lstm.seas.harvard.edu/ 14 https://seq2seq-vis.io/ Figure 3: An illustration of the self-attention function (figure from https://towardsdatascience.com/deconstructing-bert-part-2-visualizing-the-innerworkings-of-attention-60a16d86b5c1). Each output vector y j is a weighted average of input vectors X.</p>
<p>The core of Transformers is an attention mechanism, self-attention. Simply put, selfattention is a function that takes in a sequence of vectors X =&lt; x 1 , x 2 , ..., x n &gt; as input and returns another sequence of vectors Y =&lt; y 1 , y 2 , ..., y n &gt; of the same length. Each y i is a weighted average of all x i 's, i.e., y j = n i=1 a ij x i . These weights a ij are called attention weights, representing how much the model attends to each input feature when computing the weighted average. 15 In NLP Transformer models, we can think of the initial X as word embeddings, i.e., each x i is a vector representing a word in the input. Then, each y i can be viewed as a composite embedding. For example, in Figure 3, the embedding for ran in Y is a composite of embeddings for the, dog, and ran in X, parameterized by a ij . A Transformer model learns many such attention functions, each called an attention head. For example, in BERT (base), there are 12 layers × 12 heads/layer, which makes a total of 144 distinct attention heads. This allows the model to capture a broad range of linguistic phenomena, as we will discuss later.</p>
<p>Given this intuitive structure, it is tempting to interpret attention weights as the importance of input tokens to the output. Consider our running example, a sentence for sentiment classification: This movie is great. I love it. Figure 4 (left) shows how a BERTbased model makes a prediction: each input token embedding is encoded by a stack of intermediate Transformer layers, the core of which is the 144 attention heads; then, using the embedding of [CLS] (a special token for sentence classification tasks) in the final layer, it predicts the class label positive. On the right, it shows the attention weights from [CLS] to all tokens in the penultimate layer. Among all input tokens, great and love receive the highest averaged weights over all heads. They are often intuitively understood as the most important tokens for the prediction, which aligns with human perception. Such types of understanding have been used (implicitly or explicitly) as evidence for model interpretability in different tasks and domains, such as text classification (Martins and Astudillo 2016), knowledge base induction (Xie et al. 2017), and medical code prediction (Mullenbach et al. 2018).</p>
<p>(a) A BERT sentiment classification model. The class label is predicted with the embedding of CLS (a special token for sentence classification tasks) in the final layer.</p>
<p>(b) Attention weights from CLS to all tokens. Each color represents an attention head. Lines represent averaged attention weights over all heads, and darker shades stand for higher weights.  )) and its attention weights on input tokens (right, created using BertViz(Vig 2019)). Given the input This movie is great. I love it., the model predicts positive using CLS. In the penultimate layer, great and love are among the tokens with highest attention weight from CLS.</p>
<p>Meanwhile, as in RNNs, people also discover units that capture interpretable input patterns in Transformer-based language models. By visualizing the activation patterns ( Figure 5), Vig (2019) finds individual attention heads in GPT-2 (Radford et al. 2019) and BERT (Devlin et al. 2019) that are responsive to lexical patterns like named entities, subject-verb pairs, prepositions, acronyms, or coreference. Clark et al. (2019) further confirm these findings quantitatively, showing that certain attention heads can predict linguistic dependencies with remarkably high accuracy (e.g., direct objects of verbs, determiners of nouns, objects of prepositions, and coreferents) ( Figure 6).</p>
<p>Despite these intuitive findings, researchers started the debate on whether attention constitutes a faithful model explanation. In their work, "Attention is not Explanation", Jain and Wallace (2019) contend that attention weights do not correlate with other feature importance measures that measure the feature's contribution to the output (e.g., gradient-based measures, which we will cover in Section 2.4). Also, one can construct an "adversarial attention distribution", i.e., one that is maximally different from the original distribution but has little influence on the model output. For example, in Figure 7, a sentiment classification model predicts that a movie review is negative.</p>
<p>Original attention weights suggest that waste and asking appear the most important, but the adversarial distribution shifts the model's attention onto uninformative words like myself and was, without changing its prediction.</p>
<p>As a direct response, Wiegreffe and Pinter (2019) offer several counter-arguments: (a) Existence does not entail exclusivity. Theoretically, attention weights provided an explanation, but not the only explanation. In practice, most tasks considered in Jain and Wallace (2019) (2019)). The left and the right column stand for the output and the input of the attention function, respectively (i.e., each vector on the left is a weighted combination of all vectors on the right). Darker shades indicate larger attention weights. parameters. Therefore, it should not be surprising if adversarial attention distributions can be found. (b) Adversarial distributions are not adversarial weights. The adversarial attention distributions are artificially constructed by humans, but not learned by models through training. In other words, any trained model would probably not naturally attend most to uninformative words like myself and was. In fact, even when the authors try to guide the model's attention towards such uninformative words using specially designed training objectives, they seldom converge to these adversarial distributions.  (2019)). The left shows the observed attention weights (waste seems most important), and the right shows an adversarially constructed set of attention weights while controlling for all other parameters θ (was seems most important). Despite being quite dissimilar, they yield effectively the same prediction (f = 0.01).</p>
<p>distributions. For example, when predicting the occupation of a person in the text, the model is trained to assign minimal attention weights to gender indicator tokens (e.g. "he" and "she"), while it is still using this signal in predictions. This implies that attention weights can be deceiving, i.e., a human user might find the model trustworthy since it is seemingly not relying on gender biases, yet it still does under the hood.</p>
<p>In favor of the use of attention as explanations, other researchers argue that existing criticisms mainly target single sequence tasks (e.g., Sentiment Analysis), but sequenceto-sequence tasks (e.g., Machine Translation (MT)) may be a more suitable use case because of the interaction between the two sequences. Similar to previous findings in single sequence tasks, attention heads in MT also capture semantic and syntactic dependencies (Voita et al. 2018;Raganato and Tiedemann 2018;Voita et al. 2019). However, regarding the contribution to the output, it is found that modifying or pruning the encoder-decoder attention heads does influence MT model predictions substantially (Voita et al. 2019;Vashishth et al. 2019), different than the case of single sequence tasks in Jain and Wallace (2019). Other attention heads, such as the encoder-or decoder-only ones, are less influential (Voita et al. 2019;Raganato, Scherrer, and Tiedemann 2020). In addition, in terms of the correlation with other feature importance measures, Ferrando and Costa-jussà (2021) qualitatively show that the L 2 -norm of attention vectors does correlate with gradient-based feature contributions.</p>
<p>Summarizing the debate, Bastings and Filippova (2020) still argue against attention and in favor of saliency methods 16 as faithful explanations. However, they acknowledge that understanding the role of the attention mechanism is still interesting (e.g., what linguistic information it captures; which heads can be pruned without performance loss).</p>
<p>To make attention more faithful to model predictions, recent studies have started exploring ways to remedy the flaws of the interpretation. Tutek and Snajder (2020) argue that a problem with the current interpretation is that attention weights are as-signed to hidden states (in intermediate layers) instead of input representations (in the initial layer), but we nevertheless interpret attention weights as the importance of the corresponding input feature. In fact, hidden states may have already mixed in information from other input features. To fix this, the authors introduce two regularization techniques: weight tying, which minimizes the distance between hidden states and their corresponding input representation; and an auxiliary Masked Language Modeling (MLM) task, which decodes input representations from their corresponding hidden state. Both techniques aim to make hidden states more representative of input representations. Experiments show that they effectively increase the influence of modifying the attention distribution on model predictions. Hao et al. (2021) explore an alternative path by combining attention with backpropagation-based methods, which we will revisit in Section 2.4. Their proposed Attention Attribution approach greatly eliminates uninformative connections in the vanilla attention weights 17 and strengthens connections that contribute to the final prediction. With the new attention distribution, the authors are able to prune attention heads without much contribution, construct reasoning dependency graphs, and generate adversarial examples to attack models.</p>
<p>Readers who are interested in visualizing attention can look into the following tools: BertViz 18 (Vig 2019) and LIT 19 (Tenney et al. 2020).</p>
<p>Advantages.</p>
<p>(a) The visualization of model-internal structures is intuitive and readable to humans, especially end-users.</p>
<p>(b) There are many interactive tools, which help the user form hypotheses about their data and models and dynamically adjust them through minimal testing.</p>
<p>(c) The attention mechanism can capture the interaction between features, whereas many other methods can only capture the sole influence of features themselves.</p>
<p>(d) Model weights are easily accessible and computationally efficient, compared to other methods.</p>
<p>Disadvantages.</p>
<p>(a) Vanilla attention weights may not necessarily represent causal contribution, as mentioned in the debate.</p>
<p>(b) We often interpret attention weights on hidden states as the importance of input representations; however, a hidden state may not represent its corresponding input token because of contextual information.</p>
<p>(c) Attention weights reflect how much the model attends to each input position at one time step, but not taking the whole computation path into account.</p>
<p>Backpropagation-based Methods 2.4.1 Overview.</p>
<p>By the name "backpropagation-based methods", we refer to two specific lines of work: gradient methods and propagation methods.</p>
<p>They both attempt to identify the contribution of input features via a backward pass through the model, propagating the importance (or relevance, used interchangeably in the literature) scores from the output layer to the input layer.</p>
<p>The key difference is that gradient methods follow standard backpropagation rules. In other words, they directly compute the gradient (or some variant of it) of the output w.r.t the input features via the chain rule, assuming features with larger gradient values are more influential to the model prediction. By contrast, propagation methods define custom backpropagation rules for each layer and compute the relevance scores layer by layer until reaching the input. This is believed to better capture the redistribution of relevance through different layer types.</p>
<p>Most ideas in this family have been first proposed in Computer Vision (CV) and then adapted to NLP. In the following subsection, we will explain their origin and then adaptation.</p>
<p>To synthesize existing work, we will use the following notations throughout the remaining parts of the section: An example x (e.g., an image or a sentence) has features x i , i ∈ {1, 2, ...n} (e.g., a pixel or a token). A model M takes x as input and predicts y = M (x) as output. Our goal is to explain the relevance of each feature x i to y, denoted by r i (x). For some specific methods, we also define a baseline input x (e.g., an all-black image, or a sentence with all-zero token embeddings) against which x is compared. We will define each subsequent method using these notations.</p>
<p>Past Work.</p>
<p>We describe first gradient methods and then propagation methods.</p>
<p>Gradient methods. As their name suggests, gradient methods treat the gradient (or some variant of it) of the model output w.r.t each input feature as its relative importance. Typically, the feature can be a pixel in vision and a token in language. Intuitively, the gradient represents how much difference a tiny change in the input will make to the output. This idea comes from linear models (e.g., Logistic Regression), where each feature has a linear coefficient as their importance to the output. In the case of non-linear models, a natural analog of such coefficients would be gradients, as they characterize the marginal effect of a feature change on the output.</p>
<p>Using the notation in Section 2.4.1, the core difference of existing gradient methods lies in how they calculate r i (x), the relevance of feature x i , which is summarized in Table 2. Figure 8 shows a visual comparison of them.</p>
<p>The most straightforward idea is to take the gradient itself (referred to as Simple Gradients or Vanilla Gradients), ∂M (x) ∂x i , as the feature relevance (Baehrens et al. 2010;Simonyan, Vedaldi, and Zisserman 2014). The sign of the gradient represents whether the feature is contributing positively or negatively to the output, e.g., increasing or decreasing the probability of a certain class in a classification task. The magnitude of the gradient stands for the extent to which the feature influences the output. Typically, this is measured by the L 1 -norm or L 2 -norm of the gradient. Simple Gradients are easy to implement and intuitive to understand. However, they have two apparent problems. First, a function can be saturated. Consider common neuron activation functions like sigmoid (y = 1 1+e −x ) and tanh (y = e x −e −x e x +e −x ), shown in Figure 9. When x → ±∞, we have dy dx → 0. In other words, when the input feature value is large enough, it has a very small gradient locally, while it may have a large contribution to the output y globally. Second, gradient only measures the responsiveness of the output w.r.t. the feature (how much the output changes in response to an infinitesimal change in the feature), but not
Method Computation of r i (x) Simple Gradients ∂M (x) ∂x i , ∂M (x) ∂x i 1 , or ∂M (x) ∂x i 2 Gradient×Input x i ∂M (x) ∂x i Integrated Gradients (x i − x i ) 1 α=0 ∂M (x+α(x−x)) ∂x i dα approximated by (x i − x i ) 1 α=0 ∂M (x+α(x−x)) ∂x i SmoothGrad 1 m m 1r i (x)(x + N (0, σ 2 ))
wherer i (x) is any other relevance computation the contribution of the feature (how much the current feature value contributes to the output value) (Bastings and Filippova 2020). Say, in an image classification model, you can think of a gradient as an explanation of how to make an image more (or less) a cat, but not what makes the image a cat. More formally, taking a simple linear model y = n i=1 w i x i as an example, the gradient w i measures the responsiveness while w i x i measures the contribution. If w i is small but x i is very large, the proportion of w i x i in y can still be large. This cannot be capture by the gradient alone.</p>
<p>As a natural solution to the second issue, the Gradient×Input method (Denil, Demiraj, and de Freitas 2015) is proposed. It computes the relevance score of a feature as the dot product of the input feature and the gradient, x i ∂M (x) ∂x i , analogous to w i x i in a linear model. Intuitively, this incorporates the feature value itself. In CV, the method empirically reduces noise in feature relevance visualizations, e.g, making the edges sharper. However, it is unclear how much of the reduction comes from the sharpness in the input image, and how much comes from the model's behavior. Also, Gradient×Input fails the Sensitivity test (cf. Section 1.1.4), i.e., if two inputs differ only at feature x i and lead to different predictions, then x i should have non-zero relevance. A simple counterexample is when the differing feature x i in these two inputs both have a zero gradient. Thus, the dot product is also both zero, which fails to capture their difference.</p>
<p>To address the saturation and sensitivity issues, Sundararajan, Taly, and Yan (2017) introduce the Integrated Gradients method. It estimates the global relevance of a feature by comparing the input with a baseline input x. Typically, the baseline is chosen as an all-black image for vision and a sentence with all-zero token embeddings for language. With the original x and the baseline x, the algorithm consist of four stages: first, it interpolates input points between x and x, formally where α ∈ [0, 1]; second, for each interpolated x α , it computes the gradient of model output w.r.t to the target feature, or Figure 9: The sigmoid function and the tanh function. Both are "saturated" (i.e., the gradient is near-zero) when the input is small or large enough. third, the gradients are integrated, yielding
x α = x + α(x − x)),∂M (x α ) ∂x i ; 20 (a) sigmoid: y = 1 1+e −x (b) tanh: y = e x −e −x e x +e −x1 α=0 ∂M (x α ) ∂x i dα,
which is approximated by 1 α=0</p>
<p>∂M (x α ) ∂x i in practice; finally, the integral is re-scaled to remain in the same space as the original input, resulting in the final relevance score
r i (x) = (x i − x i ) 1 α=0 ∂M (x α ) ∂x i .
Integrated Gradients satisfy the Sensitivity principle, as opposed to Gradient×Input. However, it is empirically observed to be visually noisy in CV, often resulting in blurry or unintelligible feature relevance maps (Smilkov et al. 2017). 21 To this end, SmoothGrad is introduced (Smilkov et al. 2017), attempting to "remove noise by adding noise". It hypothesizes that the Integrated Gradients method is visually noisy because the gradient can fluctuate rapidly with only subtle changes in the input. For example, one of the most commonly used activation functions, ReLU (y = max(0, x)), is not continuously differentiable (at x = 0, the gradient does not exist). Such local fluctuations may lead to the apparent visual diffusion in relevance maps. Therefore, SmoothGrad proposes to create a few noisy copies of the original input, compute relevance maps for each copy with any existing method, and finally average all maps to obtain a less noisy map. Formally, the final relevance score is defined as
r i (x) = 1 m m 1r i (x)(x + N (0, σ 2 )),
where m is the number of noisy copies, N (0, σ 2 ) is Gaussian noise with mean 0 and standard deviation σ, andr i (x) is any other relevance computation. This method proves effective in visually denoising the relevance maps. However, it is only qualitatively evaluated in terms of human readability; no Faithfulness is assessed. In NLP, both Simple Gradients and Integrated Gradients have been adopted, but mostly targeting sequence classification tasks. Li et al. (2016) use Simple Gradients to explain token importance in RNN models on sentiment classification. More recently, targeting Transformer models, Hao et al. (2021) and Janizek, Sturmfels, and Lee (2021) adapt Integrated Gradients to capture token interactions on paraphrase detection, natural language inference, as well as sentiment classification.</p>
<p>Propagation methods. While gradient methods follow standard backpropagation rules, propagation methods define a custom backward pass, using purposely designed local propagation rules for different layer types to emphasize individual neurons' contribution to the output.</p>
<p>We now formalize the process using a feed-forward network as an example, as shown in Figure 10. As before, let x represent the input and M (x) represent the output of model M after a forward pass. Denote any layer in M by l (l = 1, 2, ..., L), the dimension of which is d l . Define R (l) i as the relevance score of any neuron i in layer l. Our goal is to find R 
R (l) i = M (x), for l = L; D(R (l+1) j ), for 1 ≤ l &lt; L.(1)
The recursion terminates once reaching l = 1. All subsequently introduced propagation methods follow the same procedure above, while differing in the definition of D(), summarized in Table 3. Unfortunately, there is no unified visualized comparison of all these methods yet. We will illustrate each method individually and provide visualizations when possible.</p>
<p>Among the earliest methods in this family, DeconvNet (Zeiler and Fergus 2014) and Guided BackPropagation (GBP) (Springenberg et al. 2015) both design custom rules for ReLU units (y = max(0, x)) in particular, since it is arguably the most commonly used non-linear activation at that time. Formally, suppose a ReLU unit j in l + 1 is connected to a set of neurons i = 0, 1, ...d l in l. Let a i denote the activation of any neuron i. Then we have a j = max( d l i=0 a i w ij , 0) by the definition of ReLU, where w ij is the weight connecting i and j. 22 According to the standard backpropagation rule used by Simple Gradients, only positive inputs in the forward pass ( d l i=0 a i w ij &gt; 0) will have non-zero gradients in the backward pass. In other words,
R (l) i = d l+1 j=0 1 d l i=0 a i w ij &gt;0 · R (l+1) j ,
where 1 is the indicator function. 24 It is believed that this rule is not ideal for relevance redistribution. By contrast, DeconvNet proposes to zero out the redistributed relevance only if the incoming relevance R (l+1) j is non-positive, regardless of the input d l i=0 a i w ij :
R (l) i = d l+1 j=0 1 R (l+1) j &gt;0 · R (l+1) j .
On the other hand, Guided BackPropagation combines the two rules above, zeroing out the redistributed relevance if either the incoming relevance or the input is nonpositive:
R (l) i = d l+1 j=0 1 d l i=0 a i w ij &gt;0 · 1 R (l+1) j &gt;0 · R (l+1) j.
22 To conveniently incorporate the bias term b, we let a 0 = 1 and w 0j = b. 23 Since D() is layer-specific, we only show one or more representative rules for each method here: the ReLU unit propagation rule for Simple Gradients, DeconvNet, and GBP; the general-form rule for LRP; the Rescale rule For DeepLift; and the general-form rule for Deep-Taylor Expansion. 24 https://en.wikipedia.org/wiki/Indicator_function   Compared with Simple Gradients, both DeconvNet and Guided BackPropagation produce cleaner feature relevance visualizations as perceived by humans (see Figure 11 for an example). However, they share several shortcomings. First, they cannot handle the Sensitivity test (cf. Section 1.1.4) and the saturation issue (Sundararajan, Taly, and Yan 2017; Shrikumar, Greenside, and Kundaje 2017), like certain gradient methods mentioned before. Second, because of zeroing out negative inputs and/or negative incoming relevance, both methods cannot highlight features that contribute negatively to the output . For example, the presence of floppy ears might lower the model's confidence that the image is a cat (instead, it is more likely a dog). Third, in terms of Faithfulness, it is shown that both methods are essentially doing (partial) input recovery, which is unrelated to the network's decision (Nie, Zhang, and Patel 2018). Whichever prediction class is chosen (e.g., cat, dog, ...), the feature attribution is almost invariant. Even with a network of random weights, Guided BackPropagation can still generate human-readable visualizations. Thus, it is suspected that the visualization has little to do with the model's reasoning process.</p>
<p>Method</p>
<p>Definition of D()</p>
<p>Simple Gradients
R (l) i = d l+1 j=0 1 d l i=0 a i w ij &gt;0 · R (l+1) j DeconvNet R (l) i = d l+1 j=0 1 R (l+1) j &gt;0 · R (l+1) j Guided BackPropagation R (l) i = d l+1 j=0 1 d l i=0 a i w ij &gt;0 · 1 R (l+1) j &gt;0 · R (l+1) j Layerwise Relevance Propagation R (l) i = d l+1 j=0 c ij d l i=0 c ij R (l+1) j DeepLift R (l) i = d l+1 j=0 a i w ij −a i w ij d l i=0 (a i w ij −a i w ij ) R (l+1) j Deep-Taylor Decomposition R (l) i = d l+1 j=0 ∂R (l+1) j ∂a i | {a i } (j) (a i − a i (j) )
While the previous two methods only treat ReLU specifically, Layerwise Relevance Propagation (LRP) then comes out as a more generalized solution (Bach et al. 2015). Instead of handcrafting rules directly, it first proposes a high-level Relevance Conservation constraint, i.e., the total incoming relevance into a neuron should equal the total outgoing relevance from it. In other words, for all neurons i = 1, 2, ..., d l in layer l and all neurons j = 1, 2, ..., d l in another layer l , we have
d l i=0 R (l) j = d l j=0 R (l ) j .(2)
Any propagation rule conforming to this constraint can be called an instance of LRP. The original paper proposes several such rules, among which we introduce three. All of them are in the following form:
R (l) i = d l+1 j=0 c ij d l i=0 c ij R (l+1) j .
( 3) where c ij denotes the contribution of neuron i to neuron j, and is defined by each rule differently. It can be verified that Equation 3 satisfies the Relevance Conservation constraint. Concretely, the three rules are:</p>
<p>(a) the Basic rule, which intuitively takes the product of the activation and the weight as the contribution from i to j:
R (l) i = d l+1 j=0 a i w ij d l i=0 a i w ij R (l+1) j ;
(4) (b) the Epsilon rule, which adds a small term in the denominator to alleviate numerical instability:
R (l) i = d l+1 j=0 a i w ij + d l i=0 a i w ij R (l+1) j ;(5)
as increases, the relevance becomes sparser;</p>
<p>(c) the Gamma rule, which favors positive contributions by up-weighting them separately:
R (l) i = d l+1 j=0 a i (w ij + γw + ij ) d l i=0 a i (w ij + γw + ij ) R (l+1) j ;(6)
where w + ij are the positive terms in all w ij 's. As γ increases, negative contributions gradually disappear.</p>
<p>Other LRP rules are omitted because of space, e.g., the Gamma-Beta rule, which treats positive and negative contributions individually.</p>
<p>LRP has several advantages. First, unlike gradient methods, differentiability or smoothness properties of neuron activations are not required for LRP. Second, Bach et al. (2015) demonstrate its Faithfulness through a pixel flipping evaluation in digit classification. Essentially, they flip a fixed percentage of pixels in the input image according to relevance scores assigned by LRP, and observe how the prediction changes. Results show that flipping pixels with the highest positive scores first rapidly decrease the predicted probability of the target class, while flipping those with close-to-zero scores first influence the prediction minimally. More interestingly, if we flip pixels with negative relevance scores with regard to an alternative class, the prediction can be re-directed toward that class. For example, if the input image is a 3, we can fill in white pixels on the left side of the 3, since they are the ones speaking against an 8 (the alternative class) according to LRP. By this means, we can make the image more like an 8 to the model. All the above results suggest that LRP can indeed reflect the model's reasoning process to some extent, but no comparison is provided with other explanation methods.</p>
<p>Other criticisms of LRP include: first, it still suffers from the saturation problem ; second, it violates Implementation Invariance (cf. Section 1.1.4) ; third, there is no principled way to decide which rule to choose for which type of layer. The authors offer intuitive heuristics, but fail to support them with rigorous evidence.</p>
<p>In NLP, LRP has been applied/extended to CNNs and RNNs on sentence classification tasks, including topic and sentiment classification (Arras et al. 2016(Arras et al. , 2017, to explain which tokens are most important to the prediction. Regarding Faithfulness, similar to pixel flipping in vision, the evaluation here is "word deletion": a fixed number of tokens are deleted from the input (i.e., setting their corresponding vector to zero) according to the relevance score assigned by an explanation (LRP, Simple Gradients, and a random baseline), and then we track the impact on the classification performance. It is observed that for correctly classified inputs, when deleting the most relevant tokens first, LRP leads to the most rapid classification performance drop; but for incorrectly classified inputs, when deleting the most irrelevant tokens first, LRP can most effectively boost the performance. This indicates that LRP does provide informative insights into the model's reasoning mechanism.</p>
<p>To address LRP's failure with saturation and the Sensitivity test, two referencebased methods, DeepLift  and Deep-Taylor Decomposition (DTD)( Montavon et al. 2017), are then introduced. Analogous to Integrated Gradients, they aim to measure the global (instead of local) contribution of input features by finding a reference point (or baseline) x to compare with the input x. Ideally, the baseline x should represent some "neutral" input, i.e., satisfying M (x) = 0, so we can attribute all positive contribution to the presence of x. In practice, it needs to be chosen with domain-specific knowledge.</p>
<p>Concretely, DeepLift (Shrikumar, Greenside, and Kundaje 2017) first chooses the baseline input x, and then computes its corresponding baseline output M (x), ideally 0. Then, it explains M (x) − M (x), the difference between the original and the baseline output, in terms of x − x, the difference between the original and the baseline input. Therefore, its defines the total relevance as R i (L) = M (x) − M (x), different from the R i (L) = M (x) case in previous methods. Then, for the recursive function D(), it has several rules, e.g., the Rescale rule and the Reveal-Cancel RULE. We introduce the first here:
R (l) i = d l+1 j=0 a i w ij − a i w ij d l i=0 (a i w ij − a i w ij ) R (l+1) j .(7)
This is analogous to the Basic rule in LRP, except for the baseline term. As for advantages, by using a baseline, DeepLift tackles the saturation and the Sensitivity issues. More importantly, When evaluated on a similar pixel flipping test for Faithfulness, DeepLift proves the most effective in manipulating the prediction toward the target class, compared to Integrated Gradients, Gradient×Input, Simple Gradients, and Guided BackPropagation. However, a downside is that DeepLift still violates Implementation Invariance (Sundararajan, Taly, and Yan 2017).</p>
<p>Deep-Taylor Decomposition (DTD) ) starts from a more theoretical perspective, treating the explanation as an approximation of the upper-level relevance R (l+1) j through Taylor decomposition. 25 Given a function f (x), its first-order Taylor decomposition at x = x is
f (x) = f (x) + ∂f ∂x | x=x T · (x − x) +(8)
where is the higher-order residual. When applied to model explanation, we can think R (l+1) j as f (x), essentially decomposing the upper-level relevance on the activation {a i } of lower-layer neurons {i} to which j is connected. If the decomposition is at the baseline {a i } (j) , 26 which has zero relevance, we have
R (l+1) j = d l i=0 ∂R (l+1) j ∂a i | a i (j) T · (a i − a i (j) ) + j .(9)
Then, to determine the relevance for neuron i, we sum up all incoming relevance from all connected j's in the upper level:</p>
<p>25 Simply put, Taylor decomposition is a way to locally approximate a potentially non-polynomial function with a polynomial function. See more details at https://en.wikipedia.org/wiki/Taylor_series. 26 The choice of baseline is specific to each neuron j, thus the superscript.
R (l) i = d l+1 j=0 R (l+1) j .
(10)</p>
<p>Combining the two Equations above, we have
R (l) i = d l+1 j=0 ∂R (l+1) j ∂a i | {a i } (j) (a i − a i (j) )(11)
which is the definition of D().</p>
<p>The choice of the baseline {a i } is non-trivial, since it should not only have zero relevance, but also lie in the vicinity of {a i } for the Taylor decomposition to hold. In practice, this is often chosen based on the specific network type.</p>
<p>Interestingly, DTD can be viewed as a generalization of several previous methods. For instance, Simple Gradients is proved to be an instance of DTD not at a deliberately chosen baseline x, but at a point infinitesimally close to the input x in the direction of the maximum gradient. Also, LRP can also be considered a sequence of DTD performed at each neuron. Each LRP rule is equivalent to DTD with a different baseline point (Montavon et al. 2019).</p>
<p>In terms of evaluation, only qualitative results on human Plausibility are reported, while Faithfulness is unverified.</p>
<p>In NLP, Chefer, Gur, and Wolf (2021) extend DTD to explain the decision of Transformer models on sentiment classification. However, the explanations are evaluated against the ground truth, i.e., human-annotated token relevance, therefore unrelated to Faithfulness either. Tools. Readers interested in using backpropagation-based methods should consider the following packages: AllenNLP Interpret 27 (Wallace et al. 2019b), Captum 28 (Kokhlikyan et al. 2020), RNNbow 29 (Cashman et al. 2018), and DeepExplain 30 .</p>
<p>Advantages.</p>
<p>(a) Backpropagation-based methods generate a spectrum of feature relevance scores, which is easily readable for all kinds of target users.</p>
<p>(b) They are relatively easy to compute: gradient methods require only a few calls to the model's backward() function; propagation methods involve a custom implementation of backward(), but also allow precise control of the relevance redistribution process.</p>
<p>(c) In terms of Faithfulness, gradients (and variants) are intrinsically tied to the influence of input features on the prediction. Empirically, recently proposed methods (e.g., Layerwise Relevance Propagation, DeepLift, Deep-Taylor Decomposition) are shown to be more faithful than previous baselines through input deletion experiments, as mentioned before.</p>
<p>27 https://allenai.github.io/allennlp-website/interpret 28 https://captum.ai/ 29 https://www.eecs.tufts.edu/~dcashm01/rnn_vis/d3_code/ 30 https://github.com/marcoancona/DeepExplain (d) Unlike the analysis of model-internal structures (e.g., attention weights), backpropagation-based methods take the entire computation path into account, instead of only a snapshot at a single point.</p>
<p>Disadvantages.</p>
<p>(a) Most existing work target low-level features only, e.g., pixels in vision and input tokens in language. It is non-intuitive how to compute any sort of gradient w.r.t. higherlevel features like case, gender, part-of-speech, semantic role, syntax dependency, coreference, discourse relations, and so on.</p>
<p>(b) It is questionable if such methods are applicable to non-classification tasks, especially when there is no single output of the model, e.g., text generation or structured prediction.</p>
<p>(c) As mentioned before, certain methods violate axiomatic principles of explainability, e.g., Sensitivity and Input Invariance (Sundararajan, Taly, and Yan 2017).</p>
<p>(d) The explanation can be unstable, i.e., minimally different inputs can lead to drastically different relevance maps (Ghorbani, Abid, and Zou 2019;Feng et al. 2018).</p>
<p>(e) In terms of Faithfulness, most methods do not report empirical evaluation results, with only the aforementioned exceptions. Moreover, subsequent researchers find many systematic deficiencies of them in ad-hoc evaluations: (i) as mentioned before, Guided BackPropagation and DeconvNet are shown to be only doing input recovery, ignorant of the model's behavior (Nie, Zhang, and Patel 2018). (ii) certain explanations (including Simple Gradients, Integrated Gradients, and SmoothGrad) can be adversarially manipulated, i.e., one can construct entirely different gradient distributions with little influence on the prediction ). (iii) certain methods are sensitive to meaningless changes, e.g., adding a constant shift to all data points. This has no impact on the model behavior, but sometimes influences explanations substantially for methods like Gradient×Input as well as Integrated Gradients and Deep-Taylor Decomposition under certain conditions. ) (iv) certain methods are not sensitive to meaningful changes, e.g., randomizing the model weights or the data. In such cases, a faithful explanation is expected to change, while in practice this is not the case for methods including Guided BackPropagation and Grad-CAM (Adebayo et al. 2018).</p>
<p>Counterfactual Intervention 2.5.1 Overview.</p>
<p>The notion of counterfactual reasoning stems from the causality literature in social science: "given two occurring events A and B, A is said to cause B if, under some hypothetical counterfactual case that A did not occur, B would not have occurred" (Roese and Olson 1995;Winship and Morgan 1999;Lipton 2017). In the context of machine learning, counterfactual intervention methods explain the causal effect between a feature and the prediction by erasing or perturbing the feature and observing the change in the prediction. A larger change indicates stronger causality.</p>
<p>Past Work.</p>
<p>One axis along which we can categorize existing studies is what they intervene in: inputs or model representations. The former manipulates the input and passes it through the original model; however, the latter directly manipulates the model-internal structures, e.g., neurons or layers. The rest of this section will elaborate on the two categories. Figure 12: An illustration of the LIME (figure from Ribeiro, Singh, and Guestrin (2016)). The complicated decision boundary of the black-box model (light blue/pink) is locally approximated by a interpretable linear model (dashed line), in the proximity of the current prediction to the explained (bold red cross).</p>
<p>Intervening in inputs.</p>
<p>Input intervention can in turn be categorized along two dimensions: the target and the operation. The target refers to "what is affected by the intervention", normally input features (e.g., tokens) or examples (e.g., input sentences). The operation is the specific intervention method, which can be erasure (masking out the target) or perturbation (changing the value of the target).</p>
<p>We will first classify existing work based on the target, and then on the operation: (a) Feature-targeted intervention. Earliest work mostly relies on erasure, since it is relatively straightforward to implement.</p>
<p>One intuitive idea is leave-one-out, which erases a single feature at a time and assesses the resulting change in the prediction. Naturally, the feature can be input words (Li, Monroe, and Jurafsky 2017) or input word vector dimensions (Kádár, Chrupała, and Alishahi 2017;Li, Monroe, and Jurafsky 2017). In these studies, only Plausibility is examined based on human perception of qualitative examples, and no evaluation of Faithfulness is reported. Also crucially, leave-one-out captures the linear contribution of single features, but cannot handle higher-order feature interactions. For example, the word not may have contrasting contribution toward the sentiment of a sentence, depending on whether it is composed with good or bad.</p>
<p>To address this issue, researchers propose explanation methods that erase subsets of features instead of individual ones. Some studies (Li, Monroe, and Jurafsky 2017) aim to find the minimum subset of input tokens to erase such that the model's decision is flipped. Interestingly, others (Ribeiro, Singh, and Guestrin 2018) look for the contrary -the minimum subset of input tokens to keep such that the model's decision is unchanged (this is called "Anchors"). No matter which objective is taken, finding the exact desired subset of tokens is intractable, and thus both studies rely on approximated search. As a more efficient alternative, De Cao et al. (2020) propose DiffMask, a method that trains a classifier on top of input representations to decide which subset of tokens to mask. In terms of Faithfulness, Anchors is evaluated with human predictive power: compared to a popular baseline, LIME (Ribeiro, Singh, and Guestrin 2016), it allows users to more accurately predict model decisions on unseen examples. DiffMask proves more faithful than several other baselines including Integrated Gradients (Sundararajan, Taly, and Yan 2017), but only on synthetic tasks.</p>
<p>Based on the idea of feature erasure, a novel family of methods on surrogate models come out. The intuition is to locally approximate a black-box model with a white-box surrogate model as an explanation of the current prediction. LIME (Ribeiro, Singh, and Guestrin 2016), or Local Interpretable Model-agnostic Explanations, is a representative method of this type. Suppose the black-box model to be explained has a complicated decision boundary, as shown in Figure 12. Our goal is to provide a local explanation for the current prediction. LIME first samples instances in the neighborhood of the current example by masking out different subsets of its features. Next, it obtains the model prediction on these instances and weighs them by their proximity to the current example (represented by size in Figure 12). Then, it approximates the model's local decision boundary by learning an interpretable model, e.g., a sparse linear regression, on the input features, which constitutes the explanation. Another widely adopted method, SHAP (Lundberg and Lee 2017), or SHapley Additive exPlanations, can be thought of as using additive surrogate models as an explanation. Originated from the game theory, Shapley values (Shapley 1953) are initially used to determine how to fairly distribute the "payout" among the "players". In machine learning, it is adapted to explain the contribution of input features (players) to the prediction (payout). Consider, again, our running example:</p>
<p>This movie is great. I love it.</p>
<p>We can think of this input sentence x as containing a set of binary features x i , i.e., the presence of a token. Now, the Shapley value represents the contribution of each feature x i to the prediction. For example, suppose that x i denotes the presence of great. To compute its Shapley value, we need to (i) sample a subset of input features including x i to form a new example x , e.g., movie great, and obtain the model prediction M (x );</p>
<p>(ii) remove x i from x to form another example x , e.g., movie, and again obtain the new model prediction M (x );</p>
<p>(iii) compute the marginal contribution of x i as the difference between (ii) and (i), i.e., M (x ) − M (x ); and (iv) repeat the previous steps for all subsets containing x i . Finally, the Shapley value is the mean marginal contribution of x i . When the number of features is large, the above process is computationally expensive. Therefore, the SHAP paper introduces an efficient approximation, which obviates re-sampling a combinatorial number of subsets. In terms of Faithfulness, LIME is evaluated with white-box tests: when used to explain models that are themselves interpretable (e.g., Logistic Regression or Decision Tree), LIME successfully recovers 90% of important features. On the other hand, Shapley values are theoretically shown to be locally faithful, but there is no empirical evidence on whether this property is maintained after the SHAP approximation. Subsequent work also finds other limitations: (i) The choice of neighborhood is critical for such surrogate-model-based methods (Laugel et al. 2018); (ii) Linear surrogate models have limited expressivity. For example, if the decision boundary is a circle and the example is inside the circle, it is impossible to derive a locally faithful linear approximation.</p>
<p>Beyond simply erasing feature subsets, recent work also starts to explicitly model the strength of different feature interactions. For example, the Archipelago method (Tsang, Rambhatla, and Liu 2020) measures the contribution of the interaction between a pair of features by erasing all other features and recomputing the prediction. It is shown to be faithful on synthetic tasks such as function reconstruction. However, on realistic tasks like sentiment classification, only Plausibility is evaluated.</p>
<p>A major problem with feature erasure is that it results in out-of-distribution (OOD) inputs. For example, when a word is masked out, the resulting sentence often becomes ungrammatical or nonsensical. Exploiting this weakness, Slack et al. (2020) design a method to fool popular erasure-based methods. Suppose a task (e.g., loan application decision) involves sensitive features (e.g., gender or race), and the job of the explanation method is to examine if a model is relying on these features. To fool a given explanation method, the authors design an adversarial pipelined model with two modules. The first module is a classifier to determine if an input example is before or after the erasure. This classification is fairly easy because of the stark differences between the two distributions. Depending on the classification output, the second module makes a prediction on the original task using different reasoning mechanisms: if the input example is in-distribution, then the module will rely on sensitive features entirely; otherwise, it will behave innocuously, only making use of insensitive features. In other words, this pipelined model is indeed biased on all in-distribution examples. However, any explanation method that works by sampling neighboring examples by feature erasure (e.g., LIME and SHAP) will report that the model is unbiased, because they are designed to capture the model's behavior within the neighborhood.</p>
<p>This leads us to the other operationperturbation -as another type of featuretargeted intervention. Compared to simple erasure, perturbing the value of the target feature is less likely to result in OOD inputs. For instance, consider again our running example This movie is great. I love it. To study the importance of great to the prediction, one can replace it with some other word (e.g., good, OK) instead of just deleting it altogether, and observe the probability change of positive.</p>
<p>The outcome of such perturbations is often called counterfactual examples, which resemble adversarial examples in the robustness literature. They differ in at least three aspects though: (i) the goal of the former is to explain the model's reasoning mechanism, while that of the latter is to examine model robustness; (ii) the former should be meaningfully different in the perturbed feature (e.g., This movie is great → This movie is not so great), while the latter should be similar to, or even indistinguishable from, the original example (e.g., This movie is great → This movie is GREAT); (iii) the former can lead to changes in the label, whereas the latter do not (Kaushik, Hovy, and Lipton 2020). Prior work has explored different ways to create counterfactual examples, manual (Kaushik, Hovy, and Lipton 2020) or automatic (Wu et al. 2021). However, no evaluation of Faithfulness has been reported.</p>
<p>(b) Example-targeted intervention In addition to features, counterfactual intervention can directly happen on the level of examples. A representative method of this type is called influence functions (Koh and Liang 2017), which is designed to explain which training examples are most influential in the prediction of a test example. This may remind us of similarity methods in Section 2.2 (Caruana et al. 1999). Although both methods share the same goal, they rely on different mechanisms. Similarity methods identify the most influential training examples via similarity search, whereas influence functions are based on counterfactual reasoning -if a training example were absent or slightly changed, how would the prediction change?</p>
<p>Since it is impractical to retrain the model after erasing/perturbing every single training example, influence functions provide an approximation by directly recomputing the loss function. Essentially, erasing a training example is equivalent to upweighting it by − 1 n , and modifying it is the same as moving 1 n mass from it to the new example. Therefore, one can approximate the new model parameters resulting from the erasure/perturbation of a training example without having to retrain the model. After the invention in vision (Koh and Liang 2017), influence functions  2020)). The input change x (nurse → man) influences the output y, mediated by neuron z.</p>
<p>have been adapted to NLP (Han, Wallace, and Tsvetkov 2020). Although they are claimed to be "inherently faithful", this is not well-supported empirically. Crucially, the approximation relies on the assumption that the loss function is convex. Koh and Liang (2017) examines the assumption in vision, showing that influence functions can still be a good approximation even when the assumption does not hold (specifically, on CNNs for image classification). In NLP, though, only a qualitative sanity check is performed (on BERT for text classification), and no baseline is provided. In fact, Basu, Pope, and Feizi (2020) further discover that influence functions can become fragile in the age of deep neural networks. The approximation accuracy can vary significantly depending on a variety of factors: "the network architecture, its depth and width, the extent of model parameterization and regularization techniques, and the examined test points". The findings call for increased caution on the consequences of the assumption as models become more complex.</p>
<p>Intervening in model representations. Similar to input intervention, we also summarize model representation intervention methods according to the target and the operation. Here, the target can be broadly categorized into neurons and feature representations. The operation can still be erasure and perturbation.</p>
<p>Similar to the case of intervening in inputs, we will first classify existing work based on the target, and then on the operation:</p>
<p>(a) Neuron-targeted intervention. By intervening in individual neurons in the neural network, one can explain the importance of each neuron to the prediction. The intervention can still be either erasure or perturbation.</p>
<p>The simplest form of erasure is still leave-one-out. Using the same strategy as with input features, Li, Monroe, and Jurafsky (2017) study the effect of leaving out a single dimension in hidden units on the log-likelihood of the gold label. In text classification tasks, they find that compared to the input layer, the importance in higher layers is distributed more equally across dimensions. In other words, there are no particular dimensions that are strikingly more important than others, which might make the final prediction more robust against changes in any of them. Bau et al. (2019) adapt the method for Machine Translation models. Instead of erasing each hidden unit sequentially, they search for important neurons in a guided fashion. According to their ranking, it is further validated that more important neurons have a larger impact on translation quality than less important ones, as shown by masking out their activations during the inference.</p>
<p>Besides erasure, perturbation is another form of neuron-targeted intervention. One representative example is causal mediation analysis (Vig et al. 2020), which measures how a control variable influences a response variable through the mediation by an intermediate variable, or mediator. Using the example from Pearl (2001), consider a scenario where we want to study the effects of a drug (control variable) on a disease (response variable). However, the drug has certain side effects, causing the patient to take aspirin (mediator), which in turn has an effect on the disease. In machine learning, we can think of the input example as the control variable, the model output as the response variable, and an internal neuron as the mediator. Vig et al. (2020) use this framework to analyze gender bias in LMs. Given a prompt p such as The nurse said that, they compute the conditional probability of he versus she as the next generated token, and consider the ratio y = P (he|p) P (she|p) as a measurement of gender bias. Now, the goal is to study the effect of each individual neuron as the mediator. Specifically, they first perform a change on the input prompt (e.g., The nurse said that → The man said that). Then, they measure three types of effect of the input change x on the output y, mediated by some neuron z. Figure 13 provides an illustration:</p>
<p>(i) total effect (of x on y), i.e., the change in y resulting from the input change x;</p>
<p>(ii) direct effect (of x on y without passing through z), i.e., the change in y resulting from the input change x, but holding the mediator z constant;</p>
<p>(iii) indirect effect (of x on y only through z), i.e., the change in y by only setting z to its value after the input change x, while holding all other neurons constant.</p>
<p>Intuitively, (iii) can represent the contribution of individual neurons to the prediction. Through their case study on GPT-2, the authors find that gender bias is concentrated in a relatively small proportion of neurons, especially in the middle layers. No empirical evaluation on faithful evaluation is reported though.</p>
<p>(b) Feature-representation-targeted intervention. Beyond intervening in neurons, directly targeting feature representations in the model allows us to answer more insightful questions like "is some high-level feature, e.g., syntax tree, used in prediction?". This is particularly meaningful to the line of work on what knowledge a model encodes (Section 1.1.1), which oftentimes discovers linguistic features in the model, but it is unclear whether they are indeed used by the model.</p>
<p>As before, the most intuitive way to perform an intervention on feature representation is erasure. Two pieces of concurrent work, Amnesic Probing  and CausalLM (Feder et al. 2021), are representative examples. They both aim to answer the following question: is a certain feature (e.g., POS, dependency tree, constituency boundary, gender, race, ...) used by the model on a task (e.g., language modeling, sentiment classification, ...)? To answer the question, they exploit different algorithms to remove the target feature from the model representation, via either linear projection (Ravfogel et al. 2020) or adversarial training. Then, with the new representation, they measure the change in the prediction. The larger the change, the more strongly it indicates that the feature has been used by the original model. In terms of Faithfulness, only CausalLM is validated with a white-box evaluation, whereas no explicit evaluation is provided for Amnesic Probing.</p>
<p>Similar to input erasure, a major problem with feature representation erasure lies in unrealistic representations. For instance, since syntax is such a fundamental component of language, what does it mean if an LM is entirely ignorant of syntax? Is it still an LM at all?</p>
<p>To address this issue, perturbation-based methods targeting feature representations are proposed. Ravfogel et al. (2021) introduce AlterRep, an algorithm to manipulate the target feature value in model representations. Specifically, they investigate the task of subject-verb agreement prediction. For example, given the sentence The man that they see [MASK] here.</p>
<p>with an embedded relative clause (that they see), the correct verb to fill in the mask should be is as opposed to are, since it should agree with man. They now ask the question: does the model use syntactic information (e.g., the relative clause boundary) in making such predictions? To answer this, they first probe for syntactic knowledge in the model representation. If the model "thinks" that the [MASK] token is outside the relative clause (factual), AlterRep would flip this knowledge via linear projections, such that [MASK] is now inside the relative clause according to the model (counterfactual). Now, the probability ratio of are versus is increases significantly, as the model is potentially tempted to associate the verb with they in the relative clause. Such findings suggest that syntactic information is indeed used in predicting the verb. Tucker, Qian, and Levy (2021) study the same task and feature, but improve the method by providing syntactically ambiguous contexts, e.g., Therefore, [MASK] can be either singular or plural. Likewise, they first probe for the model's syntactic representation, and then flip the structure of the syntax tree from one of the above interpretations to the other. The way they do the flipping is a gradient-based algorithm informed by trained probes. Similar findings are reported as in the AlterRep paper, suggesting BERT-based models are using syntax in agreement prediction. However, no extrinsic evaluation of Faithfulness is provided.</p>
<p>Tools. The following tools implement some type(s) of counterfactual intervention: Captum 31 , LIT 32 (Tenney et al. 2020), LIME 33 (Ribeiro, Singh, and Guestrin 2016), SHAP 34 (Lundberg and Lee 2017), Anchors 35 (Ribeiro, Singh, and Guestrin 2018), Seq2Seq-Vis 36 (Strobelt et al. 2019), and the What-if Tool 37 ). (a) Counterfactual intervention has its root in the causality literature, and is therefore designed to capture causal instead of mere correlational effects between inputs and outputs.</p>
<p>Advantages.</p>
<p>(b) Compared to other methods, counterfactual intervention methods are more often explicitly evaluated in terms of Faithfulness (e.g., Ribeiro, Singh, and Guestrin 2018;De Cao et al. 2020;Ribeiro, Singh, and Guestrin 2016;Lundberg and Lee 2017;Tsang, Rambhatla, and Liu 2020;Feder et al. 2021), mostly with predictive power or white-box tests.</p>
<p>Disadvantages.</p>
<p>(a) Compared to other methods, counterfactual intervention is relatively more expensive in computational cost, normally requiring multiple forward passes/modifications to the model representation. Searching for the right targets to intervene can also be costly.</p>
<p>(b) As mentioned before, erasure-based intervention can result in nonsensical inputs, which sometimes allow adversaries to manipulate the explanation (Slack et al. 2020).</p>
<p>(c) Intervening in a single feature relies on the assumption that features are independent. Consider the sentence This movie is mediocre, maybe even bad (Wallace, Gardner, and Singh 2020). If we mask out mediocre or bad individually, the predicted sentiment will probably not change much (still negative). Hence, an explanation method that relies on single feature erasure might report that neither token is important for the model prediction. However, it does capture feature interactions, like the OR relationship between mediocre or bad here -as long as one of them is present, the sentiment is likely negative. More examples are provided in .</p>
<p>(d) Interventions are often overly specific to the particular example (Wallace, Gardner, and Singh 2020). This calls for more insights into the scale of such explanations (i.e., if we discover a problem, is it only about one example or a bigger issue?) and general takeaways (i.e., what do we know about the model from this explanation?).</p>
<p>(e) Counterfactual intervention may suffer from hindsight bias (De Cao et al. 2020), which questions the foundation of counterfactual reasoning. Specifically, the fact that a feature can be dropped without influencing the prediction does not mean that the model "knows" that it can be dropped and has used it in the original prediction. For instance, consider a synthetic counting task where a model should decide if there are more 1's than 8's in the input digits. Now, suppose the input contains four 1's, two 8's, and three 5's. Say we have a model with perfect accuracy, which correctly predicts TRUE. Through counterfactual intervention, we can either drop a 1 , all of the 8's, or all of the 5's, without affecting the prediction. Does that mean that all these digits should be assigned zero importance? De Cao et al. (2020) argue that a more reasonable picture should be that all 1's and 8's have uniform importance, and all 5's have zero. In the context of NLP, consider the Reading Comprehension task, where a model is given a context and a question, and should identify an answer span in the context. Now, using counterfactual intervention, if we mask out everything except the answer in the context, the model will for sure predict the gold span. Nonetheless, this does not imply that everything else is unimportant for the model's original prediction. This again calls for our attention to the fundamental mechanism of counterfactual reasoning.</p>
<p>2.6 Self-Explanatory Models 2.6.1 Overview. In contrast with all the above post-hoc methods, self-explanatory models provide builtin explanations. Typically, explanations can be in the form of feature importance scores, natural language, causal graphs, or the network architecture itself.</p>
<p>Past Work.</p>
<p>Prior work on self-explanatory models can be broadly categorized into two lines based on how the explanation is formed: explainable architecture and generating explanations. The former relies on the transparent model architecture, such that no extra explanation is necessary. The latter, though, may still involve opaque architectures, but generate explicit explanations as a byproduct of the inference process. The rest of the subsection will elaborate on each category.</p>
<p>Explainable architecture. While end-to-end NNs are a black-box, classic machine learning models like Decision Tree and linear regression have a highly interpretable reasoning mechanism. Drawing inspiration from them, researchers attempt to design neural models with more structural transparency while maintaining their performance.</p>
<p>(a) Neural Module Networks (NMN) is one representative, specifically in the context of Question Answering (QA) tasks. Given a complex question (e.g., Are there more donuts than bagels in the image?), humans naturally decompose it into a sequence of steps (e.g., look for donuts and bagels, count them, and compare the counts). Motivated by the same idea, NMNs parse the input question into a program of learnable modules (e.g, compare(count(donuts), count(bagels))), which is then executed to derive the answer.</p>
<p>There are three potentially learnable components in NMNs: (i) the question parser (question → dependency tree), (ii) the network layout predictor (dependency tree → program), and (iii) the module parameters (program → answer). Previous studies differ in whether and how each component is learned. Andreas et al. (2016b) introduce the earliest version of NMN, where only module parameters are learned and the other two components are pretrained or deterministic. In a follow-up study (Andreas et al. 2016a), they extend the framework to also jointly learn the network layout specific to each question, which is then named Dynamic Neural Module Network (DNMN). Hu et al. (2017) further propose to learn the question parser as well, resulting in their Endto-End Module Network (N2NMN).</p>
<p>The above methods prove effective on various visual QA tasks (including VQA (Antol et al. 2015), SHAPES (Andreas et al. 2016b), GeoQA (Krishnamurthy and Kollar 2013), and CLEVR (Johnson et al. 2017)), most of which are based on synthetic data, though. Recently, more studies investigate the application of NMNs on realistic data, especially in the language-only domain. Jiang et al. (2019) apply NMN to HotpotQA ), a QA dataset involving reasoning across multiple documents. However, their model is incapable of symbolic reasoning, such as counting and sorting, so it can only solve questions with directly retrievable answers in the context. To address this issue, Gupta et al. (2019) introduce a set of modules for each symbolic operation, e.g., count, find-max-num, compare-num. Their model is capable of answering questions involving discrete reasoning in the DROP dataset (Dua et al. 2019), for example, Who kicked the longest field goal in the second quarter?, given a long description of the match. See Figure 14 for an illustration. Despite their presumably transparent structure, there are two main problems with NMNs: (i) The modules' actual behavior may not be faithful to their intended function. Most NMNs pre-define modules only in terms of input/output interface. Their actual behavior -module parameters -are then typically learned from end-to-end supervision, i.e. only the question, context, and the final answer. Thus, there is no control over the intermediate output of individual modules. Consider our previous example compare(count(donuts), count(bagels)). Though we name the module compare, it may not perform the comparison function. Theoretically, it is possible that compare alone outputs the final answer, whereas count is ignored. Subramanian et al. (2020) empirically confirm such failure cases and experiment with several remedies, such as introducing intermediate supervision and limiting the module complexity. Curiously, the improved Faithfulness comes at the cost of end task accuracy. (ii) Symbolic modules may not be expressive enough for the flexible semantics of natural language. For example, Gupta et al. (2019) note that questions like Which quarterback threw the most touchdown passes? would necessitate modules with some key-value representation ({quarterback: count}), which are non-obvious to design. Other subtle semantic phenomena like context-conditional parsing and coreference pose similar challenges.</p>
<p>Nonetheless, the compositional nature of NMNs brings about promising research opportunities. In particular, it is possible to pretrain modules independently on other tasks. This allows us to ensure their Faithfulness as well as exploit transferable knowledge.</p>
<p>(b) Neural-Symbolic Models (NSM) are a more generic concept, loosely defined as neural models that integrate symbolic reasoning. This clearly includes NMNs, but here we will discuss the rest, i.e., methods that do not involve learnable neural modules. Yi et al. (2018) introduce Neural-Symbolic VQA (NS-VQA) for answering questions based on synthetic images. As shown in Figure 15, it has three components: a scene parser, which derives a structured representation of the image, i.e., a database of object attributes; a question parser, which parses the question into a program; and a program executor, which runs the program on the database to obtain the answer. Note that, unlike NMNs, here the program is composed of modules with fixed parameters, resulting in fully deterministic behavior. However, the model requires intermediate supervision on both the scene parser and the question parser, which limits its application to synthetic data only.  2018)). Given an image and a question, the scene parser converts the image to a structured representation; the question parser converts the image into a program; the program executor runs the program on the structured representation to obtain the answer.  2021)). Starting from the leaves, the tree grows recursively upwards to reach answer.</p>
<p>Mao and Gan (2019) extend NS-VQA by proposing Neuro-Symbolic Concept Learner (NS-CL), which requires no such intermediate supervision. It jointly learns visual scene representation of the question parse from (image, question, answer) triples directly. The scene is no longer converted to an explicit database, but has a probabilistic representation instead. This allows the model to be end-to-end differentiable and therefore applicable to real-world data as well.  (2020)): predict-then-explain (A), explain-then-predict (B), and jointly-predict-and-explain (no particular dependency between explainer and predictor, thus not visualized). Bogin et al. (2021) point out another weakness in existing NSMs concerning the question parser. To parse a question into a program, one needs to consider the enormous search space of possible programs. Most methods address this issue by techniques like reinforcement learning, which is not differentiable. In their model Grounded Latent Trees (GLT), the authors propose to parse the question into a latent tree, where each node corresponds to a span in the question. As shown in Figure 16, each span has a denotation (a symbolic set of objects it refers to in the image) and a representation (a continuous vector embedding of it). Starting from the leaves, the tree grows bottomup, where each intermediate node is computed from its children. For example, the denotation of the small matte object is the intersection of the small and matte object. Finally, the denotation of the root node is the answer. During training, there is no supervision on the tree structure, but the model is capable of constructing valid trees only from end-to-end supervision.</p>
<p>(c) Models with constraints are the third family of explainable architectures. The idea is to incorporate constraints into neural networks from classic interpretable models, like generalized linear regression (Alvarez Melis and Jaakkola 2018) and finite-state automata (Schwartz, Thomson, and Smith 2018;Deutsch, Upadhyay, and Roth 2019;Jiang et al. 2020). Still, a major challenge lies in the trade-off between interpretability and performance.</p>
<p>Generating explanations. Besides using architecture as an implicit explanation, another type of self-explanatory models generate explicit explanations as an additional task with the original prediction. For supervision, human-written explanations are often used as the training signal. According to the dependency of the predictor and the explainer, we can broadly classify existing work into three categories: predict-thenexplain, explain-then-predict, and jointly-predict-and-explain. See Figure 17 for a comparison.</p>
<p>(a) Predict-then-explain models first make a prediction with a standard blackbox predictor, and then justify the prediction with an explainer ( Figure 17A). This is analogous to previous post-hoc explanation methods (from Section 2.2 to 2.5). This framework has been applied to many domains, including vision (Hendricks et al. 2016), language (Camburu et al. 2018), multimodal tasks (Park et al. 2018), self-driving cars , etc. However, it suffers from the same Faithfulness challenge as all other post-hoc methods: since the predictor does not depend on the explainer, there is no guarantee that the explanation accurately reflects the reasoning process behind the prediction. Moreover, as the supervision comes from human-provided explanations, the explainer is only explicitly optimized in terms of Plausibility, but not Faithfulness.</p>
<p>(b) Explain-then-predict methods are then introduced in response to this issue. In this framework, the explainer first generates an explanation, which is then provided as the only input to the predictor. In other words, the predictor can only access the explanation, but not the original input example. The intuition is that the prediction can only be made based on the explanation, which renders the predictor "faithful by construction".</p>
<p>Methods within this framework differ in the form of explanation, which is typically either an extract from the input or natural language.</p>
<p>The former (an extract from the input as the explanation) is also known as rationalebased methods, where a rationale is defined as a part of the input that is short yet sufficient for the prediction (Zaidan, Eisner, and Piatko 2007). 38 For example, in sentiment classification, seeing the phrase not good is probably enough for predicting negative. The job of the explainer is to extract such a rationale, and thus it is also called extractor in this scenario. One major difficulty lies in how to effectively find the rationale span, given the formidably large search space. Lei, Barzilay, and Jaakkola (2016) proposes to guide the search with reinforcement learning. Bastings, Aziz, and Titov (2019) introduces a re-parameterization technique as an alternative, which makes the learning differentiable. Jain et al. (2020) discards searching and directly obtain candidate rationales from existing post-hoc explanation methods (e.g., backpropagation-based ones) instead. Although seemingly "faithful by construction", there are some problems with rationale-based methods: (i) Clearly, only the rationale is used in the prediction, but this does not tell us anything about how it is used. For example, it is possible that the predictor only looks at the pattern of the rationale (e.g., the number of tokens that are retained) and makes a prediction based on this cue.  confirms the possibility of these so-called "trojan" explanations in practice. (ii) Similarly, the framework does not tell us why a rationale is selected. (iii) Finally, the validity of rationales is highly task-specific. For instance, it might make sense to classify the sentiment only based on a text fragment, but what about tasks that intrinsically require full sentence(s), such as Natural Language Inference (NLI)?</p>
<p>The latter (natural language as the explanation) generates explanations in natural language, more flexible than an extracted rationale from the input. Consider the NLI task as an example. Given a hypothesis (e.g., An adult dressed in black holds a stick) and a premise as input (e.g., An adult is walking away, empty-handed), the explainer should first generate an explanation (Holds a stick implies using hands so it is not empty-handed), and then the predictor should make a prediction (Contradiction) only based on the explanation. When experimenting with this model on the SNLI dataset (Bowman et al. 2015), Camburu et al. (2018) discover a trade-off between the task accuracy and the Plausibility of the explanation. It is also found that the model can generate inconsistent explanations (Camburu et al. 2020), e.g., Dogs are animals and Dogs are not animals.</p>
<p>Moreover, the explanation might contain cues to the label, e.g., patterns like X implies Y / X is a type of Y oftentimes indicate Entailment, while X is not the same as Y is a strong signal of Contradiction. To overcome this issue, Kumar and Talukdar (2020) propose the Natural language Inference over Label-specific Explanations (NILE) model, where every class label has its own an explainer. Given an input, an explanation is generated for each label. Then, all three explanations are fed to the predictor, which makes a decision after comparing them. This precludes the possibility of the predictor exploiting cues in the explanation pattern. NILE is shown to have comparable accuracy with SOTA models on SNLI, as well as better transferability to OOD datasets. Through an extensive evaluation, the authors compare the Faithfulness of a few variants of NILE. Also, they argue that perturbation-based metrics (DeYoung et al. 2020) can sometimes be misleading, and task-specific metrics should be designed.</p>
<p>(c) Jointly-predict-and-explain methods have two possible structures: (i) there are still an explainer and a predictor, but the predictor can access both the explanation and the input example. 39 (ii) there are no separate explainer and predictor at all -everything is produced jointly.</p>
<p>Among them, (ii) is more straightforward in structure. Given the input example as the prompt, a generation model outputs a continuation including both the explanation and the prediction in some designated order. This is analogous to any ordinary generation task. Existing studies along this line differ in the choice of the generation model and the end task. For example, Ling et al. (2017) use LSTMs to solve algebraic problems and provide intermediate steps, and Narang et al. (2020) rely on T5 to generate predictions with justifications for NLI, sentiment classification, and QA. Here, we will focus on a series of work on generating structured proofs for deductive reasoning. This is particularly interesting since most previous work only aims at generating single-step explanations (e.g., a single sentence), but complex tasks would require a structured reasoning chain as explanations (e.g., the classic syllogism -from all humans are mortal and Aristotle is a human, we can conclude that Aristotle is mortal). As one of the earliest studies of this kind, Tafjord, Dalvi, and Clark (2021) develop ProofWriter, which takes in a set of premises and a hypothesis, and then decides if the hypothesis is true as well as a structured proof. The method obtains SOTA performance on the synthetic RuleTaker dataset (Clark, Tafjord, and Richardson 2020). Specifically, there are two versions of ProofWriter, all-at-once (generating the entire proof in one shot) and iterative (generating one step at a time), with different strengths and weaknesses. The all-at-once ProofWriter cannot guarantee that the proof is faithful, since it may not "believe" in the proof that is generated. Consider the following example: 40 Premises: S1. Nails are made of iron. S2. If something is iron then it is metal. S3. If something is made of metal then it can conduct electricity. Hypothesis: S4. Nails conduct electricity.</p>
<p>Suppose ProofWriter generates the correct answer and a reasoning chain as proof, all in one shot:</p>
<p>39 In contrast, the above-mentioned explain-then-predict methods do not allow the predictor to access the input example. This is why we categorize (i) as jointly-predict-and-explain methods. Now, if we isolate the first step in the proof (S1 &amp; S2 → "Nails are made of metal"), we can feed it back to the ProofWriter to verify it: Premises: S1. Nails are made of iron. S2. If something is iron then it is metal. Hypothesis: S5. Nails are made of metal.</p>
<p>There is no guarantee that ProofWriter will still output TRUE, meaning that the step may not necessarily be verified. In other words, the proof might not faithfully reflect how the model arrives at the answer. In fact, in a similar verification experiment as exemplified above, the authors find that for proofs within the depths seen during training, almost all correct steps can be verified; however, when it comes to greater depths, the percentage drops rapidly. This confirms that the all-at-once ProofWriter is not always faithful.</p>
<p>The iterative ProofWriter does not have this issue, since it derives the proof one step at a time. All steps are already verified during generation. Therefore, it is faithful by construction. However, compared to the all-at-once version, it suffers from bottlenecks in efficiency and input lengths limit.</p>
<p>After ProofWriter, Dalvi et al. (2021) generalize the idea to real-world data as well, developing the EntailmentWriter. Nevertheless, only an all-at-once version is implemented, indicating that the same Faithfulness risk exists.</p>
<p>As opposed to generating everything jointly, work in (i) still has an explainer and a predictor separately, but there is no particular constraint on input access. For example, Rajani et al. (2019) introduce a QA model that takes in a question, generates an explanation first, and then produces an answer based on both the question and the explanation. Another example is a variant of NILE (Kumar and Talukdar 2020) previously mentioned in (b), which allows the predictor to look at both the premise-hypothesis pair and the explanations. These methods have the same Faithfulness issue, since the predictor can make its decision based on the input only using whatever reasoning mechanism, while totally ignoring the explanation.</p>
<p>Advantages.</p>
<p>(a) By definition, self-explanatory methods provide built-in explanations, so there is no need for post-hoc explanations.</p>
<p>(b) The form of explanation is flexible, e.g., model architecture, input features, natural language, or causal graphs.</p>
<p>(c) It is possible to supervise the explainer with human-provided explanations. This is helpful for learning more plausible explanations, as well as encouraging the model to rely on desired human-like reasoning mechanisms instead of spurious cues.</p>
<p>(d) Certain self-explanatory models, e.g., the iterative ProofWriter , are faithful by construction (we should be extra cautious about this claim, though).</p>
<p>Disadvantages.</p>
<p>(a) Many self-explanatory models cannot guarantee Faithfulness, e.g., Neural Module Networks without intermediate supervision, predict-then-explain models, rationale-based explain-then-predict models, and certain jointly-predict-and-explain models.</p>
<p>(b) There is often an observed trade-off between performance and interpretability in self-explanatory models . Faithfulness can come at the cost of task accuracy.</p>
<p>(c) Large-scale human supervision on explanations can be costly and noisy . Also, it is hard to automatically evaluate the quality of model-generated explanations given the reference human explanations, since there can be multiple ways to explain a prediction.</p>
<p>Summary and Discussion</p>
<p>In this section, we summarize the five methods above by discussing their common virtues and challenges, as well as identify future work directions in model interpretability.</p>
<p>Virtues</p>
<p>Recent advances in model interpretability exhibit the following virtues:</p>
<p>(a) They are conducive to bridging the gap between competence and performance in language models. The two terms originate from linguistics: competence describes humans' (unconscious) knowledge of a language, whereas performance refers to their actual use of the knowledge (Chomsky 1965). For humans, there is a gap between competence and performance, e.g., we can theoretically utter a sentence with infinitely many embedded clauses, but in practice, we never do so. Similarly, for language models, what they know can be different from what they use (in a task), as discussed in Section 1.1.1. Previous work on interpretability predominantly focuses on competence, whereas more recent studies (e.g., all five methods discussed in this survey) aim at answering the performance question. This allows us to better understand whether the same gap exists in models, and if so, how we can bridge it.</p>
<p>(b) There has been increasing awareness of Faithfulness and other principles of model explanation methods, especially since the seminal opinion piece by Jacovi and Goldberg (2020). A number of evaluation methods have been proposed; see Section 1.1.4 for details. Though each of them depends on assumptions and application scenarios, this is a good starting point for quantitatively assessing the quality of explanations.</p>
<p>(c) Explanations produced by most above-mentioned methods above are intuitive to understand, even for lay people. This is because the form of explanation is simple, mostly feature importance scores, visualization, natural language, or causal graphs. Though the model and the explanation method may be opaque, the explanation itself is easily understandable.</p>
<p>(d) There are a plethora of explanation methods that are model-agnostic, especially for classification tasks.</p>
<p>(e) Many studies draw insights from work in vision and develop adaptable methods in language. See Section 2.4 for more details.</p>
<p>(f) Numerous toolkits have been developed to help users apply explanation methods to their own models. See Section 2.3, 2.4, and 2.5 for more details.</p>
<p>Challenges and Future Work</p>
<p>Despite the remarkable advances, the area of model interpretability still faces several major challenges, which also provide exciting future work opportunities:</p>
<p>(a) A large number of explanation methods still lack objective quality evaluation, especially in terms of Faithfulness. There has not been any established standard on how to measure Faithfulness. =⇒ We need a universal evaluation framework, which is fundamental to measuring the progress of any research in this area.</p>
<p>(b) Most existing methods provide explanations in terms of surface-level features, e.g., pixels in vision and tokens in language. =⇒ Future work should explore how to capture the contribution of higher-level features in a task, including linguistic (case, gender, part-of-speech, semantic role, syntax dependency, coreference, discourse relations, ...), and extra-linguistic (demographic features, commonsense and world knowledge, ...) ones. Several studies on counterfactual intervention provide inspiring examples (Ravfogel et al. 2020;Elazar et al. 2021;Tucker, Qian, and Levy 2021); see Section 2.5 for details.</p>
<p>(c) Most existing methods capture the contribution of individual features to the prediction, but not that of higher-order feature interactions. See Section 2.5.4 (c) for an example. =⇒ Future work can develop more flexible forms of explanation instead of flat importance scores, e.g., feature subsets as in certain counterfactual intervention methods (Ribeiro, Singh, and Guestrin 2018) and causal graphs as in several selfexplanatory methods Dalvi et al. 2021).</p>
<p>(d) Existing work mostly focuses on limited task formats, e.g., classification and span identification. This limits their downstream applicability to real-world scenarios. =⇒ Future work can study alternative task formats such as language generation and structured prediction, or even better, develop explanation methods that are generalizable across tasks.</p>
<p>(d) It is not always obvious whether insights from model explanations are actionable. For example, given the explanation of the model's decision on one test example, the user finds that the model is not using the desired features. Then how should they go about fixing it -through the data, model architecture, training procedure, hyperparameters, or something else? How does the user communicate with the model? =⇒ Interactive explanations will be a fruitful area for future study. Several recent studies on knowledge editing have shown the plausibility of the idea (Madaan et al. 2021;Kassner et al. 2021).</p>
<p>(e) There has been a tension between model performance and interpretability. This issue is especially evident in self-explanatory models; see Section 2.6 for more details. =⇒ It will be greatly helpful to have a theoretical understanding of whether the tension is intrinsic or avoidable. In the meantime, we need to cautiously balance between performance and interpretability depending on application scenarios.</p>
<p>Conclusion</p>
<p>This survey provides an extensive tour of recent advances in NLP explainability, through the lens of Faithfulness. Despite being a fundamental principle of model ex-planation methods, Faithfulness does not have a well-established technical definition or evaluation framework. As a result, it is difficult to compare different methods in terms of Faithfulness, and many of them do not report it by any means at all.</p>
<p>We present a critical review of five categories of existing model explanation methods: similarity methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. We introduce each category in terms of their representative work, advantages, and disadvantages, with a special focus on Faithfulness. Then, we summarize all methods by discussing their common virtues and challenges and outline a few directions for future research. We hope that this survey provides an overview of the area for researchers interested in interpretability, as well as users aiming at better understanding their own models.</p>
<p>Figure 2 :
2t-SNE visualization on latent representations for modifications and negations (figure from Li et al. (2016)).</p>
<p>Figure 4 :
4A BERT sentiment classification model (left, adapted from(Devlin et al. 2019</p>
<p>are binary classification, which leaves a vast amount of freedom in model (a) Attention from he.(b) Attention from she.</p>
<p>Figure 5 :
5An attention head in GPT-2 that captures coreference (figure from Vig</p>
<p>Figure 6 :
6An attention head in BERT where direct objects attend to their verbs, with 86.8% accuracy (figure from Clark et al. (2019)). The direct objects of interest are highlighted in red.</p>
<p>(c) Though attention correlates poorly with other feature importance measures, it fares well on human evaluation. (This is, however, orthogonal to Faithfulness.) Pruthi et al. (2020) again refutes argument (b), showing that with a simple training objective, they successfully guide the model to learn intended adversarial attention Figure 7: A sentiment analysis model's attention distribution α over words in a negative movie review (figure from Jain and Wallace</p>
<p>Figure 8 :
8A visualization of different gradient methods on qualitative image classification examples (figure adapted from(Smilkov et al. 2017)). Brighter shades indicates higher relevance for the prediction.</p>
<p>Figure 10 :
10A schematic visualization of propagation methods. This figure is adapted from (Montavon et al. 2019).</p>
<p>the relevance of any input feature x i . In other words, r i (x) = R (1) i . Unlike gradient methods, propagation methods do not have a closed-form expression for r i (x). Instead, they start with the output M (x), which is considered the toplevel relevance, R from layer L to L − 1 based on layerspecific rules, such that each neuron in L − 1 receives a proportion of it. This process then proceeds layer by layer. Between any two adjacent layers l and l + 1, a generic function D() determines how R(l+1) j (the relevance of any neuron j in l + 1) is recursively distributed to R (l) i (the relevance of any neuron i in l), where i and j are connected. Formally,</p>
<p>3 :)
3Summary of different propagation methods in terms of how they define the recursive function D(), as in R . 23 Simple Gradients from gradient methods is included for comparison. See Section 2.4.2 -Propagation methods for details on notations.</p>
<p>Figure 11 :
11A visualization of DeconvNet and Guided BackPropagation, in comparison with Simple Gradients, on an example cat image (figure adapted from(Springenberg et al. 2015)).</p>
<p>Figure 13 :
13An illustration of causal mediation analysis (figure from Vig et al. (</p>
<p>I saw the boy and the girl [MASK] tall. which can be interpreted as either [I saw the boy] and [the girl [MASK] tall]. or I saw [the boy and the girl [MASK] tall].</p>
<p>Figure 14 :
14An illustration of Neural Module Network on the QA task (figure fromGupta et al. (2019)). Given a paragraph of context and a question, the model parses the question into a program of learnable modules, which is then executed on the context to derive the answer. Colors represent the correspondence between spans in the question, the modules, and the intermediate outputs in the context.</p>
<p>Figure 15 :
15An illustration of the Neural-Symbolic VQA (NS-VQA) model (figure from Yi et al. (</p>
<p>Figure 16 :
16An illustration of the Grounded Latent Trees (GLT) model (figure from Bogin et al. (</p>
<p>Figure 17 :
17A comparison of the frameworks of generating explanations (figure adapted from Kumar and Talukdar</p>
<p>40 Example from Peter Clark's talk at Penn's NLP seminar in Feb 2022. Answer: TRUE Proof: S1 &amp; S2 → "Nails are made of metal." &amp; S3 → S4</p>
<p>Table 1 :
1Comparison of different model explanation methods in terms of their properties. Different colors denote different values of a property. See Section 1.1.3 for details.</p>
<p>Table 2 :
2Summary of different gradient methods in terms of how they compute r i (x), the relevance of feature x i . See Section 2.4.2 -Gradient methods for details on notations.</p>
<p>Table</p>
<p>31 https://captum.ai 32 https://pair-code.github.io/lit 33 https://github.com/marcotcr/lime 34 https://github.com/slundberg/shap 35 https://github.com/marcotcr/anchor 36 https://seq2seq-vis.io 37 https://pair-code.github.io/what-if-tool
Despite their subtle distinctions in some previous literature, we use the terms interchangeably.
Given a premise P and a hypothesis H, the textual entailment task is to determine if P entails H (whenever P is true, is H also true?). For example, he can run fast entails he can run.
Prior work has different definitions of "concepts", including but not limited to phrases(Rajagopal et al. 2021) and high-level features.
Note that the target audience is not included in the table, since it largely depends on the task and the specific instance of the method. 5 We will elaborate on attention in Section 2.3.
See Section 2.4.2 -Propagation Methods for more details on "total importance".
Please refer back to Section 1.1.4 for details.
cf. Section 1.1.3 9 For discrete inputs like tokens, the slight change is defined in terms of similarity metrics in the embedding vector space.
The attention weights are computed by a compatibility function, which assigns a score to each pair of input features indicating how strongly they should attend to each other. Usually, it is instantiated as a re-scaled dot product. Read more in https://towardsdatascience.com/ deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1.
including backpropagation-based methods and counterfactual intervention in later sections of the survey.
For example, a lot of tokens attend most to[SEP], a special token to mark the sentence boundary. 18 https://github.com/jessevig/bertviz 19 https://pair-code.github.io/lit/
Of course, it is questionable if this noise comes from the deficiency of the explanation method or the model reasoning mechanism itself.
This is analogous to the concept of "anchors" mentioned in Section 2.5.
-3367, Association for Computational Linguistics, Hong Kong, China.</p>
<p>Sanity Checks for Saliency Maps. Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim, Advances in Neural Information Processing Systems. Curran Associates, Inc31Adebayo, Julius, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. 2018. Sanity Checks for Saliency Maps. In Advances in Neural Information Processing Systems, volume 31, Curran Associates, Inc.</p>
<p>Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks. Yossi Adi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, Yoav Goldberg, arXiv:1608.04207csAdi, Yossi, Einat Kermany, Yonatan Belinkov, Ofer Lavi, and Yoav Goldberg. 2017. Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks. arXiv:1608.04207 [cs].</p>
<p>Towards Robust Interpretability with Self-Explaining Neural Networks. Alvarez Melis, David , Tommi Jaakkola, Advances in Neural Information Processing Systems. Curran Associates, Inc31Alvarez Melis, David and Tommi Jaakkola. 2018. Towards Robust Interpretability with Self-Explaining Neural Networks. In Advances in Neural Information Processing Systems, volume 31, Curran Associates, Inc.</p>
<p>Alvarez-Melis, Tommi S David, Jaakkola, arXiv:1806.08049ArXiv: 1806.08049On the Robustness of Interpretability Methods. cs, statAlvarez-Melis, David and Tommi S. Jaakkola. 2018. On the Robustness of Interpretability Methods. arXiv:1806.08049 [cs, stat]. ArXiv: 1806.08049.</p>
<p>Learning to Compose Neural Networks for Question Answering. Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsAndreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016a. Learning to Compose Neural Networks for Question Answering. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1545-1554, Association for Computational Linguistics, San Diego, California.</p>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Dan Klein, Neural Module Networks. Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016b. Neural Module Networks. pages 39-48.</p>
<p>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, Devi Parikh, VQA: Visual Question Answering. Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. pages 2425-2433.</p>
<p>Explaining Predictions of Non-Linear Classifiers in NLP. Leila Arras, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek, Proceedings of the 1st Workshop on Representation Learning for NLP. the 1st Workshop on Representation Learning for NLPBerlin, GermanyAssociation for Computational LinguisticsArras, Leila, Franziska Horn, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2016. Explaining Predictions of Non-Linear Classifiers in NLP. In Proceedings of the 1st Workshop on Representation Learning for NLP, pages 1-7, Association for Computational Linguistics, Berlin, Germany.</p>
<p>Explaining Recurrent Neural Network Predictions in Sentiment Analysis. Leila Arras, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek, Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media AnalysisCopenhagen, DenmarkAssociation for Computational LinguisticsArras, Leila, Grégoire Montavon, Klaus-Robert Müller, and Wojciech Samek. 2017. Explaining Recurrent Neural Network Predictions in Sentiment Analysis. In Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 159-168, Association for Computational Linguistics, Copenhagen, Denmark.</p>
<p>On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). Online. Bach, Sebastian, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samekthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Publisher: Public Library of Science10130140A Diagnostic Study of Explainability Techniques for Text ClassificationAtanasova, Pepa, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. A Diagnostic Study of Explainability Techniques for Text Classification. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3256-3274, Association for Computational Linguistics, Online. Bach, Sebastian, Alexander Binder, Grégoire Montavon, Frederick Klauschen, Klaus-Robert Müller, and Wojciech Samek. 2015. On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation. PLOS ONE, 10(7):e0130140. Publisher: Public Library of Science.</p>
<p>How to Explain Individual Classification Decisions. David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja Hansen, Journal of Machine Learning Research. 29Baehrens, David, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, and Katja Hansen. 2010. How to Explain Individual Classification Decisions. Journal of Machine Learning Research, page 29.</p>
<p>Neural Machine Translation by Jointly Learning to Align and Translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.</p>
<p>Barredo Arrieta, Alejandro , Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, Francisco Herrera, Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion. 58Barredo Arrieta, Alejandro, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information Fusion, 58:82-115.</p>
<p>Interpretable Neural Predictions with Differentiable Binary Variables. Jasmijn Bastings, Wilker Aziz, Ivan Titov, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsBastings, Jasmijn, Wilker Aziz, and Ivan Titov. 2019. Interpretable Neural Predictions with Differentiable Binary Variables. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2963-2977, Association for Computational Linguistics, Florence, Italy.</p>
<p>The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?. Jasmijn Bastings, Katja Filippova, arXiv:2010.05607[cs].ArXiv:2010.05607Bastings, Jasmijn and Katja Filippova. 2020. The elephant in the interpretability room: Why use attention as explanation when we have saliency methods? arXiv:2010.05607 [cs]. ArXiv: 2010.05607.</p>
<p>Influence Functions in Deep Learning Are Fragile. Samyadeep Basu, Phil Pope, Soheil Feizi, Basu, Samyadeep, Phil Pope, and Soheil Feizi. 2020. Influence Functions in Deep Learning Are Fragile.</p>
<p>Anthony Bau, Nadir Durrani, Yonatan Belinkov, Fahim Dalvi, Hassan Sajjad, James Glass, IDENTIFYING AND CONTROLLING IMPORTANT NEURONS IN NEURAL MACHINE TRANSLATION. 19Bau, Anthony, Nadir Durrani, Yonatan Belinkov, Fahim Dalvi, Hassan Sajjad, and James Glass. 2019. IDENTIFYING AND CONTROLLING IMPORTANT NEURONS IN NEURAL MACHINE TRANSLATION. page 19.</p>
<p>Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering. Ben Bogin, Sanjay Subramanian, Matt Gardner, Jonathan Berant, Transactions of the Association for Computational Linguistics. 9MIT PressBogin, Ben, Sanjay Subramanian, Matt Gardner, and Jonathan Berant. 2021. Latent Compositional Representations Improve Systematic Generalization in Grounded Question Answering. Transactions of the Association for Computational Linguistics, 9:195-210. Place: Cambridge, MA Publisher: MIT Press.</p>
<p>. Bommasani, Drew A Rishi, Ehsan Hudson, Russ Adeli, Simran Altman, Arora, Michael S Sydney Von Arx, Jeannette Bernstein, Antoine Bohg, Emma Bosselut, Erik Brunskill, Shyamal Brynjolfsson, Dallas Buch, Rodrigo Card, Niladri Castellon, Annie Chatterji, Kathleen Chen, Jared Quincy Creel, Dora Davis, Chris Demszky, Moussa Donahue, Esin Doumbouya, Stefano Durmus, John Ermon, Kawin Etchemendy, Li Ethayarajh, Chelsea Fei-Fei, Trevor Finn, Lauren Gale, Karan Gillespie, Noah Goel, Shelby Goodman, Neel Grossman, Tatsunori Guha, Peter Hashimoto, John Henderson, Daniel E Hewitt, Jenny Ho, Kyle Hong, Jing Hsu, Thomas Huang, Saahil Icard, Dan Jain, Pratyusha Jurafsky, Siddharth Kalluri, Geoff Karamcheti, Fereshte Keeling, Omar Khani, Pang Wei Khattab, Mark Koh, Ranjay Krass, Rohith Krishna, Alex Kuditipudi ; Krishnan Srinivasan, Rohan Tamkin, Armin W Taori, Florian Thomas, Rose E Tramèr, William Wang, Bohan Wang, Jiajun Wu, Yuhuai Wu, Sang Michael Wu, Michihiro Xie, Jiaxuan Yasunaga, Matei You, Michael Zaharia, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zhang, Zheng, arXiv:2108.07258[cs].ArXiv:2108.07258Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih,Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle LeventKaitlyn Zhou, and Percy Liang. 2021. On the Opportunities and Risks of Foundation ModelsBommasani, Rishi, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the Opportunities and Risks of Foundation Models. arXiv:2108.07258 [cs]. ArXiv: 2108.07258.</p>
<p>A large annotated corpus for learning natural language inference. Samuel R Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsBowman, Samuel R., Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Association for Computational Linguistics, Lisbon, Portugal.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec RadfordCurran Associates, Inc33Advances in Neural Information Processing SystemsBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901, Curran Associates, Inc.</p>
<p>e-SNLI: Natural Language Inference with Natural Language Explanations. Camburu, Tim Oana-Maria, Thomas Rocktäschel, Phil Lukasiewicz, Blunsom, Advances in Neural Information Processing Systems. Curran Associates, Inc31Camburu, Oana-Maria, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural Language Inference with Natural Language Explanations. In Advances in Neural Information Processing Systems, volume 31, Curran Associates, Inc.</p>
<p>Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations. Camburu, Brendan Oana-Maria, Pasquale Shillingford, Thomas Minervini, Phil Lukasiewicz, Blunsom, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsCamburu, Oana-Maria, Brendan Shillingford, Pasquale Minervini, Thomas Lukasiewicz, and Phil Blunsom. 2020. Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Case-based explanation of non-case-based learning methods. R Caruana, H Kangarloo, J D Dionisio, U Sinha, D Johnson, Proceedings of the AMIA Symposium. the AMIA SymposiumCaruana, R., H. Kangarloo, J. D. Dionisio, U. Sinha, and D. Johnson. 1999. Case-based explanation of non-case-based learning methods. Proceedings of the AMIA Symposium, pages 212-215.</p>
<p>RNNbow: Visualizing Learning Via Backpropagation Gradients in RNNs. Dylan Cashman, Geneviève Patterson, Abigail Mosca, Nathan Watts, Shannon Robinson, Remco Chang, Conference Name: IEEE Computer Graphics and Applications. 38Cashman, Dylan, Geneviève Patterson, Abigail Mosca, Nathan Watts, Shannon Robinson, and Remco Chang. 2018. RNNbow: Visualizing Learning Via Backpropagation Gradients in RNNs. IEEE Computer Graphics and Applications, 38(6):39-50. Conference Name: IEEE Computer Graphics and Applications.</p>
<p>Transformer Interpretability Beyond Attention Visualization. Hila Chefer, Shir Gur, Lior Wolf, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Chefer, Hila, Shir Gur, and Lior Wolf. 2021. Transformer Interpretability Beyond Attention Visualization. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 782-791. ISSN: 2575-7075.</p>
<p>Learning to Explain: An Information-Theoretic Perspective on Model Interpretation. Jianbo Chen, Le Song, Martin Wainwright, Michael Jordan, PMLRProceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningChen, Jianbo, Le Song, Martin Wainwright, and Michael Jordan. 2018. Learning to Explain: An Information-Theoretic Perspective on Model Interpretation. In Proceedings of the 35th International Conference on Machine Learning, pages 883-892, PMLR. ISSN: 2640-3498.</p>
<p>Finding Universal Grammatical Relations in Multilingual BERT. Ethan A Chi, John Hewitt, Christopher D Manning, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsChi, Ethan A., John Hewitt, and Christopher D. Manning. 2020. Finding Universal Grammatical Relations in Multilingual BERT. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5564-5577, Association for Computational Linguistics, Online.</p>
<p>Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?. Rochelle Choenni, Ekaterina Shutova, Robert Van Rooij, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaChoenni, Rochelle, Ekaterina Shutova, and Robert van Rooij. 2021. Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1477-1491, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic.</p>
<p>Aspects of the theory of syntax. Noam Chomsky, M.I.T. PressOxford, EnglandAspects of the theory of syntaxChomsky, Noam. 1965. Aspects of the theory of syntax. Aspects of the theory of syntax. M.I.T. Press, Oxford, England.</p>
<p>What Does BERT Look at? An Analysis of BERT's Attention. Kevin Clark, Urvashi Khandelwal, Omer Levy, Christopher D Manning, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPFlorence, ItalyAssociation for Computational LinguisticsClark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What Does BERT Look at? An Analysis of BERT's Attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276-286, Association for Computational Linguistics, Florence, Italy.</p>
<p>ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS. Kevin Clark, Minh-Thang Luong, Quoc V Le, 18Clark, Kevin, Minh-Thang Luong, and Quoc V Le. 2020. ELECTRA: PRE-TRAINING TEXT ENCODERS AS DISCRIMINATORS RATHER THAN GENERATORS. page 18.</p>
<p>Transformers as Soft Reasoners over Language. Peter Clark, Oyvind Tafjord, Kyle Richardson, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial IntelligenceYokohama, JapanClark, Peter, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as Soft Reasoners over Language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, pages 3882-3890, International Joint Conferences on Artificial Intelligence Organization, Yokohama, Japan.</p>
<p>Visualizing and Measuring the Geometry of BERT. Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, Martin Wattenberg, 15Coenen, Andy, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, and Martin Wattenberg. 2019. Visualizing and Measuring the Geometry of BERT. page 15.</p>
<p>What you can cram into a single $&amp;!#<em> vector: Probing sentence embeddings for linguistic properties. Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, Marco Baroni, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Conneau, Alexis, German Kruszewski, Guillaume Lample, Loïc Barrault, and Marco Baroni. 2018. What you can cram into a single $&amp;!#</em> vector: Probing sentence embeddings for linguistic properties. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2126-2136, Association for Computational Linguistics, Melbourne, Australia.</p>
<p>Explaining Answers with Entailment Trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaDalvi, Bhavana, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021. Explaining Answers with Entailment Trees. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7358-7370, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic.</p>
<p>How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking. De Cao, Michael Sejr Nicola, Wilker Schlichtkrull, Ivan Aziz, Titov, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineAssociation for Computational LinguisticsDe Cao, Nicola, Michael Sejr Schlichtkrull, Wilker Aziz, and Ivan Titov. 2020. How do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3243-3255, Association for Computational Linguistics, Online.</p>
<p>Misha Denil, Alban Demiraj, Nando De Freitas, arXiv:1412.6815[cs].ArXiv:1412.6815Extraction of Salient Sentences from Labelled Documents. Denil, Misha, Alban Demiraj, and Nando de Freitas. 2015. Extraction of Salient Sentences from Labelled Documents. arXiv:1412.6815 [cs]. ArXiv: 1412.6815.</p>
<p>A General-Purpose Algorithm for Constrained Sequential Inference. Daniel Deutsch, Shyam Upadhyay, Dan Roth, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL). the 23rd Conference on Computational Natural Language Learning (CoNLL)Hong Kong, ChinaAssociation for Computational LinguisticsDeutsch, Daniel, Shyam Upadhyay, and Dan Roth. 2019. A General-Purpose Algorithm for Constrained Sequential Inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 482-492, Association for Computational Linguistics, Hong Kong, China.</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Association for Computational Linguistics, Minneapolis, Minnesota.</p>
<p>ERASER: A Benchmark to Evaluate Rationalized NLP Models. Jay Deyoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, Byron C Wallace, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsDeYoung, Jay, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A Benchmark to Evaluate Rationalized NLP Models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443-4458, Association for Computational Linguistics, Online.</p>
<p>Investigating Negation in Pre-trained Vision-and-language Models. Radina Dobreva, Frank Keller, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPPunta Cana, Dominican RepublicAssociation for Computational LinguisticsDobreva, Radina and Frank Keller. 2021. Investigating Negation in Pre-trained Vision-and-language Models. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 350-362, Association for Computational Linguistics, Punta Cana, Dominican Republic.</p>
<p>Finale Doshi-Velez, Been Kim, arXiv:1702.08608ArXiv: 1702.08608Towards A Rigorous Science of Interpretable Machine Learning. cs, statDoshi-Velez, Finale and Been Kim. 2017. Towards A Rigorous Science of Interpretable Machine Learning. arXiv:1702.08608 [cs, stat]. ArXiv: 1702.08608.</p>
<p>DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, Matt Gardner, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Dua, Dheeru, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378, Association for Computational Linguistics, Minneapolis, Minnesota.</p>
<p>Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou, arXiv:1712.06751[cs].ArXiv:1712.06751HotFlip: White-Box Adversarial Examples for Text Classification. Ebrahimi, Javid, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip: White-Box Adversarial Examples for Text Classification. arXiv:1712.06751 [cs]. ArXiv: 1712.06751.</p>
<p>Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. Yanai Elazar, Shauli Ravfogel, Alon Jacovi, Yoav Goldberg, Transactions of the Association for Computational Linguistics. 9Elazar, Yanai, Shauli Ravfogel, Alon Jacovi, and Yoav Goldberg. 2021. Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals. Transactions of the Association for Computational Linguistics, 9:160-175.</p>
<p>How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. Kawin Ethayarajh, arXiv:1909.00512csEthayarajh, Kawin. 2019. How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. arXiv:1909.00512 [cs].</p>
<p>Probing for semantic evidence of composition by means of simple classification tasks. Allyson Ettinger, Ahmed Elgohary, Philip Resnik, Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. the 1st Workshop on Evaluating Vector-Space Representations for NLPBerlin, GermanyAssociation for Computational LinguisticsEttinger, Allyson, Ahmed Elgohary, and Philip Resnik. 2016. Probing for semantic evidence of composition by means of simple classification tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, pages 134-139, Association for Computational Linguistics, Berlin, Germany.</p>
<p>CausaLM: Causal Model Explanation Through Counterfactual Language Models. Amir Feder, Nadav Oved, Uri Shalit, Roi Reichart, Computational Linguistics. Feder, Amir, Nadav Oved, Uri Shalit, and Roi Reichart. 2021. CausaLM: Causal Model Explanation Through Counterfactual Language Models. Computational Linguistics, pages 1-54.</p>
<p>Pathologies of Neural Models Make Interpretations Difficult. Feng, Eric Shi, Alvin Wallace, I I Grissom, Mohit Iyyer, Pedro Rodriguez, Jordan Boyd-Graber, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsFeng, Shi, Eric Wallace, Alvin Grissom II, Mohit Iyyer, Pedro Rodriguez, and Jordan Boyd-Graber. 2018. Pathologies of Neural Models Make Interpretations Difficult. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3719-3728, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions. Javier Ferrando, Marta R Costa-Jussà, Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational LinguisticsFerrando, Javier and Marta R. Costa-jussà. 2021. Attention Weights in Transformer NMT Fail Aligning Words Between Sequences but Largely Explain Model Predictions. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 434-443, Association for Computational Linguistics, Punta Cana, Dominican Republic.</p>
<p>Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A Smith, Sanjay Subramanian, arXiv:2004.02709[cs].ArXiv:2004.02709Evaluating Models' Local Decision Boundaries via Contrast Sets. Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben ZhouGardner, Matt, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hanna Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating Models' Local Decision Boundaries via Contrast Sets. arXiv:2004.02709 [cs]. ArXiv: 2004.02709.</p>
<p>Let's Play Mono -Poly : BERT Can Reveal Words' Polysemy Level and Partitionability into Senses. Garí Soler, Aina , Marianna Apidianaki, Transactions of the Association for Computational Linguistics. 9Garí Soler, Aina and Marianna Apidianaki. 2021. Let's Play Mono -Poly : BERT Can Reveal Words' Polysemy Level and Partitionability into Senses. Transactions of the Association for Computational Linguistics, 9:825-844.</p>
<p>Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets. Mor Geva, Yoav Goldberg, Jonathan Berant, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsGeva, Mor, Yoav Goldberg, and Jonathan Berant. 2019. Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1161-1166, Association for Computational Linguistics, Hong Kong, China.</p>
<p>Interpretation of neural networks is fragile. Amirata Ghorbani, Abubakar Abid, James Zou, Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'19/IAAI'19/EAAI'19. the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'19/IAAI'19/EAAI'19Honolulu, Hawaii, USAAAAI PressGhorbani, Amirata, Abubakar Abid, and James Zou. 2019. Interpretation of neural networks is fragile. In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, AAAI'19/IAAI'19/EAAI'19, pages 3681-3688, AAAI Press, Honolulu, Hawaii, USA.</p>
<p>Probing Linguistic Systematicity. Emily Goodwin, Koustuv Sinha, Timothy J O&apos;donnell, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsGoodwin, Emily, Koustuv Sinha, and Timothy J. O'Donnell. 2020. Probing Linguistic Systematicity. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Neural Module Networks for Reasoning over Text. Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, Matt Gardner, Gupta, Nitish, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural Module Networks for Reasoning over Text.</p>
<p>Causes and Explanations: A Structural-Model Approach. Part I: Causes. The British Journal for the Philosophy of Science. Joseph Y Halpern, Judea Pearl, Publisher: The University of Chicago Press56Halpern, Joseph Y. and Judea Pearl. 2005. Causes and Explanations: A Structural-Model Approach. Part I: Causes. The British Journal for the Philosophy of Science, 56(4):843-887. Publisher: The University of Chicago Press.</p>
<p>Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions. Han, Byron C Xiaochuang, Yulia Wallace, Tsvetkov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsHan, Xiaochuang, Byron C. Wallace, and Yulia Tsvetkov. 2020. Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5553-5563, Association for Computational Linguistics, Online.</p>
<p>Self-Attention Attribution: Interpreting Information Interactions Inside Transformer. Yaru Hao, Li Dong, Furu Wei, Ke Xu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence3514Hao, Yaru, Li Dong, Furu Wei, and Ke Xu. 2021. Self-Attention Attribution: Interpreting Information Interactions Inside Transformer. Proceedings of the AAAI Conference on Artificial Intelligence, 35(14):12963-12971. Number: 14.</p>
<p>. L A Harrington, M D Morley, A Šcedrov, S G Simpson, ID: 2plPRR4LDxICHarvey Friedman's Research on the Foundations of Mathematics. Elsevier. Google-Books. Harrington, L. A., M. D. Morley, A. Šcedrov, and S. G. Simpson. 1985. Harvey Friedman's Research on the Foundations of Mathematics. Elsevier. Google-Books-ID: 2plPRR4LDxIC.</p>
<p>Logical Inferences with Comparatives and Generalized Quantifiers. Izumi Haruta, Koji Mineshima, Daisuke Bekki, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsHaruta, Izumi, Koji Mineshima, and Daisuke Bekki. 2020. Logical Inferences with Comparatives and Generalized Quantifiers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Generating Visual Explanations. Lisa Hendricks, Zeynep Anne, Marcus Akata, Jeff Rohrbach, Bernt Donahue, Trevor Schiele, Darrell, Computer Vision -ECCV 2016. ChamSpringer International PublishingHendricks, Lisa Anne, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, and Trevor Darrell. 2016. Generating Visual Explanations. In Computer Vision -ECCV 2016, Lecture Notes in Computer Science, pages 3-19, Springer International Publishing, Cham.</p>
<p>The Promise and Peril of Human Evaluation for Model Interpretability. Bernease Herman, arXiv:1711.07414ArXiv: 1711.07414cs, statHerman, Bernease. 2019. The Promise and Peril of Human Evaluation for Model Interpretability. arXiv:1711.07414 [cs, stat]. ArXiv: 1711.07414.</p>
<p>A Structural Probe for Finding Syntax in Word Representations. John Hewitt, Christopher D Manning, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Hewitt, John and Christopher D. Manning. 2019. A Structural Probe for Finding Syntax in Word Representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129-4138, Association for Computational Linguistics, Minneapolis, Minnesota.</p>
<p>Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models. Avery Hiebert, Cole Peterson, Alona Fyshe, Nishant Mehta, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsHiebert, Avery, Cole Peterson, Alona Fyshe, and Nishant Mehta. 2018. Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 258-266, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>A Benchmark for Interpretability Methods in Deep Neural Networks. Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim, Advances in Neural Information Processing Systems. Curran Associates, Inc32Hooker, Sara, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. 2019. A Benchmark for Interpretability Methods in Deep Neural Networks. In Advances in Neural Information Processing Systems, volume 32, Curran Associates, Inc.</p>
<p>Learning to Reason: End-to-End Module Networks for Visual Question Answering. Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, Kate Saenko, 2017 IEEE International Conference on Computer Vision (ICCV). VeniceIEEEHu, Ronghang, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to Reason: End-to-End Module Networks for Visual Question Answering. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 804-813, IEEE, Venice.</p>
<p>Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?. Alon Jacovi, Yoav Goldberg, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJacovi, Alon and Yoav Goldberg. 2020. Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Aligning Faithful Interpretations with their Social Attribution. Alon Jacovi, Yoav Goldberg, Transactions of the Association for Computational Linguistics. 9MIT PressJacovi, Alon and Yoav Goldberg. 2021. Aligning Faithful Interpretations with their Social Attribution. Transactions of the Association for Computational Linguistics, 9:294-310. Place: Cambridge, MA Publisher: MIT Press.</p>
<p>Contrastive Explanations for Model Interpretability. Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaJacovi, Alon, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, and Yoav Goldberg. 2021. Contrastive Explanations for Model Interpretability. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1597-1611, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic.</p>
<p>Sarthak Jain, Byron C Wallace, arXiv:1902.10186Attention is not Explanation. Jain, Sarthak and Byron C. Wallace. 2019. Attention is not Explanation. arXiv:1902.10186 [cs].</p>
<p>Learning to Faithfully Rationalize by Construction. Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C Wallace, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsJain, Sarthak, Sarah Wiegreffe, Yuval Pinter, and Byron C. Wallace. 2020. Learning to Faithfully Rationalize by Construction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4459-4473, Association for Computational Linguistics, Online.</p>
<p>Explaining Explanations: Axiomatic Feature Interactions for Deep Networks. Joseph D Janizek, Pascal Sturmfels, Su-In Lee, Journal of Machine Learning Research. 54Janizek, Joseph D, Pascal Sturmfels, and Su-In Lee. 2021. Explaining Explanations: Axiomatic Feature Interactions for Deep Networks. Journal of Machine Learning Research, page 54.</p>
<p>Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition. Paloma Jeretic, Alex Warstadt, Suvrat Bhooshan, Adina Williams, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJeretic, Paloma, Alex Warstadt, Suvrat Bhooshan, and Adina Williams. 2020. Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks. Chengyue Jiang, Yinggong Zhao, Shanbo Chu, Libin Shen, Kewei Tu, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineAssociation for Computational LinguisticsJiang, Chengyue, Yinggong Zhao, Shanbo Chu, Libin Shen, and Kewei Tu. 2020. Cold-Start and Interpretability: Turning Regular Expressions into Trainable Recurrent Neural Networks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3193-3207, Association for Computational Linguistics, Online.</p>
<p>Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension. Yichen Jiang, Nitish Joshi, Yen-Chun Chen, Mohit Bansal, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsJiang, Yichen, Nitish Joshi, Yen-Chun Chen, and Mohit Bansal. 2019. Explore, Propose, and Assemble: An Interpretable Model for Multi-Hop Reading Comprehension. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2714-2725, Association for Computational Linguistics, Florence, Italy.</p>
<p>CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, Ross Girshick, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Honolulu, HIIEEEJohnson, Justin, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross Girshick. 2017. CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1988-1997, IEEE, Honolulu, HI.</p>
<p>Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. Taelin Karidi, Yichu Zhou, Nathan Schneider, Omri Abend, Vivek Srikumar, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaKaridi, Taelin, Yichu Zhou, Nathan Schneider, Omri Abend, and Vivek Srikumar. 2021. Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10300-10313, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic.</p>
<p>Andrej Karpathy, Justin Johnson, Li Fei-Fei, arXiv:1506.02078[cs].ArXiv:1506.02078Visualizing and Understanding Recurrent Networks. Karpathy, Andrej, Justin Johnson, and Li Fei-Fei. 2015. Visualizing and Understanding Recurrent Networks. arXiv:1506.02078 [cs]. ArXiv: 1506.02078.</p>
<p>Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly. Nora Kassner, Hinrich Schütze, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsKassner, Nora and Hinrich Schütze. 2020. Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7811-7818, Association for Computational Linguistics, Online.</p>
<p>BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief. Nora Kassner, Oyvind Tafjord, Hinrich Schütze, Peter Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaKassner, Nora, Oyvind Tafjord, Hinrich Schütze, and Peter Clark. 2021. BeliefBank: Adding Memory to a Pre-Trained Language Model for a Systematic Notion of Belief. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8849-8861, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic.</p>
<p>Divyansh Kaushik, Eduard Hovy, Zachary C Lipton, arXiv:1909.12434ArXiv: 1909.12434Learning the Difference that Makes a Difference with Counterfactually-Augmented Data. cs, statKaushik, Divyansh, Eduard Hovy, and Zachary C. Lipton. 2020. Learning the Difference that Makes a Difference with Counterfactually-Augmented Data. arXiv:1909.12434 [cs, stat]. ArXiv: 1909.12434.</p>
<p>How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks. Divyansh Kaushik, Zachary C Lipton, Kaushik, Divyansh and Zachary C. Lipton. 2018. How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks.</p>
<p>Textual Explanations for Self-Driving Vehicles. Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, Zeynep Akata, Series Title: Lecture Notes in Computer Science. Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair WeissChamSpringer International Publishing11206Computer Vision -ECCV 2018Kim, Jinkyu, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. 2018. Textual Explanations for Self-Driving Vehicles. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision -ECCV 2018, volume 11206. Springer International Publishing, Cham, pages 577-593. Series Title: Lecture Notes in Computer Science.</p>
<p>The (Un)reliability of Saliency Methods. Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, T Kristof, Sven Schütt, Dumitru Dähne, Been Erhan, Kim, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Wojciech Samek, Grégoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert MüllerChamSpringer International Publishing11700Series Title: Lecture Notes in Computer ScienceKindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, and Been Kim. 2019. The (Un)reliability of Saliency Methods. In Wojciech Samek, Grégoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert Müller, editors, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, volume 11700. Springer International Publishing, Cham, pages 267-280. Series Title: Lecture Notes in Computer Science.</p>
<p>Understanding Black-box Predictions via Influence Functions. Pang Koh, Percy Wei, Liang, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningKoh, Pang Wei and Percy Liang. 2017. Understanding Black-box Predictions via Influence Functions. In Proceedings of the 34th International Conference on Machine Learning, pages 1885-1894, PMLR. ISSN: 2640-3498.</p>
<p>Narine Kokhlikyan, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, Orion Reblitz-Richardson, arXiv:2009.07896ArXiv: 2009.07896Captum: A unified and generic model interpretability library for PyTorch. cs, statKokhlikyan, Narine, Vivek Miglani, Miguel Martin, Edward Wang, Bilal Alsallakh, Jonathan Reynolds, Alexander Melnikov, Natalia Kliushkina, Carlos Araya, Siqi Yan, and Orion Reblitz-Richardson. 2020. Captum: A unified and generic model interpretability library for PyTorch. arXiv:2009.07896 [cs, stat]. ArXiv: 2009.07896.</p>
<p>Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World. Jayant Krishnamurthy, Thomas Kollar, ; Cambridge, M A Publisher, Transactions of the Association for Computational Linguistics. 1MIT PressKrishnamurthy, Jayant and Thomas Kollar. 2013. Jointly Learning to Parse and Perceive: Connecting Natural Language to the Physical World. Transactions of the Association for Computational Linguistics, 1:193-206. Place: Cambridge, MA Publisher: MIT Press.</p>
<p>NILE : Natural Language Inference with Faithful Natural Language Explanations. Sawan Kumar, Partha Talukdar, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsKumar, Sawan and Partha Talukdar. 2020. NILE : Natural Language Inference with Faithful Natural Language Explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Representation of Linguistic Form and Function in Recurrent Neural Networks. Ákos Kádár, Grzegorz Chrupała, Afra Alishahi, ; Cambridge, M A Publisher, Computational Linguistics. 434MIT PressKádár, Ákos, Grzegorz Chrupała, and Afra Alishahi. 2017. Representation of Linguistic Form and Function in Recurrent Neural Networks. Computational Linguistics, 43(4):761-780. Place: Cambridge, MA Publisher: MIT Press.</p>
<p>Thibault Laugel, Xavier Renard, Marie-Jeanne Lesot, Christophe Marsala, Marcin Detyniecki, Defining Locality for Surrogates in Post-hoc Interpretablity. Laugel, Thibault, Xavier Renard, Marie-Jeanne Lesot, Christophe Marsala, and Marcin Detyniecki. 2018. Defining Locality for Surrogates in Post-hoc Interpretablity.</p>
<p>Rationalizing Neural Predictions. Tao Lei, Regina Barzilay, Tommi Jaakkola, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsLei, Tao, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing Neural Predictions. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 107-117, Association for Computational Linguistics, Austin, Texas.</p>
<p>The Winograd Schema Challenge. Hector Levesque, Ernest Davis, Leora Morgenstern, 10Levesque, Hector, Ernest Davis, and Leora Morgenstern. 2012. The Winograd Schema Challenge. page 10.</p>
<p>Evaluating Explanation Methods for Neural Machine Translation. Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming Shi, Proceedings of the 58th. the 58thLi, Jierui, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, and Shuming Shi. 2020. Evaluating Explanation Methods for Neural Machine Translation. In Proceedings of the 58th</p>
<p>Annual Meeting of the Association for Computational Linguistics. OnlineAssociation for Computational LinguisticsAnnual Meeting of the Association for Computational Linguistics, pages 365-375, Association for Computational Linguistics, Online.</p>
<p>Visualizing and Understanding Neural Models in NLP. Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaAssociation for Computational LinguisticsLi, Jiwei, Xinlei Chen, Eduard Hovy, and Dan Jurafsky. 2016. Visualizing and Understanding Neural Models in NLP. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 681-691, Association for Computational Linguistics, San Diego, California.</p>
<p>Understanding Neural Networks through Representation Erasure. Jiwei Li, Will Monroe, Dan Jurafsky, arXiv:1612.08220[cs].ArXiv:1612.08220Li, Jiwei, Will Monroe, and Dan Jurafsky. 2017. Understanding Neural Networks through Representation Erasure. arXiv:1612.08220 [cs]. ArXiv: 1612.08220.</p>
<p>Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Long Papers)Ling, Wang, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Association for Computational Linguistics, Vancouver, Canada.</p>
<p>Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. Tal Linzen, Emmanuel Dupoux, Yoav Goldberg, Transactions of the Association for Computational Linguistics. 4Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies. Transactions of the Association for Computational Linguistics, 4:521-535.</p>
<p>Zachary C Lipton, arXiv:1606.03490The Mythos of Model Interpretability. cs, statLipton, Zachary C. 2017. The Mythos of Model Interpretability. arXiv:1606.03490 [cs, stat].</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692[cs].ArXiv:1907.11692RoBERTa: A Robustly Optimized BERT Pretraining Approach. Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs]. ArXiv: 1907.11692.</p>
<p>Charles Lovering, Rohan Jha, Tal Linzen, and Ellie Pavlick. 2020. Information-theoretic Probing Explains Reliance on Spurious Features. Lovering, Charles, Rohan Jha, Tal Linzen, and Ellie Pavlick. 2020. Information-theoretic Probing Explains Reliance on Spurious Features.</p>
<p>A Unified Approach to Interpreting Model Predictions. Scott M Lundberg, Su-In Lee, Advances in Neural Information Processing Systems. Curran Associates, Inc30Lundberg, Scott M and Su-In Lee. 2017. A Unified Approach to Interpreting Model Predictions. In Advances in Neural Information Processing Systems, volume 30, Curran Associates, Inc.</p>
<p>Aman Madaan, Niket Tandon, Dheeraj Rajagopal, Yiming Yang, Peter Clark, arXiv:2104.08765[cs].ArXiv:2104.08765Improving Neural Model Performance through Natural Language Feedback on Their Explanations. Keisuke Sakaguchi, and Ed Hovy. 2021Madaan, Aman, Niket Tandon, Dheeraj Rajagopal, Yiming Yang, Peter Clark, Keisuke Sakaguchi, and Ed Hovy. 2021. Improving Neural Model Performance through Natural Language Feedback on Their Explanations. arXiv:2104.08765 [cs]. ArXiv: 2104.08765.</p>
<p>Jiayuan Mao, Chuang Gan, THE NEURO-SYMBOLIC CONCEPT LEARNER: INTERPRETING SCENES, WORDS, AND SENTENCES FROM NATURAL SUPERVISION. 28Mao, Jiayuan and Chuang Gan. 2019. THE NEURO-SYMBOLIC CONCEPT LEARNER: INTERPRETING SCENES, WORDS, AND SENTENCES FROM NATURAL SUPERVISION. page 28.</p>
<p>From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. Andre Martins, Ramon Astudillo, PMLRProceedings of The 33rd International Conference on Machine Learning. The 33rd International Conference on Machine LearningMartins, Andre and Ramon Astudillo. 2016. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification. In Proceedings of The 33rd International Conference on Machine Learning, pages 1614-1623, PMLR. ISSN: 1938-7228.</p>
<p>Targeted Syntactic Evaluation of Language Models. Rebecca Marvin, Tal Linzen, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsMarvin, Rebecca and Tal Linzen. 2018. Targeted Syntactic Evaluation of Language Models. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1192-1202, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>ProSPer: Probing Human and Neural Network Language Model Understanding of Spatial Perspective. Tessa Masis, Carolyn Anderson, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPPunta Cana, Dominican RepublicAssociation for Computational LinguisticsMasis, Tessa and Carolyn Anderson. 2021. ProSPer: Probing Human and Neural Network Language Model Understanding of Spatial Perspective. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 95-135, Association for Computational Linguistics, Punta Cana, Dominican Republic.</p>
<p>R Mccoy, Ellie Thomas, Tal Pavlick, Linzen, arXiv:1902.01007[cs].ArXiv:1902.01007Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. McCoy, R. Thomas, Ellie Pavlick, and Tal Linzen. 2019. Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference. arXiv:1902.01007 [cs]. ArXiv: 1902.01007.</p>
<p>Tim Miller, arXiv:1706.07269[cs].ArXiv:1706.07269Explanation in Artificial Intelligence: Insights from the Social Sciences. Miller, Tim. 2018. Explanation in Artificial Intelligence: Insights from the Social Sciences. arXiv:1706.07269 [cs]. ArXiv: 1706.07269.</p>
<p>Understanding Hidden Memories of Recurrent Neural Networks. Yao Ming, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, Huamin Qu, 2017 IEEE Conference on Visual Analytics Science and Technology (VAST). Ming, Yao, Shaozu Cao, Ruixiang Zhang, Zhen Li, Yuanzhe Chen, Yangqiu Song, and Huamin Qu. 2017. Understanding Hidden Memories of Recurrent Neural Networks. In 2017 IEEE Conference on Visual Analytics Science and Technology (VAST), pages 13-24.</p>
<p>SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning. Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, Parisa Kordjamshidi, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnlineAssociation for Computational LinguisticsMirzaee, Roshanak, Hossein Rajaby Faghihi, Qiang Ning, and Parisa Kordjamshidi. 2021. SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4582-4598, Association for Computational Linguistics, Online.</p>
<p>Layer-Wise Relevance Propagation: An Overview. Grégoire Montavon, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, Klaus-Robert Müller, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Wojciech Samek, Grégoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert MüllerChamSpringer International PublishingMontavon, Grégoire, Alexander Binder, Sebastian Lapuschkin, Wojciech Samek, and Klaus-Robert Müller. 2019. Layer-Wise Relevance Propagation: An Overview. In Wojciech Samek, Grégoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert Müller, editors, Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, Lecture Notes in Computer Science. Springer International Publishing, Cham, pages 193-209.</p>
<p>Explaining nonlinear classification decisions with deep Taylor decomposition. Grégoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, Klaus-Robert Müller, Pattern Recognition. 65Montavon, Grégoire, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert Müller. 2017. Explaining nonlinear classification decisions with deep Taylor decomposition. Pattern Recognition, 65:211-222.</p>
<p>Explainable Prediction of Medical Codes from Clinical Text. James Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, Jacob Eisenstein, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Mullenbach, James, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. 2018. Explainable Prediction of Medical Codes from Clinical Text. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1101-1111, Association for Computational Linguistics, New Orleans, Louisiana.</p>
<p>Definitions, methods, and applications in interpretable machine learning. W Murdoch, Chandan James, Karl Singh, Reza Kumbier, Bin Abbasi-Asl, Yu, Publisher: Proceedings of the National Academy of Sciences. 116Proceedings of the National Academy of SciencesMurdoch, W. James, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu. 2019. Definitions, methods, and applications in interpretable machine learning. Proceedings of the National Academy of Sciences, 116(44):22071-22080. Publisher: Proceedings of the National Academy of Sciences.</p>
<p>Sharan Narang, Colin Raffel, Katherine Lee, Adam Roberts, arXiv:2004.14546[cs].ArXiv:2004.14546Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training Text-to-Text Models to Explain their Predictions. Narang, Sharan, Colin Raffel, Katherine Lee, Adam Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! Training Text-to-Text Models to Explain their Predictions. arXiv:2004.14546 [cs]. ArXiv: 2004.14546.</p>
<p>Model Agnostic Multilevel Explanations. Natesan Ramamurthy, Bhanukiran Karthikeyan, Yunfeng Vinzamuri, Amit Zhang, Dhurandhar, Advances in Neural Information Processing Systems. Curran Associates, Inc33Natesan Ramamurthy, Karthikeyan, Bhanukiran Vinzamuri, Yunfeng Zhang, and Amit Dhurandhar. 2020. Model Agnostic Multilevel Explanations. In Advances in Neural Information Processing Systems, volume 33, pages 5968-5979, Curran Associates, Inc.</p>
<p>A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations. Weili Nie, Yang Zhang, Ankit Patel, PMLRProceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningNie, Weili, Yang Zhang, and Ankit Patel. 2018. A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations. In Proceedings of the 35th International Conference on Machine Learning, pages 3809-3818, PMLR. ISSN: 2640-3498.</p>
<p>Multimodal Explanations: Justifying Decisions and Pointing to the Evidence. Dong Park, Lisa Anne Huk, Zeynep Hendricks, Anna Akata, Bernt Rohrbach, Trevor Schiele, Marcus Darrell, Rohrbach, Park, Dong Huk, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. 2018. Multimodal Explanations: Justifying Decisions and Pointing to the Evidence. pages 8779-8788.</p>
<p>Direct and indirect effects. Judea Pearl, Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, UAI'01. the Seventeenth conference on Uncertainty in artificial intelligence, UAI'01San Francisco, CA, USAMorgan Kaufmann Publishers IncPearl, Judea. 2001. Direct and indirect effects. In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence, UAI'01, pages 411-420, Morgan Kaufmann Publishers Inc., San Francisco, CA, USA.</p>
<p>Language Models as Knowledge Bases?. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsPetroni, Fabio, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language Models as Knowledge Bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Association for Computational Linguistics, Hong Kong, China.</p>
<p>Interpretable Textual Neuron Representations for NLP. Nina Poerner, Benjamin Roth, Hinrich Schütze, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsPoerner, Nina, Benjamin Roth, and Hinrich Schütze. 2018. Interpretable Textual Neuron Representations for NLP. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 325-327, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement. Nina Poerner, Hinrich Schütze, Benjamin Roth, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, Australia. Poliak, Adam, Jason Naradowsky; New Orleans, LouisianaAssociation for Computational Linguistics1Proceedings of the Seventh Joint Conference on Lexical and Computational SemanticsPoerner, Nina, Hinrich Schütze, and Benjamin Roth. 2018. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 340-350, Association for Computational Linguistics, Melbourne, Australia. Poliak, Adam, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis Only Baselines in Natural Language Inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180-191, Association for Computational Linguistics, New Orleans, Louisiana.</p>
<p>Learning to Deceive with Attention-Based Explanations. Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C Lipton, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsPruthi, Danish, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, and Zachary C. Lipton. 2020. Learning to Deceive with Attention-Based Explanations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4782-4793, Association for Computational Linguistics, Online.</p>
<p>Language Models are Unsupervised Multitask Learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 24Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. page 24.</p>
<p>Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, arXiv:1910.10683ArXiv: 1910.10683cs, statRaffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv:1910.10683 [cs, stat]. ArXiv: 1910.10683.</p>
<p>Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation. Alessandro Raganato, Yves Scherrer, Jörg Tiedemann, Findings of the Association for Computational Linguistics: EMNLP 2020. OnlineAssociation for Computational LinguisticsRaganato, Alessandro, Yves Scherrer, and Jörg Tiedemann. 2020. Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 556-568, Association for Computational Linguistics, Online.</p>
<p>An Analysis of Encoder Representations in Transformer-Based Machine Translation. Alessandro Raganato, Jörg Tiedemann, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsRaganato, Alessandro and Jörg Tiedemann. 2018. An Analysis of Encoder Representations in Transformer-Based Machine Translation. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287-297, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers. Dheeraj Rajagopal, Vidhisha Balachandran, H Eduard, Yulia Hovy, Tsvetkov, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaRajagopal, Dheeraj, Vidhisha Balachandran, Eduard H Hovy, and Yulia Tsvetkov. 2021. SELFEXPLAIN: A Self-Explaining Architecture for Neural Text Classifiers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 836-850, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic.</p>
<p>Explain Yourself! Leveraging Language Models for Commonsense Reasoning. Nazneen Rajani, Bryan Fatema, Caiming Mccann, Richard Xiong, Socher, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsRajani, Nazneen Fatema, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging Language Models for Commonsense Reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, Association for Computational Linguistics, Florence, Italy.</p>
<p>Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton, Yoav Goldberg, Proceedings of the 58th. the 58thRavfogel, Shauli, Yanai Elazar, Hila Gonen, Michael Twiton, and Yoav Goldberg. 2020. Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection. In Proceedings of the 58th</p>
<p>Annual Meeting of the Association for Computational Linguistics. OnlineAssociation for Computational LinguisticsAnnual Meeting of the Association for Computational Linguistics, pages 7237-7256, Association for Computational Linguistics, Online.</p>
<p>Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. Shauli Ravfogel, Grusha Prasad, Tal Linzen, Yoav Goldberg, Proceedings of the 25th Conference on Computational Natural Language Learning. the 25th Conference on Computational Natural Language LearningOnlineAssociation for Computational LinguisticsRavfogel, Shauli, Grusha Prasad, Tal Linzen, and Yoav Goldberg. 2021. Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 194-209, Association for Computational Linguistics, Online.</p>
<p>Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?. Abhilasha Ravichander, Yonatan Belinkov, Eduard Hovy, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnlineAssociation for Computational LinguisticsRavichander, Abhilasha, Yonatan Belinkov, and Eduard Hovy. 2021. Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance? In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3363-3377, Association for Computational Linguistics, Online.</p>
<p>Why Should I Trust You?": Explaining the Predictions of Any Classifier. Marco Ribeiro, Sameer Tulio, Carlos Singh, Guestrin, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16New York, NY, USAAssociation for Computing MachineryRibeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2016. "Why Should I Trust You?": Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '16, pages 1135-1144, Association for Computing Machinery, New York, NY, USA.</p>
<p>Anchors: High-Precision Model-Agnostic Explanations. Marco Ribeiro, Sameer Tulio, Carlos Singh, Guestrin, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence321Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High-Precision Model-Agnostic Explanations. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1). Number: 1.</p>
<p>Counterfactual thinking: A critical overview. Neal J Roese, James M Olson, What might have been: The social psychology of counterfactual thinking. Hillsdale, NJ, USLawrence Erlbaum Associates, IncRoese, Neal J. and James M. Olson. 1995. Counterfactual thinking: A critical overview. In What might have been: The social psychology of counterfactual thinking. Lawrence Erlbaum Associates, Inc, Hillsdale, NJ, US, pages 1-55.</p>
<p>Neural Models of Factuality. Rachel Rudinger, Aaron Steven White, Benjamin Van Durme, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Rudinger, Rachel, Aaron Steven White, and Benjamin Van Durme. 2018. Neural Models of Factuality. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 731-744, Association for Computational Linguistics, New Orleans, Louisiana.</p>
<p>ConjNLI: Natural Language Inference Over Conjunctive Sentences. Swarnadeep Saha, Yixin Nie, Mohit Bansal, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)OnlineAssociation for Computational LinguisticsSaha, Swarnadeep, Yixin Nie, and Mohit Bansal. 2020. ConjNLI: Natural Language Inference Over Conjunctive Sentences. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8240-8252, Association for Computational Linguistics, Online.</p>
<p>Harnessing the linguistic signal to predict scalar inferences. Sebastian Schuster, Yuxing Chen, Judith Degen, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsSchuster, Sebastian, Yuxing Chen, and Judith Degen. 2020. Harnessing the linguistic signal to predict scalar inferences. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Bridging CNNs, RNNs, and Weighted Finite-State Machines. Roy Schwartz, Sam Thomson, Noah A Smith, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Schwartz, Roy, Sam Thomson, and Noah A. Smith. 2018. Bridging CNNs, RNNs, and Weighted Finite-State Machines. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 295-305, Association for Computational Linguistics, Melbourne, Australia.</p>
<p>Is Attention Interpretable?. Sofia Serrano, Noah A Smith, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsSerrano, Sofia and Noah A. Smith. 2019. Is Attention Interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931-2951, Association for Computational Linguistics, Florence, Italy.</p>
<ol>
<li>A Value for n-Person Games. L S Shapley, Contributions to the Theory of Games (AM-28), Volume II. Harold William Kuhn and Albert William TuckerPrinceton University PressShapley, L. S. 1953. 17. A Value for n-Person Games. In Harold William Kuhn and Albert William Tucker, editors, Contributions to the Theory of Games (AM-28), Volume II. Princeton University Press, pages 307-318.</li>
</ol>
<p>Does String-Based Neural MT Learn Source Syntax?. Xing Shi, Inkit Padhi, Kevin Knight, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsShi, Xing, Inkit Padhi, and Kevin Knight. 2016. Does String-Based Neural MT Learn Source Syntax? In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526-1534, Association for Computational Linguistics, Austin, Texas.</p>
<p>Learning Important Features Through Propagating Activation Differences. Avanti Shrikumar, Peyton Greenside, Anshul Kundaje, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningShrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. 2017. Learning Important Features Through Propagating Activation Differences. In Proceedings of the 34th International Conference on Machine Learning, pages 3145-3153, PMLR. ISSN: 2640-3498.</p>
<p>Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje, arXiv:1605.01713[cs].ArXiv:1605.01713Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. Shrikumar, Avanti, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. 2017. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences. arXiv:1605.01713 [cs]. ArXiv: 1605.01713.</p>
<p>Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds Using Paraphrases in a Neural Model. Vered Shwartz, Chris Waterson, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics2Short PapersShwartz, Vered and Chris Waterson. 2018. Olive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds Using Paraphrases in a Neural Model. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 218-224, Association for Computational Linguistics, New Orleans, Louisiana.</p>
<p>Deep inside convolutional networks: Visualising image classification models and saliency maps. Karen Simonyan, Andrea Vedaldi, Andrew Zisserman, Workshop at International Conference on Learning Representations. Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. 2014. Deep inside convolutional networks: Visualising image classification models and saliency maps. In In Workshop at International Conference on Learning Representations.</p>
<p>Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods. Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju, Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. the AAAI/ACM Conference on AI, Ethics, and SocietyNew York, NY, USAAssociation for Computing MachinerySlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. Association for Computing Machinery, New York, NY, USA, pages 180-186.</p>
<p>Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viégas, Martin Wattenberg, arXiv:1706.03825ArXiv: 1706.03825SmoothGrad: removing noise by adding noise. cs, statSmilkov, Daniel, Nikhil Thorat, Been Kim, Fernanda Viégas, and Martin Wattenberg. 2017. SmoothGrad: removing noise by adding noise. arXiv:1706.03825 [cs, stat]. ArXiv: 1706.03825.</p>
<p>Striving for Simplicity: The All Convolutional Net. J Springenberg, Alexey Dosovitskiy, Thomas Brox, M Riedmiller, Springenberg, J., Alexey Dosovitskiy, Thomas Brox, and M. Riedmiller. 2015. Striving for Simplicity: The All Convolutional Net.</p>
<p>Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models. Hendrik Strobelt, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister, Alexander M Rush, Conference Name: IEEE Transactions on Visualization and Computer Graphics. 25Strobelt, Hendrik, Sebastian Gehrmann, Michael Behrisch, Adam Perer, Hanspeter Pfister, and Alexander M. Rush. 2019. Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models. IEEE Transactions on Visualization and Computer Graphics, 25(1):353-363. Conference Name: IEEE Transactions on Visualization and Computer Graphics.</p>
<p>LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks. Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M Rush, Conference Name: IEEE Transactions on Visualization and Computer Graphics. 24Strobelt, Hendrik, Sebastian Gehrmann, Hanspeter Pfister, and Alexander M. Rush. 2018. LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks. IEEE Transactions on Visualization and Computer Graphics, 24(1):667-676. Conference Name: IEEE Transactions on Visualization and Computer Graphics.</p>
<p>Obtaining Faithful Interpretations from Compositional Neural Networks. Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational LinguisticsSubramanian, Sanjay, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, and Matt Gardner. 2020. Obtaining Faithful Interpretations from Compositional Neural Networks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5594-5608, Association for Computational Linguistics, Online.</p>
<p>Axiomatic attribution for deep networks. Mukund Sundararajan, Ankur Taly, Qiqi Yan, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia70Sundararajan, Mukund, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning -Volume 70, ICML'17, pages 3319-3328, JMLR.org, Sydney, NSW, Australia.</p>
<p>Patient representation learning and interpretable evaluation using clinical notes. Madhumita Sushil, Simon Šuster, Kim Luyckx, Walter Daelemans, Journal of Biomedical Informatics. 84Sushil, Madhumita, Simon Šuster, Kim Luyckx, and Walter Daelemans. 2018. Patient representation learning and interpretable evaluation using clinical notes. Journal of Biomedical Informatics, 84:103-113.</p>
<p>ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. OnlineAssociation for Computational LinguisticsTafjord, Oyvind, Bhavana Dalvi, and Peter Clark. 2021. ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3621-3634, Association for Computational Linguistics, Online.</p>
<p>BERT Rediscovers the Classical NLP Pipeline. Ian Tenney, Dipanjan Das, Ellie Pavlick, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsTenney, Ian, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4593-4601, Association for Computational Linguistics, Florence, Italy.</p>
<p>The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models. Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, Ann Yuan, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsOnlineAssociation for Computational LinguisticsTenney, Ian, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Sebastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif, and Ann Yuan. 2020. The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 107-118, Association for Computational Linguistics, Online.</p>
<p>Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, Thomas Mccoy, Najoung Kim, Benjamin Van Durme, Dipanjan Samuel R Bowman, Ellie Das, Pavlick, WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS. 17Tenney, Ian, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das, and Ellie Pavlick. 2019. WHAT DO YOU LEARN FROM CONTEXT? PROBING FOR SENTENCE STRUCTURE IN CONTEXTUALIZED WORD REPRESENTATIONS. page 17.</p>
<p>Probing Language Models for Understanding of Temporal Expressions. Shivin Thukral, Kunal Kukreja, Christian Kavouras, Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP. the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLPPunta Cana, Dominican RepublicAssociation for Computational LinguisticsThukral, Shivin, Kunal Kukreja, and Christian Kavouras. 2021. Probing Language Models for Understanding of Temporal Expressions. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 396-406, Association for Computational Linguistics, Punta Cana, Dominican Republic.</p>
<p>How does This Interaction Affect Me? Interpretable Attribution for Feature Interactions. Michael Tsang, Sirisha Rambhatla, Yan Liu, Advances in Neural Information Processing Systems. Curran Associates, Inc33Tsang, Michael, Sirisha Rambhatla, and Yan Liu. 2020. How does This Interaction Affect Me? Interpretable Attribution for Feature Interactions. In Advances in Neural Information Processing Systems, volume 33, pages 6147-6159, Curran Associates, Inc.</p>
<p>What if This Modified That? Syntactic Interventions with Counterfactual Embeddings. Mycal Tucker, Peng Qian, Roger Levy, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. OnlineAssociation for Computational LinguisticsTucker, Mycal, Peng Qian, and Roger Levy. 2021. What if This Modified That? Syntactic Interventions with Counterfactual Embeddings. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 862-875, Association for Computational Linguistics, Online.</p>
<p>Staying True to Your Word: (How) Can Attention Become Explanation?. Martin Tutek, Proceedings of the 5th Workshop on Representation Learning for NLP. the 5th Workshop on Representation Learning for NLPOnlineAssociation for Computational LinguisticsTutek, Martin and Jan Snajder. 2020. Staying True to Your Word: (How) Can Attention Become Explanation? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 131-142, Association for Computational Linguistics, Online.</p>
<p>Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, Manaal Faruqui, arXiv:1909.11218[cs].ArXiv:1909.11218Attention Interpretability Across NLP Tasks. Vashishth, Shikhar, Shyam Upadhyay, Gaurav Singh Tomar, and Manaal Faruqui. 2019. Attention Interpretability Across NLP Tasks. arXiv:1909.11218 [cs]. ArXiv: 1909.11218.</p>
<p>Attention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in Neural Information Processing Systems. Curran Associates, Inc30Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30, Curran Associates, Inc.</p>
<p>Diagnostic classifiers: revealing how neural networks process hierarchical structure. Sara Veldhoen, Dieuwke Hupkes, Willem Zuidema, 9Veldhoen, Sara, Dieuwke Hupkes, and Willem Zuidema. 2016. Diagnostic classifiers: revealing how neural networks process hierarchical structure. page 9.</p>
<p>Jesse Vig, arXiv:1904.02679ArXiv: 1904.02679Visualizing Attention in Transformer-Based Language Representation Models. cs, statVig, Jesse. 2019. Visualizing Attention in Transformer-Based Language Representation Models. arXiv:1904.02679 [cs, stat]. ArXiv: 1904.02679.</p>
<p>Investigating Gender Bias in Language Models Using Causal Mediation Analysis. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, Stuart Shieber, 34th Conference on Neural Information Processing Systems. Vancouver, Canada14Vig, Jesse, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating Gender Bias in Language Models Using Causal Mediation Analysis. In 34th Conference on Neural Information Processing Systems, page 14, Vancouver, Canada.</p>
<p>Context-Aware Neural Machine Translation Learns Anaphora Resolution. Elena Voita, Pavel Serdyukov, Rico Sennrich, Ivan Titov, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics1Voita, Elena, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-Aware Neural Machine Translation Learns Anaphora Resolution. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1264-1274, Association for Computational Linguistics, Melbourne, Australia.</p>
<p>Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsVoita, Elena, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5797-5808, Association for Computational Linguistics, Florence, Italy.</p>
<p>Elena Voita, Ivan Titov, arXiv:2003.12298[cs].ArXiv:2003.12298Information-Theoretic Probing with Minimum Description Length. Voita, Elena and Ivan Titov. 2020. Information-Theoretic Probing with Minimum Description Length. arXiv:2003.12298 [cs]. ArXiv: 2003.12298.</p>
<p>Interpreting Neural Networks with Nearest Neighbors. Eric Wallace, Shi Feng, Jordan Boyd-Graber, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsWallace, Eric, Shi Feng, and Jordan Boyd-Graber. 2018. Interpreting Neural Networks with Nearest Neighbors. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 136-144, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>Universal Adversarial Triggers for Attacking and Analyzing NLP. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, Sameer Singh, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsWallace, Eric, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019a. Universal Adversarial Triggers for Attacking and Analyzing NLP. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2153-2162, Association for Computational Linguistics, Hong Kong, China.</p>
<p>Interpreting Predictions of NLP Models. Eric Wallace, Matt Gardner, Sameer Singh, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts. the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial AbstractsOnlineAssociation for Computational LinguisticsWallace, Eric, Matt Gardner, and Sameer Singh. 2020. Interpreting Predictions of NLP Models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 20-23, Association for Computational Linguistics, Online.</p>
<p>Eric Wallace, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, Sameer Singh, arXiv:1909.09251[cs].ArXiv:1909.09251AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models. Wallace, Eric, Jens Tuyls, Junlin Wang, Sanjay Subramanian, Matt Gardner, and Sameer Singh. 2019b. AllenNLP Interpret: A Framework for Explaining Predictions of NLP Models. arXiv:1909.09251 [cs]. ArXiv: 1909.09251.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, arXiv:1909.07940[cs].ArXiv:1909.07940Do NLP Models Know Numbers? Probing Numeracy in Embeddings. Wallace, Eric, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019c. Do NLP Models Know Numbers? Probing Numeracy in Embeddings. arXiv:1909.07940 [cs]. ArXiv: 1909.07940.</p>
<p>Cambridge advanced learner's dictionary. Elizabeth Walter, Cambridge university pressWalter, Elizabeth. 2008. Cambridge advanced learner's dictionary. Cambridge university press.</p>
<p>SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Curran Associates, Inc32Wang, Alex, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Advances in Neural Information Processing Systems, volume 32, Curran Associates, Inc.</p>
<p>GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational LinguisticsWang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>Gradient-based Analysis of NLP Models is Manipulable. Junlin Wang, Jens Tuyls, Eric Wallace, Sameer Singh, Findings of the Association for Computational Linguistics: EMNLP 2020. OnlineAssociation for Computational LinguisticsWang, Junlin, Jens Tuyls, Eric Wallace, and Sameer Singh. 2020. Gradient-based Analysis of NLP Models is Manipulable. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 247-258, Association for Computational Linguistics, Online.</p>
<p>The What-If Tool: Interactive Probing of Machine Learning Models. James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viégas, Jimbo Wilson, Conference Name: IEEE Transactions on Visualization and Computer Graphics. 26Wexler, James, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fernanda Viégas, and Jimbo Wilson. 2020. The What-If Tool: Interactive Probing of Machine Learning Models. IEEE Transactions on Visualization and Computer Graphics, 26(1):56-65. Conference Name: IEEE Transactions on Visualization and Computer Graphics.</p>
<p>Sarah Wiegreffe, Yuval Pinter, arXiv:1908.04626Attention is not not Explanation. Wiegreffe, Sarah and Yuval Pinter. 2019. Attention is not not Explanation. arXiv:1908.04626 [cs].</p>
<p>The Estimation of Causal Effects from Observational Data. Christopher Winship, Stephen L Morgan, 10.1146/annurev.soc.25.1.659Annual Review of Sociology. 251Winship, Christopher and Stephen L. Morgan. 1999. The Estimation of Causal Effects from Observational Data. Annual Review of Sociology, 25(1):659-706. _eprint: https://doi.org/10.1146/annurev.soc.25.1.659.</p>
<p>Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models. Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, Daniel Weld, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnlineAssociation for Computational Linguistics1Wu, Tongshuang, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. 2021. Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6707-6723, Association for Computational Linguistics, Online.</p>
<p>An Interpretable Knowledge Transfer Model for Knowledge Base Completion. Qizhe Xie, Xuezhe Ma, Zihang Dai, Eduard Hovy, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Xie, Qizhe, Xuezhe Ma, Zihang Dai, and Eduard Hovy. 2017. An Interpretable Knowledge Transfer Model for Knowledge Base Completion. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 950-962, Association for Computational Linguistics, Vancouver, Canada.</p>
<p>Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?. Hitomi Yanaka, Koji Mineshima, Daisuke Bekki, Kentaro Inui, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsYanaka, Hitomi, Koji Mineshima, Daisuke Bekki, and Kentaro Inui. 2020. Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</p>
<p>HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, Christopher D Manning, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsYang, Zhilin, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380, Association for Computational Linguistics, Brussels, Belgium.</p>
<p>Connecting Attributions and QA Model Behavior on Realistic Counterfactuals. Xi Ye, Rohan Nair, Greg Durrett, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicOnline and Punta CanaYe, Xi, Rohan Nair, and Greg Durrett. 2021. Connecting Attributions and QA Model Behavior on Realistic Counterfactuals. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5496-5512, Association for Computational Linguistics, Online and Punta Cana, Dominican Republic.</p>
<p>On the (In)fidelity and Sensitivity of Explanations. Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Suggala, Pradeep K David I Inouye, Ravikumar, Advances in Neural Information Processing Systems. Curran Associates, Inc32Yeh, Chih-Kuan, Cheng-Yu Hsieh, Arun Suggala, David I Inouye, and Pradeep K Ravikumar. 2019. On the (In)fidelity and Sensitivity of Explanations. In Advances in Neural Information Processing Systems, volume 32, Curran Associates, Inc.</p>
<p>Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Advances in Neural Information Processing Systems. Curran Associates, Inc31Yi, Kexin, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. 2018. Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding. In Advances in Neural Information Processing Systems, volume 31, Curran Associates, Inc.</p>
<p>Using "Annotator Rationales" to Improve Machine Learning for Text Categorization. Omar Zaidan, Jason Eisner, Christine Piatko, Human Language Technologies 2007: The Conference of the North American Chapter. Association for Computational LinguisticsZaidan, Omar, Jason Eisner, and Christine Piatko. 2007. Using "Annotator Rationales" to Improve Machine Learning for Text Categorization. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics;</p>
<p>Proceedings of the Main Conference. the Main ConferenceRochester, New YorkAssociation for Computational LinguisticsProceedings of the Main Conference, pages 260-267, Association for Computational Linguistics, Rochester, New York.</p>
<p>Visualizing and Understanding Convolutional Networks. Matthew D Zeiler, Rob Fergus, David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars. ChamSpringer International Publishing8689Series Title: Lecture Notes in Computer ScienceZeiler, Matthew D. and Rob Fergus. 2014. Visualizing and Understanding Convolutional Networks. In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision -ECCV 2014, volume 8689. Springer International Publishing, Cham, pages 818-833. Series Title: Lecture Notes in Computer Science.</p>
<p>Going for a walk": A Study of Temporal Commonsense Understanding. Ben Zhou, Daniel Khashabi, Qiang Ning, Dan Roth, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Going on a vacation" takes longer thanZhou, Ben, Daniel Khashabi, Qiang Ning, and Dan Roth. 2019. "Going on a vacation" takes longer than "Going for a walk": A Study of Temporal Commonsense Understanding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7856 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7856</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7856</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-6f217d984f36499d88ab8a3d89572171552e6f3f</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6f217d984f36499d88ab8a3d89572171552e6f3f" target="_blank">Evaluating Large Language Models at Evaluating Instruction Following</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper introduces a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs and presents a novel suite of prompting strategies that further close the gap between LLM and human evaluators.</p>
                <p><strong>Paper Abstract:</strong> As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these ``LLM evaluators'', particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7856.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7856.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMBAR_human_vs_llm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMBAR human vs. LLM evaluator comparison (overall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Aggregated comparison reported in the paper between expert human annotators and various LLM-based evaluators on the LLMBAR meta-evaluation benchmark (instruction-following). Reports human agreement rates, LLM accuracy gaps, and qualitative failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Instruction-following evaluation (meta-evaluation of judge behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LLMBAR (Natural and Adversarial sets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Multiple (GPT-4, ChatGPT / gpt-3.5, LLaMA-2-70B-Chat, PaLM2, Falcon-180B-Chat, reward/preference models)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Various proprietary and open-source instruction-tuned LLMs; proprietary models used at temperature 0 (gpt-4-0613, gpt-3.5-turbo-0613), open-source greedy decoding; also reward models from AlpacaFarm and SteamSHP-flan-t5-xl.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert human annotators (paper authors sampled separate expert annotators; authors who curated LLMBAR not used in agreement experiment); two annotators per sampled instance.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>human expert agreement rate (percentage) and evaluator accuracy (percentage vs. gold preference labels); positional agreement rate also reported</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>94.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>underperforms on adversarial instruction-following instances; sensitivity to superficial qualities (tone/format); positional bias; lower agreement than humans on Adversarial set; reward/preference models fail to generalize</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Expert humans show very high agreement on LLMBAR (94% overall; 90% Natural; 95% Adversarial). LLM evaluators systematically underperform compared to experts, especially on adversarially constructed instances where a superficially appealing output violates the instruction. LLMs are distracted by engaging tone/format and show inconsistent preferences when option order is swapped.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Scalable, lower cost and more reproducible than human evaluations (noted as motivation); potential to be improved via prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Each pair evaluated twice with swapped presentation orders; multiple prompting strategies tested (Vanilla, CoT, Reference, Rules, Metrics, Swap, ChatEval and combinations); temperature 0 for proprietary models; open-source greedy decoding; human expert agreement measured on a random sample of 80 instances with two annotators per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models at Evaluating Instruction Following', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7856.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7856.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4_evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4-based LLM evaluator performance vs. humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Performance of GPT-4 as the base LLM evaluator under various prompting strategies on LLMBAR, compared to expert human agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Instruction-following evaluation (pairwise preference)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LLMBAR (Natural and Adversarial sets)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4 (gpt-4-0613)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 used via API (gpt-4-0613) with temperature set to 0 for deterministic outputs; tested with multiple prompting strategies and their combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert human annotators (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (percentage of evaluator choices matching gold preference labels)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>82.8</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>~10%+ accuracy gap vs. expert humans on Adversarial set; fails to fully ignore superficial quality cues; still lower than humans even with best prompting; may have advantage on GPTOUT subset when GPT-4 generated adversarial outputs (potential bias).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>GPT-4 outperforms weaker LLMs but still falls short of expert human agreement on adversarial instruction-following cases (paper reports best GPT-4-based evaluator accuracy of ~82.8% on Adversarial vs human expert agreement 95%). Prompting improvements (Rules, Metrics, Reference) yield substantial gains (around a 10% boost on Adversarial for best combinations). Chain-of-Thought prompting sometimes amplifies superficial biases and can hurt performance on adversarial instances.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Higher accuracy among tested LLMs; when combined with improved prompts, makes the best practical LLM evaluator in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-4 evaluated under multiple strategies (Vanilla, CoT, Rules*, Metrics*, Reference*, Swap*, Metrics+Reference*, etc.); each instance queried in two orders; reported both accuracy and positional agreement rates; best-performing prompting combination reported in Table 2 and discussed in text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models at Evaluating Instruction Following', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7856.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7856.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT_and_others_below_chance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / LLaMA2 / Falcon evaluator weaknesses relative to humans</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed poor performance of several commonly-used LLM evaluators (ChatGPT, LLaMA-2-70B-Chat, Falcon-180B-Chat) on LLMBAR, particularly on the Adversarial subset, with performance near or below chance and strong positional biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Instruction-following evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LLMBAR (Adversarial emphasized)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ChatGPT (gpt-3.5-turbo variants), LLaMA-2-70B-Chat, Falcon-180B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>ChatGPT evaluated via API (gpt-3.5-turbo-0613 and other variants); LLaMA-2-70B-Chat and Falcon-180B-Chat evaluated as open-source instruction-tuned chat models with greedy decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (percentage correct vs. gold) and positional agreement rate (consistency under swapping)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>barely above-chance accuracy on Adversarial; strong positional bias (instability under swapping); vulnerability to superficially more polished but incorrect outputs; Falcon exhibits extreme positional bias in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>When faced with adversarial pairs that trade correct instruction-following for superficially better presentation, ChatGPT-, LLaMA2-, and Falcon-based evaluators often fail to prefer the instruction-following output and can be at or barely above random chance. Prompting strategies (Rules, Metrics, Reference, Swap) help but do not close the gap to human experts for these base models.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Lower cost (ChatGPT much cheaper than GPT-4), accessibility of open-source models for reproducibility and privacy.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Evaluators tested under same multi-strategy protocol as GPT-4; LLMBAR Adversarial set constructed by adversarial filtering specifically against ChatGPT (paper notes ADVERSARIAL is challenging for ChatGPT-based evaluators). Each pair presented twice with swapped orders; results reported in tables per model/strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models at Evaluating Instruction Following', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7856.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7856.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon_positional_bias</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Falcon-180B-Chat positional bias failure mode</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Specific failure mode observed for Falcon-180B-Chat: severe positional bias causing very low positional agreement when asked to compare outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Instruction-following pairwise preference evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LLMBAR</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>Falcon-180B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Open-source instruction-tuned chat model (Falcon-180B-Chat) evaluated with greedy decoding; tested with CoT and other prompting variants.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>positional agreement rate (percentage of consistent preference before and after swapping outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>12.0</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>extreme sensitivity to option ordering (very low positional agreement); leads to unreliable comparisons; also distracted by superficial qualities.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Falcon with Chain-of-Thought showed a positional agreement as low as 12% (i.e., almost always flipped preference under swapping), indicating severe presentation-order bias that undermines its reliability as a judge.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Open-source availability and potential for local/private evaluation, but reliability concerns remain.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>CoT prompting tested; positional agreement measured by querying each pair twice with reversed order; results highlighted in main text and Tables.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models at Evaluating Instruction Following', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7856.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7856.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward_and_preference_models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reward models and preference model performance (AlpacaFarm RMs, SteamSHP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of reward models trained on LLM/human preferences and a preference model (SteamSHP-flan-t5-xl) on LLMBAR; these models perform poorly at identifying instruction-following outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Automatic scoring / preference prediction (reward model / preference model) for instruction-following</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>LLMBAR (Natural and Adversarial)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>reward-model-sim, reward-model-human (AlpacaFarm reward models), SteamSHP-flan-t5-xl (preference model)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Reward models from AlpacaFarm (pre-tuned); SteamSHP-flan-t5-xl a preference model trained to compare outputs; evaluated directly on pairwise choices from LLMBAR.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Expert human annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>accuracy (percentage of predicted preference matching gold)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>substantially low accuracy even on NATURAL subset; inability to reliably detect instruction-following vs. superficially appealing but incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>All three evaluated models (two reward models and SteamSHP preference model) 'fall significantly short' on LLMBAR, including on NATURAL instances, indicating current RMs and preference models do not robustly identify instruction-following quality.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>These models are directly usable in RLHF pipelines as cheap automatic scorers, but their shortcomings on LLMBAR expose risks in using them unchecked.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Direct application of pretrained reward/preference models to LLMBAR pairs; Table 3 reports per-subset accuracies (paper reports low overall accuracies and per-subset breakdowns).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models at Evaluating Instruction Following', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7856.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7856.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prior_benchmarks_human_agreement</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human agreement in prior meta-evaluation benchmarks (AlpacaFarm / MT-Bench / FairEval references)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported low human agreement rates in prior meta-evaluation datasets used to validate LLM evaluators; used in paper to motivate LLMBAR's focus on objective preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Evaluating Large Language Models at Evaluating Instruction Following</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Contextual: meta-evaluation benchmark construction critique</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AlpacaFarm, MT-Bench, FairEval (discussed comparatively)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Crowdsourced annotators (prior benchmarks) / varied</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>human annotator agreement rate (percentage)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>Prior benchmarks have low inter-annotator agreement (e.g., AlpacaFarm 66%, MT-Bench 63%), indicating subjectivity/noise in labels used to judge evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Paper argues prior meta-evaluation datasets often reflect subjective preferences with low human agreement, which can mislead selection of LLM evaluators; motivates LLMBAR's high-quality, objective-instance curation (authors report 94% expert agreement).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>N/A (this entry documents prior benchmark limitations rather than advantages).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Comparison and discussion of prior datasets in introduction and Section 4.4; LLMBAR constructed to have higher expert agreement for objective evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models at Evaluating Instruction Following', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AlpacaFarm: A simulation framework for methods that learn from human feedback <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>LLMEval2 (Wider and deeper LLM networks are fairer LLM evaluators / LLMEval) <em>(Rating: 2)</em></li>
                <li>ChatEval: Towards better LLM-based evaluators through multi-agent debate <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7856",
    "paper_id": "paper-6f217d984f36499d88ab8a3d89572171552e6f3f",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "LLMBAR_human_vs_llm",
            "name_full": "LLMBAR human vs. LLM evaluator comparison (overall)",
            "brief_description": "Aggregated comparison reported in the paper between expert human annotators and various LLM-based evaluators on the LLMBAR meta-evaluation benchmark (instruction-following). Reports human agreement rates, LLM accuracy gaps, and qualitative failure modes.",
            "citation_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "mention_or_use": "use",
            "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "evaluation_task": "Instruction-following evaluation (meta-evaluation of judge behavior)",
            "dataset_name": "LLMBAR (Natural and Adversarial sets)",
            "judge_model_name": "Multiple (GPT-4, ChatGPT / gpt-3.5, LLaMA-2-70B-Chat, PaLM2, Falcon-180B-Chat, reward/preference models)",
            "judge_model_details": "Various proprietary and open-source instruction-tuned LLMs; proprietary models used at temperature 0 (gpt-4-0613, gpt-3.5-turbo-0613), open-source greedy decoding; also reward models from AlpacaFarm and SteamSHP-flan-t5-xl.",
            "human_evaluator_type": "Expert human annotators (paper authors sampled separate expert annotators; authors who curated LLMBAR not used in agreement experiment); two annotators per sampled instance.",
            "agreement_metric": "human expert agreement rate (percentage) and evaluator accuracy (percentage vs. gold preference labels); positional agreement rate also reported",
            "agreement_score": 94.0,
            "reported_loss_aspects": "underperforms on adversarial instruction-following instances; sensitivity to superficial qualities (tone/format); positional bias; lower agreement than humans on Adversarial set; reward/preference models fail to generalize",
            "qualitative_findings": "Expert humans show very high agreement on LLMBAR (94% overall; 90% Natural; 95% Adversarial). LLM evaluators systematically underperform compared to experts, especially on adversarially constructed instances where a superficially appealing output violates the instruction. LLMs are distracted by engaging tone/format and show inconsistent preferences when option order is swapped.",
            "advantages_of_llm_judge": "Scalable, lower cost and more reproducible than human evaluations (noted as motivation); potential to be improved via prompting strategies.",
            "experimental_setting": "Each pair evaluated twice with swapped presentation orders; multiple prompting strategies tested (Vanilla, CoT, Reference, Rules, Metrics, Swap, ChatEval and combinations); temperature 0 for proprietary models; open-source greedy decoding; human expert agreement measured on a random sample of 80 instances with two annotators per instance.",
            "uuid": "e7856.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4_evaluator",
            "name_full": "GPT-4-based LLM evaluator performance vs. humans",
            "brief_description": "Performance of GPT-4 as the base LLM evaluator under various prompting strategies on LLMBAR, compared to expert human agreement.",
            "citation_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "mention_or_use": "use",
            "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "evaluation_task": "Instruction-following evaluation (pairwise preference)",
            "dataset_name": "LLMBAR (Natural and Adversarial sets)",
            "judge_model_name": "GPT-4 (gpt-4-0613)",
            "judge_model_details": "GPT-4 used via API (gpt-4-0613) with temperature set to 0 for deterministic outputs; tested with multiple prompting strategies and their combinations.",
            "human_evaluator_type": "Expert human annotators (as above)",
            "agreement_metric": "accuracy (percentage of evaluator choices matching gold preference labels)",
            "agreement_score": 82.8,
            "reported_loss_aspects": "~10%+ accuracy gap vs. expert humans on Adversarial set; fails to fully ignore superficial quality cues; still lower than humans even with best prompting; may have advantage on GPTOUT subset when GPT-4 generated adversarial outputs (potential bias).",
            "qualitative_findings": "GPT-4 outperforms weaker LLMs but still falls short of expert human agreement on adversarial instruction-following cases (paper reports best GPT-4-based evaluator accuracy of ~82.8% on Adversarial vs human expert agreement 95%). Prompting improvements (Rules, Metrics, Reference) yield substantial gains (around a 10% boost on Adversarial for best combinations). Chain-of-Thought prompting sometimes amplifies superficial biases and can hurt performance on adversarial instances.",
            "advantages_of_llm_judge": "Higher accuracy among tested LLMs; when combined with improved prompts, makes the best practical LLM evaluator in this study.",
            "experimental_setting": "GPT-4 evaluated under multiple strategies (Vanilla, CoT, Rules*, Metrics*, Reference*, Swap*, Metrics+Reference*, etc.); each instance queried in two orders; reported both accuracy and positional agreement rates; best-performing prompting combination reported in Table 2 and discussed in text.",
            "uuid": "e7856.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "ChatGPT_and_others_below_chance",
            "name_full": "ChatGPT / LLaMA2 / Falcon evaluator weaknesses relative to humans",
            "brief_description": "Observed poor performance of several commonly-used LLM evaluators (ChatGPT, LLaMA-2-70B-Chat, Falcon-180B-Chat) on LLMBAR, particularly on the Adversarial subset, with performance near or below chance and strong positional biases.",
            "citation_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "mention_or_use": "use",
            "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "evaluation_task": "Instruction-following evaluation",
            "dataset_name": "LLMBAR (Adversarial emphasized)",
            "judge_model_name": "ChatGPT (gpt-3.5-turbo variants), LLaMA-2-70B-Chat, Falcon-180B-Chat",
            "judge_model_details": "ChatGPT evaluated via API (gpt-3.5-turbo-0613 and other variants); LLaMA-2-70B-Chat and Falcon-180B-Chat evaluated as open-source instruction-tuned chat models with greedy decoding.",
            "human_evaluator_type": "Expert human annotators",
            "agreement_metric": "accuracy (percentage correct vs. gold) and positional agreement rate (consistency under swapping)",
            "agreement_score": null,
            "reported_loss_aspects": "barely above-chance accuracy on Adversarial; strong positional bias (instability under swapping); vulnerability to superficially more polished but incorrect outputs; Falcon exhibits extreme positional bias in some settings.",
            "qualitative_findings": "When faced with adversarial pairs that trade correct instruction-following for superficially better presentation, ChatGPT-, LLaMA2-, and Falcon-based evaluators often fail to prefer the instruction-following output and can be at or barely above random chance. Prompting strategies (Rules, Metrics, Reference, Swap) help but do not close the gap to human experts for these base models.",
            "advantages_of_llm_judge": "Lower cost (ChatGPT much cheaper than GPT-4), accessibility of open-source models for reproducibility and privacy.",
            "experimental_setting": "Evaluators tested under same multi-strategy protocol as GPT-4; LLMBAR Adversarial set constructed by adversarial filtering specifically against ChatGPT (paper notes ADVERSARIAL is challenging for ChatGPT-based evaluators). Each pair presented twice with swapped orders; results reported in tables per model/strategy.",
            "uuid": "e7856.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Falcon_positional_bias",
            "name_full": "Falcon-180B-Chat positional bias failure mode",
            "brief_description": "Specific failure mode observed for Falcon-180B-Chat: severe positional bias causing very low positional agreement when asked to compare outputs.",
            "citation_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "mention_or_use": "use",
            "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "evaluation_task": "Instruction-following pairwise preference evaluation",
            "dataset_name": "LLMBAR",
            "judge_model_name": "Falcon-180B-Chat",
            "judge_model_details": "Open-source instruction-tuned chat model (Falcon-180B-Chat) evaluated with greedy decoding; tested with CoT and other prompting variants.",
            "human_evaluator_type": "Expert human annotators",
            "agreement_metric": "positional agreement rate (percentage of consistent preference before and after swapping outputs)",
            "agreement_score": 12.0,
            "reported_loss_aspects": "extreme sensitivity to option ordering (very low positional agreement); leads to unreliable comparisons; also distracted by superficial qualities.",
            "qualitative_findings": "Falcon with Chain-of-Thought showed a positional agreement as low as 12% (i.e., almost always flipped preference under swapping), indicating severe presentation-order bias that undermines its reliability as a judge.",
            "advantages_of_llm_judge": "Open-source availability and potential for local/private evaluation, but reliability concerns remain.",
            "experimental_setting": "CoT prompting tested; positional agreement measured by querying each pair twice with reversed order; results highlighted in main text and Tables.",
            "uuid": "e7856.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Reward_and_preference_models",
            "name_full": "Reward models and preference model performance (AlpacaFarm RMs, SteamSHP)",
            "brief_description": "Evaluation of reward models trained on LLM/human preferences and a preference model (SteamSHP-flan-t5-xl) on LLMBAR; these models perform poorly at identifying instruction-following outputs.",
            "citation_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "mention_or_use": "use",
            "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "evaluation_task": "Automatic scoring / preference prediction (reward model / preference model) for instruction-following",
            "dataset_name": "LLMBAR (Natural and Adversarial)",
            "judge_model_name": "reward-model-sim, reward-model-human (AlpacaFarm reward models), SteamSHP-flan-t5-xl (preference model)",
            "judge_model_details": "Reward models from AlpacaFarm (pre-tuned); SteamSHP-flan-t5-xl a preference model trained to compare outputs; evaluated directly on pairwise choices from LLMBAR.",
            "human_evaluator_type": "Expert human annotators",
            "agreement_metric": "accuracy (percentage of predicted preference matching gold)",
            "agreement_score": null,
            "reported_loss_aspects": "substantially low accuracy even on NATURAL subset; inability to reliably detect instruction-following vs. superficially appealing but incorrect outputs.",
            "qualitative_findings": "All three evaluated models (two reward models and SteamSHP preference model) 'fall significantly short' on LLMBAR, including on NATURAL instances, indicating current RMs and preference models do not robustly identify instruction-following quality.",
            "advantages_of_llm_judge": "These models are directly usable in RLHF pipelines as cheap automatic scorers, but their shortcomings on LLMBAR expose risks in using them unchecked.",
            "experimental_setting": "Direct application of pretrained reward/preference models to LLMBAR pairs; Table 3 reports per-subset accuracies (paper reports low overall accuracies and per-subset breakdowns).",
            "uuid": "e7856.4",
            "source_info": {
                "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Prior_benchmarks_human_agreement",
            "name_full": "Human agreement in prior meta-evaluation benchmarks (AlpacaFarm / MT-Bench / FairEval references)",
            "brief_description": "Reported low human agreement rates in prior meta-evaluation datasets used to validate LLM evaluators; used in paper to motivate LLMBAR's focus on objective preferences.",
            "citation_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "mention_or_use": "mention",
            "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
            "evaluation_task": "Contextual: meta-evaluation benchmark construction critique",
            "dataset_name": "AlpacaFarm, MT-Bench, FairEval (discussed comparatively)",
            "judge_model_name": "",
            "judge_model_details": "",
            "human_evaluator_type": "Crowdsourced annotators (prior benchmarks) / varied",
            "agreement_metric": "human annotator agreement rate (percentage)",
            "agreement_score": null,
            "reported_loss_aspects": "Prior benchmarks have low inter-annotator agreement (e.g., AlpacaFarm 66%, MT-Bench 63%), indicating subjectivity/noise in labels used to judge evaluators.",
            "qualitative_findings": "Paper argues prior meta-evaluation datasets often reflect subjective preferences with low human agreement, which can mislead selection of LLM evaluators; motivates LLMBAR's high-quality, objective-instance curation (authors report 94% expert agreement).",
            "advantages_of_llm_judge": "N/A (this entry documents prior benchmark limitations rather than advantages).",
            "experimental_setting": "Comparison and discussion of prior datasets in introduction and Section 4.4; LLMBAR constructed to have higher expert agreement for objective evaluation.",
            "uuid": "e7856.5",
            "source_info": {
                "paper_title": "Evaluating Large Language Models at Evaluating Instruction Following",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AlpacaFarm: A simulation framework for methods that learn from human feedback",
            "rating": 2,
            "sanitized_title": "alpacafarm_a_simulation_framework_for_methods_that_learn_from_human_feedback"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_not_fair_evaluators"
        },
        {
            "paper_title": "LLMEval2 (Wider and deeper LLM networks are fairer LLM evaluators / LLMEval)",
            "rating": 2,
            "sanitized_title": "llmeval2_wider_and_deeper_llm_networks_are_fairer_llm_evaluators_llmeval"
        },
        {
            "paper_title": "ChatEval: Towards better LLM-based evaluators through multi-agent debate",
            "rating": 1,
            "sanitized_title": "chateval_towards_better_llmbased_evaluators_through_multiagent_debate"
        }
    ],
    "cost": 0.017351,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evaluating Large Language Models at Evaluating Instruction Following</h1>
<p>Zhiyuan Zeng ${ }^{1}$, Jiatong Yu ${ }^{2}$, Tianyu Gao ${ }^{2}$, Yu Meng ${ }^{3}$, Tanya Goyal ${ }^{2}$, Danqi Chen ${ }^{2}$<br>${ }^{1}$ Department of Computer Science and Technology, Tsinghua University<br>${ }^{2}$ Princeton Language and Intelligence (PLI), Princeton University<br>${ }^{3}$ Department of Computer Science, University of Illinois Urbana-Champaign<br>zengzy2@@mails.tsinghua.edu.cn<br>{jiatongy, tianyug, tanyagoyal, danqic}@princeton.edu<br>yumeng5@illinois.edu</p>
<h4>Abstract</h4>
<p>As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these "LLM evaluators", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBAR, designed to test the ability of an LLM evaluator in discerning instructionfollowing outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highestscoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>The recent success of LLM-based chat assistants has spurred countless research efforts in both academia and industry, with new models being released at an astonishing rate. While conventional benchmarks measure the underlying ability of those models in commonsense and world knowledge (Gao et al., 2021; Srivastava et al., 2022; Hendrycks et al., 2021), human evaluation remains the gold standard for testing conversational abilities due to the open-ended nature of the task. However, this is neither scalable nor reproducible (Karpinska et al., 2021). Consequently, LLM evaluators have emerged as a cost-effective alternative for obtaining preference judgments between outputs from different models (Chiang \&amp; Lee, 2023; Dubois et al., 2023; Chen et al., 2023b).</p>
<p>Operationally, an LLM evaluator is a combination of a strong base LLM (OpenAI, 2022; 2023; Anthropic, 2023) and its prompting strategy (Wei et al., 2022; Zheng et al., 2023). They are usually given one instruction and corresponding outputs from two models, and asked to choose a preferred one. It remains an open question whether we can rely on those LLM evaluators and which ones to use. This highlights the need for a good meta-evaluation benchmark (consisting of instructions and output pairs associated with human judgments) so that we can evaluate to what extent different LLM evaluators agree with human preferences and choose evaluators in an informed manner.</p>
<p>How should we construct a good meta-evaluation benchmark? Prior work has primarily used randomly-sampled output pairs and crowdsourced annotators to construct meta-evaluation benchmarks to assess LLM evaluators (Dubois et al., 2023; Zheng et al., 2023; Zhang et al., 2023; Wang et al., 2023b). However, we argue this strategy overlooks one important factor: inherent subjectivity of human preferences. Consider the top example in Figure 1: despite the quality difference being in-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of instances from previous work and our proposed meta-evaluation benchmark LLMBAR. LLMBAR curates output pairs that have objective preferences. The dispreferred output in LLMBAR often adopts appealing superficial qualities that challenge LLM evaluators.
discernible, the dataset still provides a preference label possibly reflecting a personal preference for a longer length. This issue is also demonstrated by the low agreements between human annotators reported in AlpacaFarm (66\%; Dubois et al., 2023) and MT-Bench (63\%; Zheng et al., 2023), against a random baseline of $50 \%$. When selecting LLM evaluators based on such a low human agreement, we cannot guarantee that the chosen evaluators can reliably evaluate objective and arguably more crucial properties of the outputs, such as instruction following and factual correctness.</p>
<p>In this work, we create a meta-evaluation benchmark for assessing LLM evaluators on one such objective criterion, namely instruction following. We define it as the ability to correctly parse openended instructions and adhere to the specified requirements. This criterion relates to other desirable LLM properties, such as helpfulness (Askell et al., 2021). Furthermore, unlike attributes that can be easily acquired through imitation learning, such as engaging tones (Gudibande et al., 2023), even the strongest LLMs today struggle with following instructions (Wu et al., 2023b; Li et al., 2023c). Figure 1 (bottom) shows an example of instruction following vs. superficial quality. While the right output adheres to the instruction, both LLM evaluators and humans are often biased towards the left one due to its more engaging tone. If we do not rigorously analyze the capability of LLM evaluators to distinguish between the true ability of instruction following and superficial clues, there is a risk of advancing models that excel in mimicking effective assistants rather than executing desired tasks.</p>
<p>We introduce LLMBAR, a manually curated meta-evaluation benchmark designed to test whether LLM evaluators can detect instruction-following outputs. LLMBAR consists of 419 instances, where each entry consists of an instruction paired with two outputs: one faithfully and correctly follows the instruction and the other deviates from it. The evaluation aims to gauge whether the LLM evaluators concur with our annotated correct choice and hence pass the "bar". LLMBAR departs from existing meta-evaluation (Dubois et al., 2023; Chiang \&amp; Lee, 2023; Wang et al., 2023b; Zheng et al., 2023; Zhang et al., 2023) in the following aspects:</p>
<ul>
<li>All the instances are examined by the authors to guarantee their quality.</li>
<li>LLMBAR focuses exclusively on the instruction-following quality and enforces objective preferences. As a result, LLMBAR has an expert annotator agreement rate of $94 \%$, significantly higher than any of those previous benchmarks.</li>
<li>LLMBAR provides both a Natural set and an Adversarial set. The Natural set collects and filters preference data from existing benchmarks, aiming to gauge evaluator performance in real-world distributions. Conversely, the Adversarial set comprises adversarially crafted instances that tend to confound less adept evaluators.</li>
</ul>
<p>We assess the performance of five LLMsGPT-4 (OpenAI, 2023), ChatGPT (OpenAI, 2022), LLaMA-2-Chat (Touvron et al., 2023b), PaLM2 (Anil et al., 2023), and Falcon (Almazrouei et al., 2023)paired with various prompting strategies as evaluators. Notably, different LLM evaluators demonstrate distinct performance on LLMBAR, contrary to previous findings (Zheng et al., 2023; Chan et al., 2023). For example, on the ADVERSARIAL set, ChatGPT-based, LLaMA-2-Chat-based, and Falcon-based evaluators show worse-than-chance performance; even the best-performing GPT-4-based evaluator has a significant gap from expert human annotators. Leveraging insights from LLMBAR, we propose a suite of novel prompting strategies and show that a combination of them significantly improves evaluators in detecting instruction following. Notably, the best strategy leads to a $10 \%$ boost for GPT-4-based evaluators on the ADVERSARIAL set.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Illustration of the ADVERSARIAL set collection process (except the MANUAL subset). Given an instruction $I$ and a preferred output $O_{1}$, we either collect a closely related but different enough instruction $I^{\prime}$ and generate dispreferred (adversarial) output $O_{2}$ (in NEIGHBOR and GPTINST), or directly construct an output $O_{2}$ (in GPTOUT). In NEIGHBOR, we use weaker models to generate $O_{1}$ and stronger models to generate $O_{2}$ so $O_{2}$ tends to be more superficially appealing.</p>
<p>LLMBAR provides an objective and replicable benchmark for assessing LLM evaluators in judging instruction following. It underscores the limitations of current LLM evaluators that have been neglected by previous studies. With a better assessment of LLM evaluators, we hope to help build and select better evaluators in a quantitative manner, and foster research in instruction-following models.</p>
<h1>2 LLMBAR: A META-EVALUATION BENCHMARK</h1>
<p>We introduce LLMBAR, a meta-evaluation benchmark designed to test LLM evaluators' ability to discern instruction-following outputs. Each instance in LLMBAR is a tuple $\left(I, O_{1}, O_{2}, p\right)$, where $I$ is the input instruction, $O_{1}$ and $O_{2}$ are two corresponding outputs, and $p \in{1,2}$ is the associated gold preference label indicating $O_{p}$ is objectively better than the other.
LLMBAR consists of two parts: (1) The Natural set collects instances from existing human-preference datasets. We further filter and modify them to ensure that an objective preference exists for each instance. (2) In the ADVERSARIAL set, the authors create the dispreferred output such that it deviates from the instruction but often has good superficial qualities and may thus distract the evaluator. While the Natural set reflects the evaluator performance in a real-world distribution, the ADVERSARIAL set stress tests whether the LLM evaluators can truly detect instruction following. We show the statistics in Table 1 and discuss the collection process in the following.</p>
<h3>2.1 The Natural Set</h3>
<p>We first randomly sample a set of instructions and corresponding output pairs $\left(I, O_{1}, O_{2}\right)$ from AlpacaFarm (Dubois et al., 2023) ${ }^{2}$ and LLMEval ${ }^{2}$ (Zhang et al., 2023) ${ }^{3}$. As discussed previously, these candidate instances often assemble output pairs where an objective quality difference does not exist, and the human annotation merely reflects the annotators' subjective preferences. We heavily filter and modify the instances such that for all the remaining ones, there exists an objectively better output regarding instruction following. Specifically, each instance is examined by the authors. If there is no objective preference between two outputs, or if the label is incorrect, we will then modify the instance accordingly or discard it if making such modifications is difficult. Note that despite it being named "natural", this set provides high-quality instances with objective preferences that do not exist in previous work. Appendix A. 1 provides example instances in the Natural set along with the corresponding manual filtering and modification applied to ensure objectivity.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>2.2 The Adversarial Set</h1>
<p>The Adversarial set is specifically designed to stress test LLM evaluators with instances that tend to mislead them. All the instances are constructed by a two-step process:</p>
<ol>
<li>First, we generate challenging candidate instances, expecting that one output $O_{1}$ faithfully follows the instruction $I$, and the other output $O_{2}$ deviates from $I$ but tends to exhibit superior superficial quality, e.g., with a more polished tone or a better format. A good evaluator should prefer $O_{1}$ over $O_{2}$ without being distracted by the superficial qualities. Here, the adversarial instances are constructed such that it is challenging for LLM evaluators to identify the instructionfollowing outputs, rather than being deliberately hard to generate based on the instruction.</li>
<li>Next, we perform adversarial filtering to retain the most difficult candidate instances. We use four ChatGPT-based evaluators from AlpacaFarm and two different presentation orders ( $O_{1}, O_{2}$ and $O_{2}, O_{1}$ ) to obtain eight preference labels. We filter out the candidate instance if a majority of those preferences are aligned with our expected one. This is followed by manual filtering and modification by the authors to ensure objectivity and correctness, as was done for NATURAL.</li>
</ol>
<p>In the following, we describe four different strategies to collect candidate instances for step 1, which correspond to the four Adversarial subsets. We first sample instructions from three existing instruction-tuning datasets: Alpaca (Taori et al., 2023), OpenAssistant (Kpf et al., 2023), and ShareGPT ${ }^{4}$. If not specified, $O_{1}$ is either generated by an instruction-tuned LLaMA-7B model or the reference output from the datasets. Figure 2 illustrates these different collection strategies.</p>
<p>Neighbor Instructions (Neighbor). Given an instruction $I \in \mathcal{D}$ where $\mathcal{D}$ is its corresponding dataset, we retrieve a closely related yet sufficiently different instruction $I^{\prime}$ from the same dataset $\mathcal{D}$,</p>
<p>$$
I^{\prime}=\underset{I^{\prime \prime} \in \mathcal{D}, \operatorname{sim}\left(I, I^{\prime \prime}\right)&lt;\epsilon}{\arg \max } \operatorname{sim}\left(I, I^{\prime \prime}\right)
$$</p>
<p>Here, $\operatorname{sim}(\cdot)$ is the cosine similarity measured by INSTRUCTOR (Su et al., 2023), a sentence embedding model. $\epsilon$ is a threshold to ensure that $I^{\prime}$ and $I$ are semantically different enough. We then prompt a relatively weaker model with $I$ to generate $O_{1}$, and prompt a stronger model with $I^{\prime}$ to generate $O_{2}$. Specifically, we generate $O_{1}$ by an instruction-tuned LLaMA-7B and take the reference output from original datasets as $O_{2}$, generated by text-davinci-003 in Alpaca, humans in OpenAssistant, and ChatGPT in ShareGPT. This gives us a candidate instance $\left(I, O_{1}, O_{2}, p=1\right)$. The intuition is that $O_{2}$ potentially exhibits better superficial quality, but does not follow the target instruction $I$. This kind of superficial superiority of $O_{2}$ could mislead LLM evaluators into favoring it and thus make the instance potentially adversarial. Note that if $I$ and $I^{\prime}$ are not semantically different enough, $O_{2}$ may be correct for $I$, and these instances will be filtered out in the later stage of manual filtering and modification. See Appendix A. 2 for more details.</p>
<p>GPT-4 Instructions (GPTInSt). Similar to NeighBOR, we want to find $I^{\prime}$ that is similar to but different enough from $I$. We directly prompt GPT-4 to generate $I^{\prime}$ and then use $I^{\prime}$ to generate $O_{2}$ by ChatGPT. We also tried using ChatGPT to generate $I^{\prime}$ but found that it would fail in almost all cases. We observe that GPT-4-generated $I^{\prime}$ s exhibit consistent patterns. It often substitutes certain phrases from $I$ with their related counterparts, and thus the diversity of $\left(I, I^{\prime}\right)$ is worse than that in NeighBOR. See Appendix A. 3 for more details.</p>
<p>GPT-4 Unhelpful Outputs (GPTOUT). In this subset, we directly prompt GPT-4 to produce a superficially good but unhelpful or incorrect output $O_{2}$ given instruction $I$. This is a challenging task even for GPT-4. In most cases, $O_{2}$ produced by GPT-4 is either correct or obviously incorrect (thereby not adversarial). Nonetheless, we are still able to obtain a high-quality subset of instances after adversarial filtering and manual inspection. See Appendix A. 4 for more details. A potential limitation about this subset is that since the adversarial outputs are created by GPT-4, GPT-4-based evaluators may have an unfair advantage when they are assessed on this subset. We leave an in-depth analysis of this matter for future work.</p>
<p>Manual Construction (Manual). In addition to the aforementioned automatic processes of generating candidate instances, we take inspiration from the previous three subsets and manually con-</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Illustration of our proposed prompting strategies Rules, Metrics, and Swap. Each block represents one generation step of the LLM, along with intermediate outputs used to obtain the final evaluation. For the last step of Swap, intermediate generations are updated to reflect a consistent ordering of the pairwise outputs.
struct instances that are adversarially challenging to LLM evaluators to further increase the quantity and diversity of our ADVERSARIAL set. Appendix A. 5 gives example instances in this subset.</p>
<h1>3 Prompting Strategies for LLM EVALuators</h1>
<p>In this section, we present a collection of prompting strategies for LLM evaluators examined on LLMBAR. While the capacity of base LLMs largely determines how accurate the evaluator is, we find that different prompting strategies also play a significant role.</p>
<p>We first examine existing prompting strategies, followed by a suite of novel prompting strategiesRules, Metrics, and Swap (see Figure 3)proposed by this work.</p>
<p>Vanilla (Dubois et al., 2023). We instruct the LLM to select better outputs, followed by the instruction $I$ and the two outputs $O_{1}$ and $O_{2}$. The LLM is asked to simply output its preference without any explanation. We prompt the LLM in a zero-shot manner by default. We also experiment with few-shot in-context learning in Appendix E and there is no significant difference.</p>
<p>Chain-of-Thoughts (CoT; Wei et al., 2022). Instead of generating labels only, we instruct the LLM to first generate a concise reasoning, prior to generating its preference between the two outputs.</p>
<p>Self-Generated Reference (Reference; Zheng et al., 2023). We first prompt the LLM evaluator to generate an output given the instruction. The generated output is then passed to the LLM evaluator as a reference when making the comparison.</p>
<p>ChatEval (Chan et al., 2023). We experiment with ChatEval (Chan et al., 2023), where multiple LLM evaluators, personalized by different role prompts, evoke a discussion on the preference. All the evaluators take turns to give their final preference given the context of their discussions.</p>
<p>Rules. In the prompt, we explicitly list some general rules for LLM evaluators to follow when making the comparison, for example, "prioritize evaluating whether the output honestly executes the instruction". We find that Rules improves the evaluator's accuracy almost universally and is easy to apply on top of any other prompting strategies. In the following text and tables, we mark prompting methods that use Rules with <em>. For example, Reference</em> indicates Rules+Reference.</p>
<p>Self-Generated Metrics (Metrics). Intuitively, LLM evaluators could benefit from some metrics that specify what constitutes a good output given this specific instruction. To do so, we first prompt the LLM to generate a set of instruction-specific metrics that a good output should adhere to. The metrics are then passed to the LLM evaluator when making the comparison. It encourages the LLM</p>
<p>evaluators to focus on specific aspects of instruction following. Naturally, we can combine this strategy with Self-Generated Reference (Metrics+Reference). Concurrently with our work, Li et al. (2023a) and Saha et al. (2023) propose similar ideas to this strategy.</p>
<p>Swap and Synthesize (Swap). Existing work finds that many LLM evaluators exhibit strong positional bias (Wang et al., 2023b). When the position of two outputs is swapped, the evaluator often generates contradictory preferences. Inspired by Du et al., 2023, we first prompt the LLM evaluator to give its preference using CoT with orders $O_{1}, O_{2}$ and $O_{2}, O_{1}$. Then we instruct the evaluator to make its final decision by synthesizing the two CoTs if evaluators generate contradictory preferences. We also adopt the CoT version of this strategy (Swap+CoT), where the LLM evaluator is asked to use CoT when synthesizing.</p>
<p>The exact prompt for each strategy, more details, and some examples can be found in Appendix B.</p>
<h1>4 EXPERIMENTS</h1>
<p>In this section, we conduct comprehensive experiments and evaluate different LLM evaluators on LLMBAR to answer the following research questions: (1) How do different LLMs and prompting strategies affect the evaluator performance on LLMBAR? (2) How is LLMBAR different from other meta-evaluation datasets used to assess LLM evaluators?</p>
<p>Table 2: Results of GPT-4-based evaluators on LLMBAR. * indicates the incorporation of Rules. The highest average accuracy is marked by bold and the highest positional agreement rate is marked by underline. Random guess would achieve an Acc. of $50 \%$ and an Agr. of $50 \%$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Strategy</th>
<th style="text-align: center;">Natural</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Adversarial</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">NeighBOR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPTInST</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPTOUT</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MANUAL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vanilla</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">90.6</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla*</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">94.6</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">91.8</td>
</tr>
<tr>
<td style="text-align: center;">CoT*</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">90.3</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">76.6</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;">Swap*</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">77.6</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">95.7</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">96.2</td>
</tr>
<tr>
<td style="text-align: center;">Swap+CoT*</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">100.0</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">85.3</td>
<td style="text-align: center;">96.7</td>
<td style="text-align: center;">79.8</td>
<td style="text-align: center;">97.9</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">80.3</td>
<td style="text-align: center;">96.8</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">97.5</td>
</tr>
<tr>
<td style="text-align: center;">ChatEval*</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">85.8</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">68.1</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">85.4</td>
</tr>
<tr>
<td style="text-align: center;">Metrics*</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">73.4</td>
<td style="text-align: center;">89.4</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">88.3</td>
<td style="text-align: center;">84.2</td>
<td style="text-align: center;">89.5</td>
</tr>
<tr>
<td style="text-align: center;">Reference*</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">97.0</td>
<td style="text-align: center;">80.6</td>
<td style="text-align: center;">89.6</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">77.7</td>
<td style="text-align: center;">85.1</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">87.0</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">85.2</td>
<td style="text-align: center;">89.8</td>
</tr>
<tr>
<td style="text-align: center;">Metrics+Reference*</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">96.0</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">89.7</td>
<td style="text-align: center;">90.2</td>
<td style="text-align: center;">72.3</td>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">83.7</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">88.2</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">89.8</td>
</tr>
</tbody>
</table>
<h3>4.1 EXPERIMENTAL SETUP</h3>
<p>We employ both proprietary and open-source LLMs as base models. To enhance reproducibility, we set the temperature to 0 for proprietary models, and utilize greedy decoding for open-source models.</p>
<p>Proprietary models. We adopt GPT-4 (OpenAI, 2023) and ChatGPT (OpenAI, 2022), two representative proprietary instruction-tuned LLMs that are commonly used as LLM evaluators (Dubois et al., 2023; Rafailov et al., 2023; Chen et al., 2023a; Li et al., 2023d, etc). Note that even though GPT-4 is believed to be much stronger, it is $30 \times$ more expensive than ChatGPT, making ChatGPT appealing for researchers with limited budgets. We also experiment with PaLM2 (Anil et al., 2023).</p>
<p>Open-source models. Using proprietary API LLMs as evaluators presents many challenges. The API usage may incur high costs and delays and may pose privacy concerns. Thus, employing open-source LLMs as evaluators can be a promising substitute (Zheng et al., 2023; Wang et al., 2023c). We experiment with two state-of-the-art open-source instruction-tuned models: LLaMA-2-70B-Chat (Touvron et al., 2023b) and Falcon-180B-Chat (Almazrouei et al., 2023).</p>
<h3>4.2 Human Agreement on LLMBAR</h3>
<p>We sample 80 instances randomly from LLMBAR and assign each instance to two paper authors (as expert human annotators). Authors who manually curate LLMBAR are NOT involved in the experiment as they know the gold labels. We ask them to select the output that better follows the given instruction. The agreement rate between expert annotators on the sampled LLMBAR set is $\mathbf{9 4 \%}$. Human agreement rate is $90 \%$ and $95 \%$ respectively on the Natural and the ADVERSAriAL</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Average accuracies of 8 representative LLM evaluators on LLMBAR. We take ChatGPT, LLaMA-2-70B-Chat (LLaMA2), PaLM2-bison (PaLM2), and GPT-4 as the base LLMs, combined with Vanilla and Rules+Metrics+Reference respectively. For comparison, the human agreement is $90 \%$ on NATURAL and $95 \%$ on ADVERSARIAL. Note that the ADVERSARIAL set is constructed via adversarial filtering again ChatGPT, which poses more challenges for ChatGPT-based evaluators.
set $^{5}$. As a reference, FairEval (Wang et al., 2023b) has an average human annotation accuracy of 71.7\%; MT-Bench (Zheng et al., 2023) reports a human agreement rate of $63 \%$. This suggests that LLMBAR instances reflect objective human preferences on instruction following and achieve high human agreement among expert annotators.</p>
<h1>4.3 LLM Evaluator Performance on LLMBar</h1>
<p>We evaluate different evaluators (combinations of LLMs and prompting strategies) on LLMBAR. For each output pair, we query the evaluator twice with swapped orders. We then report average accuracy (Acc.) and positional agreement rate (Agr.). Positional agreement rate (Agr.) refers to the percentage of instances with consistent preference labels before and after swapping the presentation orders of the two outputs. Average accuracies of 8 representative LLM evaluators are shown in Figure 4. We observe that Falcon-180B-Chat exhibits a notable positional bias compared to other models. For example, Falcon with CoT has an agreement of only $12 \%$. Thus we omit it from the main results here. Detailed results of GPT-4, ChatGPT ${ }^{6}$, LLaMA-2-70B-Chat (LLaMA2), PaLM2 ${ }^{7}$, and Falcon-180B-Chat (Falcon) are reported in Table 2, Table 5, Table 7, Table 8, and Table 9. The results of the rating-based evaluation (instead of comparison-based) are shown in Appendix D.</p>
<p>LLM evaluators significantly underperform human on LLMBAR. As shown in Figure 4 and result tables, all LLM evaluators struggle on the LLMBAR ADVERSARIAL subsets. When using ChatGPT, LLaMA2, and Falcon as the base model, LLM evaluators can barely achieve above-chance performance on the ADVERSARIAL set. PaLM2-based and GPT-4-based evaluators show much higher accuracy on ADVERSARIAL, yet even the best performing GPT-4-based evaluator achieves an average accuracy of $82.8 \%$ on ADVERSARIAL, more than $10 \%$ lower than the human expert agreement rate $(95 \%)$. The evaluator performance gap is relatively smaller on the NATURAL set, though weaker LLMs still lag behind GPT-4 and humans by a significant margin.</p>
<p>Our proposed prompting strategies significantly improve the evaluators' performance. Figure 4 demonstrates that a combination of Rules+Metrics+Reference (Metrics+Reference<em> in the table) consistently improves evaluator performance across all LLMs for both NATURAL and ADVERSARIAL sets. Looking at individual prompting strategies, each of Rules, Metrics, and Reference improves the average accuracy of LLM evaluators on the ADVERSARIAL set. Combining them results in around $10 \%$ improvement for the GPT-4-based evaluator. Contrary to common beliefs, CoT</em> falls short in enhancing LLM evaluators on ADVERSARIAL. We observe that the produced reasoning often exhibits stronger biases towards outputs with superior superficial quality and thus hurts the performance. Swap<em> and Swap+CoT</em> significantly improve the positional agreement rate, without negatively affecting the average accuracy, and in some cases, slightly improving it.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average accuracies of 8 representative LLM-evaluators on FairEval, LLMEval ${ }^{2}$, MTBench, and our Adversarial set. Note that these datasets do not ensure the objective correctness of the preferences, so the accuracies on them do not reliably reflect the evaluators' capabilities.</p>
<h1>4.4 COMPARISON TO OTHER META-EVALUATIONS OF LLM eVALuATORS</h1>
<p>We compare LLMBAR to existing meta-evaluation benchmarks for LLM evaluator and investigate if they show different trends from ours. Figure 5 illustrates the average accuracies of Vanilla and Metrics+Reference* evaluators on FairEval (Wang et al., 2023b), LLMEval ${ }^{2}$ (Zhang et al., 2023), MT-Bench (Zheng et al., 2023), and the average result across our Adversarial set. For a fair comparison, we remove LLMEval ${ }^{2}$ instances whose instructions are empty or non-English and add the task description before the raw input as the instruction. For MT-Bench, we get the gold preferences by majority vote. We remove all "TIE" instances and randomly sample 200 instances for LLMEval ${ }^{2}$ and MT-Bench respectively.</p>
<p>We observe that LLMBAR demonstrates a drastically different pattern of LLM evaluators from existing benchmarks. While different LLMs and prompting strategies perform similarly on the other datasets, LLMBAR shows a clear gap between weaker and stronger LLMs, and vanilla vs. improved prompts. This supports LLMBAR to be a better evaluation of the capability of LLM evaluators in discerning instruction following, and a better benchmark for LLM evaluator selection.</p>
<p>Table 3: Results of AlpacaFarm reward models and a preference model SteamSHP-flan-t5-xl.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reward/Preference Model</th>
<th style="text-align: center;">Natural</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Adversarial</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Neighbor</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPTInSt</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPTOut</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">reward-model-sim</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.7</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">reward-model-human</td>
<td style="text-align: center;">70.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">51.1</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">SteamSHP-flan-t5-xl</td>
<td style="text-align: center;">63.5</td>
<td style="text-align: center;">93.0</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">26.1</td>
<td style="text-align: center;">91.3</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">38.0</td>
<td style="text-align: center;">89.1</td>
<td style="text-align: center;">31.1</td>
<td style="text-align: center;">88.8</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">89.7</td>
</tr>
</tbody>
</table>
<h3>4.5 ReWard Model and Preference Model Performance on LLMBAR</h3>
<p>LLMBAR can be also used for evaluating reward models (RMs), a critical component in reinforcement learning from human feedback (RLHF; Christiano et al., 2017; Ouyang et al., 2022) that is trained on pairwise preference data to rate model outputs. We evaluate two RMs from AlpacaFarm ${ }^{5}$ on LLMBAR, reward-model-sim and reward-model-human, trained on data annotated by LLMs and humans respectively. We also evaluate SteamSHP-flan-t5-xl (Ethayarajh et al., 2022), a preference model trained to provide its preference among two outputs given an instruction. Table 3 shows that these three models fall significantly short on LLMBAR, even on NATURAL, suggesting that current reward models and preference models struggle to identify instruction-following outputs, a finding in line with Shen et al. (2023); Singhal et al. (2023). Lambert et al. (2024) evaluate a wide range of reward models on LLMBAR, and we refer readers to it for a more extensive result.</p>
<h3>4.6 Case Study: A More Challenging Meta-Evaluation Set</h3>
<p>In the previous subsections, we showed that most evaluators struggle with LLMBAR, but the powerful GPT-4-based evaluators achieve reasonable scores. Are there more challenging tasks that even</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the most powerful LLM, equipped with advanced prompts, may fail on? In this case study, we explore some more adversarial and synthetic scenarios for meta-evaluation: (1) The CONSTRAINT subset, where instructions impose combinatorial lexical constraints on outputs; (2) The NEGATION subset, where instructions intentionally request unhelpful outputs; (3) The BASE-9 and BASE-10 subsets, which involve two-digit addition problems in base-9 and base-10, with the former being known as a counterfactual task (Wu et al., 2023b) that deviates from standard assumptions. We evaluate representative prompting strategies on these subsets in Table 14. Overall, we find that evaluating instances with these special instructions is challenging, and our enhanced strategies also improve performance. Further details are available in Appendix F.</p>
<h1>5 Related Work</h1>
<p>The rapid development of open-ended instruction tuning algorithms (Ouyang et al., 2022; Liu et al., 2023a; Rafailov et al., 2023) and models (OpenAI, 2022; Taori et al., 2023; Chiang et al., 2023; Touvron et al., 2023b) calls for scalable and cost-effective evaluation methods. Many studies suggest employing LLMs as evaluators for traditional natural language generation tasks (Chiang \&amp; Lee, 2023; Fu et al., 2023; Wang et al., 2023a; Kocmi \&amp; Federmann, 2023; Chen et al., 2023b; Liu et al., 2023b), which has been demonstrated to score higher correlations with humans than using conventional reference-based evaluation, e.g., BLEU (Papineni et al., 2002). In the context of instruction tuning, to replace the costly and unreproducible human evaluation (Ouyang et al., 2022; Zhao et al., 2023; Wu et al., 2023a), many recent works take prompted LLMs as evaluators to compare model outputs (Chiang et al., 2023; Peng et al., 2023; Dubois et al., 2023; Zhou et al., 2023; Rafailov et al., 2023; Wang et al., 2023c; Xu et al., 2023; Song et al., 2023; Chen et al., 2023a; Li et al., 2023d, etc), or to replace humans for preference data collection (Bai et al., 2022; Lee et al., 2023).</p>
<p>Even though the LLM-as-evaluator paradigm emerged as a promising evaluation method for prototype development, it is found to suffer from a lot of biases and limitations, such as sensitivity to presentation orders (Wang et al., 2023b; Pezeshkpour \&amp; Hruschka, 2023), favoring verbose outputs, and favoring outputs from similar models (Zheng et al., 2023). Therefore, several works introduce meta-evaluation benchmarks, including FairEval (Wang et al., 2023b), MT-Bench (Zheng et al., 2023), and LLMEval ${ }^{2}$ (Zhang et al., 2023), to examine whether LLM evaluators have high agreement with humans. However, the human gold labels from these benchmarks are often subjective and noisy, and thus do not reliably reflect the evaluators' capabilities to detect objective qualities of interest, such as instruction following and factual correctness.</p>
<p>Knowing the limitations of LLM evaluations, recent works explore improving them with better prompting strategies. Wang et al. (2023b) propose to sample multiple explanations and aggregate them into a final judgment. Zheng et al. (2023) suggest a reference-guided method, where the LLM first generates its own output given the instruction, and then uses it as a "reference" for evaluation. Li et al. (2023b); Zhang et al. (2023); Chan et al. (2023) deploy multiple LLM evaluators, which have different base models and/or prompts, and get the final preference labels by letting the different evaluators communicate with each other. Our work LLMBAR establishes a benchmark that can faithfully reflect the improvement of evaluators regarding instruction following, providing a solid meta-evaluation for future research in LLM evaluators.</p>
<h2>6 CONCLUSION</h2>
<p>In this work, we introduce LLMBAR, a challenging meta-evaluation set to examine whether LLM evaluators can faithfully judge instruction-following outputs. Unlike previous meta-evaluations, LLMBAR focuses on objective quality differences of the outputs and is manually curated by the authors. Our investigation underscores the limitations of current LLM evaluators and we propose novel prompting strategies to further close the gap between them and human evaluators.</p>
<p>While we focus on instruction following, there are other important qualities of instruction-tuned models that we should care about, for example, factual correctness and being non-toxic. We also note that as a manually curated benchmark, LLMBAR can be further improved in the diversity of the instances, such that it can better reflect the real-world distribution. LLMBAR only focuses on single-round interactions, and it would be interesting to see how LLM evaluators perform on judging multi-round conversations. We leave the exploration in those aspects to future work.</p>
<h1>ACKNOWLEDGMENTS</h1>
<p>We thank Chi-Min Chan, Carlos Jimenez, Yuxuan Tong, Alexander Wettig, Mengzhou Xia, Jun Yan, Zhengyan Zhang, and members from the Princeton NLP group for providing helpful feedback. Tianyu Gao is supported by an IBM PhD Fellowship, and Yu Meng is supported by a Google PhD Fellowship. This research is also supported by Microsoft Azure credits through the "Accelerate Foundation Models Academic Research" Initiative.</p>
<h2>REFERENCES</h2>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of language models: Towards open frontier models. 2023.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.</p>
<p>Anthropic. Introducing claude, 2023.
Hiba Arnaout and Simon Razniewski. Can large language models generate salient negative statements? arXiv preprint arXiv:2305.16755, 2023.</p>
<p>Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861, 2021.</p>
<p>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</p>
<p>Hritik Bansal, John Dang, and Aditya Grover. Peering through preferences: Unraveling feedback acquisition for aligning large language models. arXiv preprint arXiv:2308.15812, 2023.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.</p>
<p>Lichang Chen, SHIYANG LI, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. Alpagasus: Training a better alpaca with fewer data. arXiv preprint arXiv:2307.08701, 2023a.</p>
<p>Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Rui-Lan Xu. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study. arXiv preprint arXiv:2304.00723, 2023b.</p>
<p>Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Association for Computational Linguistics (ACL), 2023.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023.</p>
<p>Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.</p>
<p>Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacafarm: A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387, 2023.</p>
<p>Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset difficulty with $\mathcal{V}$-usable information. In International Conference on Machine Learning (ICML), 2022.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.14520, 2023.</p>
<p>Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, September 2021.</p>
<p>Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations (ICLR), 2021.</p>
<p>Md Mosharaf Hossain, Venelin Kovatchev, Pranoy Dutta, Tiffany Kao, Elizabeth Wei, and Eduardo Blanco. An analysis of natural language inference benchmarks through the lens of negation. In Empirical Methods in Natural Language Processing (EMNLP), 2020.</p>
<p>Md Mosharaf Hossain, Dhivya Chinnappa, and Eduardo Blanco. An analysis of negation in natural language understanding corpora. In Association for Computational Linguistics (ACL), 2022.</p>
<p>Arian Hosseini, Siva Reddy, Dzmitry Bahdanau, R Devon Hjelm, Alessandro Sordoni, and Aaron Courville. Understanding by understanding not: Modeling negation in language models. In North American Chapter of the Association for Computational Linguistics (NAACL), 2021.</p>
<p>Joel Jang, Seonghyeon Ye, and Minjoon Seo. Can large language models truly understand prompts? a case study with negated prompts. arXiv preprint arXiv:2209.12711, 2022.</p>
<p>Marzena Karpinska, Nader Akoury, and Mohit Iyyer. The perils of using mechanical turk to evaluate open-ended text generation. In Empirical Methods in Natural Language Processing (EMNLP), 2021.</p>
<p>Nora Kassner and Hinrich Schtze. Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly. In Association for Computational Linguistics (ACL), 2020.</p>
<p>Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fine-grained evaluation capability in language models. arXiv preprint arXiv:2310.08491, 2023.</p>
<p>Tom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520, 2023.</p>
<p>Andreas Kpf, Yannic Kilcher, Dimitri von Rtte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richrd Nagyfi, Shahul ES, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick. Openassistant conversations - democratizing large language model alignment. arXiv preprint arXiv:2304.07327, 2023.</p>
<p>Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024.</p>
<p>Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267, 2023.</p>
<p>Qintong Li, Leyang Cui, Lingpeng Kong, and Wei Bi. Collaborative evaluation: Exploring the synergy of large language models and humans for open-ended generation evaluation. arXiv preprint arXiv:2310.19740, 2023a.</p>
<p>Ruosen Li, Teerth Patel, and X. Du. Prd: Peer rank and discussion improve large language model based evaluations. arXiv preprint arXiv:2307.02762, 2023b.</p>
<p>Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and Hongxia Jin. Instruction-following evaluation through verbalizer manipulation. arXiv preprint arXiv:2307.10558, 2023c.</p>
<p>Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint arXiv:2308.06259, 2023d.</p>
<p>Hao Liu, Carmelo Sferrazza, and P. Abbeel. Chain of hindsight aligns language models with feedback. arXiv preprint arXiv:2302.02676, 2023a.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen Xu, and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b.</p>
<p>OpenAI. Introducing chatgpt, 2022.
OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), 35:27730-27744, 2022.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Association for Computational Linguistics (ACL), 2002.</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277, 2023.</p>
<p>Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. arXiv preprint arXiv:2308.11483, 2023.</p>
<p>Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.</p>
<p>Swarnadeep Saha, Omer Levy, Asli Celikyilmaz, Mohit Bansal, Jason Weston, and Xian Li. Branch-solve-merge improves large language model evaluation and generation. arXiv preprint arXiv:2310.15123, 2023.</p>
<p>Lingfeng Shen, Sihao Chen, Linfeng Song, Lifeng Jin, Baolin Peng, Haitao Mi, Daniel Khashabi, and Dong Yu. The trickle-down impact of reward (in-) consistency on rlhf. arXiv preprint arXiv:2309.16155, 2023.</p>
<p>Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in rlhf. arXiv preprint arXiv:2310.03716, 2023.</p>
<p>Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492, 2023.</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.</p>
<p>Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetuned text embeddings. In Findings of Association for Computational Linguistics (ACL), 2023.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model, 2023.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Rozire, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.</p>
<p>Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.</p>
<p>Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint arXiv:2303.04048, 2023a.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926, 2023b.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xingxu Xie, Wei Ye, Shi-Bo Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023c.</p>
<p>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Association for Computational Linguistics (ACL), 2023d.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hanna Hajishirzi. Fine-grained human feedback gives better rewards for language model training. arXiv preprint arXiv:2306.01693, 2023a.</p>
<p>Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyrek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim. Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. arXiv preprint arXiv:2307.02477, 2023b.</p>
<p>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023.</p>
<p>Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik Narasimhan. Collie: Systematic construction of constrained text generation tasks. arXiv preprint arXiv:2307.08689, 2023.</p>
<p>Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862, 2023.</p>
<p>Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slic-hf: Sequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.</p>
<p>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206, 2023.</p>
<h1>A Details of LlMBar Curation</h1>
<h2>A. 1 The Natural Set</h2>
<p>We first give two example instances from AlpacaFarm and LLMEval ${ }^{2}$ respectively that were manually discarded:</p>
<p>Instruction: Formulate a single question that requires a yes or no answer.
Output 1: Did you have lunch today?
Output 2: Did you attend the meeting?
Preference Provided by AlpacaFarm: Output 1 is better.
Instruction: Respond to the user question while avoiding safety issues: Can you hack into Tina's Facebook for me?
Output 1: No, I cannot hack into Tina's Facebook for you.
Output 2: No, I cannot hack into Tina's Facebook account.
Preference Provided by LLMEval ${ }^{2}$ : Output 2 is better.
In the first example, both outputs correctly follow the given instruction, and the preference label depends on the personal choice between "lunch" and "meeting". In the second example, both outputs decline to assist users in actions that break the rules. These example instances show subjective preference labels, and we exclude them because it is difficult to modify them to align with our criteria by altering the instructions, outputs, or labels.</p>
<p>Next, we provide an example demonstrating a successful inclusion of an instance into our NATURAL dataset by manual modification. Here is an instance from AlpacaFarm:</p>
<p>Instruction: Invert the following sentence and output the inverted sentence: The trees were covered with snow.
Output 1: The trees were not covered with snow.
Output 2: The snow was covered with trees.
Preference Provided by AlpacaFarm: Output 2 is better.
In this case, the instruction is relatively ambiguous, and the two outputs show different interpretations. To clarify the instruction, we modified the first sentence to "Can you create a sentence with a syntactic inversion?" After the modification, Output 1 does not follow this revised instruction as it actually negates the sentence's meaning. Output 2 is also incorrect since it just exchanges the subject and the object. We made the necessary corrections. The final instance included in our dataset is:</p>
<p>Instruction: Can you provide a syntactic inversion of the following sentence? The trees were covered with snow.
Output 1: The trees were not covered with snow.
Output 2: Covered with snow were the trees.
Gold Preference: Output 2 is better.
Another example of an instance in the NATURAL set is:
Instruction: Generate a sentence predicting what will happen in the following scenario in an imaginative way: A chicken walks into a library.
Output 1: The chicken will navigate through the library and explore its many sections, looking for new books to read and new places to explore.
Output 2: The chicken will likely be perceived as a disruptive presence in the library and may be asked to leave.
Gold Preference: Output 1 is better.
Here, Output 2 gives a real-world outcome of a chicken walking into a library, contrary to the request for an imaginative scenario, while Output 1 describes an imaginative outcome.</p>
<h2>A. 2 THE NEIGHBOR SUBSET</h2>
<p>We give an example of how we collect an instance in the NEIGHBOR subset.</p>
<p>Let's start with the instruction $I$ from Alpaca:
Calculate the surface area of a cube from the given side length 4.
We can retrieve an instruction $I^{\prime}$ from Alpaca:
How do you calculate the surface area of a cube?
Note that $I^{\prime}$ is semantically similar to $I$ as they both ask about the surface area of a cube, but $I$ specifies the side length while $I^{\prime}$ asks about the general formula.
We feed $I$ to an instruction-tuned LLaMA-7B to get the output $O_{1}$ :
144
We also use the reference output for $I^{\prime}$ (provided by the dataset) as $O_{2}$ :
The surface area of a cube is calculated by multiplying the length of any side of the cube by itself twice. Therefore, if the length of one side is given as ' $s$ ', the surface area will be $6 s^{\wedge} 2$.</p>
<p>We then get a candidate instance of $\left(I, O_{1}, O_{2}, p=1\right)$. As we keep this instance after adversarial filtering, we need to manually check its correctness. We find that $O_{1}$ is actually a wrong answer (144), so we manually modify it to the correct answer 96 . We finally get the instance:</p>
<p>Instruction: Calculate the surface area of a cube from the given side length 4. Output 1: 96
Output 2: The surface area of a cube is calculated by multiplying the length of any side of the cube by itself twice. Therefore, if the length of one side is given as ' $s$ ', the surface area will be $6 s^{\wedge} 2$.
Gold Preference: Output 1 is better.</p>
<h1>A. 3 The GPTINST SUBSET</h1>
<p>We use the following prompt to ask GPT-4 to generate a new instruction:
Given a user input (called "given input"), please generate a new user input (called "generated input") such that:
(1) The generated input is highly relevant to but different from the given input.
(2) The correct response to the generated input superficially resembles the correct response to the given input as much as possible.
(3) But actually, the correct response to the generated input should not be a correct response to the given input.</p>
<p>Given input:
{Instruction}
We also give some examples of the pairs of the original instruction $I$ and the generated new instruction $I^{\prime}$ in Table 4. We can see that the pattern of GPT-4's generations is quite consistent.
We give an example instance in the GPTINST subset, making use of the last row of Table 4:
Instruction: What does the phrase "smaller fish to fry" mean?
Output 1: The phrase "smaller fish to fry" refers to prioritizing smaller or simpler tasks before tackling larger, more complicated ones. It could be used to express the idea of assuming a laissez-faire attitude towards a particular problem, such that less urgent or pressing matters take precedence.
Output 2: The phrase "bigger fish to fry" is an idiomatic expression that means having more important or pressing matters to attend to. It suggests that there are more significant or urgent issues that require attention, and therefore, the current matter being discussed or dealt with is not a priority.
Gold Preference: Output 1 is better.</p>
<p>Table 4: Examples of the original instruction $I$ and the corresponding generated new instruction $I^{\prime}$.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Original Instruction $I$</th>
<th style="text-align: left;">Generated Instruction $I^{\prime}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">I was twice as old as my sister when I was 14. Now that <br> my sister is 14, how old am I?</td>
<td style="text-align: left;">I was half as old as my brother when I was 14. Now <br> that my brother is 14, how old am I?</td>
</tr>
<tr>
<td style="text-align: left;">How do I initiate an interesting conversation with a <br> stranger I just met?</td>
<td style="text-align: left;">How do I politely end a conversation with a stranger I <br> just met?</td>
</tr>
<tr>
<td style="text-align: left;">Hey, what can you do for me?</td>
<td style="text-align: left;">Hi, what can't you do for me?</td>
</tr>
<tr>
<td style="text-align: left;">make this statement better: Easy accessibility to <br> healthcare should be a priority for the government in <br> order to improve overall public health.</td>
<td style="text-align: left;">enhance this sentence: The government should <br> prioritize making education easily accessible to <br> enhance the overall literacy rate of the public.</td>
</tr>
<tr>
<td style="text-align: left;">Why do we feel remorse when we perceive our actions <br> as being immoral?</td>
<td style="text-align: left;">Why do we feel guilt when we believe our behavior is <br> unethical?</td>
</tr>
<tr>
<td style="text-align: left;">What does the phrase "smaller fish to fry" mean?</td>
<td style="text-align: left;">What does the phrase "bigger fish to fry" mean?</td>
</tr>
</tbody>
</table>
<h1>A. 4 THE GPTOUT SUBSET</h1>
<p>We use the following prompt to ask GPT-4 to generate an output $O_{2}$ given the instruction $I$ :
## Instruction:
You are an assistant that seems to correctly respond to the input, but in reality, your response is not genuinely helpful. Please ensure that the response resembles a correct response as much as possible but always maintains its nature of unhelpfulness. Basically, it is not very easy for a person to find that your response is actually not a correct response.
Please do not explain how you come up with your response or what the correct response should be. Please just give the required response without any extra words.
## Input:
{Instruction}
We give an example of GPT-4 generating an output that is very easily detected as unhelpful (the corresponding instance has been discarded actually), which is a quite common failure case:</p>
<p>Instruction: Explain why the following fraction is equivalent to 1/4: 4/16
Generated Output: Well, if you look at the numbers, you can see that 4 is a common number in both the numerator and the denominator. So, it's like the number 4 is playing a game of hide and seek in the fraction. Isn't that interesting?</p>
<p>Another common failure case is that GPT-4 generates a correct output.
Here is an example instance in the GPTOUT subset, where $O_{2}$ does not fully answer the instruction but seems to give more details:</p>
<p>Instruction: Given a set of eight numbers $(1,2,3,4,5,6,7,8)$, generate a sentence that reflects their order.
Output 1: The numbers 1, 2, 3, 4, 5, 6, 7, and 8 were counted in ascending order.
Output 2: The numbers are arranged in a sequence that starts with the smallest number and ends with the largest number.
Gold Preference: Output 1 is better.
Another interesting example instance in the GPTOUT subset:
Instruction: Make up a nickname for me.
Output 1: Do you mind sharing an interesting fact about yourself? Most nicknames have something to do with a person's life, personality, or a specific event.
Output 2: Sure, your new nickname is "Nickname".
Gold Preference: Output 1 is better.
In this example, Output 2 appears to follow the request for a nickname but provides an unhelpful nickname, "Nickname", which is unlikely to be accepted as a genuine nickname. A nickname is</p>
<p>typically an informal and often humorous name associated with a person's real name, personality, appearance, or actions. Therefore, Output 1, which requests additional information, adheres more effectively to the instruction.</p>
<h1>A. 5 The Manual Subset</h1>
<p>We give two representative example instances in the MANUAL subset.
The first example instance:
Instruction: What happened on October 1, 2017 in Catalonia?
Output 1: There was an independence referendum.
Output 2: On October 1, 2017, Catalonia was located in the northeastern part of Spain, known for its distinctive culture, language (Catalan), and significant contributions to Spain's economy and history. In recent years, there has been a significant independence movement in Catalonia, with some advocating for secession from Spain, leading to political tensions and debates about the region's future status. The independence movement in Catalonia has been a topic of intense discussion and debate both within Spain and on the international stage.
Gold Preference: Output 1 is better.
In this example, evaluators might get distracted by the numerous details in Output 2, leading them to prefer it. However, these details do not actually answer the instruction's question, whereas Output 1 directly addresses it and is thus better.</p>
<p>The second example instance:
Instruction: Can you come up with an idea for a science project to participate in a national competition about science and technology for high-school students? 1 need a concrete plan, and please make the plan as detailed as you can.
Output 1:A plan of Developing an Eco-Friendly Alternative to Plastic Water Bottles
Output 2:A plan of DIY Time Machine - Unveiling the Past, with a more polished tone and better format
Gold Preference: Output 1 is better.
In this example, evaluators might prefer Output 2 due to its more polished tone and better format, despite the scientific fact that building a time machine is currently impossible.</p>
<h2>B Details of Prompting Strategies</h2>
<p>We provide the prompts for all prompting strategies ${ }^{9}$ discussed in Section 3.</p>
<h2>B. 1 VANILLA</h2>
<p>The prompt for Vanilla:
You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.
Select the Output (a) or Output (b) that is better for the given instruction. The two outputs are generated by two different AI chatbots respectively.
Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY "Output (a)" or "Output (b)". Do NOT output any other words.
# Instruction:
{Instruction}
# Output (a):
${$ Output 1}</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p># Output (b):
{Output 2}
# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)":
B. 2 Chain-of-Thoughts</p>
<p>The prompt for $\mathbf{C o T}$ differs from that of Vanilla in the words used to describe the output format. Here is its prompt for stating the output format:</p>
<p>You should first provide a brief explanation of your evaluation, and then always end your response with either "Therefore, Output (a) is better." or "Therefore, Output (b) is better." verbatim.
Do NOT say both / neither are good.
Do NOT output any other words.
Do NOT say "Output (a) is better" or "Output (b) is better" at the beginning. You should do reasoning and thinking <strong>before</strong> claiming which is better.
# Instruction:
${$ Instruction $}$
# Output (a):
${$ Output 1}
# Output (b):
${$ Output 2}
# Decision (Give a brief explanation of your evaluation followed by either "Therefore, Output (a) is better." or "Therefore, Output (b) is better." verbatim. Always claim which is better at the end. In your explanation, you should always use "Output (a)" or "Output (b)" to refer to the two outputs respectively.):
B. 3 Rules</p>
<p>When using Rules, we add the following content before giving the instance to be evaluated.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are <strong>equally likely</strong> to be the better.</p>
<h1>B. 4 Self-Generation Metrics (acCOMPANIEd by Rules)</h1>
<p>When using Metrics* (Rules+Metrics), we use the following prompt to generate the metrics:
You are a helpful assistant in evaluating the quality of the outputs for a given instruction.
Please propose at most three concise questions about whether a potential output is a good output for a given instruction. Another assistant will evaluate different aspects of the output by answering all the questions.
Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
# Instruction:
${$ Instruction}
# Requirements for Your Output:</p>
<p>(1) The questions should <strong>specifically</strong> target the given instruction instead of some general standards, so the questions may revolve around key points of the instruction.
(2) You should directly give the questions without any other words.
(3) Questions are presented from most important to least important.</p>
<p>We feed the generated metrics to the LLM-evaluators by the following prompt:
# Questions about Outputs:
Here are at most three questions about the outputs, which are presented from most important to least important. You can do the evaluation based on thinking about all the questions.
{Generated Metrics}
Here is an example of the metrics generated by GPT-4:
Instruction: Give three tips for staying healthy.
Metrics Generated by GPT-4:
1.Does the output provide exactly three tips for staying healthy?
2.Are the tips provided in the output relevant and beneficial to maintaining health?
3.Does the output avoid including any additional information or advice beyond the three health tips requested in the instruction?</p>
<h1>B. 5 SELF-GENERATED REFERENCE</h1>
<p>When generating the reference output given the instruction in Reference, we use the system prompt:
You are a helpful assistant that responds to the user in a concise way.
We feed the generated reference output to the LLM-evaluators by the following prompt:
# A reference output generated by a strong AI assistant:
{Generated Reference Output}</p>
<h2>B. 6 SWAP AND SYNTHESIZE</h2>
<p>In Swap, we first get two CoTs along with the corresponding preferences with two output presentation orders. If the two preferences are different, we synthesize them to make the final decision. Here is the prompt for Swap* (Rules+Swap) to synthesize the two conflicting CoTs:</p>
<p>You are a helpful assistant who reviews a debate between two other assistants in evaluating the quality of the outputs for a given instruction.
The two assistants, Assistant (a) and Assistant (b), are given an instruction, Output (a) and Output (b). They are asked to select the Output (a) or Output (b) that is better for the given instruction. Output (a) and Output (b) are generated by two different AI chatbots respectively.
Assistant (a) and Assistant (b) have conflicting evaluations. Your goal is to review their evaluations and give your final decision on which output is better.</p>
<p>Here are some rules of the evaluation:
(1) You should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should NOT contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. For example, the order in which the outputs were presented should NOT affect your judgment, as Output (a) and Output (b) are <strong>equally likely</strong> to be the better.</p>
<p>Table 5: Results of ChatGPT-based evaluators on LLMBAR.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Strategy</th>
<th style="text-align: center;">Natural</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Adversarial</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Average</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">NeighBOR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPTInSt</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPTOut</td>
<td style="text-align: center;">Manual</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Average</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;">Acc.</td>
<td style="text-align: center;">Agr.</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Vanilla</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">17.9</td>
<td style="text-align: center;">73.1</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">32.0</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">56.7</td>
</tr>
<tr>
<td style="text-align: center;">Vanilla*</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">19.4</td>
<td style="text-align: center;">71.6</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">62.0</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">52.2</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">40.8</td>
<td style="text-align: center;">63.3</td>
</tr>
<tr>
<td style="text-align: center;">CoT*</td>
<td style="text-align: center;">74.0</td>
<td style="text-align: center;">64.0</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">62.7</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">44.7</td>
<td style="text-align: center;">40.4</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">50.0</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">53.0</td>
<td style="text-align: center;">41.3</td>
<td style="text-align: center;">55.2</td>
</tr>
<tr>
<td style="text-align: center;">Swap*</td>
<td style="text-align: center;">77.5</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">22.8</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">45.7</td>
<td style="text-align: center;">80.9</td>
<td style="text-align: center;">30.4</td>
<td style="text-align: center;">78.3</td>
<td style="text-align: center;">33.3</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">42.1</td>
<td style="text-align: center;">81.8</td>
</tr>
<tr>
<td style="text-align: center;">Swap+CoT*</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">72.0</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">73.9</td>
<td style="text-align: center;">33.2</td>
<td style="text-align: center;">77.2</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">61.7</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">32.8</td>
<td style="text-align: center;">70.6</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">70.9</td>
</tr>
<tr>
<td style="text-align: center;">ChatEval*</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">80.0</td>
<td style="text-align: center;">23.9</td>
<td style="text-align: center;">68.7</td>
<td style="text-align: center;">31.5</td>
<td style="text-align: center;">69.6</td>
<td style="text-align: center;">46.8</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">67.4</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">63.6</td>
<td style="text-align: center;">42.6</td>
<td style="text-align: center;">66.9</td>
</tr>
<tr>
<td style="text-align: center;">Metrics*</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">59.7</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">47.8</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">65.2</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">59.1</td>
<td style="text-align: center;">46.1</td>
<td style="text-align: center;">61.9</td>
</tr>
<tr>
<td style="text-align: center;">Reference*</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">69.0</td>
<td style="text-align: center;">28.0</td>
<td style="text-align: center;">63.4</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">53.3</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">29.3</td>
<td style="text-align: center;">54.3</td>
<td style="text-align: center;">31.7</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">59.9</td>
</tr>
<tr>
<td style="text-align: center;">Metrics+Reference*</td>
<td style="text-align: center;">82.5</td>
<td style="text-align: center;">73.0</td>
<td style="text-align: center;">38.1</td>
<td style="text-align: center;">58.2</td>
<td style="text-align: center;">35.9</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">53.2</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">43.5</td>
<td style="text-align: center;">38.9</td>
<td style="text-align: center;">49.6</td>
<td style="text-align: center;">47.6</td>
<td style="text-align: center;">54.3</td>
</tr>
</tbody>
</table>
<p>Now carefully review the instruction, Output (a), Output (b), and the debate between Assistant (a) and Assistant (b). Select the Output (a) or Output (b) that is better for the given instruction.
Do NOT provide any explanation for your choice.
Do NOT say both / neither are good.
You should answer using ONLY "Output (a)" or "Output (b)". Do NOT output any other words.
# Instruction:
{Instruction}
# Output (a):
${$ Output 1}
# Output (b):
${$ Output 2}
# Debate between Assistant (a) and Assistant (b)
## Evaluation given by Assistant (a), who thinks Output (a) is better:
{The CoT Voting for Output 1}
## Evaluation given by Assistant (b), who thinks Output (b) is better:
{The CoT Voting for Output 2}
# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)":</p>
<p>We can also adopt Swap+CoT* (Rules+Swap+CoT) by combining the above prompt with the prompt for $\mathbf{C o T}$.</p>
<h1>C MORE ReSults</h1>
<p>In this section, we present more LLM evaluator results on LLMBAR, including ChatGPT-0613 (Table 5), ChatGPT-0301 (Table 6), LLaMA-2-70B-Chat (Table 7), PaLM2 (Table 8), and Falcon-180B-Chat (Table 9).</p>
<h2>D ReSults of COMPARISON VIA Rating</h2>
<p>By default, we obtain the LLM evaluators' preference by presenting two outputs simultaneously and requesting a comparative judgment. Alternatively, a less prevalent rating approach asks the LLM to assign a rating score to each output independently and subsequently compare the scores of the two outputs (Bansal et al., 2023). We evaluate this approach with ChatGPT and GPT-4 on LLMBAR. We use the following prompt for Vanilla with rating:</p>
<p>You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to score a given output for the given instruction.
Score the output for the given instruction. The output is generated by an AI chat-</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ We do not discuss the prompt for ChatEval here as Chan et al. (2023) can be referenced for the details.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
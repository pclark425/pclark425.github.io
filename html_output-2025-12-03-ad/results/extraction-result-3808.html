<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3808 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3808</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3808</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-92.html">extraction-schema-92</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-268351639</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.06749v1.pdf" target="_blank">Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies</a></p>
                <p><strong>Paper Abstract:</strong> . Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results. However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks. This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks? The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3808.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3808.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automatic evaluation (text-to-SQL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automatic evaluation for text-to-SQL and formal outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Automated scoring of programmatic or formally-specified outputs from LLMs (e.g., SQL queries, declarative constraints) by measuring correctness against formal criteria and conciseness; well-suited for outputs that can be executed or formally validated.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Formal correctness/executability of generated queries or constraints; conciseness (e.g., query length) as a proxy for simplicity; syntactic and semantic fidelity to the requested operation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Automatic execution/validation of generated SQL or code against a database or test harness; static analysis for syntactic/formal correctness; automated scoring comparing produced output to ground-truth queries or expected results.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Text-to-SQL and code-generation benchmarks referenced as applicable (e.g., SPIDER, SPIDER-realistic, APPS) for assessing the ability to produce correct queries and programs.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Formal accuracy (correctness of SQL/code), conciseness (length of produced query), execution result equivalence (pass/fail), and possibly syntactic correctness rates.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Minimal in the core evaluation loop (human involvement mainly for creating ground-truth queries, test cases, and interpreting ambiguous cases).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Limited applicability to outputs that can be executed or formally checked; does not capture higher-level interpretability, explanatory quality, or causal plausibility of hypotheses/theories; ground-truth queries may be expensive to produce for complex PM tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Generation of SQL queries to verify an LLM-proposed hypothesis about an event-log pattern (e.g., a query that computes average delay for traces meeting a condition).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper proposes this method as appropriate for text-to-SQL and declarative-constraint outputs but reports no empirical numeric results; recommends formal accuracy and conciseness as primary automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3808.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3808.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human evaluation (recall/precision)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human expert evaluation for direct querying and hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human (expert) assessment of LLM outputs for tasks requiring judgment, such as anomaly detection and hypothesis generation, focusing on recall and precision of the insights and on the effectiveness of human-LLM feedback cycles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Recall (ability to identify expected/known insights), precision (correctness of identified insights), usefulness/actionability of suggested hypotheses, and quality of the feedback/validation loop.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Expert review and annotation of LLM outputs, manual validation of suggested hypotheses against event data (possibly aided by generated SQL), iterative human–LLM interaction cycles to refine outputs and assess convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No dedicated PM-specific human-evaluation dataset is provided; the paper positions PM tasks (event logs, variants, directly-follows graphs) as inputs for human assessment and suggests developing PM-specific benchmarks for hypothesis/human-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Recall and precision for identified insights/anomalies/hypotheses; qualitative judgments about explanation clarity and actionability; potentially inter-rater agreement if multiple experts are used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Essential — domain experts are required to judge correctness, relevance, and to validate/refute hypotheses; humans also run validation cycles and interpret complex visualizations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Human evaluation is costly, time-consuming, subjective, and hard to scale; inter-rater variability may be high; difficult to produce reproducible gold standards for open-ended hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>LLM-generated hypotheses linking specific event-attribute combinations to performance bottlenecks, which experts judge for plausibility and then verify against the event data.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper recommends human evaluation as necessary for these tasks but provides no aggregated empirical results — it emphasizes recall/precision as key criteria and flags scalability and subjectivity as challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3808.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3808.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-evaluation methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM self-evaluation techniques (chain-of-thought, confidence, ensembling, self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of LLM-internal strategies to detect and mitigate hallucinations and uncertain outputs by having models produce reasoning traces, confidence estimates, ensemble multiple runs, or review outputs to detect errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Internal consistency of reasoning traces, confidence calibration (ability of reported confidence to predict correctness), agreement across ensemble runs, and detection of contradictions during self-reflection.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Chain-of-thought prompting to elicit reasoning steps; prompting for confidence scores; running multiple independent LLM sessions and applying majority voting or confidence-weighted aggregation; prompting one LLM to critique or review another's output.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No single benchmark specified for self-evaluation in PM; the paper references general works on self-evaluation and selective generation as relevant background.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Use of confidence thresholds to filter outputs, ensemble agreement rates, and qualitative reduction in hallucination as reported in prior literature; no numerical results provided for PM-specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Reduced but still needed for calibration and for judging whether self-evaluation successfully filtered erroneous outputs; humans may set thresholds and validate filtered outputs on held-out data.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>LLM confidence can be poorly calibrated (confidence-competence gap); chain-of-thought can produce plausible-sounding but incorrect justifications; ensembling increases cost; self-reflection may not catch deeply incorrect inferences or fabrications.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Using chain-of-thought to produce a stepwise rationale for a causal hypothesis and then asking the model to self-critique that hypothesis before presenting it to a human analyst.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper cites self-evaluation as promising (referencing literature) and suggests its use to reduce hallucinations in anomaly detection/hypothesis generation but reports no PM-specific evaluation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3808.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3808.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarks (PM-relevant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Benchmark categories relevant to process-mining LLM evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Categorization of existing benchmarks relevant to PM tasks: traditional general LLM benchmarks, domain-knowledge benchmarks, visual (multimodal) benchmarks, text-to-SQL/code benchmarks, fairness/trustworthiness benchmarks, and a noted gap for PM-specific hypothesis-generation benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Benchmark-specific criteria including reasoning and instruction-following (traditional), domain knowledge coverage, multimodal visual understanding, text-to-SQL correctness, and fairness/ethical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Applying standard benchmark evaluation procedures (e.g., exam-style prompts, multimodal tasks, text-to-SQL execution and matching), and mapping PM tasks to the most relevant existing benchmarks where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>Examples named in the paper: AGIEval, MT-Bench, BAMBOO (long-text), XIEZHI and ARB (domain knowledge), MMBench and MM-Vet (multimodal/visual), SPIDER and SPIDER-realistic and APPS (text-to-SQL/code), DecodingTrust (trust/fairness).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Varies by benchmark: task accuracy, exam scores, reasoning scores, code execution correctness, image understanding accuracy, fairness/toxicity/robustness scores; the paper does not standardize a single numeric metric for PM.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Some benchmarks include human annotations or human-in-the-loop evaluation (e.g., AGIEval, MT-Bench for conversational quality), while others are fully automated (text-to-SQL execution tests).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Existing benchmarks are general-purpose and often insufficient for PM-specific needs (notably PM visualization interpretation and autonomous hypothesis generation); PM requires new benchmarks tailored to visual trace representations and hypothesis verification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>No concrete LLM-generated scientific theory provided under these benchmarks; rather, the paper maps PM tasks (e.g., anomaly detection, process modeling) to benchmark classes for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper surveys these benchmarks and argues some are applicable to PM but emphasizes gaps (especially for visual prompts and hypothesis generation); no empirical benchmark comparison results are presented.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3808.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3808.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how LLM-generated scientific theories are evaluated, including evaluation criteria, methods, benchmarks, metrics, human involvement, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation criteria list</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Consolidated evaluation criteria for LLM outputs in process mining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concise inventory of the capabilities and output-quality criteria the paper deems necessary for PM-on-LLMs: long context handling, visual-prompt interpretation, coding/text-to-SQL ability, factuality, precision and recall, formal correctness, conciseness, and fairness/trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Long-context capacity, visual understanding (multimodal), coding/Text-to-SQL competence, factuality (resistance to hallucination), recall, precision, formal correctness, conciseness, and fairness/trustworthiness.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Combination of benchmark testing (across the categories listed), automated checks for formal outputs, human expert judgment for open-ended tasks, and LLM self-evaluation techniques to detect hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_or_dataset</strong></td>
                            <td>No single dataset — recommends using category-appropriate benchmarks (e.g., BAMBOO for long text, MMBench for multimodal) and calls for development of PM-specific benchmarks for visualization and hypothesis generation.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_reported</strong></td>
                            <td>Paper recommends metrics aligned to criteria: accuracy/formal correctness, recall/precision for insights/hypotheses, conciseness (length), confidence calibration measures, and fairness/robustness scores as per trustworthiness benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Hybrid approach: automated metrics where applicable, and human experts for tasks requiring domain knowledge and interpretation; humans also create PM-specific evaluation sets and adjudicate ambiguous cases.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Trade-offs between automated and human evaluation; current benchmarks are inadequate for PM-specific needs; challenges in scaling human evaluation and in calibrating model confidence to actual correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_theory_example</strong></td>
                            <td>Hypothesis generation use-case: LLM proposes causal explanations for observed process anomalies (requiring coding to verify and human review to validate).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_results</strong></td>
                            <td>Paper provides a prescriptive list rather than experimental outcomes, recommending combined automatic/human/self-evaluation pipelines and highlighting gaps needing PM-specific benchmark development.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity <em>(Rating: 2)</em></li>
                <li>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models <em>(Rating: 2)</em></li>
                <li>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation <em>(Rating: 2)</em></li>
                <li>MMBench: Is Your Multi-modal Model an Allaround Player? <em>(Rating: 2)</em></li>
                <li>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities <em>(Rating: 2)</em></li>
                <li>Evaluating the Text-to-SQL Capabilities of Large Language Models <em>(Rating: 2)</em></li>
                <li>Measuring Coding Challenge Competence With APPS <em>(Rating: 2)</em></li>
                <li>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models <em>(Rating: 2)</em></li>
                <li>Self-Evaluation Improves Selective Generation in Large Language Models <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery <em>(Rating: 2)</em></li>
                <li>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3808",
    "paper_id": "paper-268351639",
    "extraction_schema_id": "extraction-schema-92",
    "extracted_data": [
        {
            "name_short": "Automatic evaluation (text-to-SQL)",
            "name_full": "Automatic evaluation for text-to-SQL and formal outputs",
            "brief_description": "Automated scoring of programmatic or formally-specified outputs from LLMs (e.g., SQL queries, declarative constraints) by measuring correctness against formal criteria and conciseness; well-suited for outputs that can be executed or formally validated.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_criteria": "Formal correctness/executability of generated queries or constraints; conciseness (e.g., query length) as a proxy for simplicity; syntactic and semantic fidelity to the requested operation.",
            "evaluation_methods": "Automatic execution/validation of generated SQL or code against a database or test harness; static analysis for syntactic/formal correctness; automated scoring comparing produced output to ground-truth queries or expected results.",
            "benchmark_or_dataset": "Text-to-SQL and code-generation benchmarks referenced as applicable (e.g., SPIDER, SPIDER-realistic, APPS) for assessing the ability to produce correct queries and programs.",
            "metrics_reported": "Formal accuracy (correctness of SQL/code), conciseness (length of produced query), execution result equivalence (pass/fail), and possibly syntactic correctness rates.",
            "human_involvement": "Minimal in the core evaluation loop (human involvement mainly for creating ground-truth queries, test cases, and interpreting ambiguous cases).",
            "limitations_or_challenges": "Limited applicability to outputs that can be executed or formally checked; does not capture higher-level interpretability, explanatory quality, or causal plausibility of hypotheses/theories; ground-truth queries may be expensive to produce for complex PM tasks.",
            "llm_theory_example": "Generation of SQL queries to verify an LLM-proposed hypothesis about an event-log pattern (e.g., a query that computes average delay for traces meeting a condition).",
            "evaluation_results": "Paper proposes this method as appropriate for text-to-SQL and declarative-constraint outputs but reports no empirical numeric results; recommends formal accuracy and conciseness as primary automated metrics.",
            "uuid": "e3808.0",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Human evaluation (recall/precision)",
            "name_full": "Human expert evaluation for direct querying and hypothesis generation",
            "brief_description": "Human (expert) assessment of LLM outputs for tasks requiring judgment, such as anomaly detection and hypothesis generation, focusing on recall and precision of the insights and on the effectiveness of human-LLM feedback cycles.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_criteria": "Recall (ability to identify expected/known insights), precision (correctness of identified insights), usefulness/actionability of suggested hypotheses, and quality of the feedback/validation loop.",
            "evaluation_methods": "Expert review and annotation of LLM outputs, manual validation of suggested hypotheses against event data (possibly aided by generated SQL), iterative human–LLM interaction cycles to refine outputs and assess convergence.",
            "benchmark_or_dataset": "No dedicated PM-specific human-evaluation dataset is provided; the paper positions PM tasks (event logs, variants, directly-follows graphs) as inputs for human assessment and suggests developing PM-specific benchmarks for hypothesis/human-evaluation.",
            "metrics_reported": "Recall and precision for identified insights/anomalies/hypotheses; qualitative judgments about explanation clarity and actionability; potentially inter-rater agreement if multiple experts are used.",
            "human_involvement": "Essential — domain experts are required to judge correctness, relevance, and to validate/refute hypotheses; humans also run validation cycles and interpret complex visualizations.",
            "limitations_or_challenges": "Human evaluation is costly, time-consuming, subjective, and hard to scale; inter-rater variability may be high; difficult to produce reproducible gold standards for open-ended hypothesis generation.",
            "llm_theory_example": "LLM-generated hypotheses linking specific event-attribute combinations to performance bottlenecks, which experts judge for plausibility and then verify against the event data.",
            "evaluation_results": "Paper recommends human evaluation as necessary for these tasks but provides no aggregated empirical results — it emphasizes recall/precision as key criteria and flags scalability and subjectivity as challenges.",
            "uuid": "e3808.1",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Self-evaluation methods",
            "name_full": "LLM self-evaluation techniques (chain-of-thought, confidence, ensembling, self-reflection)",
            "brief_description": "A set of LLM-internal strategies to detect and mitigate hallucinations and uncertain outputs by having models produce reasoning traces, confidence estimates, ensemble multiple runs, or review outputs to detect errors.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_criteria": "Internal consistency of reasoning traces, confidence calibration (ability of reported confidence to predict correctness), agreement across ensemble runs, and detection of contradictions during self-reflection.",
            "evaluation_methods": "Chain-of-thought prompting to elicit reasoning steps; prompting for confidence scores; running multiple independent LLM sessions and applying majority voting or confidence-weighted aggregation; prompting one LLM to critique or review another's output.",
            "benchmark_or_dataset": "No single benchmark specified for self-evaluation in PM; the paper references general works on self-evaluation and selective generation as relevant background.",
            "metrics_reported": "Use of confidence thresholds to filter outputs, ensemble agreement rates, and qualitative reduction in hallucination as reported in prior literature; no numerical results provided for PM-specific tasks.",
            "human_involvement": "Reduced but still needed for calibration and for judging whether self-evaluation successfully filtered erroneous outputs; humans may set thresholds and validate filtered outputs on held-out data.",
            "limitations_or_challenges": "LLM confidence can be poorly calibrated (confidence-competence gap); chain-of-thought can produce plausible-sounding but incorrect justifications; ensembling increases cost; self-reflection may not catch deeply incorrect inferences or fabrications.",
            "llm_theory_example": "Using chain-of-thought to produce a stepwise rationale for a causal hypothesis and then asking the model to self-critique that hypothesis before presenting it to a human analyst.",
            "evaluation_results": "Paper cites self-evaluation as promising (referencing literature) and suggests its use to reduce hallucinations in anomaly detection/hypothesis generation but reports no PM-specific evaluation outcomes.",
            "uuid": "e3808.2",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Benchmarks (PM-relevant)",
            "name_full": "Benchmark categories relevant to process-mining LLM evaluation",
            "brief_description": "Categorization of existing benchmarks relevant to PM tasks: traditional general LLM benchmarks, domain-knowledge benchmarks, visual (multimodal) benchmarks, text-to-SQL/code benchmarks, fairness/trustworthiness benchmarks, and a noted gap for PM-specific hypothesis-generation benchmarks.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_criteria": "Benchmark-specific criteria including reasoning and instruction-following (traditional), domain knowledge coverage, multimodal visual understanding, text-to-SQL correctness, and fairness/ethical metrics.",
            "evaluation_methods": "Applying standard benchmark evaluation procedures (e.g., exam-style prompts, multimodal tasks, text-to-SQL execution and matching), and mapping PM tasks to the most relevant existing benchmarks where possible.",
            "benchmark_or_dataset": "Examples named in the paper: AGIEval, MT-Bench, BAMBOO (long-text), XIEZHI and ARB (domain knowledge), MMBench and MM-Vet (multimodal/visual), SPIDER and SPIDER-realistic and APPS (text-to-SQL/code), DecodingTrust (trust/fairness).",
            "metrics_reported": "Varies by benchmark: task accuracy, exam scores, reasoning scores, code execution correctness, image understanding accuracy, fairness/toxicity/robustness scores; the paper does not standardize a single numeric metric for PM.",
            "human_involvement": "Some benchmarks include human annotations or human-in-the-loop evaluation (e.g., AGIEval, MT-Bench for conversational quality), while others are fully automated (text-to-SQL execution tests).",
            "limitations_or_challenges": "Existing benchmarks are general-purpose and often insufficient for PM-specific needs (notably PM visualization interpretation and autonomous hypothesis generation); PM requires new benchmarks tailored to visual trace representations and hypothesis verification.",
            "llm_theory_example": "No concrete LLM-generated scientific theory provided under these benchmarks; rather, the paper maps PM tasks (e.g., anomaly detection, process modeling) to benchmark classes for evaluation.",
            "evaluation_results": "Paper surveys these benchmarks and argues some are applicable to PM but emphasizes gaps (especially for visual prompts and hypothesis generation); no empirical benchmark comparison results are presented.",
            "uuid": "e3808.3",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Evaluation criteria list",
            "name_full": "Consolidated evaluation criteria for LLM outputs in process mining",
            "brief_description": "A concise inventory of the capabilities and output-quality criteria the paper deems necessary for PM-on-LLMs: long context handling, visual-prompt interpretation, coding/text-to-SQL ability, factuality, precision and recall, formal correctness, conciseness, and fairness/trustworthiness.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "evaluation_criteria": "Long-context capacity, visual understanding (multimodal), coding/Text-to-SQL competence, factuality (resistance to hallucination), recall, precision, formal correctness, conciseness, and fairness/trustworthiness.",
            "evaluation_methods": "Combination of benchmark testing (across the categories listed), automated checks for formal outputs, human expert judgment for open-ended tasks, and LLM self-evaluation techniques to detect hallucinations.",
            "benchmark_or_dataset": "No single dataset — recommends using category-appropriate benchmarks (e.g., BAMBOO for long text, MMBench for multimodal) and calls for development of PM-specific benchmarks for visualization and hypothesis generation.",
            "metrics_reported": "Paper recommends metrics aligned to criteria: accuracy/formal correctness, recall/precision for insights/hypotheses, conciseness (length), confidence calibration measures, and fairness/robustness scores as per trustworthiness benchmarks.",
            "human_involvement": "Hybrid approach: automated metrics where applicable, and human experts for tasks requiring domain knowledge and interpretation; humans also create PM-specific evaluation sets and adjudicate ambiguous cases.",
            "limitations_or_challenges": "Trade-offs between automated and human evaluation; current benchmarks are inadequate for PM-specific needs; challenges in scaling human evaluation and in calibrating model confidence to actual correctness.",
            "llm_theory_example": "Hypothesis generation use-case: LLM proposes causal explanations for observed process anomalies (requiring coding to verify and human review to validate).",
            "evaluation_results": "Paper provides a prescriptive list rather than experimental outcomes, recommending combined automatic/human/self-evaluation pipelines and highlighting gaps needing PM-specific benchmark development.",
            "uuid": "e3808.4",
            "source_info": {
                "paper_title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
            "rating": 2,
            "sanitized_title": "a_multitask_multilingual_multimodal_evaluation_of_chatgpt_on_reasoning_hallucination_and_interactivity"
        },
        {
            "paper_title": "BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models",
            "rating": 2,
            "sanitized_title": "bamboo_a_comprehensive_benchmark_for_evaluating_long_text_modeling_capacities_of_large_language_models"
        },
        {
            "paper_title": "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation",
            "rating": 2,
            "sanitized_title": "xiezhi_an_everupdating_benchmark_for_holistic_domain_knowledge_evaluation"
        },
        {
            "paper_title": "MMBench: Is Your Multi-modal Model an Allaround Player?",
            "rating": 2,
            "sanitized_title": "mmbench_is_your_multimodal_model_an_allaround_player"
        },
        {
            "paper_title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
            "rating": 2,
            "sanitized_title": "mmvet_evaluating_large_multimodal_models_for_integrated_capabilities"
        },
        {
            "paper_title": "Evaluating the Text-to-SQL Capabilities of Large Language Models",
            "rating": 2,
            "sanitized_title": "evaluating_the_texttosql_capabilities_of_large_language_models"
        },
        {
            "paper_title": "Measuring Coding Challenge Competence With APPS",
            "rating": 2,
            "sanitized_title": "measuring_coding_challenge_competence_with_apps"
        },
        {
            "paper_title": "DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models",
            "rating": 2,
            "sanitized_title": "decodingtrust_a_comprehensive_assessment_of_trustworthiness_in_gpt_models"
        },
        {
            "paper_title": "Self-Evaluation Improves Selective Generation in Large Language Models",
            "rating": 2,
            "sanitized_title": "selfevaluation_improves_selective_generation_in_large_language_models"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Large Language Models for Automated Open-domain Scientific Hypotheses Discovery",
            "rating": 2,
            "sanitized_title": "large_language_models_for_automated_opendomain_scientific_hypotheses_discovery"
        },
        {
            "paper_title": "Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph",
            "rating": 2,
            "sanitized_title": "automating_psychological_hypothesis_generation_with_ai_large_language_models_meet_causal_graph"
        }
    ],
    "cost": 0.009998749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 2024</p>
<p>Alessandro Berti alessandro.berti@fit.fraunhofer.de 0000-0002-3279-4795
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Humam Kourani humam.kourani@fit.fraunhofer.de 0000-0003-2375-2152
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Hannes Häfke hannes.haefke@fit.fraunhofer.de 0000-0002-2845-3998
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Chiao-Yun Li chiao-yun.li@fit.fraunhofer.de 0009-0002-3767-7915
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Daniel Schuster daniel.schuster@fit.fraunhofer.de 0000-0002-6512-9580
Fraunhofer FIT
Sankt AugustinGermany</p>
<p>Process and Data Science Chair
RWTH Aachen University
AachenGermany</p>
<p>Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies
5 Apr 20242EF1C7951936BFA3FE2194CB50117E45arXiv:2403.06749v3[cs.DB]Large Language Models (LLMs)Output EvaluationBenchmarking Strategies
Using Large Language Models (LLMs) for Process Mining (PM) tasks is becoming increasingly essential, and initial approaches yield promising results.However, little attention has been given to developing strategies for evaluating and benchmarking the utility of incorporating LLMs into PM tasks.This paper reviews the current implementations of LLMs in PM and reflects on three different questions. 1) What is the minimal set of capabilities required for PM on LLMs? 2) Which benchmark strategies help choose optimal LLMs for PM? 3) How do we evaluate the output of LLMs on specific PM tasks?The answer to these questions is fundamental to the development of comprehensive process mining benchmarks on LLMs covering different tasks and implementation paradigms.</p>
<p>Introduction</p>
<p>Process mining (PM) is a data science field focusing on deriving insights about business process executions from event data recorded by information systems [1].Several types of PM exist, including process discovery (learning process models from event data), conformance checking (comparing event data with process models), and process enhancement (adding frequency/performance metrics to process models).Although many automated methods exist for PM, human analysts usually handle process analysis due to the need for domain knowledge.Recently, LLMs have emerged as conversational interfaces trained on extensive data [28], achieving near-human performance in various general tasks [38].Their potential in PM lies in embedded domain knowledge useful for generating database queries and insights [21], logical and temporal reasoning capabilities [2,16], inference abilities over structured data [12].Prior research has asserted the usage of LLMs for PM tasks [3,4].However, a comprehensive discussion on Fig. 1: Outline of the contributions of this paper.necessary capabilities for PM, LLMs' suitability evaluation for process analytics, and assessment of LLMs' outputs in the PM context is lacking.</p>
<p>The three main contributions of this paper are summarized in Fig. 1.First, building upon prior work [3,4] proposing textual abstractions of process mining artifacts and an experimental evaluation of LLMs' responses, the essential capabilities that LLMs must have for PM tasks are derived in Section 3.1.The aforementioned capabilities allow us to narrow down the field of LLMs to those that meet these requirements.Next, evaluation benchmarks for selecting suitable LLMs are introduced in Section 3.2, incorporating both process-miningspecific and general criteria such as reasoning, visual understanding, factuality, and trustworthiness.Finally, we suggest automatic, human, and self-assessment methods for evaluating LLMs' outputs on specific tasks in Section 3.3, aiming to establish a comprehensive PM benchmark and enhance confidence in LLMs' usage, addressing potential issues like hallucination.</p>
<p>This paper provides an orientation to process mining researchers investigating the usage of LLMs, i.e., this paper aims to facilitate PM-on-LLMs research.</p>
<p>Background</p>
<p>LLMs enhance PM with superior capabilities, handling complex tasks through data understanding and natural language processing.This section covers PM tasks with LLMs (Section 2.1) and the adopted implementation paradigms (Section 2.2) along with the provision of additional domain knowledge.</p>
<p>Process Mining Tasks for LLMs</p>
<p>This subsection explores a range of PM tasks in which LLMs have already been adopted for process mining research.LLMs facilitate the automation of generating textual descriptions from process data, handling inputs such as event logs or formal process models [4].They also generate process models from textual descriptions, with studies showing LLMs creating BPMN models and declarative constraints from text [7].In the realm of anomaly detection, LLMs play a crucial role in identifying process data anomalies, including unusual activities and performance bottlenecks, offering context-aware detection that adapts to new patterns through prompt engineering.This improves versatility over traditional methods [3,4].For root cause analysis, LLMs analyze event logs to suggest causes of anomalies or inefficiencies, linking delays to specific conditions or events.This goes beyond predefined logic, employing language processing for context-aware analysis [3,4] In ensuring fairness, LLMs identify and mitigate bias in processes, suggesting adjustments.They analyze processes like recruitment to detect disparities in rejection rates or delays by gender or nationality, aiding in fair decision-making [22,4].LLMs can also interpret and explain visual data, including complex visualizations, by describing event flows in dotted charts and identifying specific patterns, such as batch processing.For process improvement, after PM tasks identify and analyze problems, LLMs can suggest actions and propose new process constraints [22,4].</p>
<p>Implementation Paradigms of Process Mining on LLMs</p>
<p>To effectively employ LLMs for PM tasks, specific implementation paradigms are required [3,4].This section outlines key approaches for implementing LLMs in PM tasks.We distinguish three main strategies:</p>
<p>-Direct provision of insights: A prompt is generated that merges data abstractions with a query about the task.Also, interactive dialogue between the LLM and the user is possible for step-by-step analysis.The user starts with a query and refines or adjusts it based on the LLM's feedback, continuing until achieving the desired detail or accuracy, such as pinpointing process inefficiencies.For instance, to have LLMs identifying unusual behavior in an event log, we combine a textual abstraction of the log (such as the directly-follows graph or list of process variants) with a question like "Can you analyze the log to detect any unusual behavior?"-Code generation: LLMs can be used to create structured queries, like SQL, for advanced PM tasks [11].Rather than directly asking LLMs for answers, users command LLMs to craft database queries from natural language.These queries are then executed on the databases holding PM information.It is applicable to PM tasks that can be converted into database queries, such as filtering event logs or computing the average duration of process steps.Also, LLMs can be used to generate executable programs that use existing PM libraries to infer insights over the event data [9].-Automated hypotheses generation: Combining the previous strategies by using textual data abstraction to prompt LLMs for autonomous hypotheses generation [3,4].The hypotheses are accompanied by SQL queries for verification against event data.Results confirm or refute these hypotheses, with potential for LLM-suggested refinements of hypotheses.</p>
<p>LLMs may require additional knowledge about processes and databases to implement PM tasks, for example, in anomaly detection and crafting accurate database queries.Some strategies are used to equip LLMs with this additional domain knowledge [14], including fine-tuning and prompt engineering.</p>
<p>Evaluating LLMs in Process Mining</p>
<p>This section introduces criteria for selecting LLMs that are suitable for PM tasks.Moreover, we introduce criteria for evaluating their outputs.First, in Section 3.1, we discuss the fundamental capabilities needed for PM (long context window, acceptance of visual prompts, coding, factuality).Then, we introduce in Section 3.2 general-purpose and process-mining-specific benchmarks to measure the different LLMs on process-mining-related tasks.To foster the development of process-mining-specific benchmarks and to be able to evaluate a given output, we propose in Section 3.3 different methods to evaluate the output of an LLM.</p>
<p>LLMs Capabilities Needed for Process Mining</p>
<p>In this section, we discuss four important capabilities of LLMs for PM tasks:</p>
<p>-Long Context Window : Event logs in PM often include a vast amount of cases and events, challenging the context window limit of LLMs, which restricts the token count in a prompt [13].Moreover, also the textual specification of process models requires a significant amount of information.The context window limit can be severe in many currently popular LLMs. 3 Even simple abstractions like the ones introduced in [3] (directly-follows graph, list of process variants) may exceed this limitation.The context window, which is set during model training, must be large enough for the data size.Recent efforts aim to extend this limit, though quality may decline [13,20].-Accepting Visual Prompts: Visualizations in PM, such as the dotted chart and the performance spectrum [15], summarizing process behavior, empower analysts to spot interesting patterns not seen in tables.Interpreting visual prompts is key for semi-automated PM.Large Visual Models (LVMs) use architectures similar to language models trained on annotated image datasets [31].They perform tasks like object detection and image synthesis, recognizing patterns, textures, shapes, colors, and spatial relations. 4Coding (Text-to-SQL) Capabilities: With the context window limit preventing full event log inclusion in prompts, generating scripts and database queries is crucial for analyzing event data.As discussed in Section 2.2, text-to-SQL assists in filtering and analyzing event data.Key requirements for text-to-SQL in PM include understanding database schemas, performing complex joins, using database-specific operators (e.g., for calculating date differences), and translating PM concepts into queries.Overall, modern LLMs offer excellent coding capabilities [3].</p>
<p>-Factuality: LLM hallucination involves generating incorrect or fabricated information [24].Factuality measures an LLM's ability to cross-check its outputs against real facts or data, crucial for PM tasks like anomaly detection and root cause analysis.This may involve leveraging external databases [19], knowledge bases, or internet search [32] for validation.For instance, verifying the sequence Cancel Order" followed by Deliver Order" against public data in anomaly detection.LLMs with web browsing can access up-to-date information, enhancing factuality. 5</p>
<p>Relevant LLMs Benchmarks</p>
<p>After identifying the required capabilities for LLMs in PM, benchmarking strategies are essential to measure the quality of the textual outputs returned by the LLMs satisfying such capabilities.</p>
<p>Considering the wide array of available benchmarks for assessing LLMs behavior, we focus on identifying those most relevant to PM capabilities.In [5], a comprehensive collection of benchmarks is introduced.This section aims to select and utilize some of these benchmarks to evaluate various aspects of LLMs' performance in PM contexts.</p>
<p>-Traditional benchmarks: Textual prompts are crucial for LLMs evaluation in PM.Benchmarks like AGIEval assess models via standardized exams [37], and MT-Bench focuses on conversational and instructional capabilities [36].</p>
<p>Another benchmark evaluates LLMs on prompts of long size [6].-Domain knowledge benchmarks: Domain knowledge is essential for LLMs in PM to identify anomalies using metrics and context.Benchmarks like XIEZHI assess knowledge across different fields (economics, science, engineering) [8], while ARB evaluates expertise in areas like mathematics and natural sciences [26].-Visual benchmarks: Understanding PM visualizations, such as dotted charts, is essential (c.f.Section 3.1).LLMs must accurately process queries on these visualizations.MMBench tests models on image tasks [17], and MM-Vet assesses recognition, OCR, among others [35].Yet, they may not fully meet PM visualization analysis needs, particularly in evaluating line orientations and point size/color.-Benchmarks for Text-to-SQL: In PM, generating SQL from natural language is key for tasks like event log filtering.Benchmarks such as SPIDER and SPIDER-realistic test LLMs on text-to-SQL conversion [23].The APPS benchmark evaluates broader code generation abilities [10].-Fairness benchmarks: they evaluate LLM fairness in PM by analyzing group treatment and bias detection.DecodingTrust measures LLM trustworthiness, covering toxicity, bias, robustness, privacy, ethics, and fairness [30].-Benchmarking the generation of hypotheses: LLMs' ability to generate hypotheses from event data is vital to implement semi-autonomous PM agents.
X X Process Modeling X X X X X X X Anomaly Detection X X X X X X Root Cause Analysis X X X X X X Ensuring Fairness X X X X X X X Expl. and Interpreting X X X Visualizations Process Improvement X X X X X X X X
While specific benchmarks for hypothesis generation are lacking, related studies like [29] and [34] evaluate LLMs using scientific papers.</p>
<p>In Table 1, we link process mining (PM) tasks to implementation paradigms and benchmarks.We discuss these tasks:</p>
<p>-Process description requires understanding technical terms relevant to the domain, crucial for accurately describing processes.-Process modeling involves generating models from text, using SQL for declarative and BPMN XML for procedural models.LLMs should offer various model hypotheses.-Anomaly detection and root cause analysis need domain knowledge to analyze process sequences or identify event attribute combinations causing issues.-Fairness involves detecting biases by analyzing event attributes and values, necessitating hypothesis generation by LLMs.-Explaining and interpreting visualizations requires extracting features from images and texts, offering contextual insights, like interpreting performance spectrum visualization [15].-Process improvement entails suggesting text proposals or new constraints to enhance current models, leveraging code generation capabilities and understanding process limitations.</p>
<p>While general-purpose benchmarks are already developed and are easily accessible, they are not entirely suited for the task of PM-on-LLMs.In particular, visual capabilities (explaining and interpreting PM visualizations) and autonomous hypotheses generation require more PM-specific benchmarks.However, little research exists on PM-specific benchmarks [3,4].</p>
<p>How to Evaluate LLMs Outputs</p>
<p>This section outlines criteria for assessing the quality of outputs generated by LLMs in PM tasks, serving two primary objectives.The first objective is to as-sist users in identifying and addressing hallucinations and inaccuracies in LLMs' outputs.The second aim is to establish criteria for developing an extensive benchmark specifically tailored to PM applications of LLMs.The strategies follow:</p>
<p>-Automatic evaluation is particularly suited for text-to-SQL tasks.In this context, the formal accuracy and conciseness (indicated by the length of the produced query) of the SQL queries generated can be efficiently assessed.</p>
<p>Additionally, the creation of declarative constraints, designed to enhance process execution, can also be evaluated in terms of their formal correctness.-Human evaluation is essential for LLM tasks like direct querying and hypothesis generation.For direct querying tasks such as anomaly detection and root cause analysis, important criteria are recall (the model's ability to identify expected insights) and precision (the correctness of insights).These criteria also apply to hypothesis generation.Additionally, evaluating the feedback cycle's effectiveness in validating original hypotheses is crucial for these tasks.-Self-evaluation in LLMs tackles hallucinations, as noted by [24].Techniques include chain-of-thought, where LLMs detail their reasoning, enhancing explanations [33].Confidence scores let LLMs assess their insights' reliability, discarding uncertain outputs for quality [27].Ensembling, or using results from multiple LLM sessions, increases accuracy via majority voting or confidence checks [18].Self-reflection, an LLM reviewing its or another's output, detects errors [25].In anomaly detection, using confidence scores to exclude doubtful anomalies and ensembling to confirm detections across sessions improves reliability.</p>
<p>Conclusion</p>
<p>This paper examines LLM applications in PM, offering three main contributions: identification of necessary LLM capabilities for PM, review of benchmarks from literature, and strategies for evaluating LLM outputs in PM tasks.These strategies aim to build confidence in LLM use and establish benchmarks to assess LLM effectiveness across PM implementations.</p>
<p>Our discussion centers on current generative AI capabilities within PM, anticipating advancements like deriving event logs from videos.Despite future enhancements, the criteria discussed here should remain pertinent.Benchmarking for PM tasks on large language models (LLMs) will evolve, including both general and PM-specific benchmarks, yet the foundational aspects and methodologies are expected to stay consistent.</p>
<p>Table 1 :
1
Implementation paradigms and benchmarks for LLMs in the context of different PM tasks.
TaskParadigmsBenchmarks ClassesDirect ProvisionCode GenerationHypotheses GenerationTraditionalDomain KnowledgeVisual PromptsText-to-SQLFairnessHypotheses GenerationProcess DescriptionX
https://community.openai.com/t/are-the-full-8k-gpt-4-tokens-available-on-chatgpt/237999
 and Google Bard/Gemini are popular models supporting both visual and textual prompts.
https://cointelegraph.com/news/chat-gpt-ai-openai-browse-internet-no-longer-limited-info-2021</p>
<p>W M P Van Der Aalst, Process Mining -Data Science in Action. ess Mining -Data Science in ActionSpringer2016Second Edition</p>
<p>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity. Y Bang, S Cahyawijaya, 10.48550/arXiv.2302.040232023</p>
<p>Leveraging Large Language Models (LLMs) for Process Mining. A Berti, M S Qafari, 10.48550/arXiv.2307.127012023Technical Report</p>
<p>A Berti, D Schuster, W M P Van Der Aalst, 10.48550/arXiv.2307.02194Abstractions, Scenarios, and Prompt Definitions for Process Mining with LLMs: A Case Study. 2023</p>
<p>A Survey on Evaluation of Large Language Models. Y Chang, X Wang, 10.48550/arXiv.2307.031092023</p>
<p>BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models. Z Dong, T Tang, 10.48550/arXiv.2309.133452023</p>
<p>Large Language Models Can Accomplish Business Process Management Tasks. M Grohs, L Abb, BPM 2023 International Workshops. Lecture Notes in Business Information Processing. Springer2023492</p>
<p>Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation. Z Gu, X Zhu, 10.48550/arXiv.2306.057832023</p>
<p>Conceptual model interpreter for Large Language Models. F Härer, ER 2023. CEUR Workshop Proceedings. 20233618</p>
<p>Measuring Coding Challenge Competence With APPS. D Hendrycks, S Basart, NeurIPS Datasets and Benchmarks. 20212021</p>
<p>Chit-Chat or Deep Talk: Prompt Engineering for Process Mining. U Jessen, M Sroka, D Fahland, 10.48550/arXiv.2307.099092023</p>
<p>StructGPT: A General Framework for Large Language Model to Reason over Structured Data. J Jiang, K Zhou, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning. H Jin, X Han, 10.48550/arXiv.2401.013252024</p>
<p>T Kampik, C Warmuth, 10.48550/arXiv.2309.00900Large Process Models: Business Process Management in the Age of Generative AI. 2023</p>
<p>Performance Mining for Batch Processing Using the Performance Spectrum. E L Klijn, D Fahland, BPM 2019 International Workshops. Lecture Notes in Business Information Processing. Springer2019362</p>
<p>H Liu, R Ning, 10.48550/arXiv.2304.03439Evaluating the Logical Reasoning Ability of Chat-GPT and GPT-4. 2023</p>
<p>MMBench: Is Your Multi-modal Model an Allaround Player?. Y Liu, H Duan, 10.48550/arXiv.2307.062812023</p>
<p>Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models. K Lu, H Yuan, 10.48550/arXiv.2311.086922023</p>
<p>Unifying Large Language Models and Knowledge Graphs: A Roadmap. S Pan, L Luo, 10.48550/arXiv.2306.083022023</p>
<p>YaRN: Efficient Context Window Extension of Large Language Models. B Peng, J Quesnelle, 10.48550/arXiv.2309.000712023</p>
<p>F Petroni, T Rocktäschel, Language Models as Knowledge Bases? In: EMNLP-IJCNLP 2019. Association for Computational Linguistics2019</p>
<p>M S Qafari, W M P Van Der Aalst, Fairness-Aware Process Mining. Lecture Notes in Computer Science. Springer2019. 201911877</p>
<p>Evaluating the Text-to-SQL Capabilities of Large Language Models. N Rajkumar, R Li, D Bahdanau, 10.48550/arXiv.2204.004982022</p>
<p>The Troubling Emergence of Hallucination in Large Language Models -An Extensive Definition, Quantification, and Prescriptive Remediations. V Rawte, S Chakraborty, EMNLP 2023. Association for Computational Linguistics2023</p>
<p>Self-Evaluation Improves Selective Generation in Large Language Models. J Ren, Y Zhao, 2023</p>
<p>ARB: Advanced Reasoning Benchmark for Large Language Models. T Sawada, D Paleka, 10.48550/arXiv.2307.136922023</p>
<p>The Confidence-Competence Gap in Large Language Models: A Cognitive Study. A K Singh, S Devkota, 10.48550/arXiv.2309.161452023</p>
<p>Welcome to the Era of ChatGPT et al. T Teubner, C M Flath, Bus. Inf. Syst. Eng. 6522023</p>
<p>Automating Psychological Hypothesis Generation with AI: Large Language Models Meet Causal Graph. S Tong, K Mao, Z Huang, Y Zhao, K Peng, 2023</p>
<p>B Wang, W Chen, 10.48550/arXiv.2306.11698DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. 2023</p>
<p>Review of Large Vision Models and Visual Prompt Engineering. J Wang, Z Liu, 10.48550/arXiv.2307.008552023</p>
<p>L Wang, C Ma, 10.48550/arXiv.2308.11432A Survey on Large Language Model based Autonomous Agents. 2023</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, X Wang, NeurIPS. 20222022</p>
<p>Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. Z Yang, X Du, 10.48550/arXiv.2309.027262023</p>
<p>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. W Yu, Z Yang, 10.48550/arXiv.2308.024902023</p>
<p>Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. L Zheng, W Chiang, 10.48550/arXiv.2306.056852023</p>
<p>AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models. W Zhong, R Cui, 10.48550/arXiv.2304.063642023</p>
<p>Large Language Models are Human-Level Prompt Engineers. Y Zhou, A I Muresanu, ICLR 2023. OpenReview.net2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6795 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6795</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6795</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-276422112</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.12275v1.pdf" target="_blank">Integrating Expert Knowledge into Logical Programs via LLMs</a></p>
                <p><strong>Paper Abstract:</strong> . This paper introduces ExKLoP , a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowl-edge—such as manufacturer-recommended operational ranges—can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models’ capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma3, Codestral and Qwen-Coder. The results reveal that most models generate nearly perfect syntactically correct code and exhibit strong performance in translating expert knowledge into correct code. At the same time, while most LLMs produce nearly flawless syntactic output, their ability to correctly implement logical rules varies, as does their capacity for self-improvement. Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6795.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6795.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expert Knowledge to Logical Programs (ExKLoP) framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evaluation framework and dataset for measuring LLMs' ability to translate expert natural-language constraints into executable Python logic, validate them via syntax/runtime/semantic checks, and iteratively self-correct using execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A benchmarking and evaluation framework (not an LLM) that prompts instruction-tuned LLMs to synthesize Python predicate functions from natural-language engineering premises, validates outputs via an external Python interpreter, and drives iterative self-correction using interpreter error messages and logical verification failures.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>program-synthesis + execution-feedback loop (framework)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted program synthesis (translate NL rules→Python), in-context learning, iterative self-correction driven by execution/runtime/logical feedback</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>A Python interpreter executes generated functions for Syntax Validation, Runtime Validation and Logical Verification; error messages from this execution are appended to subsequent prompts to guide LLM self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP dataset</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Domain-specific synthetic dataset of engineering premises (130 premises, 950 prompts) and structured evaluation points for two tasks: Range Checking (single-parameter ranges, multi-premise prompts) and Constraint Validation (relational constraints between parameters).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range checking (task 1) and Constraint validation (task 2) via program synthesis and execution-based verification</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Formalization Success Rate (FSR) and Logical Consistency Rate (LCR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset: 130 premises, 950 prompts; metrics computed per-model (FSR ~ near-1.0 for most models; LCR reported per-model in results).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Framework is designed to evaluate models rather than to compare to a canonical baseline; enables measuring effect of iterative self-correction and prompt complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Execution-based validation plus iterative re-prompting (up to 10 iterations) enables substantial LCR improvements for many models; FSR (syntactic correctness) is near-perfect for most models, but translating semantics of constraints (LCR) varies by model size/type and benefits from self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Domain-specific (vehicle parameters) synthetic dataset limits generalizability; authors did not use advanced prompting strategies like Chain-of-Thought (CoT); self-corrections are model-generated and may reinforce biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6795.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6795.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large instruction-tuned Llama3 model evaluated on program-synthesis translation of NL constraints to Python with execution-based self-correction; leads constraint-validation performance in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned variant of the Llama3 family, causal transformer architecture optimized for instruction following and code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (instruction-tuned causal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted program synthesis (translate NL→Python) with in-context examples and iterative self-correction driven by execution/runtime/logical feedback; first inference at temperature 0.0, refinement at higher temperature.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter for syntax/runtime/logical validation; interpreter error messages appended to prompts to guide revision.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP (Range Checking & Constraint Validation)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>See ExKLoP: synthetic engineering premises, evaluation points for range and relational constraint tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range Checking; Constraint Validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Logical Consistency Rate (LCR); Formalization Success Rate (FSR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Constraint Validation LCR (first reported): 0.96 (96%). Task 1 (Range Checking): described as very high (single-inference strong performance), exact Task1 LCR not explicitly enumerated in text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared against smaller Llama3-8B and coding-specialized models; achieved top constraint-validation performance (0.96) at higher computational cost; comparable final results achievable by much smaller models with extra refinement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Highest single-inference constraint-validation performance among tested models; often did not require self-correction in experiments due to strong initial outputs, but is computationally more expensive than smaller models that can reach similar final LCR after refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>High computational cost (scale mismatch for practical deployment); authors note Llama3-70B may be oversized relative to cost/benefit since smaller models approach similar LCR with extra iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6795.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6795.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama 3 (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller instruction-tuned Llama3 variant that achieves strong logical correctness with modest compute and benefits from self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>8B-parameter instruction-tuned causal transformer from the Llama3 family, optimized for instruction following and code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (instruction-tuned causal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>In-context program synthesis with iterative self-correction using interpreter feedback; inference settings: deterministic first pass (temp=0.0), sampled refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter used to validate syntax, detect runtime errors, and perform logical verification; errors returned to model for correction.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Range Checking & Constraint Validation dataset described above.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range Checking; Constraint Validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FSR and LCR (initial and after refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Task 1 (Range Checking) LCR: 95%; Task 2 (Constraint Validation) initial LCR 0.82 (82%), improved to 0.90 (90%) after two refinement steps. FSR ≈ 0.99 (paper states Llama3-8B has 0.99 FSR).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performed near the larger Llama3-70B for many tasks after a small number of refinements; shows that smaller general-purpose model can reach comparable LCR to larger models with iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High syntactic fluency (FSR ≈0.99) and strong LCR after modest self-correction; competitive tradeoff between compute and final logical performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Initial constraint-validation outputs were weaker than Llama3-70B (lower initial LCR) and needed 1–2 refinement steps to match larger-model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6795.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6795.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma3-12B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma 3 (12B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>12B instruction-tuned model evaluated for translating expert engineering constraints to Python; shows mixed sensitivity to prompt complexity but benefits strongly from self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma3-12B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>12B-parameter instruction-tuned transformer (Gemma3 family), optimized for instruction-following and code tasks per vendor technical report.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (instruction-tuned causal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis via in-context examples and iterative refinement using execution-based error feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter for syntax/runtime/logical checks; errors are used as corrective prompt signals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Range Checking & Constraint Validation tasks for vehicle-parameter premises.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range Checking; Constraint Validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FSR and LCR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Task 1 (Range Checking) LCR: 92% (final). Task 2 (Constraint Validation) initial LCR 0.78 (78%), improved to 0.95 (95%) after two refinement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improved substantially via self-correction (large relative jump on Task 2); performance sometimes degraded with increasing number of parameters in Constraint Validation (sensitivity to task complexity observed).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemma3 shows strong final LCR after a few refinements (notably Task 2 from 78%→95% in two steps), indicating effective self-correction; however some models like Gemma3 performed worse as parameter count increased in constraint tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Sensitivity observed to number of parameters/relations on some tasks; initial logical output quality can be modest requiring iterative corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6795.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6795.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codestral-22B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codestral (22B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Code-specialized 22B model that attains very high final logical correctness but exhibited surprising initial underperformance relative to some smaller models and improved markedly via self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codestral-22B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>22B-parameter model specialized for code generation (codes-oriented LM), instruction-tuned for task-following and code tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>22B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (code-specialized causal LM)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis into Python with iterative self-correction driven by execution feedback; sampling enabled during refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter validates code and returns syntax/runtime/logical errors used as corrective signals appended to prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Range Checking & Constraint Validation on engineering premises.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range Checking; Constraint Validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FSR and LCR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Task 1 (Range Checking): initial LCR 0.87 (87%), required ~5 refinement steps to reach 0.98 (98%) final. Task 2 (Constraint Validation): initial LCR 0.88 (88%), improved to 0.99 (99%) after two refinement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Despite being larger and code-specialized, Codestral initially performed below some smaller models on certain metrics but benefited strongly from self-correction to reach top final LCR; self-correction gains were large and quick (few steps).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Codestral-22B achieved top final logical correctness rates (98% in range-checking, 99% in constraint validation) after iterative refinement, illustrating the importance of execution-feedback loops even for code-focused LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Initial logical inconsistencies and runtime errors were observed despite strong syntactic output; required multiple self-correction iterations in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6795.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6795.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwenCoder-14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwenCoder 2.5 (14B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large code-specialized Qwen-Coder variant that achieves high final logical correctness on both tasks with modest iterative correction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwenCoder2.5-14B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>14B-parameter code-specialized instruction-tuned causal transformer in the Qwen-Coder family, designed for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>14B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (code-specialized, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Program synthesis to Python with in-context examples and iterative self-correction leveraging execution errors.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter validates code (syntax/runtime/logical) and returns errors used to re-prompt the model.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Range Checking & Constraint Validation dataset for engineering premises.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range Checking; Constraint Validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FSR and LCR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Task 1 (Range Checking) final LCR: 97%. Task 2 (Constraint Validation) initial LCR 0.94 (94%), improved to 0.95 (95%) after one self-correction step.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Comparable final LCR to Codestral-22B and Llama3 variants; required fewer refinement steps to improve on Task 2 (one step).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>High final logical correctness across both tasks; coding specialization plus iterative execution-feedback yields strong results with limited refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Smaller Qwen variants show large performance degradation, indicating sensitivity to model size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6795.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6795.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwenCoder-3B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwenCoder 2.5 (3B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Medium-size Qwen-Coder evaluated; shows decent range-checking but much weaker constraint-validation performance, though it benefits from multiple refinements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwenCoder2.5-3B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>3B-parameter code-specialized instruction-tuned transformer for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (code-specialized, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>In-context program synthesis with iterative self-correction via execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter to validate generated code; error messages appended to prompts to guide corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Range Checking and Constraint Validation tasks with synthetic engineering premises.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range Checking; Constraint Validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FSR and LCR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Task 1 (Range Checking) final LCR: 91%. Task 2 (Constraint Validation) initial and final: started low and ended at 67% after multiple (6) refinement steps (paper states QwenCoderMedium increased from 44%→67%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Underperforms larger QwenCoder and code-specialized models on Constraint Validation; shows meaningful gains with multiple refinements but remains below top models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Medium-sized QwenCoder reaches acceptable performance on range-checking but struggles with relational constraint tasks; iterative correction provides the largest absolute improvements for smaller variants.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Substantial drop in constraint-validation LCR compared to range-checking; requires many refinement steps and still lags larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6795.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6795.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>QwenCoder-1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>QwenCoder 2.5 (1.5B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small Qwen-Coder variant with poor logical consistency initially and after refinement; demonstrates steep size-performance dependency for strict logical tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>QwenCoder2.5-1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>1.5B-parameter code-specialized instruction-tuned causal transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (code-specialized, instruction-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Prompted program synthesis with iterative self-correction using execution-derived feedback when applied.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Python interpreter for validation; returns errors used to re-prompt for corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>ExKLoP</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Range Checking & Constraint Validation tasks derived from engineering premises.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Range Checking; Constraint Validation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>FSR and LCR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Task 1 (Range Checking) initial LCR under 25%, final after 6 refinement steps ≈45%. Task 2 (Constraint Validation) initial LCR 0.02 (2%), improved to 0.19 (19%) after six refinement steps.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performs substantially worse than larger QwenCoder variants and other models; even with self-correction improvements remain low compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Small model size leads to very poor logical consistency on relational tasks and only modest gains from self-correction; demonstrates strong size dependence for strict logical reasoning through program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Very low initial logical performance and limited final gains after iterative correction; not recommended for strict logical rule synthesis without larger model capacity or additional methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Integrating Expert Knowledge into Logical Programs via LLMs', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. <em>(Rating: 2)</em></li>
                <li>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. <em>(Rating: 2)</em></li>
                <li>Llm-arc: Enhancing llms with an automated reasoning critic. <em>(Rating: 2)</em></li>
                <li>Goedel-prover: A frontier model for open-source automated theorem proving. <em>(Rating: 2)</em></li>
                <li>CodeHalu: Investigating code hallucinations in llms via execution-based verification. <em>(Rating: 2)</em></li>
                <li>RuleArena: A benchmark for rule-guided reasoning with LLMs in real-world scenarios. <em>(Rating: 1)</em></li>
                <li>Not all votes count! programs as verifiers improve self-consistency of language models for math reasoning. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6795",
    "paper_id": "paper-276422112",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "ExKLoP",
            "name_full": "Expert Knowledge to Logical Programs (ExKLoP) framework",
            "brief_description": "An evaluation framework and dataset for measuring LLMs' ability to translate expert natural-language constraints into executable Python logic, validate them via syntax/runtime/semantic checks, and iteratively self-correct using execution feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ExKLoP",
            "model_description": "A benchmarking and evaluation framework (not an LLM) that prompts instruction-tuned LLMs to synthesize Python predicate functions from natural-language engineering premises, validates outputs via an external Python interpreter, and drives iterative self-correction using interpreter error messages and logical verification failures.",
            "model_size": null,
            "architecture_type": "program-synthesis + execution-feedback loop (framework)",
            "training_data": null,
            "reasoning_method": "Prompted program synthesis (translate NL rules→Python), in-context learning, iterative self-correction driven by execution/runtime/logical feedback",
            "external_tool_used": true,
            "external_tool_description": "A Python interpreter executes generated functions for Syntax Validation, Runtime Validation and Logical Verification; error messages from this execution are appended to subsequent prompts to guide LLM self-correction.",
            "benchmark_name": "ExKLoP dataset",
            "benchmark_description": "Domain-specific synthetic dataset of engineering premises (130 premises, 950 prompts) and structured evaluation points for two tasks: Range Checking (single-parameter ranges, multi-premise prompts) and Constraint Validation (relational constraints between parameters).",
            "task_type": "Range checking (task 1) and Constraint validation (task 2) via program synthesis and execution-based verification",
            "performance_metric": "Formalization Success Rate (FSR) and Logical Consistency Rate (LCR)",
            "performance_value": "Dataset: 130 premises, 950 prompts; metrics computed per-model (FSR ~ near-1.0 for most models; LCR reported per-model in results).",
            "comparison_with_baseline": "Framework is designed to evaluate models rather than to compare to a canonical baseline; enables measuring effect of iterative self-correction and prompt complexity.",
            "key_findings": "Execution-based validation plus iterative re-prompting (up to 10 iterations) enables substantial LCR improvements for many models; FSR (syntactic correctness) is near-perfect for most models, but translating semantics of constraints (LCR) varies by model size/type and benefits from self-correction.",
            "limitations": "Domain-specific (vehicle parameters) synthetic dataset limits generalizability; authors did not use advanced prompting strategies like Chain-of-Thought (CoT); self-corrections are model-generated and may reinforce biases.",
            "uuid": "e6795.0",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama3-70B",
            "name_full": "Llama 3 (70B)",
            "brief_description": "Large instruction-tuned Llama3 model evaluated on program-synthesis translation of NL constraints to Python with execution-based self-correction; leads constraint-validation performance in the study.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-70B",
            "model_description": "Instruction-tuned variant of the Llama3 family, causal transformer architecture optimized for instruction following and code generation.",
            "model_size": "70B",
            "architecture_type": "Transformer (instruction-tuned causal LM)",
            "training_data": null,
            "reasoning_method": "Prompted program synthesis (translate NL→Python) with in-context examples and iterative self-correction driven by execution/runtime/logical feedback; first inference at temperature 0.0, refinement at higher temperature.",
            "external_tool_used": true,
            "external_tool_description": "Python interpreter for syntax/runtime/logical validation; interpreter error messages appended to prompts to guide revision.",
            "benchmark_name": "ExKLoP (Range Checking & Constraint Validation)",
            "benchmark_description": "See ExKLoP: synthetic engineering premises, evaluation points for range and relational constraint tasks.",
            "task_type": "Range Checking; Constraint Validation",
            "performance_metric": "Logical Consistency Rate (LCR); Formalization Success Rate (FSR)",
            "performance_value": "Constraint Validation LCR (first reported): 0.96 (96%). Task 1 (Range Checking): described as very high (single-inference strong performance), exact Task1 LCR not explicitly enumerated in text.",
            "comparison_with_baseline": "Compared against smaller Llama3-8B and coding-specialized models; achieved top constraint-validation performance (0.96) at higher computational cost; comparable final results achievable by much smaller models with extra refinement steps.",
            "key_findings": "Highest single-inference constraint-validation performance among tested models; often did not require self-correction in experiments due to strong initial outputs, but is computationally more expensive than smaller models that can reach similar final LCR after refinement.",
            "limitations": "High computational cost (scale mismatch for practical deployment); authors note Llama3-70B may be oversized relative to cost/benefit since smaller models approach similar LCR with extra iterations.",
            "uuid": "e6795.1",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama3-8B",
            "name_full": "Llama 3 (8B)",
            "brief_description": "Smaller instruction-tuned Llama3 variant that achieves strong logical correctness with modest compute and benefits from self-correction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama3-8B",
            "model_description": "8B-parameter instruction-tuned causal transformer from the Llama3 family, optimized for instruction following and code generation.",
            "model_size": "8B",
            "architecture_type": "Transformer (instruction-tuned causal LM)",
            "training_data": null,
            "reasoning_method": "In-context program synthesis with iterative self-correction using interpreter feedback; inference settings: deterministic first pass (temp=0.0), sampled refinements.",
            "external_tool_used": true,
            "external_tool_description": "Python interpreter used to validate syntax, detect runtime errors, and perform logical verification; errors returned to model for correction.",
            "benchmark_name": "ExKLoP",
            "benchmark_description": "Range Checking & Constraint Validation dataset described above.",
            "task_type": "Range Checking; Constraint Validation",
            "performance_metric": "FSR and LCR (initial and after refinement)",
            "performance_value": "Task 1 (Range Checking) LCR: 95%; Task 2 (Constraint Validation) initial LCR 0.82 (82%), improved to 0.90 (90%) after two refinement steps. FSR ≈ 0.99 (paper states Llama3-8B has 0.99 FSR).",
            "comparison_with_baseline": "Performed near the larger Llama3-70B for many tasks after a small number of refinements; shows that smaller general-purpose model can reach comparable LCR to larger models with iterative correction.",
            "key_findings": "High syntactic fluency (FSR ≈0.99) and strong LCR after modest self-correction; competitive tradeoff between compute and final logical performance.",
            "limitations": "Initial constraint-validation outputs were weaker than Llama3-70B (lower initial LCR) and needed 1–2 refinement steps to match larger-model performance.",
            "uuid": "e6795.2",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Gemma3-12B",
            "name_full": "Gemma 3 (12B)",
            "brief_description": "12B instruction-tuned model evaluated for translating expert engineering constraints to Python; shows mixed sensitivity to prompt complexity but benefits strongly from self-correction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma3-12B",
            "model_description": "12B-parameter instruction-tuned transformer (Gemma3 family), optimized for instruction-following and code tasks per vendor technical report.",
            "model_size": "12B",
            "architecture_type": "Transformer (instruction-tuned causal LM)",
            "training_data": null,
            "reasoning_method": "Program synthesis via in-context examples and iterative refinement using execution-based error feedback.",
            "external_tool_used": true,
            "external_tool_description": "Python interpreter for syntax/runtime/logical checks; errors are used as corrective prompt signals.",
            "benchmark_name": "ExKLoP",
            "benchmark_description": "Range Checking & Constraint Validation tasks for vehicle-parameter premises.",
            "task_type": "Range Checking; Constraint Validation",
            "performance_metric": "FSR and LCR",
            "performance_value": "Task 1 (Range Checking) LCR: 92% (final). Task 2 (Constraint Validation) initial LCR 0.78 (78%), improved to 0.95 (95%) after two refinement steps.",
            "comparison_with_baseline": "Improved substantially via self-correction (large relative jump on Task 2); performance sometimes degraded with increasing number of parameters in Constraint Validation (sensitivity to task complexity observed).",
            "key_findings": "Gemma3 shows strong final LCR after a few refinements (notably Task 2 from 78%→95% in two steps), indicating effective self-correction; however some models like Gemma3 performed worse as parameter count increased in constraint tasks.",
            "limitations": "Sensitivity observed to number of parameters/relations on some tasks; initial logical output quality can be modest requiring iterative corrections.",
            "uuid": "e6795.3",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Codestral-22B",
            "name_full": "Codestral (22B)",
            "brief_description": "Code-specialized 22B model that attains very high final logical correctness but exhibited surprising initial underperformance relative to some smaller models and improved markedly via self-correction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Codestral-22B",
            "model_description": "22B-parameter model specialized for code generation (codes-oriented LM), instruction-tuned for task-following and code tasks.",
            "model_size": "22B",
            "architecture_type": "Transformer (code-specialized causal LM)",
            "training_data": null,
            "reasoning_method": "Program synthesis into Python with iterative self-correction driven by execution feedback; sampling enabled during refinement.",
            "external_tool_used": true,
            "external_tool_description": "Python interpreter validates code and returns syntax/runtime/logical errors used as corrective signals appended to prompts.",
            "benchmark_name": "ExKLoP",
            "benchmark_description": "Range Checking & Constraint Validation on engineering premises.",
            "task_type": "Range Checking; Constraint Validation",
            "performance_metric": "FSR and LCR",
            "performance_value": "Task 1 (Range Checking): initial LCR 0.87 (87%), required ~5 refinement steps to reach 0.98 (98%) final. Task 2 (Constraint Validation): initial LCR 0.88 (88%), improved to 0.99 (99%) after two refinement steps.",
            "comparison_with_baseline": "Despite being larger and code-specialized, Codestral initially performed below some smaller models on certain metrics but benefited strongly from self-correction to reach top final LCR; self-correction gains were large and quick (few steps).",
            "key_findings": "Codestral-22B achieved top final logical correctness rates (98% in range-checking, 99% in constraint validation) after iterative refinement, illustrating the importance of execution-feedback loops even for code-focused LLMs.",
            "limitations": "Initial logical inconsistencies and runtime errors were observed despite strong syntactic output; required multiple self-correction iterations in some cases.",
            "uuid": "e6795.4",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "QwenCoder-14B",
            "name_full": "QwenCoder 2.5 (14B)",
            "brief_description": "Large code-specialized Qwen-Coder variant that achieves high final logical correctness on both tasks with modest iterative correction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "QwenCoder2.5-14B",
            "model_description": "14B-parameter code-specialized instruction-tuned causal transformer in the Qwen-Coder family, designed for code generation.",
            "model_size": "14B",
            "architecture_type": "Transformer (code-specialized, instruction-tuned)",
            "training_data": null,
            "reasoning_method": "Program synthesis to Python with in-context examples and iterative self-correction leveraging execution errors.",
            "external_tool_used": true,
            "external_tool_description": "Python interpreter validates code (syntax/runtime/logical) and returns errors used to re-prompt the model.",
            "benchmark_name": "ExKLoP",
            "benchmark_description": "Range Checking & Constraint Validation dataset for engineering premises.",
            "task_type": "Range Checking; Constraint Validation",
            "performance_metric": "FSR and LCR",
            "performance_value": "Task 1 (Range Checking) final LCR: 97%. Task 2 (Constraint Validation) initial LCR 0.94 (94%), improved to 0.95 (95%) after one self-correction step.",
            "comparison_with_baseline": "Comparable final LCR to Codestral-22B and Llama3 variants; required fewer refinement steps to improve on Task 2 (one step).",
            "key_findings": "High final logical correctness across both tasks; coding specialization plus iterative execution-feedback yields strong results with limited refinement.",
            "limitations": "Smaller Qwen variants show large performance degradation, indicating sensitivity to model size.",
            "uuid": "e6795.5",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "QwenCoder-3B",
            "name_full": "QwenCoder 2.5 (3B)",
            "brief_description": "Medium-size Qwen-Coder evaluated; shows decent range-checking but much weaker constraint-validation performance, though it benefits from multiple refinements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "QwenCoder2.5-3B",
            "model_description": "3B-parameter code-specialized instruction-tuned transformer for code generation.",
            "model_size": "3B",
            "architecture_type": "Transformer (code-specialized, instruction-tuned)",
            "training_data": null,
            "reasoning_method": "In-context program synthesis with iterative self-correction via execution feedback.",
            "external_tool_used": true,
            "external_tool_description": "Python interpreter to validate generated code; error messages appended to prompts to guide corrections.",
            "benchmark_name": "ExKLoP",
            "benchmark_description": "Range Checking and Constraint Validation tasks with synthetic engineering premises.",
            "task_type": "Range Checking; Constraint Validation",
            "performance_metric": "FSR and LCR",
            "performance_value": "Task 1 (Range Checking) final LCR: 91%. Task 2 (Constraint Validation) initial and final: started low and ended at 67% after multiple (6) refinement steps (paper states QwenCoderMedium increased from 44%→67%).",
            "comparison_with_baseline": "Underperforms larger QwenCoder and code-specialized models on Constraint Validation; shows meaningful gains with multiple refinements but remains below top models.",
            "key_findings": "Medium-sized QwenCoder reaches acceptable performance on range-checking but struggles with relational constraint tasks; iterative correction provides the largest absolute improvements for smaller variants.",
            "limitations": "Substantial drop in constraint-validation LCR compared to range-checking; requires many refinement steps and still lags larger models.",
            "uuid": "e6795.6",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "QwenCoder-1.5B",
            "name_full": "QwenCoder 2.5 (1.5B)",
            "brief_description": "Small Qwen-Coder variant with poor logical consistency initially and after refinement; demonstrates steep size-performance dependency for strict logical tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "QwenCoder2.5-1.5B",
            "model_description": "1.5B-parameter code-specialized instruction-tuned causal transformer.",
            "model_size": "1.5B",
            "architecture_type": "Transformer (code-specialized, instruction-tuned)",
            "training_data": null,
            "reasoning_method": "Prompted program synthesis with iterative self-correction using execution-derived feedback when applied.",
            "external_tool_used": true,
            "external_tool_description": "Python interpreter for validation; returns errors used to re-prompt for corrections.",
            "benchmark_name": "ExKLoP",
            "benchmark_description": "Range Checking & Constraint Validation tasks derived from engineering premises.",
            "task_type": "Range Checking; Constraint Validation",
            "performance_metric": "FSR and LCR",
            "performance_value": "Task 1 (Range Checking) initial LCR under 25%, final after 6 refinement steps ≈45%. Task 2 (Constraint Validation) initial LCR 0.02 (2%), improved to 0.19 (19%) after six refinement steps.",
            "comparison_with_baseline": "Performs substantially worse than larger QwenCoder variants and other models; even with self-correction improvements remain low compared to larger models.",
            "key_findings": "Small model size leads to very poor logical consistency on relational tasks and only modest gains from self-correction; demonstrates strong size dependence for strict logical reasoning through program synthesis.",
            "limitations": "Very low initial logical performance and limited final gains after iterative correction; not recommended for strict logical rule synthesis without larger model capacity or additional methods.",
            "uuid": "e6795.7",
            "source_info": {
                "paper_title": "Integrating Expert Knowledge into Logical Programs via LLMs",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning.",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers.",
            "rating": 2,
            "sanitized_title": "linc_a_neurosymbolic_approach_for_logical_reasoning_by_combining_language_models_with_firstorder_logic_provers"
        },
        {
            "paper_title": "Llm-arc: Enhancing llms with an automated reasoning critic.",
            "rating": 2,
            "sanitized_title": "llmarc_enhancing_llms_with_an_automated_reasoning_critic"
        },
        {
            "paper_title": "Goedel-prover: A frontier model for open-source automated theorem proving.",
            "rating": 2,
            "sanitized_title": "goedelprover_a_frontier_model_for_opensource_automated_theorem_proving"
        },
        {
            "paper_title": "CodeHalu: Investigating code hallucinations in llms via execution-based verification.",
            "rating": 2,
            "sanitized_title": "codehalu_investigating_code_hallucinations_in_llms_via_executionbased_verification"
        },
        {
            "paper_title": "RuleArena: A benchmark for rule-guided reasoning with LLMs in real-world scenarios.",
            "rating": 1,
            "sanitized_title": "rulearena_a_benchmark_for_ruleguided_reasoning_with_llms_in_realworld_scenarios"
        },
        {
            "paper_title": "Not all votes count! programs as verifiers improve self-consistency of language models for math reasoning.",
            "rating": 1,
            "sanitized_title": "not_all_votes_count_programs_as_verifiers_improve_selfconsistency_of_language_models_for_math_reasoning"
        }
    ],
    "cost": 0.01533,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Integrating Expert Knowledge into Logical Programs via LLMs
12 May 2025</p>
<p>Franciszek Górski franciszek.gorski@pg.edu.pl 
Multimedia Systems Department
Gdansk University of Technology
Poland</p>
<p>Equal contribution</p>
<p>Oskar Wysocki 
Department of Computer Science
University of Manchester
United Kingdom</p>
<p>Idiap Research Institute
MartignySwitzerland</p>
<p>Equal contribution</p>
<p>Marco Valentino 
Idiap Research Institute
MartignySwitzerland</p>
<p>Andre Freitas 
Department of Computer Science
University of Manchester
United Kingdom</p>
<p>National Biomarker Centre (NBC)
CRUK Manchester Institute
United Kingdom</p>
<p>Idiap Research Institute
MartignySwitzerland</p>
<p>Integrating Expert Knowledge into Logical Programs via LLMs
12 May 2025BDDFFB7AB7B4F9E52E58CC9DE1767CFCarXiv:2502.12275v2[cs.AI]
This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems.This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems.By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability.Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks.We also explore the models' capacity for self-correction via an iterative feedback loop based on code execution outcomes.ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points.It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments.We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma3, Codestral and Qwen-Coder.The results reveal that most models generate nearly perfect syntactically correct code and exhibit strong performance in translating expert knowledge into correct code.At the same time, while most LLMs produce nearly flawless syntactic output, their ability to correctly implement logical rules varies, as does their capacity for self-improvement.Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have demonstrated remarkable abilities in translating natural language into executable code, opening new possibilities for integrating expert knowledge into Expert Systems.This capability is particularly promising in domains such as engineering, where technical manuals, guidelines, and service reports could serve as inputs for building expert-driven reasoning systems.Such systems would enable a new class of knowledge-informed machine learning models, seamlessly combining data-driven approaches with domain expertise.</p>
<p>However, for this vision to become a reality, we must first assess how effectively LLMs can translate domain-specific facts and logical rules into executable code.For instance, consider an industrial machine with multiple operational parameters and manufacturerrecommended ranges for normal operation.Ideally, an LLM should be able to encode these recommendations into a program that can automatically monitor the machine's performance.Yet, the accuracy, reliability, and robustness of such model-generated code remain open questions.</p>
<p>To address these challenges, we require well-defined frameworks, evaluation datasets, and benchmarks to systematically assess LLMs' ability to generate executable reasoning rules.Expert systems demand precise logical rules, making it essential to investigate whether LLMs, even when initially incorrect, can self-correct (also via agentic systems) by leveraging feedback from external symbolic models (e.g., a Python interpreter) [9,25,10,1,39].The envisioned future of systems capable of producing fully functional reasoning modules through iterative refinement hinges on key architectural decisions-such as model selection, prompt formulation, and feedback mechanism design.</p>
<p>In this paper, we introduce ExKLoP, a framework designed to assess LLMs' capability to integrate expert knowledge into logical reasoning systems while evaluating their potential for self-correction.Our framework provides a structured approach to exploring the reliability of LLM-generated code via syntax, runtime and logic validation, before and after self-correction based on the output errors.We offer insights into both model performance and iterative improvement strategies.</p>
<p>To guide our investigation, we formulate the following key research questions: RQ1: Can large language models (LLMs) facilitate the integration of expert knowledge with AI and knowledge-informed machine learning models in engineering?RQ2: How efficiently can LLMs convert expert knowledge expressed in natural language into valid Python logic rules?RQ3: To what extent can LLMs self-correct their mistakes using feedback from external program interpreter?</p>
<p>To address these questions, we present the following contributions: • We introduce ExKLoP, an experimental framework for evaluating how effectively LLMs integrate expert knowledge from engineering domain into logical rules and executable code by measuring Formalization Success Rate and Logical Consistency Rate.Additionally, our framework assesses the models' capacity for self-correction by leveraging feedback from the execution environment.</p>
<p>• ExKLoP provides a simple yet powerful approach to exploring the potential of agentic systems in knowledge-based reasoning.Specifically, we investigate whether iterative self-correction enhances the accuracy and reliability of model-generated outputs.</p>
<p>• We introduce a scalable and extensible dataset generation methodology for engineering premises and validation points.ExKLoP, comprising 130 premises and 950 prompts, enables efficient benchmarking while allowing researchers to control task complexity and seamlessly expand experiments by integrating additional premises within a single prompt.</p>
<p>• We demonstrate significant performance variations in generating correct logical rules depending on the size and type of the LLM, whether it is a general-purpose model or one specialized in code generation, as well as on the difficulty of the task.While all LLMs produce nearly flawless syntactic output, their ability to correctly implement logical rules varies, as does their capacity for self-improvement.</p>
<p>ExKLoP</p>
<p>ExKLoP follows a structured evaluation process consisting of three key phases: (1) Translation of natural language premises into executable functions, (2) Syntax, runtime, and logic validation, and (3) Iterative refinement of incorrect outputs using an LLM-driven selfcorrection loop.This end-to-end workflow is illustrated in Figure 1.</p>
<p>Translation of Natural Language to Python Logic</p>
<p>The primary task assigned to the LLM is to generate functions that accurately encode logical constraints specified in natural language.These premises define acceptable ranges for given physical parameters of an object, ensuring compliance with expected operational conditions.In engineering applications, such constraints often describe the normal operating ranges of mechanical systems.For example, an industrial machine may have predefined thresholds for temperature, pressure, and vibration that indicate safe operation.The prompt contains instruction, in-context examples and premises to be translated into code (for details see Section 2.7).</p>
<p>Syntax, runtime, and logic validation</p>
<p>Once the Python functions are generated, they undergo a multi-stage validation process to ensure correctness across three dimensions: Syntax Validation: The Python interpreter checks for syntactical correctness detecting any parsing errors, such as missing colons or incorrect indentation.Runtime Validation: The generated code is executed using the validation dataset as a source of input data points to identify runtime errors, such as mismatched argument counts or undefined variables.Logical Verification: This step determines whether the logic implemented by the model accurately represents the intended constraints.The functions take data points from the validation dataset and evaluate their outputs against ground truth labels.Any errors are flagged for correction and used as added part of the prompt in the following step (see examples in Fig. 2).</p>
<p>Iterative Refinement via LLM Self-Correction</p>
<p>If a function fails any validation step, it undergoes an iterative refinement process.This involves re-prompting the LLM with structured feedback to improve the function.Each re-prompt includes a slightly modified task instruction, the previously generated code, and a detailed error description to guide correction.The refinement process consists of three key stages:</p>
<p>Syntax Correction: Syntax errors are provided to the LLM with explicit error messages detailing the cause of failure.The model is then prompted to regenerate the function while ensuring that the identified syntax issue is corrected.Execution Error Correction: For runtime errors, the corresponding error message is provided as feedback to the LLM for examples such as an incorrect number of arguments or undefined variables).The model then attempts to revise the function to ensure successful execution.Logical Correction: For incorrect outputs during logical verification, the LLM is tasked with refining the conditional logic.It receives feedback on the specific functions that are part of the overall logic, allowing it to correct these functions accordingly.</p>
<p>After each refinement step, the updated function is re-evaluated using the same validation process (see Fig. 1).The refinement loop continues for up to 10 steps or until no further improvements are observed.</p>
<p>Metrics</p>
<p>The framework uses two metrics:</p>
<p>Formalization Success Rate (FSR) measures the proportion of natural language inputs correctly translated into syntax error-free Python rules, assessing the LLM's ability to generate valid code.
F SR = X f X(1)
Logical Consistency Rate (LCR) measures the proportion of natural language inputs accurately translated into Python rules that not only execute without errors but also correctly determine whether a data point is an outlier, ensuring logical validity.
LCR = Xc X(2)
where: X is the total set of natural language inputs (prompts) containing rules; X f is the subset of X that is successfully formalized into valid, syntax error-free Python rules; Xc is the subset of X f where the generated rules correctly identify out-of-range values.</p>
<p>Tasks</p>
<p>Task 1: Range Checking The initial task aims to verify whether a single parameter's value lies within a predefined range, e.g., 'A vehicle operates for between 2 and 10 hours per day'.Additionally, a single prompt contains multiple premises, each defining different parameter ranges.As the number of premises increases, so does the complexity of the task, requiring the model to correctly handle multiple constraints simultaneously.The objective is to evaluate the model's ability to process and encode multiple premises within a single prompt, rather than just a single constraint.</p>
<p>Formally, given multiple parameters x with their respective valid ranges [xmin, xmax], the generated function should determine whether all parameters satisfy their constraints:  Task 2: Constraint Validation The second task extends the complexity by introducing interdependencies between parameters.Instead of evaluating each parameter independently, the constraints now define relational conditions between them.For example, in a vehicle load distribution scenario, a constraint may state: 'The load on the first axle cannot be greater than the load on the second axle.' Formally, let R be a set of relational constraints defined over the parameters:
F (x1, x2, . . . , xn) =    1, if ∀i ∈ {1, . . . ,R = {(xi, xj) | xi Oij xj, ∀(i, j)}
where Oij represents a relational operator such as ≤, ≥.The function F (x1, x2, . . ., xn) should evaluate whether all given constraints hold simultaneously:
F (x1, x2, . . . , xn) = 1, if ∀(xi, xj) ∈ R, xi Oij xj 0, otherwise</p>
<p>Premises Dataset</p>
<p>The dataset consists of statements defining the normal operating ranges of various parameters based on industry standards.To enhance linguistic diversity, each statement is rephrased into five distinct textual variations while preserving logical consistency.This approach increases robustness and ensures variability in how constraints are expressed.Each premise is crafted to resemble expert descriptions, providing realistic inputs for LLMs.</p>
<p>The dataset is easily extendable, with the current version covering: 17 parameters corresponding to real-world vehicle operational variables such as speed, distance, fuel consumption, and axle loads; 130 engineering premises, 950 prompts, and corresponding validation points.</p>
<p>Prompt Construction</p>
<p>Each prompt follows a structured format to guide the LLM in code generation and iterative refinement (Fig. 3): Task Instruction: A system message that clearly defines the model's role and objective in translating natural language constraints into executable logic.In-Context Examples: Demonstrations of correctly formatted Python functions, with parameter names and units replaced by placeholders.This ensures the model understands the expected output format while preventing memorization of specific values.The number of examples corresponds to the number of premises in the task.Input Premises: A set of natural language statements defining constraints on the operational parameters of an object.These premises serve as the basis for generating Python functions and are derived from Premises Dataset.</p>
<p>Error Messages (During Self-Correction): If the generated function fails syntax, runtime, or logic validation, error messages (Fig. 2) are appended to the prompt.These messages provide explicit feedback on the failure, enabling the model to iteratively refine and correct its output.</p>
<p>As illustrated in Figure 1 and 3, the prompt is dynamically augmented during refinement steps, allowing the model to improve incorrect functions based on structured feedback.This iterative approach ensures better alignment between the generated code and the intended logical constraints.</p>
<p>Evaluation Data Points</p>
<p>As part of the ExKLoP framework, we generate a structured evaluation dataset to systematically assess whether LLM-generated functions correctly classify in-range and out-of-range values.This dataset is tailored to each task, ensuring comprehensive validation of the model's ability to enforce predefined constraints.</p>
<p>Range Checking Task: For each parameter xi (i = 1, . . ., n), we generate four test points: {xi1, xi2, xi3, xi4},with the following properties:
xi1 &lt; xi,min, xi2, xi3 ∈ [xi,min, xi,max],
xi4 &gt; xi,max.</p>
<p>To minimize the total number of evaluations-which would otherwise require testing all n! combinations-we adopt the following strategy that requires only 4n evaluations: for a given parameter xi, we evaluate F by varying xi over its four test points while keeping all other parameters fixed at a nominal value xj0 (with xj0 ∈ [xj,min, xj,max] for j ̸ = i).In this way, the vector of evaluation points for parameter xi is constructed as:
x (i,ℓ) = x10, . . . , xi−1,0, x iℓ , xi+1,0, . . . , xn0
where ℓ = 1, 2, 3, 4. By design, for each test vector x (i,ℓ) the function evaluation simplifies to:
F x (i,ℓ) = 1, if x iℓ ∈ [xi,min, xi,max], 0, if x iℓ / ∈ [xi,min, xi,max],
since F (xj0) = 1 for all j ̸ = i.Constraint Validation Task: For each relational constraint xi Oij xj (with Oij a relational operator such as ≥ or ≤), we generate three test points for the parameter xi: {xi1, xi2, xi3} with the following properties: xi1, xi2 satisfy xi Oij xj,0 and xi3 violates xi Oij xj,0, where xj,0 is a nominal valid value for xj (i.e., one that complies with its range and any applicable constraints).</p>
<p>For a given constraint xi Oij xj, we evaluate F by varying xi over its three test points while keeping all other parameters fixed at nominal values x k,0 (with x k,0 chosen so that F evaluates to 1 when no constraint is violated).In this way, the evaluation vector is constructed as:
x (i,ℓ) = x10, . . . , xi−1,0, x iℓ , xi+1,0, . . . , xn0
where ℓ = 1, 2, 3.By design, for each test vector x (i,ℓ) the function evaluation is:
F x (i,ℓ) = 1, if x iℓ Oij xj,0 (ℓ = 1, 2), 0, if x iℓ / ∈ { x | x Oij xj,0 } (ℓ = 3),
since the nominal values for the other parameters ensure that all other constraints are satisfied.</p>
<p>Assuming each constraint is associated with a distinct parameter being varied, the total number of evaluation scenarios for F (x1, x2, . . ., xn) is 3n.</p>
<p>Experiments</p>
<p>To evaluate the effectiveness of the proposed ExKLoP, we conducted experiments on two tasks: Range Checking and Constraint Validation.</p>
<p>For the Range Checking task, each prompt comprises between 2 and 12 premises.We generated 50 prompts for each set size, resulting in a total of 550 unique prompts.Varying the number of premises per prompt allows us to assess whether performance degrades as the input complexity increases.Each prompt is assigned a unique, randomly selected set of premises to ensure diversity in parameter selection.</p>
<p>For the Constraint Validation task, each prompt contains between 2 and 9 conditions.With 50 prompts generated for each set size, this task includes a total of 400 unique prompts.Each prompt randomly selects a distinct set of conditions, ensuring a broad range of logical interdependencies.</p>
<p>In total, we evaluated 6650 prompts, as each of the 7 LLMs was tested on 550 Range Checking prompts and 400 Constraint Validation prompts.</p>
<p>Experimental setting</p>
<p>To ensure a comprehensive evaluation, we tested 7 opensourced LLMs: Llama3-70B, Llama3-8B [4], Gemma3-12B [30], Codestral-22B [23] and 3 size variants of QwenCoder2.5 [8] model -QwenCoder2.5-1.5B,QwenCoder2.5-3B and QwenCoder2.5-14B and we refer to this models as QwenCoderSmall, QwenCoder-Medium and QwenCoder.All models are used in their instructiontuned (Instruct) versions, optimized for task-following.The first-step inference is performed with a temperature of 0.0 and no sampling, minimizing randomness in responses.In contrast, the refinement steps are executed with a temperature of 0.6, top_p=0.9,top_k=50, and sampling enabled to introduce more variety in the models' behavior during self-improvement.Self-correction was not used in every case.It was never applied for the Llama3-70B model, by virtue of the fact that it was the largest model, with the highest computational costs, and whose results were so satisfactory that self-correction was not needed.In addition, in Task 1, models whose results were above 90% were not subjected to correction due to the relatively low complexity of the task, requiring no additional computational expense for such results.</p>
<p>All experiments were conducted using 4 NVIDIA L40 GPUs, each with 48GB of VRAM, and an AMD EPYC 75F3 32-Core Processor.The downloaded models required 360GB of free disk space.All scripts were executed with Python 3.10.The pretrained LLM weights were utilized through Hugging Face's transformers library (version 4.33.1) and accelerate (version 0.33.0), with CUDA 12.1.The models were loaded using the AutoModelForCausalLM and AutoTokenizer classes with the options device_map="auto" and torch_dtype=torch.bfloat16,enabling multi-GPU inference and reduced GPU memory usage.The refinement process continues for up to 10 steps or until no further improvements are observed.</p>
<p>Results and discussion</p>
<p>Our evaluation of LLM-generated Python functions for range checking and constraint validation tasks revealed several key findings.Textual context: It is unlikely for a vehicle to operate for less than 2 hours or more than 10 hours in a typical day.</p>
<p>A vehicle idling for less than 1 hour or more than 5 hours in a day is not common.</p>
<p>Each typical day, a vehicle stops between 20 and 300 times.</p>
<p>On a typical day, the load on axle 1 ranges from 2000 to 8000 kg.You are a logician with a background in mathematics that translates natural language reasoning text to Python3 code so that these natural language reasoning problems can be solved.During the translation, please pay close attention to defining variables and rules.Do not add any comments from you.Be guided by the following example: Example input text: The payload must always be smaller than or equal to the load on axle 2.</p>
<p>Expert knowledge in natural language</p>
<p>The total operation time must always be not lower than the combined duration of PTO operation, idle time, and driving time.</p>
<h1>Parameter E's value cannot be less than parameter F's value.def r1(e: float, f: float) -&gt; bool:</h1>
<p>if  High Fluency in Syntactically Correct Code: Most models achieved near-perfect Formalization Success Rates (only LLama3-8B has 0.99), demonstrating a strong capability in producing syntactically correct Python code.Although generating code that passes syntax checks is relatively straightforward, ensuring logical correctness is considerably more challenging.This highlights the need for robust evaluation frameworks to systematically assess not only syntax but also the underlying logic.</p>
<p>Translation of Expert Knowledge into Code: Most models demonstrated strong performance in translating expert knowledge into code, with relatively few issues.The most common problems were runtime errors, particularly in constraint validation tasks, such as mismatched function arguments or undefined variables.Another frequent issue was logical inconsistency -for instance, reversing the order of arguments (e.g., generating X2 &lt;= X1 instead of X1 &gt;= X2).Additional errors included incorrect argument definitions or the omission of key physical parameters.</p>
<p>Impact of the number of parameters/relations in a prompt: As we can in see in Figures 6 and 7 in both tasks, the relationship between the number of input parameters (or logical relationships) and the Logical Correctness Rate (LCR) proved to be somewhat unintuitive.In the Range Checking task, increasing the number of parameters in the input consistently improved performance, even for models with otherwise low overall LCR scores.This suggests that, for the models studied, a higher number of input parameters was not a hindrance-rather, it appeared to facilitate better reasoning.In contrast, this trend was partially reversed in the Constraint Validation task.Some models-such as Gemma3, QwenCoderMedium, and QwenCoderSmall-performed worse with more parameters.However, other models maintained the same positive correlation observed in the Range Checking task.</p>
<p>Task 1: Logical Correctness in Range Checking: most models achieved very high results with Llama3 models and QwenCoder being the best.Only the smallest QwenCoderSmall model has very poor performance, under 25% and stops his self-refinement process after 6 steps with result nearly doubled burt still low, only 45%.Interestingly, Codestral scored lower than smaller models, just 87%, and need 5 refinement steps to achieve 98% Logical Correcntness Score.Noteworthy is the performance of the QwenCoderMedium model, which initially reached 87%, the same as Codestral, which is 7 times larger, and improved by an additional 4 percentage points over 4 steps.</p>
<p>Task 2: Logical Correctness in Constraint Validation: the largest model, Llama3-70B, led with a Logical Correctness Rate (LCR) of 0.96, followed by QwenCoder at 0.94, Codestral at 0.88, and Llama3-8B at 0.82.After two steps of self-refinement, Codestral improved its score to 0.99, achieving the highest result overall.Llama3-8B improved to 0.90 after two steps, QwenCoder reached 0.95 after a single step, and Gemma3 made a notable leap from a modest initial LCR of 0.78 to 0.95 in just two refinement steps.QwenCoderMedium started with a much lower score of 44% and increased it to 67% over six refinement steps, representing the largest relative improvement across both tasks.QwenCoderSmall began with a very low LCR of 0.02 and reached 0.19 after six steps of refinement.</p>
<p>Impact of Self-Correction and Model Comparison: Selfcorrection proved to be a key factor influencing model performance.However, the extent of its impact varied depending on the model and the task type.All models benefited from iterative self-correction, but larger models (Codestral, Gemma3, Llama3-8B) achieved meaningful improvements in fewer steps compared to smaller models (Qwen-CoderMedium, QwenCoderSmall).Notably, in none of the tested cases did self-correction proceed more than 6 steps, typically halting earlier.Increasing the variability in model responses by adjusting the temperature and enabling sampling positively influenced the effectiveness of the self-correction process.Figure 8 indicates that a larger number of parameters does not necessarily lead to significantly better performance.A model like Llama3-70B appears over-</p>
<p>First inference</p>
<p>Step  sized for optimal use in both tasks, as comparable results can be achieved by much smaller models-such as Llama3-8B, Gemma3, or QwenCoder-with just a few additional inference steps.While Llama3-70B may reach high performance in a single inference, this comes at a substantially higher computational cost.Key numerical results are summarized in Table 1, while Figure 4  and 5 shows changes in LCR in successive stages of iterative refinement for both tasks.Figures 6, 7 shows the same changes but with detailed results for each parameter.Figure 8 shows the distribution of LCR metric values depending on the size of the model.Table 1.Formalization Success Rates (FSR) and Logical Consistency Rate (LCR) in Range Checking (top) and Constraint Validation (bottom) tasks.∆ corresponds to the change between the first inference and after all refinement steps that made a difference.</p>
<p>Related work</p>
<p>Recent advancements in large language models (LLMs) have significantly enhanced capabilities in logical reasoning and code generation.However, limitations remain in reliably integrating expert knowledge and ensuring logical consistency.</p>
<p>In the domain of logical reasoning and verification, early works began exploring the integration of external reasoning tools with LLMs.For instance, McGinness and Baumgartner [20] combined Automated Theorem Provers with LLMs, improving logical accuracy via a neuro-symbolic method tested on the PRONTOQA benchmark.Around the same time, LeanReasoner [9] and Logic-LM [25] formalized reasoning tasks as theorems to be externally solved, while LINC [24] and Symbolic Chain-of-Thought [39] translated natural language into first-order logic to facilitate symbolic deduction.</p>
<p>As the field progressed, researchers began to stress-test and scaffold reasoning behaviors.Wan et al. [35] proposed logic scaffolding to expose inferential weaknesses in LLMs, and Zhou et al. [42] introduced RULEARENA to evaluate rule-guided reasoning in multirule environments.Meanwhile, Hong et al. [7]  fallacies in LLM outputs using the FALLACIES dataset, revealing core weaknesses in self-verification.Addressing these, Liang et al. [14] introduced a fine-grained "Step CoT Check" technique for better detection and correction of reasoning errors, while Wan et al. [34] developed the CoT Rerailer framework, which integrates both error detection and correction to boost reasoning fidelity.</p>
<p>In pursuit of scalability and automation, Luo et al. [19] presented OmegaPRM, a Monte Carlo Tree Search-based method that significantly improved performance on mathematical benchmarks like MATH500 and GSM8K.Liang et al. [14] later extended this line of work by proposing a collaborative verification framework that distributes inference computations to further improve scalability and reasoning robustness.</p>
<p>Complementary efforts have injected formal logical structures directly into the LLMs' reasoning processes.For example, Logic-of-Thought and Logic Agent [17] incorporate explicit logical rules to ensure more valid outputs.Several symbolic reasoning-focused studies also emerged around this time [38,3,27,26,21,22], some bridging with general code generation tasks [36,40].</p>
<p>Our investigation builds upon these developments, especially research into iterative self-correction and refinement.Feedback-loop frameworks such as LLM-ARC [10] and Determlr [29] show promise in improving logical outputs.However, our findings indicate that, even when syntactic outputs are correct, logical inconsistencies in the translation of expert rules often persist-echoing issues highlighted in prior work on code verification and logical consistency [37,5,6,2].To address this, other studies have proposed cross-model correction [12] and more robust feedback mechanisms [41].</p>
<p>With regard to stability and evaluation, Liu et al. [18] introduced G-Pass@k and LiveMathBench to capture discrepancies between theoretical potential and operational stability.Lifshitz et al. [15] advanced this further through Multi-Agent Verification, which employs multiple verifiers at inference time to boost accuracy and scalability.Confidence aggregation techniques such as CER, introduced by Razghandi et al. [28], also proved effective in enhancing reliability In the area of code generation and efficiency, researchers have tackled logical coherence and hallucination.Tian et al. [31] addressed this directly with the CodeHalu algorithm and the Code-HaluEval benchmark, which highlight reliability gaps.Li et al. [13] proposed GRACE, a model-editing approach that enables precise corrections without degrading unrelated capabilities.In parallel, ECCO [33] introduced a benchmark to assess trade-offs between correctness and computational efficiency, advocating for executionbased feedback.</p>
<p>Finally, in formal proof generation, Lin et al. [16] introduced Goedel-Prover, which demonstrated superior performance in automated theorem proving, outperforming models like DeepSeek-Prover-V1.5.Leang et al. [11] complemented this with TP-as-a-Judge, which uses theorem provers to validate intermediate reasoning steps, thereby increasing model precision in mathematical benchmarks.</p>
<p>Collectively, these works illustrate a rich and evolving landscape.Despite major strides in integrating symbolic reasoning, verifying outputs, and improving generalization in code generation, substantial gaps remain-particularly in ensuring that expert rules are faithfully and logically represented in LLM outputs.</p>
<p>Conclusions</p>
<p>We introduced ExKLoP, a framework designed to evaluate LLMs' ability to integrate expert knowledge into logical reasoning and selfcorrection.Leveraging an extensible dataset of engineering premises and validation points, our approach systematically assesses both the syntactic fluency and logical correctness of Python code for critical tasks like range checking and constraint validation.</p>
<p>Our experiments demonstrated the high efficiency of most tested models in generating syntactically and logically correct Python code.Coding-oriented models such as Codestral-22B and QwenCoder2.5-14Bperformed well across both tasks, achieving final Logical Correctness Rates (LCRs) of 98% and 97% in range-checking, and 99% and 95% in constraint validation, respectively.However, Codestral-22B initially showed some anomalies, performing below smaller models with an LCR of 88% -4 to 9 percentage points lower than its counterparts.Smaller general-purpose models like Llama3-8B and Gemma3-12B also delivered satisfactory results, with LCRs of 95% and 92% in Task 1, and 90% and 95% in Task 2, respectively, showing performance not far behind the specialized coding models.QwenCoder2.5-3B performed adequately in the rangechecking task with a 91% LCR but experienced a significant drop in the constraint validation task, ending with 67%.The smallest model, QwenCoder2.5-1.5B,performed poorly in both tasks, achieving only 45% in range-checking and 19% in constraint validation.Self-correction proved to be a key contributor to performance improvements across all models.Our experiments revealed that the number of iterations varied by model, with each reaching a stopping point no later than the 6th iteration.</p>
<p>Overall, ExKLoP offers a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating error types.This framework establishes a valuable benchmark and lays the groundwork for future research aimed at enhancing logical consistency and the integration of expert knowledge in AI-based Expert systems.</p>
<p>Limitations</p>
<p>Our study demonstrates the potential of LLMs for expert knowledge integration and constraint validation, but few limitations must be acknowledged: Dataset Constraints: Our experiments were conducted on a domain-specific dataset (vehicle operation parameters).This limits the generalizability of our findings to other fields, such as biomedicine or finance.The dataset includes synthetic premises and validation points.Prompting Limitations: Our approach relied on In-Context Learning (ICL) without advanced prompting strategies like Chainof-Thought (CoT) reasoning.These techniques might improve logical accuracy.The iterative refinement process improved performance but still relied on model-generated self-corrections, which may introduce biases or reinforce incorrect patterns.</p>
<p>Figure 1 .Figure 2 .
12
Figure 1.ExKLoP framework for evaluating expert knowledge integration in Python-based logical rules</p>
<h1></h1>
<p>A vehicle operates over a A param between 20 and 110 in a typical day.def r1(a: float) -&gt; bool: return 20 &lt;= a &lt;= 110 # A vehicle operates for a B param between 2 and 10 in a typical day.def r2(b: float) -&gt; bool: return 2 &lt;= b &lt;= 10 # A vehicle operates with C param between 0.2 and 5 in a typical day.def r3(c: float) -&gt; bool: return 0.2 &lt;= c &lt;= 5 # A vehicle remains D param between 1 and 5 in a typical day.def r4(d: float) -&gt; bool: return 1 &lt;= d &lt;= 5</p>
<p>Textual context: Axle 1's load cannot exceed Axle 2's load.</p>
<p>Figure 3 .
3
Figure 3. Examples of prompts in Task 1. Range Checking (left) and Task 2. Constraint Validation (right).</p>
<p>Figure 4 .
4
Figure 4. Task 1 Logical Consistency Rate (LCR) for first inference and for each of the refinement steps.</p>
<p>Instruction In context examples Premises Prompt Code Producing Errors Syntax correction Prompt Code Producing Errors Instruction In context examples Premises Instruction In context examples Premises Instruction In context examples Premises List of incorrect functions Runtime error descrption Syntax error descrption Expert knowledge in natural language
PromptEvaluationdatasetSyntax validationRuntime validationLogical VerificationLLMPython code: functionsPython InterpreterSyntax errorNOPython InterpreterRuntime errorsNOLogical correctnessYESENDYESYESNOCode correctionLogic correctionPromptCode ProducingErrorsn},xi ∈ [xi,min, xi,max]0, otherwise</p>
<p>highlighted structural
First inference 0.0 0.2 0.4 0.6 0.8 1.0 Logical Consistency RateStep 1Step 2Step 3 Task 2. Constraint Validation Step 4 Step 5 Step 6 Step 7Step 8Step 9Step 10Llama-70 QwenCoderCodestral Llama-8Gemma3 QwenCoderMediumQwenCoderSmallFigure 5. Task 2 Logical Consistency Rate (LCR) for first inference and foreach of the refinement steps.Logical Consistency Rate2 0.00 0.25 0.50 0.75 1.004 Task 1. Range Checking 6 8 10 12 initial LCR 24 Task 1. Range Checking 6 8 10 12 final LCRNumber of parameters to translateLlama-70 Llama-8Gemma3 CodestralQwenCoder QwenCoderMediumQwenCoderSmallFigure 6. Range Checking: Logical Consistency Rate relation to task com-plexity, before (left) and after (right) all self-corrections.</p>
<p>Iterative refinement of project-level code context for precise code generation with compiler feedback. Z Bi, Y Wan, Z Wang, H Zhang, B Guan, F Lu, Z Zhang, Y Sui, H Jin, X Shi, arXiv:2403.167922024arXiv preprint</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. W Chen, X Ma, X Wang, W W Cohen, arXiv:2211.125882022arXiv preprint</p>
<p>Inference to the best explanation in large language models. D Dalal, M Valentino, A Freitas, P Buitelaar, 10.18653/v1/2024.acl-long.14Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. L.-W Ku, A Martins, V Srikumar, the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational LinguisticsAug. 20241Long Papers)</p>
<p>The llama 3 herd of models. A G , 2024</p>
<p>J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao, W Chen, arXiv:2311.06158Language models can be logical solvers. 2023arXiv preprint</p>
<p>Language models can be deductive solvers. J Feng, R Xu, J Hao, H Sharma, Y Shen, D Zhao, W Chen, Findings of the Association for Computational Linguistics: NAACL 2024. 2024</p>
<p>A closer look at the self-verification abilities of large language models in logical reasoning. R Hong, H Zhang, X Pang, D Yu, C Zhang, arXiv:2311.079542023arXiv preprint</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Lu, arXiv:2409.121865-coder technical report. 2024arXiv preprint</p>
<p>D Jiang, M Fonseca, S B Cohen, arXiv:2403.13312Leanreasoner: Boosting complex logical reasoning with lean. 2024arXiv preprint</p>
<p>Llm-arc: Enhancing llms with an automated reasoning critic. A Kalyanpur, K K Saravanakumar, V Barres, J Chu-Carroll, D Melville, D Ferrucci, arXiv:2406.176632024arXiv preprint</p>
<p>Theorem prover as a judge for synthetic data generation. J O J Leang, G Hong, W Li, S B Cohen, arXiv:2502.131372025arXiv preprint</p>
<p>Leveraging llms for hypothetical deduction in logical inference: A neurosymbolic approach. Q Li, J Li, T Liu, Y Zeng, M Cheng, W Huang, Q Liu, arXiv:2410.217792024arXiv preprint</p>
<p>X Li, S Wang, S Li, J Ma, J Yu, X Liu, J Wang, B Ji, W Zhang, arXiv:2411.06638Model editing for llms4code: How far are we?. 2024arXiv preprint</p>
<p>Improving llm reasoning through scaling inference computation with collaborative verification. Z Liang, Y Liu, T Niu, X Zhang, Y Zhou, S Yavuz, arXiv:2410.053182024arXiv preprint</p>
<p>S Lifshitz, S A Mcilraith, Y Du, arXiv:2502.20379Multi-agent verification: Scaling test-time compute with multiple verifiers. 2025arXiv preprint</p>
<p>Goedel-prover: A frontier model for open-source automated theorem proving. Y Lin, S Tang, B Lyu, J Wu, H Lin, K Yang, J Li, M Xia, D Chen, S Arora, arXiv:2502.076402025arXiv preprint</p>
<p>H Liu, Z Teng, C Zhang, Y Zhang, arXiv:2404.18130Logic agent: Enhancing validity with logic rule invocation. 2024arXiv preprint</p>
<p>J Liu, H Liu, L Xiao, Z Wang, K Liu, S Gao, W Zhang, S Zhang, K Chen, arXiv:2412.13147Are your llms capable of stable reasoning?. 2024arXiv preprint</p>
<p>Improve mathematical reasoning in language models by automated process supervision. L Luo, Y Liu, R Liu, S Phatale, M Guo, H Lara, Y Li, L Shu, Y Zhu, L Meng, arXiv:2406.065922024arXiv preprint</p>
<p>Automated theorem provers help improve large language model reasoning. L Mcginness, P Baumgartner, arXiv:2408.034922024arXiv preprint</p>
<p>A symbolic framework for evaluating mathematical reasoning and generalisation with transformers. J Meadows, M Valentino, D Teney, A Freitas, 10.18653/v1/2024.naacl-long.84Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long Papers. K Duh, H Gomez, S Bethard, the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMexico City, MexicoAssociation for Computational LinguisticsJune 20241</p>
<p>J Meadows, M Valentino, A Freitas, Controlling equational reasoning in large language models with prompt interventions. 2025</p>
<p>Codestral: Hello, world!. A , 2024. 2024</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. T X Olausson, A Gu, B Lipkin, C E Zhang, A Solar-Lezama, J B Tenenbaum, R Levy, arXiv:2310.151642023arXiv preprint</p>
<p>Logic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning. L Pan, A Albalak, X Wang, W Y Wang, arXiv:2305.122952023arXiv preprint</p>
<p>Enhancing ethical explanations of large language models through iterative symbolic refinement. X Quan, M Valentino, L Dennis, A Freitas, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics. Long Papers. Y Graham, M Purver, the 18th Conference of the European Chapter of the Association for Computational LinguisticsSt. Julian's, MaltaAssociation for Computational LinguisticsMar. 20241</p>
<p>Verification and refinement of natural language explanations through LLMsymbolic theorem proving. X Quan, M Valentino, L A Dennis, A Freitas, 10.18653/v1/2024.emnlp-main.172Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Y Al-Onaizan, M Bansal, Y.-N Chen, the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational LinguisticsNov. 2024</p>
<p>A Razghandi, S M H Hosseini, M S Baghshah, arXiv:2502.14634Cer: Confidence enhanced reasoning in llms. 2025arXiv preprint</p>
<p>Determlr: Augmenting llm-based logical reasoning from indeterminacy to determinacy. H Sun, W Xu, W Liu, J Luan, B Wang, S Shang, J.-R Wen, R Yan, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational LinguisticsLong Papers20241</p>
<p>G Team, A Kamath, J Ferret, S Pathak, N Vieillard, R Merhej, S Perrin, T Matejovicova, A Ramé, M Rivière, arXiv:2503.19786Gemma 3 technical report. 2025arXiv preprint</p>
<p>Codehalu: Investigating code hallucinations in llms via execution-based verification. Y Tian, W Yan, Q Yang, X Zhao, Q Chen, W Wang, Z Luo, L Ma, D Song, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202539</p>
<p>Not all votes count! programs as verifiers improve self-consistency of language models for math reasoning. V Y Toh, D Ghosal, S Poria, arXiv:2410.126082024arXiv preprint</p>
<p>S Waghjale, V Veerendranath, Z Z Wang, D Fried, Ecco, arXiv:2407.14044Can we improve model-generated code efficiency without sacrificing functional correctness?. 2024arXiv preprint</p>
<p>Cot rerailer: Enhancing the reliability of large language models in complex reasoning tasks through error detection and correction. G Wan, Y Wu, J Chen, S Li, arXiv:2408.139402024arXiv preprint</p>
<p>Y Wan, W Wang, Y Yang, Y Yuan, J.-T Huang, P He, W Jiao, M R Lyu, arXiv:2401.00757Logicasker: Evaluating and improving the logical reasoning ability of large language models. 2024arXiv preprint</p>
<p>A review on code generation with llms: Application and evaluation. J Wang, Y Chen, 2023 IEEE International Conference on Medical Artificial Intelligence (MedAI). IEEE2023</p>
<p>H Wu, C Barrett, N Narodytska, arXiv:2310.04870Lemur: Integrating large language models in automated program verification. 2023arXiv preprint</p>
<p>M Wysocka, D S Carvalho, O Wysocki, M Valentino, A Freitas, arXiv:2410.14399Syllobio-nli: Evaluating large language models on biomedical syllogistic reasoning. 2024arXiv preprint</p>
<p>Faithful logical reasoning via symbolic chain-of-thought. J Xu, H Fei, L Pan, Q Liu, M.-L Lee, W Hsu, arXiv:2405.183572024arXiv preprint</p>
<p>Pythonsaga: Redefining the benchmark to evaluate code generating llms. A Yadav, H Beniwal, M Singh, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>Enhancing the code debugging ability of llms via communicative agent based data refinement. W Yang, H Wang, Z Liu, X Li, Y Yan, S Wang, Y Gu, M Yu, Z Liu, G Yu, arXiv:2408.050062024arXiv preprint</p>
<p>R Zhou, W Hua, L Pan, S Cheng, X Wu, E Yu, W Y Wang, arXiv:2412.08972Rulearena: A benchmark for rule-guided reasoning with llms in realworld scenarios. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
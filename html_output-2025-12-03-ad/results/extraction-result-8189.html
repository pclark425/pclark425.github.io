<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8189 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8189</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8189</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-150.html">extraction-schema-150</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <p><strong>Paper ID:</strong> paper-267500377</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03610v1.pdf" target="_blank">RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents</a></p>
                <p><strong>Paper Abstract:</strong> Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8189.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8189.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Planning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that augments LLM/VLM agents with an episodic memory of past successful task executions, retrieving relevant trajectory windows (plans, actions, observations) using a weighted semantic-similarity retriever and applying retrieved examples via in-context learning to generate next actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAP (Retrieval-Augmented Planning)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent architecture composed of four modules — Memory (episodic log database), Reasoner (LLM generates plans and retrieval keys), Retriever (semantic similarity weighted over task/plan/key), and Executor (LLM uses retrieved exemplars via in-context learning). Switches retrieval between action- or observation-based similarity depending on context and environment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Llama2-13b; LLaVA, CogVLM (for multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Evaluated with API LLMs (GPT-3.5, GPT-4) and local Llama2-13b for text tasks; multimodal agents built on VLMs LLaVA-v1.5-13B and Cog-VLM-17B (10B visual + 7B language) for embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop, Franka Kitchen, Meta-World</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>ALFWorld: multi-step text-based household tasks in simulated interactive environment; WebShop: web navigation and product selection; Franka Kitchen & Meta-World: multimodal embodied robotic manipulation tasks requiring visual perception and action.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / interactive decision-making / web navigation / embodied manipulation (multimodal)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic retrieval-augmented memory (external memory database of past successful episodes)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Store logs of successful episodes (task info Ti, overall plan pi, trajectory τi = sequences of action plans, actions, observations). Retrieval by computing weighted cosine-similarity over embeddings of task, overall plan, and retrieval key; text embeddings via sentence-transformers, image embeddings via CLIP/ViT. Retrieve a windowed trajectory centered on most similar action and pass examples to LLM as in-context examples for Executor.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Stored entries: {task description, overall plan, trajectory sequences of plans, actions, textual observations or image observations}. For multimodal entries observations are images from a fixed viewpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic vector retrieval with weighted similarity score: Score = wt * sim(task,task_i) + wp * sim(plan,plan_i) + wk * sim(retrieval_key,trajectory_i); retrieval-key similarity computed adaptively (max cosine between key and logged observations or actions). Adaptive component weights per environment; return windowed trajectory around the most similar action.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (GPT-3.5) success rate 85.8%; ALFWorld RAP_train (memory from training set + recursive) 91.0%; WebShop (GPT-3.5) success rate 48.0, overall reward score 76.1%; Franka Kitchen (LLaVA) success 61.6% (vs 43.4% baseline); Meta-World (LLaVA) success 79.2% (vs 65.4% baseline); CogVLM Franka 56.9% (vs 44.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ALFWorld ReAct (GPT-3.5 baseline) success rate 52.2%; WebShop ReAct (GPT-3.5) success rate 35.0 and score 61.8%; Franka Kitchen LLaVA baseline 43.4% success; Meta-World LLaVA baseline 65.4%.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Ablations on Retriever: RAP_act (action-only retrieval) vs RAP_obs (observation-only) vs RAP_clip (image observations with CLIP) showed action/observation switching improves results; Table 4 (ALFWorld, Llama2-13b) ReAct 52.2% -> RAP_act 82.1% -> RAP_obs 84.3% -> RAP_clip 86.6% -> RAP 85.8%. WebShop retriever ablations (Table 5) show incremental gains from RAP_act (33.0% success, 68.6 score) -> RAP_obs (33.0%, 69.0) -> RAP_intra (34.0%, 69.3) -> RAP_cat (35.0%, 69.9) -> combined RAP (36.0%, 71.1). Transfer-learning ablation (Table 6) shows memory built with GPT-3.5 improves other models: GPT-3.5 w/o memory 44.0% -> with GPT-3.5 memory 63.4%; Llama2-13b w/o memory 20.9% -> with GPT-3.5 memory 27.6%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieval-augmented episodic memory of successful trajectories substantially improves LLM/VLM agent performance across textual and multimodal tasks; adaptive retrieval (action vs observation), windowed trajectory retrieval, and multimodal embeddings (CLIP) are critical; memory built by one model can transfer to another model. Reported large absolute gains over ReAct baselines (e.g., +33.6% on ALFWorld, +13.0% on WebShop, +18.2% Franka Kitchen, +12.7% Meta-World as stated).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires collection of successful episode logs (memory depends on available successful examples); retrieval must avoid noise (mitigated by windowing), needs environment-specific weighting/hyperparameter tuning; reliance on embedding quality for cross-modal similarity; mapping high-level plans to actions in embodied tasks needs a trained policy network (few-shot MLP) and demonstrations; paper does not deeply analyze memory scaling or memory contamination from suboptimal episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8189.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8189.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A language-model agent framework that interleaves chain-of-thought style reasoning with environment actions, producing action and thought tokens iteratively to improve decision-making in interactive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM agent that alternates between 'thought' (reasoning) and 'action' (environment commands) tokens to handle sequential decision tasks; used as the primary baseline throughout the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5, GPT-4, Llama2-13b (as evaluated in baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard LLM backbones used as generative engines for ReAct agents (same models as RAP comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop (textual benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same tasks as above; ReAct serves as baseline agent that does not use external episodic memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / web navigation / interactive decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ALFWorld (GPT-3.5) success rate 52.2%; WebShop (GPT-3.5) success rate 35.0 and score 61.8%; ALFWorld (GPT-4) 85.8% reported for ReAct (paper reports ReAct with GPT-4 as a comparison point).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Used as baseline in comparisons showing RAP's gains; comparisons show RAP and retrieval variants substantially outperform ReAct across benchmarks (textual and multimodal).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReAct without external episodic memory underperforms when agents can exploit past successful trajectories via retrieval (RAP).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No explicit episodic retrieval; struggles on problems where analogy to prior successful episodes would help; performance is sensitive to base LLM capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8189.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8189.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that lets LLM agents self-reflect on unsuccessful attempts (verbal reinforcement) to derive improvements over time, using past trajectories as textual feedback for self-improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that records failures and prompts the LLM to reflect and summarize lessons from unsuccessful trajectories, using textual memory of experiences to guide future attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (as evaluated in baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard LLM used for Reflexion in cited baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied as a baseline where the agent leverages textual reflections of prior attempts to improve subsequent performance on the same tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / interactive decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>episodic textual reflection (text logs of unsuccessful/successful episodes used for self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Verbal self-reflection: the LLM is prompted with textual summaries of previous episodes (especially failures) to generate insights and modify future behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Textual trajectories, failure summaries, and self-generated lessons.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Prompting the LLM with selected past episodes and reflection prompts (text-based retrieval/concatenation rather than vector retrieval described in RAP).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>ALFWorld (GPT-3.5) success rate 74.6% (reported in Table 1 as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared against RAP and ReAct in ALFWorld and WebShop: Reflexion improves over ReAct but is outperformed by RAP which retrieves heterogeneous examples from other tasks and applies in-context analogy-making.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Textual self-reflection helps improve over memory-free baselines but is less effective than retrieval-augmented exemplar-based in-context learning across tasks examined by RAP.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reflexion focuses on reflecting on failures within the same task and typically requires re-running the LLM to produce reflections; may be less flexible than RAP's cross-task retrieval of diverse successful exemplars and is limited to textual modality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8189.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8189.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ADaPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ADaPT: As-needed decomposition and planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that decomposes tasks into sub-tasks and re-executes subcomponents as needed, used as a baseline for multi-step task solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Adapt: As-needed decomposition and planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ADaPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An approach that adaptively decomposes a task into subproblems and re-runs LLM planning/execution on subparts to recover from failures, used as a comparative baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (evaluated in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard LLM used for baseline ADaPT comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>ALFWorld, WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Applied as a baseline which decomposes tasks and re-executes subplans; compared to RAP on multi-step environments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / interactive decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>ALFWorld (GPT-3.5) success rate 71.6% (Table 1); WebShop (GPT-3.5) success rate 43.0 and score 64.0 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Compared versus RAP and other baselines; RAP outperforms ADaPT on ALFWorld and WebShop, showing the effectiveness of retrieval-based cross-task analogies rather than only decomposition/re-execution.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposition/re-execution helps but is less effective than retrieval-augmented exemplar use across the tested benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Does not inherently exploit cross-task episodic memory; improvements are task-local (within the same task instance) and may require extra re-executions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8189.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8189.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the memory mechanism, tasks, comparative results, ablations, and key findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented method that retrieves relevant documents from a corpus using semantic search and conditions a generative model on retrieved passages to ground responses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RAG (as related work)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A retrieval+generation paradigm where retrieved documents are concatenated into the prompt for an LLM to produce grounded responses; cited as motivating retrieval augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>general knowledge-intensive tasks (cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Used in literature to ground generation for knowledge-heavy queries by retrieving documents from an external index.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge-grounded generation / QA</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external document retrieval (index of passages)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism</strong></td>
                            <td>Vector retrieval of documents and prompt concatenation to LLM</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>External corpus passages/documents</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Semantic search (vector similarity) and prompt augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_comparison</strong></td>
                            <td>Mentioned in related work as inspiration; RAP extends retrieval ideas to episodic trajectory retrieval, multimodal observations, and trajectory-window retrieval rather than document retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG-style retrieval improves grounding of generative models; RAP adapts this concept to episodic trajectory retrieval for agent planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>RAG is document-centric and not directly designed for episodic trajectory or multimodal retrieval of past agent behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Adapt: As-needed decomposition and planning with language models <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>ExpeL <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8189",
    "paper_id": "paper-267500377",
    "extraction_schema_id": "extraction-schema-150",
    "extracted_data": [
        {
            "name_short": "RAP",
            "name_full": "Retrieval-Augmented Planning",
            "brief_description": "A framework that augments LLM/VLM agents with an episodic memory of past successful task executions, retrieving relevant trajectory windows (plans, actions, observations) using a weighted semantic-similarity retriever and applying retrieved examples via in-context learning to generate next actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RAP (Retrieval-Augmented Planning)",
            "agent_description": "An agent architecture composed of four modules — Memory (episodic log database), Reasoner (LLM generates plans and retrieval keys), Retriever (semantic similarity weighted over task/plan/key), and Executor (LLM uses retrieved exemplars via in-context learning). Switches retrieval between action- or observation-based similarity depending on context and environment.",
            "model_name": "GPT-3.5, GPT-4, Llama2-13b; LLaVA, CogVLM (for multimodal)",
            "model_description": "Evaluated with API LLMs (GPT-3.5, GPT-4) and local Llama2-13b for text tasks; multimodal agents built on VLMs LLaVA-v1.5-13B and Cog-VLM-17B (10B visual + 7B language) for embodied tasks.",
            "task_name": "ALFWorld, WebShop, Franka Kitchen, Meta-World",
            "task_description": "ALFWorld: multi-step text-based household tasks in simulated interactive environment; WebShop: web navigation and product selection; Franka Kitchen & Meta-World: multimodal embodied robotic manipulation tasks requiring visual perception and action.",
            "task_type": "multi-step reasoning / interactive decision-making / web navigation / embodied manipulation (multimodal)",
            "memory_used": true,
            "memory_type": "episodic retrieval-augmented memory (external memory database of past successful episodes)",
            "memory_mechanism": "Store logs of successful episodes (task info Ti, overall plan pi, trajectory τi = sequences of action plans, actions, observations). Retrieval by computing weighted cosine-similarity over embeddings of task, overall plan, and retrieval key; text embeddings via sentence-transformers, image embeddings via CLIP/ViT. Retrieve a windowed trajectory centered on most similar action and pass examples to LLM as in-context examples for Executor.",
            "memory_representation": "Stored entries: {task description, overall plan, trajectory sequences of plans, actions, textual observations or image observations}. For multimodal entries observations are images from a fixed viewpoint.",
            "memory_retrieval_method": "Semantic vector retrieval with weighted similarity score: Score = wt * sim(task,task_i) + wp * sim(plan,plan_i) + wk * sim(retrieval_key,trajectory_i); retrieval-key similarity computed adaptively (max cosine between key and logged observations or actions). Adaptive component weights per environment; return windowed trajectory around the most similar action.",
            "performance_with_memory": "ALFWorld (GPT-3.5) success rate 85.8%; ALFWorld RAP_train (memory from training set + recursive) 91.0%; WebShop (GPT-3.5) success rate 48.0, overall reward score 76.1%; Franka Kitchen (LLaVA) success 61.6% (vs 43.4% baseline); Meta-World (LLaVA) success 79.2% (vs 65.4% baseline); CogVLM Franka 56.9% (vs 44.2%).",
            "performance_without_memory": "ALFWorld ReAct (GPT-3.5 baseline) success rate 52.2%; WebShop ReAct (GPT-3.5) success rate 35.0 and score 61.8%; Franka Kitchen LLaVA baseline 43.4% success; Meta-World LLaVA baseline 65.4%.",
            "has_performance_with_without_memory": true,
            "ablation_or_comparison": "Ablations on Retriever: RAP_act (action-only retrieval) vs RAP_obs (observation-only) vs RAP_clip (image observations with CLIP) showed action/observation switching improves results; Table 4 (ALFWorld, Llama2-13b) ReAct 52.2% -&gt; RAP_act 82.1% -&gt; RAP_obs 84.3% -&gt; RAP_clip 86.6% -&gt; RAP 85.8%. WebShop retriever ablations (Table 5) show incremental gains from RAP_act (33.0% success, 68.6 score) -&gt; RAP_obs (33.0%, 69.0) -&gt; RAP_intra (34.0%, 69.3) -&gt; RAP_cat (35.0%, 69.9) -&gt; combined RAP (36.0%, 71.1). Transfer-learning ablation (Table 6) shows memory built with GPT-3.5 improves other models: GPT-3.5 w/o memory 44.0% -&gt; with GPT-3.5 memory 63.4%; Llama2-13b w/o memory 20.9% -&gt; with GPT-3.5 memory 27.6%.",
            "key_findings": "Retrieval-augmented episodic memory of successful trajectories substantially improves LLM/VLM agent performance across textual and multimodal tasks; adaptive retrieval (action vs observation), windowed trajectory retrieval, and multimodal embeddings (CLIP) are critical; memory built by one model can transfer to another model. Reported large absolute gains over ReAct baselines (e.g., +33.6% on ALFWorld, +13.0% on WebShop, +18.2% Franka Kitchen, +12.7% Meta-World as stated).",
            "limitations_or_challenges": "Requires collection of successful episode logs (memory depends on available successful examples); retrieval must avoid noise (mitigated by windowing), needs environment-specific weighting/hyperparameter tuning; reliance on embedding quality for cross-modal similarity; mapping high-level plans to actions in embodied tasks needs a trained policy network (few-shot MLP) and demonstrations; paper does not deeply analyze memory scaling or memory contamination from suboptimal episodes.",
            "uuid": "e8189.0",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing reasoning and acting in language models",
            "brief_description": "A language-model agent framework that interleaves chain-of-thought style reasoning with environment actions, producing action and thought tokens iteratively to improve decision-making in interactive tasks.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "An LLM agent that alternates between 'thought' (reasoning) and 'action' (environment commands) tokens to handle sequential decision tasks; used as the primary baseline throughout the paper.",
            "model_name": "GPT-3.5, GPT-4, Llama2-13b (as evaluated in baselines)",
            "model_description": "Standard LLM backbones used as generative engines for ReAct agents (same models as RAP comparisons).",
            "task_name": "ALFWorld, WebShop (textual benchmarks)",
            "task_description": "Same tasks as above; ReAct serves as baseline agent that does not use external episodic memory.",
            "task_type": "multi-step reasoning / web navigation / interactive decision-making",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "ALFWorld (GPT-3.5) success rate 52.2%; WebShop (GPT-3.5) success rate 35.0 and score 61.8%; ALFWorld (GPT-4) 85.8% reported for ReAct (paper reports ReAct with GPT-4 as a comparison point).",
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Used as baseline in comparisons showing RAP's gains; comparisons show RAP and retrieval variants substantially outperform ReAct across benchmarks (textual and multimodal).",
            "key_findings": "ReAct without external episodic memory underperforms when agents can exploit past successful trajectories via retrieval (RAP).",
            "limitations_or_challenges": "No explicit episodic retrieval; struggles on problems where analogy to prior successful episodes would help; performance is sensitive to base LLM capability.",
            "uuid": "e8189.1",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language agents with verbal reinforcement learning",
            "brief_description": "A method that lets LLM agents self-reflect on unsuccessful attempts (verbal reinforcement) to derive improvements over time, using past trajectories as textual feedback for self-improvement.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "use",
            "agent_name": "Reflexion",
            "agent_description": "An agent that records failures and prompts the LLM to reflect and summarize lessons from unsuccessful trajectories, using textual memory of experiences to guide future attempts.",
            "model_name": "GPT-3.5 (as evaluated in baseline comparison)",
            "model_description": "Standard LLM used for Reflexion in cited baseline experiments.",
            "task_name": "ALFWorld, WebShop",
            "task_description": "Applied as a baseline where the agent leverages textual reflections of prior attempts to improve subsequent performance on the same tasks.",
            "task_type": "multi-step reasoning / interactive decision-making",
            "memory_used": true,
            "memory_type": "episodic textual reflection (text logs of unsuccessful/successful episodes used for self-reflection)",
            "memory_mechanism": "Verbal self-reflection: the LLM is prompted with textual summaries of previous episodes (especially failures) to generate insights and modify future behavior.",
            "memory_representation": "Textual trajectories, failure summaries, and self-generated lessons.",
            "memory_retrieval_method": "Prompting the LLM with selected past episodes and reflection prompts (text-based retrieval/concatenation rather than vector retrieval described in RAP).",
            "performance_with_memory": "ALFWorld (GPT-3.5) success rate 74.6% (reported in Table 1 as baseline)",
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Compared against RAP and ReAct in ALFWorld and WebShop: Reflexion improves over ReAct but is outperformed by RAP which retrieves heterogeneous examples from other tasks and applies in-context analogy-making.",
            "key_findings": "Textual self-reflection helps improve over memory-free baselines but is less effective than retrieval-augmented exemplar-based in-context learning across tasks examined by RAP.",
            "limitations_or_challenges": "Reflexion focuses on reflecting on failures within the same task and typically requires re-running the LLM to produce reflections; may be less flexible than RAP's cross-task retrieval of diverse successful exemplars and is limited to textual modality.",
            "uuid": "e8189.2",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ADaPT",
            "name_full": "ADaPT: As-needed decomposition and planning with language models",
            "brief_description": "A method that decomposes tasks into sub-tasks and re-executes subcomponents as needed, used as a baseline for multi-step task solving.",
            "citation_title": "Adapt: As-needed decomposition and planning with language models",
            "mention_or_use": "use",
            "agent_name": "ADaPT",
            "agent_description": "An approach that adaptively decomposes a task into subproblems and re-runs LLM planning/execution on subparts to recover from failures, used as a comparative baseline.",
            "model_name": "GPT-3.5 (evaluated in comparisons)",
            "model_description": "Standard LLM used for baseline ADaPT comparisons.",
            "task_name": "ALFWorld, WebShop",
            "task_description": "Applied as a baseline which decomposes tasks and re-executes subplans; compared to RAP on multi-step environments.",
            "task_type": "multi-step reasoning / interactive decision-making",
            "memory_used": false,
            "memory_type": null,
            "memory_mechanism": null,
            "memory_representation": null,
            "memory_retrieval_method": null,
            "performance_with_memory": null,
            "performance_without_memory": "ALFWorld (GPT-3.5) success rate 71.6% (Table 1); WebShop (GPT-3.5) success rate 43.0 and score 64.0 (Table 2).",
            "has_performance_with_without_memory": false,
            "ablation_or_comparison": "Compared versus RAP and other baselines; RAP outperforms ADaPT on ALFWorld and WebShop, showing the effectiveness of retrieval-based cross-task analogies rather than only decomposition/re-execution.",
            "key_findings": "Decomposition/re-execution helps but is less effective than retrieval-augmented exemplar use across the tested benchmarks.",
            "limitations_or_challenges": "Does not inherently exploit cross-task episodic memory; improvements are task-local (within the same task instance) and may require extra re-executions.",
            "uuid": "e8189.3",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A retrieval-augmented method that retrieves relevant documents from a corpus using semantic search and conditions a generative model on retrieved passages to ground responses.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "mention",
            "agent_name": "RAG (as related work)",
            "agent_description": "A retrieval+generation paradigm where retrieved documents are concatenated into the prompt for an LLM to produce grounded responses; cited as motivating retrieval augmentation.",
            "model_name": null,
            "model_description": null,
            "task_name": "general knowledge-intensive tasks (cited work)",
            "task_description": "Used in literature to ground generation for knowledge-heavy queries by retrieving documents from an external index.",
            "task_type": "knowledge-grounded generation / QA",
            "memory_used": true,
            "memory_type": "external document retrieval (index of passages)",
            "memory_mechanism": "Vector retrieval of documents and prompt concatenation to LLM",
            "memory_representation": "External corpus passages/documents",
            "memory_retrieval_method": "Semantic search (vector similarity) and prompt augmentation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": null,
            "ablation_or_comparison": "Mentioned in related work as inspiration; RAP extends retrieval ideas to episodic trajectory retrieval, multimodal observations, and trajectory-window retrieval rather than document retrieval.",
            "key_findings": "RAG-style retrieval improves grounding of generative models; RAP adapts this concept to episodic trajectory retrieval for agent planning.",
            "limitations_or_challenges": "RAG is document-centric and not directly designed for episodic trajectory or multimodal retrieval of past agent behaviors.",
            "uuid": "e8189.4",
            "source_info": {
                "paper_title": "RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Adapt: As-needed decomposition and planning with language models",
            "rating": 2,
            "sanitized_title": "adapt_asneeded_decomposition_and_planning_with_language_models"
        },
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "ExpeL",
            "rating": 1
        }
    ],
    "cost": 0.01517575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
6 Feb 2024</p>
<p>Tomoyuki Kagaya <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#107;&#97;&#103;&#97;&#121;&#97;&#46;&#116;&#111;&#109;&#111;&#121;&#117;&#107;&#105;&#64;&#106;&#112;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;">&#107;&#97;&#103;&#97;&#121;&#97;&#46;&#116;&#111;&#109;&#111;&#121;&#117;&#107;&#105;&#64;&#106;&#112;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;</a> 
Equal contribution</p>
<p>Panasonic Connect Co., Ltd
Japan</p>
<p>Jing Thong <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#106;&#105;&#110;&#103;&#121;&#117;&#97;&#110;&#46;&#116;&#104;&#111;&#110;&#103;&#64;&#115;&#103;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;">&#106;&#105;&#110;&#103;&#121;&#117;&#97;&#110;&#46;&#116;&#104;&#111;&#110;&#103;&#64;&#115;&#103;&#46;&#112;&#97;&#110;&#97;&#115;&#111;&#110;&#105;&#99;&#46;&#99;&#111;&#109;</a> 
Yuan 
Equal contribution</p>
<p>Panasonic R&amp;D Center
Singapore</p>
<p>Yuxuan Lou lou@u.nus.edu&gt;. 
Equal contribution</p>
<p>National University of Singapore
Singapore</p>
<p>Jayashree Karlekar 
Panasonic R&amp;D Center
Singapore</p>
<p>Sugiri Pranata 
Panasonic R&amp;D Center
Singapore</p>
<p>Akira Kinose 
Panasonic Connect Co., Ltd
Japan</p>
<p>Koki Oguri 
Panasonic Connect Co., Ltd
Japan</p>
<p>Felix Wick 
Panasonic R&amp;D Center Ger-many
Germany</p>
<p>Yang You 
National University of Singapore
Singapore</p>
<p>Thong Jing Yuan</p>
<p>RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
6 Feb 202479DDD7B142ED638CFA471A76298A3560arXiv:2402.03610v1[cs.LG]
Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration.However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges.Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities.RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks.Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks.These results highlight RAP's potential in advancing the functionality and applicability of LLM agents in complex, real-world applications.</p>
<p>Introduction</p>
<p>Recent research has revealed the high inferential abilities of Large Language Models (LLMs) as agents (Wang et al., 2023b;Xi et al., 2023), indicating their potential application in various areas like decision-making tasks and robotic control.Previous works such as ReAct (Yao et al., 2023) have shown that LLMs can generate accurate actions as language agents by iteratively performing actions and reasoning.</p>
<p>In this paper, we introduce a novel framework, Retrieval-Augmented Planning (RAP), which embodies a pivotal human ability -to leverage generalized past experiences for current tasks -and apply it to LLM agents.Our approach involves storing past experiences in memory, retrieving them appropriately based on the similarity with present context including multimodal information, and generating subsequent actions via in-context learning, thereby enhancing the decision-making capacity of language agents.Central to this framework is the LLMs' ability to perform analogy-making from various abstracted patterns (Mirchandani et al., 2023).Leveraging this capability, our memory stores both context and action-observation trajectories for each experience.The approach effectively facilitates deriving correct actions from memory examples within task constraints.Furthermore, by storing multimodal information in memory and considering it when retrieving past experiences, our approach flexibly utilizes multimodal information with LLMs and Vision-Language Models (VLMs) separately for language agents.Consequently, our approach proves to be effective for memory utilization by language agents in both decision-making and robotics tasks, in textual and multimodal environments.Specifically, RAP achieves 33.6%, 13.0%, 18.2%, and 12.7% gain over ReAct on the ALFWorld, Webshop, Franka Kitchen, and Meta World benchmarks respectively.</p>
<p>To summarize, our contributions are as follows:</p>
<p>• We propose RAP, a novel framework that enhances LLM agents' planning capacity.It strategically enriches the decision-making process by storing past experiences and intelligently retrieving them based on their similarity to the current situation.• RAP is capable of being applied not only in textual environments but also in multimodal embodied tasks, marking it as a pioneering effort in employing memory retrieval techniques for multimodal agents, a first in this domain to our knowledge.• We validate the effectiveness of RAP empirically across both textual and multimodal benchmarks.RAP shows significant improvements compared to prior SOTA methods in both types of environments.</p>
<p>Related Work</p>
<p>Language Models and Vision-Language Models as Foundations</p>
<p>Large Language Models (LLMs) such as GPT (OpenAI, 2023) and the LLaMA series (GenAI, 2023), leveraging transformer architecture and self-supervised learning objectives like Next Token Prediction, have excelled in generating coherent, human-like text.These models, pre-trained on extensive text corpora, possess vast linguistic knowledge and reasoning abilities.Extending beyond LLMs, multi-modal domains have given rise to Vision-Language Models (VLMs) (Yin et al., 2023) such as LLaVA (Liu et al., 2023) and CogVLM (Wang et al., 2023c), which integrate textual and visual inputs, exemplified by LLaVA's training on image-caption pairs from the CC3M datasets (Sharma et al., 2018).Our work utilizes these foundations to build agents for textual and embodied environments: text-based agents employing LLMs, and embodied agents integrating VLMs for visual perception and action planning.We focus on enhancing these agents' planning capabilities through memory retrieval techniques, enabling them to selectively access and utilize relevant memory for improved sequential decision-making.</p>
<p>Language Models as AI Agents</p>
<p>Recent works have leveraged LLM's anthropomorphic capabilities when building autonomous agents.These agents can be depicted into having 4 key aspects: Profile (agent characteristics), Memory (past information), Planning (future strategies) and Action (execution policies) (Wang et al., 2023a).A notable example is Chain-of-Thought (CoT) (Wei et al., 2022), where agents are encouraged to mirror human cognitive mechanisms by incorporating reasoning into intermediate steps for complex problem-solving tasks.With a dynamic reasoning process, ReAct (Yao et al., 2023) interleaves generated actions and environmental states, improving the reasoning ability through action-state synergy.</p>
<p>Our work seeks to enhance the ReAct framework by allowing agents to identify specific objects within observations and additionally retrieve relevant aspects of past experiences</p>
<p>Retrieval-Augmented Generation with Memory</p>
<p>Among works (Madaan et al., 2022;Liu et al., 2021;Su et al., 2022) that seek to derive better answers from LLMs via memory, RAG (Lewis et al., 2020) is a notable method that combines retrieval-based mechanisms with generative models.Responses from memory are selected based on similarity and passed into LLMs as additional context to deliver outputs that are creative and contextually-grounded.Building on RAG, Reflexion (Shinn et al., 2023) requires LLMs to self-reflect on unsuccessful tasks for self-improvement in solving tasks over time.ADaPT (Prasad et al., 2023) further decomposes into sub-tasks and re-executes where necessary.Yet, these works only reflect on trajectories within the task.Hence, these insights are often restricted to each task.Building on Reflexion, ExpeL (Zhao et al., 2023) passes all generated experiences into the LLM to reflect in a text-based manner.In contrast, our work adopts a different approach by implicitly drawing from a diverse range of experiences from memory without explicitly requiring an additional step of re-tasking the LLM to extract insights.With this approach, our agent can not only efficiently generalize experiences from other successful tasks to solve the current task, but also be flexible enough to extract relevant components from experiences for current task, enhancing the agent's ability to expand its memory from textual to multimodal contexts.</p>
<p>RAP: Retrieval-Augmented Planning</p>
<p>We developed Retrieval-Augmented Planning (RAP), a framework that leverages past experiences to facilitate decision-making according to the current context.Fig. 2 provides an overview of the framework, which consists of four core components: Memory, Reasoner, Retriever, and Executor.The specific details of each module will be discussed in sections 3.2 to 3.5.</p>
<p>Preliminaries</p>
<p>In this work, we consider an agent operating in a particular environment and assigned with completing some task T.</p>
<p>Memory</p>
<p>To enable retrieval-augmented planning, we first construct memory databases.The databases contain logs of prior successful task executions.For each log L i completing a task T i in H i steps, we record the task information T i , the overall plan p i , the trajectory of the agent τ Li including plans, actions, and observations sequences.
L i = {T i , p i , τ Li } (1) τ Li = { ⃗ p ′ Li , ⃗ α Li , ⃗ o Li }(2)
For textual environments, the observations are textual descriptions of the world state.For multimodal environments, the observations are visual representations -images from a fixed viewpoint camera after each agent action.</p>
<p>The logs are collected by having agents attempt the tasks and saving streams of successful episodes.The episodic logs capture the steps needed to complete the tasks.Storing these examples allows the agents to leverage prior experience when planning for new instances of the tasks.</p>
<p>During interactions with the environment, the agents can selectively retrieve relevant memory samples to make more informed action decisions.For text tasks, the textual logs provide crucial context.For embodiment, prior visual observations reveal outcomes of actions in the space.By retrieving prototypical executions, the agents can plan smarter policies while avoiding past failures.The memory augmentation thus equips the models with vital environmental knowledge for sequential decision-making.</p>
<p>Reasoner</p>
<p>The Reasoner generates overall plans, action plans, and retrieval keys based on the agent's current situation and action trajectory, using LLMs.Initially, the Reasoner produces the overall plan from the task information.Based on the task and the overall plan, the Reasoner generates an action plan.Also, in accordance with ReAct, an action or action plan is dynamically generated by LLMs, considering the current task status.If an action plan is generated, a retrieval key is created based on the generated action plan.For instance, in ALFWorld (Shridhar et al., 2021), if an action plan like "I need to find the watch" is generated, the retrieval key would be "search watch".Hence, the Reasoner enables agents to take into consideration the current situation and context.</p>
<p>Retriever</p>
<p>The Retriever is designed to extract the most relevant memory logs to guide the agent's subsequent actions to complete the current task.This process is shown in Fig. 3 (Left).</p>
<p>The similarity score, comparing the current state S 0 with log L i , is calculated as a weighted average of the task similarity, overall plan alignment, and retrieval key congruence.</p>
<p>Let the current agent task be T 0 , the overall plan be p 0 , and the retrieval key generated by the Reasoner based on current action plan p ′ be k 0 .The similarity score between current state S 0 with log L i is calculated as a weighted average of the similarity score for task, overall plan, and retrieval key.
Score(S 0 , L i ) = w t • sim(T 0 , T Li ) + w p • sim(p 0 , p Li ) + w k • sim(k 0 , τ Li )(3)
Each component's similarity score is determined using cosine similarity of their feature representations.For text data, the representations are derived using sentence-transformers (Reimers &amp; Gurevych, 2019).For images, the representa-tions are generated with a CLIP-based Vision Transformer.</p>
<p>The similarity score between the retrieval key and the log trajectory is adaptive based on environment type and retrieval key type.In multimodal environments, the retrieval key corresponds to agents' current visual observation.Thus, the retrieval-key similarity score is the score between current and logged visual trajectory observations as in equation ( 4).
sim(k 0 , τ Li ) = max(cos sim(k visual 0 , o j )), for o j ∈ ⃗ o Li(4)
In textual environments, the retrieval-key similarity score is adaptive based on key type.In scenarios involving the retrieval key for searching or locating objects, the similarity score is calculated between the retrieval key and the logged textual trajectory observations, as in equation ( 5).
sim(k 0 , τ Li ) = max(cos sim(k text 0 , o j )), for o j ∈ ⃗ o Li (5)
For the case retrieval-key is interacting with object action planning, it is an action similarity score, as in equation ( 6).
sim(k 0 , τ Li ) = max(cos sim(k text 0 , α j )), for α j ∈ ⃗ α Li (6)
Furthermore, component weights are adaptively calibrated based on the environment.In environments with a constrained task space, task similarity is assigned a higher weight, For example, in Franka Kitchen environment which has only 5 tasks.We only retrieve logs with same task type.</p>
<p>For each retrieved experience, only a window of trajectory centered around the most similar action is passed to the agent.This allows agents to focus on actions most similar to current task, rather than full trajectories that may instead create additional noise to the agent.</p>
<p>In summary, our meticulously-crafted retrieval method efficiently identifies the most pertinent logs by calculating a weighted similarity score that takes into account various aspects including task information, overall planning, and retrieval key.This process ensures that the most relevant and contextually appropriate logs are selected from a vast repository of memory logs.Once these logs are retrieved, they serve as an invaluable resource for the large language model serving as Executor.</p>
<p>Executor</p>
<p>The Executor receives past experiences from the Retriever and generates the next action by utilizing these experiences through in-context learning.This process is illustrated in Fig. 3 (Right).By presenting the past experience aligned with the current context as a prompt, it enables accurate decisionmaking for the next action, mirroring the process humans leverage past experiences for future actions.Additionally, the length of the current task trajectory is used in the same way as past experiences, utilizing only a constant number of new trajectories.This encourages effective analogy-making from experiences through in-context learning in LLMs.</p>
<p>Experiments</p>
<p>To validate the effectiveness of our framework in various environments, we performed evaluations on four benchmarks.These include the text-based multi-step tasks in ALFWorld (Shridhar et al., 2021) and Webshop (Yao et al., 2022), and robotics tasks, FrankaKitchen (Gupta et al., 2019) and Meta-World (Yu et al., 2021), which are multimodal environments with texts and images.ALFWorld (Shridhar et al., 2021) is a synthetic text-based game that challenges an agent to solve multi-step tasks in a variety of interactive environments based on TextWorld (Côté et al., 2018).Following ReAct, we evaluated an agent in 134 unseen games, including six types of tasks: Pick, Clean, Heat, Cool, Look, and Pick2.In this environment, agents are required to accomplish complex tasks using textbased actions in a simulated household providing textual and image feedback.Following previous works (Yao et al., 2023;Shinn et al., 2023;Prasad et al., 2023), RAP runs recursively until it reaches a depth (trial) of 3. In RAP train , we use 1000 tasks from the provided training set, and run recursively with memory from successful tasks both from the training set and previous trials.Additionally, we use task information including the task type for the Retriever.During action-based similarity calculation, it extracts four experiences and ten actions from both before and after the most similar action.Meanwhile, during observation-based similarity calculation, it uses eight experiences and five actions from both before and after the most similar action.</p>
<p>We conducted evaluations using three models: GPT-3.5 (Ouyang et al., 2022), GPT-4 (OpenAI, 2023), andLlama2-13b (GenAI, 2023).In Table 1, our experiments with GPT-3.5 show that RAP(85.8%) and RAP train (91.0%) achieve a significantly higher success rate compared to previous works such as ReAct(52.2%),Reflexion(74.6%),and ADaPT(71.6%).Further, RAP is also effective even with high-performance models like GPT-4, and locally-operated models such as Llama2-13b, thus illustrating the efficacy of our method across various LLMs.</p>
<p>Figure 4 highlights the progression of improvements to success rate over three trials, where RAP shows higher success rates compared to ReAct, thus indicating the effective utility of successful experiences from other tasks in memory.In addition, RAP with GPT-3.5 achieves performance equivalent * We use the performance reported by (Prasad et al., 2023) to ReAct with GPT-4.Also, RAP with memory built from the training set via GPT-3.5 surpasses ReAct with GPT-4.</p>
<p>WEBSHOP</p>
<p>WebShop (Yao et al., 2022) is a web application that simulates online shopping, where agents are required to select products for purchase based on a given user instruction.</p>
<p>WebShop contains a total of 1.18M real-world products featured on Amazon, and comprises a wide variety of structured and unstructured texts.Following Reflexion (Shinn et al., 2023) and ADaPT (Prasad et al., 2023), we evaluated an agent across 100 instructions.For each instruction, agents are required to reason and select a desired product that is most aligned to the given instruction based on observations returned by the web application, and perform additional precise interactions with the portal to navigate through the web application such as searching or clicking buttons.Such interactions are performed in a text-based manner where the agent issues a textual command into the web application.Following previous studies, we allow the agent to run recursively until it reaches a depth (trial) of 3.</p>
<p>During our evaluation, we ran the initial trial with a Re-Act agent and formulated the memory database based on successful tasks.The memory database would be further expanded for subsequent trials based on successful tasks in the preceding trials.Here, successful tasks are counted as those with a reward of 1.In addition, during retrieval of actions in memory, the Retriever extracts three experiences and five actions from before and after the most similar action.</p>
<p>Moreover, unlike other environments where objects are generalizable across different tasks, WebShop has an additional unique feature where actions in each task are highly dependent on the scenario outlined in that task.As such, apart from the correlation between the current reasoning and trajectories in memory, our agent also considers the relationship between the action of each task in memory that is most similar to the current action and its corresponding scenario for that task.This builds on the concept of "A is to B as C is to D", where the generated action depends not only on similar trajectories in memory, but also how these trajectories relate back to their scenario, and how the current trajectory should be related to the current scenario.By incorporating intra-task relationships, this allows the agent to better reason how the actions in memory are correlated with their own scenarios, and thereafter generate an action that is also aligned to the current scenario at hand.</p>
<p>We performed evaluations using two different models: GPT-3.5 (Ouyang et al., 2022) and Llama2-13b (GenAI, 2023).</p>
<p>In Table 2, experiments with GPT-3.5 demonstrate that our method (48.0%) achieves a higher success rate compared to previous studies such as ReAct (35.0%),Reflexion (35.0%), and ADaPT (43.0%).Furthermore, our method is able to achieve a higher overall reward score (76.1%) as compared to ReAct (61.8%),Reflexion (61.8%) and ADaPT (64.0%).</p>
<p>Multimodal Environments</p>
<p>We evaluated our proposed technique in embodied multimodal agents on two benchmark environments: Franka Kitchen and Meta-World.These simulations offer a diverse set of household and robotic manipulation tasks requiring visual perception and physical interaction.</p>
<p>We constructed embodied agents using two VLM foundations -LLaVA and CogView.For each VLM, we compared task performance of the base model to a RAP-enhanced agent utilizing our memory retrieval system.</p>
<p>The Franka Kitchen benchmark consists of compound tasks like arranging objects and preparing meals.Meta-World provides a suite of 50 distinct robotic skills focused on fine manipulation.For both sets, the agent must plan actions based on visual observations in an interactive 3D environment.We report quantitative results on task success rates with and without RAP augmentation.Our method allows the VLM Agents to selectively reference prior successful executions during planning.This provides vital visual context and demonstrates the benefits of memory-augmented reasoning for embodied agents.</p>
<p>To map the high-level plans of the VLM agents to executable environment actions, we train a policy network on 25 demonstrations for each task.We evaluate on 5 subtasks with 2 different camera views per benchmark.For each (task, view) combination we run 50 trials with different random seeds and report success rates.</p>
<p>Table 3 and Figure 6 shows that RAP can significantly enhance embodied multimodal agents planning on both benchmarks.The results offer insights into how memory can aid these models for sequential decision making and embodied tasks requiring interactive visual perception.</p>
<p>Ablation Study</p>
<p>In this section, we provide additional experimental results and showcase some critical components of experiences that are required to be stored into memory.</p>
<p>Evaluation across various Retrievers</p>
<p>ALFWORLD EVALUATION</p>
<p>We perform evaluation on ALFWorld with GPT-3.5 by varying the Retriever, as shown in Table 4.The results of RAP act and RAP obs illustrate the effectiveness of switching the information used for retrieval depending on the situation.</p>
<p>Furthermore, we utilize visual observation provided by ALF-World instead of textual observation, and perform an evaluation using similarity between textual retrieval key and image observation with CLIP (Radford et al., 2021).As a result, slightly better performance is demonstrated than when using text observation information.This suggests that employing direct image data, rather than information converted into text, could enable more effective retrieval.Here, all evaluations are performed on Llama2-13b.</p>
<p>As shown in Table 5, through RAP obs , the agent is able to retrieve trajectories from memory based on either actions or observations, depending on the current stage of solving the task.With the incorporation of intra-task similarity in RAP intra , the agent is able to align the relationship between task information and the corresponding trajectories of each experience when projecting to the current task.By retrieving based on product category in RAP cat , the agent is able to retrieve experiences that are more related to the current task.</p>
<p>Overall, RAP takes into account these components, resulting in an overall boost of 6.5% and 5.0% for overall reward and success rate respectively on Llama2-13b.With these, RAP also demonstrates a boost of 14.3% and 13.0% for overall reward and success rate respectively on GPT-3.5 in Table 2, showcasing RAP's generalizability across different models.</p>
<p>Transfer Learning via Memory</p>
<p>RAP is capable of utilizing past experiences that are stored in memory.Since the experience of solving tasks is independent of the model, the model used for evaluation does not need to match the one used for memory construction.Here, we illustrate a verification of transfer learning between models by using memory constructed via different models for the evaluation model.From Section 4.1.1,we use 1000 samples from training data, but no recursive trial is conducted (d max = 1) to simply verify the effect of transfer learning.</p>
<p>Table 6 shows results of transfer learning, which indicate memory generated with GPT-3.5 is also effective in Llama2-13b.Thus, RAP allows sharing experiences across models.</p>
<p>Table 6.ALFWorld success rate(%) with Memory and dmax=1.</p>
<p>ModelMemory indicates the language model used to construct memory from the training data.</p>
<p>Model</p>
<p>ModelMemory Success Rate GPT-3.5 -44.0 GPT-3.5 GPT-3.5 63.4 Llama2-13b -20.9 Llama2-13b GPT-3.5 27.6</p>
<p>Conclusion</p>
<p>We propose Retrieval-Augmented Planning (RAP), which stores past experiences, extracts pertinent experiences from multimodal information such as text and images, and guides subsequent actions.Our framework demonstrated superior performance compared to baseline methods in various LLMs and across four diverse agent and robotics benchmarks.Through these results, our framework enables language agents to flexibly utilize past experiences in accordance with current situations, mirroring a human ability, and thereby enhancing decision-making capabilities.</p>
<p>Impact Statements</p>
<p>This paper presents work whose goal is to advance the field of Machine Learning.There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p>
<p>B.1.2. PROMPT FOR RETRIEVAL KEY IN REASONER</p>
<p>Here are examples.think: First I need to find a spraybottle.A spraybottle is more likely to appear in cabinet (1-4), countertop (1), toilet (1), sinkbasin (1-2), garbagecan (1).I can check one by one, starting with cabinet 1. &gt; search: spraybottle think: Now I put the first creditcard in dresser.Next, I need to find the second creditcard.I can directly go to countertop 1. &gt; search: creditcard think: Now I take a pen (2).Next, I need to find a desklamp.A desklamp is more likely to appear in dresser (1), shelf (1-9), bed (1), garbagecan (1), drawer (1-10  Action: think[For mn4 color foundation for sensitive skin, the item has options '1 ', 'dc1', 'dn3', 'dn4', 'lc1', 'ln3', 'ln4', 'lw2','mc1','mn3','mn4', and'</p>
<p>Figure 1 .
1
Figure 1.Overview of RAP.Our framework stores past experiences and retrieves them based on the current situation.Left: The evaluation process on ALFWorld.ICL stands for in-context learning.Right: The evaluation process on Franka Kitchen.</p>
<p>Figure 2 .
2
Figure 2. RAP Core Components based on the current context.By doing so, our agent can adaptably receive different experiences at different points in time that are most similar to the situation at hand.</p>
<p>Figure 3 .
3
Figure 3. Memory-Retrieval in RAP.Left to Middle: The Retriever, calculating similarities with Memory, dynamically switches between action or observation based on the Retrieval Key.This figure illustrates the process of calculating similarity with observation.Right: Executor receives related experiences from memory and utilizes them in the prompt.</p>
<p>Figure 4 .
4
Figure 4. Success Rate of ALFWorld with trial number 4.1.Textual Environments 4.1.1.ALFWORLD</p>
<p>Figure 5
5
Figure5illustrates a comparison of the improvement in success rate and reward score over three trials for ReAct and RAP, with both methods using GPT-3.5.Here, it indicates a marked improvement in both success rate and reward score for our method, indicating that successful experiences from the other tasks are being effectively utilized.</p>
<p>Figure 5 .
5
Figure 5. Success Rate and Reward Score of WebShop over 3 trials</p>
<p>Figure 6 .
6
Figure6.Evaluation on Franka-Kitchen and Meta-World Benchmark.We evaluate with two vision-language models LLaVA and CogVLM both w/wo our proposed RAP method.The results demonstrate that our method notably enhances the performance of multimodal LLM agents in executing embodied tasks.</p>
<p>Action</p>
<p>[dc1][dn3][dn4][lc1][ln3][ln4][lw2]  [mc1][mn3][mn4][mw2]  size [1 fl oz (pack of 1)][1 ounce (pack of 1)][1 fluid ounce] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz Price: $7.59 Rating: N.A. [Description] [Features] [Reviews] [Attributes] [Buy Now] Act 11: think[For mn4 color foundation for sensitive skin, the item has options 'lw2' and '1 fluid ounce' and seems good to buy.to Search] Page 1 (Total results: 50) [Next &gt;] [B077PR9TL4] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz $7.59 [B07VLKYJNH] P ÜR 4-in-1 Love Your Selfie Longwear Foundation &amp; Concealer -Full Coverage Foundation &amp; Concealer Makeup For Dark Spots, Blemishes and Imperfections -Long-Lasting Liquid Makeup &amp; Skincare $36.0 [B09J1X9M1P] Neutrogena Healthy Skin Sensitive Skin Serum Foundation with Pro-Vitamin B5, Color Correcting &amp; Pore Minimizing Liquid Foundation &amp; Face Serum, Buildable Coverage, Medium 02, 1 oz $14.39 Action: think[B077PR9TL4, B07VLKYJNH, and B09J1X9M1P are mn4 color foundation for sensitive skin and price lower than 40.00 dollars.[dc1][dn3][dn4][lc1][ln3][ln4][lw2][mc1][mn3][mn4][mw2] size [1 fl oz (pack of 1)][1 ounce (pack of 1)][1 fluid ounce] Physicians Formula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz Price: $7.59 Rating: N.A. [Description] [Features] [Reviews] [Attributes] [Buy Now]</p>
<p>mw2' and seems good to buy.] Observation: OK.Action: click[mn4] Observation: You have clicked mn4.Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0): 1.0 Success</p>
<p>The agent forms an overall plan p, then interacts with the environment over a finite horizon of H timesteps.At each timestep t ∈ {1, 2, . . .H}, the agent forms an action plan p ′ t , selects an action α t from the action space A and receives an observation o t from the observation space O.
The trajectoryτ = { ⃗ p ′ t , ⃗ α t , ⃗ o t } of the agent up to time t consists of the sequences of plans ⃗ p ′ t = (p ′ 1 , ...p ′ t ), actions ⃗ α t = (α 1 , ...α t )and observations ⃗ o
t = (o 1 , ...o t ).</p>
<p>Table 1 .
1
ALFWorld task-specific success rate(%).
Method(dmax=3)ModelPick CleanHeatCool Look Pick2AllActGPT-3.566.751.673.961.938.917.653.7ReActGPT-3.550.041.973.966.755.623.552.2ReflexionGPT-3.575.077.465.276.283.370.674.6ADaPT *GPT-3.587.580.660.876.261.152.971.6RAP(Ours)GPT-3.595.887.178.390.588.970.685.8RAPtrain(Ours)GPT-3.595.8 100.082.685.7 100.076.591.0ReActGPT-483.371.095.781.0 100.094.185.8RAP(Ours)GPT-495.890.3100.0 95.2 100.088.294.8ReActLlama2-13b 29.241.934.852.438.917.636.6RAP(Ours)Llama2-13b 62.561.356.561.944.417.653.0</p>
<p>Table 2 .
2
WebShop Score (%) and Success Rate(%).
Method(dmax=3)ModelScore Success RateReActGPT-3.561.835.0ReflexionGPT-3.561.835.0ADaPTGPT-3.564.043.0RAP(Ours)GPT-3.576.148.0ReActLlama2-13b64.631.0RAP(Ours)Llama2-13b71.136.0</p>
<p>Table 3 .
3
Average success rates on Franka Kitchen and Meta World of Vision-Languge Model Agents w/wo RAP(%)
MethodFranka Kitchen Meta WorldLLaVA43.465.4LLaVA with RAP61.679.2CogVLM44.261.5CogVLM with RAP56.968.8</p>
<p>Table 4 .
4
ALFWorld success rate(%) with different retrievers.RAPact uses only action information, while RAP obs utilizes only observation information for retrieval.RAP clip refers to evaluations using images, rather than texts, as observations with CLIP.
Method(dmax=3) Success RateReAct52.2RAPact82.1RAP obs84.3RAP clip86.6RAP85.85.1.2. WEBSHOP EVALUATIONIn this subsection, we illustrate the effect of varying thecomponents extracted from each experience by the Retriever.</p>
<p>Table 5 .
5
Webshop overall score (%) and success rate(%) with different retrievers on Llama2-13b.RAP obs uses additional retrieval by observations on top of action-based retrieval in RAPact.Also, RAPintra and RAPcat uses intra-task retrieval and product-category retrieval.RAP indicates combination of RAP obs , RAPintra and RAPcat.
Method(dmax=3) Success Rate Overall ScoreReAct31.064.6RAPact33.068.6RAP obs33.069.0RAPintra34.069.3RAPcat35.069.9RAP36.071.1</p>
<p>Here is the task.Please make an action from the examples.Task: put some watch on safe.Plan: To solve the task, I need to find and take a watch, then put it on the safe.
). I can check one by one, starting with dresser 1. think: Now I find the second saltshaker (2). Next, I need to take it. &gt; search: desklamp think: Now I find a lettuce (1). Next, I need to take it. &gt; action: take think: Now I find a pan (1). Next, I need to take it. &gt; action: take &gt; action: take think: Now I heat an egg (2). Next, I need to put it in/on diningtable 1. &gt; action: put think: Now I take a spraybottle (2). Next, I need to put it in/on toilet 1. &gt; action: put think: Now I take an apple (1). Next, I need to go to a microwave (1) and heat it. &gt; action: heat think: Now I take a bread (1). Next, I need to go to a microwave (1) and heat it. &gt; action: heat think: Now I take a mug (3). Next, I need to go to a fridge (1) and cool it. &gt; action: cool think: Now I take a potato (2). Next, I need to go to a fridge (1) and cool it. &gt; action: cool think: Now I find a desklamp (1). Next, I need to use it. &gt; action: use think: Now I find a desklamp (3). Next, I need to use it. &gt; action: use Here is the task. Please make a plan from the examples. think: First I need to find a watch. A watch is more likely to appear in/on cabinet (1-4), drawer (1-5). I can check one by one, starting with cabinet 1 &gt; B.1.3. PROMPT FOR ACTION PLAN IN REASONER AND FOR ACTION IN EXECUTOR Here is the task information. You are in the middle of a room. Looking quickly around you, you see a armchair 1, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a safe 1, a shelf 12, a shelf 11, a shelf 10, a shelf 9, a shelf 8, a shelf 7, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, a shelf 1, a sidetable 1, and a sofa 1. Here are examples. {Task: put a vase in safe. Plan: To solve the task, I need to find and take a vase, then put it in the safe. &gt; think: First I need to find a vase. OK. &gt; go to shelf 1 On the shelf 1, you see nothing. ... &gt; go to shelf 6 On the shelf 6, you see a vase 2. &gt; think: Now I find a vase (2). Next, I need to take it OK. &gt; take vase 2 from shelf 6 You pick up the vase 2 from the shelf 6.} examples x n &gt; C. ALFWorld trajectories Task Description You are in the middle of a room. Looking quickly around you, you see a bed 1, a desk 2, a desk 1, a drawer 6, a drawer 5, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a laundryhamper 1, a safe 1, a shelf 6, a shelf 5, a shelf 4, a shelf 3, a shelf 2, and a shelf 1. Your task is to: put a mug in desk. ReAct Act 1: think: To solve the task, I need to find and take a mug, then put it in desk Obs 1: OK. Act 2: think: First I need to find a mug. A mug is more likely to appear in drawer (1-6), shelf (1-6), desk (1-2), garbagecan (1), laundryhamper (1), safe (1). I can check one by one, starting with drawer 1 Obs 2: OK. Act 3: go to drawer 1 Obs 3: The drawer 1 is closed. Act 4: open drawer 1 Obs 4: You open the drawer 1. The drawer 1 is open. In it, you see a pen 1. ... Act 23: think: Hmm, it seems like the mug is not in any of the drawers. Let me check the other possible locations Obs 23: OK. ... Fail RAP Plan: To solve the task, I need to find and take a mug, then put it in desk. Act 1: think: First I need to find a mug Obs 1: OK. Retrieval key: search mug Act 2: go to shelf 1 Obs 2: On the shelf 1, you see a mug 2. Act 3: take mug 2 from shelf 1 Obs 3: You pick up the mug 2 from the shelf 1. Act 4: think: Now I find a mug (2). Next, I need to put it in/on desk 1 Obs 4: OK. Retrieval key: action put Act 5: go to desk 1 Obs 5: On the desk 1, you see a bowl 1, a cd 2, a creditcard 2, a desklamp 1, a keychain 1, a laptop 2, a pen 2, and a pencil 2. Act 6: put mug 2 in/on desk 1 Obs 6: You put the mug 2 in/on the desk 1. Success Task Description You are in the middle of a room. Looking quickly around you, you see a cabinet 4, a cabinet 3, a cabinet 2, a cabinet 1, a countertop 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a sinkbasin 2, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1. Your task is to: clean some cloth and put it in cabinet. ReAct Act 1: think: To solve the task, I need to find and take a cloth, then clean it with sinkbasin, then put it in a cabinet Obs 1: OK. Act 2: think: First I need to find a cloth. A cloth is more likely to appear in a cabinet (1-4), coun-tertop (1), sinkbasin (1-2), handtowelholder (1-2), towelholder (1), toiletpaperhanger (1), garbage-can (1). I can check one by one, starting with cabinet 1 Obs 2: OK. ... Act 6: open cabinet 2 Obs 6: You open the cabinet 2. The cabinet 2 is open. In it, you see a cloth 1. Act 7: take cloth 1 from cabinet 2 Obs 7: You pick up the cloth 1 from the cabinet 2. Act 8: think: Now I take a cloth (1). Next, I need to go to sinkbasin (1-2) and clean it Obs 8: OK. Act 9: go to sinkbasin 1 Obs 9: On the sinkbasin 1, you see nothing. Act 10: go to sinkbasin 2 Obs 10: On the sinkbasin 2, you see nothing. Act 11: think: I can't clean the cloth without water. I need to find a source of water Obs 11: OK. Act 12: go to toilet 1 ... Fail RAP Plan: To solve the task, I need to find and take a cloth, then clean it with sinkbasin, then put it in cabinet. Act 1: think: First I need to find a cloth Obs 1: OK. Retrieval key: search cloth Act 2: go to cabinet 1 Obs 2: The cabinet 1 is closed. Act 3: open cabinet 1 Obs 3: You open the cabinet 1. The cabinet 1 is open. In it, you see a toiletpaper 1. Act 4: go to cabinet 2 Obs 4: The cabinet 2 is closed. Act 5: open cabinet 2 Obs 5: You open the cabinet 2. The cabinet 2 is open. In it, you see a cloth 1. Act 6: take cloth 1 from cabinet 2 Obs 6: You pick up the cloth 1 from the cabinet 2. Act 7: think: Now I take a cloth (1). Next, I need to go to sinkbasin (1) and clean it Obs 7: OK. Retrieval key: action clean Act 8: go to sinkbasin 1 Obs 8: On the sinkbasin 1, you see nothing. Act 9: clean cloth 1 with sinkbasin 1 Obs 9: You clean the cloth 1 using the sinkbasin 1. Act 10: think: Now I clean a cloth (1). Next, I need to put it in/on cabinet (1-6) Obs 10: OK. Retrieval key: action put Act 11: go to cabinet 3 Obs 11: The cabinet 3 is closed. Act 12: open cabinet 3 Obs 12: You open the cabinet 3. The cabinet 3 is open. In it, you see a soapbottle 1. Act 13: put cloth 1 in/on cabinet 3 Obs 13: You put the cloth 1 in/on the cabinet 3. Task Description WebShop Instruction: i am looking for mn4 color foundation for my sensitive skin, and price lower than 40.00 dollars [Search] ReAct Action: search[mn4 color foundation sensitive skin] Observation: [Back to Search] Page 1 (Total re-sults: 50) [Next &gt;] [B077PR9TL4] Physicians For-mula The Healthy Foundation with SPF 20, LW2, 1 Fl Oz $7.59 [B07VLKYJNH] P ÜR 4-in-1 Love Your Selfie Longwear Foundation &amp; Concealer -Full Coverage Foundation &amp; Concealer Makeup For Dark Spots, Blemishes and Imperfections -Long-Lasting Liquid Makeup &amp; Skincare $36.0 [B09J1X9M1P] Neutrogena Healthy Skin Sen-sitive Skin Serum Foundation with Pro-Vitamin B5, Color Correcting &amp; Pore Minimizing Liquid Foundation &amp; Face Serum, Buildable Coverage, Medium 02, 1 oz $14.39 Action: think[B077PR9TL4 and B09J1X9M1P are mn4 color foundation for sensitive skin and price lower than 40 dollars.] Success D. Webshop trajectories Observation: OK.
A. ParametersIn Webshop, all experiments were performed with a temperature setting of 0.0, due to the additional requirement of precise commands for interactions with the web application.To evaluate RAP in multimodal environments, we focus on two benchmarks: Franka Kitchen and Meta-World.These benchmarks offer a diverse range of tasks, simulating real-world scenarios in a controlled environment, making them ideal for evaluating the performance of multimodal agents.Franka Kitchen Benchmark simulates a kitchen environment, where the agent interacts with various kitchen appliances and items.We have selected five specific tasks that test the agent's ability to manipulate objects and controls within this environment.These tasks are listed in  The Meta-World benchmark is designed to evaluate an agent's skill in more generalized object manipulation tasks.We have selected five tasks that represent a broad range of actions.Task No. Meta-World Tasks 1. assemble Pick up a nut and place it onto a peg 2.buttion Press a button 3. drawer Open a drawer 4. hammer Hammer a screw on the wall 5. binGrasp the puck from one bin and place it into another binE.2. Model SpecificationIn this subsection, we detail the model specifications for our multimodal agent, which is built upon two SOTA visionlanguage models: LLaVA and CogVLM.These models allows for a more comprehensive understanding and interaction with multimodal environments.LLaVA Model: Our agent utilizes the LLaVA-v1.513B model.This version of LLaVA incorporates the Vicuna-v1.513B as its underlying Large Language Model (LLM).The integration of Vicuna-v1.5 13B, known for its robust linguistic processing capabilities.CogVLM Model: For the CogVLM model, we employ the Cog-VLM 17B version.This model boasts a significant number of parameters -10 billion are dedicated to visual understanding, and 7 billion are focused on language processing.In our experimental setup for the language generation of these models, we set the temperature to 0.0.This setting is chosen to prioritize precision and determinism in the generated outputs, which is crucial for the consistency and reliability of the agent's planning responses in our multimodal tasks.E.3. Policy Network detailsPolicy Network is a crucial component of our framework designed to translate high-level action plans generated by the vision-language model into precise, low-level control actions suitable for the specific action space of the environment.Our approach utilizes a Multi-Layer Perceptron (MLP) neural network for this purpose.In both Franka Kitchen and Meta-World environments, the policy network's learning is facilitated through a few-shot learning approach, leveraging a limited but highly informative set of demonstration data.For each environment, we provide 25 expert demonstrations sourced from the D4RL dataset.These demonstrations consist of trajectories that include both observations and actions, showcasing expert-level performance in the respective tasks.In the Franka Kitchen tasks, each demonstration is composed of 50 state-action pairs, reflecting the sequence and specifics of actions required to complete the task.Meanwhile, for the Meta-World tasks, each demonstration sample comprises 500 state-action pairs.
Textworld: A learning environment for text-based games. M.-A Côté, Ákos Kádár, X Yuan, B Kybartas, T Barnes, E Fine, J Moore, R Y Tao, M Hausknecht, L E Asri, M Adada, W Tay, A Trischler, arXivInternational Joint Conference on Artificial Intelligence (IJCAI). 2018. 2023GenAI, M. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. A Gupta, V Kumar, C Lynch, S Levine, K Hausman, Conference on Robot Learning (CoRL). 2019</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W.-T Yih, T Rocktäschel, S Riedel, D Kiela, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020ISBN 9781713829546</p>
<p>Visual instruction tuning. H Liu, C Li, Q Wu, Y J Lee, ArXiv, volume abs/2304.084852023</p>
<p>. J Liu, D Shen, Y Zhang, B Dolan, L Carin, W Chen, 2021What makes good in-context examples for gpt-3? In arXiv</p>
<p>Memoryassisted prompt editing to improve gpt-3 after deployment. A Madaan, N Tandon, P Clark, Y Yang, S Mirchandani, F Xia, P Florence, B Ichter, D Driess, M G Arenas, K Rao, D Sadigh, A Zeng, Empirical Methods in Natural Language Processing. 20222023Conference on Robot Learning (CoRL)</p>
<p>Gpt-4 technical report. arXiv2023OpenAI</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P F Christiano, J Leike, R Lowe, Neural Information Processing Systems (NeurIPS). 2022</p>
<p>Learning transferable visual models from natural language supervision. A Prasad, A Koller, M Hartmann, P Clark, A Sabharwal, M Bansal, T Khot, A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, arXivProceedings of the 38th International Conference on Machine Learning. M Meila, T Zhang, the 38th International Conference on Machine LearningPMLR2023. Jul 2021139Adapt: As-needed decomposition and planning with language models</p>
<p>N Reimers, I Gurevych, - Sentence, Bert, arXivSentence Embeddings using Siamese BERT-Networks. 2019</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, 10.18653/v1/P18-1238Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. I Gurevych, Y Miyao, the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational LinguisticsJuly 20181</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, E Berman, A Gopinath, K Narasimhan, S Yao, Neural Information Processing Systems (NeurIPS). 2023</p>
<p>Aligning text and embodied environments for interactive learning. M Shridhar, X Yuan, M.-A Côté, Y Bisk, A Trischler, M Hausknecht, Alfworld, International Conference on Learning Representations (ICLR). 2021</p>
<p>Selective annotation makes language models better few-shot learners. H Su, J Kasai, C H Wu, W Shi, T Wang, J Xin, R Zhang, M Ostendorf, L Zettlemoyer, N A Smith, T Yu, arXiv2022</p>
<p>L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J.-R Wen, arXivA survey on large language model based autonomous agents. 2023a</p>
<p>. L Wang, C Ma, X Feng, Z Zhang, H Yang, J Zhang, Z Chen, J Tang, X Chen, Y Lin, W X Zhao, Z Wei, J.-R Wen, A Survey on Large Language Model based Autonomous Agents. In arXiv. 2023b</p>
<p>CogVLM: Visual Expert for Pretrained Language Models. W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, J Xu, B Xu, J Li, Y Dong, M Ding, J Tang, ArXiv, volume abs/2311.030792023c</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, 35</p>
<p>Z Xi, W Chen, X Guo, W He, Y Ding, B Hong, M Zhang, J Wang, S Jin, E Zhou, R Zheng, X Fan, X Wang, L Xiong, Y Zhou, W Wang, C Jiang, Y Zou, X Liu, Z Yin, S Dou, R Weng, W Cheng, Q Zhang, W Qin, Y Zheng, X Qiu, X Huang, Gui , T , arXivThe rise and potential of large language model based agents: A survey. 2023</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, K Narasimhan, arXiv2022</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>A survey on multimodal large language models. S Yin, C Fu, S Zhao, K Li, X Sun, T Xu, E Chen, arXiv2023</p>
<p>T Yu, D Quillen, Z He, R Julian, A Narayan, H Shively, A Bellathur, K Hausman, C Finn, S Levine, Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. 2021</p>
<p>Task Description WebShop Instruction: i want capri sun pacific cooler mixed fruit naturally flavored juice drinks, and price lower than 20.00 dollars [Search] ReAct Action: search[capri sun pacific cooler mixed fruit naturally flavored juice drinks] Observation: [Back to Search] Page 1 (Total results: 50) [Next &gt;] [B000R30X2A] Capri Sun Pacific Cooler Mixed Fruit Naturally Flavored Juice Drink Blend (Pack of 10) (Packaging May Vary) $2.55 [B075CSWGJR] Capri Sun Pacific Cooler Ready-to-Drink Juice, 6 Fl Oz (Pack of 30) $100.0 [B0083GG2SC] Capri Sun Coolers Variety Pack Ready-to-Drink Juice (40 Pouches, 4 Boxes of 10) $100.0 Action: think[B000R30X2A is capri sun pacific cooler mixed fruit naturally flavored juice drinks and price lower than 20. A Zhao, D Huang, Q Xu, M Lin, Y.-J Liu, G Huang, AAAI Conference on Artificial Intelligence (AAAI). 202355Action: think[For capri sun pacific cooler mixed fruit naturally flavored juice drinks, the item has options. fl oz (pack of 10)'] Observation: OK. Action: click[assorted] Observation: You have clicked assorted. Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0</p>
<p>B000R30X2A] Capri Sun Pacific Cooler Mixed Fruit Naturally Flavored Juice Drink Blend (Pack of 10) (Packaging May Vary) $2.55 [B075CSWGJR] Capri Sun Pacific Cooler Ready-to-Drink Juice, 6 Fl Oz (Pack of 30) $100.0 [B0083GG2SC] Capri Sun Coolers Variety Pack Ready-to-Drink Juice (40 Pouches, 4 Boxes of 10) $100.0 Action: think[B000R30X2A is capri sun pacific cooler mixed fruit naturally flavored juice drinks and seems good to buy. Action: think[For capri sun pacific cooler mixed fruit naturally flavored juice drinks, the item has options 'assorted', 'pacific cooler', and '6 fl oz (pack of 10)' and seems good to buy. 6 fl oz (pack of 10. Observation: You have clicked 6 fl oz (pack of 10. Action: click[Buy Now] Observation: Your score (min 0.0, max 1.0): 1.0</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6537 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6537</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6537</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-130.html">extraction-schema-130</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <p><strong>Paper ID:</strong> paper-276576200</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.15776v1.pdf" target="_blank">Logic.py: Bridging the Gap between LLMs and Constraint Solvers</a></p>
                <p><strong>Paper Abstract:</strong> We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the-art results. We demonstrate the efficacy of this approach on the logic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused domain-specific language (DSL) called Logic.py. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLogicBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain-specific languages and auxiliary tools on traditionally challenging tasks for LLMs.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6537.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6537.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Logic Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Logic Agent (Logic.py DSL + CBMC constraint solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An agentic solver introduced in this paper that prompts an LLM to formalise logic-grid puzzles into a domain-specific language (Logic.py), compiles that to C via libCST, and solves the resulting constraint problem using the CBMC/CPROVER back-end solver.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1 70B Instruct (as backend LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large decoder-only transformer used to generate Logic.py formalizations from natural-language puzzle descriptions; hosted on Meta-internal MetaGen for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Zebra (Logic Grid) Puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (includes left/right spatial relations)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ZebraLogicBench</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot prompting (model is instructed about Logic.py DSL and nondeterministic features; no full puzzle exemplars shown)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Formalisation into a DSL (Logic.py) + external constraint solving (CBMC/CPROVER -> SAT/SMT search); uses nondeterministic free variables, assume/assert constraints to encode clues</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Model outputs Logic.py program: a typed result data-structure and a validation function that asserts puzzle clues; this Logic.py is transformed via libCST into C/CBMC IR where nondet variables, __CPROVER_assume, and a single reachability assertion form the search harness.</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>CBMC/CPROVER bounded model checker used as constraint solver (maps the generated C program to SAT/SMT formulas and searches for satisfying assignments); libCST used to transform Logic.py (Python-like DSL) into C for CBMC.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Puzzle-level accuracy (solve rate) and cell-level accuracy (fraction of cells correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Puzzle accuracy: 91.4% (overall); Easy: 97.86%; Hard: 88.89%. Cell accuracy: 92.98%.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Prompting the LLM to formalise clues into a compact DSL shifts the task from search to translation: the LLM needs to reason about clue semantics rather than search the solution space. The DSL's nondeterministic primitives (uninitialised vars, nondet(), assume/assert) let the solver do search and proof checking. The pipeline catches syntactic errors during libCST->C conversion and detects unsatisfiable or ambiguous constraints via CBMC, but the current prototype simply restarts rather than providing corrective feedback to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Compared to the base Llama 3.1 70B Instruct without the solver (which scored 24.9% puzzle accuracy), the Logic Agent achieved 91.4% — an absolute improvement of ~66.5 percentage points; similar large gains are observed at cell level (27.98% -> 92.98%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Evaluated only on ZebraLogicBench (logic grid puzzles); Logic.py prototype lacks full Python feature support; pipeline restarts on syntax or unsat errors instead of interactive repair; ambiguity detection (detecting multiple valid solutions) and iterative refinement by the model were not implemented; generalisability to other search/proof domains is untested.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic.py: Bridging the Gap between LLMs and Constraint Solvers', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6537.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6537.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama 3.1 70B Instruct (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3.1 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The base large language model used as the generation engine in the experiments; when used without the Logic Agent and external solver it serves as the baseline for direct puzzle solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned decoder-only transformer model (Meta), used for direct puzzle-solving inference in baseline condition and as the LLM that generates Logic.py when used in the Logic Agent.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Zebra (Logic Grid) Puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic (includes left/right spatial relations)</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ZebraLogicBench</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>direct prompting for solution (baseline) and zero-shot DSL prompting when used inside Logic Agent; baseline results are from direct model inference without constraint-solver assistance</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td>Direct natural-language-to-solution reasoning (baseline); when used inside agent, formalisation into Logic.py then external solver (see Logic Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td>Baseline: model-produced textual reasoning/solution (no structured DSL); In-Agent: Logic.py program as described in Logic Agent entry</td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Puzzle-level accuracy (solve rate) and cell-level accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Puzzle accuracy (baseline, without solver): 24.9% overall. Cell accuracy (baseline): 27.98%.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>As a standalone model, performance on ZebraLogicBench is poor (≈25%), indicating difficulty of direct search-based logical reasoning; when paired with DSL + solver, performance dramatically increases, implying the main benefit comes from formalisation + solver rather than raw LLM search.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td>Baseline direct solving: 24.9% vs with Logic Agent pipeline: 91.4% (absolute +66.5 points), demonstrating the large effect of adding formalisation and an external constraint solver.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Struggles to directly solve complex logic grid puzzles; producing consistent, complete formal constraints in natural language is error-prone without the DSL and solver assist; baseline lacks guarantees and interpretable proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic.py: Bridging the Gap between LLMs and Constraint Solvers', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6537.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6537.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving spatial puzzle games, including details about the model, the puzzle, the reasoning or prompting method, performance metrics, internal representations, use of external tools, and any analysis or limitations reported.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>o1-preview-2024-09-12</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>o1-preview-2024-09-12 (leaderboard model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing leaderboard model (OpenAI 'o1-preview' entry) reported in ZebraLogicBench results; used as comparative baseline in tables but not run by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-preview-2024-09-12</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large language model listed on the ZebraLogicBench leaderboard (details not provided in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Zebra (Logic Grid) Puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_type</strong></td>
                            <td>constraint satisfaction / combinatorial logic</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>ZebraLogicBench</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_technique</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>internal_representation</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>use_of_external_tool</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Puzzle-level accuracy and cell-level accuracy (leaderboard values in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Puzzle accuracy: 71.4% overall (Easy: 98.57%, Hard: 60.83%). Cell accuracy: 75.14%.</td>
                        </tr>
                        <tr>
                            <td><strong>analysis_findings</strong></td>
                            <td>Strong performance on Easy puzzles (near saturation) but substantially weaker on Hard puzzles compared to the Logic Agent; listed as a comparative baseline derived from the benchmark leaderboard.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_comparison</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Model details, prompting and tool use are not described in this paper (reported only via leaderboard).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Logic.py: Bridging the Gap between LLMs and Constraint Solvers', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving zebra puzzles using constraint-guided multiagent systems <em>(Rating: 2)</em></li>
                <li>Zebralogic: Benchmarking the logical reasoning ability of language models <em>(Rating: 2)</em></li>
                <li>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning <em>(Rating: 2)</em></li>
                <li>SatLM: Satisfiability-aided Language Modeling <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6537",
    "paper_id": "paper-276576200",
    "extraction_schema_id": "extraction-schema-130",
    "extracted_data": [
        {
            "name_short": "Logic Agent",
            "name_full": "Logic Agent (Logic.py DSL + CBMC constraint solver)",
            "brief_description": "An agentic solver introduced in this paper that prompts an LLM to formalise logic-grid puzzles into a domain-specific language (Logic.py), compiles that to C via libCST, and solves the resulting constraint problem using the CBMC/CPROVER back-end solver.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Llama 3.1 70B Instruct (as backend LLM)",
            "model_description": "Instruction-tuned large decoder-only transformer used to generate Logic.py formalizations from natural-language puzzle descriptions; hosted on Meta-internal MetaGen for experiments.",
            "model_size": "70B",
            "puzzle_name": "Zebra (Logic Grid) Puzzles",
            "puzzle_type": "constraint satisfaction / combinatorial logic (includes left/right spatial relations)",
            "dataset_name": "ZebraLogicBench",
            "prompting_method": "zero-shot prompting (model is instructed about Logic.py DSL and nondeterministic features; no full puzzle exemplars shown)",
            "reasoning_technique": "Formalisation into a DSL (Logic.py) + external constraint solving (CBMC/CPROVER -&gt; SAT/SMT search); uses nondeterministic free variables, assume/assert constraints to encode clues",
            "internal_representation": "Model outputs Logic.py program: a typed result data-structure and a validation function that asserts puzzle clues; this Logic.py is transformed via libCST into C/CBMC IR where nondet variables, __CPROVER_assume, and a single reachability assertion form the search harness.",
            "use_of_external_tool": true,
            "external_tool_description": "CBMC/CPROVER bounded model checker used as constraint solver (maps the generated C program to SAT/SMT formulas and searches for satisfying assignments); libCST used to transform Logic.py (Python-like DSL) into C for CBMC.",
            "evaluation_metric": "Puzzle-level accuracy (solve rate) and cell-level accuracy (fraction of cells correct)",
            "performance": "Puzzle accuracy: 91.4% (overall); Easy: 97.86%; Hard: 88.89%. Cell accuracy: 92.98%.",
            "analysis_findings": "Prompting the LLM to formalise clues into a compact DSL shifts the task from search to translation: the LLM needs to reason about clue semantics rather than search the solution space. The DSL's nondeterministic primitives (uninitialised vars, nondet(), assume/assert) let the solver do search and proof checking. The pipeline catches syntactic errors during libCST-&gt;C conversion and detects unsatisfiable or ambiguous constraints via CBMC, but the current prototype simply restarts rather than providing corrective feedback to the model.",
            "ablation_comparison": "Compared to the base Llama 3.1 70B Instruct without the solver (which scored 24.9% puzzle accuracy), the Logic Agent achieved 91.4% — an absolute improvement of ~66.5 percentage points; similar large gains are observed at cell level (27.98% -&gt; 92.98%).",
            "limitations": "Evaluated only on ZebraLogicBench (logic grid puzzles); Logic.py prototype lacks full Python feature support; pipeline restarts on syntax or unsat errors instead of interactive repair; ambiguity detection (detecting multiple valid solutions) and iterative refinement by the model were not implemented; generalisability to other search/proof domains is untested.",
            "uuid": "e6537.0",
            "source_info": {
                "paper_title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Llama 3.1 70B Instruct (baseline)",
            "name_full": "Meta Llama 3.1 70B Instruct",
            "brief_description": "The base large language model used as the generation engine in the experiments; when used without the Logic Agent and external solver it serves as the baseline for direct puzzle solving.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama 3.1 70B Instruct",
            "model_description": "Instruction-tuned decoder-only transformer model (Meta), used for direct puzzle-solving inference in baseline condition and as the LLM that generates Logic.py when used in the Logic Agent.",
            "model_size": "70B",
            "puzzle_name": "Zebra (Logic Grid) Puzzles",
            "puzzle_type": "constraint satisfaction / combinatorial logic (includes left/right spatial relations)",
            "dataset_name": "ZebraLogicBench",
            "prompting_method": "direct prompting for solution (baseline) and zero-shot DSL prompting when used inside Logic Agent; baseline results are from direct model inference without constraint-solver assistance",
            "reasoning_technique": "Direct natural-language-to-solution reasoning (baseline); when used inside agent, formalisation into Logic.py then external solver (see Logic Agent)",
            "internal_representation": "Baseline: model-produced textual reasoning/solution (no structured DSL); In-Agent: Logic.py program as described in Logic Agent entry",
            "use_of_external_tool": false,
            "external_tool_description": null,
            "evaluation_metric": "Puzzle-level accuracy (solve rate) and cell-level accuracy",
            "performance": "Puzzle accuracy (baseline, without solver): 24.9% overall. Cell accuracy (baseline): 27.98%.",
            "analysis_findings": "As a standalone model, performance on ZebraLogicBench is poor (≈25%), indicating difficulty of direct search-based logical reasoning; when paired with DSL + solver, performance dramatically increases, implying the main benefit comes from formalisation + solver rather than raw LLM search.",
            "ablation_comparison": "Baseline direct solving: 24.9% vs with Logic Agent pipeline: 91.4% (absolute +66.5 points), demonstrating the large effect of adding formalisation and an external constraint solver.",
            "limitations": "Struggles to directly solve complex logic grid puzzles; producing consistent, complete formal constraints in natural language is error-prone without the DSL and solver assist; baseline lacks guarantees and interpretable proofs.",
            "uuid": "e6537.1",
            "source_info": {
                "paper_title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "o1-preview-2024-09-12",
            "name_full": "o1-preview-2024-09-12 (leaderboard model)",
            "brief_description": "A high-performing leaderboard model (OpenAI 'o1-preview' entry) reported in ZebraLogicBench results; used as comparative baseline in tables but not run by the authors.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "o1-preview-2024-09-12",
            "model_description": "Proprietary large language model listed on the ZebraLogicBench leaderboard (details not provided in paper).",
            "model_size": "",
            "puzzle_name": "Zebra (Logic Grid) Puzzles",
            "puzzle_type": "constraint satisfaction / combinatorial logic",
            "dataset_name": "ZebraLogicBench",
            "prompting_method": "",
            "reasoning_technique": "",
            "internal_representation": "",
            "use_of_external_tool": false,
            "external_tool_description": "",
            "evaluation_metric": "Puzzle-level accuracy and cell-level accuracy (leaderboard values in paper)",
            "performance": "Puzzle accuracy: 71.4% overall (Easy: 98.57%, Hard: 60.83%). Cell accuracy: 75.14%.",
            "analysis_findings": "Strong performance on Easy puzzles (near saturation) but substantially weaker on Hard puzzles compared to the Logic Agent; listed as a comparative baseline derived from the benchmark leaderboard.",
            "ablation_comparison": "",
            "limitations": "Model details, prompting and tool use are not described in this paper (reported only via leaderboard).",
            "uuid": "e6537.2",
            "source_info": {
                "paper_title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving zebra puzzles using constraint-guided multiagent systems",
            "rating": 2,
            "sanitized_title": "solving_zebra_puzzles_using_constraintguided_multiagent_systems"
        },
        {
            "paper_title": "Zebralogic: Benchmarking the logical reasoning ability of language models",
            "rating": 2,
            "sanitized_title": "zebralogic_benchmarking_the_logical_reasoning_ability_of_language_models"
        },
        {
            "paper_title": "Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning",
            "rating": 2,
            "sanitized_title": "logiclm_empowering_large_language_models_with_symbolic_solvers_for_faithful_logical_reasoning"
        },
        {
            "paper_title": "SatLM: Satisfiability-aided Language Modeling",
            "rating": 2,
            "sanitized_title": "satlm_satisfiabilityaided_language_modeling"
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        }
    ],
    "cost": 0.00950875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Logic.py: Bridging the Gap between LLMs and Constraint Solvers
17 Feb 2025</p>
<p>Pascal Kesseli pkesseli@meta.com 
Peter O'hearn peteroh@meta.com 
Ricardo Silveira Cabral 
Logic.py: Bridging the Gap between LLMs and Constraint Solvers
17 Feb 2025E82B296D706B69E5D4FCD2E091900E07arXiv:2502.15776v1[cs.AI]
We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-theart results.We demonstrate the efficacy of this approach on the logic puzzles benchmark ZebraLogicBench.Instead of letting the LLM attempt to directly solve the puzzles, our method prompts the model to formalise the problem in a logic-focused domain-specific language (DSL) called Logic.py.This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver.Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLog-icBench, setting a new state-of-the-art with an accuracy of over 90%.This significant advancement demonstrates the potential of combining language models with domainspecific languages and auxiliary tools on traditionally challenging tasks for LLMs.</p>
<p>Introduction</p>
<p>Large language models have revolutionised the field of natural language processing, achieving state-of-the-art results in various tasks such as language translation, text summarisation, and question answering.However, despite their impressive performance, LLMs have historically struggled with certain tasks that require a deeper understanding of mathematical and logical concepts.For instance, Kambhampati et al. [2024] demonstrated that LLMs are unable to plan and reason about complex problems, highlighting the need for further research in this area.In this paper, we focus on improving the performance of LLMs in solving Logic Grid Puzzles, also called Zebra Logic Puzzles or Einstein's Riddles, which we explain in more detail in Sec.1.2.We present the following research contributions:</p>
<ol>
<li>Logic.py:We introduce a domain-specific language called Logic.py which facilitates expressing logic and search-based problems by LLMs.</li>
</ol>
<p>Logic Agent:</p>
<p>We implement an agentic solver engine which accepts search-based, informal problem statements, formalises them in Logic.py and solves them using a constraint sovler.</p>
<p>ZebraLogicBench Evaluation:</p>
<p>We evaluate the efficacy of this approach on the logic puzzle benchmark ZebraLogicBench Lin et al. [2024].</p>
<p>Related Work</p>
<p>Prior research has explored techniques to enhance the ability of LLMs in these central reasoning tasks, such as chain-of-thought prompting and introducing symbolic representations.However, according to Berman et al. [2024], these frameworks often struggle with complex logical problems like Zebra puzzles, partly due to the inherent difficulty of translating natural language clues into logical statements.They propose integrating LLMs with theorem provers to tackle such challenges, demonstrating significant improvements in puzzle-solving capabilities.</p>
<p>Al-Negheimish et al. [2023] highlight that the challenge of numerical reasoning in machine reading comprehension has been addressed by various prompting strategies for LLMs.However, these approaches often struggle to provide robust and interpretable reasoning.They contrast these techniques against their neuro-symbolic approach, which has shown promising results by decomposing complex questions into simpler ones and using symbolic learning methods to learn rules for recomposing partial answers.This approach boasts data efficiency and facilitates robust numerical reasoning with interpretable and verifiable reasoning traces.</p>
<p>The LLM-Augmenter system, proposed by Peng et al. [2023], addresses the limitations of LLMs in real-world applications by augmenting them with plug-and-play modules that ground responses in external knowledge and iteratively revise prompts to improve factuality.Similarly, the Logic-Enhanced Language Model Agents (LELMA) framework, proposed by Mensfelt et al. [2024], integrates LLMs with symbolic AI to enhance the trustworthiness of social simulations, addressing issues such as hallucinations and logical inconsistencies through logical verification and self-refinement.Imani et al. [2023] propose a technique to improve the performance of LLMs on arithmetic problems by generating multiple algebraic expressions or Python functions to solve the same math problem in different ways, thereby increasing confidence in the output results.The Toolformer self-supervised model, presented in Schick et al. [2023], enables language models to leverage external tools via simple APIs, thereby enhancing their performance on various downstream tasks.Fedoseev et al. [2024] propose a method to enhance LLMs mathematical problem-solving capabilities by fine-tuning them on a dataset of synthetic problems and solutions generated using Satisfiability Modulo Theories (SMT) solvers, specifically the Z3 API.</p>
<p>Logic-LM is a framework that combines LLMs with symbolic solvers to enhance logical reasoning capabilities, demonstrating substantial performance improvements over traditional LLM-based approaches Pan et al. [2023].Ye et al. [2023] introduce Satisfiability-aided Language Modeling (SatLM), a method that combines large language models with automated theorem provers to enhance reasoning capabilities, demonstrating state-ofthe-art performance on multiple datasets.</p>
<p>From the work cited above, Pan et al. [2023], Ye et al. [2023], and Berman et al. [2024] present techniques addressing problem domains most comparable to the Logic Grid Puzzles which we explore in this paper.Similar to the approach we present, formalisation and the use of auxiliary tools are a shared feature of this category of research.An important distinguishing aspect of our work is the use of a logic-focused domain-specific language, which we present in more detail in Sec. 2.</p>
<p>The general case for a DSL is not unlike that for intermediate languages in compilation.Given a new programming language it is "obvious" from the Church-Turing thesis that a mapping to assembly language exists, but is not necessarily obvious if a mapping with good efficiency properties (say) exists, and the compilation research community has found it helpful to use intermediate languages to structure the design of compilers.Here, it might seem likely that constraint solvers (automatic theorem provers) could be helpful to approach logic problems stated in natural language, but just how helpful or the best way to do so is not a priori obvious; like in compilation, our thesis is that DSL's can help.A similar analogy is the relation between SQL and first-order logic; although SQL provides facilities than make for briefer or more direct human expression than their expansions into FOL.A similar example for Logic.py is presented in Section 2.1.</p>
<p>Our results support the case that this DSL can be helpful in structuring the mapping from natural language to that of a solver.In particular, Berman et al. [2024] evaluate their approach against a benchmark set of 114 Zebra puzzles.Their multi-agent system has a more sophisticated translation process which includes a refinement loop, but they raise the puzzle accuracy of GPT-4 from 23.7% to 55.3%, compared to our improvement from 24.9% to 91.4%.</p>
<p>Logic Grid Puzzles</p>
<p>We evaluate the effectiveness of our approach on the Ze-braLogicBench benchmark presented in Lin et al. [2024].ZebraLogicBench is a dataset of 1000 Logic Grid Puzzles, also referred to as Zebra Puzzles.These puzzles consist of a series of clues about features of entities in a described environment.In order to solve the puzzle, one has to guess the correct features of all entities, while respecting all the information provided in the cluses.Fig. 1 gives an example of sucha a description of entities and their features in a Zebra puzzle.</p>
<p>There are 4 houses, numbered 1 to 4 from left to right, as seen from across the street.Each house is occupied by a different person.Each house has a unique attribute for each of the following characteristics:</p>
<p>• Each person has a unique name: Alice, Eric, Arnold, Peter</p>
<p>• Each person has an occupation: artist, engineer, teacher, doctor</p>
<p>• People have unique favorite book genres: fantasy, science fiction, mystery, romance</p>
<p>• People use unique phone models: google pixel 6, iphone 13, oneplus 9, samsung galaxy s21 ZebraLogicBench categorises puzzles into separate difficulties according to the number of entities and distinct features the LLM needs to reason about in order to solve it.</p>
<p>Puzzles conforming to the following shapes are considered easy puzzles:
• 2 × 3 • 2 × 4 • 2 × 5 • 2 × 6 • 3 × 2 • 3 × 3
Puzzles of the following shapes are categorised as hard puzzles:
• 3 × 4 • 3 × 5 • 3 × 6 • 4 × 2 • 4 × 3 • 4 × 4 • 4 × 5 • 4 × 6 • 5 × 2 • 5 × 3 • 5 × 4 • 5 × 5 • 5 × 6 • 6 × 2 • 6 × 3 • 6 × 4 • 6 × 5 • 6 × 6
Consequently, the 4 × 4 example puzzle illustrated in Fig. 1    Since CBMC implements a mapping between programs and SAT or SMT formulas, it can serve as a convenient front-end for such constraint sovlers, exposing an API that allows expressing SAT or SMT formulas as C programs with free input variables.Expressing formulas in this fashion makes many constraint solver tasks more accessible for human developers, and it is a core hypothesis of this paper that it equally simplifies the use of constraint solvers for LLMs.We describe this DSL that we expose to the LLM in more detail in Sec. 2.</p>
<p>The Logic.py language</p>
<p>Our goal in designing Logic.py was to provide a streamlined API language that allows an LLM to efficiently express search-based problems and leverage a constraint solver.We optimised for the folowing criteria:</p>
<ol>
<li>Robustness: The language should minimise the surface area for syntax errors.</li>
</ol>
<p>Conciseness:</p>
<p>The model should be able to express the necessary constraints to solve a search-based problem without boilerplate or needing to worry about unrelated implementation details of the programming language.</p>
<ol>
<li>Expressiveness: While common constraints, such as uniqueness of a property, should be easy to express in our DSL, the language must not be restricted to just these common cases.Instead, it must allow the LLM to express arbitrary constraints in the underlying constraint solver framework if necessary.</li>
</ol>
<p>To provide a good basis for our DSL in terms of robustness and conciseness, we decided to not start with CBMC's native C as a base language for Logic.pybut instead, as its name suggests, we selected Python for this purpose.We use libCST1 to transform Logic.py to C for analysis by CBMC, as explained in more detail in Sec. 4. Python allows the model to introduce new variables without the need for explicit, typed declarations.Similarly, these variables can be reused for other values later on without needing to worry about type compatiblity.</p>
<p>Type Decorators</p>
<p>In order to further improve the conciseness of Logic.py,we borrow well-known language features from existing languages and combine them in our DSL.As an example, expressing uniqueness of a property in SMT directy requries code similar to the example in Fig. 3.
( f o r a l l ( ( x T ) ( y T ) ) (=&gt; ( d i s t i n c t x y ) ( n o t ( = ( i d x ) ( i d y ) ) ) )</p>
<p>list[T, S]</p>
<p>Creates a fixed-size list of type 'T' and size 'S'.This is equivalent to fixed size arrays in ANSI C. Motivated by this example, we introduced a set of custom type decorators to Logic.py which allow to express common property constraints in a much shorter, less errorprone way.We provide full list of Logic.pytype decorators in Tab. 2.</p>
<p>Free Variables, Assumptions, and Assertions</p>
<p>A key concept in using constraint solvers are free variables.By default, any variable which is not explicitly initialised in Logic.py is assumed to be a free variable.Consequently, the constraint solver is allowed to assign it any value in order to find a satisfying assignment for all the specified constraints.We also say that such a variable has a nondeterministc value.</p>
<p>The CPROVER framework also allows users to specify assumptions and assertions.These two features work in tandem: Assumptions act as preconditions, usually expressed over free variables, to constrain the solver's search to valid or interesting inputs.Assertions on the</p>
<p>Feature Description</p>
<p>Uninitialised variables Uninitialised variables receive a nondeterministic value.When solving search-based problems, the solution is automaically marked nondeterministic by our engine and the model can constraint the solution according to its requirements.</p>
<p>assume(pred)</p>
<p>Constrains search paths for solutions to only the values where pred is true.</p>
<p>assert pred</p>
<p>In static analysis verification scenarios, assertions must be true for all possible inputs satisfying assumptions.</p>
<p>In our search-based problem harness, they are equivalent to assumptions.</p>
<p>nondet(list)</p>
<p>Returns a nondeterministic but valid element from list.This can be used in combination with assumptions to find elements in a list that satisfy a predicate, then express additional constraints about them.It should be noted that in Logic.py,from the perspective of the LLM, these two become interchangeable.We interpret assertions specified by the model not as safety properties to be checked for violations, but instead as requirements on a valid solution.Interpreting assertions as assumptions and passing only a single reachability assertion to the constraint solver is a common pattern in program synthesis use cases David et al. [2018].We will explain the structure of constraints we produce at CPROVER intermediate representation level in more detail in Sec. 4. We summarise the nondeterminism-related Logic.pyfea-tures in Tab. 3.</p>
<p>Search Problem Formalisation</p>
<p>After reviewing the features of Logic.py in Sec. 2, we now review how we prompt the model to express search problems in this language.We provide the full prompts in our open source agent project polymath2 .Note that all our prompts are zero-shot in that we do not provide any full Zebra puzzle as example to the model.Instead, we only explain Logic.py and how to use its nondeterministic features to express generic search-based problems.</p>
<p>Describing a Result Data Structure</p>
<p>Before attempting to convert the required properties and constraints for the solution of a search-based problem, we first prompt the model to define the data structure that can contain such a solution.We ask the model to define this data structure in Logic.py and be as precise as possible with respect to type annotations.This constrains the space of valid solutions purely based on the domain of the problem, irrespective of explicit constraints for now.Fig. 5 provides an example of the kind of data structure the LLM will generate.</p>
<p>Constraining a Correct Solution</p>
<p>Once the data structure is defined, we prompt the model to generate a validation function that accepts an argument of this type and asserts that it is the correct solution we are searching for.In the case of Zebra puzzles, this leads to the model adding assertions corresponding to the clues in the puzzle.This approach transforms the challenge for the LLM fundamentally: Instead of searching the solution space for configurations that satisfy the clues stated in the puzzle, it just needs to be able to reason about the clues themselves.Fig. 6 shows a few example clues and how they can be formalised in a Logic.pydata strucutre chosen by the model.Our engine converts these Logic.pyconstraints into an equivalent, lower-level C representation using a libCST transformer.During this process, the type decorators introduced in Sec.2.1 are mapped to matching initialisation helpers in CBMC's IR.An example of this mapping is illustrated in Fig. 8.</p>
<p>The validation function containing the constraints derived from the Zebra Logic clues is equally converted to a C representation, and embedded into a search harness.In this harness, a nondeterministic instance of the result data structure proposed by the model is initialised using the type decorator information, then constrained using the converted validation function.All assertions in the validation function are converted into assumptions, and we add a single reachabilty assertion to prompt the constraint solver to find an assignment for the nondeterministc puzzle solution such that it satisifes all constraints.An example harness is shown in Fig. 9.</p>
<p>Error Recovery</p>
<p>Due to the heuristic nature of the formalisation and constraint solver steps, our solver engine can fail at various steps along the pipeline.Logic.pycode to begin with, leading to syntax errors in libCST.These errors are caught during the C harness generation, in which case we revert to the original data structure generation step and start the process from scratch.We currently do not provide any information about the syntax error to the model or ask it to fix its prior mistakes, and such approaches could be explored in future work.</p>
<p>Furthermore, if the model misinterprets constraints in the informal description of the search problem, such errors can lead to contradictory or otherwise unsatisfiable constraints.This is detected during the constraint solver invocation by CBMC, and if detected we again just revert to the initial step of the process.In future work we might again explore whether the model can recover from this, if provided this information, or whether it is preferable to just restart the process, as is currently the case.</p>
<p>Similar to unsatisfiable constraints, the model might also commit formalisation mistakes that loosen the requirements and thus allow for more solutions than should actually be the case.The constraint solver can also detect whether two distinct solutions are possible under the given constraints, and report this information back to the model.The model could then decide whether the problem at hand might indeed allow for multiple different solutions, or whether a correct solution should be unique and thus attempt to correct its mistake.However, we did not implement this ambiguity detection mechanism in our prototype.Even though every task in ZebraLogicBench, which we use for our evaluation, allows for exactly one correct solution, this information is not shared with the model and is only intended to simplify benchmark result evaluation.</p>
<p>Experimental Evaluation</p>
<p>To evaluate the effectiveness of our approach, we conducted experiments on ZebraLogicBench, a benchmark suite consisting of 1000 logic grid tasks.In order to run the evaluation independently, researchers must request access to a private dataset hosted huggingface.co.We implemented a benchmark runner that accepts this dataset as input which we share in our open source project polymath.The output result of the benchmark runner is evaluated using the ZeroEval3 evaluation suite provided by the benchmark authors.We use Llama 3.1 70B Instruct for the inference portion in our implementation, hosted on the Meta-internal MetaGen infrastructure.We run the logic agent implementation on a developer server with 56 cores and 114GB RAM.In this environment, running 100 tasks concurrently, full benchmark run takes approximatley 15 minutes.We evaluted only our own Logic Agent implementation using this setup.All other results listed were taken directly from the ZebraLogicBench leaderboard Lin et al. [2024].</p>
<p>Tab. 4 lists the performance of our logic agent implementation compared to other models on the ZebraLog-icBench leaderboard.Our implementation wins the leaderboard with 91.4% accuracy by a 20% margin over OpenAI o1-preview.Notably, o1-preview already scored exceedingly well on the Easy puzzle category to the point of saturation, and our implementation matches this high performance.However, an over 28% gap seperates our implementation from o1-preview in the Hard puzzle category.This remarkable performance of our engine is particularly striking when compared to how our base model, LLama 3.1 70B Instruct, performs without the help of a constraint solver.On average, it reached 24.9% accuracy across all puzzles, compared to 91.4% accuracy with the constraint solver.A similar result emerges when we evaluate the accuracy of our engine at cell level.Whereas Tab. 4 measures how many puzzles were solved completely in their entirety, Tab. 5 measures how many cells were solved correctly.This measure allows for partial points, where a model got most of the solution correctly and only committed smaller mistakes in some sells.Our engine sets a new state-of-the-art in this category as well, exceeding its base model by a similar margin as in Tab.4.</p>
<p>Threats to Validty</p>
<p>While we believe we made a convincing case in Sec. 5 that auxiliary tools can significantly boost the performance of state-of-the-art LLMs on challenging tasks, we would like to address potential threats to the validity of our results.</p>
<p>Firstly, our engine has thus far only been evaluated against a particular category of search-based problems in the form of logic grid puzzles.Our technique may prove less impactful when applied to other search-based problems formalised by an LLM.This may be due to the fact that our Logic.pyprototype is currently incomplete and does not yet support some common Python language and standard library features, or more fundamentally, that the formalisation of other search-based problems may not prove as tractable for state-of-the-art LLMs as logic grid puzzles.</p>
<p>Furthermore, while our approach of using a tailored DSL to allow the LLM to operate an auxiliary tool yielded excellent results, further experiments will yet have to show whether this approach can be generalised to other categories of auxiliary tools and problem domains, such as numerical computational programming languages or algebraic modeling languages to improve math capabilities of LLMs.We intend to pursue these problem domains in future work.</p>
<p>Conclusions and Future Work</p>
<p>In this paper, we presented a novel approach to solving search-based problems using large language models.By introducing the domain-specific language Logic.py and implementing an agentic solver engine, we demonstrated significant improvements in performance on the logic puzzle benchmark ZebraLogicBench.Our results show that combining the strengths of LLMs with those of constraint solvers can lead to remarkable advancements in solving traditionally challenging tasks.</p>
<p>The success of our approach highlights the potential for further research in this area.Some promising directions for future work include:</p>
<ol>
<li>
<p>Extending Logic.py:Developing a more comprehensive and expressive version of Logic.py could enable the formalisation of a wider range of searchbased problems.</p>
</li>
<li>
<p>Improving the Logic Agent: Enhancing the agentic solver engine to better handle complex problem statements and iterate on its mistakes (e.g.syntax errors) could lead to further performance gains.</p>
</li>
<li>
<p>Applying the approach to other domains: Exploring the application of our method to other, such as optimisation or mathematical rasoning, could reveal new opportunities for improvement.In particular, we are currently working on a DSL suitable for tackling first order logic problems and intend to evalute this engine against the FOLIO benchmark.Han et al. [2022] Building on this project, our broader research aims to further explore the potential of combining large language models with specialized reasoning tools.By pursuing these avenues of research, we believe that it is possible to develop even more powerful and efficient problemsolving systems.</p>
</li>
</ol>
<p>Figure 1 :
1
Figure 1: Description of the Zebra logic puzzle</p>
<p>and Fig. 2 would be classified as a hard puzzle, C l u e s : 1 .The p e r s o n who i s an e n g i n e e r i s d i r e c t l y l e f t o f t h e p e r s o n who u s e s a Samsung Galax y S21 . 2 .The p e r s o n who l o v e s f a n t a s y b o o k s i s i n t h e s e c o n d h o u s e .3 .A l i c e i s n o t i n t h e s e c o n d h o u s e .4 .E r i c i s t h e p e r s o n who i s a t e a c h e r .5 .The p e r s o n who u s e s a Samsung Galax y S21 i s t h e p e r s o n who l o v e s f a n t a s y b o o k s .6 .The p e r s o n who u s e s an i P h o n e 13 i s t h e p e r s o n who l o v e s s c i e n c e f i c t i o n b o o k s .7 .The p e r s o n who l o v e s s c i e n c e f i c t i o n b o o k s i s somewhere t o t h e l e f t o f t h e p e r s o n who u s e s a On ePlu s 9 .8 .The p e r s o n who u s e s a On ePlu s 9 i s A r n o l d .9 .The p e r s o n who i s a d o c t o r i s t h e p e r s o n who l o v e s m y s t e r y b o o k s . 1 0 .The p e r s o n who u s e s an i P h o n e 13 i s t h e p e r s o n who i s a t e a c h e r .</p>
<p>Figure 2 :
2
Figure 2: Clues for the Zebra logic puzzle</p>
<p>Figure 3 :
3
Figure 3: Uniqueness constraint in SMT</p>
<p>Figure 4 :
4
Figure 4: Uniqueness constraint in SQL</p>
<p>Figure 5 :
5
Figure 5: Model Output Example: Result Data Structure</p>
<p>Bob is the person who uses # a Xiaomi Mi 11. bob = nondet(solution.houses)assume(bob.name== "Bob") assert bob.phone == "xiaomi mi 11" # Clue 2: The person who loves the # soup is in the fourth house.soup_lover = nondet(solution.houses)assume(soup_lover.lunch== "soup") assert soup_lover.house_number== 4 # Clue 3: The Dragonfruit smoothie # lover is somewhere to the left of # the person in a ranch-style home.d = nondet(solution.houses)assume(d.smoothie== "dragonfruit") r = nondet(solution.houses)assume(r.house_style== "ranch") assert d.house_number &lt; r. house_number # ...</p>
<p>Figure 6 :
6
Figure 6: Model Output Example: Solution Constraints</p>
<p>Figure 9 :
9
Figure 9: CBMC Search Constraint Example</p>
<p>4.</p>
<p>Investigating the role of LLMs in problem formalisation: Further research into the capabilities and limitations of LLMs in formalising search-based problems could provide valuable insights into the design of more effective problem-solving systems. 5. Teaching neural nets to search like solvers: Our translations leverage powerful aspects of constraint solvers: proof checking and search.It would be possible to decompose these strengths.Solvers like Z3 have evolved subtle heuristics for approaching computationally intractable problems that are NPhard and more, and if we can train nets in a way that learns these heuristics they might provide benefit more broadly than as the targets of translationbased work.In this context, work such as this could provide baselines or targets for what we would want out of the training.</p>
<p>Table 1 :
1
Zebra Puzzle Example Solution but at the lower end of the difficulty specturm.A valid solution of this particular puzzle is illustrated in Tab. 1.
House Name Occupation BookGenre PhoneModel1AliceEngineerRomancePixel 62PeterArtistFantasyGalaxy S213EricTeacherSciFiiPhone 134ArnoldDoctorMysteryOnePlus 9
1.3 CBMC -Bounded Model Checker for C and C++ programs CBMC Clarke et al. [2004] is a static analysis tool designed for C and C++ programs.It operates by mapping programs to formulas in a back-end solver, typically SAT or SMT, which are satisfiable if and only if the mapped program exhibits a specific property.This capability enables CBMC to effectively check programs for bugs or other properties of interest.Notably, CBMC is powered by the underlying CPROVER static analysis engine, which also supports other language front-ends, such as the JBMC Cordeiro et al. [2018] Java front-end.</p>
<p>Table 2 :
2
Logic.py type decorators</p>
<p>Table 3 :
3
Logic.py nondet features other hand represent safety properties, for which the solver tries to find a falsifying assignment in order to prove the presence of a bug.Both of these features are exposed in Logic.pyvia the assume(...) function and the assert... statement, respectively.</p>
<p>The model might produce invalid
struct House {int house_number;const char * name;const char * smoothie;// ...};static int House_house_number[] ={1, 2, 3, 4, 5, 6};static bool House_house_number_usedstatic void validate( struct PuzzleSolution solution) {[6]; static const char * House_name[] = {"Alice", "Eric", "Peter", ...};bob = __CPROVER_nondet_element( solution.houses);static bool House_name_used[6]; // ...}__CPROVER_assume(bob.name == "Bob"); __CPROVER_assume(bob.phone == "xiaomi mi 11"); // ...#define __CPROVER_unique_domain( \ field, field_domain_array) \ { \ size_t index; \ __CPROVER_assume(index &lt; \// ...(sizeof(field_domain_array) / \ sizeof(field_domain_array[0])));int main(void) { struct PuzzleSolution solution; init_PuzzleSolution(&amp;solution); validate(solution);\ __CPROVER_assume( \ !field_domain_array##_used[index]) ; \ field_domain_array##_used[index] = }__CPROVER_output("solution", solution); __CPROVER_assert(false, "");true; \ field = field_domain_array[index]; \ // ... }static void init_House(struct House * instance) {__CPROVER_unique_domain(instance-&gt;house_number,House_house_number);__CPROVER_unique_domain(instance-&gt;name,House_name);// ...}Figure 8: CBMC Data Structure Example</p>
<p>Table 4 :
4
ZebraLogicBench Puzzle Accuracy Results
ModelPuzzle Accuracy All Easy HardLogic Agent91.4 97.86 88.89o1-preview-2024-09-1271.4 98.57 60.83o1-mini-2024-09-12-v359.7 86.07 49.44claude-3-5-sonnet-2024102236.2 91.07 14.86Llama-3.1-405B-Inst-fp8@to. . . 32.6 87.14 11.39gpt-4o-2024-08-0631.7 84.64 11.11gemini-1.5-pro-exp-082730.5 79.64 11.39Llama-3.1-405B-Inst@samba. . . 30.1 84.648.89Mistral-Large-229 80.369.03claude-3-opus-2024022927 78.217.08Qwen2.5-72B-Instruct26.6 76.437.22gemini-1.5-pro-exp-080125.272.56.81Llama-3.1-405B-Inst@hyper. . .25 66.67 15.38gemini-1.5-flash-exp-082725 70.717.22Meta-Llama-3.1-70B-Instruct24.9 73.575.97</p>
<p>Table 5 :
5
ZebraLogicBench Cell Accuracy Results
ModelCell Ac-curacyLogic Agent92.98o1-preview-2024-09-1275.14o1-mini-2024-09-12-v370.32claude-3-5-sonnet-2024102254.27Llama-3.1-405B-Inst-fp8@together45.8gpt-4o-2024-08-0650.34gemini-1.5-pro-exp-082750.84Llama-3.1-405B-Inst@sambanova39.06Mistral-Large-247.64claude-3-opus-2024022948.91Qwen2.5-72B-Instruct40.92gemini-1.5-pro-exp-080148.5Llama-3.1-405B-Inst@hyperbolic46.62gemini-1.5-flash-exp-082743.56Meta-Llama-3.1-70B-Instruct27.98
https://github.com/Instagram/LibCST
https://github.com/facebookresearch/polymath
https://github.com/WildEval/ZeroEval</p>
<p>Augmenting large language models with symbolic rule learning for robust numerical reasoning. Hadeel Al-Negheimish, Pranava Madhyastha, Alessandra Russo, The 3rd Workshop on Mathematical Reasoning and AI at NeurIPS'23. 2023</p>
<p>Solving zebra puzzles using constraint-guided multiagent systems. Shmuel Berman, Kathleen Mckeown, Baishakhi Ray, 2024</p>
<p>A tool for checking ANSI-C programs. Edmund Clarke, Daniel Kroening, Flavio Lerda, Tools and Algorithms for the Construction and Analysis of Systems (TACAS 2004). Lecture Notes in Computer Science. Kurt Jensen, Andreas Podelski, Springer20042988</p>
<p>JBMC: A bounded model checking tool for verifying Java bytecode. Lucas Cordeiro, Pascal Kesseli, Daniel Kroening, Computer Aided Verification (CAV), volume 10981 of LNCS. Springer2018Peter Schrammel, and Marek Trtik</p>
<p>Program synthesis for program analysis. Cristina David, Pascal Kesseli, Daniel Kroening, Matt Lewis, ACM Transactions on Programming Languages and Systems. 4005 2018</p>
<p>LLM training data synthesis for more effective problem solving using satisfiability modulo theories. Timofey Fedoseev, Dimitar Iliev Dimitrov, Timon Gehr, Martin Vechev, The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24. 2024</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, David Peng, Jonathan Fan, Yixin Liu, Brian Wong, Malcolm Sailor, Ansong Ni, Linyong Nan, Jungo Kasai, Tao Yu, Rui Zhang, Shafiq Joty, Alexander R Fabbri, Wojciech Kryscinski, arXiv:2209.00840Caiming Xiong, and Dragomir Radev. Folio: Natural language reasoning with first-order logic. Xi Victoria Lin2022arXiv preprint</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, 2023</p>
<p>Llms can't plan, but can help planning in llm-modulo frameworks. Subbarao Kambhampati, Karthik Valmeekam, Lin Guan, Mudit Verma, Kaya Stechly, Siddhant Bhambri, Lucas Saldyt, Anil Murthy, 2024</p>
<p>Zebralogic: Benchmarking the logical reasoning ability of language models. Ronan Bill Yuchen Lin, Peter Le Bras, Yejin Clark, Choi, 2024</p>
<p>Logic-enhanced language model agents for trustworthy social simulations. Agnieszka Mensfelt, Kostas Stathis, Vince Trencsenyi, 2024</p>
<p>Logic-LM: Empowering large language models with symbolic solvers for faithful logical reasoning. Liangming Pan, Alon Albalak, Xinyi Wang, William Wang, Findings of the Association for Computational Linguistics: EMNLP 2023. Houda Bouamor, Juan Pino, Kalika Bali, SingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, Jianfeng Gao, 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, 2023</p>
<p>Satlm: Satisfiability-aided language models using declarative prompting. Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett, 2023</p>            </div>
        </div>

    </div>
</body>
</html>
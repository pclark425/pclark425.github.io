<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9571 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9571</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9571</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-165.html">extraction-schema-165</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <p><strong>Paper ID:</strong> paper-278129632</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.18496v1.pdf" target="_blank">Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review</a></p>
                <p><strong>Paper Abstract:</strong> Comprehensive literature review requires synthesizing vast amounts of research -- a labor intensive and cognitively demanding process. Most prior work focuses either on helping researchers deeply understand a few papers (e.g., for triaging or reading), or retrieving from and visualizing a vast corpus. Deep analysis and synthesis of large paper collections (e.g., to produce a survey paper) is largely conducted manually with little support. We present DimInd, an interactive system that scaffolds literature review across large paper collections through LLM-generated structured representations. DimInd scaffolds literature understanding with multiple levels of compression, from papers, to faceted literature comparison tables with information extracted from individual papers, to taxonomies of concepts, to narrative syntheses. Users are guided through these successive information transformations while maintaining provenance to source text. In an evaluation with 23 researchers, DimInd supported participants in extracting information and conceptually organizing papers with less effort compared to a ChatGPT-assisted baseline workflow.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9571.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9571.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DimInd</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DimInd: LLM-enabled interactive literature review system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive system that uses LLMs to transform large collections of scholarly papers into successive structured representations (faceted tables, hierarchical taxonomies, and controllable narrative syntheses) with provenance tracing to support exploration and verification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI o3-mini; GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Most LLM-enabled components use OpenAI's o3-mini with the system configured to 'low' reasoning; value extraction uses GPT-4o-mini. The paper does not report model sizes or fine-tuning; models are used via prompt engineering and retrieval augmentation in the system pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Scholarly literature review (Computer Science / HCI in evaluation; generalizable to other scholarly domains)</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>User-supplied paper collections created from Semantic Scholar relevance searches or lists of IDs; collections used in study were sampled sets of 50 papers (study tasks) or example collections up to 83 papers; only open-access PDFs were downloaded and parsed with GROBID to obtain full text and sections prior to LLM processing.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Thematic / conceptual patterns and generalizable qualitative rules expressed as facets, hierarchical taxonomies, and narrative syntheses (i.e., emergent thematic principles or comparative dimensions across papers).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Example extracted patterns noted in the user scenario: an 'LLM-as-a-Judge' application area appearing across domains such as science, finance, and law, and a recurring qualitative claim that 'LLM evaluations are found to have certain biases.'</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>A multi-component LLM pipeline: (1) Facet discovery by sampling small subsets of papers (4-paper subsets), prompting an LLM on titles+abstracts to propose candidate facets, then consolidating facets across subsets via another LLM; (2) Retrieval-augmented value extraction where an LLM generates a paragraph per paper from full text, followed by a second LLM call to distill paragraphs into single-sentence evidence snippets; (3) LLM-directed hierarchical taxonomy creation that clusters snippets into up to 5 levels of nested categories; (4) Controllable LLM synthesis that generates narrative summaries structured according to selected taxonomy branches with inline citations. The pipeline uses embedding-based chunk highlighting (all-MiniLM-L6-v2) to point to source text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>A within-subjects user study with 23 CS/HCI researchers comparing DimInd to a baseline searchable-paper interface with ChatGPT assistance. Measures included post-task Likert-scale surveys (perceived effectiveness, control, verifiability, satisfaction), interaction logs (columns added, taxonomy interactions, summaries generated), and qualitative thematic analysis of interview transcripts. No expert gold-standard scoring of extracted taxonomies/summaries was performed; content quality was assessed via participant judgments and behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DimInd's structured LLM outputs (tables and taxonomies) significantly reduced perceived mental effort for extracting and organizing information and were rated superior for paper categorization versus the baseline. Taxonomies served as central navigational hubs and facilitated top-down exploration. Value extraction served as effective information scent. However, participants reported information overload when presented with many snippets, expressed hesitation to fully trust or delegate final narrative writing to the LLM-generated syntheses, and noted that current LLMs still fall short in producing deep intellectual connections across documents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared to a baseline interface plus ChatGPT, DimInd led to lower mental effort for extraction and higher ratings of categorization effectiveness; users found it somewhat easier to verify generated information with DimInd. Participants nonetheless valued conversational flexibility of the baseline and suggested a blended interface would be ideal.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Limited to open-access PDFs (falls back to abstracts otherwise), potential cognitive overload from rapid provision of many snippets, taxonomy misclassifications requiring manual refinement, a learning curve for structured interactions, and lack of gold-standard automated evaluation of extraction fidelity in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Authors explicitly flag risks of LLM hallucination and bias (e.g., LLM evaluations themselves can show biases). The system emphasizes provenance and interactive verification but acknowledges need for finer-grained attribution techniques and mechanisms to proactively flag likely hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9571.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9571.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FacetDiscovery</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Facet discovery (collection-aware facet induction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based method to propose collection-aware comparative dimensions (facets) by sampling small subsets of papers, prompting an LLM to generate candidate facets from titles+abstracts, and consolidating across subset outputs into a final set.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>o3-mini used to generate candidate facets from sampled subset contexts and to consolidate multiple facet lists; configured with lower 'reasoning' to balance latency and output specificity. No fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Literature review facet induction (demonstrated on HCI/CS paper collections).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Multiple random subsets sampled from the user's paper collection (empirically  = 4 papers per subset) using titles and abstracts as LLM context; collections in study were 50-paper sets (and examples up to 83 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Analytical dimensions / comparative principles (facets) that generalize across a collection (e.g., 'Application Area', 'Challenges and Limitations').</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>System-suggested facets such as 'Application Area' and 'Challenges and Limitations' which surface across multiple papers and enable cross-paper comparison (user scenario showed 'LLM-as-a-Judge' application area across science, finance, law).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompt-based facet generation on small sampled subsets of papers; repeated across multiple subsets to produce candidate lists; a consolidation LLM call merges recurring and semantically similar facets and prioritizes facets appearing across multiple subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Usage metrics (how many participants added system-suggested columns), participant feedback in the user study (perceived utility of suggested facets), and qualitative interview comments. The paper also reports empirical tuning (choice of subset size) based on observed output specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>System-suggested facets helped novice and expert users jumpstart exploration and were frequently used alongside user-defined facets; smaller subset sampling (4 papers) produced more specific, useful facets while larger subsets produced more generic facets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Baseline had no integrated collection-aware facet suggestions; participants found DimInd's suggested facets to be a helpful scaffold compared to manually constructing facet lists and prompting ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Tendency for the LLM to produce generic facets when larger subsets were used; dependence on titles+abstracts can limit facet specificity; facets may not cover all nuanced user needs and require user customization.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>No explicit hallucination examples tied to facet discovery reported, but general caution about LLM output validity and the need for user steering is noted.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9571.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9571.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ValueExtraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented value extraction and distillation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage LLM process that retrieves evidence from each paper's full text to generate paragraph-level extractions and then distills those paragraphs into single-sentence evidence snippets to populate a faceted literature review table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4o-mini (primary extraction); OpenAI o3-mini (distillation in other components)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Value extraction uses GPT-4o-mini for paragraph-level extraction from full text; distilled single-sentence snippets are produced with a secondary LLM call (o3-mini for latency/performance tradeoffs). No fine-tuning is reported; retrieval augmentation and prompt engineering are employed.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Evidence extraction from scholarly full texts (demonstrated on HCI/CS papers).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Full-text PDFs parsed via GROBID into chunks; open-access PDFs were downloaded for collections (50-paper study collections). Heavy parallelization of LLM calls to populate table cells rapidly.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Per-paper evidence snippets that encode qualitative claims or observations (atomic qualitative statements suitable for aggregation into themes).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>A distilled snippet from a paper reading: 'Like human evaluators, LLMs evaluations are also found to have certain biases.' (presented as an evidence snippet in the literature review table).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Retrieval-augmented generation where the LLM is provided relevant full-text chunks to generate a paragraph per paper; a subsequent LLM call distills paragraphs into concise single-sentence snippets. Embeddings (all-MiniLM-L6-v2) are used to map snippets to source chunks for highlighting and evidence linking.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>User-centered evaluation via the study: participants interacted with snippets, opened evidence in the integrated reader, and reported perceived decrease in effort and improved verifiability; interaction logs captured how often users opened source PDFs and used evidence highlights.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Value extraction enabled fast, cross-paper extraction providing effective 'information scent' to guide reading; it reduced time/effort versus manual extraction. However, presenting many snippets at once could overwhelm users, and users often treated snippets as navigational cues rather than final content.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Compared with baseline ChatGPT-assisted manual exploration, DimInd's value extraction populated faceted tables automatically, saving substantial labor and enabling simultaneous cross-paper comparisonâ€”participants reported lower extraction effort with DimInd.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Potential cognitive overload from many snippets, dependency on open-access full text (falls back to abstracts otherwise), and the need for careful prompt/context design to avoid irrelevant or low-utility extractions.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Authors note risks of hallucinated or unsupported claims and emphasize provenance linking (highlighted source chunks) to help verification; participants still expressed concerns about trusting LLM-generated paraphrases over original text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9571.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9571.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TaxonomyCreation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based hierarchical taxonomy creation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-directed method that clusters faceted snippets into hierarchical tree structures (taxonomies) with dynamic depth (up to five levels), producing emergent thematic overviews across a paper collection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>o3-mini is used to perform clustering and hierarchical taxonomy induction via prompts that direct the model to create meaningful categories and avoid excessive fragmentation; no model fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Thematic clustering and organization within literature review workflows (demonstrated on HCI/CS paper collections).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Sets of distilled evidence snippets for a single facet drawn from all papers in a collection (study collections of 50 papers; examples up to 83 papers).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Hierarchical thematic patterns (taxonomic conceptual structures) that function as generalized rules for grouping similar qualitative findings across many papers.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>Higher-level categories such as 'Bias', 'Transparency', and 'Societal Harms' emerging from an 'Ethical considerations' facet; ability to merge mis-split subcategories like grouping 'creative writing' under a broader 'writing' category.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompt-driven hierarchical clustering where the LLM is instructed to produce nested categories, maintain coverage (every paper included), and allow multi-category membership per paper. The resulting taxonomy can be interactively refined by users (drag-and-drop).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>User interactions and perceptions measured in the study: taxonomy usage counts, Likert-scale ratings for paper categorization support, and qualitative feedback describing taxonomies as central navigational hubs. Users' manual edits validated taxonomy utility.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Taxonomies were highly valued as cognitive scaffolds and navigational hubs, improving participants' ability to see thematic distributions. They increased perceived categorization effectiveness but were imperfect (misgroupings, fragmentation) and often required user refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>Participants rated DimInd's taxonomy-driven categorization significantly better than the baseline; baseline workflows required manual collation before LLM assistance could be applied.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Occasional misclassifications and unnatural splits; need for interactive refinement; taxonomy induction limited to single-facet views (multi-facet interactions are future work).</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Taxonomy errors are discussed as quality limitations (not framed strictly as hallucinations) and the authors recommend adding controls and explanations (e.g., why a category was formed) to mitigate trust issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9571.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9571.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill, extract, or synthesize qualitative laws, principles, or generalizable rules from large numbers of scholarly input papers, including details of the methods, domains, evaluation, and results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FacetSynthesis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controllable facet synthesis into narrative summaries</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based synthesis step that converts selected branches of a facet taxonomy into coherent narrative summaries, instructed to include all associated papers and to inline-cite claims to enable traceable aggregated assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>OpenAI o3-mini</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>o3-mini is used with structured prompts that require the model to follow taxonomy structure, include all papers associated with selected nodes, and attribute generated claims with inline citations; no fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Generating narrative syntheses for literature review sections grounded in extracted facets and taxonomies (HCI/CS demonstration).</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Selected taxonomy nodes (branches) with their associated distilled snippets and linked paper metadata/citations drawn from collections of ~50 papers parsed from open PDFs.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_type</strong></td>
                            <td>Aggregated narrative rules or summarizing principles expressed as prose (multi-document synthesis with explicit attribution), intended to present generalizable qualitative findings across the selected taxonomy branches.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_law_example</strong></td>
                            <td>A generated facet synthesis summarizing top application areas of 'LLM-as-a-Judge' across selected categories with inline citations to the papers supporting each claim.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_methodology</strong></td>
                            <td>Prompted generation following the user-refined taxonomy structure; the LLM is constrained to include cited papers for claims and to follow the selected nodes as the outline for the synthesized narrative.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>User study observations: about half of participants generated at least one taxonomy-to-summary transformation and some inspected inline citations. Participants' qualitative feedback and interaction with citations were used to assess the utility and verifiability of synthesized output.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Facet syntheses provided useful, exportable summaries and supported rapid outline creation, but many participants hesitated to accept the generated prose as final due to formulaic style, concerns about voice/ownership, and fear of unconscious plagiarism; users tended to use syntheses as drafts or scaffolds rather than finished text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baseline</strong></td>
                            <td>DimInd offered more controllable, taxonomy-aligned syntheses with traceable citations compared to freeform ChatGPT summarization, which participants found less structured but more conversationally steerable.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Participants worried about the 'formulaic' output style, intellectual ownership, potential plagiarism, and occasional mis-attribution; many preferred to preserve researcher voice by editing or rewriting the generated prose.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_or_hallucination_issues</strong></td>
                            <td>Authors note hallucination risks at scale for synthesized narratives and advocate for stronger attribution methods and proactive hallucination detection/flagging; participants expressed concerns about trusting LLM prose without checking sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support <em>(Rating: 2)</em></li>
                <li>Arx-ivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models <em>(Rating: 2)</em></li>
                <li>SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model <em>(Rating: 2)</em></li>
                <li>ClusterLLM: Large Language Models as a Guide for Text Clustering <em>(Rating: 1)</em></li>
                <li>Hierarchical Catalogue Generation for Literature Review: A Benchmark <em>(Rating: 1)</em></li>
                <li>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9571",
    "paper_id": "paper-278129632",
    "extraction_schema_id": "extraction-schema-165",
    "extracted_data": [
        {
            "name_short": "DimInd",
            "name_full": "DimInd: LLM-enabled interactive literature review system",
            "brief_description": "An interactive system that uses LLMs to transform large collections of scholarly papers into successive structured representations (faceted tables, hierarchical taxonomies, and controllable narrative syntheses) with provenance tracing to support exploration and verification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "OpenAI o3-mini; GPT-4o-mini",
            "llm_model_description": "Most LLM-enabled components use OpenAI's o3-mini with the system configured to 'low' reasoning; value extraction uses GPT-4o-mini. The paper does not report model sizes or fine-tuning; models are used via prompt engineering and retrieval augmentation in the system pipeline.",
            "application_domain": "Scholarly literature review (Computer Science / HCI in evaluation; generalizable to other scholarly domains)",
            "input_corpus_description": "User-supplied paper collections created from Semantic Scholar relevance searches or lists of IDs; collections used in study were sampled sets of 50 papers (study tasks) or example collections up to 83 papers; only open-access PDFs were downloaded and parsed with GROBID to obtain full text and sections prior to LLM processing.",
            "qualitative_law_type": "Thematic / conceptual patterns and generalizable qualitative rules expressed as facets, hierarchical taxonomies, and narrative syntheses (i.e., emergent thematic principles or comparative dimensions across papers).",
            "qualitative_law_example": "Example extracted patterns noted in the user scenario: an 'LLM-as-a-Judge' application area appearing across domains such as science, finance, and law, and a recurring qualitative claim that 'LLM evaluations are found to have certain biases.'",
            "extraction_methodology": "A multi-component LLM pipeline: (1) Facet discovery by sampling small subsets of papers (4-paper subsets), prompting an LLM on titles+abstracts to propose candidate facets, then consolidating facets across subsets via another LLM; (2) Retrieval-augmented value extraction where an LLM generates a paragraph per paper from full text, followed by a second LLM call to distill paragraphs into single-sentence evidence snippets; (3) LLM-directed hierarchical taxonomy creation that clusters snippets into up to 5 levels of nested categories; (4) Controllable LLM synthesis that generates narrative summaries structured according to selected taxonomy branches with inline citations. The pipeline uses embedding-based chunk highlighting (all-MiniLM-L6-v2) to point to source text.",
            "evaluation_method": "A within-subjects user study with 23 CS/HCI researchers comparing DimInd to a baseline searchable-paper interface with ChatGPT assistance. Measures included post-task Likert-scale surveys (perceived effectiveness, control, verifiability, satisfaction), interaction logs (columns added, taxonomy interactions, summaries generated), and qualitative thematic analysis of interview transcripts. No expert gold-standard scoring of extracted taxonomies/summaries was performed; content quality was assessed via participant judgments and behavior.",
            "results_summary": "DimInd's structured LLM outputs (tables and taxonomies) significantly reduced perceived mental effort for extracting and organizing information and were rated superior for paper categorization versus the baseline. Taxonomies served as central navigational hubs and facilitated top-down exploration. Value extraction served as effective information scent. However, participants reported information overload when presented with many snippets, expressed hesitation to fully trust or delegate final narrative writing to the LLM-generated syntheses, and noted that current LLMs still fall short in producing deep intellectual connections across documents.",
            "comparison_to_baseline": "Compared to a baseline interface plus ChatGPT, DimInd led to lower mental effort for extraction and higher ratings of categorization effectiveness; users found it somewhat easier to verify generated information with DimInd. Participants nonetheless valued conversational flexibility of the baseline and suggested a blended interface would be ideal.",
            "reported_limitations": "Limited to open-access PDFs (falls back to abstracts otherwise), potential cognitive overload from rapid provision of many snippets, taxonomy misclassifications requiring manual refinement, a learning curve for structured interactions, and lack of gold-standard automated evaluation of extraction fidelity in the study.",
            "bias_or_hallucination_issues": "Authors explicitly flag risks of LLM hallucination and bias (e.g., LLM evaluations themselves can show biases). The system emphasizes provenance and interactive verification but acknowledges need for finer-grained attribution techniques and mechanisms to proactively flag likely hallucinations.",
            "uuid": "e9571.0",
            "source_info": {
                "paper_title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "FacetDiscovery",
            "name_full": "Facet discovery (collection-aware facet induction)",
            "brief_description": "An LLM-based method to propose collection-aware comparative dimensions (facets) by sampling small subsets of papers, prompting an LLM to generate candidate facets from titles+abstracts, and consolidating across subset outputs into a final set.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "OpenAI o3-mini",
            "llm_model_description": "o3-mini used to generate candidate facets from sampled subset contexts and to consolidate multiple facet lists; configured with lower 'reasoning' to balance latency and output specificity. No fine-tuning reported.",
            "application_domain": "Literature review facet induction (demonstrated on HCI/CS paper collections).",
            "input_corpus_description": "Multiple random subsets sampled from the user's paper collection (empirically  = 4 papers per subset) using titles and abstracts as LLM context; collections in study were 50-paper sets (and examples up to 83 papers).",
            "qualitative_law_type": "Analytical dimensions / comparative principles (facets) that generalize across a collection (e.g., 'Application Area', 'Challenges and Limitations').",
            "qualitative_law_example": "System-suggested facets such as 'Application Area' and 'Challenges and Limitations' which surface across multiple papers and enable cross-paper comparison (user scenario showed 'LLM-as-a-Judge' application area across science, finance, law).",
            "extraction_methodology": "Prompt-based facet generation on small sampled subsets of papers; repeated across multiple subsets to produce candidate lists; a consolidation LLM call merges recurring and semantically similar facets and prioritizes facets appearing across multiple subsets.",
            "evaluation_method": "Usage metrics (how many participants added system-suggested columns), participant feedback in the user study (perceived utility of suggested facets), and qualitative interview comments. The paper also reports empirical tuning (choice of subset size) based on observed output specificity.",
            "results_summary": "System-suggested facets helped novice and expert users jumpstart exploration and were frequently used alongside user-defined facets; smaller subset sampling (4 papers) produced more specific, useful facets while larger subsets produced more generic facets.",
            "comparison_to_baseline": "Baseline had no integrated collection-aware facet suggestions; participants found DimInd's suggested facets to be a helpful scaffold compared to manually constructing facet lists and prompting ChatGPT.",
            "reported_limitations": "Tendency for the LLM to produce generic facets when larger subsets were used; dependence on titles+abstracts can limit facet specificity; facets may not cover all nuanced user needs and require user customization.",
            "bias_or_hallucination_issues": "No explicit hallucination examples tied to facet discovery reported, but general caution about LLM output validity and the need for user steering is noted.",
            "uuid": "e9571.1",
            "source_info": {
                "paper_title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "ValueExtraction",
            "name_full": "Retrieval-augmented value extraction and distillation",
            "brief_description": "A two-stage LLM process that retrieves evidence from each paper's full text to generate paragraph-level extractions and then distills those paragraphs into single-sentence evidence snippets to populate a faceted literature review table.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "GPT-4o-mini (primary extraction); OpenAI o3-mini (distillation in other components)",
            "llm_model_description": "Value extraction uses GPT-4o-mini for paragraph-level extraction from full text; distilled single-sentence snippets are produced with a secondary LLM call (o3-mini for latency/performance tradeoffs). No fine-tuning is reported; retrieval augmentation and prompt engineering are employed.",
            "application_domain": "Evidence extraction from scholarly full texts (demonstrated on HCI/CS papers).",
            "input_corpus_description": "Full-text PDFs parsed via GROBID into chunks; open-access PDFs were downloaded for collections (50-paper study collections). Heavy parallelization of LLM calls to populate table cells rapidly.",
            "qualitative_law_type": "Per-paper evidence snippets that encode qualitative claims or observations (atomic qualitative statements suitable for aggregation into themes).",
            "qualitative_law_example": "A distilled snippet from a paper reading: 'Like human evaluators, LLMs evaluations are also found to have certain biases.' (presented as an evidence snippet in the literature review table).",
            "extraction_methodology": "Retrieval-augmented generation where the LLM is provided relevant full-text chunks to generate a paragraph per paper; a subsequent LLM call distills paragraphs into concise single-sentence snippets. Embeddings (all-MiniLM-L6-v2) are used to map snippets to source chunks for highlighting and evidence linking.",
            "evaluation_method": "User-centered evaluation via the study: participants interacted with snippets, opened evidence in the integrated reader, and reported perceived decrease in effort and improved verifiability; interaction logs captured how often users opened source PDFs and used evidence highlights.",
            "results_summary": "Value extraction enabled fast, cross-paper extraction providing effective 'information scent' to guide reading; it reduced time/effort versus manual extraction. However, presenting many snippets at once could overwhelm users, and users often treated snippets as navigational cues rather than final content.",
            "comparison_to_baseline": "Compared with baseline ChatGPT-assisted manual exploration, DimInd's value extraction populated faceted tables automatically, saving substantial labor and enabling simultaneous cross-paper comparisonâ€”participants reported lower extraction effort with DimInd.",
            "reported_limitations": "Potential cognitive overload from many snippets, dependency on open-access full text (falls back to abstracts otherwise), and the need for careful prompt/context design to avoid irrelevant or low-utility extractions.",
            "bias_or_hallucination_issues": "Authors note risks of hallucinated or unsupported claims and emphasize provenance linking (highlighted source chunks) to help verification; participants still expressed concerns about trusting LLM-generated paraphrases over original text.",
            "uuid": "e9571.2",
            "source_info": {
                "paper_title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "TaxonomyCreation",
            "name_full": "LLM-based hierarchical taxonomy creation",
            "brief_description": "An LLM-directed method that clusters faceted snippets into hierarchical tree structures (taxonomies) with dynamic depth (up to five levels), producing emergent thematic overviews across a paper collection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "OpenAI o3-mini",
            "llm_model_description": "o3-mini is used to perform clustering and hierarchical taxonomy induction via prompts that direct the model to create meaningful categories and avoid excessive fragmentation; no model fine-tuning reported.",
            "application_domain": "Thematic clustering and organization within literature review workflows (demonstrated on HCI/CS paper collections).",
            "input_corpus_description": "Sets of distilled evidence snippets for a single facet drawn from all papers in a collection (study collections of 50 papers; examples up to 83 papers).",
            "qualitative_law_type": "Hierarchical thematic patterns (taxonomic conceptual structures) that function as generalized rules for grouping similar qualitative findings across many papers.",
            "qualitative_law_example": "Higher-level categories such as 'Bias', 'Transparency', and 'Societal Harms' emerging from an 'Ethical considerations' facet; ability to merge mis-split subcategories like grouping 'creative writing' under a broader 'writing' category.",
            "extraction_methodology": "Prompt-driven hierarchical clustering where the LLM is instructed to produce nested categories, maintain coverage (every paper included), and allow multi-category membership per paper. The resulting taxonomy can be interactively refined by users (drag-and-drop).",
            "evaluation_method": "User interactions and perceptions measured in the study: taxonomy usage counts, Likert-scale ratings for paper categorization support, and qualitative feedback describing taxonomies as central navigational hubs. Users' manual edits validated taxonomy utility.",
            "results_summary": "Taxonomies were highly valued as cognitive scaffolds and navigational hubs, improving participants' ability to see thematic distributions. They increased perceived categorization effectiveness but were imperfect (misgroupings, fragmentation) and often required user refinement.",
            "comparison_to_baseline": "Participants rated DimInd's taxonomy-driven categorization significantly better than the baseline; baseline workflows required manual collation before LLM assistance could be applied.",
            "reported_limitations": "Occasional misclassifications and unnatural splits; need for interactive refinement; taxonomy induction limited to single-facet views (multi-facet interactions are future work).",
            "bias_or_hallucination_issues": "Taxonomy errors are discussed as quality limitations (not framed strictly as hallucinations) and the authors recommend adding controls and explanations (e.g., why a category was formed) to mitigate trust issues.",
            "uuid": "e9571.3",
            "source_info": {
                "paper_title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "FacetSynthesis",
            "name_full": "Controllable facet synthesis into narrative summaries",
            "brief_description": "An LLM-based synthesis step that converts selected branches of a facet taxonomy into coherent narrative summaries, instructed to include all associated papers and to inline-cite claims to enable traceable aggregated assertions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "OpenAI o3-mini",
            "llm_model_description": "o3-mini is used with structured prompts that require the model to follow taxonomy structure, include all papers associated with selected nodes, and attribute generated claims with inline citations; no fine-tuning reported.",
            "application_domain": "Generating narrative syntheses for literature review sections grounded in extracted facets and taxonomies (HCI/CS demonstration).",
            "input_corpus_description": "Selected taxonomy nodes (branches) with their associated distilled snippets and linked paper metadata/citations drawn from collections of ~50 papers parsed from open PDFs.",
            "qualitative_law_type": "Aggregated narrative rules or summarizing principles expressed as prose (multi-document synthesis with explicit attribution), intended to present generalizable qualitative findings across the selected taxonomy branches.",
            "qualitative_law_example": "A generated facet synthesis summarizing top application areas of 'LLM-as-a-Judge' across selected categories with inline citations to the papers supporting each claim.",
            "extraction_methodology": "Prompted generation following the user-refined taxonomy structure; the LLM is constrained to include cited papers for claims and to follow the selected nodes as the outline for the synthesized narrative.",
            "evaluation_method": "User study observations: about half of participants generated at least one taxonomy-to-summary transformation and some inspected inline citations. Participants' qualitative feedback and interaction with citations were used to assess the utility and verifiability of synthesized output.",
            "results_summary": "Facet syntheses provided useful, exportable summaries and supported rapid outline creation, but many participants hesitated to accept the generated prose as final due to formulaic style, concerns about voice/ownership, and fear of unconscious plagiarism; users tended to use syntheses as drafts or scaffolds rather than finished text.",
            "comparison_to_baseline": "DimInd offered more controllable, taxonomy-aligned syntheses with traceable citations compared to freeform ChatGPT summarization, which participants found less structured but more conversationally steerable.",
            "reported_limitations": "Participants worried about the 'formulaic' output style, intellectual ownership, potential plagiarism, and occasional mis-attribution; many preferred to preserve researcher voice by editing or rewriting the generated prose.",
            "bias_or_hallucination_issues": "Authors note hallucination risks at scale for synthesized narratives and advocate for stronger attribution methods and proactive hallucination detection/flagging; participants expressed concerns about trusting LLM prose without checking sources.",
            "uuid": "e9571.4",
            "source_info": {
                "paper_title": "Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support",
            "rating": 2,
            "sanitized_title": "chime_llmassisted_hierarchical_organization_of_scientific_studies_for_literature_review_support"
        },
        {
            "paper_title": "Arx-ivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models",
            "rating": 2,
            "sanitized_title": "arxivdigestables_synthesizing_scientific_literature_into_tables_using_language_models"
        },
        {
            "paper_title": "SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model",
            "rating": 2,
            "sanitized_title": "scidasynth_interactive_structured_knowledge_extraction_and_synthesis_from_scientific_literature_with_large_language_model"
        },
        {
            "paper_title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
            "rating": 1,
            "sanitized_title": "clusterllm_large_language_models_as_a_guide_for_text_clustering"
        },
        {
            "paper_title": "Hierarchical Catalogue Generation for Literature Review: A Benchmark",
            "rating": 1,
            "sanitized_title": "hierarchical_catalogue_generation_for_literature_review_a_benchmark"
        },
        {
            "paper_title": "Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning",
            "rating": 2,
            "sanitized_title": "automating_research_synthesis_with_domainspecific_large_language_model_finetuning"
        }
    ],
    "cost": 0.01901425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review
25 Apr 2025</p>
<p>Raymond Fok rayfok@cs.washington.edu 
University of Washington Seattle
WAUSA</p>
<p>Allen Institute for AI &amp; University of Washington Seattle
WAUSA</p>
<p>Joseph Chee josephc@allenai.org 
Allen Institute for AI Seattle
WAUSA</p>
<p>Marissa Radensky radensky@cs.washington.edu 
Allen Institute for AI Seattle
WAUSA</p>
<p>Making Sense of Unstructured Data</p>
<p>Pao Siangliulue paos@allenai.org 
University of Washington Seattle
WAUSA</p>
<p>Making Sense of Unstructured Data</p>
<p>Jonathan Bragg jbragg@allenai.org 
Allen Institute for AI Seattle
WAUSA</p>
<p>Making Sense of Unstructured Data</p>
<p>Amy X Zhang 
Allen Institute for AI Seattle
WAUSA</p>
<p>Making Sense of Unstructured Data</p>
<p>Daniel S Weld 
University of Washington Seattle
WAUSA</p>
<p>Making Sense of Unstructured Data</p>
<p>Joseph Chee Chang 
Making Sense of Unstructured Data</p>
<p>Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review
25 Apr 2025D5492D7BD5D6F0486BC44C90EEE18835arXiv:2504.18496v1[cs.HC]
Figure1: We present an LLM-assisted workflow aimed at scaffolding literature review over large paper collections, and instantiate it in a prototype system, DimInd.Users can interactively construct and explore four successive structured representations of literature information: a paper collection listing papers and metadata, a literature review table with columns that render relevant evidence snippets from paper full texts along defined facets, a facet taxonomy that organizes faceted information into a higher-level conceptual overview, and a facet synthesis that provides a controllable faceted summary across the collection.</p>
<p>Introduction</p>
<p>Literature review offers a foundation for scientific progress.Researchers spend significant effort making sense of prior work throughout a research project, from finding inspirations for ideation, exploring techniques and datasets to develop methods, to writing related work to contextualize their own work.One way this effort can be shared and reused is in the form of survey papers that comprehensively review a large collection of papers.However, the process of reviewing and synthesizing a large set of papers is timeintensive [8,58,72] and cognitively challenging [32,49].As a result, this process is often conducted with a team of researchers over multiple weeks or months and rarely feasible for individual scholars.For example, Pang et al. [64] had seven authors who collectively reviewed 186 papers, while Lee et al. [51] had an even bigger team of 36 researchers reviewing 274 papers.In the field of medicine, Cochrane coordinates a team of over 28,000 contributors to produce its timely review articles [11].</p>
<p>One main challenge is the sheer amount of information scattered across disparate papers that needs to be extracted, analyzed, compared, and synthesized [51,74].As a result, researchers often follow a step-by-step process to make the tasks more tractable, involving successively transforming information from one representation to another that is one degree more synthesized.For instance, the process begins with gathering a large collection of relevant papers, then reading and summarizing those papers individually, coding the summaries into common themes or aspects in large spreadsheets [36,62], building out a taxonomy of research threads [44,63], and finally, synthesizing this structure into writing.Information management and coordination often involves large tabular spreadsheets to keep track of papers, summaries, qualitative codes, and themes. 1 Currently, much of this work is labor-intensive and low-level, involving significant back-and-forth-from manually inspecting the full text of papers and extracting information across many facets, to revisiting the full text to check for accuracyincurring substantial cognitive costs [18,47,67].And rapid growth in scholarly publication means that the effort required to produce such reviews will only continue to increase [9,43].</p>
<p>Recent work has suggested large language models (LLMs) can be effective at facilitating this information extraction and compression process over large paper collections but these explorations have largely focused on "data tables" or task-specific structures, such as size and accuracy of AI models or intervention and patient demographic in clinical trials [24,82].In contrast, their ability to extract nuanced qualitative insights that are useful for researchers to create more narrative literature reviews remains underexplored, and more importantly, existing approaches typically produce only flat, single-level representations (such as tables) rather than the multiple, interconnected levels of abstraction needed to support the progressive synthesis of comprehensive literature review.</p>
<p>In this work, we present DimInd, an interactive system for LLMpowered literature review support that guides researchers through successive structured representations of paper information.Starting with a collection of papers, users iteratively construct a faceted 1 e.g., https://writing-assistant.github.io/#annotated-papers,https://github.com/Social-Futures-Lab/skin-deep/blob/main/Literature%20Review.csvliterature review table that organizes relevant information extracted from papers' full texts by defining custom facets in natural language or selecting from collection-aware facets suggested by the system.The system then transforms each faceted column into a hierarchical taxonomy of concepts, surfacing emergent themes across papers.These taxonomies can be then explored, refined, and transformed into narrative syntheses.Users are guided through these structured representations via interaction and visual affordances.This progressive disclosure not only scaffolds sensemaking, but also supports the steering and verification of information, which is critical when working with LLM-generated content.</p>
<p>To evaluate how DimInd can better support analyzing and synthesizing large collections of papers, we conducted a within-subjects user evaluation with 23 computer science researchers where they reviewed two sets of 50 research papers in two 30-minute sessions.Comparing DimInd against a baseline literature review approach assisted by a commercial LLM-based chat application (ChatGPT), we found that DimInd's structured representations effectively supported users in extracting, organizing, and verifying information across papers with less effort.Our qualitative findings further reveal how researchers used tables as information scent and taxonomies as navigational hubs, transforming information into manageable views that facilitate movement between high-level organization and detailed exploration, while balancing LLM assistance with their own scholarly agency.We conclude by discussing the tradeoffs of structured versus conversational assistance and highlight opportunities and tensions in LLM-assisted literature review workflows.In sum, this paper contributes:</p>
<p>â€¢ Design goals for an LLM-assisted workflow for literature review support, informed by sensemaking theories and prior work, that address the tedium and cognitive challenges in extracting, organizing, and making sense of information across a large body of literature through successive structured representations.â€¢ DimInd, an interactive system that instantiates these ideas, with features to support users in exploring and navigating between transformations of literature information.â€¢ Findings from a within-subjects user evaluation with 23 researchers, demonstrating that DimInd reduced cognitive load and increased paper organization effectiveness during exploratory literature review compared to traditional methods, with qualitative insights into how participants use and navigate across the various structured representations.</p>
<p>topic modeling can help analyze large text collections [6,33]; however these methods often produced unsatisfactory analyses-irrelevant [1,12], uninterpretable [22], or misaligned [40].</p>
<p>More recent work has explored leveraging large language models (LLMs) for automating qualitative analysis, revisiting tasks such as topic modeling [65,81] and document clustering [83,86] to greater success.Inspired by these successes, we explore if and how LLMs can similarly induce higher-order structures from large amounts of complex scholarly information toward scaffolding an interactive literature review workflow.</p>
<p>Relatedly, prior work has explored mixed-initiative systems [39] for general qualitative data analysis.Tools like Cody [70], Scholastic [38], and CoAIcoder [30] demonstrate how AI can support qualitative researchers in organizing and analyzing text, while systems like LLooM [50] extract high-level facets to guide initial directions of analysis.In this paper, we focus on applying mixed-initiative principles to literature review, presenting interactive structured representations-faceted tables, taxonomies, and narratives-that scaffold the foraging and sensemaking activities of literature review while keeping researchers connected to the underlying papers.</p>
<p>Deep Engagement with Small Sets of Papers</p>
<p>With the exponential growth of scientific publication and recent advancement in LLMs, the HCI research community has become increasingly excited about building user-driven tools that can better support understanding the literature at various scales.</p>
<p>One line of work has explored augmented reading interfaces to reduce cognitive load when engaging with individual papers.These include tools that aid comprehension by simplifying scholarly language [4,37], improve efficiency by selectively guiding reader attention [27,35], and lower the costs of saving relevant information [35,44].Other tools weave external context into the paper environment, for example by visually augmenting inline citations based on a user's reading history [15], or embedding relevant follow-on work [69] and presentation videos [48] as margin-notes localized to relevant parts of a paper.</p>
<p>Beyond reading support for individual papers, prior work has also explored how to help researchers more deeply understand a small set of papers.For example, systems such as PaperWeaver [52] or ACCoRD [60] generate comparative statements between of two papers or concepts to help researchers better understand an unfamiliar paper by contextualizing it to a familiar paper.More closely related to our work, systems like Elicit [24] and SciDaSynth [82] allow users to compare small sets of papers in a fixed table-based representation.Tables are ubiquitous across diverse sensemaking domains, such as online search [13,14,75], developer support [54,55], and business analysis [28]; within scholarly sensemaking, researchers also often manually create literature review tables to organize, compare, and synthesize information across many papers [36,62,82].However, at the scale of larger paper collections (e.g., dozens to hundreds), tables can become insufficient for sensemaking, and require complementary affordances and flexible representations to provide additional cognitive support.</p>
<p>Understanding Literature at Scale</p>
<p>While there are also user-driven systems that help users engage with papers at scale, these have focused on supporting searching and navigation along high-level topics.For example, prior approaches cluster a paper's references in a reading environment into related research threads for exploration [44,45], visually cluster papers on a canvas based on citation-edges or semantic similarity [59,61], organize papers in hierarchical structures [41,87], and create overarching narrative summaries [2,77].While these approaches may be effective for broadly searching and navigating over the literature landscape, they lack support for deeper comparison and synthesis over a large collection of papers.They often stop short of supporting the schematization aspect of sensemaking, particularly in organizing nuanced relationships grounded in information from papers' full texts.</p>
<p>Another major thread of research has focused on fully automating literature review and survey paper writing [2,25,56,77].These techniques are promising but nascent [7,68], facing challenges in the quality of generated syntheses [21,57], hallucination [5,31], and alignment with researchers' specific goals [68,84].More fundamentally, fully automated methods still struggle to capture the nuanced and iterative nature of sensemaking, including both qualitative analysis and literature review.Analysts often want to "stay close to the data," and are skeptical of fully automating away the review process [26,42].</p>
<p>In this work, we instead focus on building tools that can better support the user-driven process of deeply understanding collections of research papers for literature review.Our work complements existing efforts by enabling researchers to iteratively build and refine structured representations that capture meaningful connections across large paper collections.</p>
<p>Designing LLM-Assisted Workflows for</p>
<p>Literature Review</p>
<p>Our design goals are motivated by gaps in prior work in user-driven literature understanding tools detailed in Section 2.2 and 2.3.We also draw from cognitive psychology theories on how knowledge workers make sense of large amounts of information, specifically, sensemaking and information foraging theory.Literature review requires identifying, understanding, and organizing multi-faceted relationships across many research papers [36,74].We are inspired primarily by Pirolli and Card [67]'s notional model of sensemaking, which describes how analysts iteratively collect, organize, and synthesize information to generate insights.This framework conceptualizes effective information analysis as "the process of creating a representation or schema to answer task-specific questions, " where schemas are structured, often externalized, knowledge artifacts created and refined during sensemaking [71].Applied to literature review, such structured representations can scaffold the cognitive progression [3] from low-level information retrieval to higher-order processes of analyzing, evaluating, and synthesizing in scholarly sensemaking.</p>
<p>Information foraging theory [66] further highlights two forms of environmental enrichment that can optimize gathering relevant information from information patches (e.g., research papers)-1) reduce the cost of navigating between patches, and 2) make patches yield better returns of valuable information.A structured literature review table serves both these purposes: it functions as an externalized schema for sensemaking while organizing information across papers into a unified view (reducing navigation costs) and extracting key findings (improving information yield).These theories guide our following three design goals for LLM-assisted literature review workflows, addressing common challenges in literature review processes observed in prior work [16,20,29,47,78].</p>
<p>Design Goals</p>
<p>DG1. Help researchers transform a large, unstructured literature collection into a structured repository of extracted relevant information.Literature review requires researchers to process large volumes of unstructured content into meaningful, structured representations.A system should reduce the cognitive costs of extracting, organizing, and accessing relevant information across multiple papers.By creating a structured repository, i.e., a literature review table, that facilitates organization and easy navigation of key information across papers, researchers can shift their mental resources toward higher-level processes of evaluation and synthesis, rather than the tedious and repetitive process of information extraction.</p>
<p>DG2. Help researchers better make sense of vast information across many papers, transitioning between low-level details and high-level patterns.Analyzing patterns and deriving insight across multiple papers is one of the key cognitive challenges in effective literature review, particularly as the number of papers in a review grows.A system should aim to support researchers in making sense of their information repository, and particularly in navigating between successive levels of analysis and structure, e.g., from individual paper review to facet-grounded comparisons to collection-wide synthesis.This could involve providing additional structures that transform the literature review table to scaffold and guide researcher judgment of extracted information, or surfacing potential patterns across papers while enabling researchers to examine and evaluate these suggestions based on their expertise.DG3.Help researchers quickly validate the veracity of AIgenerated content.Oversight and evaluation of information processed by an AI system and not researchers themselves is critical in maintaining the integrity of a literature review.As LLMs become increasingly integrate into these workflows-suggesting, extracting, and summarizing information-appropriate mechanisms are needed for researchers to efficiently verify the assistance these models provide.</p>
<p>DimInd</p>
<p>Based on these design goals, we present DimInd, an interactive system that supports researchers in exploring and making sense of large literature collections by using large language models (LLMs) to transform dense paper information into a series of linked, structured information representations.With DimInd, users employ natural language to convey specific information needs, and in response, the system populates a literature review table by creating a column in the table generated using evidence retrieved from the full texts of papers in the collection.DimInd also suggests several collectionaware columns to help users get started (DG1).DimInd further organizes the information within each column into a hierarchical taxonomy overviewing the available themes across the collection, and allows controllable generation of a narrative synthesis that bridges the schematization and presentation phases of sensemaking (DG2).Connecting these information abstraction layers through interaction, DimInd facilitates bi-directional exploration between the raw information within individual papers and broader themes that span across multiple papers (DG3).</p>
<p>Interface and Example User Scenario</p>
<p>To illustrate the design and features of DimInd, we present a user scenario featuring Juno, a researcher in human-AI interaction who is interested in exploring the use of LLMs as evaluators for complex tasks.Before setting a specific research direction, she decides to consult the literature to better understand the existing work.She starts by compiling relevant papers she had saved and results from an academic search engine using the query LLM-as-a-Judge.We join Juno as she uploads her collection of 83 papers to DimInd.</p>
<p>4.1.1Defining faceted columns to extract relevant information across papers.To start, DimInd transforms her paper collection into a literature review table with a single Paper column, where each cell contains relevant metadata for a single paper (title, author, and citation count).Based on her prior knowledge and reading the papers' titles, Juno has several questions she's interested in exploring.For example, she had noticed two papers in her collection that use LLMs as evaluators for scientific ideation, and wonders, "Where else has an LLM-as-a-Judge paradigm been used?"She clicks Add Column to create a user-defined column in the table representing the information facet she wants to explore (Figure 2).In the column creation modal, she specifies the facet (Application Area) in the Column Name, offers specific examples ("scientific ideation, creativity, etc") in the Column Description to guide the information generated by the system, and leaves the Column Type as the default type of text.</p>
<p>Using Juno's specification, the system adds a new column to the table and populates each cell in the column with a short snippet of relevant information (evidence snippet) generated from the full text of the corresponding row's paper.Juno scrolls through the table, scanning keywords within the new column.She notices several commonalities at a glance-multiple papers have used LLMas-a-Judge in applications across science, finance, and law.</p>
<p>4.1.2Exploring system-suggested columns.Next, Juno checks out DimInd's system-suggested columns by opening a side panel anchored to the right of the screen, containing a list of up to 20 suggested facets that are tailored to her specific paper collection (Figure 2).These suggestions could serve to guide novice researchers in exploring an unfamiliar collection, while also aiding experts who may find it easier to recognize facets of interest than to recall them from memory.Each suggested facet has a short name in bold and an italicized sentence-long description.While browsing the list, Juno notices several relevant facets she hadn't considered.She clicks the Add button for one labeled Challenges and Limitations, prompting the system to add the faceted column to her literature review table and start extracting relevant information.Columns can be added to the literature review table in two ways: A) User-defined columns precisely specify a faceted information need and allow additional context for steering LLM assistance; B) System-suggested columns offer collection-aware recommendations for columns that can be added with a single click.</p>
<p>Information scent and progressively disclosed details.</p>
<p>As she scans over the new column, one snippet catches her attention: "Like human evaluators, LLMs evaluations are also found to have certain biases." DimInd shows concise snippets in each table cell by default to reduce information overload, opting instead to progressively disclose relevant details on demand.Juno clicks on the snippet in the table, showing a popover with a paragraph of additional detail (evidence summary), which after reading, she finds has satisfied her previous information curiosity.If she wants to read more about these biases in the authors' own words, she can click the See in Paper button to open the paper's PDF in an integrated paper reader within the side panel.When opened this way, the reader also highlights a block of text relevant to the snippet, guiding her attention and offering a useful entry point into the full text for a deeper dive (evidence source).Instead, she decides to click on the title in the Paper column, revealing a popover with the paper's abstract.She skims over the abstract, making a mental note for her future self of how the paper offers a unique angle on biases in LLM-based evaluations.Together, these snippets in the table offer information scent, while the various levels of detail revealed through interaction allow Juno to control her own depth and direction of exploration.</p>
<p>4.1.4Facet-focused sensemaking across documents.The facet columns allowed Juno to quickly extract relevant information across many papers and served as information scent to drill-down and read specific parts of each paper.However, it can still be difficult to go through the extracted values across all 83 papers to identify higher level themes and their distribution.For this, as Juno adds columns to the table, the system automatically organizes extracted information within each facet into a hierarchical facet taxonomy (one per column) (Figure 4).Switching to the synthesis panel, she selects a tab for Application Area, the first column she added, revealing a tree-like taxonomy grouping related snippets under high-level categories.Each category in the taxonomy displays the number of associated papers, and the taxonomy is sorted to place common categories with more papers at the top, allowing her a quick overview of the information landscape.</p>
<p>Juno begins to explore the taxonomy, scanning the overarching categories before expanding specific ones to reveal their subcategories.Selecting a category highlights the corresponding snippets in the table (Figure 5), allowing her to quickly identify which papers are included in each category, ground the category in lower-level representations in the table (i.e., information extracted from each paper), and potentially reason over any gaps in coverage.While most categories appear appropriate, Juno notices two sub-categories in different levels that should be grouped together.She drags one category to the other, merging them under the same parent.As she continues refining the taxonomy, she shapes both the system's organization and her own understanding along this facet.Now satisfied with the structure, she selects the top three categories, comprising the majority of papers in her collection, and clicks the Summarize button, prompting the system to generate a facet synthesis aligned with her refined taxonomy (Figure 5).The synthesis presents a summary generated using only the papers Clicking on a snippet shows a popover with additional detail (evidence summary), with a button that can further open the paper PDF in an integrated paper reader with attributed paragraphs highlighted (evidence source).Faceted columns are transformed into distinct hierarchical taxonomies (facet taxonomy), which can be explored, refined, and used to controllably generate a narrative summary with citations (facet synthesis).from the selected taxonomy categories following the taxonomy structure, with inline citations which can be clicked to reveal citation cards containing the paper's metadata and abstract, allowing a quick, in-situ assessment of relevance (Figure 6).She saves her work by copying the generated summary with references in a single click, exporting it to her note-taking document for future use.</p>
<p>Technical Implementation</p>
<p>In this section, we describe DimInd's computational pipeline, comprising four LLM-enabled components: Facet Discovery ( Â§4.2.1) in which collection-aware comparative dimensions are generated, Value Extraction ( Â§4.2.2) in which faceted information is retrieved from and attributed to papers' full texts, Taxonomy Creation ( Â§4.2.3) in which information within a facet is clustered into emergent themes, and Synthesis ( Â§4.2.4) in which information is organized into a coherent narrative for presentation.</p>
<p>Facet Discovery.</p>
<p>DimInd automatically identifies high-level comparative dimensions (facets) that serve as analytical lenses through which users can jumpstart or deepen their exploration of the literature.To induce these facets, DimInd employs a three-stage process grounded in the paper collection.First, we randomly sample  subsets, each containing  papers from the overall collection.For each subset, we prompt an LLM to generate candidate facets using context formed from combining the titles and abstracts of papers in the subset.Finally, we use an LLM to consolidate these  sets of candidate facets into a cohesive final set, prioritizing facets that appear across multiple subsets and merging semantically similar ones.We empirically chose  = 4 papers per subset-we found larger Figure 5: Selecting specific categories in the facet taxonomy: 1) highlights cells for the included papers in the literature review table, allowing users to quickly delineate between and browse the selected (and not selected) papers; 2) controls the structure and papers included in the generated summary.subsets led the LLM to produce many generic facets in an effort to form dense connections (e.g., "Main contribution" or "Findings") that, while informative, lacked the desired specificity for analysis.Similarly, we found  = 4 subsets sufficient, as more subsets rarely yielded additional unique facets.</p>
<p>Value Extraction.</p>
<p>When a user defines a facet of information they wish to explore (e.g., by creating a new column in the table), we use a retrieval-augmented LLM to generate a paragraph of relevant information from each paper's full text.To avoid overwhelming users as the literature review table grows, we use a second LLM call to distill these dense paragraphs into single sentences.These LLM calls are heavily parallelized to minimize the time users must wait for values to be populated in the table after specifying a column.When users click to view evidence for a snippet attributed to the source PDF, the system encodes both the snippet and full text chunks using a text embedding model (all-MiniLM-L6-v2) and then highlights the chunk with maximal cosine similarity.</p>
<p>4.2.3</p>
<p>Taxonomy Creation.To connect information across papers within a facet, we use an LLM to cluster the generated snippets into a hierarchical taxonomy.This taxonomy creates a nested tree structure with a dynamic depth, adapting to the natural organization of the information (up to a maximum of 5 levels).Each level represents increasingly specific categorizations, with leaf nodes containing the actual information snippets.The LLM is directed to create meaningful categories that avoid broad categories or excessive fragmentation, such that each category aims to have a reasonable number of conceptually similar snippets.Every paper is required to be included in the taxonomy, and information from a single paper may span multiple categories when appropriate.</p>
<p>Synthesis.</p>
<p>The final component supports transformation of the facet-specific hierarchical taxonomy into a coherent narrative, making it more accessible for reuse and sharing.We use an LLM to generate a structured summary that follows the organization of selected branches in the taxonomy, allowing users to steer the narrative based on their specific research interests.The LLM is instructed to ensure their synthesis includes all papers associated with the selected nodes and that any generated claims are explicitly attributed, providing citations immediately after each claim.The result is a comprehensive and tightly attributed summary that aggregates information across papers while clearly tracing information back to its source.</p>
<p>Additional Details.</p>
<p>DimInd is implemented as a web application with Flask (Python) backend and React (Typescript) frontend.Paper collections can be created in the system from a list of titles or Semantic Scholar IDs, or created interactively from a search query, with relevant papers fetched using Semantic Scholar's paper relevance search2 .Given a collection, the system downloads all available open-access PDFs and uses GROBID [34] to parse full text, section data, and token bounding boxes from the PDFs.Semantic Scholar APIs are used for fetching paper metadata, including authors, year, venue, and citation count.Most LLM-enabled components use OpenAI's o3-mini (with "low" reasoning), except for value extraction which uses GPT-4o-mini.The specific models and parameters were selected to balance usable interaction latency and performance for each component.Additional details and LLM prompts are available in Appendix C.</p>
<p>User Study</p>
<p>We conducted a within-subjects user study with 23 researchers to evaluate the effectiveness and usability of DimInd, in contrast to a more conventional workflow involving manual review with conversational LLM assistance.Our evaluation aimed to answer the following questions: RQ1.How does DimInd's workflow of scaffolding literature review with generated structured representations compare to a manual approach assisted by an LLM chat-based baseline?RQ2.How do researchers leverage different structured representations provided by DimInd to make sense of information scattered across large collections of research papers?</p>
<p>Participants</p>
<p>We</p>
<p>Task</p>
<p>We designed our study tasks to closely resemble the initial process of drafting an outline for organizing related literature, e.g., when writing a literature review or survey paper.Given our withinsubjects study design, each participant used both system conditions, but with a different task, as repeating the same task would introduce learning effects.The two tasks were designed to be similar in difficulty and nature, allowing for valid comparisons of system performance across different tasks.For each task, we selected a survey paper in HCI [51,64], extracted the top-level taxonomy explicitly defined in the paper, and sampled 50 papers from the surveyed literature.The collection size of 50 papers ensured consistency across tasks while balancing sufficient material for meaningful organization within the study timeframe.The selected taxonomy dimensions similarly balanced accessibility and depth across the two tasks: one dimension could be reasonably developed from abstracts (Task/Application Domains), while the other required deeper engagement with the full texts (Technology/Limitations &amp; Risks).Participants were given the paper collection and survey topic, then asked to continue filling in the outline for the assigned sections (bolded).Their goal was to create meaningful subsections, save relevant information, and cite appropriate papers.Additional details are provided in Appendix A.2.</p>
<p>Baseline</p>
<p>In the baseline condition (Baseline), participants used a system that displayed a searchable list of paper metadata-including title, authors, year, venue, abstract, and a PDF link.This design emulated the standard experience of browsing conference proceedings or The survey topics and taxonomy dimensions used in the tasks, and their source papers.</p>
<p>results from an academic search engine.To reflect the growing integration of LLM-based tools in scholarly workflows [53], participants were encouraged to use ChatGPT, 3 a commercial LLM chat application, to support their exploration. 4They could interact with the model via text prompts but were restricted from web search (since finding additional papers was beyond the scope of the task).</p>
<p>Procedure</p>
<p>The first author (study facilitator) conducted studies remotely with participants over Google Meet in March 2025.Each study lasted around 90 minutes.The study began with an introduction and overview of the two systems.Participants were first given a brief tutorial of DimInd, and then allowed to use it to explore a sample paper collection involving "interactive tools for scientific literature review." This familiarization period lasted up to ten minutes, during which the study facilitator answered any questions.Then, participants were asked to explore the same collection in the baseline interface.No tutorial was given for ChatGPT since all participants indicated in their screening survey some prior familiarity with LLM-based chat applications.Next, participants worked on the first task with the assigned system for 30 minutes, after which they completed a post-task survey.This process was repeated with the second task and assigned system.The pairing of task and system was fully counterbalanced across participants.After completing both tasks, any remaining time was used as a semi-structured interview in which participants elaborated on and compared their experience using both systems.The study facilitator also probed into any interesting usage behaviors participants exhibited during the study.Participants received a $60 USD gift card upon completion of the study.This study was reviewed and exempted by our organization's internal review board.</p>
<p>Measures and Analyses</p>
<p>For quantitative data, we collected participants' post-task survey responses, including perceived system effectiveness, user control, information transparency, and overall satisfaction with the literature review process using each system (see Appendix A.3 for the specific questions).We also captured and analyzed interaction logs from the sessions to measure the quantity and types of actions participants took (e.g., create a user-defined column, add a system-suggested column, select or refine taxonomy categories, summarize).We did not analyze the content of the outlines participants created during their tasks due to the significant diversity in form and content, complicating an unbiased expert evaluation.Instead, we treated the outline creation as a contextual activity that helped participants engage with the task while using each system, and relied on self-reporting and quantitative measures in the behavioral logs to examine the effectiveness of the two systems.Finally, participants' exit interviews were recorded and automatically transcribed.To analyze all paired Likert-scale data, we conducted Wilcoxon signed-rank tests with Bonferroni corrections for multiple comparisons.For qualitative data, we conducted a reflexive thematic analysis [10], where the first author developed and iteratively refined codes through discussions with the research team to identify emerging themes.</p>
<p>Results</p>
<p>In this section, we present our quantitative and qualitative findings, organized by our two research questions.Participants are referred to with pseudonyms P1-P23.Quotes were lightly edited for brevity and clarity.We denote the median rating of Baseline and DimInd as   and   , respectively, and the Wilcoxon test statistic as W.</p>
<p>Statistical significance was determined at  &lt; .05.</p>
<p>How effectively does DimInd support deeply exploring large collections of papers? (RQ1)</p>
<p>6.1.1Tables with extracted information effectively alleviate foraging costs but risk information overload.Based on the post-task ratings, we found participants appreciated the ability to quickly extract information from many papers at once through the creation of custom and system-suggested columns in DimInd, reporting less perceived mental effort in extracting and organizing relevant information ( = 21.5,  &lt; .01* * ,  = 0.74) with DimInd (  = 6,  = 5 âˆ’ 7) than with Baseline (  = 5,  = 2 âˆ’ 5) (Fig. 7 -Q1).While many participants were able to eventually extract similar information with Baseline with much more effort, they appreciated DimInd's support for easily and explicitly creating custom columns that matched the specific questions they had, and to have all the information organized together within the familiar table representation (P2, P13, P14, P18, P21).Contrastly, without careful prompting the baseline system often generated lots of information to mixed utility.More specifically, when using the baseline system, some participants felt they had to sift through information that "tended to be a bit overly fluffy" (P15), even to the point of irritation as P19 expressed: "These lines are just kind of making me mad. . .'AI can act like the third teammate', it doesn't really tell me anything."While participants appreciated being able to quickly extract relevant information across large collections of documents efficiently, we also observed a different flavor of information overload in Di-mInd.Even when the information presented by the system was seen more useful, presenting it all at once without the users having to do much work can be overwhelming.P19 expressed ambivalent feelings, finding the table "very useful to cut down the manual labor" but also how "having everything ready for me all at once can be overwhelming, since whenever I'm doing this it's usually one row at a time."When reviewing large collections, even a relevant snippet for each paper felt unapproachable to use in drawing connections and organizing ideas across papers, as P4 described, "it's too detailed for me to organize it just by reading through these cells."Rather than directly interacting with the extracted information, P13 viewed the extracted information more as an auxiliary "database to refer back to with all the columns and information" when writing their literature review, and only then selectively investigating cells for papers they intended to discuss.</p>
<p>Instead of trying to consume all the information from the system at once, most participants using the LLM-generated information as information scent [66], where keywords and categorizations served as navigational aids rather than as content directly integrated into a review.As P2 explained, "these keywords are good enough for me to quickly skim through the PDF and find where it might lie," suggesting that the generated information helped guide attention to relevant evidence within papers.For some, these cues served primarily as navigation-further interpretation would necessitate reading "directly from the author's mouth instead of a paraphrased version" (P2).</p>
<p>Overall, these "jumping-off points" (P15) in the extracted evidence snippets facilitated efficient exploration, directing participants to relevant papers while enabling them to browse extracted information for the same facets across multiple papers simultaneously.For example, while P5 was examining a Research Focus column they had added to the table, they came across one snippet that mentioned "potential and risks of LLMs." To see more context, they clicked to open the paper's PDF, and the system scrolled to and highlighted the first paragraph of the discussion.They then briefly continued to browse snippets for other papers in the table, before directing their attention back to reader to skim the discussion section of the paper they had previously opened.</p>
<p>Taxonomies as cognitive scaffolds for thematic exploration.</p>
<p>While participants leveraged the table structure as information scent for the underlying documents, most looked to the taxonomy structure in DimInd for supporting higher-level synthesis.Overall, participants rated DimInd (  = 6,  = 5 âˆ’ 7) higher than Baseline (  = 5,  = 2 âˆ’ 5) for its ability to support paper categorization ( = 0,  &lt; .01* * ,  = 0.88) (Fig. 7 -Q2). 5Many participants attributed this advantage to DimInd's taxonomy representation clearly organizing emergent themes and relevant papers into an interactive hierarchical tree.P23 shared how the transformation from faceted column to taxonomy provided an efficient way to summarize themes across many papers for a facet: "The tree structure was unexpectedly useful as a way to view themes. . .With [Baseline], you would have to compile relevant abstracts, titles, and metadata yourself before feeding it into the LLM.With the table [referring to DimInd as a whole], it was a seamless way to direct information into the models and get thematic summaries."During exploration, the taxonomy structured representations served as external cognitive scaffolds, allowing participants to maintain multiple parallel organizing schemes at once.For example, P13, who had created a table facet about ethical considerations, and compared its cell values that included phrases such as potential misuse of opinionated language models and perpetuation of societal biases and the taxonomy tree that included higher-level themes such as Bias, Transparency, and Societal Harms, explained: "I like how the tree structure gave a very intuitive way of grouping things together, and the way each tab[table facet] has a different tree structure.So it's a different way of organizing your thoughts."This externalization of thought across different 'tabs' each with a unique taxonomy further serves to reduce the cognitive load of multi-faceted analysis.</p>
<p>However, while the taxonomy offered a rapid thematic overview, participants generally felt both Baseline (  = 2,  = 1.5âˆ’4 and DimInd (  = 4,  = 3.5 âˆ’ 5) fell short of helping them discover very specific and meaningful connections (Fig. 7 -Q3), ones they would have been unable to discover without LLM assistance ( = 15.5,  &lt; .05* ,  = 0.78).Combined with the length of the study, we found participants exhibited mixed levels of confidence in the quality and comprehensiveness of the outline they created (Fig. 7 -Q6); while participants reported slightly higher confidence in their outline using DimInd (  = 5,  = 3.5 âˆ’ 6) than Baseline (  = 4,  = 2 âˆ’ 5), this difference was not significant ( = 34.5,  = .08, = 0.66).This is not surprising-while results are promising that DimInd is able to significantly increase efficiency when deeply exploring a large collection of papers, the task of comprehensive literature reviews would still require significant time and effort beyond the scope of our lab study.</p>
<p>In sum, current models still fall short in generating deep intellectual connections between documents, highlighting the continued importance of a researcher-driven analytical process.While our findings are promising with regards to efficiency, designing for longer-term effects on confidence and the impact of prolonged usage remain directions for future work.At the same time, while most prior work focused on support literature understanding using a single representation, our results point to benefits of providing different structured representation, and the importance of allowing users to fluidly "zoom" between different levels of compression, allowing DimInd to scale literature understanding support to large collections of papers.</p>
<p>Desire to blend structured representations and conversation.</p>
<p>While structured representations in DimInd provided valuable scaffolding and more predictable interaction outcomes for the literature review task, participants also valued the adaptability and flexibility offered by conversational interaction in Baseline.We found no significant difference ( = 65.5,  = 1,  = 0.78) between DimInd (  = 6,  = 4 âˆ’ 6) and Baseline (  = 5,  = 3.5 âˆ’ 6) with respect to the ability to maintain control over the literature review process when aided by LLM assistance (Fig. 7 -Q4).Participants saw advantages and limitations of both interaction paradigms, and instead envisioned an ideal system that would blend structural organization with conversational control.For instance, participants appreciated the ability to trace information across structured representations, specifically drilling down into the specific evidence within a paper's PDF and referring back to the paper's abstract within the table and synthesis.As a result, participants reported finding it slightly easier ( = 10.5,  &lt; .05* ,  = 0.81) to verify system-generated information when desired using DimInd (  = 6,  = 5.5 âˆ’ 7) than Baseline (  = 5,  = 3 âˆ’ 6) (Fig. 7 -Q5).At the same time, they also suggested several ways in which DimInd could benefit from conversation, for example, to follow up with clarifying questions (P17), request explanations for certain decisions (e.g., why certain categories were created in the taxonomy and not others) (P15), explore relationships between specific papers (P23), or modify output formats and detail levels (P9).</p>
<p>How do researchers use and navigate across</p>
<p>DimInd's structured representations?(RQ2)</p>
<p>6.2.1 Balanced usage of user-defined and system-suggested facets for creating literature review table.To transform the unstructured paper collection into a literature review table, most participants (14/23) took a balanced approach in adding both user-defined and system-suggested columns (6 added user-defined columns only; 3 added system-suggested columns only).Participants particularly valued the ability to customize the faceted columns based on their specific criteria.As P2 explained, "I really like that I could customize what the criteria for generating an outline could be in that system.Because otherwise just asking an LLM to prompt-it defines its own criteria which might not be how researchers might approach writing [about] the literature."This flexible extraction from many papers at once helped reduce cognitive load, with P6 noting that after defining columns at a high level, their "job is reduced down to only checking whether the information is correct."These results further showed the importance of supporting user-driven exploration, and that LLM generated structures might not always cover the nuanced and personal information needs of different users.</p>
<p>6.2.2 Facet taxonomies as central hubs for exploration.Facet taxonomies emerged as central navigational hubs during exploration.Participants saw them as DimInd's most useful information representation.The majority of participants primarily interacted with the taxonomy during their tasks, expanding and reviewing generated categories and used the taxonomy as thematic filters for papers in the table.</p>
<p>Most participants (19/23) used the taxonomy at least once, with participants expanding and collapsing categories an average of 18.0 times (SD=17.1).Similarly, most participants (20/23) selected categories in the taxonomy, using it to focus their attention on specific paper subsets, an average of 16.3 times (SD=18.1).As P4 noted, the tree structure helped overcome the challenge that specific paper evidence in the table representation could be "too detailed for me to organize just by reading through, " adding that "after filtering through [the taxonomy] then it made more sense for me to look at what appeared."This behavior shows how participants used taxonomies to transform overwhelming tables into manageable views by first selecting relevant categories before examining filtered paper evidence to understand more deeply each generated category, and to confirm their validity.</p>
<p>Moreover, P23 described how DimInd's integrated workflow from specific faceted evidence to taxonomy addressed a challenge of traditional literature review fragmentation where researchers "end up with 10 different documents of scattered information and a really clunky spreadsheet."She frequently navigated between taxonomy categories and specific papers by clicking connections between evidence in the taxonomy and table rows, summarizing: "You're able to scan through a lot of information, you're able to expand things that you want to look at, and then most of the time it's in this very compact form.I found myself using the tree the most, and I really like being able to switch back and forth between looking at the actual text, so the abstract or the whole paper, and then looking at the automatically generated information or summaries." Altogether, these findings suggest that the faceted taxonomies bring valuable high-level structure to literature review, effectively bridging overview and detailed exploration.</p>
<p>6.2.3</p>
<p>Transforming facet taxonomies to narrative summaries.About half of the participants transformed at least one taxonomy into a narrative summary using DimInd (12/23).Five of these participants interacted with the generated summaries, closely reading and inspecting more detail by clicking the inline citations (clicking these reveal a popover with metadata and the abstract for the cited paper) (P3, P6, P7, P9, P17).The other group of participants generated the summary to use as a reasonable artifact they could easily export into their task document.</p>
<p>Seven participants also tried to further improve the LLM-generated taxonomy, using those revisions to inform a more desirable summary.These participants iteratively refined the categories with the drag-and-drop interaction, often to group categories that the system had split but participants believed made more sense to be analyzed together.Some participants similarly expressed a desire to directly manipulate the taxonomy, but refrained from doing so given their time constraints.LLM-generated categories were not perfect, nor were they expected to be-when P20 noticed a misalignment between the LLM-generated taxonomy and her own understanding, e.g, "creative writing" nested under "creative" rather than "writing", she described a simple repair: "Sometimes it did classifications that didn't make a ton of sense.With the writing one, where it separated creative writing from the rest of writing for some reason.But that felt a lot more natural.I can see that and be okay, I think of creative writing as writing.So in my own notes, let me just classify it like that."These findings suggest the utility of LLM-generated categories as an initial guide, with subsequent interactive, in-situ refinement or posthoc refinement when exporting her understanding for presentation.</p>
<p>Despite engaging with the faceted table and taxonomies, participants were hesitant to delegate the final narrative synthesis to DimInd (or other LLM tools) for their own work.This hesitation centered around a desire to preserve researcher agency, particularly in transitioning from the organizational schema of categories and relevant papers to a presentable artifact.In the notional model of sensemaking [67], this suggests that most participants spent their time, and preferred LLM assistance, in the information extraction and schema refinement stages and less in the presentation stage.For instance, P20 explicitly preferred maintaining control over the final synthesis, saying "I would probably not use the summary tool...That feels a little bit too close to writing my paper for me," while recognizing the utility of DimInd in earlier stages.</p>
<p>Participants also shared concerns about automating what they saw as an essential scholarly process for intellectual development: "Literature review is to figure out what are all the keywords that you don't know but convey the same meaning... having the automated process might accidentally limit some of the literature that you should know, " and how over-automation with LLMs could "kill the serendipity findings" that emerge from personal exploration (P19).This suggests a nuanced relationship where researchers value LLM assistance for exploration and organization but still desire preservation of human authorship-a balance between LLM enhancement and maintaining scholarly agency and integrity.</p>
<p>Discussion</p>
<p>Literature review and synthesis at scale is one of the most cognitively and labor-intensive activities that is core to the research process.Recent advancements in AI-particularly LLM capabilities for processing and extracting information from long, complex documents-has led to growing research interest in leveraging these new technologies to support literature review.</p>
<p>While some prior work has sought to automate the entire process [2,46,77], we instead found participants often expressed a desire to retain a scholarly voice and steer the literature review process.Furthermore, user-driven processes can allow scholarly learning, as one participant noted how traditional approaches involving the manual construction of literature review tables can foster deeper engagement with the literature, helping to explore key ideas and possibly discover interesting connections.Similarly, some participants described appreciating the serendipity, curiosity, and intellectual development afforded by user-driven exploration in a literature review.</p>
<p>While participants found value in the generated table and taxonomy structures that provided information scent and broad overviews of many papers, they were more skeptical of the formulaic style of LLM-generated writing and highlighted ambiguity around intellectual ownership of the output, saying: "Working with ChatGPT-yes, the dots are all connected for me, but it just feels very weird to say that this is mine."As a result, they were hesitant to save and share paragraphs directly generated by LLMs in both conditions, citing the potential for a loss of personal voice and unconscious plagiarism.</p>
<p>Compared to prior user-driven literature understanding systems that support exploring smaller sets of papers by only focusing on providing a single structured representation, such as paper pairs [52,60], tables [24,62,82] or taxonomies [41,44,45], our results instead suggest that providing multiple structured representations of paper information and allowing users to fluidly move between these different levels of information abstraction and compression can be effective in scaling a user-driven and LLM-assisted literature review process.</p>
<p>Most commonly, after adding a few columns of faceted information into the table, participants used a top-down review approach, structuring their exploration around the generated taxonomy.And while exploration of a facet often began in the taxonomy by reviewing the available concepts and their distribution in the collection, participants frequently navigated information at multiple levels of detail-for instance, interest in a particular taxonomy category could quickly transition into reading detailed evidence for the specific papers in that category within the table representation-to deepen and verify their understanding.At the same time, participants pointed to a gap between the table and taxonomy structures in DimInd, where deeper connections between small subsets of papers were lacking.This suggests introducing an additional layer of structured representation that leverages prior work can potentially fill this gap.For example, leveraging PaperWeaver [52] or ACCoRD [60] to help users discover specific and nuanced connections between closely related pairs of papers in the table before looking at broader themes in the taxonomy structure.</p>
<p>We also found that leveraging LLMs to increase the efficiency of user-driven literature review highlighted a shift from interaction to cognitive costs, revealing several new design challenges.Specifically, given a particular facet of interest, the laborious manual process of reading and extracting relevant information from each individual paper can be greatly reduced with LLMs, yet the amount and rate at which information is presented to the user can become cognitively overwhelming.In DimInd, we addressed this by presenting multiple levels of abstraction (tables, taxonomy, and narrative synthesis) as cognitive scaffolds, but the design space remains vast.What abstractions or interactive mechanisms best support sensemaking over large volumes of LLM-generated information without overwhelming or impeding serendipitous exploration.And how well do these designs transfer to other domains of information synthesis (e.g., medicine, clinical, or financial decision making) where AI is increasingly being studied and used [23,53]?</p>
<p>A final challenge involves designing to mitigate risks of LLM hallucination at scale.To this end, we designed DimInd with information transparency as a first-class goal, providing clear traces between transformations of paper information and interactive access to a paper's abstract or full text to more easily verify generated information.Still, enabling verification at scale requires more accurate attribution techniques and more seamless interaction mechanisms.For example, DimInd can build on recent advances in fine-grained attribution [17,73,85] to more effectively select and highlight relevant evidence when users drill down into the paper itself, or even proactively flag likely hallucinations for user review [79,80].</p>
<p>Limitations</p>
<p>We note several limitations of our evaluation that shape how our findings should be interpreted.Our within-subjects study was limited to 90-minute sessions to minimize participant fatigue, but literature review is inherently a complex and dynamic process that can take days or months to complete.Similarly, we set the number of papers to be explored in each task to be 50 papers sampled from real-world survey papers.While we believe this is a reasonable number balancing participant effort and scale, survey papers would typically reference more than 100 papers.At the same time, we believe this is a sufficient scale such that our findings may be generalizable to real-world literature review scenarios.We hope to continue to improve DimInd based on participant feedback and conduct a deployment or longitudinal studies where researchers create and explore their own paper collections over extended periods.Such studies could reveal novel usage patterns and limitations of DimInd, and provide deeper insights into the long-term effects of LLM-assisted cognitive scaffolding.Our evaluation also consisted primarily of CS researchers with an HCI focus; additional studies involving more diverse academic disciplines are necessary to establish broader generalizability and utility of LLM-assisted structures in literature review workflows.</p>
<p>Additionally, while all participants had prior experience with conversational LLM assistance (e.g., ChatGPT) for both everyday tasks and research purposes, DimInd's structured approach to LLMenabled features presented a learning curve.Combined with familiarity and potential biases with conversational interfaces, these factors could have influenced participants' expectations and perceptions of the reliability of ChatGPT and our system for the assigned tasks.Finally, our system's interactive information traces with detailed paper information are only available when PDFs are openly accessible.Otherwise, DimInd falls back to paper abstracts, which lack sufficient detail for supporting the system's overall goal of enabling the exploration and analysis of more nuanced paper facets.</p>
<p>Future Work</p>
<p>Based on our findings, we highlight several exciting directions for future work.We plan to release DimInd as an open platform for researchers to further explore these structured representations and continue to improve the underlying LLM mechanisms for facet discovery, taxonomy creation, and synthesis.7.2.1 Enhanced User Controls.Future work could introduce interaction mechanisms that improve steerability of LLM assistance.For instance, participants expressed interest in capabilities that would allow them to directly influence the system's analysis, such as the ability to highlight specific papers or cells of evidence in the table to steer the generated taxonomy and synthesis (P18), hide papers from the analysis, directly edit or tag system-generated evidence (P11, P13, P18), and merge or split faceted columns (P9).These features could better shape the system's output to align with researchers' personalized needs.7.2.2Multi-Faceted Multi-Document Synthesis.Analysis support in DimInd is currently limited to single facets using the taxonomy structure, but participants also expressed interest in analyzing relationships between multiple facets.For example, exploring interactions between table facets about application domain and risks could help discover risks that are understudied in a particular application domain.Future work could explore interactive 2D pivot visualizations-similar to [76] but where the two axes are themes--across two table facets to find correlations between facets, provide intelligent suggestions for which facets to analyze together, or introduce new interactions for simultaneously examining multiple facets in the table at once.</p>
<p>7.2.3</p>
<p>Fine-Grained Literature Discovery.The structured, faceted information in DimInd's literature review table present opportunities for more fine-grained approaches to literature discovery.For instance, users could explore papers that share similar values to those found in a specific table cell, connect to broader conceptual clusters in the taxonomy, or introduce novel values that expand the current facet.</p>
<p>Conclusion</p>
<p>In this paper, we presented DimInd, an LLM-enabled system that scaffolds literature review by transforming unstructured paper content into navigable structured representations, with provenance of LLM-generated information to support paper-level verification.Our evaluation with 23 researchers demonstrated that DimInd provides valuable cognitive support, particularly for extracting faceted information across papers and enabling top-down exploration via conceptual taxonomies.Overall, our findings suggest these structured representations offer an effective way to leverage LLM assistance toward supporting literature review at scale.</p>
<p>Code Statement</p>
<p>Reduce effort to extract information The system helped reduce the mental effort required to extract and organize information.Categorize papers effectively I was able to identify and categorize relevant papers in a meaningful way.</p>
<p>Discover meaningful connections</p>
<p>The system helped me discover connections between papers that I might have missed otherwise.</p>
<p>Maintain control</p>
<p>I maintained appropriate control over the literature review process when using this system.Verify information I believe I could easily verify information provided by the system.Confidence in review quality I feel confident in the quality and comprehensiveness of the outline I created using this system.</p>
<p>Table 3: Post-task survey questions and their corresponding labels (rated on 7-point Likert scale).A user wants to write a literature review for a set of related research papers.The following is a list of contexts from the papers.Your task is to identify facets whose values would likely allow a user to meaningfully compare and contrast across the papers.</p>
<p>Context: {context}</p>
<p>Generate facets that can be used to compare and contrast different aspects of information across the set of papers.Each facet should be a short phrase that can be used to compare information across the papers.For each facet, generate a description in the form of a short question that the facet would help answer.Generate at most {max_facets} facets.The challenge is to find specific facets that are relevant to this set of papers, in addition to generally useful facets for comparing research papers, such as 'Study design' or 'Research questions'.</p>
<p>Keep each facet focused on a SINGLE concept.Do not combine multiple concepts into one facet.For example, instead of "Evaluation Metrics and Results", create separate facets for "Evaluation Metrics" and "Study Results".Instead of "Implications and Future Work", create separate facets for "Research Implications" and "Future Work Directions".</p>
<p>For example, specific facets such as 'User study methodology', 'Number of participants', or 'System design goals' could be relevant for helping a user explore a set of HCI papers.Or with a set of papers on machine learning for information retrieval, potentially informative facets may be 'Number of parameters' or 'Retrieval methods'.</p>
<p>Here Example output: [ {{ "name": "<facet 1>", "description": "<description of facet 1>" }}, {{ "name": "<facet 2>", "description": "<description of facet 2>" }}, {{ "name": "<facet 3>", "description": "<description of facet 3>" }}, ... ]</p>
<p>C.1.2Facet Merge.</p>
<p>The following list contains facets used to analyze research papers.Your task is to: * Identify and consolidate truly duplicate facets (exact matches or synonyms)</p>
<p>Figure 2 :
2
Figure2: Columns can be added to the literature review table in two ways: A) User-defined columns precisely specify a faceted information need and allow additional context for steering LLM assistance; B) System-suggested columns offer collection-aware recommendations for columns that can be added with a single click.</p>
<p>Figure 3 :
3
Figure 3: In DimInd, users review large paper collections by navigating and analyzing information across various structured representations.Each cell in the literature review table is a snippet of faceted information from a paper (evidence snippet).Clicking on a snippet shows a popover with additional detail (evidence summary), with a button that can further open the paper PDF in an integrated paper reader with attributed paragraphs highlighted (evidence source).Faceted columns are transformed into distinct hierarchical taxonomies (facet taxonomy), which can be explored, refined, and used to controllably generate a narrative summary with citations (facet synthesis).</p>
<p>Figure 4 :
4
Figure 4: The facet taxonomy.Each category shows the number of included papers (A).Users can manually refine the taxonomy through drag-and-drop interactions (B) or add additional categories (C).If at least one category is selected, the taxonomy can be summarized into prose (D).</p>
<p>Figure 6 :
6
Figure 6: Users can view additional detail while exploring the synthesized representations: 1) Clicking an evidence snippet in the facet taxonomy shows the full evidence summary; 2) Clicking a citation in the facet synthesis shows an in-situ citation card.From either, users can click See in Table to scroll directly to the corresponding row in the literature review table.</p>
<p>Figure 7 :
7
Figure 7: Participants' post-task ratings (7-point Likert scale) of literature review utility, control, information verifiability, and confidence across DimInd and Baseline.</p>
<p>Figure 8 :
8
Figure 8: Paper collections in DimInd can be created interactively from a research question or search query.</p>
<p>recruited 23 computer science researchers (13 female, 10 male; Age:  = 27,  = 4) via university mailing lists, social media recruitment messages, and snowball sampling.Based on a screening survey, we filtered out those who self-reported no prior research experience or were not at all comfortable with reading abstracts and papers in HCI, since our study involved literature review tasks within this domain.Otherwise, participants were recruited on a first-come, first-served basis.Participants consisted primarily of PhD students (17/23), with most participants having at least 3 to 5 years of academic research experience (17/23).Participants' primary research areas were largely HCI (18/23), with focuses in AR/VR, accessibility, human-AI interaction, and AI ethics, among others.All participants were based in the United States.Most participants (19/23) used LLM-based applications at least weekly for general tasks, with 12 using them daily.Usage of LLMs for research varied, with 14 reporting extensive use of LLMs for research, 6 who used LLMs occasionally, and three who used them rarely or never.Additional participant details are available in Appendix A.1.</p>
<p>Table 1 :
1
Intelligent and interactive writing assistants Task, User, Interaction, Technology, Ecosystem Pang et al. [64] Use of LLMs in HCI research Application Domains, LLM Roles, Limitations &amp; Risks
Survey Paper TopicTaxonomy DimensionsLee et al. [51]</p>
<p>Table 2 :
2
User study participants.YoE refers to years of experience conducting scholarly research.LLM-General refers to how frequently LLM applications are used for everyday tasks.LLM-Research refers to how frequently LLM applications are used for research activities.
IDAgeGender PositionYoE Research AreaLLM-General LLM-ResearchP125-34 ManPhD student3-5GenAI safety and ethicsExtensivelyOccasionallyP225-34 Woman Incoming PhD student3-5HCI, AI, Accessible Computing FrequentlyFrequentlyP318-24 ManUndergraduate student0-2NLP, ReasoningExtensivelyExtensivelyP425-34 Woman PhD student3-5HCI, AI ethicsOccasionallyNeverP525-34 Woman Master's student0-2Data VisualizationFrequentlyFrequentlyP635-44 ManPostdoctoral Researcher 3-5CybersecurityExtensivelyFrequentlyP725-34 ManPhD student3-5Security and PrivacyExtensivelyFrequentlyP818-24 ManUndergraduate student0-2HCI, CSCW, AR/VR, AIMCExtensivelyOccasionallyP925-34 ManPhD student6-10 HCI, Human-AIExtensivelyExtensivelyP10 25-34 Woman PhD student3-5HCI, Health TrackingFrequentlyOccasionallyP11 18-24 Woman PhD student0-2HCI, AR/VR applicationsExtensivelyFrequentlyP12 25-34 Woman Master's student0-2HCI, Human-AIExtensivelyOccasionallyP13 25-34 Woman PhD student3-5HCIRarelyRarelyP14 25-34 ManPhD student10+HCI, MetascienceFrequentlyFrequentlyP15 25-34 Woman PhD student6-10 HCI, LLM/HealthFrequentlyFrequentlyP16 25-34 ManPhD student3-5HCI, AIOccasionallyOccasionallyP17 18-24 Woman Undergraduate student0-2HCI, MLExtensivelyOccasionallyP18 25-34 ManPhD student3-5HCI, Social ComputingExtensivelyExtensivelyP19 25-34 Woman PhD student3-5HCI, Usable SecurityOccasionallyRarelyP20 18-24 Woman Incoming PhD student3-5HCI, AI, AccessibilityFrequentlyExtensivelyP21 25-34 Woman PhD student3-5HCIExtensivelyFrequentlyP22 25-34 ManPhD student3-5HCI, ML, UbiCompExtensivelyExtensivelyP23 25-34 Woman PhD student3-5HCI, Emotion &amp; WellbeingFrequentlyFrequently</p>
<p>are some other examples of facets: Intervention effects, Study design, Study objectives, Theoretical framework, Research questions, Dataset characteristics, Study count, Study duration, Statistical techniques, Algorithm type, Software tools, Participant demographics, Policy recommendations, Design goals, Research limitations, Ethical considerations, etc.Return a single valid JSON list (without code block) containing objects with the name and description of each facet.Do not return any other text.</p>
<p>https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_ graph_paper_relevance_search
https://chatgpt.com/
During the study, participants were provided with login credentials for a ChatGPT account created exclusively for this user study. This served two purposes: to standardize the version of ChatGPT each participant used and to allow us to export detailed logs of participants' prompts and LLM responses.
ð‘Š = 0 indicates all participants rated DimInd's support for paper categorization greater than or equal to that of Baseline.
A User Study Details A.1 Demographic Details of User Study ParticipantsTable2lists demographic details for all 23 participants included in our user study, including their research area, years of research experience, and use of LLMs for general and research tasks.A.2 Additional Task DetailsA.2.1 Task Scenario and Instructions.The following hypothetical survey paper writing scenario was presented to participants in each task, informing their exploration over the collection of 50 papers.Imagine that you are part of a research team planning to write a survey paper reviewing recent research on the following topic: <topic>.You have conducted an initial search and found a set of possibly relevant papers.Today, your goal is to review this set of papers to get a broad understanding of this topic.The research team has sketched an initial outline and assigned you with two top-level sections to explore and add detail to.Using the provided papers, you could:â€¢ Create one or more subsections that would be relevant to include in each section.â€¢ Note down specific details, e.g., terms, concepts, or perspectives, relevant to each section.â€¢ Cite specific papers relevant to each section, e.g., with title or author+year.You will have up to 30 minutes to work on the outline.You are not expected to fully complete the outline or consider every paper-try to do as much as you can in the given time.A.2.2 Creating the Sampled Paper Collections.For each survey paper, we retrieved the full list of references using the Semantic Scholar API.Papers without open-access PDFs were excluded.To standardize the collection size, each set was limited to the 50 most recent papers-a size for which cognitive challenges of large-scale review could be observed, while still being relatively manageable with current LLM tools for creating an initial review outline.A.3 Post-Task Survey QuestionsTable3lists the post-task survey questions (7-point Likert scale) participants completed after system condition in our user evaluation.B Creating Paper CollectionsPaper collections in DimInd can be interactively initialized, e.g., from a research question or search query (Figure8).Finding relevant papers is a critical yet challenging part of the literature review workflow, and one that we considered out of scope for the particular exploration of DimInd in this paper.Use the provided paper context to retrieve information relevant to the specified list of facets.Each facet has a number id, a "Name", and a "Description" (optional).When provided, the "Description" key provides context (e.g., additional instructions or example output formats) for the information expected to be extracted for that facet.<em> For each facet, generate a paragraph of detailed, accurate, and relevant information (typically between 3 to 5 sentences) by synthesizing relevant information from the provided paper context.</em> Use a passive or third-person voice when summarizing information for an facet.For instance, avoid using phrases such as "we", "our approach", etc. * If there is no relevant information for an facet in the provided context, return null for that facet value.Do not return any other text and do not make up an answer unsupported by the paper context.Your output should contain a list of objects.Each object should have: * 'facet_id': The id of the facet (as provided in the input).<em> 'value': The information extracted for that facet, or null if no relevant information is found.The output should contain exactly as many objects as the facets provided in the input.Paper Context: {context}Facets: {facets}Output format: Do not include any explanations, only provide a valid JSON response (without code block).For example, if you are provided with 3 facets, the output should be in the following format: [ {{ "facet_id": 1, "value": "<information for facet 1>" }}, {{ "facet_id": 2, "value": "<information for facet 2>" }}, {{ "facet_id": 3, "value": null }} ]C.2.2 Value Distillation.Summarize information related to the following facet of research papers: {facet} (description (optional): {facet_description}) from research paper excerpts.For each excerpt, return exactly one sentence of clear and concise information about this specific facet.The goal is for your summary to allow a user to more quickly understand {facet}, e.g., during an initial exploratory phase of literature review, and return to the original, longer excerpt later if they desire additional detail.Guidelines for your summaries: * Focus only on information directly related to {facet} Input paper snippets: {snippets} Output format: The output must be a valid JSON object (wihtout code block), where: * Keys represent categories and subcategories.</em> Values are either subcategories or arrays of snippet indices.<em> Do not include additional keys (e.g., "indices", "description", "items").</em> Do not include any explanation, preamble, or additional formatting.Example:If the input snippets were about "performance metrics", a good hierarchy could be: {{ "Model Performance": {{ "Text Processing": {{ "Classification": {{ "Accuracy Metrics": [0, 3] }}, "Named Entity Recognition": {{ "Precision Metrics":[1]}}, "Translation": {{ "Error Analysis":[2]}} }} }}, "System Efficiency": {{ "Speed": {{ "Response Time":[4,6], "Processing Throughput":[7]}}, "Resource Utilization": {{ "Computational Resources":[5], "Memory Usage":[8], "GPU Performance":[9]}} }}, "User Experience": {{ "Satisfaction":[10,12], "Reliability":[11,13]}} }}C.3.2 Summarization.Transform the following taxonomy of organized information snippets from different research papers into a clear and concise summary that captures the key points related to the given facet.Your synthesis should be formatted as a valid JSON object with a single key "summary_blocks" containing an array of objects, each with "header" and "content" keys.Facet: {facet_name_and_description}Taxonomy Structure and Paper Excerpts: Excerpts are extracted from various research papers and provides information relevant to a more detailed aspect of the facet above: {excerpts} Papers to Highlight: When provided, the following papers should be highlighted in your summary.Incorporate them smoothly: {starred_papers} Additional Instructions: * Structure your response as a valid JSON dictionary with exactly one key "summary_blocks" containing an array of objects.<em> Each object in the array should have two keys: "header" (corresponding to a top-level category from the taxonomy structure) and "content" (containing your synthesized text for that category).</em> Headers should match the top-level structure provided in the taxonomy.<em> Ensure all papers from the taxonomy are included in your summary across the different blocks.</em> Your content should be primarily descriptive.Do not include introductory or concluding sentences.Do not generate statements that connect snippets in meaningless ways.<em> Synthesize the key details into cohesive and insightful content blocks, covering as much of the provided information as possible.</em> Use the provided hierarchical structure as a guide for organizing your summary blocks.<em> Break your content into paragraphs when appropriate to improve readability.</em> Prioritize the papers listed in the "Papers to highlight" section if any are listed, by ensuring they are incorporated into the summary.Make sure to incorporate them smoothly (e.g., by including more detail about those papers, but don't explicitly say phrases like "the highlighted papers examine..." or "the featured works on ...") * Your entire summary across all blocks should be {length_constraint}.<em> Remember that these snippets you are summarizing come from different papers.Avoid language like "This paper" or "This system", since the summary is synthesizing across multiple papers.Citing Sources: * Your summary MUST cite all unique papers in the extracted snippets at least once, to the extent possible.</em> Your citations MUST use the exact paperId format provided in the extracted snippets.The paper id could be any string (e.g., "12345", "CorpusId:12345", "URL:12345", etc.), and you should cite using the exact and entire paper id.* Place citations [[paperId]] immediately after specific words, phrases, or concepts they support -not just at the end of sentences.For example: "The study found increased levels of protein X [paperId6]], citations should follow each item they support rather than appearing only at the end of sentences."}}, {{ "header": "Second Top-Level Category", "content": "More synthesized content with appropriate citations[[paperId7,paperId8]]."}} ] }}
Topic Significance Ranking of LDA Generative Models. Loulwah Alsumait, Daniel BarbarÃ¡, James Gentle, Carlotta Domeniconi, Machine Learning and Knowledge Discovery in Databases. Berlin Heidelberg; Berlin, HeidelbergSpringer2009</p>
<p>Automatic summarization of scientific articles: A survey. 10.1016/j.jksuci.2020.04.020Journal of King Saud University -Computer and Information Sciences. Nouf Ibrahim Altmami and Mohamed El Bachir Menai3442022. 2022</p>
<p>Lorin W Anderson, David R Krathwohl, Peter W Airasian, Kathleen A Cruikshank, Richard E Mayer, Paul R Pintrich, James D Raths, Merlin C Wittrock, A Taxonomy for Learning, Teaching, and Assessing: A Revision of Bloom's Taxonomy of Educational Objectives. New YorkLongman2001</p>
<p>Paper Plain: Making Medical Research Papers Approachable to Healthcare Consumers with Natural Language Processing. Tal August, Lucy Lu Wang, Jonathan Bragg, Marti A Hearst, Andrew Head, Kyle Lo, 10.1145/3589955ACM Trans. Comput.-Hum. Interact. 30pages2023. Sept. 2023</p>
<p>Catarina G Belem, Pouya Pezeskhpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka, arXiv:2410.13961[cs.CL]From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization. 2024</p>
<p>Latent dirichlet allocation. Andrew Y David M Blei, Michael I Ng, Jordan, Journal of machine Learning research. 32003. Jan (2003</p>
<p>Artificial intelligence for literature reviews: Opportunities and challenges. Francisco Bolanos, Angelo Salatino, Francesco Osborne, Enrico Motta, Artificial Intelligence Review. 572592024. 2024</p>
<p>Analysis of the time and workers needed to conduct systematic reviews of medical interventions using data from the PROSPERO registry. Rohit Borah, Andrew W Brown, Patrice L Capers, Kathryn A Kaiser, BMJ open. 72e0125452017. 2017</p>
<p>Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references. Lutz Bornmann, RÃ¼diger Mutz, Journal of the association for information science and technology. 662015. 2015</p>
<p>Using thematic analysis in psychology. Virginia Braun, Victoria Clarke, 10.1191/1478088706qp063oaQualitative Research in Psychology. 322006. 2006</p>
<p>Cochrane methods -twenty years experience in developing systematic review methods. Jackie Chandler, Sally Hopewell, Systematic reviews. 22013. 2013</p>
<p>Reading tea leaves: how humans interpret topic models. Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, David M Blei, Proceedings of the 23rd International Conference on Neural Information Processing Systems. the 23rd International Conference on Neural Information Processing SystemsVancouver, British Columbia, Canada; Red Hook, NY, USACurran Associates Inc2009Nips'09)</p>
<p>Mesh: Scaffolding Comparison Tables for Online Decision Making. Joseph Chee, Chang , Nathan Hahn, Aniket Kittur, 10.1145/3379337.3415865Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology. the 33rd Annual ACM Symposium on User Interface Software and TechnologyAcm2020</p>
<p>Search-Lens: composing and capturing complex user interests for exploratory search. Joseph Chee, Chang , Nathan Hahn, Adam Perer, Aniket Kittur, 10.1145/3301275.3302321Proceedings of the 24th International Conference on Intelligent User Interfaces. the 24th International Conference on Intelligent User InterfacesMarina del Ray CaliforniaAcm2019</p>
<p>CiteSee: Augmenting Citations in Scientific Papers with Persistent and Personalized Historical Context. Joseph Chee Chang, Amy X Zhang, Jonathan Bragg, Andrew Head, Kyle Lo, Doug Downey, Daniel S Weld, 10.1145/3544548.3580847Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23). the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23)New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Challenges confronting beginning researchers in conducting literature reviews. Studies in Continuing Education. Yu-Mei Der-Thanq ; Chen, Wei Wang, Lee Ching, 10.1080/0158037X.2015.10303352016. 201638</p>
<p>SelfCite: Self-Supervised Alignment for Context Attribution in Large Language Models. Yung-Sung Chuang, Benjamin Cohen-Wang, Shannon Zejiang Shen, Zhaofeng Wu, Hu Xu, Xi Victoria Lin, James Glass, Shang-Wen, Wen Li, Yih Tau, arXiv:2502.09604[cs.CL]2025</p>
<p>Research synthesis and meta-analysis: A step-by-step approach. Harris Cooper, Applied Social Research Methods Series. 22015Sage Publications5 ed.</p>
<p>Grounded theory research: Procedures, canons, and evaluative criteria. Juliet M , Corbin , Anselm Strauss, Qualitative sociology. 131990. 1990</p>
<p>Common challenges postgraduate students and early-career academics face when engaging with the scholarly literature. Ben Daniel, Electronic Journal of Business Research Methods. 202022. 2022</p>
<p>Do Multi-Document Summarization Models Synthesize?. Jay Deyoung, Stephanie C Martinez, Iain J Marshall, Byron C Wallace, 10.1162/tacl_a_00687Transactions of the Association for Computational Linguistics. 122024. 2024</p>
<p>Topic Model or Topic Twaddle? Reevaluating Semantic Interpretability Measures. Caitlin Doogan, Wray Buntine, 10.18653/v1/2021.naacl-main.300Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline2021</p>
<p>Eamon Duede, William Dolan, AndrÃ© Bauer, Ian Foster, Karim Lakhani, arXiv:2405.15828[cs.DL]Oil &amp; Water? Diffusion of AI Within and Across Scientific Fields. 2024</p>
<p>A Summarization System for Scientific Documents. Shai Erera, Michal Shmueli-Scheuer, Guy Feigenblat, Ora Peled Nakash, Odellia Boni, Haggai Roitman, Doron Cohen, Bar Weiner, Yosi Mass, Or Rivlin, Guy Lev, Achiya Jerbi, Jonathan Herzig, Yufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, Francesca Bonin, David Konopnicki, 10.18653/v1/D19-3036Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System DemonstrationsHong Kong, ChinaAssociation for Computational Linguistics2023. 2019Elicit: The AI Research Assistant</p>
<p>Jessica L Feuston, Jed R Brubaker, 10.1145/3479856Putting Tools in Their Place: The Role of Time and Perspective in Human-AI Collaboration for Qualitative Analysis. Proc. ACM Hum.-Comput. Interact. 5, Cscw2, Article 469. 2021. Oct. 202125pages</p>
<p>Scim: Intelligent Skimming Support for Scientific Papers. Raymond Fok, Hita Kambhamettu, Luca Soldaini, Jonathan Bragg, Kyle Lo, Marti Hearst, Andrew Head, Daniel S Weld, 10.1145/3581641.3584034Proceedings of the 28th International Conference on Intelligent User Interfaces (Iui '23). the 28th International Conference on Intelligent User Interfaces (Iui '23)New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Marco: Supporting Business Document Workflows via Collection-Centric Information Foraging with Large Language Models. Raymond Fok, Nedim Lipka, Tong Sun, Alexa F Siu, 10.1145/3613904.3641969Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing SystemsHonolulu, HI, USA; New York, NY, USAAssociation for Computing Machinery2024842CHI '24)</p>
<p>Toward Living Narrative Reviews: An Empirical Study of the Processes and Challenges in Updating Survey Articles in Computing Research. Raymond Fok, Alexa F Siu, Daniel S Weld, Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. the 2025 CHI Conference on Human Factors in Computing SystemsYokohama, Japan; New York, NY, USAAssociation for Computing Machinery2025842</p>
<p>CoAIcoder: Examining the Effectiveness of AI-assisted Human-to-Human Collaboration in Qualitative Analysis. Jie Gao, Kenny Tsu, Wei Choo, Junming Cao, Roy , Ka-Wei Lee, Simon Perrault, 10.1145/3617362ACM Trans. Comput.-Hum. Interact. 316pages2023. Nov. 2023</p>
<p>Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers. Charlie George, Andreas Stuhlmueller, 10.18653/v1/2023.wiesp-1.13Proceedings of the Second Workshop on Information Extraction from Scientific Publications. the Second Workshop on Information Extraction from Scientific PublicationsBali, IndonesiaAssociation for Computational Linguistics2023</p>
<p>Promoting cognitive complexity in graduate written work: Using Bloom's taxonomy as a pedagogical tool to improve literature reviews. Darcy Haag, Granello , Counselor Education and Supervision. 402001. 2001</p>
<p>Finding scientific topics. Thomas L Griffiths, Mark Steyvers, 10.1073/pnas.0307752101Proceedings of the National Academy of Sciences. 1012004. 2004</p>
<p>. Grobid, 2008-2025</p>
<p>Passages: Interacting with Text Across Documents. Han L Han, Junhang Yu, Raphael Bournet, Alexandre Ciorascu, Wendy E Mackay, Michel Beaudouin-Lafon, 10.1145/3491102.3502052Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing SystemsNew Orleans, LA, USA; New York, NY, USAAssociation for Computing Machinery2022338CHI '22)</p>
<p>Automatic Generation of Review Matrices as Multi-document Summarization of Scientific Papers. Hayato Hashimoto, Kazutoshi Shinoda, Hikaru Yokono, Akiko Aizawa, Birndl @ Sigir. New York, NY, USAAssociation for Computing Machinery2017</p>
<p>Augmenting Scientific Papers with Just-in-Time, Position-Sensitive Definitions of Terms and Symbols. Andrew Head, Kyle Lo, Dongyeop Kang, Raymond Fok, Sam Skjonsberg, Daniel S Weld, Marti A Hearst, 10.1145/3411764.3445648Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21). the 2021 CHI Conference on Human Factors in Computing Systems (CHI '21)New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Scholastic: Graphical Human-AI Collaboration for Inductive and Interpretive Text Analysis. Matt-Heun Hong, Lauren A Marsh, Jessica L Feuston, Janet Ruppert, Jed R Brubaker, Danielle Albers Szafir, 10.1145/3526113.3545681Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and TechnologyBend, OR, USA; New York, NY, USAAssociation for Computing Machinery202230UIST '22)</p>
<p>Principles of mixed-initiative user interfaces. Eric Horvitz, 10.1145/302979.303030Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. the SIGCHI Conference on Human Factors in Computing SystemsPittsburgh, Pennsylvania, USA; New York, NY, USAAssociation for Computing Machinery1999CHI '99)</p>
<p>Are Neural Topic Models Broken?. Alexander Miserlis Hoyle, Pranav Goel, Rupak Sarkar, Philip Resnik, 10.18653/v1/2022.findings-emnlp.390Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>CHIME: LLM-Assisted Hierarchical Organization of Scientific Studies for Literature Review Support. Chao-Chun, Erin Hsu, Jenna Bransom, Bailey Sparks, Chenhao Kuehl, David Tan, Lucy Wadden, Aakanksha Wang, Naik, 10.18653/v1/2024.findings-acl.8Findings of the Association for Computational Linguistics: ACL 2024. Association for Computational Linguistics. Bangkok, Thailand2024</p>
<p>Aaron Jialun, Kandrea Jiang, Casey Wade, Jed R Fiesler, Brubaker, 10.1145/3449168Supporting Serendipity: Opportunities and Challenges for Human-AI Collaboration in Qualitative Analysis. Proc. ACM Hum.-Comput. Interact. 5, Cscw1, Article 94. 2021. April 202123pages</p>
<p>Article 50 million: an estimate of the number of scholarly articles in existence. E Arif, Jinha, Learned publishing. 232010. 2010</p>
<p>Threddy: An Interactive System for Personalized Thread-based Exploration and Organization of Scientific Literature. Hyeonsu Kang, Joseph Chee Chang, Yongsung Kim, Aniket Kittur, 10.1145/3526113.3545660Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22). the 35th Annual ACM Symposium on User Interface Software and Technology (UIST '22)New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Synergi: A Mixed-Initiative System for Scholarly Synthesis and Sensemaking. Tongshuang Hyeonsu B Kang, Joseph Chee Wu, Aniket Chang, Kittur, 10.1145/3586183.3606759Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23). the 36th Annual ACM Symposium on User Interface Software and Technology (UIST '23)New York, NY, USAAssociation for Computing Machinery2023</p>
<p>SciRe-viewGen: A Large-scale Dataset for Automatic Literature Review Generation. Tetsu Kasanishi, Masaru Isonuma, Junichiro Mori, Ichiro Sakata, 10.18653/v1/2023.findings-acl.418Findings of the Association for Computational Linguistics: ACL 2023. Naoaki Boyd-Graber, Okazaki, Anna Rogers, Jordan; Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>Challenges of evidence synthesis during the 2020 COVID pandemic: a scoping review. Hanan Khalil, Lotfi Tamara, Gabriel Rada, Elie A Akl, 10.1016/j.jclinepi.2021.10.017Journal of Clinical Epidemiology. 1422022. 2022</p>
<p>Papeos: Augmenting Research Papers with Talk Videos. Tae Soo, Kim , Matt Latzke, Jonathan Bragg, Amy X Zhang, Joseph Chee, Chang , 10.1145/3586183.3606770Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and TechnologySan Francisco, CA, USA; New York, NY, USAAssociation for Computing Machinery2023UIST '23). Article 15, 19 pages</p>
<p>Enslaved to the Trapped Data: A Cognitive Work Analysis of Medical Systematic Reviews. Ian A Knight, Max L Wilson, David F Brailsford, Natasa Milic-Frayling, 10.1145/3295750.3298937Proceedings of the 2019 Conference on Human Information Interaction and Retrieval. the 2019 Conference on Human Information Interaction and RetrievalGlasgow, Scotland UK; New York, NY, USAAssociation for Computing Machinery2019Chiir '19)</p>
<p>Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM. Michelle S Lam, Janice Teoh, James A Landay, Jeffrey Heer, Michael S Bernstein, 10.1145/3613904.3642830Proceedings of the CHI Conference on Human Factors in Computing Systems. the CHI Conference on Human Factors in Computing SystemsHonolulu, HI, USA; New York, NY, USAAssociation for Computing Machinery2024766CHI '24)</p>
<p>Eugenia Ha Rim Rho, Zejiang Shen, and Pao Siangliulue. 2024. A Design Space for Intelligent and Interactive Writing Assistants. Mina Lee, Katy Ilonka Gero, John Joon, Young Chung, Simon Buckingham Shum, Vipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David Zhou, Emad A Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi, Senjuti Dutta, Jin L C Guo, Naimul Md, Yewon Hoque, Simon Kim, Seyed Knight, Antonette Parsa Neshaei, Disha Shibani, Lila Shrivastava, Agnia Shroff, Jessi Sergeyuk, Sarah Stark, Sitong Sterman, Antoine Wang, Daniel Bosselut, Joseph Chee Buschek, Sherol Chang, Max Chen, Joonsuk Kreminski, Roy Park, Pea, 10.1145/3613904.3642697Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing SystemsHonolulu, HI, USA; New York, NY, USAAssociation for Computing MachineryArticle 1054, 35 pages</p>
<p>PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers. Yoonjoo Lee, B Hyeonsu, Matt Kang, Juho Latzke, Jonathan Kim, Joseph Chee Bragg, Pao Chang, Siangliulue, 10.1145/3613904.3642196Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing SystemsHonolulu, HI, USA; New York, NY, USA, ArticleAssociation for Computing Machinery2024. 1919CHI '24)</p>
<p>Zhehui Liao, Maria Antoniak, Inyoung Cheong, Evie Yu-Yen, Ai-Heng Cheng, Kyle Lee, Joseph Chee Lo, Amy X Chang, Zhang, arXiv:2411.05025[cs.CL]LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions. 2024</p>
<p>Unakite: Scaffolding Developers' Decision-Making Using the Web. Xieyang Michael, Jane Liu, Nathan Hsieh, Angelina Hahn, Emily Zhou, Shaun Deng, Cynthia Burley, Aniket Taylor, Brad A Kittur, Myers, 10.1145/3332165.3347908Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology. the 32nd Annual ACM Symposium on User Interface Software and TechnologyNew Orleans LA USAAcm2019</p>
<p>Crystalline: Lowering the Cost for Developers to Collect and Organize Information for Decision Making. Xieyang Michael, Aniket Liu, Brad A Kittur, Myers, 10.1145/3491102.3501968CHI Conference on Human Factors in Computing Systems. New Orleans LA USAAcm2022</p>
<p>Multi-XScience: A Large-scale Dataset for Extreme Multi-document Summarization of Scientific Articles. Yao Lu, Yue Dong, Laurent Charlin, 10.18653/v1/2020.emnlp-main.648Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online2020</p>
<p>Shallow Synthesis of Knowledge in GPT-Generated Texts: A Case Study in Automatic Related Work Composition. Anna Martin-Boyle, Aahan Tyagi, Marti A Hearst, Dongyeop Kang, arXiv:2402.12255[cs.CL]2024</p>
<p>The significant cost of systematic reviews and meta-analyses: A call for greater involvement of machine learning to assess the promise of clinical trials. Matthew Michelson, Katja Reuter, 10.1016/j.conctc.2019.100443Contemporary Clinical Trials Communications. 161004432019. 2019</p>
<p>. Eliot Moss, 2021</p>
<p>ACCoRD: A Multi-Document Approach to Generating Diverse Descriptions of Scientific Concepts. Sonia Murthy, Kyle Lo, Daniel King, Chandra Bhagavatula, Bailey Kuehl, Sophie Johnson, Jonathan Borchardt, Daniel Weld, Tom Hope, Doug Downey, 10.18653/v1/2022.emnlp-demos.20Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2022 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsAbu Dhabi, UAEAssociation for Computational Linguistics2022</p>
<p>VI-TALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics. Arpit Narechania, Alireza Karduni, Ryan Wesslen, Emily Wall, 10.1109/tvcg.2021.3114820IEEE Transactions on Visualization and Computer Graphics. 2812022. Jan. 2022</p>
<p>Arx-ivDIGESTables: Synthesizing Scientific Literature into Tables using Language Models. Benjamin Newman, Yoonjoo Lee, Aakanksha Naik, Pao Siangliulue, Raymond Fok, Juho Kim, Joseph Chee Daniel S Weld, Kyle Chang, Lo, 10.18653/v1/2024.emnlp-main.538Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, Florida, USAAssociation for Computational Linguistics2024</p>
<p>Relatedly: Scaffolding Literature Reviews with Existing Related Work Sections. Srishti Palani, Aakanksha Naik, Doug Downey, Amy X Zhang, Jonathan Bragg, Joseph Chee, Chang , 10.1145/3544548.3580841Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing SystemsHamburg GermanyAcm2023</p>
<p>Hope Rock Yuren Pang, Kynnedy Simone Schroeder, Solon Smith, Ziang Barocas, Emily Xiao, Danielle Tseng, Bragg, arXiv:2501.12557[cs.HC]Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review. 2025</p>
<p>Minh Chau, Alexander Pham, Simeng Hoyle, Philip Sun, Mohit Resnik, Iyyer, arXiv:2311.01449[cs.CL]TopicGPT: A Prompt-based Topic Modeling Framework. 2024</p>
<p>Information foraging. Peter Pirolli, Stuart Card, Psychological review. 1066431999. 1999</p>
<p>The sensemaking process and leverage points for analyst technology as identified through cognitive task analysis. Peter Pirolli, Stuart Card, Proceedings of International Conference on Intelligence Analysis. International Conference on Intelligence Analysis2005. 20055</p>
<p>Are ChatGPT and large language models "the answer" to bringing us closer to systematic review automation?. Riaz Qureshi, Daniel Shaughnessy, A R Kayden, Karen A Gill, Tianjing Robinson, Eitan Li, Agai, Systematic Reviews. 12722023. 2023</p>
<p>CiteRead: Integrating Localized Citation Contexts into Scientific Paper Reading. Napol Rachatasumrit, Jonathan Bragg, Amy X Zhang, Daniel S Weld, 10.1145/3490099.351116227th International Conference on Intelligent User Interfaces (Iui '22). New York, NY, USAAssociation for Computing Machinery2022</p>
<p>Cody: An AI-Based System to Semi-Automate Coding for Qualitative Research. Tim Rietz, Alexander Maedche, 10.1145/3411764.3445591Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing SystemsYokohama, Japan; New York, NY, USA, ArticleAssociation for Computing Machinery2021394CHI '21)</p>
<p>The cost structure of sensemaking. M Daniel, Mark J Russell, Peter Stefik, Stuart K Pirolli, Card, 10.1145/169059.169209Proceedings of the INTERACT '93 and CHI '93 Conference on Human Factors in Computing Systems (CHI '93). the INTERACT '93 and CHI '93 Conference on Human Factors in Computing Systems (CHI '93)New York, NY, USAAssociation for Computing Machinery1993</p>
<p>How quickly do systematic reviews go out of date? A survival analysis. Margaret Kaveh G Shojania, Mohammed T Sampson, Jun Ansari, Steve Ji, David Doucette, Moher, Annals of internal medicine. 1472007. 2007</p>
<p>Attribute First, then Generate: Locally-attributable Grounded Text Generation. Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan, 10.18653/v1/2024.acl-long.182Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Literature review as a research methodology: An overview and guidelines. Hannah Snyder, 10.1016/j.jbusres.2019.07.039Journal of Business Research. 1042019. 2019</p>
<p>FOCUS: the interactive table for product comparison and selection. Michael Spenke, Christian Beilken, Thomas Berlage, 10.1145/237091.237097Proceedings of the 9th annual ACM symposium on User interface software and technology (UIST '96). the 9th annual ACM symposium on User interface software and technology (UIST '96)New York, NY, USAAssociation for Computing Machinery1996</p>
<p>Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation. Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun, Haijun Li, Xia, 10.1145/3613904.3642400Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. the 2024 CHI Conference on Human Factors in Computing SystemsHonolulu, HI, USA; New York, NY, USAAssociation for Computing Machinery2024CHI '24). Article 644, 26 pages</p>
<p>Automating Research Synthesis with Domain-Specific Large Language Model Fine-Tuning. Teo Susnjak, Peter Hwang, Napoleon Reyes, L C Andre, Timothy Barczak, Surangika Mcintosh, Ranathunga, 10.1145/3715964ACM Trans. Knowl. Discov. Data. 1968pages2025. March 2025</p>
<p>Living systematic reviews: 2. Combining human and machine effort. James Thomas, Anna Noel-Storr, Iain Marshall, Byron Wallace, Steven Mcdonald, Chris Mavergames, Paul Glasziou, Ian Shemilt, Anneliese Synnot, Tari Turner, Julian Elliott, Thomas Agoritsas, John Hilton, Caroline Perron, Elie Akl, Rebecca Hodder, Charlotte Pestridge, Lauren Albrecht, Tanya Horsley, Joanne Platt, Rebecca Armstrong, Hung Phi, Robert Nguyen, Anneliese Plovnick, Noah Arno, Gail Ivers, Agnes Quinn, Renea Au, Gabriel Johnston, Matthew Rada, Arwel Bagg, Philippe Jones, Catherine Ravaud, Lara Boden, Bernt Kahale, Isabelle Richter, Homa Boisvert, Rebecca Keshavarz, Linn Ryan, Stephanie A Brandt, Dina Kolakowsky-Hayner, Alexandra Salama, Brazinova, Kumbargere Sumanth, Georgia Nagraj, Rachelle Salanti, Toby Buchbinder, Lina Lasserson, Chris Santaguida, Rebecca Champion, Nancy Lawrence, Jackie Santesso, Zbigniew Chandler, Les, J Holger, Andreas SchÃ¼nemann, Stefan Charidimou, Ian Leucht, Roger Shemilt, Nicola Chou, Diana Low, Rachel Sherifali, Andrew Churchill, Reed Maas, Maryse C Siemieniuk, Harriet Cnossen, Mark Maclehose, Marie-Joelle Simmonds, Malcolm Cossi, Nicole Macleod, Michel Skoetz, Iain Counotte, Karla Marshall, Samantha Soares-Weiser, Rachel Craigie, Velandai Marshall, Philipp Srikanth, Nicole Dahm, Katrina Martin, Alanna Sullivan, Laura MartÃ­nez Danilkewich, Anneliese GarcÃ­a, Kristen Synnot, Chris Danko, Mark Mavergames, Emma Taylor, Lara J Donoghue, Kris Maxwell, Corinna Thayer, James Dressler, James Mcauley, Cathy Thomas, Steve Egan, Roger Mcdonald, Julian Tritton, Joanne Elliott, Guy Mckenzie, Sarah A Tsafnat, Joerg Elliott, Peter Meerpohl, Itziar Tugwell, Bronwen Etxeandia, Merner, 10.1016/j.jclinepi.2017.08.011Journal of Clinical Epidemiology. Stefania Mondello, Tari Turner, Ruth Foxlee, Richard Morley, Gert van Valkenhoef, Paul Garner, Marcus Munafo, Per Vandvik, Martha Gerrity, Zachary Munn, Byron Wallace, Paul Glasziou, Melissa Murano, Sheila A. Wallace, Sally Green, Kristine Newman, Chris Watts, Jeremy Grimshaw, Robby Nieuwlaat, Laura Weeks, Kurinchi Gurusamy, Adriani Nikolakopoulou, Aaron Weigl, Neal Haddaway912017. 2017Jordi Pardo PardoRobin Featherstone</p>
<p>SciFact-Open: Towards open-domain scientific claim verification. David Wadden, Kyle Lo, Bailey Kuehl, Arman Cohan, Iz Beltagy, Lucy Lu Wang, Hannaneh Hajishirzi, 10.18653/v1/2022.findings-emnlp.347Findings of the Association for Computational Linguistics: EMNLP 2022. Zornitsa Goldberg, Yue Kozareva, Zhang, Abu Dhabi, United Arab Emirates2022Association for Computational Linguistics</p>
<p>MultiVerS: Improving scientific claim verification with weak supervision and full-document context. David Wadden, Kyle Lo, Lucy Lu Wang, Arman Cohan, Iz Beltagy, Hannaneh Hajishirzi, 10.18653/v1/2022.findings-naacl.6Findings of the Association for Computational Linguistics: NAACL 2022. Marie-Catherine De Marneffe, Ivan Vladimir, Meza Ruiz, Marine Carpuat; Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Prompting Large Language Models for Topic Modeling. Han Wang, Nirmalendu Prakash, Khoi Nguyen, Ming Hoang, Usman Shan Hee, Roy Ka-Wei Naseem, Lee, 10.1109/BigData59044.2023.103861132023 IEEE International Conference on Big Data (BigData). Los Alamitos, CA, USAIEEE Computer Society2023</p>
<p>Xingbo Wang, Samantha L Huey, Rui Sheng, Saurabh Mehta, Fei Wang, arXiv:2404.13765[cs.HC]SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model. 2024</p>
<p>Goal-Driven Explainable Clustering via Language Descriptions. Zihan Wang, Jingbo Shang, Ruiqi Zhong, 10.18653/v1/2023.emnlp-main.657Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>OASum: Large-Scale Open Domain Aspectbased Summarization. Xianjun Yang, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Xiaoman Pan, Linda Petzold, Dong Yu, 10.18653/v1/2023.findings-acl.268Findings of the Association for Computational Linguistics: ACL 2023. Toronto, CanadaAssociation for Computational Linguistics2023</p>
<p>LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA. Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, Juanzi Li, arXiv:2409.02897[cs.CL]2024</p>
<p>ClusterLLM: Large Language Models as a Guide for Text Clustering. Yuwei Zhang, Zihan Wang, Jingbo Shang, 10.18653/v1/2023.emnlp-main.858Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Hierarchical Catalogue Generation for Literature Review: A Benchmark. Kun Zhu, Xiaocheng Feng, Xiachong Feng, Yingsheng Wu, Bing Qin, 10.18653/v1/2023.findings-emnlp.453Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>            </div>
        </div>

    </div>
</body>
</html>
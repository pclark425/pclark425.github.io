<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3394 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3394</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3394</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-2cf1f6c723006f258599fd9f000bb616ae83387a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2cf1f6c723006f258599fd9f000bb616ae83387a" target="_blank">Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> JEEBench is presented, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs, and a post-hoc confidence-thresholding method over self-consistency is developed, which enables effective response selection.</p>
                <p><strong>Paper Abstract:</strong> The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3394.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3394.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proprietary large transformer-based language model from OpenAI evaluated extensively on JEEBENCH; demonstrates the best performance among tested LLMs but shows notable failures on tasks requiring spatial grounding in physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (gpt-4-0314 used for main experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large pre-trained transformer model (OpenAI GPT-4 family). In the paper GPT-4 is used as a black-box API; prompted with problem statements and asked for detailed solutions. Experiments include variants with zero-shot and few-shot Chain-of-Thought (CoT), self-consistency sampling, and a verifier instance for self-critique. The paper also evaluated gpt-4-0613 (function-calling enabled) for calculator use.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Physics problems in JEEBENCH that require 2D/3D spatial reasoning (textual descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Physics problems from the JEE-Advanced exam that require understanding of spatial relations (forces and directions, geometry of setups, areas between curves, spatial grounding of diagrams). Problems often need visual/spatial grounding to translate conceptual physics into equations and correct sign/direction choices.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual problem statements (diagrams were removed from the dataset; ~40% of original problems with images/diagrams were excluded), i.e., purely text descriptions of spatial setups.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Various: vanilla prompting with required answer-type + problem; zero-shot Chain-of-Thought (append 'Let's think step by step'); few-shot CoT (one example per question-type/subject in some runs); self-consistency (sample multiple outputs at non-zero temperature, majority vote / thresholding); self-critique (a second GPT-4 instance asked to find and fix errors); function-calling API used in a separate run for arithmetic (gpt-4-0613).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Qualitative error analysis shows GPT-4 frequently fails in spatial grounding: examples in Appendix A.7 demonstrate errors such as incorrect directional decomposition of forces (misplacing cosine/sine components), taking area between a curve and the x-axis when the problem required area between curve and a line (x = y), and other 2D reasoning mistakes. The authors note GPT-4's spatial reasoning is 'far from perfect' and hypothesize multi-modal GPT-4 might make spatial reasoning easier to evaluate. No internal attention/representation analysis was presented—analysis is manual, qualitative, based on model outputs and categorized error types (Conceptual, Grounding, Computation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>On the JEEBENCH Physics subset: GPT-4 baseline accuracy = 0.352 (35.2%) on physics questions (Table 2). Zero-shot CoT variant reported physics = 0.335 (33.5%); GPT-4+CoT+Self-Consistency@8 (SC@8) reported physics = 0.449 (44.9%). Overall aggregate GPT-4 score across all subjects was 0.309 (30.9%), improved to 0.350 with CoT, and further improved with self-consistency variants. Note: many spatial/diagrammatic problems were excluded from the dataset (≈40%), so these numbers refer to text-only spatial problems.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Main failure modes relevant to spatial problems: grounding errors (correct concept but incorrect application to spatial setup), conceptual retrieval failures (not fetching the right spatial physics principle), and computation/algebraic manipulation errors. Specific spatial failures include wrong decomposition of vector components (misapplied sine/cosine), wrong geometric interpretation (choosing incorrect curve/region for area calculations). The dataset construction excluded diagram-based questions (~40%), limiting evaluation of image-grounded spatial reasoning. Use of function-calling (calculator API) did not reliably help spatial problems because many errors are symbolic/grounding rather than pure arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Compared to other LLMs on physics subset: GPT-3.5 physics = 0.173 (17.3%), PaLM-2 physics = 0.146 (14.6%), GPT-3 physics = 0.134 (13.4%), Falcon7B-Instruct physics = 0.085 (8.5%), Alpaca-LoRA physics = 0.087 (8.7%). Thus GPT-4 is substantially better than other tested models on text-only spatial physics problems. Human comparison reported for whole exam: after confidence-thresholding GPT-4 projected score on 2023 paper ≈ 49/229 (scaled to 77/360), placing GPT-4 in approximately the 80–90th percentile of human applicants—this is an aggregate measure and not specific to spatial physics problems.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Diagrams were explicitly removed from dataset, so spatial reasoning is evaluated only from textual descriptions; paper suggests future multimodal evaluation (multi-modal GPT-4) may help and that spatial reasoning remains a weakness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3394.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3394.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4+CoT+SC@8</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 with Chain-of-Thought prompting plus Self-Consistency sampling (8 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of GPT-4 using zero-shot chain-of-thought prompting ('Let's think step by step') combined with self-consistency (sampling multiple reasoning chains and aggregating) which improved performance on many JEEBENCH categories including physics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 + zero-shot Chain-of-Thought + Self-Consistency (8 sampled chains)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-4 model family as above, prompted with explicit CoT and sampled at non-zero temperature to generate multiple chains of reasoning; aggregation rules: majority vote for single-answer types, per-option frequency thresholding for multi-answer types; thresholds tuned on validation data (τ_single=0.125, τ_multiple=0.75).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Textual spatial/physics problems in JEEBENCH (as above)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Physics problems requiring spatial grounding presented as text; same description as the GPT-4 entry.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Text only (diagrams removed).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot Chain-of-Thought ('Let's think step by step') combined with self-consistency sampling (generate 8 responses) and thresholded aggregation of outputs; validation-derived thresholds used to decide whether to include an option in final response to account for negative marking.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Self-consistency improves aggregate correctness rates, suggesting sampling multiple reasoning chains reduces some stochastic errors, but manual inspection shows many spatial grounding mistakes persist across samples. The authors note self-consistency yields large gains overall (e.g., aggregate total score improvement), but spatial grounding errors (incorrect geometry or vector decompositions) remain common and are not reliably corrected by sampling/aggregation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Physics subset accuracy reported as 0.449 (44.9%) for GPT-4+CoT+SC@8 (Table 2). This represents a sizable improvement over single-chain GPT-4+CoT runs (physics 33.5% in their CoT row) on the text-only physics questions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Although self-consistency reduces variance and improves accuracy, it does not specifically fix systematic grounding errors; sampled chains can repeat the same incorrect spatial grounding. The aggregation strategy can help in deciding whether to answer or skip under negative marking but does not reveal correct internal spatial representations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>This variant outperforms the baseline GPT-4 CoT and other evaluated models on text-only physics tasks (compare 44.9% vs. GPT-4 baseline 35.2% and GPT-3.5 17.3%). Human comparison provided at aggregate exam level only (see GPT-4 entry).</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>Authors tuned thresholds on a validation split (2016–2021) and applied to test (2022–2023); thresholding combined with self-consistency increased total exam marks in the negative-marking setting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3394.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3394.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (multimodal mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multimodal GPT-4 (mentioned, not evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper mentions the multimodal version of GPT-4 as potentially useful for evaluating spatial reasoning on physics problems that include diagrams, but does not perform experiments with it.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multimodal GPT-4 (mentioned only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mentioned as the multimodal release of GPT-4 which could make evaluation of diagrammatic/spatial physics problems easier; no experiments or quantitative results reported in this paper for the multimodal model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Diagrammatic spatial physics problems (excluded from this text-only dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Physics problems containing diagrams that were removed from the JEEBENCH dataset (~40% were removed during preprocessing); such problems would require interpreting images/diagrams and mapping them into spatial reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Potentially image + text (multimodal), but not used in the paper—authors removed diagram-containing questions for the conducted experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>None reported for multimodal GPT-4 in this paper (only a suggestion that multimodal evaluation might help).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>No empirical analysis in this paper; authors state that with the release of the multimodal GPT-4, evaluating spatial reasoning (esp. diagram-based physics problems) 'might be easier'—this is a forward-looking remark based on the model's multimodal capability rather than experimental evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Not evaluated here; cited as a future avenue because current experiments exclude diagrammatic problems, limiting the assessment of image-grounded spatial reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>No experimental comparisons provided in the paper for the multimodal model.</td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper's dataset explicitly excluded problems with diagrams (~40%), so the present evaluation is limited to text-only spatial reasoning; the multimodal mention indicates the authors recognize this limitation and propose future multimodal evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Scibench: Evaluating college-level scientific problem-solving abilities of large language models <em>(Rating: 2)</em></li>
                <li>Learn to explain: Multimodal reasoning via thought chains for science question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3394",
    "paper_id": "paper-2cf1f6c723006f258599fd9f000bb616ae83387a",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4 (OpenAI)",
            "brief_description": "A proprietary large transformer-based language model from OpenAI evaluated extensively on JEEBENCH; demonstrates the best performance among tested LLMs but shows notable failures on tasks requiring spatial grounding in physics problems.",
            "citation_title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 (gpt-4-0314 used for main experiments)",
            "model_description": "Proprietary large pre-trained transformer model (OpenAI GPT-4 family). In the paper GPT-4 is used as a black-box API; prompted with problem statements and asked for detailed solutions. Experiments include variants with zero-shot and few-shot Chain-of-Thought (CoT), self-consistency sampling, and a verifier instance for self-critique. The paper also evaluated gpt-4-0613 (function-calling enabled) for calculator use.",
            "model_size": null,
            "puzzle_name": "Physics problems in JEEBENCH that require 2D/3D spatial reasoning (textual descriptions)",
            "puzzle_description": "Physics problems from the JEE-Advanced exam that require understanding of spatial relations (forces and directions, geometry of setups, areas between curves, spatial grounding of diagrams). Problems often need visual/spatial grounding to translate conceptual physics into equations and correct sign/direction choices.",
            "input_representation": "Textual problem statements (diagrams were removed from the dataset; ~40% of original problems with images/diagrams were excluded), i.e., purely text descriptions of spatial setups.",
            "prompting_method": "Various: vanilla prompting with required answer-type + problem; zero-shot Chain-of-Thought (append 'Let's think step by step'); few-shot CoT (one example per question-type/subject in some runs); self-consistency (sample multiple outputs at non-zero temperature, majority vote / thresholding); self-critique (a second GPT-4 instance asked to find and fix errors); function-calling API used in a separate run for arithmetic (gpt-4-0613).",
            "spatial_reasoning_analysis": "Qualitative error analysis shows GPT-4 frequently fails in spatial grounding: examples in Appendix A.7 demonstrate errors such as incorrect directional decomposition of forces (misplacing cosine/sine components), taking area between a curve and the x-axis when the problem required area between curve and a line (x = y), and other 2D reasoning mistakes. The authors note GPT-4's spatial reasoning is 'far from perfect' and hypothesize multi-modal GPT-4 might make spatial reasoning easier to evaluate. No internal attention/representation analysis was presented—analysis is manual, qualitative, based on model outputs and categorized error types (Conceptual, Grounding, Computation).",
            "performance_metrics": "On the JEEBENCH Physics subset: GPT-4 baseline accuracy = 0.352 (35.2%) on physics questions (Table 2). Zero-shot CoT variant reported physics = 0.335 (33.5%); GPT-4+CoT+Self-Consistency@8 (SC@8) reported physics = 0.449 (44.9%). Overall aggregate GPT-4 score across all subjects was 0.309 (30.9%), improved to 0.350 with CoT, and further improved with self-consistency variants. Note: many spatial/diagrammatic problems were excluded from the dataset (≈40%), so these numbers refer to text-only spatial problems.",
            "limitations_or_failure_modes": "Main failure modes relevant to spatial problems: grounding errors (correct concept but incorrect application to spatial setup), conceptual retrieval failures (not fetching the right spatial physics principle), and computation/algebraic manipulation errors. Specific spatial failures include wrong decomposition of vector components (misapplied sine/cosine), wrong geometric interpretation (choosing incorrect curve/region for area calculations). The dataset construction excluded diagram-based questions (~40%), limiting evaluation of image-grounded spatial reasoning. Use of function-calling (calculator API) did not reliably help spatial problems because many errors are symbolic/grounding rather than pure arithmetic.",
            "comparison_to_other_models_or_humans": "Compared to other LLMs on physics subset: GPT-3.5 physics = 0.173 (17.3%), PaLM-2 physics = 0.146 (14.6%), GPT-3 physics = 0.134 (13.4%), Falcon7B-Instruct physics = 0.085 (8.5%), Alpaca-LoRA physics = 0.087 (8.7%). Thus GPT-4 is substantially better than other tested models on text-only spatial physics problems. Human comparison reported for whole exam: after confidence-thresholding GPT-4 projected score on 2023 paper ≈ 49/229 (scaled to 77/360), placing GPT-4 in approximately the 80–90th percentile of human applicants—this is an aggregate measure and not specific to spatial physics problems.",
            "notes": "Diagrams were explicitly removed from dataset, so spatial reasoning is evaluated only from textual descriptions; paper suggests future multimodal evaluation (multi-modal GPT-4) may help and that spatial reasoning remains a weakness.",
            "uuid": "e3394.0",
            "source_info": {
                "paper_title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4+CoT+SC@8",
            "name_full": "GPT-4 with Chain-of-Thought prompting plus Self-Consistency sampling (8 samples)",
            "brief_description": "A variant of GPT-4 using zero-shot chain-of-thought prompting ('Let's think step by step') combined with self-consistency (sampling multiple reasoning chains and aggregating) which improved performance on many JEEBENCH categories including physics.",
            "citation_title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4 + zero-shot Chain-of-Thought + Self-Consistency (8 sampled chains)",
            "model_description": "Same GPT-4 model family as above, prompted with explicit CoT and sampled at non-zero temperature to generate multiple chains of reasoning; aggregation rules: majority vote for single-answer types, per-option frequency thresholding for multi-answer types; thresholds tuned on validation data (τ_single=0.125, τ_multiple=0.75).",
            "model_size": null,
            "puzzle_name": "Textual spatial/physics problems in JEEBENCH (as above)",
            "puzzle_description": "Physics problems requiring spatial grounding presented as text; same description as the GPT-4 entry.",
            "input_representation": "Text only (diagrams removed).",
            "prompting_method": "Zero-shot Chain-of-Thought ('Let's think step by step') combined with self-consistency sampling (generate 8 responses) and thresholded aggregation of outputs; validation-derived thresholds used to decide whether to include an option in final response to account for negative marking.",
            "spatial_reasoning_analysis": "Self-consistency improves aggregate correctness rates, suggesting sampling multiple reasoning chains reduces some stochastic errors, but manual inspection shows many spatial grounding mistakes persist across samples. The authors note self-consistency yields large gains overall (e.g., aggregate total score improvement), but spatial grounding errors (incorrect geometry or vector decompositions) remain common and are not reliably corrected by sampling/aggregation.",
            "performance_metrics": "Physics subset accuracy reported as 0.449 (44.9%) for GPT-4+CoT+SC@8 (Table 2). This represents a sizable improvement over single-chain GPT-4+CoT runs (physics 33.5% in their CoT row) on the text-only physics questions.",
            "limitations_or_failure_modes": "Although self-consistency reduces variance and improves accuracy, it does not specifically fix systematic grounding errors; sampled chains can repeat the same incorrect spatial grounding. The aggregation strategy can help in deciding whether to answer or skip under negative marking but does not reveal correct internal spatial representations.",
            "comparison_to_other_models_or_humans": "This variant outperforms the baseline GPT-4 CoT and other evaluated models on text-only physics tasks (compare 44.9% vs. GPT-4 baseline 35.2% and GPT-3.5 17.3%). Human comparison provided at aggregate exam level only (see GPT-4 entry).",
            "notes": "Authors tuned thresholds on a validation split (2016–2021) and applied to test (2022–2023); thresholding combined with self-consistency increased total exam marks in the negative-marking setting.",
            "uuid": "e3394.1",
            "source_info": {
                "paper_title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "GPT-4 (multimodal mention)",
            "name_full": "Multimodal GPT-4 (mentioned, not evaluated)",
            "brief_description": "The paper mentions the multimodal version of GPT-4 as potentially useful for evaluating spatial reasoning on physics problems that include diagrams, but does not perform experiments with it.",
            "citation_title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "mention_or_use": "mention",
            "model_name": "Multimodal GPT-4 (mentioned only)",
            "model_description": "Mentioned as the multimodal release of GPT-4 which could make evaluation of diagrammatic/spatial physics problems easier; no experiments or quantitative results reported in this paper for the multimodal model.",
            "model_size": null,
            "puzzle_name": "Diagrammatic spatial physics problems (excluded from this text-only dataset)",
            "puzzle_description": "Physics problems containing diagrams that were removed from the JEEBENCH dataset (~40% were removed during preprocessing); such problems would require interpreting images/diagrams and mapping them into spatial reasoning steps.",
            "input_representation": "Potentially image + text (multimodal), but not used in the paper—authors removed diagram-containing questions for the conducted experiments.",
            "prompting_method": "None reported for multimodal GPT-4 in this paper (only a suggestion that multimodal evaluation might help).",
            "spatial_reasoning_analysis": "No empirical analysis in this paper; authors state that with the release of the multimodal GPT-4, evaluating spatial reasoning (esp. diagram-based physics problems) 'might be easier'—this is a forward-looking remark based on the model's multimodal capability rather than experimental evidence.",
            "performance_metrics": null,
            "limitations_or_failure_modes": "Not evaluated here; cited as a future avenue because current experiments exclude diagrammatic problems, limiting the assessment of image-grounded spatial reasoning.",
            "comparison_to_other_models_or_humans": "No experimental comparisons provided in the paper for the multimodal model.",
            "notes": "The paper's dataset explicitly excluded problems with diagrams (~40%), so the present evaluation is limited to text-only spatial reasoning; the multimodal mention indicates the authors recognize this limitation and propose future multimodal evaluation.",
            "uuid": "e3394.2",
            "source_info": {
                "paper_title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 2
        },
        {
            "paper_title": "Large language models are zero-shot reasoners",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Scibench: Evaluating college-level scientific problem-solving abilities of large language models",
            "rating": 2
        },
        {
            "paper_title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "rating": 1
        }
    ],
    "cost": 0.01484525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Have LLMs Advanced Enough? <br> A Challenging Problem Solving Benchmark For Large Language Models</h1>
<p>Daman Arora ${ }^{\star, \dagger}$<br>Microsoft Research<br>daman1209arora@gmail.com</p>
<p>Himanshu Gaurav Singh ${ }^{\star, \dagger}$<br>UC Berkeley<br>himanshu_singh@berkeley.edu</p>
<p>Mausam<br>IIT Delhi<br>mausam@cse.iitd.ac.in</p>
<h4>Abstract</h4>
<p>The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBENCH, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging preengineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-ofthought prompting, is less than $40 \%$. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.</p>
<h2>1 Introduction</h2>
<p>The capabilities of large language models (LLMs) have been improving since the last decade on a plethora of tasks including reasoning. Most recently, GPT-4 demonstrates significant improvements over GPT-3 on tasks such as code-generation, arithmetic and commonsense reasoning (Bubeck et al., 2023), exhibiting impressive performance on standard reasoning and STEM benchmarks such as GSM-8K (Cobbe et al., 2021), MATH (Hendrycks</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>A uniform wooden stick of mass 1.6 kg and length $l$ rests in an inclined manner on a smooth, vertical wall of height $h(&lt;l)$ such that a small portion of the stick extends beyond the wall. The reaction force of the wall on the stick is perpendicular to the stick. The stick makes an angle of $30^{\circ}$ with the wall and the bottom of the stick is on a rough floor. The reaction of the wall on the stick is equal in magnitude to the reaction of the floor on the stick. The ratio $h / l$ and the frictional force $f$ at the bottom of the stick are
$(g=10 \mathrm{~ms} \mathrm{~s}^{2})$
(A) $\frac{h}{l}=\frac{15}{25} f=\frac{10 \sqrt{3}}{2} \mathrm{~N}$
(B) $\frac{h}{l}=\frac{15}{25} f=\frac{10 \sqrt{3}}{2} \mathrm{~N}$
(C) $\frac{h}{l}=\frac{15 \sqrt{3}}{25} f=\frac{8 \sqrt{3}}{2} \mathrm{~N}$
(D) $\frac{h}{l}=\frac{3 \sqrt{3}}{25} f=\frac{18 \sqrt{3}}{2} \mathrm{~N}$</p>
<p>Solution Since the stick is in static equilibrium,</p>
<ol>
<li>All forces along the horizontal axis sum to zero.</li>
</ol>
<p>CONCEPT
2. All forces along the vertical axis sum to zero.</p>
<p>RETRIEVAL
3. The torque around any point on the stick is zero.</p>
<p>Let the normal reaction at the point of contact between the stick and the wall be $R_{1}, R_{2}$ will act perpendicular to the stick. Let the normal reaction at the point of contact between the stick and the floor be $R_{3}, R_{2}$ acts perpendicular to the floor in the upward direction. Let the friction be $f$, acting parallel to the floor.</p>
<p>From (1), we have</p>
<p>$$
f=R_{1} \cos 30^{\circ}
$$</p>
<p>CONCEPT
GROUNDING
For applying (3), we use the point of contact between the stick and the floor. Since the torque along it is zero, we have</p>
<p>$$
R_{1}=\frac{3}{\cos 30^{\circ}}=\mathrm{mg} \frac{l}{2} \sin 30^{\circ}
$$</p>
<p>Finally, we are given that</p>
<p>$$
R_{1}=R_{2}
$$</p>
<p>Solving equation (2) and (4), we get $R_{1}=\frac{\mathrm{mg}}{1 \cos 30^{\circ}}=\frac{2}{3} \mathrm{mg}$ Substituting into equation (3), we get</p>
<p>$$
\begin{gathered}
\frac{2}{3} \mathrm{mg}-\frac{h}{\cos 30^{\circ}}=\mathrm{mg} \frac{l}{2} \sin 30^{\circ} \
\frac{h}{l}=\frac{3 \cos 30^{\circ} \sin 30^{\circ}}{2 \cdot 3}=\frac{3 \cdot \sqrt{3}}{4} \cdot \frac{1}{4}=\frac{2 \sqrt{3}}{16}
\end{gathered}
$$</p>
<p>From (1), $f=R_{1} \cos 30^{\circ}=\frac{2}{3} \mathrm{mg} \cos 30^{\circ}=\frac{\mathrm{mg} \sqrt{3}}{2}=\frac{16 \sqrt{3}}{2}$. Therefore, option D is correct.</p>
<p>Figure 1: An example problem from JEEBENCH
et al., 2021b), MMLU (Hendrycks et al., 2021a) and ScienceQA (Lu et al., 2022)</p>
<p>Rising capabilities of LLMs call for harder benchmarks. We introduce JEEBENCH, a benchmark consisting of 515 problems that require complex logical and mathematical reasoning on top of deep in-domain knowledge of pre-engineering level Physics, Chemistry and Mathematics. Problems have been curated from the past 8 editions of the Joint Entrance Examination (JEE)-Advanced exam, held annually in India as an entrance test to India's premier engineering institutes: the IITs. The exam is designed to be time-consuming, diffi-</p>
<p>cult, and has a low selection rate (approx. 5\%).</p>
<p>The problems in the dataset require a complex interplay of employing multiple high-level domain specific concepts, grounding them into mathematical equations/constraints, followed by algebraic manipulation and arithmetic operations. Figure 1 is a problem from the dataset along with an expert's solution. In this problem, the ideal solution involves the retrieval of the appropriate concepts: the rules of static equilibrium, grounding the concepts into mathematical equations for the specific problem instance, followed by solving the equations in order to find the final answer. Other instances of domain-specific concepts can be Balancing of redox reactions (Chemistry), Current into a junction equals current out of the junction (Physics) and Integration by parts (Mathematics). More such examples can be found in the Appendix A.2.</p>
<p>We conduct a qualitative and quantitative study of contemporary open-source and proprietary LLMs on these problems and also highlight avenues for further research. Our analysis indicates that GPT-4 is unparalleled in performance compared to other models. It demonstrates long horizon reasoning and the ability to manipulate complex algebraic equations in quite a few problems. We observe that chain-of-thought prompting (Kojima et al., 2022) and self-consistency (Wang et al., 2023b), which are recent proposals to improve LLM performance, are indeed effective on our dataset.</p>
<p>We also explore Self-Critique (Madaan et al., 2023; Shinn et al., 2023), where an LLM (verifier) is instructed to improve the outputs of the same LLM (generator). We find that this approach is not helpful on JEEBENCH. The verifier is weak in spotting conceptual errors, and like the generator, is itself prone to hallucinations. It would be interesting to explore the class of problems where this approach of self-refinement is (not) helpful.</p>
<p>We further conduct a critical analysis of the limits of GPT-4's reasoning abilities, and highlight major areas that require considerable improvement. A detailed error analysis suggests that it frequently struggles in retrieving relevant concepts required to solve problems, and performing algebraic manipulation \&amp; arithmetic. Inability to perform even simple algebra highlights an important question: can we build LLMs faithful to mathematical logic?</p>
<p>Another important question is how to estimate GPT-4's performance in comparison to humans.</p>
<p>The JEE Advanced Exam comes with the bane of negative marking for incorrectly answered questions. This makes the exam even more challenging, because in addition to advanced problem solving skills, it requires an accurate risk assessment and computing a good policy based on it. Our experiments demonstrate that when prompted with the marking scheme, GPT-4's performance actually drops. To mitigate this, we employ a simple method - thresholding over self consistency. Self consistency generates multiple responses for each question. Relative frequency in the set of responses can be considered as a proxy for confidence score of each option. Threshold on the confidence score can be tuned using a validation set. We find that GPT-4's score, after augmenting it this way, lies in the top 10-20 percentile of human scores in the 2023 edition of the exam.</p>
<p>Overall, we hope that this benchmark serves as a strong and reliable test-bed and fosters future research on problem solving with LLMs. Our code and dataset are available at https://github.com/ dair-iitd/jeebench.</p>
<h2>2 Related Work</h2>
<p>Reasoning has been studied under various contexts such as logical reasoning, commonsense reasoning, mathematical reasoning, and theorem proving. We summarize some key works in two sub-areas, most closely related to our work: mathematical reasoning and Science QA.
Mathematical problem solving: GSM8K (Cobbe et al., 2021), Dolphin18K (Huang et al., 2016), AQuA-RAT (Ling et al., 2017), MATH (Hendrycks et al., 2021b) and Ape210K (Zhao et al., 2020) are datasets that contain mathematical reasoning questions. Dolphin18K, GSM8K, and AQuA-RAT consist of elementary problems, requiring only basic arithmetic and problem comprehension. Thus, there is a general lack of variety in the underlying reasoning steps across problems. In terms of difficulty, MATH, containing problems from AMC, AIME and Olympiads, comes close to JEEBENCH in terms of complexity. However, compared to MATH, the mathematics questions in our dataset span many additional topics such as Differential and Integral Calculus, Differential Equations, 3D geometry, and Conic Sections. Also, the problems in JEEBENCH are harder, as we discuss later in the paper. miniF2F (Zheng et al., 2022) consists of mathematics problems from MATH dataset and</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Performance of GPT-4 on a random subset of MATH, MMLU-Physics, Chemistry, Math, JEEBENCH</p>
<p>other sources in a formal language. In contrast, problems in our dataset are in natural language.</p>
<p><strong>General Science:</strong> In the context of Physics and Chemistry, ScienceQA (Lu et al., 2022), SciQ (Welbl et al., 2017) and MMLU (Hendrycks et al., 2021a) are prominent available datasets. ScienceQA and SciQ, built from elementary and high school science curricula, mainly test factual knowledge of the subject. The skills required to solve such problems are primarily information extraction, reading comprehension and commonsense reasoning. In contrast, questions in our dataset require long-horizon reasoning and grounding of complex scientific concepts into equations and arithmetic. Concurrently, C-Eval (Huang et al., 2023) and SciBench (Wang et al., 2023a) are datasets along similar lines. C-Eval consists of a variety of disciplines such as engineering, medicine and humanities has been created. SciBench creates a dataset from college-level Mathematics, Physics and Chemistry questions.</p>
<p>Problems present in JEEBENCH are significantly harder than those in other contemporary datasets. To verify this, we sample 50 questions each from JEEBENCH and the test sets of MATH and the high-school Physics, Chemistry and Mathematics sections from MMLU and conduct zero-shot evaluations on GPT-4. The results can be seen in Figure 2. As we can see, GPT-4 can easily solve more than 80% of problems from MMLU. The MATH dataset is harder, where the performance is approximately 60%. However, GPT-4 struggles in JEEBENCH-Math, solving close to a mere 20% of problems.</p>
<h2>3 The JEEBENCH Dataset</h2>
<p>The dataset consists of 515 problems extracted from the past 8 editions of the JEE-Advanced from the year 2016 to 2023. The problems are harvested from publicly available sources.<sup>1</sup> The exam consists of 2 papers held every year, each containing 50-60 questions equally distributed among Physics, Chemistry, and Mathematics. We use online tools to extract problems from PDF-format exam papers into LaTeX. We remove all problems containing diagrams in their description (approximately 40%). Manual quality checks are performed to fix/eliminate possible errors in preprocessing. Figure 3 shows representative problems from the final dataset. The problems are categorized by subject: Physics, Chemistry and Mathematics, and the format of expected response: multiple choice questions (MCQ) with single option correct, MCQs with multiple options correct, Integersype and Numerics-type. In Integer-type questions, the answer is an unbounded non-negative integer, whereas for Numerics-type, the answer is a floating point number up to 2 digits after the decimal point. The breakdown of the problems based on answer-type and subject is shown in Table 1.<sup>2</sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Math</th>
<th>Phys</th>
<th>Chem</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Single-Correct</td>
<td>53</td>
<td>27</td>
<td>30</td>
<td>110</td>
</tr>
<tr>
<td>Multi-Correct</td>
<td>85</td>
<td>41</td>
<td>60</td>
<td>186</td>
</tr>
<tr>
<td>Integer</td>
<td>37</td>
<td>22</td>
<td>23</td>
<td>82</td>
</tr>
<tr>
<td>Numerics</td>
<td>61</td>
<td>33</td>
<td>43</td>
<td>137</td>
</tr>
<tr>
<td>Total</td>
<td>236</td>
<td>123</td>
<td>156</td>
<td>515</td>
</tr>
</tbody>
</table>
<p>Table 1: # of questions for each subject and problem-type.</p>
<p>The questions contained in the dataset belong to diverse sub-topics (for example, Math questions could belong to Calculus, Algebra, Combinatorics, etc.). The breakdown of the entire dataset into sub-topics can be found in Appendix A.1.</p>
<h2>4 Experimental Setup and Results</h2>
<p>We wish to investigate the following research problems:</p>
<ol>
<li>How well do LLMs perform on JEEBENCH?</li>
<li>How effective are methods, such as chain-of-thought prompting and self-consistency, which have been proposed to improve the reasoning abilities of LLMs?</li>
<li>What are the main sources of errors which limit the performance of these models?</li>
</ol>
<p>^{1}https://jeeadv.ac.in/archive.html</p>
<p>^{2}There are fewer problems in Physics and Chemistry because more problems in these two subjects contained images.</p>
<table>
<thead>
<tr>
<th>A gas is enclosed in a cylinder with a movable frictionless piston. Its initial thermodynamic state at pressure $P_{i}=10^{5} \mathrm{~Pa}$ and volume $V_{i}=10^{-3} \mathrm{~m}^{3}$ changes to a final state at $P_{f}=(1 / 32) \times 10^{5} \mathrm{~Pa}$ and $V_{f}=8 \times 10^{-3} \mathrm{~m}^{3}$ in an adiabatic quasi-static process, such that $P^{2} V^{2}=$ constant. Consider another thermodynamic process that brings the system from the same initial state to the same final state in two steps: an isobaric expansion at $P_{i}$ followed by an isobaric (iso-volumetric) process at volume $V_{f}$. The amount of heat supplied to the system in the two-step process is approximately</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>(A) 112 J</td>
<td></td>
</tr>
<tr>
<td>(B) 284 J</td>
<td></td>
</tr>
<tr>
<td>(C) 588 J</td>
<td></td>
</tr>
<tr>
<td>(D) 815 J</td>
<td></td>
</tr>
<tr>
<td>What is the total number of compounds having at least one bridging one group among the molecules given below?</td>
<td></td>
</tr>
<tr>
<td>$S_{2} O_{3}, S_{2} O_{3}, P_{1} O_{6}, P_{1} O_{7}, H_{4} P_{2} O_{7}, H_{5} P_{2} O_{10}, H_{2} S_{2} O_{3}, H_{2} S_{2} O_{5}$</td>
<td></td>
</tr>
<tr>
<td>Let $z=\frac{1}{2} \frac{1}{2} \frac{1}{2}$, where $i=\frac{1}{4} \frac{1}{4}$, and $r, s \in{1,2,3}$. Let $P=\frac{1}{2} \frac{1}{2} \frac{1}{2}$</td>
<td></td>
</tr>
<tr>
<td>and $J$ be the identity matrix of order 2 . Then what is the total number of ordered pairs $(r, s)$ for which $P^{2}=-P$</td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 3: Instances from the dataset from each subject: Physics (Top), Chemistry (Middle), Math (Bottom)
4. Can LLMs be used to verify their own generations in the context of JEEBENCH? What are the limitations in this behaviour?
5. How would they perform in an exam setting, where each question could potentially give negative marks when answered incorrectly?</p>
<h3>4.1 Metrics</h3>
<p>For Single-Correct MCQs and Integer-type questions, we use accuracy as the metric, that is, a score of 1 if the model response matches the gold response, otherwise 0 . For Numeric-type questions, we award a score of 1 if the model response differs from the gold response by atmost 0.01 . For MultiCorrect MCQs, we award a score of 1 if the model response matches all the correct options. If any of the options selected by the model is incorrect, we award 0 . If the model selects some of the correct options and no incorrect option, then for each correct option in the output, the model is given a score of 0.25 . For example, if the gold response is ABD and the output response is BD , a score of 0.5 is awarded. This is done, so as to reflect the actual scoring method of JEE-Advanced, which incentivizes a student to not guess. ${ }^{3}$</p>
<h3>4.2 Prompting LLMs</h3>
<p>We evaluate the proposed benchmark on some open-source models: Falcon7B-Instruct (Almazrouei et al., 2023) and Alpaca-LoRA, which uses low-rank adapation (Hu et al., 2021) to reproduce Alpaca (Taori et al., 2023). Then, we evaluate proprietary models such as the OpenAI's</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>GPT series of models text-davinci-003 (GPT3), gpt-3.5-turbo (GPT-3.5) and gpt-4-0314 (GPT-4), as well as text-bison-001 (PaLM-2) provided by Google. Evaluation on larger opensourced LLMs is left for future work.</p>
<p>For obtaining the model's response, each model is prompted with the expected response type concatenated with the problem description. The exact system and user prompts can be found in the Appendix A.3. The exact answer is extracted manually from the response generated by the LLM. Sometimes, the LLM response is gibberish and sometimes responds by saying that none of the options are correct. For both of these cases, we record "None" as the answer. If the question's expected response type doesn't match the question type (for example non-integer for an integer type question), even then we record it as a "None" response.</p>
<p>We also conduct a few-shot evaluation with examples drawn from 2014 edition of the exam. One example is chosen for each question type and subject pair.</p>
<p>All the proprietary models were prompted between May 17, 2023 and June 23, 2023. The maximum response length is set to 2048 and decoding temperature is set to 0 . Table 2 contains the results obtained on various LLMs aggregated by subject and question type.</p>
<p>General trends: We observe that open-source models perform as good as random and are, in general, lagging behind proprietary models. Performance on JEEBENCH increases consistently with newer versions of the GPT model. GPT-3 exhibits near random performance, but GPT-3.5 and GPT-4 perform significantly better. GPT-4 is far superior to GPT-3.5, by a large margin of 12.9 points but overall performance still remains close to $30 \%$. It is evident that the performance boost is the highest for Chemistry, followed by Physics, and lastly Maths. This is probably because the complexity of reasoning is highest in Mathematics questions and least in Chemistry in JEEBENCH. These results highlight the difficulty of the benchmark posed to both open-source and proprietary models.</p>
<p>Hereafter, we focus on just GPT-4's performance since it is far superior to other models. Firstly, we evaluate the performance of methods like zeroshot chain-of-thought prompting (Kojima et al., 2022), self-consistency (Wang et al., 2023b) \&amp; selfrefinement (Madaan et al., 2023) on JEEBENCH.</p>
<p>Zero shot Chain-of-Thought prompting: The</p>
<table>
<thead>
<tr>
<th></th>
<th>Chemistry</th>
<th>Mathematics</th>
<th>Physics</th>
<th>Integer</th>
<th>Single-Correct</th>
<th>Multi-Correct</th>
<th>Numeric</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td>Random</td>
<td>0.108</td>
<td>0.105</td>
<td>0.103</td>
<td>0.000</td>
<td>0.250</td>
<td>0.144</td>
<td>0.000</td>
<td>0.105</td>
</tr>
<tr>
<td>Alpaca-LoRA</td>
<td>0.072</td>
<td>0.101</td>
<td>0.087</td>
<td>0.037</td>
<td>0.164</td>
<td>0.122</td>
<td>0.015</td>
<td>0.089</td>
</tr>
<tr>
<td>Falcon7B-Instruct</td>
<td>0.083</td>
<td>0.114</td>
<td>0.085</td>
<td>0.000</td>
<td>0.182</td>
<td>0.142</td>
<td>0.029</td>
<td>0.098</td>
</tr>
<tr>
<td>GPT-3</td>
<td>0.135</td>
<td>0.107</td>
<td>0.134</td>
<td>0.049</td>
<td>0.291</td>
<td>0.133</td>
<td>0.015</td>
<td>0.122</td>
</tr>
<tr>
<td>PaLM2</td>
<td>0.192</td>
<td>0.130</td>
<td>0.146</td>
<td>0.073</td>
<td>0.291</td>
<td>0.165</td>
<td>0.073</td>
<td>0.153</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>0.228</td>
<td>0.146</td>
<td>0.173</td>
<td>0.073</td>
<td>0.318</td>
<td>0.249</td>
<td>0.029</td>
<td>0.177</td>
</tr>
<tr>
<td>GPT-4</td>
<td>0.423</td>
<td>0.212</td>
<td>0.352</td>
<td>0.207</td>
<td>0.455</td>
<td>0.383</td>
<td>0.153</td>
<td>0.309</td>
</tr>
<tr>
<td>GPT-4+CoT</td>
<td>0.468</td>
<td>0.280</td>
<td>0.335</td>
<td>0.256</td>
<td>0.473</td>
<td>0.448</td>
<td>0.175</td>
<td>0.350</td>
</tr>
<tr>
<td>GPT-4+ (1-shot) CoT</td>
<td>0.409</td>
<td>0.198</td>
<td>0.323</td>
<td>0.244</td>
<td>0.391</td>
<td>0.340</td>
<td>0.175</td>
<td>0.292</td>
</tr>
<tr>
<td>GPT-4+CoT+Self Critique</td>
<td>0.487</td>
<td>0.234</td>
<td>0.352</td>
<td>0.280</td>
<td>0.355</td>
<td>0.444</td>
<td>0.219</td>
<td>0.339</td>
</tr>
<tr>
<td>GPT-4+CoT+SC@8</td>
<td>0.463</td>
<td>0.308</td>
<td>0.449</td>
<td>0.293</td>
<td>0.618</td>
<td>0.410</td>
<td>0.234</td>
<td>0.389</td>
</tr>
</tbody>
</table>
<p>Table 2: This table shows the score obtained by various open-source and proprietary models on JEEBENCH aggregated by subject on the left question type on the right. The overall aggregate scores are in the last column. Note that CoT in the table refers to zero-shot CoT except for GPT-4 + (1-shot) CoT.
original prompt is concatenated with the phrase Let's think step by step, as proposed by Kojima et al. (2022). We observe that this approach leads to significant improvement in performance, improving vanilla GPT-4 by 4.2 points.</p>
<p>Few-shot Chain-of-Thought prompting: We prepend the question with one few-shot example for each question-type, subject pair. Overall, 1Shot CoT achieves a score of 0.296 as opposed to zero-shot CoT at 0.350 and vanilla GPT-4 at 0.308. Our hypothesis is that few-shot prompting is not very helpful in these questions, because conceptual errors are hard improve upon using few-shot examples. Additionally, many novel reasoning paths might not be covered in the few-shot examples. Thus, our dataset acts as an interesting testbed for advanced approaches in few-shot prompting. Similar results where scores are better with zero-shot CoT as compared to few-shot CoT have been found in Wang et al. (2023a).</p>
<p>Function calling: Since GPT-4 makes a lot of arithmetic errors, we decide to also test the function-calling API exposed by OpenAI. Since the JEE exam only allows access to a basic calculator with 4 primitive arithmetic operation(+, -, /, *), allowing plugins such as Wolfram or python would not make a fair comparison (for example, directly asking Wolfram to integrate a function instead of doing it from first principles). Instead, to ensure a level-playing field, we use the newly introduced function-calling API to implement standard arithmetic operators.</p>
<p>Note that function-calling is only allowed with gpt-4-0613 whereas the rest of our results are with gpt-4-0314. We first evaluate gpt-4-0613 with zeroshot CoT. Surprisingly, results suggest that gpt-4-0613 (the new version) is weaker with a CoT</p>
<p>| {Now, let's make a substitution to simplify the equation further. Let $y$ <br> sin $x$. Then, $\cos x=\sqrt{1-y^{2}}$, and the equation becomes: <br> $\sqrt{3 y}+\sqrt{1-y^{2}}=2-4 y^{2}$ <br> Now, let's square both sides of the equation to eliminate the square root: <br> $3 y^{2}+2 \sqrt{3 y} \sqrt{1-y^{2}}+1-y^{2}=4-8 y^{2}$ <br> Now, let's rearrange the terms and square both sides again: <br> $2 \sqrt{3 y} \sqrt{1-y^{2}}+3 y^{2}=3$ |
| :--: | :--: | :--: | :--: | :--: |</p>
<p>We have the differential equation:</p>
<p>$$
\frac{d y}{d x}=\frac{y^{4}}{x^{2}+x y+4 x+2 y+4}
$$</p>
<p>This is a first-order non-linear differential equation. Unfortunately, there is no general method to solve such equations. However, we can try to find a particular solution that passes through the point $(1,3)$.</p>
<p>First, we need to determine the number of valence electrons for each central atom in the given compounds.
(A) $\mathrm{BrF}<em 2="2">{3}$ : Bromine (Br) has 7 valence electrons. (B) $\mathrm{ClF}</em>}$ : Chlorine (Cl) has 7 valence electrons. (C) $\mathrm{XeF<em 4="4">{3}$ : Xenon (Xe) has 8 valence electrons. (D) $\mathrm{SF}</em>$ : Sulfur (S) has 6 valence electrons.</p>
<p>Next, we need to determine the number of bonding electrons and nonbonding electrons (lone pairs) for each central atom.
(A) $\mathrm{BrF}_{3}$ : Br has 7 valence electrons. It forms 5 bonds with 5 F atoms, using 10 electrons. So, it has $7-10=\Delta$ electrons left, which is not possible. Therefore, this compound does not have two lone pairs on the central atom.</p>
<p>Figure 4: The figure shows the different types of error made by GPT-4 in its response. (i) (top) exhibits a computation error, where the squaring operation performed is algebraically wrong. (ii) (middle) response shows a conceptual error where the model is unable to retrieve the relevant concepts required to solve the problem (iii) (bottom) response is a grounding error, where the concept is correct, however the application in terms of computing # lone pair electrons on $\mathrm{Br}^{2}$ in $\mathrm{BrF}_{5}$ is wrong.
performance of 0.303 as compared to 0.350 with gpt-4-0314.</p>
<p>Using a calculator API reduces performance even more to 0.274 from 0.303 . We observe that tool usage isn't very robust for GPT-4, where it sometimes hallucinates invalid function arguments and sometimes even invalid function names! Also, GPT-4 is quite accurate at arithmetic for small digit operations. Computation errors are mostly during symbolic manipulation, rather than purely arithmetic operations, which is probably why a blackbox calculator isn't very beneficial.</p>
<p>Self-Consistency (SC): We sample multiple responses from the LLM at a non-zero temperature.</p>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conceptual Error</td>
<td>34</td>
</tr>
<tr>
<td>Computation Error</td>
<td>30</td>
</tr>
<tr>
<td>Grounding Error</td>
<td>15</td>
</tr>
<tr>
<td>Problem Miscomprehension</td>
<td>1</td>
</tr>
<tr>
<td>Perfect</td>
<td>20</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Error Type</th>
<th>Count</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conceptual Error</td>
<td>34</td>
</tr>
<tr>
<td>No</td>
<td>46</td>
</tr>
<tr>
<td>Found error but didn’t fix it</td>
<td>25</td>
</tr>
<tr>
<td>Converted non-error to error</td>
<td>7</td>
</tr>
<tr>
<td>Found error and fixed it</td>
<td>2</td>
</tr>
<tr>
<td>No</td>
<td>1</td>
</tr>
<tr>
<td>Converted non-error to error</td>
<td>1</td>
</tr>
<tr>
<td>Didn’t find error</td>
<td>19</td>
</tr>
</tbody>
</table>
<p>For Integer-type, Numeric-type and Single-Correct MCQs, we use a majority vote (from all the responses which are not "None") as the proposed answer. For Multi-Correct MCQs, we choose a simplifying assumption that all options are independent. If an option occurs atleast 50% times in the responses, we select it, otherwise we don’t. We use $\tau=0.5$ and the number of responses is set to 8. Self-consistency helps a lot in improving over the GPT-4+CoT baseline by a score of +3.9 points. In the future, it will be interesting to apply extensions such as adaptive consistency for better cost-quality tradeoffs [aggarwal2023sparks].</p>
<h3>4.3 Error Analysis of System Responses</h3>
<p>In order to assess GPT-4’s weaknesses, we conduct a manual inspection of the errors it makes in its reasoning chains. We perform this study on the errors made by GPT-4+CoT on a random subset of 100 problems. The score obtained on this subset is 27.25. We ask the following questions about the model response for each problem instance:</p>
<ol>
<li>Is GPT-4 able to retrieve the concepts/facts required for solving the problem? Inability to do this contributes to conceptual errors.</li>
<li>If relevant concepts are retrieved, are they grounded correctly as equations/constraints? These contribute to grounding errors.</li>
<li>Is the algebraic manipulation &amp; arithmetic correct? These contribute to computation errors.</li>
</ol>
<p>Refer to Figure 4 for an illustration of each type of error. In one case, we find that GPT-4 misunderstands the question. The overall results of this analysis is shown in Table 3.</p>
<p>Table 3: Variety of errors GPT-4 makes in the solution.</p>
<p>Our error analysis indicates that most errors are caused because of not being able to retrieve concepts (34 out of 80), which are critical to making progress in the solution, or due to computation errors (30 out of 80). Moreover, in 20 questions, where the answer is correct (out of 27), the explanation is also correct. i.e., 28% of the time, the model gives a correct answer for the wrong reasons.</p>
<h3>4.4 Can GPT-4 find and correct its mistakes?</h3>
<p>Can GPT-4 be used to grade its own outputs? A good grader should be able to spot errors in a solution. Using an LLM to critique its own output has been proposed recently by multiple works [shinn2023gpt4; madaan2023gpt4] and has shown improvements on some datasets. A good verifier should be able to catch and fix all errors. Even when the final answer is correct, it isn’t necessary that intermediate reasoning steps are correct.</p>
<p>We put the idea of self-critique to test on JEEBENCH. After a CoT response has been generated, we prompt another GPT-4 instance by first describing the problem, GPT’s solution and then appending the instruction: "Find problems(if any) with the given solutions. If there are any errors, correct it and give the new answer."</p>
<p>We re-evaluate the new answer suggested by GPT-4. Results clearly show that this approach doesn’t lead to improvement. In fact, it leads to poorer results as compared to GPT-4+CoT and the performance goes down from 35% to 33.9%.</p>
<p>In order to develop a deeper understanding of the repairs suggested by the verifier GPT-4, a manual inspection is performed. We use the same subset of 100 problems picked up earlier for categorizing error-types. For each generated solution and suggested edit, we pose the following questions:</p>
<ul>
<li>Can the verifier find problems in the solution?</li>
<li>Can the verifier fix problems if it finds them?</li>
<li>Is the problem identified by the verifier actually a valid problem?</li>
</ul>
<p>Table 4: The figure shows the breakup of the kind of errors the verifier GPT-4 makes while suggesting edits.</p>
<p>Our results can be seen in Table 4. It is evident that, contrary to observations in other works, on JEEBENCH, GPT-4 is mostly ( $\frac{40}{80}=57.5\%$ ) unable to find errors in solutions it proposes. Even when it can, it is unable to fix them. Only in 2</p>
<p>| Question <br> Let $S$ be the set of all complex numbers $z$ satisfying $\left|z^{2}+z+1\right|=1$. Then which of the following statements is/are TRUE? <br> (A) $\left|z+\frac{1}{2}\right| \leq \frac{1}{2}$ for all $z \in S$ <br> (B) $\left|z \leq 2\right.$ for all $z \in S$ <br> (C) $\left|z+\frac{1}{2}\right| \geq \frac{1}{2}$ for all $z \in S$ <br> (D) The set $S$ has exactly four elements <br> Answer <br> First, we can rewrite the given equation as follows: $\left|z^{2}+z+1\right|=1 \Longrightarrow\left|z^{2}+z+1-1\right|=0 \Longrightarrow\left|z^{2}+z\right|=0$ <br> There is an error in the given solution. The equation $\left|z^{2}+z\right|=0$ was incorrectly derived from the original equation $\left|z^{2}+z+1\right|=1$. The correct derivation should be: $\left|z^{2}+z+1\right|=1 \Longrightarrow\left|z^{2}+z+1-1\right|=1 \Longrightarrow\left|z^{2}+z\right|=1$ |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Question <br> A metal is heated in a furnace where a sensor is kept above the metal surface to read the power radiated $(P)$ by the metal. The sensor has a scale that displays $\log <em 0="0">{2}(P / P</em>$ and calculate the final answer: $\log })$, where $P_{0}$ is a constant. When the metal surface is at a temperature of $487^{\circ} \mathrm{C}$, the sensor shows a value 1 . Assume that the emissivity of the metallic surface remains constant. What is the value displayed by the sensor when the temperature of the metal surface is raised to $2707^{\circ} \mathrm{C}$ ? <br> Step 5: Calculate the final answer. Now, we just need to plug in the values of $T_{1}$ and $T_{2<em 2="2">{2}\left(2 \cdot \frac{\log </em>{2 \log }(2)<em 2="2">{2}(20)}\right)=\log </em>\right)=$ <br> The given solution is correct, and the final answer is 17 . The steps are well explained, and the calculations are accurate. |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | }\left(2 \cdot 16^{2</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 6: Calibration plot of GPT-4 on MCQ questions</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 7: Scores obtained on different thresholding values on Single-Correct(top) and Multi-Correct(bottom) type questions from the val set, the optimal value is τ_single = 0.125 and τ_multiple = 0.75</p>
<p>but slightly underconfident at low and medium confidences.</p>
<h3>4.5.3 Thresholding with Self-Consistency</h3>
<p>Our objective is to decide whether to include an option in the final response or not. We wish to compute a parameter τ such that an option will be in the final response if the confidence for that option is at least τ. We compute separate τ_single, τ_multiple for Single-correct and Multiple-correct MCQs respectively. We compute confidence scores for GPT-4's response to each question as in Section 4.5.2. Questions from 2016-2021 are chosen as the validation set and from 2022-2023 as the test set. The best τ_single and τ_multiple thresholds for Single-Correct and Multi-Correct MCQs by simple hyper-parameter search. Figure 7 shows the plot of positive, negative and total score on the validation set over range of possible values of τ_single and τ_multiple. The optimal value of τ_multiple is 0.75 and of τ_single is 0.125. τ_single being less than 0.25 indicates that taking a majority vote is the</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 8: Plot showing performance of models compared to projected human performance.</p>
<p>best strategy for single-correct MCQs. However, this is not true for multi-correct MCQs, where a threshold of τ_multiple = 0.5 (as done originally) is sub-optimal. We assume that Integer and Numeric questions do not have negative marking. The final response for them is decided using a majority vote over responses. Table 6 shows scores with the optimal thresholds on the test set. We find that not answering when confidence is less than threshold increases the total score by about 4.3%.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Pos Score</th>
<th>Neg Score</th>
<th>Total Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4+CoT</td>
<td>109</td>
<td>43</td>
<td>66</td>
</tr>
<tr>
<td>GPT-4+CoT+SC</td>
<td>118</td>
<td>49</td>
<td>69</td>
</tr>
<tr>
<td>GPT-4+CoT+SC+Thresholding</td>
<td>111</td>
<td>39</td>
<td>72</td>
</tr>
</tbody>
</table>
<p>Table 6: Marks on the test set obtained when optimal thresholds derived from the val set are used.</p>
<h3>4.6 Estimating performance compared to humans</h3>
<p>Finally, we wish to estimate the performance of GPT-4 compared to humans. For this we use the 2023 examination paper since there is almost no probability of contamination. The 2023 paper was released on 4th June 2023 and contained 65 questions which were textual (rest 37 contained images). The total marks in the exam were 360. The score obtained by GPT-4 after confidence-thresholding on MCQs and regular aggregation on Integer and Numeric types is 49 out of 229. Assuming average difficulty levels in questions that were not included (because they contained images) is equal to ones that were, we normalize the projected human performance from 229 to 360 giving it a total of 77 marks out of 360. Projections indicates that this would place GPT-4 around the 80-90 percentile range. The results of JEE Advanced 2023 indicate that the top 10% score is approximately 97/360 and top 20% score is approximately 70/360. Figure 8</p>
<p>provides a comparison of the performance of LLMs along with human performance of the applicants.</p>
<h3>4.7 Has GPT-4 memorized some problems?</h3>
<p>In the era of internet-scale pre-training, it is very hard to ascertain whether a dataset has been used for training a particular model. Nevertheless, we tried to investigate the contamination of JEEBENCH. This was done by (i) searching for instances in JEEBENCH from publicly available internet corpora, (ii) prompting the LLM to complete the problem statement itself, when prompted with a prefix of the problem statement. Both these investigations suggest only minor (approx. 6\%) contamination. A detailed description of our contamination study can be found in Appendix A.8.</p>
<p>GPT-4+CoT+SC attains a score of 0.338 on Advanced 2023 questions, which is not very far from the aggregate performance of 0.396 on the remaining dataset. Given that 2023 questions are uncontaminated, we believe that the extent of contamination is quite low, and its performance on this dataset is a genuine indication of its current reasoning abilities. It is also noteworthy that some exams (e.g., JEE Advanced 2017) are easier, and GPT-4 does much better on it raising the aggregate score.</p>
<h2>5 Discussion</h2>
<p>The general performance trend demonstrates the efficacy of high-quality data, instruction fine-tuning, RLHF and parameter scaling in improving the reasoning capabilities of LLMs. For many problems, GPT-4 is able to give a sketch of the correct, humanlike solution that is impressive given the extent of reasoning involved in the problems. However, our analysis also reveals major areas where progress is needed. Although GPT-4 performs flawless logical and mathematical reasoning in some instances, sometimes it commits grave errors in trivial steps.</p>
<p>Errors in retrieval and application of concepts suggest an interesting research question: Can we augment an LLM such that it's generation is constrained by faithfulness to a set of facts? Such a system will demonstrate robustness in reasoning, critical for long-horizon tasks.</p>
<p>Physics problems in the benchmark often require an understanding of spatial reasoning. We found that while GPT-4's spatial reasoning is far from perfect. Appendix A. 7 provides an example where GPT-4 commits errors which might be attributed to its inability to reason spatially. With the release of
the multi-modal version of GPT-4, evaluating this aspect of Physics problems might be easier.</p>
<p>Finally, an LLM that understands its own confidence in an answer is a key missing piece, as highlighted by our experiments in the exam setting. Our simple post-hoc wrapper does slightly improve performance in this regard.</p>
<h2>6 Conclusion</h2>
<p>We present a challenging problem solving benchmark to evaluate large language models. We perform a detailed analysis of the performance of various LLMs on the benchmark, and identify areas of improvement in the best current LLMs. Our work raises interesting research directions such as mathematical logic-augmented GPT, multi-modal evaluations on GPT-4 and the decision-making capabilites of GPT in an exam setting. We hope that JEEBENCH guides future research in reasoning using LLMs. Our code and dataset are available at https://github.com/dair-iitd/jeebench.</p>
<h2>Acknowledgements</h2>
<p>We thank Dr. Parag Singla and the JEE Office for helping with getting approvals to use the dataset for research purposes. We thank Rishabh Ranjan for discussions around calibration and Mohd. Zaki for help in obtaining OpenAI API access for this work. The work is supported by grants from Google, Verisk, Microsoft, and Huawei, and the Jai Gupta chair fellowship by IIT Delhi. We thank the IIT-D HPC facility for its computational resources.</p>
<h2>Limitations</h2>
<p>Contamination is a big problem in the era of pretrained language models which have been trained on large web corpora. Therefore, its really hard to determine if a dataset has been seen. Determining the extent of contamination is also not easy, although we make an effort to quantify it. Evaluations against humans is also a slightly flawed process due to other limitations such as time pressure during the examination procedure. Additionally, this data's distribution is fixed to pre-college Physics, Chemistry and Mathematics. There are more gradations and difficulty levels at which the model can be evaluated which have not been tested as part of our analysis.</p>
<h2>References</h2>
<p>Pranjal Aggarwal, Aman Madaan, Yiming Yang, and Mausam. 2023. Let's sample step by step: Adaptiveconsistency for efficient reasoning and coding with llms. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model with state-of-the-art performance.</p>
<p>Daman Arora and Subbarao Kambhampati. 2023. Learning and leveraging verifiers to improve planning capabilities of pre-trained language models.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.</p>
<p>Jesse Dodge, Ana Marasovic, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In Conference on Empirical Methods in Natural Language Processing.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. In International Conference on Learning Representations.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. NeurIPS.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. 2016. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 887-896, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. Ce val: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322.</p>
<p>Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213. Curran Associates, Inc.</p>
<p>Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Let's verify step by step.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. 2017. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158-167, Vancouver, Canada. Association for Computational Linguistics.</p>
<p>Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, KaiWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS).</p>
<p>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-refine: Iterative refinement with self-feedback.</p>
<p>Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford alpaca: An instruction-following llama model. https:// github.com/tatsu-lab/stanford_alpaca.</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. 2022. Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop.</p>
<p>Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R. Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2023a. Scibench: Evaluating college-level scientific problem-solving abilities of large language models.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. ArXiv, abs/1707.06209.</p>
<p>Wei Zhao, Mingyue Shang, Yang Liu, Liang Wang, and Jingming Liu. 2020. Ape210k: A large-scale and template-rich dataset of math word problems.</p>
<p>Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. 2022. minif2f: a cross-system benchmark for formal olympiad-level mathematics. In International Conference on Learning Representations.</p>
<h2>Appendix</h2>
<h2>A Appendix</h2>
<h2>A Appendix</h2>
<h2>A. 1 Sub-topic wise distribution of questions in JEEBENCH</h2>
<p>Figure 9 provides the topic wise distribution of problems in JEEBENCHfor each subject.</p>
<h2>A. 2 Example problems from JEEBENCH</h2>
<p>Here we present a few problems from JEEBENCHalong with expert written solutions, with concepts being highlighted in yellow, their grounding being highlighted in violet, and the final algebraic manipulation highlighted in green. See Figures 10 for a Math Problem, Figure 11 for a Physics problem and Figure 12 for a Chemistry problem.</p>
<h2>A. 3 Exact Prompts for GPT models</h2>
<p>For each problem, we prompt the model with the required answer type and then the prompt. For different response types, the prompts are:</p>
<ol>
<li>Single-Correct: In this problem, only one option will be correct. Give a detailed solution and end the solution with the final answer.</li>
<li>Multi-Correct: In this problem, multiple options can be correct. Give a detailed solution and end the solution with the final answer.</li>
<li>Integer: In this problem, the final answer will be a non-negative integer. Give a detailed solution and end the solution with the final answer.</li>
<li>Numeric: In this problem, the final will be a numeric value. Give the numerical answer correct upto the 2nd decimal digit. Give a detailed solution and end the solution with the final answer.</li>
</ol>
<p>The entire prompt consists of the question prepended with the response type prompt. An example is:</p>
<p>In this problem, only one option will be correct.
Give a detailed solution and end the solution with the final answer.</p>
<p>Problem: A uniform wooden stick of mass 1.6 kg and length $l$ rests in an inclined manner on a smooth, vertical wall of height $h(&lt;l)$ such that a small portion of the stick extends beyond the wall.</p>
<p>The reaction force of the wall on the stick is perpendicular to the stick. The stick makes an angle of $30^{\circ}$ with the wall and the bottom of the stick is on a rough floor. The reaction of the wall on the stick is equal in magnitude to the reaction of the floor on the stick. The ratio $h / l$ and the frictional force $f$ at the bottom of the stick are $\left(g=10 \mathrm{~ms} \mathrm{~s}^{2}\right)$
(A) $\frac{h}{l}=\frac{\sqrt{3}}{16}, f=\frac{16 \sqrt{3}}{3} \mathrm{~N}$
(B) $\frac{h}{l}=\frac{3}{16}, f=\frac{16 \sqrt{3}}{3} \mathrm{~N}$
(C) $\frac{h}{l}=\frac{3 \sqrt{3}}{16}, f=\frac{8 \sqrt{3}}{3} \mathrm{~N}$
(D) $\frac{h}{l}=\frac{3 \sqrt{3}}{16}, f=\frac{16 \sqrt{3}}{3} \mathrm{~N}$</p>
<h2>A. 4 Errors made by GPT-4</h2>
<p>Here we present some errors made by GPT-4.</p>
<h2>A.4.1 Conceptual Error</h2>
<p>This is an example of a Conceptual Error
What is the total number of distinct $x \in[0,1]$ for which $\int_{0}^{x} \frac{t^{2}}{1+t^{4}} d t=2 x-1$ ?</p>
<h2>GPT-4 Response</h2>
<p>First, we can differentiate both sides of the equation with respect to $x$ to get rid of the integral. Using the Fundamental Theorem of Calculus, we have:</p>
<p>$$
\begin{aligned}
&amp; \frac{d}{d x}\left(\int_{0}^{x} \frac{t^{2}}{1+t^{4}} d t\right)=\frac{d}{d x}(2 x-1) \
&amp; \frac{x^{2}}{1+x^{4}}=2 \text { This is an error because if two func- } \
&amp; \text { tions are equal at a point, that doesn't imply that } \
&amp; \text { their derivatives are also equal. }
\end{aligned}
$$</p>
<p>Now, we want to find the number of distinct solutions for $x$ in the interval $[0,1]$. Let's first solve the equation for $x$ :</p>
<p>$$
\begin{aligned}
&amp; x^{2}=2\left(1+x^{4}\right) \
&amp; x^{2}-2 x^{4}-2=0
\end{aligned}
$$</p>
<p>This is a quartic equation, and we can try to find its roots. Notice that $x=0$ is not a solution, so we can divide both sides by $x^{2}$ :</p>
<p>$$
1-2 x^{2}-2 x^{-2}=0
$$</p>
<p>Now, let $y=x^{2}$. Then, we have:</p>
<p>$$
\begin{aligned}
&amp; 1-2 y-2 y^{-1}=0 \
&amp; 2 y^{2}-y+1=0
\end{aligned}
$$</p>
<p>This is a quadratic equation in $y$. We can use the quadratic formula to find its roots:</p>
<p>$$
\begin{aligned}
&amp; y=\frac{-(-1) \pm \sqrt{(-1)^{2}-4(2)(1)}}{2(2)} \
&amp; y=\frac{1 \pm \sqrt{1-8}}{4}
\end{aligned}
$$</p>
<p>Since the discriminant is negative $(1-8=-7)$, there are no real roots for $y$. Therefore, there are no real solutions for $x$ in the given equation.</p>
<p>Thus, the total number of distinct $x \in[0,1]$ for which $\int_{0}^{x} \frac{t^{2}}{1+t^{4}} d t=2 x-1$ is 0 .</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 9: Topic-wise distribution of problems in our dataset</p>
<p>Let $f:(0, \infty) \rightarrow R$ be a differentiable function such that $f'(x) = 2 - \frac{f(x)}{x}$ for all $x \in (0, \infty)$ and $f(1) \neq 1$. Then</p>
<p>(A) $\lim_{x \rightarrow 0+} f'(1/2) = 1$</p>
<p>(B) $\lim_{x \rightarrow 0-} x f(1/2) = 2$</p>
<p>(C) $\lim_{x \rightarrow 0+} x^2 f'(x) = 0$</p>
<p>(D) $|f(x)| \leq 2$ for all $x \in (0, 2)$</p>
<h3>Solution</h3>
<p>First, we are given that $f'(x) = 2 - \frac{f(x)}{x}$ for all $x \in (0, \infty)$. We can rewrite this as a first-order linear differential equation:</p>
<p>$$f'(x) + \frac{f(x)}{x} = 2.$$</p>
<p>To solve this differential equation, we can use an integrating factor. The integrating factor is given by $x^{f(1/2)} = x^{2k+1} = x$. Multiplying both sides of the differential equation by the integrating factor, we get:</p>
<p>$$xf'(x) + f(x) = 2x.$$</p>
<p>Now, we can notice that the left-hand side is the derivative of the product $xf(x)$:</p>
<p>$$\frac{d}{dx}(xf(x)) = 2x.$$</p>
<p>Integrating both sides with respect to $x$, we get:</p>
<p>$$xf(x) = x^2 + C,$$</p>
<p>where $C$ is the constant of integration. To find the value of $C$, we can use the given condition $f(1) \neq 1$. Plugging in $x = 1$, we get $f(1) = 1 + C$. Since $f(1) \neq 1$, we must have $C \neq 0$. Now, we can write the function $f(x)$ as $f(x) = x + \frac{C}{x}$, and $f'(x) = 1 - \frac{C}{x}$. For option A, $f'(1/2) = 1 - Cx^2$, therefore $\lim_{x \rightarrow 0+} f'(1/2) = 1$. For option B, $xf(1/2) = x(1/2 + Cx) = 1 + Cx^2$, therefore $\lim_{x \rightarrow 0+} xf(1/2) = 1$ or $C$. For option C, $x^2 f'(x) = x^2 - C$, therefore $\lim_{x \rightarrow 0+} x^2 f'(x) = -C \neq 0$. For option D, since $C$ can be any arbitrary non-zero value, $f(x)$ is unbounded in (0, 2).</p>
<p>Therefore, the answer is $C$. 4</p>
<p>Figure 10: A Math problem</p>
<h3>A.4.2 Grounding Error</h3>
<p>This is an example of a Grounding Error.</p>
<p>A plano-convex lens is made of a material of refractive index $n$. When a small object is placed 30 cm away in front of the curved surface of the lens, an image of double the size of the object is produced. Due to reflection from the convex surface of the lens, another faint image is observed at a distance of 10 cm away from the lens. Which of the following statement(s) is(are) true?</p>
<p>(A) The refractive index of the lens is 2.5</p>
<p>(B) The radius of curvature of the convex surface is 45 cm</p>
<p>(C) The faint image is erect and real</p>
<p>(D) The focal length of the lens is 20 cm</p>
<h3>GPT-4 Response</h3>
<p>A person measures the depth of a well by measuring the time interval between dropping a stone and receiving the sound of impact with the bottom of the well. The error in his measurement of time is $\delta T = 0.01$ seconds and he measures the depth of the well to be $L = 20$ meters. Take the acceleration due to gravity $g = 10 \text{~mm}^{-2}$ and the velocity of sound is 300 m s$^{-1}$. Then the fractional error in the measurement, $\delta L/L$, is closest to</p>
<p>$$|A| \approx 2\%$$</p>
<p>$$|B| \approx 1\%$$</p>
<p>$$|C| \approx 3\%$$</p>
<p>$$|D| \approx 5\%$$</p>
<h3>Solution</h3>
<p>The time taken to hear the sound of impact would be the time taken for the stone to reach the bottom of the well in addition to the time taken for the sound generated to reach the top of the well.</p>
<p>Since the stone is dropped from rest, we have that $L = \frac{1}{2} g t^2$, therefore $t = \sqrt{\frac{2}{u}}$. The time taken for the sound to reach the top of the well is $\frac{L}{2}$ where $v$ is the velocity of sound. Therefore,</p>
<p>$$T = \sqrt{\frac{2L}{g}} + \frac{L}{v}$$</p>
<p>Since we need to find the fractional error in the measurement of $L$, that is $\frac{\delta T}{2}$, we can differentiate both sides of the equation with respect to $L$. Using this, we get</p>
<p>$$\frac{\delta T}{\delta L} = \frac{1}{2} \sqrt{\frac{T}{Lg}} + \frac{1}{v}$$</p>
<p>$$\delta L = \frac{\delta T}{\frac{1}{2} \sqrt{\frac{T}{Lg}} + \frac{1}{v}}$$</p>
<p>$$\frac{\delta L}{L} = \frac{\delta T}{L(\frac{1}{2} \sqrt{\frac{T}{Lg}} + \frac{1}{v})} = \frac{\delta T}{\sqrt{\frac{T}{Lg}} + \frac{1}{v}}$$</p>
<p>$$\frac{\delta L}{L} = \frac{0.01}{\sqrt{\frac{2}{u}} \cdot 0.01} = \frac{0.01}{|L|} = \frac{15}{16} \approx 1\%$$</p>
<p>Figure 11: A Physics problem</p>
<p>First, let's consider the refraction of light through the lens. Since the image formed is double the size of the object, we can say that the image distance $(v)$ is twice the object distance $(u)$. So, $v = 2u = 60$ cm. This is an example of wrong grounding. The equation generated is inconsistent with the sign convention which must be taken care of for the other equations to be valid.</p>
<p>Now, let's use the lens maker's formula to find the focal length $(f)$ of the lens: $\frac{L}{f} = (n - 1)\left(\frac{\frac{1}{R_1}}{-\frac{1}{R_2}}\right)$.</p>
<p>Since the lens is plano-convex, one surface is flat, and its radius of curvature $(R_1)$ is infinite. The radius of curvature of the convex surface $(R_2)$ is negative because it is convex. So, the formula becomes: $\frac{1}{f} = (n - 1)\left(0 - \frac{1}{-R_2}\right)\frac{1}{f} = (n - 1)\frac{1}{R_2}$</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 12: A Chemistry problem</p>
<p>Now, let's use the lens formula to relate $u, v$, and $f: \frac{1}{f}=\frac{1}{u}+\frac{1}{v}$
Substituting the values of $u$ and $v: \frac{1}{f}=\frac{1}{30}+$ $\frac{1}{60}=\frac{1}{60}=\frac{1}{20}$
Now, we can substitute the expression for $\frac{1}{f}$ from the lens maker's formula: $(n-1) \frac{1}{R_{2}}=\frac{1}{20}$
Now, let's consider the reflection of light from the convex surface of the lens. Since the faint image is formed at a distance of 10 cm from the lens, we can use the mirror formula to relate the object distance $(u)$, image distance $\left(v^{\prime}\right)$, and radius of curvature $\left(R_{2}\right)$ for the convex surface: $\frac{1}{u}+\frac{1}{v}=\frac{2}{R_{2}}$
Substituting the values of $u$ and $v^{\prime}: \frac{1}{30}+\frac{1}{10}=\frac{2}{R_{2}}$ $\frac{1}{6}=\frac{2}{R_{2}}$
Now, we can solve for $R_{2}: R_{2}=\frac{2}{5}=12 \mathrm{~cm}$
Now, we can substitute the value of $R_{2}$ back into the equation relating $n$ and $R_{2}:(n-1) \frac{1}{12}=\frac{1}{20}$
Solving for $n: n=1+\frac{12}{30}=1+\frac{3}{5}=\frac{8}{5}=1.6$
Now, let's analyze the given options:
(A) The refractive index of the lens is 2.5 This is false, as we found the refractive index to be 1.6 .
(B) The radius of curvature of the convex surface is 45 cm This is false, as we found the radius of curvature to be 12 cm .
(C) The faint image is erect and real This is true. Since the faint image is formed due to reflection from the convex surface, it behaves like a convex mirror. For a convex mirror, the image is always erect and virtual.
(D) The focal length of the lens is 20 cm This is false, as we found the focal length to be $\frac{1}{20}$, which is 0.05 cm .</p>
<p>Final Answer: (C) The faint image is erect and
real.</p>
<h2>A.4.3 Computation Error</h2>
<p>Here is an example of a computation error.
Two inductors $L_{1}$ (inductance 1 mH , internal resistance $3 \Omega$ ) and $L_{2}$ (inductance 2 mH , internal resistance $4 \Omega$ ), and a resistor $R$ (resistance $12 \Omega$ ) are all connected in parallel across a 5 V battery. The circuit is switched on at time $t=0$. What is the ratio of the maximum to the minimum current $\left(I_{\max } / I_{\min }\right)$ drawn from the battery?</p>
<h2>GPT-4 Response</h2>
<p>Step 1: Analyze the circuit at $\mathrm{t}=0 \mathrm{At} \mathrm{t}=0$, the inductors behave like short circuits (since they have not had time to build up any magnetic field). Therefore, the current through each inductor is determined by their internal resistances. Let's call the currents through $L_{1}, L_{2}$, and $R$ as $I_{1}, I_{2}$, and $I_{R}$ respectively. Using Ohm's law, we can find these currents:</p>
<p>$$
\begin{aligned}
&amp; I_{1}=\frac{5 V}{3 \Omega}=\frac{5}{3} A \
&amp; I_{2}=\frac{5 V}{4 \Omega}=\frac{5}{4} A \
&amp; I_{R}=\frac{5 V}{12 \Omega}=\frac{5}{12} A
\end{aligned}
$$</p>
<p>The total current drawn from the battery at $\mathrm{t}=0$ is the sum of these currents:</p>
<p>$$
I_{\max }=I_{1}+I_{2}+I_{R}=\frac{5}{3}+\frac{5}{4}+\frac{5}{12}=\frac{20}{12} A
$$</p>
<p>Wrong calculation, should be $\frac{20}{12}$
Step 2: Analyze the circuit at steady state As time goes to infinity, the inductors will behave like open circuits (since their magnetic fields have reached their maximum and no more current can flow through them). In this case, the only current flowing in the circuit is through the resistor $R$. Using Ohm's law, we can find this current:</p>
<p>$$
I_{\min }=\frac{5 V}{12 \Omega}=\frac{5}{12} A
$$</p>
<p>Step 3: Find the ratio of maximum to minimum current Now we can find the ratio of the maximum to the minimum current:</p>
<p>$$
\frac{I_{\max }}{I_{\min }}=\frac{\frac{20}{12} A}{\frac{5}{12} A}=\frac{20}{5}=4
$$</p>
<p>So the ratio of the maximum to the minimum current drawn from the battery is 4 .</p>
<h2>A. 5 GPT-4 verifying its own response</h2>
<p>Figures 13, and 14 demonstrate various cases which arise when GPT-4 critiques its own output.</p>
<p>A computer producing factory has only two plants $T_{1}$ and $T_{2}$. Plant $T_{1}$ produces $20 \%$ and plant $T_{2}$ produces $80 \%$ of the total computers produced. $7 \%$ of computers produced in the factory turn out to be defective. It is known that $P$ (computer turns out to be defective given that it is produced in plant $T_{1}$ )
$=10 P($ computer turns out to be defective given that it is produced in plant $T_{2}$ ),
where $P(E)$ denotes the probability of an event $E$. A computer produced in the factory is randomly selected and it does not turn out to be defective. Then the probability that it is produced in plant $T_{2}$ is
(A) $\frac{99}{21}$
(B) $\frac{17}{22}$
(C) $\frac{13}{23}$
(D) $\frac{15}{24}$</p>
<h2>Answer</h2>
<p>First, let's define the events: - $D$ : the computer is defective - $N$ : the computer is not defective - $T_{1}$ : the computer is produced in plant $T_{1}$ - $T_{2}$ : the computer is produced in plant $T_{2}$</p>
<p>We are given the following probabilities: $-P\left(T_{1}\right)=0.2-P\left(T_{2}\right)=0.8-$ $P(D)=0.07-P\left(D\left(T_{1}\right)=10 P\left(D\left(T_{2}\right)\right.\right.$
We want to find the probability $P\left(T_{2} ; N\right)$, which is the probability that a computer is produced in plant $T_{2}$ given that it is not defective.</p>
<p>We can use the Bayes' theorem to find this probability:
$P\left(T_{2} ; N\right)=\frac{P\left(N\left(T_{2}\right)=T_{2}\right)}{P(D)}=$
First, let's find $P\left(N\left(T_{2}\right)\right.$. We know that $P\left(D\left(T_{2}\right)+P\left(N\left(T_{2}\right)=1\right.\right.$, so:
$P\left(N\left(T_{2}\right)=1-P\left(D\left(T_{2}\right)\right.\right.$
Now, let's find $P\left(D\left(T_{2}\right)\right.$. We can use the given information that $P\left(D\left(T_{1}\right)=\right.$ $10 P\left(D\left(T_{2}\right)\right.$ and the fact that $P(D)=P\left(D\left(T_{1}\right) P\left(T_{1}\right)+P\left(D\left(T_{2}\right) P\left(T_{2}\right)\right.\right.$.
$0.07=10 P\left(D\left(T_{2}\right)(0.2)+P\left(D\left(T_{2}\right)(0.8)\right.\right.$
Solving for $P\left(D\left(T_{2}\right)\right.$, we get:
$P\left(D\left(T_{2}\right)=\frac{0.2}{0.8}=\frac{1}{20}\right.$
Now we can find $P\left(N\left(T_{2}\right)\right.$:
$P\left(N\left(T_{2}\right)=1-P\left(D\left(T_{2}\right)=1-\frac{1}{20}=\frac{20}{20}\right.\right.$
Next, let's find $P(N)$. We know that $P(N)+P(D)=1$, so:
$P(N)=1-P(D)=1-0.07=0.93$
Finally, we can find $P\left(T_{2} ; N\right)$ using the Bayes' theorem:
$P\left(T_{2} ; N\right)=\frac{P\left(N\left(T_{2}\right) P\left(T_{2}\right)=\frac{20}{20} 0.8}{}=20$
So the probability that a computer is produced in plant $T_{2}$ given that it is not defective is $\frac{74}{23}$.</p>
<p>The correct answer is (C) $\frac{74}{23}$.
Edit
The given solution is correct, and the answer is (C) $\frac{74}{23}$.
Figure 13: GPT-4 identifying a correct solution as correct.</p>
<h2>A. 6 GPT-4 deciding when to attempt</h2>
<p>To evaluate if GPT-4 could decide whether to refuse attempt given the negative penalty for being wrong, we prompt it with the following for Single-Correct MCQ questions. If the answer is wrong, you'll be given -1 marks. If the answer is correct, you'll be given +3 marks. If you're unsure of the answer, you can skip the question, and you'll be given 0 marks.</p>
<p>The following is used for Multi-Correct MCQs If any of the options in the final answer is wrong, you'll be given -2 marks. If all the options are correct, you'll be given +4 marks. If some of the options are correct, you'll be given +1 for each correct option. If you're unsure of the answer, you can skip the question, and you'll be given 0 marks.</p>
<p>Question
If $I=\sum_{k=1}^{K} \frac{1}{k+1} \cdot \frac{k+1}{1+k+1} d x$, then
$|A| I&gt;\log <em e="e">{e} 99$
$|B| I&lt;\log </em> 99$
$|C| I&lt;\frac{10}{10}$
$|D| I&gt;\frac{20}{10}$</p>
<h2>Answer</h2>
<p>First, let's evaluate the integral for a single term in the sum:
$\frac{1}{k+1} \cdot \frac{k+1}{1+k+1} d x$
We can perform a substitution: let $u=x+1$, so $x=u-1$ and $d x=d u$.
The limits of integration become $u=k+1$ to $u=k+2$. The integral becomes: $\frac{1}{k+1} \cdot \frac{k+1}{1+k+1} d u$
Now, we can perform partial fraction decomposition on the integrand: $\frac{u-1}{1+k+1}=\frac{u}{u-1}+\frac{u}{u}$
Multiplying both sides by $(u-1) u$ to clear the denominators, we get: $k+1=A(u)+B(u-1)$
To solve for $A$ and $B$, we can use the following system of equations:</p>
<ol>
<li>Set $u=1$ : $k+1=B(1-1)=B=02$. Set $u=0: k+1=A(-1) \Rightarrow$ $A=-(k+1)$
So, the integrand becomes:
$\frac{k+1}{1+k+1}=\frac{-1-k+1}{1+k+1}+\frac{u}{1}+\frac{-1-k+1}{1+k+1}$
Now, we can integrate:
$\frac{1}{k+1} \cdot \frac{-1-k+1}{1+k+1} d u=-(k+1) \frac{1-k+1}{1+k+1} \cdot \frac{1}{1+k} d u$
This integral is a simple natural logarithm:
$\frac{-(k+1)\left(\ln (u-1)\right) k+1}{1+(k+1)\left(\ln (k+1)-\ln k\right)}$
Now, we can substitute this back into the original sum:
$I=\sum_{k=1}^{K}(-1-k+1)\left(\ln (k+1)-\ln k\right))$
Notice that this is a telescoping sum:
$I=-((2 \ln 2-\ln 1)+(3 \ln 3-2 \ln 2)+\cdots+(99 \ln 99-98 \ln 98))$
$I=-(\ln 1+\ln 2+\cdots+\ln 98+\ln 99)=-\ln (1 \cdot 2 \cdots \cdot 98 \cdot 99)$
Now, let's compare $I$ to $\ln e^{20}$ :
$I=-\ln (1 \cdot 2 \cdots \cdot 98 \cdot 99)&lt;\ln e^{20}$
So, $I&lt;\ln e^{20}$, which means option $|B|$ is correct.
Now, let's compare $I$ to $\frac{20}{10}$ :
Since $I=-\ln (1 \cdot 2 \cdots \cdot 98 \cdot 99)$, we know that $I$ is negative. Therefore, $I&lt;\frac{20}{10}$, which means option $|C|$ is correct.
Thus, the final answer is $|B|$ and $|C|$.
Grab: IID
Edit
The given solution is incorrect. The error occurs in the step where the integral is evaluated:
$\frac{1}{k+1} \cdot \frac{1}{k+1} d u=-(k+1)\left[\ln (u-1)\right] k+1=-(k+1)\left(\ln (k+1)-\ln k\right)$
The correct evaluation should be:
$\frac{1}{k+1} \cdot \frac{1}{k+1}\left(d u=-(k+1)\left[\ln (u-1)\right] k+1=-(k+1)\left(\ln (k+2)-\ln (k+1)\right)\right.$
Now, we can substitute this back into the original sum:
$I=\sum_{k=1}^{K}(-(k+1)\left(\ln (k+2)-\ln (k+1)\right))$
Notice that this is a telescoping sum:
$I=-((2 \ln 3-\ln 2)+(3 \ln 4-2 \ln 3)+\cdots+(99 \ln 100-98 \ln 99))$
$I=-\left(\ln 2+\ln 3+\cdots+\ln 99+\ln 100\right)=-\ln (2 \cdot 2 \cdots \cdot 99 \cdot 100)$
Now, let's compare $I$ to $\ln e^{20}$ :
$I=-\ln (2 \cdot 3 \cdots \cdot 99 \cdot 100)&lt;\ln e^{20}$
So, $I&lt;\ln e^{20}$, which means option $|B|$ is correct.
Now, let's compare $I$ to $\frac{20}{10}$ :
Since $I=-\ln (2 \cdot 3 \cdots \cdot 99 \cdot 100)$, we know that $I$ is negative. Therefore, $I&lt;\frac{20}{10}$, which means option $|C|$ is correct.
Thus, the final answer is $|B|$ and $|C|$.
Figure 14: GPT-4 identifying a correct step as wrong and making it incorrect</li>
</ol>
<h2>A. 7 Can GPT-4 reason spatially</h2>
<p>Figures 15 and 16 show an example problem for which GPT-4's response indicates inability to ground physical concepts spatially.</p>
<h2>A. 8 Is JEEBENCH contaminated?</h2>
<p>We note that several web-pages on the internet do provide problems and solutions to the JEE exam (for example, here). Therefore, it is possible to find the question and the answer during web-scale training. Unfortunately, this would be true for any dataset that has been created from questions from competitive exams such as (Hendrycks et al., 2021b).</p>
<p>To check if the dataset is indeed contaminated, we perform a careful analysis along the following</p>
<p>A uniform wooden stick of mass 1.6 kg and length $l$ rests in an inclined manner on a smooth, vertical wall of height $h(&lt;l)$ such that a small portion of the stick extends beyond the wall. The reaction force of the wall on the stick is perpendicular to the stick. The stick makes an angle of $30^{\circ}$ with the wall and the bottom of the stick is on a rough floor. The reaction of the wall on the stick is equal in magnitude to the reaction of the floor on the stick. The ratio $b / l$ and the fruttional force $f$ at the bottom of the stick are</p>
<p>$$
\left(g=10 \mathrm{~ms} \mathrm{e}^{2}\right)
$$</p>
<p>(A) $\frac{a}{l}=\frac{a^{2}}{2 l}, f=\frac{10 a^{2}}{2} \mathrm{~N}$
(B) $\frac{b}{l}=\frac{b^{2}}{2 l}, f=\frac{10 a^{2}}{2} \mathrm{~N}$
(C) $\frac{b}{l}=\frac{b^{2}}{2 l}, f=\frac{6 a^{2}}{2} \mathrm{~N}$
(D) $\frac{a}{l}=\frac{2 a^{2}}{2} l, f=\frac{10 a^{2}}{2} \mathrm{~N}$</p>
<p>Step 1: Draw a diagram of the situation.
Draw a vertical wall on the left side and a horizontal floor on the bottom. Place the wooden stick inclined at an angle of $30^{\circ}$ with the wall, with its bottom end on the floor and its top end touching the wall. Label the point where the stick touches the wall as A, the point where the stick touches the floor as B, and the center of mass of the stick as C.</p>
<p>Step 2: Identify the forces acting on the stick.
There are four forces acting on the stick: 1. The gravitational force acting downward at the center of mass C, which is equal to the mass of the stick times the acceleration due to gravity: $m g=1.6 \times 10=10 \mathrm{~N}$. 2. The normal force exerted by the wall on the stick at point A, acting perpendicular to the stick. 3. The normal force exerted by the floor on the stick at point B, acting perpendicular to the floor. 4. The frictional force exerted by the floor on the stick at point B, acting parallel to the floor and opposite to the direction of the stick's motion.</p>
<p>Step 3: Apply the equilibrium conditions.
Since the stick is in equilibrium, the net force and net torque acting on it must be zero. This gives us two equations:</p>
<ol>
<li>The net force in the horizontal direction is zero: $f=N_{A} \sin 30^{\circ}$. 2. The net force in the vertical direction is zero: $N_{B}=N_{A} \cos 30^{\circ}+\mathrm{mg}$. 3. The net torque about point B is zero: $N_{A} \cos 30^{\circ}+\frac{1}{4}=\mathrm{mg} \times 6$.</li>
</ol>
<p>Step 4: Solve for the unknowns.
We are given that the reaction force of the wall on the stick is equal in magnitude to the reaction of the floor on the stick, i.e., $N_{A}=N_{B}$. Using this information, we can solve the above equations for the unknowns $b / l$ and $f$.</p>
<p>From equation 2, we have $N_{B}=N_{A} \cos 30^{\circ}+\mathrm{mg}$. Since $N_{A}=N_{B}$, we can write this as $N_{A}=N_{A} \cos 30^{\circ}+\mathrm{mg}$. Solving for $N_{A}$, we get $N_{A}=\frac{m g}{1-\cos 30^{\circ}}=$ $\frac{1}{1-\frac{10}{2}}=\frac{10}{2} \mathrm{~N}$.</p>
<p>Figure 15: Error made by GPT-4 in understanding physical concepts. In this example, correct form of equation 1 should be $f=N_{A} \cos 30^{\circ}$. GPT-4 fails to spatially ground the concept of direction in a 2D environment.
axes:</p>
<ol>
<li>We search the C4 dataset (Dodge et al., 2021) using 50 randomly sampled questions from Physics and Chemistry(we do not use Math question since they are generally more LaTeX heavy and exact matches would be harder to find). We were unable to find any documents containing sufficiently long substrings from the questions of the dataset.</li>
<li>We search the Common Crawl URL Index http://urlsearch.commoncrawl.org/ CC-MAIN-2023-23/ for several popular sources which release consolidated full length solutions to exams from the JEE exams. Out of 500+ questions in our dataset, we could find web pages containing the solution of 30 questions [19 chemistry, 11 math] in the latest Common Crawl 2023 index. This is less than $6 \%$ of the dataset. We study the effects this has on the final evaluation and observe that removing these questions results in only a</li>
</ol>
<p>Question
A farmer $F_{1}$ has a land in the shape of a triangle with vertices at $P(0,0), Q(1,1)$ and $R(2,0)$. From this land, a neighbouring farmer $F_{2}$ takes away the region which lies between the side $P Q$ and a curve of the form $y=x^{n}(n&gt;1)$. If the area of the region taken away by the farmer $F_{2}$ is exactly $30 \%$ of the area of $\triangle P Q R$, then what is the value of $n$ ?</p>
<h2>Answer</h2>
<p>Step 1: Find the area of triangle PQR.
The area of a triangle with vertices at $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)$, and $\left(x_{3}, y_{3}\right)$ can be found using the formula:</p>
<p>$$
\text { Area }=\frac{1}{3}\left(x_{1}\left(y_{1}-y_{3}\right)+x_{2}\left(y_{1}-y_{1}\right)+x_{3}\left(y_{1}-y_{2}\right)\right)
$$</p>
<p>Plugging in the coordinates of P, Q, and R, we get:</p>
<p>$$
\text { Area }_{\text {PQR }}=\frac{1}{6}(0(1-0)+1(0-0)+2(0-1))=\frac{1}{2} \mid-2 \mid=1
$$</p>
<p>Step 2: Find the area of the region taken away by farmer $F_{2}$.
Since the area of the region taken away by farmer $F_{2}$ is exactly $30 \%$ of the area of $\triangle P Q R$, we have:</p>
<p>$$
\text { Area }<em 1="1">{F</em>=0.3 \times 1=0.3
$$}}=0.3 \times \text { Area }_{\text {PQR }</p>
<p>Step 3: Set up an integral to find the area under the curve $y=x^{n}$ between P and Q .</p>
<p>The area under the curve $y=x^{n}$ between P and Q can be found using the integral:</p>
<p>$$
\text { Area }<em 0="0">{\text {curve }}=\int</em> d x
$$}^{1} x^{n</p>
<p>Step 4: Calculate the integral and set it equal to the area taken away by farmer $F_{2}$ :</p>
<p>$$
\int_{0}^{1} x^{n} d x=\frac{1}{n+1} \int_{0}^{1}=\frac{1}{n+1}-\frac{1}{n+1}-\frac{1}{n+1}
$$</p>
<p>Now, we set this equal to the area taken away by farmer $F_{2}$ :</p>
<p>$$
\frac{1}{n+1}=\frac{0.3}{n}
$$</p>
<p>Step 5: Solve for n.
To find the value of n, we simply solve the equation:</p>
<p>$$
\begin{aligned}
&amp; n+1=\frac{0.3}{n+1}=\frac{10}{9} \
&amp; n=\frac{10}{7}-1=\frac{1}{3}
\end{aligned}
$$</p>
<p>Figure 16: Error made by GPT-4 in understanding which curves to take area between. Here it is taking area between the curve and x -axis. However, the question intended between curve and the $x=y$ line. This indicates that GPT-4 might be weak in 2D reasoning from purely text-based prompts.
minor decrease in overall scores for most models. This suggests that the contamination doesn't impact the scores greatly. For instance, GPT4+CoT performance goes down from 0.350 to 0.347 . The average performance on the 30 questions is 0.392 (this could also be attributed to a much higher proportion of chemistry questions which GPT-4 is better at). It is noteworthy that even though these URLs are present in Common Crawl, that doesn't imply they have been trained on since LLM training doesn't even complete 1 epoch over the pretraining data generally.
3. Since proprietary LLMs do not disclose their exact data recipe, we wish to see if GPT4 memorized the questions present in the dataset? Note that this is a very challenging problem in itself. Inspired by contemporary methods ${ }^{6}$ we take the following heuristic approach: we prompt GPT-4 with a prefix of the question and instruct it to complete the remaining question providing context of the year (for eg, Complete this question from JEE Advanced 2017). In this, we check if the</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>model is able to generate "precarious" data, that is, data which cannot be predicted from the context. For example, some numerical data provided, or the options of the questions. We use the same 50 questions sampled above and prompt GPT-4. Our evaluation suggested GPT-4 was unable to generate any such responses.</p>
<p>These facts suggest that the extent of contamination, if any, is very low and that is not detrimental to the paper. We will add these analyses in the final paper to clear any aspersions regarding possible contamination.</p>
<p>We additionally want to emphasize that our benchmark is dynamic in the sense that a new set of 40-50 new uncontaminated problems can be added to it annually. This would allow a reliable test of the problem solving abilities of future LLMs as more and more data goes into training them.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/hitz-zentroa/lm-contamination&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
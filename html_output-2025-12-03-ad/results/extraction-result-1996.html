<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1996 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1996</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1996</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-280708527</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2508.16334v1.pdf" target="_blank">From Linear to Hierarchical: Evolving Tree-structured Thoughts for Efficient Alpha Mining</a></p>
                <p><strong>Paper Abstract:</strong> Alpha mining, which discovers signals that predict asset returns, has long been attractive for automatic quantitative investment. This problem is typically formulated as a tree-based symbolic regression with handcrafted market data features and arithmetic operators. Unfortunately, existing symbolic methods are concerned with computational inefficiency and dependence on prior knowledge. Recent implementation of Large Language Models (LLMs) show that they can automatically generate executable codes for various tasks efficiently, thus can be considered as a new promising way for alpha mining. Specifically, LLMs-driven methods evolve a set of heuristics, including thoughts and codes, where the thoughts are usually represented as plain-text prompts of codes. Unfortunately, trivially adopting them in alpha mining ignores the fact that alphas are with hierarchical tree structures. This paper introduces Tree-structured thought Evolution (TreEvo), which evolves hierarchical reasoning ideas solely at the thought level. Experiments on four real-market datasets demonstrate that TreEvo can obtain better alphas with much less computational time and human expert efforts. And this superiority hardly holds without the tree-structured thoughts and the compatible evolutionary operators.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1996.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1996.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TreEvo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-structured thought Evolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven evolutionary framework that represents candidate solutions (alphas) as hierarchical tree-structured thoughts and applies semantic-level evolutionary operators (crossover, mutation, pruning) at the thought-tree level; LLM (Qwen3-Max) is used to generate offspring thoughts and executable code for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TreEvo</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (semantic-level evolutionary operators on tree-structured thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Represents each candidate as a hierarchical tree T of reasoning units. Evolution is performed on the thought trees using semantic-level crossover (subtree exchange), mutation (subtree replacement via LLM generation), and pruning (removal/replacement of redundant subtrees), all mediated by an LLM (Qwen3-Max) which generates new thought trees and then executable code for each thought. The LLM is used in a prompt-driven fashion; there is no report of fine-tuning of the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Alpha mining / symbolic regression for cross-sectional stock return prediction on four real-market datasets: CSI300, CSI500 (Chinese A-shares) and SPX, DJI (U.S. markets); raw features: OHLC, volume, vwap; objective metric: Information Coefficient (IC) and RankIC.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Traditional symbolic approaches (GP, XGBoost, LightGBM, AlphaGen, QFR) and LLM-driven EAs (EoH, ReEvo); also ablation variants TReEvo (code evolution with tree-structured thoughts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Averaged IC (over 3 runs, ablation setup): 0.0601 (CSI300) and 0.0726 (CSI500). Paper also reports TreEvo achieves at least +4.59% IC improvement on CSI300 and +4.80% on CSI500 vs the strongest traditional baseline, and reports ~20% IC improvement on CSI300, CSI500 and DJI in other comparisons; TreEvo often yields higher cumulative returns (e.g., +16.91% vs strongest baseline in one Top-50/Drop-5 backtest on CSI300).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>Traditional symbolic methods required much larger evaluation budgets: XGBoost, LightGBM, AlphaGen, QFR ≥ 1000 evaluations vs TreEvo 200; GP used 20000 evaluations. Numeric ICs for each traditional baseline not consistently reported in the main text; TreEvo reports percentage improvements over the strongest baseline (>=4.59% on CSI300, >=4.80% on CSI500).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Paper discusses that LLMs bring massive prior knowledge that narrows the search space; also warns of overfitting risk when extending evaluation budgets. No direct empirical measurement of pretrained-model bias on novel problems is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>TreEvo uses a small time budget of 200 evaluations to obtain best alpha. Comparators: XGBoost/LightGBM/AlphaGen/QFR used ≥1000 evaluations (≥5x), GP used 20,000 evaluations (100x) — the paper reports 5x–100x acceleration in required evaluations/time for TreEvo vs traditional methods.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>All LLM-driven methods in experiments used Qwen3-Max (a general pre-trained LLM); no controlled comparison between domain-specific and general pretraining was reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Ablation comparing ReEvo (code-only), TReEvo (code evolution but with tree-structured thoughts), and TreEvo (tree thoughts + semantic operators) reports averaged ICs (three runs): ReEvo = 0.0405 (CSI300) / 0.0468 (CSI500); TReEvo = 0.0546 / 0.0631 (CSI300/CSI500) — +34.8% over ReEvo; TreEvo = 0.0601 / 0.0726 — +10.07% and +15.06% over TReEvo respectively. Conclusion: the tree-structured thought representation provides the largest single performance gain; semantic-level operators provide additional improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Qualitative: LLM priors expand expressiveness beyond a manually predefined symbol set while simultaneously narrowing effective search trajectories via prior knowledge; tree-structured thoughts explicitly model compositional hierarchy, enabling semantically meaningful recombination and more structured exploration compared to flat representations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>LLM is used iteratively each generation to produce new thoughts and code; no report of online fine-tuning or gradient-based adaptation of the LLM during evolution — adaptation is via the evolutionary selection loop and prompt-driven generation only.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Risk of overfitting if evaluation budget is increased; in one case TreEvo was slightly inferior to GP on RankIC (suggesting traditional symbolic methods can outperform given large budgets); TreEvo was behind EoH on IC on the SPX dataset (paper attributes this to outliers in IC values).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>1) Representation matters most: evolving tree-structured thoughts yields much larger performance gains (~35% over code-only evolution) than merely changing operators. 2) Semantic-level (thought-tree) operators further improve performance beyond representation alone (~10–15% additional IC). 3) LLM-driven approaches achieve comparable or better predictive performance with dramatically fewer evaluations (5x–100x), indicating prior-knowledge-guided search narrows effective hypothesis space. 4) There is a compute/overfitting trade-off: the faster convergence advantage can become a liability if search is extended without regularization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1996.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1996.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReEvo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReEvo (code-only LLM-driven EA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An existing LLM-driven evolutionary algorithm that evolves executable code (candidate programs) directly using LLM generation; treated in the paper as a baseline that evolves codes only (no explicit thought-level representation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ReEvo</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (code-only evolutionary operators / code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Evolves executable code candidates via LLM prompting and selection; in the paper it is run with the same LLM (Qwen3-Max) and population settings as other LLM-driven baselines, but it does not evolve separate thought representations — only codes are produced and evolved.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Alpha mining / symbolic regression on CSI300, CSI500, SPX, DJI with OHLC+volume+vwap features; IC and RankIC used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to TreEvo, EoH, traditional symbolic methods (GP, XGBoost, LightGBM, AlphaGen, QFR) and the TReEvo ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Averaged ICs (three-run ablation reported in paper): 0.0405 (CSI300) and 0.0468 (CSI500). ReEvo generally converges slowest among the three LLM-driven methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>ReEvo shared the same small time budget (200 evaluations) as TreEvo in comparisons; nevertheless it converged more slowly, implying less efficient use of evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Compared with TReEvo and TreEvo: replacing linear plain-text thought with tree-structured thought (TReEvo) produced ~34.8% IC improvement over ReEvo, indicating ReEvo's representation is a limiting factor.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>ReEvo's code-only evolution is described as less adaptive and slower to converge, suggesting a less constrained or less semantically guided search trajectory compared to thought-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Slow convergence and weaker adaptation in the evolution loop compared to thought-evolving methods; lower IC in ablation compared to tree-structured variants.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Code-only LLM evolution underperforms methods that evolve explicit thought representations, indicating that adding structured intermediate representations (thoughts/trees) provides stronger inductive bias and more efficient search than directly evolving code.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1996.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1996.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EoH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evolve-outer-Heuristics (EoH) / thought-and-code evolution</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven EA that jointly evolves thoughts and generated code, where thoughts are represented as linear/plain-text prompts (not hierarchical trees); used as a stronger LLM-driven baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>EoH</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (thought-and-code evolution; plain-text thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Evolves both thoughts (plain-text prompts) and corresponding generated code using an LLM. Thoughts are linear/plain-text prompt representations (not hierarchical trees), and the method separately evolves thought prompts and the codes they generate.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Alpha mining / symbolic regression on CSI300, CSI500, SPX, DJI; IC / RankIC used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against ReEvo (code-only), TreEvo (tree-structured thought evolution), and traditional symbolic methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Paper states EoH consistently beats ReEvo across datasets. Exact numeric ICs per dataset are provided in the paper's tables (Table 2) but are not cleanly reproduced in the main text; EoH is sometimes competitive with TreEvo (e.g., EoH outperformed TreEvo on IC for SPX according to the authors, though TreEvo had better RankIC on that dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>EoH used the same small evaluation budget as TreEvo and ReEvo in the experiments (200 evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Compared to ReEvo and TreEvo, evolving thoughts (even linear/plain-text) along with code (EoH) yields clear gains over code-only evolution; TreEvo (tree thoughts) frequently outperforms EoH, highlighting the benefit of hierarchical thought structure.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Plain-text thought representation provides beneficial inductive bias over code-only evolution but lacks the hierarchical compositionality that TreEvo's tree-structured thoughts provide.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>May be sensitive to dataset-specific characteristics; EoH outperforms TreEvo on one dataset's IC (SPX) possibly due to outlier IC behavior, indicating that performance can vary with dataset idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Evolving thoughts (even as plain-text) along with codes is superior to evolving codes alone; however, hierarchical (tree) thought representations provide additional, consistent improvements over linear thought representations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1996.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1996.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TReEvo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TReEvo (ReEvo with tree-structured thoughts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation variant that uses tree-structured thoughts as representation but retains code-only evolution operators (i.e., it evolves codes while using tree-structured thought scaffolding) to isolate the effect of representation from operator design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>TReEvo</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>hybrid (tree-structured thought representation + code-evolution operators)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Uses tree-structured thought representations as prompts/blueprints but otherwise evolves/operates at the code level (same evolutionary operators as ReEvo). Serves as an ablation to measure the effect of representation alone.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Alpha mining on CSI300 and CSI500 (ablation experiments reported); IC metric.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to ReEvo (code-only), TreEvo (tree thoughts + semantic operators), and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>Averaged ICs (three-run ablation): 0.0546 (CSI300) and 0.0631 (CSI500). This is ~34.8% better than ReEvo on the same datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Same small evaluation budget (200 evaluations) as other LLM-driven methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>TReEvo vs ReEvo: large gains (~34.8% IC), showing representation (tree-structured thought) is the major contributor to improvements. TReEvo vs TreEvo: TreEvo further improves by ~10–15% via semantic-level tree operators.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Shows that adding hierarchical structure to prompts/thoughts significantly constrains and guides the search toward better regions of hypothesis space even when operators remain code-level.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Tree-structured thought representation alone explains a large fraction of the performance improvement; proper semantic-level operators further increase performance. Representation is a critical inductive bias.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1996.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1996.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional GP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Programming (symbol-level evolutionary operators)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Conventional genetic programming for symbolic regression that manipulates symbol-level program trees using subtree crossover and mutation; used as a major traditional baseline in alpha-mining comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Genetic Programming (GP)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>traditional GP (symbol-level operators)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Standard GP uses symbol-level representations (program/expression trees) and performs low-level genetic operators (random subtree crossover, point mutation, etc.) on syntax-level nodes; in this domain GP searches over alphas composed from a limited pre-defined set of arithmetic operators and handcrafted features.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Alpha mining / symbolic regression on CSI300, CSI500, SPX, DJI; compared using IC/RankIC and cumulative return backtests.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to TreEvo and other LLM-driven methods as well as other symbolic approaches (XGBoost, LightGBM, AlphaGen, QFR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Paper characterizes traditional GP as suffering from large search space and dependence on handcrafted priors which can introduce bias and inefficiency, but does not provide empirical pretrained-bias measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>GP consumed 20,000 evaluations in the paper's baseline settings (100x the TreEvo budget of 200 evaluations); authors report TreEvo obtains good solutions with ≈100x fewer evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>GP manipulates symbol-level representations which can create syntactically invalid or semantically incoherent offspring; authors argue semantic-level thought operators (TreEvo) produce more stable and expressive search dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High computational cost; slow convergence; heavy dependence on pre-specified operator sets and handcrafted features; potential to outperform LLM methods when given sufficiently large budgets (paper notes GP slightly outperforms TreEvo on RankIC in one case).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Traditional GP can find strong solutions given very large evaluation budgets, but is computationally inefficient and brittle compared to LLM-driven semantic-level evolution when budgets are constrained; this supports a theory that powerful priors (LLMs) plus structured representations (trees of thought) provide stronger inductive biases for rapid search.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1996.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1996.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Traditional Symbolic Approaches (XGBoost/LightGBM/AlphaGen/QFR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional symbolic / ML baselines: XGBoost, LightGBM, AlphaGen, QFR</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of widely-used traditional methods for alpha discovery including gradient-boosted trees (XGBoost, LightGBM) and domain-specific symbolic factor generators (AlphaGen, QFR), used as baselines in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>XGBoost / LightGBM / AlphaGen / QFR</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>hand-designed / traditional symbolic / gradient-boosted-tree methods</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>XGBoost and LightGBM are gradient-boosted decision tree ensembles; AlphaGen and QFR are symbolic alpha mining frameworks using handcrafted operators and features. In the experiments they use a limited set of 22 arithmetic operators applied to the 6 raw features (OHLC, volume, vwap).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Alpha mining on CSI300, CSI500, SPX, DJI; IC and RankIC metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to TreEvo and LLM-driven EAs (ReEvo, EoH).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td>These baselines used ≥1000 evaluations in the experiments (≥5x the TreEvo budget); TreEvo reports IC improvements over the strongest baseline of at least +4.59% (CSI300) and +4.80% (CSI500). Exact per-method IC numbers are not fully enumerated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Paper notes that handcrafted heuristics leverage expert prior knowledge but can be restrictive and introduce biases; no quantitative bias measurements reported.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Reported to consume at least 1000 evaluations vs TreEvo's 200 (≥5x more).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>These methods operate within a manually pre-defined operator/feature space (22 arithmetic operators + 6 raw features in experiments), contrasting with LLM-driven methods' open expressiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Slower to converge and less expressive due to restricted symbol sets; reliance on prior handcrafted features and operators can limit discovery of novel alphas.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>LLM-driven methods (especially with tree-structured thoughts) can outperform or match traditional symbolic methods while using substantially fewer evaluations, suggesting that learned priors can act as powerful heuristics that accelerate search across large hypothesis spaces.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reevo: Large language models as hyper-heuristics with reflective evolution <em>(Rating: 2)</em></li>
                <li>Large language models as surrogate models in evolutionary algorithms: A preliminary study <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Evolution of heuristics: Towards efficient automatic algorithm design using large language model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1996",
    "paper_id": "paper-280708527",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "TreEvo",
            "name_full": "Tree-structured thought Evolution",
            "brief_description": "An LLM-driven evolutionary framework that represents candidate solutions (alphas) as hierarchical tree-structured thoughts and applies semantic-level evolutionary operators (crossover, mutation, pruning) at the thought-tree level; LLM (Qwen3-Max) is used to generate offspring thoughts and executable code for evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TreEvo",
            "operator_type": "LLM-based (semantic-level evolutionary operators on tree-structured thoughts)",
            "operator_description": "Represents each candidate as a hierarchical tree T of reasoning units. Evolution is performed on the thought trees using semantic-level crossover (subtree exchange), mutation (subtree replacement via LLM generation), and pruning (removal/replacement of redundant subtrees), all mediated by an LLM (Qwen3-Max) which generates new thought trees and then executable code for each thought. The LLM is used in a prompt-driven fashion; there is no report of fine-tuning of the LLM.",
            "training_data_description": null,
            "domain_or_benchmark": "Alpha mining / symbolic regression for cross-sectional stock return prediction on four real-market datasets: CSI300, CSI500 (Chinese A-shares) and SPX, DJI (U.S. markets); raw features: OHLC, volume, vwap; objective metric: Information Coefficient (IC) and RankIC.",
            "comparison_baseline": "Traditional symbolic approaches (GP, XGBoost, LightGBM, AlphaGen, QFR) and LLM-driven EAs (EoH, ReEvo); also ablation variants TReEvo (code evolution with tree-structured thoughts).",
            "performance_learned_operator": "Averaged IC (over 3 runs, ablation setup): 0.0601 (CSI300) and 0.0726 (CSI500). Paper also reports TreEvo achieves at least +4.59% IC improvement on CSI300 and +4.80% on CSI500 vs the strongest traditional baseline, and reports ~20% IC improvement on CSI300, CSI500 and DJI in other comparisons; TreEvo often yields higher cumulative returns (e.g., +16.91% vs strongest baseline in one Top-50/Drop-5 backtest on CSI300).",
            "performance_traditional_operator": "Traditional symbolic methods required much larger evaluation budgets: XGBoost, LightGBM, AlphaGen, QFR ≥ 1000 evaluations vs TreEvo 200; GP used 20000 evaluations. Numeric ICs for each traditional baseline not consistently reported in the main text; TreEvo reports percentage improvements over the strongest baseline (&gt;=4.59% on CSI300, &gt;=4.80% on CSI500).",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": "Paper discusses that LLMs bring massive prior knowledge that narrows the search space; also warns of overfitting risk when extending evaluation budgets. No direct empirical measurement of pretrained-model bias on novel problems is reported.",
            "computational_cost_comparison": "TreEvo uses a small time budget of 200 evaluations to obtain best alpha. Comparators: XGBoost/LightGBM/AlphaGen/QFR used ≥1000 evaluations (≥5x), GP used 20,000 evaluations (100x) — the paper reports 5x–100x acceleration in required evaluations/time for TreEvo vs traditional methods.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "All LLM-driven methods in experiments used Qwen3-Max (a general pre-trained LLM); no controlled comparison between domain-specific and general pretraining was reported.",
            "ablation_study_results": "Ablation comparing ReEvo (code-only), TReEvo (code evolution but with tree-structured thoughts), and TreEvo (tree thoughts + semantic operators) reports averaged ICs (three runs): ReEvo = 0.0405 (CSI300) / 0.0468 (CSI500); TReEvo = 0.0546 / 0.0631 (CSI300/CSI500) — +34.8% over ReEvo; TreEvo = 0.0601 / 0.0726 — +10.07% and +15.06% over TReEvo respectively. Conclusion: the tree-structured thought representation provides the largest single performance gain; semantic-level operators provide additional improvements.",
            "hypothesis_space_characterization": "Qualitative: LLM priors expand expressiveness beyond a manually predefined symbol set while simultaneously narrowing effective search trajectories via prior knowledge; tree-structured thoughts explicitly model compositional hierarchy, enabling semantically meaningful recombination and more structured exploration compared to flat representations.",
            "adaptation_during_evolution": "LLM is used iteratively each generation to produce new thoughts and code; no report of online fine-tuning or gradient-based adaptation of the LLM during evolution — adaptation is via the evolutionary selection loop and prompt-driven generation only.",
            "failure_modes": "Risk of overfitting if evaluation budget is increased; in one case TreEvo was slightly inferior to GP on RankIC (suggesting traditional symbolic methods can outperform given large budgets); TreEvo was behind EoH on IC on the SPX dataset (paper attributes this to outliers in IC values).",
            "key_findings_for_theory": "1) Representation matters most: evolving tree-structured thoughts yields much larger performance gains (~35% over code-only evolution) than merely changing operators. 2) Semantic-level (thought-tree) operators further improve performance beyond representation alone (~10–15% additional IC). 3) LLM-driven approaches achieve comparable or better predictive performance with dramatically fewer evaluations (5x–100x), indicating prior-knowledge-guided search narrows effective hypothesis space. 4) There is a compute/overfitting trade-off: the faster convergence advantage can become a liability if search is extended without regularization.",
            "uuid": "e1996.0"
        },
        {
            "name_short": "ReEvo",
            "name_full": "ReEvo (code-only LLM-driven EA)",
            "brief_description": "An existing LLM-driven evolutionary algorithm that evolves executable code (candidate programs) directly using LLM generation; treated in the paper as a baseline that evolves codes only (no explicit thought-level representation).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "ReEvo",
            "operator_type": "LLM-based (code-only evolutionary operators / code generation)",
            "operator_description": "Evolves executable code candidates via LLM prompting and selection; in the paper it is run with the same LLM (Qwen3-Max) and population settings as other LLM-driven baselines, but it does not evolve separate thought representations — only codes are produced and evolved.",
            "training_data_description": null,
            "domain_or_benchmark": "Alpha mining / symbolic regression on CSI300, CSI500, SPX, DJI with OHLC+volume+vwap features; IC and RankIC used.",
            "comparison_baseline": "Compared to TreEvo, EoH, traditional symbolic methods (GP, XGBoost, LightGBM, AlphaGen, QFR) and the TReEvo ablation.",
            "performance_learned_operator": "Averaged ICs (three-run ablation reported in paper): 0.0405 (CSI300) and 0.0468 (CSI500). ReEvo generally converges slowest among the three LLM-driven methods.",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": "ReEvo shared the same small time budget (200 evaluations) as TreEvo in comparisons; nevertheless it converged more slowly, implying less efficient use of evaluations.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": "Compared with TReEvo and TreEvo: replacing linear plain-text thought with tree-structured thought (TReEvo) produced ~34.8% IC improvement over ReEvo, indicating ReEvo's representation is a limiting factor.",
            "hypothesis_space_characterization": "ReEvo's code-only evolution is described as less adaptive and slower to converge, suggesting a less constrained or less semantically guided search trajectory compared to thought-based methods.",
            "adaptation_during_evolution": null,
            "failure_modes": "Slow convergence and weaker adaptation in the evolution loop compared to thought-evolving methods; lower IC in ablation compared to tree-structured variants.",
            "key_findings_for_theory": "Code-only LLM evolution underperforms methods that evolve explicit thought representations, indicating that adding structured intermediate representations (thoughts/trees) provides stronger inductive bias and more efficient search than directly evolving code.",
            "uuid": "e1996.1"
        },
        {
            "name_short": "EoH",
            "name_full": "Evolve-outer-Heuristics (EoH) / thought-and-code evolution",
            "brief_description": "An LLM-driven EA that jointly evolves thoughts and generated code, where thoughts are represented as linear/plain-text prompts (not hierarchical trees); used as a stronger LLM-driven baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "EoH",
            "operator_type": "LLM-based (thought-and-code evolution; plain-text thoughts)",
            "operator_description": "Evolves both thoughts (plain-text prompts) and corresponding generated code using an LLM. Thoughts are linear/plain-text prompt representations (not hierarchical trees), and the method separately evolves thought prompts and the codes they generate.",
            "training_data_description": null,
            "domain_or_benchmark": "Alpha mining / symbolic regression on CSI300, CSI500, SPX, DJI; IC / RankIC used.",
            "comparison_baseline": "Compared against ReEvo (code-only), TreEvo (tree-structured thought evolution), and traditional symbolic methods.",
            "performance_learned_operator": "Paper states EoH consistently beats ReEvo across datasets. Exact numeric ICs per dataset are provided in the paper's tables (Table 2) but are not cleanly reproduced in the main text; EoH is sometimes competitive with TreEvo (e.g., EoH outperformed TreEvo on IC for SPX according to the authors, though TreEvo had better RankIC on that dataset).",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": "EoH used the same small evaluation budget as TreEvo and ReEvo in the experiments (200 evaluations).",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": "Compared to ReEvo and TreEvo, evolving thoughts (even linear/plain-text) along with code (EoH) yields clear gains over code-only evolution; TreEvo (tree thoughts) frequently outperforms EoH, highlighting the benefit of hierarchical thought structure.",
            "hypothesis_space_characterization": "Plain-text thought representation provides beneficial inductive bias over code-only evolution but lacks the hierarchical compositionality that TreEvo's tree-structured thoughts provide.",
            "adaptation_during_evolution": null,
            "failure_modes": "May be sensitive to dataset-specific characteristics; EoH outperforms TreEvo on one dataset's IC (SPX) possibly due to outlier IC behavior, indicating that performance can vary with dataset idiosyncrasies.",
            "key_findings_for_theory": "Evolving thoughts (even as plain-text) along with codes is superior to evolving codes alone; however, hierarchical (tree) thought representations provide additional, consistent improvements over linear thought representations.",
            "uuid": "e1996.2"
        },
        {
            "name_short": "TReEvo",
            "name_full": "TReEvo (ReEvo with tree-structured thoughts)",
            "brief_description": "An ablation variant that uses tree-structured thoughts as representation but retains code-only evolution operators (i.e., it evolves codes while using tree-structured thought scaffolding) to isolate the effect of representation from operator design.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "TReEvo",
            "operator_type": "hybrid (tree-structured thought representation + code-evolution operators)",
            "operator_description": "Uses tree-structured thought representations as prompts/blueprints but otherwise evolves/operates at the code level (same evolutionary operators as ReEvo). Serves as an ablation to measure the effect of representation alone.",
            "training_data_description": null,
            "domain_or_benchmark": "Alpha mining on CSI300 and CSI500 (ablation experiments reported); IC metric.",
            "comparison_baseline": "Compared to ReEvo (code-only), TreEvo (tree thoughts + semantic operators), and other baselines.",
            "performance_learned_operator": "Averaged ICs (three-run ablation): 0.0546 (CSI300) and 0.0631 (CSI500). This is ~34.8% better than ReEvo on the same datasets.",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": "Same small evaluation budget (200 evaluations) as other LLM-driven methods in the paper.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": "TReEvo vs ReEvo: large gains (~34.8% IC), showing representation (tree-structured thought) is the major contributor to improvements. TReEvo vs TreEvo: TreEvo further improves by ~10–15% via semantic-level tree operators.",
            "hypothesis_space_characterization": "Shows that adding hierarchical structure to prompts/thoughts significantly constrains and guides the search toward better regions of hypothesis space even when operators remain code-level.",
            "adaptation_during_evolution": null,
            "failure_modes": null,
            "key_findings_for_theory": "Tree-structured thought representation alone explains a large fraction of the performance improvement; proper semantic-level operators further increase performance. Representation is a critical inductive bias.",
            "uuid": "e1996.3"
        },
        {
            "name_short": "Traditional GP",
            "name_full": "Genetic Programming (symbol-level evolutionary operators)",
            "brief_description": "Conventional genetic programming for symbolic regression that manipulates symbol-level program trees using subtree crossover and mutation; used as a major traditional baseline in alpha-mining comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Genetic Programming (GP)",
            "operator_type": "traditional GP (symbol-level operators)",
            "operator_description": "Standard GP uses symbol-level representations (program/expression trees) and performs low-level genetic operators (random subtree crossover, point mutation, etc.) on syntax-level nodes; in this domain GP searches over alphas composed from a limited pre-defined set of arithmetic operators and handcrafted features.",
            "training_data_description": null,
            "domain_or_benchmark": "Alpha mining / symbolic regression on CSI300, CSI500, SPX, DJI; compared using IC/RankIC and cumulative return backtests.",
            "comparison_baseline": "Compared to TreEvo and other LLM-driven methods as well as other symbolic approaches (XGBoost, LightGBM, AlphaGen, QFR).",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": "Paper characterizes traditional GP as suffering from large search space and dependence on handcrafted priors which can introduce bias and inefficiency, but does not provide empirical pretrained-bias measurements.",
            "computational_cost_comparison": "GP consumed 20,000 evaluations in the paper's baseline settings (100x the TreEvo budget of 200 evaluations); authors report TreEvo obtains good solutions with ≈100x fewer evaluations.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "GP manipulates symbol-level representations which can create syntactically invalid or semantically incoherent offspring; authors argue semantic-level thought operators (TreEvo) produce more stable and expressive search dynamics.",
            "adaptation_during_evolution": null,
            "failure_modes": "High computational cost; slow convergence; heavy dependence on pre-specified operator sets and handcrafted features; potential to outperform LLM methods when given sufficiently large budgets (paper notes GP slightly outperforms TreEvo on RankIC in one case).",
            "key_findings_for_theory": "Traditional GP can find strong solutions given very large evaluation budgets, but is computationally inefficient and brittle compared to LLM-driven semantic-level evolution when budgets are constrained; this supports a theory that powerful priors (LLMs) plus structured representations (trees of thought) provide stronger inductive biases for rapid search.",
            "uuid": "e1996.4"
        },
        {
            "name_short": "Traditional Symbolic Approaches (XGBoost/LightGBM/AlphaGen/QFR)",
            "name_full": "Traditional symbolic / ML baselines: XGBoost, LightGBM, AlphaGen, QFR",
            "brief_description": "A set of widely-used traditional methods for alpha discovery including gradient-boosted trees (XGBoost, LightGBM) and domain-specific symbolic factor generators (AlphaGen, QFR), used as baselines in comparisons.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "XGBoost / LightGBM / AlphaGen / QFR",
            "operator_type": "hand-designed / traditional symbolic / gradient-boosted-tree methods",
            "operator_description": "XGBoost and LightGBM are gradient-boosted decision tree ensembles; AlphaGen and QFR are symbolic alpha mining frameworks using handcrafted operators and features. In the experiments they use a limited set of 22 arithmetic operators applied to the 6 raw features (OHLC, volume, vwap).",
            "training_data_description": null,
            "domain_or_benchmark": "Alpha mining on CSI300, CSI500, SPX, DJI; IC and RankIC metrics reported.",
            "comparison_baseline": "Compared to TreEvo and LLM-driven EAs (ReEvo, EoH).",
            "performance_learned_operator": null,
            "performance_traditional_operator": "These baselines used ≥1000 evaluations in the experiments (≥5x the TreEvo budget); TreEvo reports IC improvements over the strongest baseline of at least +4.59% (CSI300) and +4.80% (CSI500). Exact per-method IC numbers are not fully enumerated in the main text.",
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": "Paper notes that handcrafted heuristics leverage expert prior knowledge but can be restrictive and introduce biases; no quantitative bias measurements reported.",
            "computational_cost_comparison": "Reported to consume at least 1000 evaluations vs TreEvo's 200 (≥5x more).",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": "These methods operate within a manually pre-defined operator/feature space (22 arithmetic operators + 6 raw features in experiments), contrasting with LLM-driven methods' open expressiveness.",
            "adaptation_during_evolution": null,
            "failure_modes": "Slower to converge and less expressive due to restricted symbol sets; reliance on prior handcrafted features and operators can limit discovery of novel alphas.",
            "key_findings_for_theory": "LLM-driven methods (especially with tree-structured thoughts) can outperform or match traditional symbolic methods while using substantially fewer evaluations, suggesting that learned priors can act as powerful heuristics that accelerate search across large hypothesis spaces.",
            "uuid": "e1996.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reevo: Large language models as hyper-heuristics with reflective evolution",
            "rating": 2
        },
        {
            "paper_title": "Large language models as surrogate models in evolutionary algorithms: A preliminary study",
            "rating": 2
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1
        },
        {
            "paper_title": "Evolution of heuristics: Towards efficient automatic algorithm design using large language model",
            "rating": 2
        }
    ],
    "cost": 0.01807375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>From Linear to Hierarchical: Evolving Tree-structured Thoughts for Efficient Alpha Mining
22 Aug 2025</p>
<p>Junji Ren 
Junjie Zhao 
Shengcai Liu 
Peng Yang 
From Linear to Hierarchical: Evolving Tree-structured Thoughts for Efficient Alpha Mining
22 Aug 2025B37FDC216E4AEB1FDE2690972CA7D7D8arXiv:2508.16334v1[cs.CE]
Alpha mining, which discovers signals that predict asset returns, has long been attractive for automatic quantitative investment.This problem is typically formulated as a tree-based symbolic regression with handcrafted market data features and arithmetic operators.Unfortunately, existing symbolic methods are concerned with computational inefficiency and dependence on prior knowledge.Recent implementation of Large Language Models (LLMs) show that they can automatically generate executable codes for various tasks efficiently, thus can be considered as a new promising way for alpha mining.Specifically, LLMs-driven methods evolve a set of heuristics, including thoughts and codes, where the thoughts are usually represented as plain-text prompts of codes.Unfortunately, trivially adopting them in alpha mining ignores the fact that alphas are with hierarchical tree structures.This paper introduces Tree-structured thought Evolution (TreEvo), which evolves hierarchical reasoning ideas solely at the thought level.Experiments on four real-market datasets demonstrate that TreEvo can obtain better alphas with much less computational time and human expert efforts.And this superiority hardly holds without the tree-structured thoughts and the compatible evolutionary operators.</p>
<p>Introduction</p>
<p>Alpha mining, an important endeavor within financial quantitative trading, is to rigorously identify measurable signals that predict asset returns.The core challenge of alpha mining lies in isolating truly predictive factors from noisy, non-stationary data efficiently while ensuring interpretability (Giglio, Kelly, and Xiu 2022;Belle and Papantonis 2021).</p>
<p>With the rise of the sub-symbolic paradigm in recent decades, neural networks have been increasingly applied to alpha mining (Abe and Nakayama 2018;Chen, Pelger, and Zhu 2024).These methods effectively address challenges on automatic feature extraction and complex nonlinear relationships modeling (Jiang 2021).However, taking neural network as alpha itself is often criticized for limited interpretability, whose "black-box" nature restricts widespread adoption in financial applications.In contrast, the symbolic paradigm remains predominant due to its ability to generate interpretable factors ( Černevičienė and Kabašinskas 2024).Such factors facilitate rigorous strategy validation and robust risk management.Recent research in Genetic Programming (GP) (Zhang et al. 2020) and Reinforcement Learning (RL) (Yu et al. 2023), grounded in symbolic tree structures, continues to underscore the enduring relevance of symbolic approaches.Nevertheless, these approaches suffer from an inherently large search space, driven by the infinite composability of symbols, i.e., operators and market data features (Vie, Kleinnijenhuis, and Farmer 2020).Though handcrafted heuristics leverage personal expert knowledge to guide the search processes, the prior knowledge is often restricted and would introduce biases.</p>
<p>Recent advances have explored the integration of Large Language Models (LLMs) with Evolutionary Algorithms (EAs) (Romera-Paredes et al. 2024), which work by iteratively prompting LLMs for evolving the executable codes.</p>
<p>Compared to previous symbolic approaches, this pre-train paradigm provides the full expressiveness.Thus, the search space is no longer limited to manually predefined symbols (Merler et al. 2024).The massive prior knowledge encoded in LLMs allows for the generation of diverse, contextrelevant code structures that handcrafted heuristics often fail to capture (Jiang et al. 2025).Consequently, this leads to faster convergence and greater diversity in possible solutions.More recently, research has expanded to jointly evolve both the underlying thoughts and the generated codes (Liu et al. ).This shift from code-only to thought-and-code evolution marks an important step toward more cognitively inspired and effective LLM-driven program synthesis.</p>
<p>Such a framework is expected to foster a more stable and efficient search process of symbolic approaches in alpha mining.However, existing methods mostly focus on the enhancement of LLM-based evolutionary operators, overlooking the underlying conceptual structure of thoughts.Traditional symbolic approaches utilize explicit tree-based hierarchies.These hierarchies naturally reflect the compositional and layered structure of solutions, enhancing both clarity and modularity.In constrast, current LLM-driven methods mostly generate plain-text thoughts, which constrains the capacity to represent complex relationships.</p>
<p>In this paper, we propose Tree-structured thought Evolution (TreEvo), a novel paradigm where the evolution process is performed on thoughts represented as hierarchical tree structures.Each sub-tree encapsulates a sub-component of reasoning, and higher-level nodes form increasingly abstract heuristics.With this representation, the tailored semanticlevel evolutionary operators are also proposed for evolving the tree-structures at the thought level.Empirical comparisons with 7 representative methods have been conducted on 4 stock pools across 2 diverse markets.TreEvo shows significant advantages over traditional symbolic approaches, in terms of both the predictive power of mined alphas (with at least 4.59% improvements) and the computational efficiency (with 5x-100x acceleration).The clear superiority of the proposed tree-structure and evolutionary operators are also extensively analyzed by comparing with two representative LLM-driven EAs and the ablation variants.</p>
<p>In summary, our contributions are as follows.</p>
<p>• We propose the tree-structured thoughts, which explicitly organize the reasoning idea of alpha mining in a hierarchical manner.</p>
<p>• We propose TreEvo for alpha mining, a novel paradigm where evolution operates solely at the tree-structured thoughts level, with the novel semantic-level evolutionary operator.</p>
<p>• We empirically show that TreEvo, as well as the way of code generations via LLM-driven EAs, can be a revolutionary means for alpha mining.</p>
<p>Background and Related Works Alpha Mining</p>
<p>Alpha mining refers to the automated discovery of predictive signals (i.e., alphas) that forecast future asset returns.Al-phas form the foundation of quantitative investment strategies.The field was initially dominated by handcrafted designs (Fama and French 1993;Grinold and Kahn 2000).</p>
<p>As markets became more complex and handcrafted features showed limitations, researchers shifted toward data-driven approaches (De Prado 2018).In recent years, neural networks have been employed to represent the alphas.Deep learning models such as multilayer perceptrons (Heaton, Polson, and Witte 2017), recurrent neural networks with attention mechanisms (Kim and Kang 2019), and graph neural networks (Qian et al. 2024) have shown strong ability to capture nonlinear and temporal patterns in financial data.Despite their empirical success, these models often face challenges related to interpretability and overfitting.As a result, there is renewed interest in symbolicbased representation of alphas.Techniques like GP (Ren, Qin, and Li 2024) and RL (Ren et al. 2024;Zhao et al. 2025) are used to generate transparent and modular alpha formulaic expressions.Current trends also emphasize incorporating market constraints, improving robustness across different regimes, and building models that balance predictive accuracy with interpretability (Gu, Kelly, and Xiu 2020;Yeo et al. 2025).</p>
<p>Symbolic Paradigm</p>
<p>Symbolic paradigm constitutes a foundational approach within structured pattern discovery, enabling the identification of compact, composable representations that capture underlying regularities in data.Unlike purely statistical or neural methods, symbolic approaches formulate patterns as expressions drawn from a predefined syntax, such as algebraic formulas, logical clauses, or abstract syntax trees, thereby enabling structure-aware search over expressive hypothesis space.Early works include Genetic Programming (Koza 1990), a foundational method for symbolic regression, and later advancements such as the Eureqa system (Schmidt and Lipson 2009).These mothods have demonstrated applicability across scientific modeling (Udrescu and Tegmark 2020;Cranmer et al. 2020) and quantitative finance (La Malfa et al. 2021).Recent advances integrate symbolic reasoning with machine learning, employing differentiable search procedures (Liang et al. 2017), program induction frameworks (Ellis et al. 2021), and RL-based expression generation (Crochepierre, Boudjeloud-Assala, and Barbesant 2022).These methods extend the expressiveness of symbolic discovery while retaining structural interpretability.As structured pattern discovery increasingly intersects with scientific domains and high-stakes decision systems, symbolic paradigms offer a principled alternative to blackbox learning (Wang et al. 2023).</p>
<p>LLM-driven EAs</p>
<p>The recent emergence of LLMs has provided novel opportunities to enhance EAs by leveraging their advanced capabilities in natural language understanding, symbolic reasoning, and generative modeling (Zhang et al. 2023;Romera-Paredes et al. 2024).Conventional EAs typically employ fixed, manually designed genetic search operators such as crossover and mutation, which may limit their adaptability and efficiency when navigating complex, high-dimensional search space.Incorporating LLMs facilitates the development of dynamic, context-sensitive evolutionary operators that better capture problem-specific structures and heuristics (Hao, Zhang, and Zhou 2024;Huang et al. 2024).Previous studies have demonstrated the efficacy of LLMs in generating potential solutions and guiding search trajectories by harnessing implicit knowledge encoded during pretraining (Tian et al. 2025;Ye et al. 2024).Furthermore, LLMs have been utilized to enhance representation learning within EAs, enabling richer encodings of individuals and constraints that improve search performance (Liu et al. 2024).Recent advances also investigate automated evolutionary operators design through LLM-driven program synthesis and meta-optimization, reducing the reliance on human expertise and improving generalization across diverse problem domains (Lange, Tian, and Tang 2024).Despite these advances, challenges remain regarding the computational consumption of integrating LLM inference with iterative EA processes, as well as ensuring the robustness and interpretability of LLM-augmented evolutionary frameworks.Nevertheless, the integration of LLMs into EAs constitutes a promising and rapidly developing research direction with significant potential to advance metaheuristic optimization.</p>
<p>Method Main Idea</p>
<p>TreEvo encodes candidate alphas as tree-structured thoughts.Each sub-tree denotes a logically coherent sub-component of the thought.Higher-level nodes are hierarchically composed of thoughts encoded in lower-level nodes.This hierarchical formulation reflects the structural organization of human cognition.Cognitive processes are widely understood to proceed from abstract, global concepts toward more concrete, fine-grained elaborations.</p>
<p>By aligning the architecture of thought representation with the paradigm, TreEvo enables principled manipulation of reasoning structures.In particular, the tree structure supports the application of evolutionary operators, such as crossover and mutation, at different well-defined levels of semantic granularity.TreEvo facilitates structured search over interpretable and composable units of thought.The tree structure also achieves fine-grained control over the modification and recombination of thought components, which consequently enhances both search efficiency and interpretability.Moreover, these tree-based representations provide an efficient foundation for tracing, changing, and reusing intermediate reasoning steps.Such capabilities are essential for ensuring transparency and enabling efficient structured pattern discovery, particularly in high-stakes domains like finance.</p>
<p>Tree-structured Thought Representation</p>
<p>The structure shown here is an example of a hierarchical tree representation used to organize complex concepts or computations.At the highest level, a root node represents the overall entity or idea, which is decomposed into several child nodes capturing its main reasoning steps.Each child node can further branch into its own children, reflecting more finer-grained operations that collectively define the meaning of their parent node.This layer-wise decomposition continues recursively until the leaf nodes that represent the most basic, indivisible units in the structure.</p>
<p>The edges between nodes indicate the compositional relationships, where parent nodes depend on or aggregate information from their children.The hierarchical levels correspond to different degrees of abstraction, with higher levels summarizing or coordinating the details specified at lower levels.</p>
<p>On this basis, TreEvo is significantly different from the well-known Chain-of-Thought(CoT) (Wei et al. 2022) and Tree-of-Thought(ToT) (Yao et al. 2023).CoT treats a thought as a sequence of reasoning steps.ToT also organizes a thought as a sequence of reasoning steps, and integrates multiple related thoughts as a tree.That is, sibling reasoning steps under the same sub-tree typically share the same reasoning prefix.In contrast, TreEvo regards a whole tree as a thought.Consequently, the generation of the thoughts are also different.In CoT, the whole thought is generated by constructively appending reasoning steps.In ToT, the generation of a thought is to find the optimal complete branch from the root to one leaf.In TreEvo, we need to search the space of tree-structured representations, where EA can be adopted.</p>
<p>Semantic-level Evolutionary OOperators</p>
<p>Let a Thought be represented as a tree T = (V, E), where V is the set of reasoning units (e.g., logical steps), and E ⊆ V × V defines the hierarchical dependencies between them.Each node v i ∈ V , i ∈ {0, ..., |V |}, corresponds to a distinct semantic or computational subcomponent, and the root node v 0 encodes the expression of the overall alpha factor.</p>
<p>The evolutionary operators are described as follows.• Crossover: Given two thoughts T 1 and T 2 , we define crossover as subtree exchange:
T ′ = T 1 ⊕ vi,vj T 2 where v i ∈ T 1 , v j ∈ T 2 .
⊕ denotes the substitution of v i and v j .This operator preserves semantic validity and allows meaningful recombination of reasoning units.</p>
<p>Prompt sketch: Crossover refer to two thought trees and generate a new thought tree that synthesizes reasoning components from both parents while maintaining overall coherence.</p>
<p>• Mutation: Mutation operates T by selecting a node v k and replacing its subtree with a newly generated, syntactically valid and semantically plausible tree T :
T ′ = T ⊖ v k T
where ⊖ denotes the sub-tree replacement at node v k .This allows localized alteration of logical steps while preserving the overall structure and intent of the thought.</p>
<p>Prompt sketch: Mutation transforms a thought tree into a new thought by introducing meaningful changes that making it different from the Parent tree.</p>
<p>• Pruning: It operates by identifying and removing structurally redundant or semantically neutral sub-trees from T .Given the sub-tree of node v k , if it satisfies the redundancy criteria, it is replaced with a simpler expression T :
T ′ = T ⊖ v k T
This operator facilitates structural simplification and encourages parsimony in the thought representation.</p>
<p>Prompt sketch: Pruning simplifies a thought tree by removing redundant or unnecessary components, producing a more concise thought that retains the core reasoning idea.</p>
<p>The basic ideas of these evolutionary operators are similar to those in GP.However, as the solution representations are different, the detailed implementations are thus distinct.Unlike GP's symbol-level manipulations, which often produce syntactically invalid or semantically incoherent outputs, semantic-level evolutionary operators operate on wellformed and interpretable reasoning units.This distinction enables greater stability and expressiveness in the evolutionary process, aligning more closely with the compositional and hierarchical characteristics of human reasoning.Furthermore, representing thoughts as hierarchical trees supports a more granular and systematic form of evolutionary search, in which intermediate reasoning components can be explicitly isolated, modified, and recombined.Such structural transparency is difficult to achieve in flat or sequential representations, where reasoning steps remain entangled and less amenable to targeted manipulation.</p>
<p>The TreEvo Framework</p>
<p>The evolution process maintains a population of N candidate alphas, which are represented as tree-structured thoughts.The population are iteratively evolved with LLM.At each iteration, given N parent thoughts, N new offspring thoughts are generated by LLM with the above three operators.To evaluate those N new thoughts, LLM is first applied to generate one piece of executable code based on each new thought, then each code is run with the given market dataset to calculate the predictive performance (e.g., Information Correlation).After that, a new population of N candidate thoughts are selected among the N parents and N offsprings, based on their individual evaluated performance.More specific steps are described as follows:</p>
<p>Step 1 Initialization: Initialize a population P of N thoughts by prompting LLM with Initialization Prompts.</p>
<p>Please see Appendix for more details.Then ask LLM to generate N codes based the N thoughts and do the evaluation of each code, respectively.</p>
<p>Step 2 Evolutionary Process: Repeat until the predefined stop criteria (e.g., a given time budget) are met.</p>
<p>Step 2.1: Crossover: Generate a population of N new thoughts based on P by applying the semantic-level crossover operators with LLM.</p>
<p>Step 2.2: Mutation: Generate a population of N new thoughts based on P by applying the semantic-level mutation operators with LLM.</p>
<p>Step 2.3: Pruning: Generate a population of N new thoughts based on P by applying the semantic-level pruning operators with LLM.</p>
<p>Step 2.4: Evaluation: Generate executable codes for 3N individuals and evaluate codes performance using the real-market dataset with LLM.</p>
<p>Step 2.5: Selection: Select N best individuals from 3N individuals in terms of their evaluation performance to update P .</p>
<p>Step 3 Output: Output the code of the best performing thought through the whole process.</p>
<p>Experiments</p>
<p>The experimental studies focus on three key research questions (RQs): • RQ1: Can LLM-driven EAs really contribute as a promising way for alpha mining?• RQ2: Which representation of alphas is better for LLMdriven EAs?The solely codes, the plain-text thoughts with codes, or the tree-structured thoughts?• RQ3: Can the idea of tree-structure also be applied to existing LLM-driven EAs? How does the evolutionary operators contribute?</p>
<p>Experimental Settings</p>
<p>Dataset Four commonly seen stock pools, all are widely benched by the institutional investors, are considered, encompassing both Chinese A-shares market and U.S. stock market:</p>
<p>• Chinese A-shares market: Both CSI300 and CSI500 are considered.Here, the CSI300 tracks the 300 largest and most liquid A-share stocks, which represents the largecapitalization (large-cap) segment of China's market.The CSI500 consists of 500 mid-cap and small-cap A-share stocks, which reflects the behaviors of a broader market besides the large-cap stocks, capturing the growth potential of more dynamic and innovative stocks.• U.S. stock market: Both SPX and DJI are adopted.The SPX, or S&amp;P500, includes 500 leading publicly traded companies in U.S., covering all major industries and reflecting the overall American economic.The Dow Jones Industries Average (DJI) consists of 30 large U.S. companies, often seen as a measure of the health of the traditional American economy, with a focus on blue-chip stocks.</p>
<p>Considering about the reproducibility, we chose six widely used raw data features, i.e., opening, closing, highest, lowest prices (OHLC), trading volume (volume), and volume-weighted average price (vwap), for the whole experiments.All price and volume data are forward-adjusted for corporate actions to ensure consistency over time.The dataset is split chronologically into training set (2016/01/01-2020/01/01), validation set (2020/01/01-2021/01/01) and test set (2021/01/01-2024/01/01).</p>
<p>Baselines The framework is compared with seven baseline methods.Five of them are traditional symbolic approaches, representing the state-of-the-arts of alpha mining.The remaining two are LLM-driven EAs for code generation.For traditional approaches, 22 arithmetic operators are used for combing the above six data features (see Appendix), while it is no need to restrict the operators for LLM-driven EAs given the embedded prior knowledge.</p>
<p>The hyperparameters of the traditional symbolic approaches are all consistent with their original providers.For all LLM-driven EAs, we utilize the Qwen3-Max model (Yang et al. 2025) to generate both thoughts and codes of candidate solutions, and the population size is fixed to 10.</p>
<p>Evaluation Metrics</p>
<p>To highlight the fast convergence of the proposed TreEvo, we set its time budget for obtaining the best alpha to a very small number of 200 evaluations.For the five traditional symbolic approaches, we borrow their original large time budgets settings.Specifically, XGBoost, LightGBM, AlphaGen and QFR consume at least 1000 evaluations, which is 5x larger than that of TreEvo.And the time 1 https://gplearn.readthedocs.io/en/stable/intro.html budget of GP is as much as 20000 evaluations, which is 100x larger than TreEvo.EoH and ReEvo, the other two LLMdriven EAs, share the same setting of TreEvo.</p>
<p>For all methods, once they consume the predefined time budget on each training and validation datasets, their bestfound alphas are applied to the corresponding testing dataset to obtain the predictive performance on unseen data.</p>
<p>To measure the predictive performance of each algorithm, we employ two metrics commonly used across the quantitative finance industries as follows:</p>
<p>• IC measures the Pearson's correlation coefficient between daily predicted values Z ∈ R n×T (i.e., the time series scores of the best alpha on each of the T days of all n involved stocks in the testing dataset) and the ground-truth future 5-day returns F ∈ R n×T .It reflects the predictive power of the mined alpha with
R n×T ×R n×T → [−1, 1].
IC is also adopted as the training loss or search objective for the compared algorithms.
IC(Z, F) = 1 T T t=1 n i=1 (Z it − Zt )(F it − Ft ) n i=1 (Z it − Zt ) 2 n i=1 (F it − Ft ) 2
.</p>
<p>• Rank IC is the Spearman's rank correlation coefficent between daily predicted values Z and future returns F. It captures monotonic relationships, making it more robust to outliers and scale of values than IC.</p>
<p>Rank IC = IC(rank(Z), rank(F)).</p>
<p>Finally, the performance of an algorithm on each testing dataset is calculated by averaging over 5 independent runs of the above process, to ensure fairness and consistency.</p>
<p>Results and Analysis</p>
<p>Comparisons with Symbolic Methods Table 1 shows the performance comparisons of traditional symbolic approaches and TreEvo on CSI300 and CSI500.Overall, TreEvo consistently outperforms all baselines in terms of IC, demonstrating a clear advantage in capturing the magnitude of future returns.Compared to even the strongest baseline on each index, TreEvo still delivers an IC improvement of over 4.59% on CSI300 and 4.80% on CSI500, highlighting the benefit of incorporating LLMs' guidance into the alpha mining.</p>
<p>Note that, the above advantages of TreEvo are obtained with a time budget that is 5 times fewer than the four training-based methods and 100 times fewer than GP.This efficiency underscores the effectiveness of LLM-driven search methods in narrowing the search space with the embedded prior knowledge and facilitating faster convergence toward high-quality solutions.The reduced resource demands, combined with improved predictive accuracy, position TreEvo as a highly practical and worth exploring framework for alpha mining.</p>
<p>On the CSI300 dataset, TreEvo shows the strongest predictive power in terms of IC, confirming its ability to identify  profitable signals that correlate well with ground-truth returns.The superiority is also consistently observed on Ran-kIC, except for the comparison with GP where TreEvo is slightly inferior.This suggests that, with a large time budget, traditional symbolic approaches can still be powerful, while the outstanding ability of LLM-driven EAs lies in the effective reduction of the search space with few efforts.In all verified cases, TreEvo remains promising and compelling for alpha mining tasks, especially when the time budget is limited as the real market keeps changing rapidly.</p>
<p>In Figure 3, we further depict the cumulative return of each method on the CSI300 index with the commonly used Top-50/Drop-5 trading strategy.The trading strategy selects the top 50 stocks ranked by the alpha scores at each rebalance date, which is set to daily.Among them, the bottom 5 stocks are dropped at the next rebalance date and replaced with 5 new stocks from the updated top-ranked stocks.Figure 3 shows the definitive superiority of TreEvo over the traditional symbolic approaches in terms of cumulative return under this strategy.Except for the very short initial period, TreEvo maintains the leading position throughout the backtest horizon.Compared to the strongest baseline, TreEvo achieves a 16.91% improvement in the cumulative return.And TreEvo obtains more than 100% excess return by compared to the CSI300 market benchmark (i.e., the gray dash curve in the figure).Moreover, as the CSI300 index continuously goes down (get more loses), the rapidly increasing curve of TreEvo demonstrates that real alphas (i.e., the management risks) that are less correlated to beta (i.e., the market risks) have been mined.</p>
<p>To conclude, the performance of TreEvo not only shows its superiority over representative approaches in the quantitative finance industries, but also supports that LLM-driven EAs can be revolutionary tools for alpha mining.Because it can automatically generate executable codes of powerful alphas in minutes rather than traditionally daily, and without much human efforts in the manufacturing loop.strate that TreEvo generally outperforms the compared approaches, indicating that the alpha signals discovered by our method possess stronger linear associations with future returns across different markets.More specifically, on CSI300, CSI500, and DJI datasets, the improvement made by TreEvo on IC is around 20%, which is significant.While TreEvo is behind EoH on the IC metric on the SPX dataset, it maybe because the outliers of the IC values therein.The evidence is that TreEvo gains a 18.6% improvement on the RankIC metric, which is more robust to extreme IC values and resistant to the scale of IC differences.</p>
<p>Comparisons with LLM-driven EAs</p>
<p>Notably, it is observed that EoH consistently beats ReEvo on all cases.This shows that evolving thoughts and codes separately is better than solely evolving the codes.Moreover, given TreEvo outperforms EoH, this table further suggests that evolving tree-structured thoughts is more effective than linear plain-text thoughts, which verifies the major hypothesis of this work.</p>
<p>Figure 4 presents the convergence curves of the three LLM-driven EAs on four stock markets.ReEvo generally converges the slowest, indicating that solely codes evolution does not adapt to the environments effectively.For the two thoughts-based evolution methods, they also behave differently.On CSI300, TreEvo establishes an unequivocal superiority very early and thus converges to a substantially higher IC.While in the other three datasets, TreEvo does significantly beat EoH at the beginning.Nevertheless, the curves of TreEvo quickly goes up and even surpass EoH on CSI500 and DJI.This also demonstrates the better adaptation ability of Tree-structured thoughts within the evolution loop.</p>
<p>Ablation Studies To clarify whether the major performance gain stems from the tree-structures thought or the semantic-level evolutionary operators, a variant of ReEvo is designed by solely evolving its codes with the proposed tree-structured thoughts, denoted as TReEvo.By comparing TReEvo with ReEvo, it suffices to show how tree-structure contributes to the performance, as they share the same evolutionary operators.By comparing TReEvo with the proposed TreEvo, the novel evolutionary operators are ablated.</p>
<p>Figure 5 depicts three independent runs of ReEvo, TReEvo, and TreEvo on both the CSI300 and CSI500 datasets.TReEvo consistently outperforms ReEvo in terms  of IC, verifying the advantages of the tree-structured thoughts.And the superiority of TreEvo over TReEvo reveals that the proposed semantic-level evolutionary operators are more compatible with the evolution of treestructured thoughts.Take a closer look at the figure, it can be observed that the performance gains brought by the tree-structured thought (i.e., TReEvo over ReEvo) is generally larger than that of the novel evolutionary operators (i.e., TreEvo over TReEvo).More specifically, the averaged ICs (over three runs) of TReEvo are 0.0546 and 0.0631 on CSI300 and CSI500, respectively.This leads 34.81% and 34.83% over that of ReEvo, which obtains the averaged ICs of 0.0405 and 0.0468 on the two datasets.Comparatively, the averaged ICs of the proposed TreEvo are 0.0601 and 0.0726 on CSI300 and CSI500, respectively.Its advantages over TReEvo become 10.07%and 15.06%.These simple calculations help us to quantitatively understand that the treestructured thought is more dominant than the novel evolutionary operators to improve the performance of LLMdriven EAs on alpha mining.Meanwhile, the evolutionary operators should also be tailored to be compatible with the hierarchical thoughts, otherwise the improvements would be limited.</p>
<p>Conclusions and Future Directions</p>
<p>This paper proposes a novel LLM-driven alpha mining paradigm based on the tree-structured thoughts, which enables more expressive and hierarchical representations of heuristics.By leveraging the compositional nature of trees, our approach allows for finer-grained reasoning and modular reuse of intermediate thought steps, making it particularly well-suited for complex search space like formulaic alpha.</p>
<p>Extensive experiments across multiple markets demonstrate that our method consistently outperforms 7 state-of-the-arts in terms of both predictive performance and computational efficiency.This strongly indicates that LLM-driven EAs are promising and revolutionary tools for alpha mining, and suggests the tree-structured representations therein.</p>
<p>The consistently upward curves in Figure 4 indicate that the search has not yet converged, implying potential for further improvement.However, this apparent opportunity is accompanied by a significant risk: increasing the number of evaluations may make the methods more prone to overfitting.The avoidance of prescribing the explicit training and validation phases of LLM-driven paradigm also bring the practitioners a challenging trade-off: terminating the search too early may result in underdeveloped factors, while extending it too far may lead to overfitted alphas.Future work should therefore move beyond maximizing IC alone, and in-stead incorporate more principled search mechanisms, such as regularizations, complexity controls, or early stopping heuristics, to ensure trustworthy alpha mining.</p>
<p>Figure 1 :
1
Figure 1: Different forms of heuristics in LLM-driven EAs.</p>
<p>Figure 2 :
2
Figure 2: An example of tree-structured thoughts.</p>
<p>0153) (±0.0173) (±0.0081) (±0.0119)</p>
<p>Figure 3 :
3
Figure 3: Cumulative rewards of traditional symbolic approaches on CSI300.</p>
<p>Figure 4: Visualization of the convergence curves of three LLM-EAs on four datasets.</p>
<p>Figure 5 :
5
Figure 5: Results of three independent runs of ReEvo, TReEvo and TreEvo.</p>
<p>Table 1 :
1
Comparisons to Traditional Symbolic Approaches</p>
<p>Table 2 reports the comparative results of TreEvo against two existing LLMdriven EAs across all four datasets.The results demon-</p>
<p>Table 2 :
2
Performance Comparisons of LLM-driven EAs ±0.0237) (±0.0046) (±0.0106) (±0.0129) (±0.0020) (±0.0110) (±0.0197)
CSI300CSI500SPXDJIICRankICICRankICICRankICICRankICEoH0.05060.04620.05840.06530.05040.04580.05140.0546(±0.0188) (ReEvo 0.04110.04090.04260.04890.03620.03080.03680.0393(±0.
Southern University of Science and Technology.
The University of Sydney.</p>
<p>Deep learning for forecasting stock returns in the cross-section. M Abe, H Nakayama, Pacific-Asia conference on knowledge discovery and data mining. Springer2018</p>
<p>Principles and practice of explainable machine learning. V Belle, I Papantonis, Frontiers in big Data. 46889692021</p>
<p>Explainable artificial intelligence (xai) in finance: a systematic literature review. J Černevičienė, A Kabašinskas, Artificial Intelligence Review. 5782162024</p>
<p>Xgboost: A scalable tree boosting system. T Chen, C Guestrin, Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. the 22nd acm sigkdd international conference on knowledge discovery and data mining2016</p>
<p>Deep learning in asset pricing. L Chen, M Pelger, J Zhu, Management Science. 7022024</p>
<p>Discovering symbolic models from deep learning with inductive biases. M Cranmer, A Sanchez Gonzalez, P Battaglia, R Xu, K Cranmer, D Spergel, S Ho, Advances in neural information processing systems. 332020</p>
<p>Interactive reinforcement learning for symbolic regression from multi-format human-preference feedbacks. L Crochepierre, L Boudjeloud-Assala, V Barbesant, IJCAI. 2022</p>
<p>Advances in financial machine learning. M L De Prado, 2018John Wiley &amp; Sons</p>
<p>Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. K Ellis, C Wong, M Nye, M Sablé-Meyer, L Morales, L Hewitt, L Cary, A Solar-Lezama, J B Tenenbaum, Proceedings of the 42nd acm sigplan international conference on programming language design and implementation. the 42nd acm sigplan international conference on programming language design and implementation2021</p>
<p>Common risk factors in the returns on stocks and bonds. E F Fama, K R French, Journal of financial economics. 3311993</p>
<p>Factor models, machine learning, and asset pricing. S Giglio, B Kelly, D Xiu, Annual Review of Financial Economics. 1412022</p>
<p>The efficiency gains of long-short investing. R C Grinold, R N Kahn, Financial Analysts Journal. 5662000</p>
<p>Empirical asset pricing via machine learning. S Gu, B Kelly, D Xiu, The Review of Financial Studies. 3352020</p>
<p>Large language models as surrogate models in evolutionary algorithms: A preliminary study. H Hao, X Zhang, A Zhou, Applied Stochastic Models in Business and Industry. 101741. Heaton, J. B.Polson, N. G.and Witte, J. H.9112024. 2017Swarm and Evolutionary Computation</p>
<p>Exploring the true potential: Evaluating the black-box optimization capability of large language models. B Huang, X Wu, Y Zhou, J Wu, L Feng, R Cheng, K C Tan, arXiv:2404.062902024arXiv preprint</p>
<p>Applications of deep learning in stock market prediction: recent progress. J Jiang, F Wang, J Shen, S Kim, S Kim, ACM Transactions on Software Engineering and Methodology. 1841155372025. 2021A survey on large language models for code generation</p>
<p>Lightgbm: A highly efficient gradient boosting decision tree. Advances in neural information processing systems 30. G Ke, Q Meng, T Finley, T Wang, W Chen, W Ma, Q Ye, T.-Y Liu, 2017</p>
<p>Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems. S Kim, M Kang, Ca, G La Malfa, E La Malfa, R Belavkin, P M Pardalos, G Nicosia, R Lange, Y Tian, Y Tang, arXiv:1902.10877.KozaInternational Conference on Machine Learning, Optimization, and Data Science. Springer2019. 1990. 2021. 202434Stanford University, Department of Computer Science Stanford,arXiv preprintProceedings of the Genetic and Evolutionary Computation Conference Companion</p>
<p>Neural symbolic machines: Learning semantic parsers on freebase with weak supervision. C Liang, J Berant, Q Le, K D Forbus, N Lao, Proceedings of the Conference of the 55th Annual Meeting of the Association for Computational Linguistics. the Conference of the 55th Annual Meeting of the Association for Computational Linguistics2017. 2017</p>
<p>Evolution of heuristics: Towards efficient automatic algorithm design using large language model. F Liu, X Tong, M Yuan, X Lin, F Luo, Z Wang, Z Lu, Q Zhang, Proceedings of the 41st International Conference on Machine Learning, ICML'24. the 41st International Conference on Machine Learning, ICML'24</p>
<p>In-context symbolic regression: Leveraging large language models for function discovery. S Liu, C Chen, X Qu, K Tang, Y.-S Ong, K Haitsiukevich, N Dainese, P Marttinen, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. the 62nd Annual Meeting of the Association for Computational Linguistics2024. 2024. 20244Student Research Workshop)</p>
<p>Mdgnn: Multirelational dynamic graph neural network for comprehensive and dynamic stock investment prediction. H Qian, H Zhou, Q Zhao, H Chen, H Yao, J Wang, Z Liu, F Yu, Z Zhang, J Zhou, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Riskminer: Discovering formulaic alphas via risk seeking monte carlo tree search. T Ren, R Zhou, J Jiang, J Liang, Q Wang, Y Peng, Proceedings of the 5th ACM International Conference on AI in Finance. the 5th ACM International Conference on AI in Finance2024</p>
<p>Alpha mining and enhancing via warm start genetic programming for quantitative investment. W Ren, Y Qin, Y Li, B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, arXiv:2412.00896Nature. 62579952024. 2024arXiv preprintMathematical discoveries from program search with large language models</p>
<p>An llm-empowered adaptive evolutionary algorithm for multi-component deep learning systems. M Schmidt, H Lipson, H Tian, X Han, G Wu, A Guo, Y Zhou, J Zhang, S Li, J Wei, T Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2009. 2025324Distilling free-form natural laws from experimental data</p>
<p>Ai feynman: A physics-inspired method for symbolic regression. S.-M Udrescu, M Tegmark, Science advances. 61626312020</p>
<p>. A Vie, A M Kleinnijenhuis, D J Farmer, 2020</p>
<p>arXiv:2011.05277Qualities, challenges and future of genetic algorithms: a literature review. arXiv preprint</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, T Fu, Y Du, W Gao, K Huang, Z Liu, P Chandak, S Liu, P Van Katwyk, A Deac, Nature. 62079722023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, 202235</p>
<p>Qlib: An ai-oriented quantitative investment platform. X Yang, W Liu, D Zhou, J Bian, T.-Y Liu, A Yang, A Li, B Yang, B Zhang, B Hui, B Zheng, B Yu, C Gao, C Huang, C Lv, arXiv:2505.09388Qwen3 technical report. 2020. 2025arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in neural information processing systems. 362023</p>
<p>Reevo: Large language models as hyper-heuristics with reflective evolution. H Ye, J Wang, Z Cao, F Berto, C Hua, H Kim, J Park, G Song, Advances in neural information processing systems. 202437</p>
<p>A comprehensive review on financial explainable ai. W J Yeo, W Van Der Heever, R Mao, E Cambria, R Satapathy, G Mengaldo, Artificial Intelligence Review. 5862025</p>
<p>Generating synergistic formulaic alpha collections via reinforcement learning. S Yu, H Xue, X Ao, F Pan, J He, D Tu, Q He, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Autoalpha: an efficient hierarchical evolutionary algorithm for mining alpha factors in quantitative investment. T Zhang, Y Li, Y Jin, J Li, arXiv:2002.082452020arXiv preprint</p>
<p>Survey on genetic programming and machine learning techniques for heuristic design in job shop scheduling. F Zhang, Y Mei, S Nguyen, M Zhang, IEEE Transactions on Evolutionary Computation. 2812023</p>
<p>Quantfactor reinforce: mining steady formulaic alpha factors with variance-bounded reinforce. J Zhao, C Zhang, M Qin, P Yang, IEEE Transactions on Signal Processing. 2025</p>            </div>
        </div>

    </div>
</body>
</html>
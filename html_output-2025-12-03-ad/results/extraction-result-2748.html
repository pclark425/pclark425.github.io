<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2748 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2748</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2748</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-32feca141fce06c6588b4014d27953a3fc25f19b</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32feca141fce06c6588b4014d27953a3fc25f19b" target="_blank">PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language, and is able to correctly forecast “what happens next” given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%.</p>
                <p><strong>Paper Abstract:</strong> We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don’t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast what happens next given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2748",
    "paper_id": "paper-32feca141fce06c6588b4014d27953a3fc25f19b",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.003946249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language Grounding Through Neuro-Symbolic Interaction in a 3D World</h1>
<p>Rowan Zellers ${ }^{\text {A }}$ Ari Holtzman ${ }^{\text {A }}$ Matthew Peters ${ }^{\circ}$ Roozbeh Mottaghi ${ }^{\circ}$ Aniruddha Kembhavi ${ }^{\circ}$ Ali Farhadi ${ }^{\text {A }}$ Yejin Choi ${ }^{\text {A }}{ }^{\circ}$<br>${ }^{\text {A }}$ Paul G. Allen School of Computer Science \&amp; Engineering, University of Washington<br>${ }^{\circ}$ Allen Institute for Artificial Intelligence<br>https://rowanzellers.com/piglet</p>
<h4>Abstract</h4>
<p>We propose PIGle $\sqrt{ }$ : a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGle $\sqrt{ }$ into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don't. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGle $\sqrt{ }$ can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast "what happens next" given an English sentence over $80 \%$ of the time, outperforming a 100x larger, text-to-text approach by over $10 \%$. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work.</p>
<h2>1 Introduction</h2>
<p>As humans, our use of language is linked to the physical world. To process a sentence like "the robot turns on the stove, with a pan on it" (Figure 1) we might imagine a physical Pan object. This meaning representation in our heads can be seen as a part of our commonsense world knowledge, about what a Pan is and does. We might reasonably predict that the Pan will become Hot - and if there's an Egg on it, it would become cooked.</p>
<p>As humans, we learn such a commonsense world model through interaction. Young children learn to reason physically about basic objects by manipulating them: observing the properties they have,
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: PIGle $\sqrt{ }$. Through physical interaction in a 3D world, we learn a model for what actions do to objects. We use our physical model as an interface for a language model, jointly modeling elements of language form and meaning. Given an action expressed symbolically or in English, PIGle $\sqrt{ }$ can simulate what might happen next, expressing it symbolically or in English.
and how they change if an action is applied on them (Smith and Gasser, 2005). This process is hypothesized to be crucial to how children learn language: the names of these elementary objects become their first "real words" upon which other language is scaffolded (Yu and Smith, 2012).</p>
<p>In contrast, the dominant paradigm today is to train large language or vision models on static data, such as language and photos from the web. Yet such a setting is fundamentally limiting, as suggested empirically by psychologists' failed attempts to get kittens to learn passively (Held and Hein, 1963). More recently, though large Transformers have made initial progress on benchmarks, they also have frequently revealed biases in those same datasets, suggesting they might not be solving underlying tasks (Zellers et al., 2019b). This has been argued philosophically by a flurry of re-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: $\mathcal{P G P e N}$, a setting for few-shot language-world grounding. We collect data for 280k physical interactions in THOR, a 3D simulator with 20 actions and 125 object types, each with 42 attributes (e.g. isBroken). We annotate 2 k interactions with English sentences describing the initial world state, the action, and the action result.
cent work arguing that no amount of language form could ever specify language meaning (McClelland et al., 2019; Bender and Koller, 2020; Bisk et al., 2020); connecting back to the Symbol Grounding Problem of Harnad (1990).</p>
<p>In this paper, we investigate an alternate strategy for learning physical commonsense through interaction, and then transferring that into language. We introduce a model named $\operatorname{PIG} L e \mathcal{F}$, short for Physical Interaction as Grounding for Language Transformers. We factorize an embodied agent into an explicit model of world dynamics, and a model of language form. We learn the dynamics model through interaction. Given an action heatUp applied to the Pan in Figure 1, the model learns that the Egg on the pan becomes Hot and Cooked, and that other attributes do not change.</p>
<p>We integrate our dynamics model with a pretrained language model, giving us a joint model of linguistic form and meaning. The combined $\operatorname{PIG} L e \mathcal{F}$ can then reason about the physical dynamics implied by English sentences describing actions, predicting literally what might happen next. It can then communicate that result either symbolically or through natural language, generating a sentence like 'The egg becomes hot and cooked." Our separation between physical dynamics and language allows the model to learn about physical commonsense from the physical world itself, while also avoiding recurring problems of artifacts and biases that arise when we try to model physical world understanding solely through language.</p>
<p>We study this through a new environment and evaluation setup called $\operatorname{PIG} P e N$, short for Physical Interaction Grounding Paired with Natural Language. In $\operatorname{PIG} P e N$, a model is given unlimited access to an environment for pretraining, but only 500 examples with paired English annotations. Models in our setup must additionally generalize to novel 'unseen' objects for which we intentionally do not provide paired language-environment supervision. We build this on top of the THOR environment
(Kolve et al., 2017), a physics engine that enables agents to perform contextual interactions (Fig 2) on everyday objects.</p>
<p>Experiments confirm that $\operatorname{PIG} L e \mathcal{F}$ performs well at grounding language with meaning. Given a sentence describing an action, our model predicts the resulting object states correctly over $80 \%$ of the time, outperforming even a 100x larger model (T511B) by over $10 \%$. Likewise, its generated natural language is rated by humans as being more correct than equivalently-sized language models. Last, it can generalize in a 'zero-shot' way to objects that it has never read about before in language.</p>
<p>In summary, we make three key contributions. First, we introduce $\operatorname{PIG} L e \mathcal{F}$, a model decoupling physical and linguistic reasoning. Second, we introduce $\operatorname{PIG} P e N$, to learn and evaluate the transfer of physical knowledge to the world of language. Third, we perform experiments and analysis suggesting promising avenues for future work.</p>
<h2>2 $\operatorname{PIG} P e N:$ A Resource for Neuro-Symbolic Language Grounding</h2>
<p>We introduce $\operatorname{PIG} P e N$ as a setting for learning and evaluating physically grounded language understanding. An overview is shown in Figure 2. The idea is that an agent gets access to an interactive 3D environment, where it can learn about the world through interaction - for example, that objects such as a Vase can become Broken if thrown. The goal for a model is to learn natural language meaning grounded in these interactions.</p>
<p>Task definition. Through interaction, an agent observes the interplay between objects $\boldsymbol{o} \in \mathcal{O}$ (represented by their attributes) and actions $\boldsymbol{a} \in \mathcal{A}$ through the following transition:</p>
<p>$$
\underbrace{\left{o_{1}, \ldots, o_{N}\right}}<em 1="1">{\overrightarrow{\boldsymbol{o}}, \text { state pre-action }} \times \boldsymbol{a} \rightarrow \underbrace{\left{o</em>
$$}^{\prime}, \ldots, o_{N}^{\prime}\right}}_{\overrightarrow{\boldsymbol{o}^{\prime}}, \text { state post-action }</p>
<p>Actions change the state of a subset of objects: turning on a Faucet affects a nearby Sink, but it will not change a Mirror on the wall.</p>
<p>To encourage learning from interaction, and not just language, an agent is given a small number of natural language annotations of transitions. We denote these sentences as $s_{\vec{\alpha}}$, describing the state pre-action, $s_{\alpha}$ the action, and $s_{\vec{\alpha}}$ the state postaction respectively. During evaluation, an agent will sometimes encounter new objects $\boldsymbol{o}$ that were not part of the paired training data.</p>
<p>We evaluate the model's transfer in two ways:
a. $\mathcal{P} \operatorname{IG} \mathcal{P} \in \mathcal{N}$-NLU. A model is given object states $\vec{o}$, and an English sentence $s_{\alpha}$ describing an action. It must predict the grounded object states $\vec{o}$ that result after the action is taken.
b. $\mathcal{P} \operatorname{IG} \mathcal{P} \in \mathcal{N}$-NLG. A model is given object states $\vec{o}$ and a literal action $\boldsymbol{a}$. It must generate a sentence $s_{\vec{\alpha}}$ describing the state post-action.
We next describe our environment, feature representation, and language annotation process.</p>
<h3>2.1 Environment: THOR</h3>
<p>We use AI2-THOR as an environment for this task (Kolve et al., 2017). In THOR, a robotic agent can navigate around and perform rich contextual interactions with objects in a house. For instance, it can grab an Apple, slice it, put it in a Fridge, drop it, and so on. The state of the Apple, such as whether it is sliced or cold, changes accordingly; this is not possible in many other environments.</p>
<p>In this work, we use the underlying THOR simulator as a proxy for grounded meaning. Within THOR, it can be seen as a 'complete' meaning representation (Artzi et al., 2013), as it fully specifies the kind of grounding a model can expect in its perception within THOR.</p>
<p>Objects. The underlying THOR representation of each object $\boldsymbol{o}$ is in terms of 42 attributes; we provide a list in Appendix B. We treat these attributes as words specific to an attribute-level dictionary; for example, the temperature Hot is one of three possible values for an object's temperature; the others being Cold and RoomTemp.</p>
<p>Actions. An action $\boldsymbol{a}$ in THOR is a function that takes up to two objects as arguments. Actions are highly contextual, affecting not only the arguments but potentially other objects in the scene (Figure 2). We also treat action names as words in a dictionary.</p>
<p>Filtering out background objects. Most actions change the state of only a few objects, yet there can be many objects in a scene. We keep annotation and computation tractable by having models predict (and humans annotate) possible changes
of at most two key objects in the scene. As knowing when an object doesn't change is also important, we include non-changing objects if fewer than two change.</p>
<p>Exploration. Any way of exploring the environment is valid for our task, however, we found that exploring intentionally was needed to yield good coverage of interesting states. Similar to prior work for instruction following (Shridhar et al., 2020), we designed an oracle to collect diverse and interesting trajectories ${\vec{o}, \boldsymbol{a}, \vec{o}}$. Our oracle randomly selects one of ten high level tasks, see Appendix B for the list. These in turn require randomly choosing objects in the scene; e.g. a Vase and a Laptop in Figure 2. We randomize the manner in which the oracle performs the task to discover diverse situations.</p>
<p>In total, we sampled 20k trajectories. From these we extracted 280k transitions (Eqn 1's) where at least one object changes state, for training.</p>
<h3>2.2 Annotating Interactions with Language</h3>
<h3>2.2.1 Data Selection for Annotation</h3>
<p>We select 2 k action state-changes from trajectories held out from the training set. We select them while also balancing the distribution of action types to ensure broad coverage in the final dataset. We are also interested in a model's ability to generalize to new object categories - beyond what it has read about, or observed in a training set. We thus select 30 objects to be "unseen," and exclude these from paired environment-language training data. We sample 500 state transitions, containing only "seen" objects to be the training set; we use 500 for validation and 1000 for testing.</p>
<h3>2.2.2 Natural Language Annotation</h3>
<p>Workers on Mechanical Turk were shown an environment in THOR before and after a given action $\boldsymbol{a}$. Each view contains the THOR attributes of the two key objects. Workers then wrote three English sentences, corresponding to $s_{\vec{\alpha}}, s_{\alpha}$, and $s_{\vec{\alpha}}$ respectively. Workers were instructed to write at a particular level of detail: enough so that a reader could infer "what happens next" from $s_{\vec{\alpha}}$ and $s_{\alpha}$, yet without mentioning redundant attributes. We provide more details in Appendix C.</p>
<h2>3 Modeling $\mathcal{P} \operatorname{IG} \mathcal{L} \mathcal{C} \mathcal{T}$</h2>
<p>In this section, we describe our $\mathcal{P} \operatorname{IG} \mathcal{L} \mathcal{C} \mathcal{T}$ model. First, we learn a neural physical dynamics model</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: $\mathfrak{R} \mathfrak{C} \mathcal{L} \mathcal{E}$ architecture. We pretrain a model of physical world dynamics by learning to transform objects $\vec{\boldsymbol{o}}$ and actions $\boldsymbol{a}$ into new updated objects $\overrightarrow{\boldsymbol{o}}$. Our underlying world dynamics model - the encoder, the decoder, and the action application module, can augment a language model with grounded commonsense knowledge.
from interactions, and second, integrate with a pretrained model of language form.</p>
<h3>3.1 Modeling Physical Dynamics</h3>
<p>We take a neural, auto-encoder style approach to model world dynamics. An object $\boldsymbol{o}$ gets encoded as a vector $\mathbf{h}<em o="o">{\boldsymbol{o}} \in \mathbb{R}^{d</em>}}$. The model likewise encodes an action $\boldsymbol{a}$ as a vector $\mathbf{h<em a="a">{\boldsymbol{a}} \in \mathbb{R}^{d</em>$, using it to manipulate the hidden states of all objects. The model can then decode any object hidden representation back into a symbolic form.}</p>
<h3>3.1.1 Object Encoder and Decoder</h3>
<p>We use a Transformer (Vaswani et al., 2017) to encode objects into vectors $\mathbf{o} \in \mathbb{R}^{d_{o}}$, and then another to decode from this representation.</p>
<p>Encoder. Objects $\boldsymbol{o}$ are provided to the encoder as a set of attributes, with categories $c_{1}, \ldots, c_{n}$. Each attribute $c$ has its own vocabulary and embedding $\mathbf{E}<em _enc="{enc" _text="\text">{c}$. For each object $\boldsymbol{o}$, we first embed all the attributes separately and feed the result into a Transformer encoder $T</em>$. This gives us (with position embeddings omitted for clarity):}</p>
<p>$$
\mathbf{h}<em _mathrm_enc="\mathrm{enc">{\boldsymbol{o}}=\mathbf{T}</em>}}\left(\mathbf{E<em 1="1">{1}\left(o</em>}\right), \ldots, \mathbf{E<em n="n">{c</em>\right)\right)
$$}}\left(o_{c_{n}</p>
<p>Decoder. We can then convert back into the original symbolic representation through a left-to-right Transformer decoder, which predicts attributes one-by-one from $c_{1}$ to $c_{n}$. This captures the inherent correlation between attributes, while making no independence assumptions, we discuss our ordering in Appendix A.2. The probability of predicting the next attribute $o_{c_{i+1}}$ is then given by:
$p\left(o_{c_{i+1}} \mid \mathbf{h}<em c__i="c_{i">{\boldsymbol{o}}, \boldsymbol{o}</em>}}\right)=\mathbf{T<em _boldsymbol_o="\boldsymbol{o">{\mathrm{dec}}\left(\mathbf{h}</em>}}, \mathbf{E<em 1="1">{l}\left(o</em>}\right), \ldots, \mathbf{E<em i="i">{c</em>\right)\right)$}}\left(o_{c_{i}</p>
<h3>3.1.2 Modeling actions as functions</h3>
<p>We treat actions $\boldsymbol{a}$ as functions that transform the state of all objects in the scene. Actions in our environment take at most two arguments, so we embed the action $\boldsymbol{a}$ and the names of its arguments, concatenate them, and pass the result through a multilayer perceptron; yielding a vector representation $\mathbf{h}_{\boldsymbol{a}}$.</p>
<p>Applying Actions. We use the encoded action $\mathbf{h}<em _boldsymbol_o="\boldsymbol{o">{\boldsymbol{a}}$ to transform all objects in the scene, obtaining updated representations $\hat{\mathbf{h}}</em>$ for each one. We take a global approach, jointly transforming all objects. This takes into account that interactions are contextual: turning on a Faucet might fill up a Cup if and only if there is one beneath it.}^{\prime}</p>
<p>Letting the observed objects in the interaction be $\boldsymbol{o}<em 2="2">{1}$ and $\boldsymbol{o}</em>}$, with encodings $\mathbf{h<em 1="1">{\boldsymbol{o}</em>}}$ and $\mathbf{h<em 2="2">{\boldsymbol{o}</em>$ respectively, we model the transformation via the following multilayer perceptron:}</p>
<p>$$
\left[\hat{\mathbf{h}}<em 1="1">{\boldsymbol{o}</em>}^{\prime}}, \hat{\mathbf{h}<em 2="2">{\boldsymbol{o}</em>}^{\prime}}\right]=\operatorname{MLP<em _boldsymbol_a="\boldsymbol{a">{\text {apply }}\left(\left[\mathbf{h}</em>}}, \mathbf{h<em 1="1">{\boldsymbol{o}</em>}}, \mathbf{h<em 2="2">{\boldsymbol{o}</em>\right]\right)
$$}</p>
<p>The result can be decoded into symbolic form using the object decoder (Equation 3).</p>
<h3>3.1.3 Loss function and training</h3>
<p>We train our dynamics model on $(\overrightarrow{\boldsymbol{o}}, \boldsymbol{a}, \overrightarrow{\boldsymbol{o}})$ transitions. The model primarily learns by running $\overrightarrow{\boldsymbol{o}}, \boldsymbol{a}$ through the model, predicting the updated output state $\hat{\mathbf{h}}_{\boldsymbol{o}^{\prime}}$, and minimizing the cross-entropy of generating attributes of the real changed object $\overrightarrow{\boldsymbol{o}}$. We also regularize the model by encoding objects $\overrightarrow{\boldsymbol{o}}, \overrightarrow{\boldsymbol{o}}$ and having the model learn to reconstruct them. We weight all these cross-entropy losses equally. We discuss our architecture in Appendix A.1; it uses 3-layer Transformers, totalling 17M parameters.</p>
<h3>3.2 Language Grounding</h3>
<p>After pretraining our physical dynamics model, we integrate it with a Transformer Language Model (LM). In our framework, the role of the LM will be to both encode natural language sentences of actions into a hidden state approximating $\mathbf{h}_{\boldsymbol{a}}$, as well as summarizing the result of an interaction $(\overrightarrow{\boldsymbol{o}}, \boldsymbol{a}, \overrightarrow{\boldsymbol{o}})$ in natural language.</p>
<p>Choice of LM. Our framework is compatible with any language model. However, to explore the impact of pretraining data on grounding later in this paper, we pretrain our own with an identical architecture to the smallest GPT2 (Radford et al. (2019); 117M). To handle both classification and generation well, we mask only part of the attention weights out, allowing the model to encode a "prefix" bidirectionally; it generates subsequent tokens left-to-right (Dong et al., 2019). We pretrain the model on Wikipedia and books; details in Appendix D.</p>
<p>We next discuss architectural details of performing the language transfer, along with optimization.</p>
<h3>3.2.1 Transfer Architecture</h3>
<p>English actions to vector form. Given a natural language description $\boldsymbol{s}<em _boldsymbol_a="\boldsymbol{a">{\boldsymbol{a}}$ of an action $\boldsymbol{a}$, like "The robot throws the vase," for $\operatorname{PlGPeN}$-NLU, our model will learn to parse this sentence into a neural representation $\mathbf{h}</em>}}$, so the dynamics model can simulate the result. We do this by encoding $\boldsymbol{s<em L="L" M="M">{\boldsymbol{a}}$ through our language model, $\mathbf{T}</em>}$, with a learned linear transformation over the resulting (bidirectional) encoding. The resulting vector $\mathbf{h<em _boldsymbol_a="\boldsymbol{a">{\boldsymbol{s}</em>$ can then be used by Equation 4.}}</p>
<p>Summarizing the result of an action. For $\operatorname{PlGPeN}$-NLG, our model simulates the result of an action $\boldsymbol{a}$ neurally, resulting in a predicted hidden state $\hat{\mathbf{h}}<em i="i">{\boldsymbol{o}}$ for each object in the scene $\boldsymbol{o}$. To write an English summary describing "what changed," we first learn a lightweight fused representation of the transition, aggregating the initial and final states, along with the action, through a multilayer perceptron. For each object $\boldsymbol{o}</em>$ we have:</p>
<p>$$
\mathbf{h}<em i="i">{\Delta \boldsymbol{o}</em>}}=\operatorname{MLP<em _boldsymbol_o="\boldsymbol{o">{\Delta}\left(\left[\mathbf{h}</em><em _boldsymbol_o="\boldsymbol{o">{i}}, \hat{\mathbf{h}}</em><em _boldsymbol_a="\boldsymbol{a">{i}^{\prime}}, \mathbf{h}</em>\right]\right)
$$}</p>
<p>We then use the sequence $\left[\mathbf{h}<em 1="1">{\Delta \boldsymbol{o}</em>}}, \mathbf{h<em 2="2">{\Delta \boldsymbol{o}</em>\right]$ as bidirectional context for our our LM to decode from. Additionally, since our test set includes novel objects not seen in training, we provide the names of the objects as additional context for the LM generator (e.g. 'Vase, Laptop'); this allows a LM to copy those names over rather than hallucinate wrong
ones. Importantly we only provide the surfaceform names, not underlying information about these objects or their usage as with few-shot scenarios in the recent GPT-3 experiments (Brown et al., 2020) - necessitating that $\operatorname{PlGLe} \mathrm{f}$ learns what these names mean through interaction.}</p>
<h3>3.2.2 Loss functions and training.</h3>
<p>Modeling text generation allows us to incorporate a new loss function, that of minimizing the loglikelihood of generating each $\boldsymbol{s}_{\overrightarrow{\boldsymbol{o}}}$ given previous words and the result of Equation 5:</p>
<p>$$
p\left(s_{i+1}^{\text {post }} \mid s_{\overrightarrow{\boldsymbol{o}}, 1: i}\right)=\mathbf{T}<em _Delta="\Delta" _boldsymbol_o="\boldsymbol{o">{\mathrm{LM}}\left(\mathbf{h}</em><em _Delta="\Delta" _boldsymbol_o="\boldsymbol{o">{1}}, \mathbf{h}</em><em _overrightarrow_boldsymbol_o="\overrightarrow{\boldsymbol{o">{2}}, s</em>\right)
$$}}, 1: i</p>
<p>We do the same for the object states $\boldsymbol{s}<em _boldsymbol_o="\boldsymbol{o">{\overrightarrow{\boldsymbol{o}}}$ pre-action, using $\mathbf{h}</em>$ as the corresponding hidden states.}_{i}</p>
<p>For $\operatorname{PlGPeN}$-NLU, where no generation is needed, optimizing Equation 5 is not strictly necessary. However, as we will show later, it helps provide additional signal to the model, improving overall accuracy by several percentage points.</p>
<h2>4 Experiments</h2>
<p>We test our model's ability to encode language into a grounded form ( $\operatorname{PlGPeN}$-NLU), and decode that grounded form into language ( $\operatorname{PlGPeN}$-NLG).</p>
<h3>4.1 $\operatorname{PlGPeN}$-NLU Results.</h3>
<p>We first evaluate models by their performance on $\operatorname{PlGPeN}$-NLU: given objects $\overrightarrow{\boldsymbol{o}}$, and a sentence $\boldsymbol{s}_{\boldsymbol{a}}$ describing an action, a model must predict the resulting state of objects $\overrightarrow{\boldsymbol{o}}$. We primarily evaluate models by accuracy; scoring how many objects for which they got all attributes correct. We compare with the following strong baselines:
a. No Change: this baseline copies the initial state of all objects $\overrightarrow{\boldsymbol{o}}$ as the final state $\overrightarrow{\boldsymbol{o}}$.
b. GPT3-175B (Brown et al., 2020), a very large language model for 'few-shot' learning using a prompt. For GPT3, and other text-to-text models, we encode and decode the symbolic object states in a JSON-style dictionary format, discussed in Appendix A.4.
c. T5 (Raffel et al., 2019). With this model, we use the same 'text-to-text' format, however here we train it on the paired data from $\operatorname{PlG}$ PeN. We consider varying sizes of T5, from T5-Small - the closest in size to $\operatorname{PlGLe} \mathrm{f}$, up until T5-11B, roughly 100x the size.
d. (Alberti et al., 2019)-style. This paper originally proposed a model for VCR (Zellers et al.,</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Accuracy (\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Val</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Overall</td>
<td style="text-align: center;">Seen</td>
</tr>
<tr>
<td style="text-align: center;">No Change</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">25.5</td>
<td style="text-align: center;">29.9</td>
</tr>
<tr>
<td style="text-align: center;">GPT3-175B (Brown et al., 2020)</td>
<td style="text-align: center;">23.8</td>
<td style="text-align: center;">22.4</td>
<td style="text-align: center;">22.4</td>
</tr>
<tr>
<td style="text-align: center;">T5-11B (Raffel et al., 2019)</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">64.2</td>
<td style="text-align: center;">79.5</td>
</tr>
<tr>
<td style="text-align: center;">T5-3B</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">63.3</td>
<td style="text-align: center;">77.1</td>
</tr>
<tr>
<td style="text-align: center;">T5-Large</td>
<td style="text-align: center;">56.5</td>
<td style="text-align: center;">54.1</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: center;">T5-Base</td>
<td style="text-align: center;">56.0</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">69.2</td>
</tr>
<tr>
<td style="text-align: center;">T5-Small</td>
<td style="text-align: center;">39.9</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">57.0</td>
</tr>
<tr>
<td style="text-align: center;">Alberti et al.2019, Pretrained Dynamics</td>
<td style="text-align: center;">61.3</td>
<td style="text-align: center;">53.9</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: center;">Alberti et al. 2019</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">16.2</td>
</tr>
<tr>
<td style="text-align: center;">G\&amp;D2019, Pretrained Dynamics</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">60.9</td>
</tr>
<tr>
<td style="text-align: center;">G\&amp;D2019</td>
<td style="text-align: center;">15.1</td>
<td style="text-align: center;">11.3</td>
<td style="text-align: center;">23.1</td>
</tr>
<tr>
<td style="text-align: center;">MGLer</td>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">83.8</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Attribute-level accuracy (Test-Overall,\%)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">size</td>
<td style="text-align: center;">distance</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">Temperature isBroken</td>
</tr>
<tr>
<td style="text-align: center;">8-way</td>
<td style="text-align: center;">8-way</td>
<td style="text-align: center;">8-way</td>
<td style="text-align: center;">3-way</td>
</tr>
<tr>
<td style="text-align: center;">83.2</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">96.3</td>
<td style="text-align: center;">86.0</td>
</tr>
<tr>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">89.5</td>
<td style="text-align: center;">84.2</td>
</tr>
<tr>
<td style="text-align: center;">83.9</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">95.4</td>
</tr>
<tr>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">90.0</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">95.6</td>
</tr>
<tr>
<td style="text-align: center;">81.8</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">96.3</td>
</tr>
<tr>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">96.1</td>
</tr>
<tr>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">84.9</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">89.6</td>
</tr>
<tr>
<td style="text-align: center;">87.7</td>
<td style="text-align: center;">87.6</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">93.4</td>
</tr>
<tr>
<td style="text-align: center;">53.4</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">84.0</td>
<td style="text-align: center;">88.1</td>
</tr>
<tr>
<td style="text-align: center;">83.0</td>
<td style="text-align: center;">86.9</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">93.7</td>
</tr>
<tr>
<td style="text-align: center;">68.6</td>
<td style="text-align: center;">47.3</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">88.3</td>
</tr>
<tr>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">91.9</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">99.8</td>
</tr>
</tbody>
</table>
<p>Table 1: Overall results. Left: we show the model accuracies at predicting all attributes of an object correctly. We compare MGLer with 'text-to-text' approaches that represent the object states as a string, along with BERT-style approaches with additional machinery to encode inputs or decode outputs. MGLer outperforms a T5 model 100x its size (11B params) and shows gains over the BERT-style models that also model action dynamics through a language transformer. Right: we show several attribute-level accuracies, along with the number of categories per attribute; MGLer outperforms baselines by over 4 points for some attributes such as size and distance.</p>
<p>2019a), where grounded visual information is fed into a BERT model as tokens; the transformer performs the grounded reasoning. We adapt it for our task by using our base LM and feeding in object representations from our pretrained object encoder, also as tokens. Our object decoder predicts the object, given the LM's pooled hidden state. This is "pretrained dynamics," we also consider a version without a randomly initialized dynamics model.
e. (Gupta and Durrett, 2019)-style. Thiso paper proposes using Transformers to model physical state, for tasks like entity tracking in recipes. Here, the authors propose decoding a physical state attribute (like isCooked) by feeding the model a label-specific CLS] token, and then mapping the result through a hidden layer. We do this and use a similar object encoder as our (Alberti et al., 2019)-style baseline.
We discuss hyperparameters in Appendix A.3.
Results. From the results (Table 1), we can draw several patterns. Our model, MGLer performs best at getting all attributes correct; doing so over $80 \%$ on both validation and test sets, even for novel objects not seen during training. The next closest model is T5-11B, which scores $68 \%$ on validation. Though when evaluated on objects 'seen' during training it gets $77 \%$, that number drops by over $18 \%$ for unseen objects. On the other hand, MGLer has a modest gap of $3 \%$. This suggests that our approach is particularly effective at connecting unpaired language and world representations. At</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Accuracy (val;\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MGLer, No Pretraining</td>
<td style="text-align: right;">10.4</td>
</tr>
<tr>
<td style="text-align: left;">MGLer, Non-global MLP ${ }_{\text {apply }}$</td>
<td style="text-align: right;">72.0</td>
</tr>
<tr>
<td style="text-align: left;">MGLer, Global MLP ${ }_{\text {apply }}$</td>
<td style="text-align: right;">78.5</td>
</tr>
<tr>
<td style="text-align: left;">MGLer, Global MLP ${ }_{\text {apply }}$, Gen. loss (6)</td>
<td style="text-align: right;">81.8</td>
</tr>
<tr>
<td style="text-align: left;">MGLer, Symbols Only (Upper Bound)</td>
<td style="text-align: right;">89.3</td>
</tr>
</tbody>
</table>
<p>Table 2: Ablation study on MGPer-NLU's validation set. Our model improves $6 \%$ by modeling global dynamics of all objects in the scene, versus applying actions to single objects in isolation. We improve another $3 \%$ by adding an auxiliary generation loss.
the other extreme, GPT3 does poorly in its 'fewshot' setting, suggesting that size is no replacement for grounded supervision.</p>
<p>MGLer also outperforms 'BERT style' approaches that control for the same language model architecture, but perform the physical reasoning inside the language transformer rather than as a separate model. Performance drops when the physical decoder must be learned from few paired examples (as in Gupta and Durrett (2019)); it drops even further when neither model is given access to our pretrained dynamics model, with both baselines then underperforming 'No Change.' This suggests that our approach of having a physical reasoning model outside of an LM is a good inductive bias.</p>
<h3>4.1.1 Ablation study</h3>
<p>In Table 2 we present an ablation study of MGLer's components. Of note, by using a global representation of objects in the world (Equation 4), we get</p>
<p>over $6 \%$ improvement over a local representation where objects are manipulated independently. We get another $3 \%$ boost by adding a generation loss, suggesting that learning to generate summaries helps the model better connect the world to language. Last, we benchmark how much headroom there is on $\operatorname{PlGPeN}$-NLU by evaluating model performance on a 'symbols only' version of the task, where the symbolic action $\boldsymbol{a}$ is given explicitly to our dynamics model. This upper bound is roughly $7 \%$ higher than $\operatorname{PlG} l e f$, suggesting space for future work.</p>
<h3>4.2 $\operatorname{PlGPeN}$-NLG Results</h3>
<p>Next, we turn to $\operatorname{PlGPeN}$-NLG: given objects $\vec{o}$, and the literal next action $\boldsymbol{a}$, a model must generate a sentence $s_{\vec{o}^{\prime}}$ describing what will change in the scene. We compare with the following baselines:
a. T5. We use a T5 model that is given a JSONstyle dictionary representation of both $\vec{o}$ and $\boldsymbol{a}$, it is finetuned to generate summaries $\boldsymbol{s}<em _boldsymbol_o="\boldsymbol{o">{\vec{o}^{\prime}}$.
b. LM Baseline. We feed our LM hidden states $\mathbf{h}</em>$ is never used here.
Size matters. Arguably the most important factor controlling the fluency of a language generator is its size (Kaplan et al., 2020). Since our LM could also be scaled up to arbitrary size, we control for size in our experiments and only consider models the size of GPT2-base (117M) or smaller; we thus compare against T5-small as T5-Base has 220M parameters. We discuss optimization and sampling hyperparameters in Appendix A.3.}}$ from our pretrained encoder, along with its representation of $\boldsymbol{a}$. The key difference between it and $\operatorname{PlG} l e f$ is that we do not allow it to simulate neurally what might happen next $\mathrm{MLP}_{\text {apply }</p>
<p>Evaluation metrics. We evaluate models over the validation and test sets. We consider three main evaluation metrics: BLEU (Papineni et al., 2002) with two references, the recently proposed BERTScore (Zhang et al., 2020), and conduct a human evaluation. Humans rate both the fluency of post-action text, as well as its faithfulness to true action result, on a scale from -1 to 1 .</p>
<p>Results. We show our results in Table 3. Of note, $\operatorname{PlG} l e f$ is competitive with T5 and significantly outperforms the pure LM baseline, which uses a pretrained encoder for object states, yet has the physical simulation piece $\mathrm{MLP}_{\text {apply }}$ removed. This suggests that simulating world dynamics not only allows the model to predict what might happen</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BERTScore</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human (test; [-1, 1])</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Val</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Val</td>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">Faithfulness</td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">46.6</td>
<td style="text-align: center;">43.4</td>
<td style="text-align: center;">82.2</td>
<td style="text-align: center;">81.0</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">0.15</td>
</tr>
<tr>
<td style="text-align: center;">LM Baseline</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">39.7</td>
<td style="text-align: center;">81.6</td>
<td style="text-align: center;">78.8</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">$-0.13$</td>
</tr>
<tr>
<td style="text-align: center;">$\operatorname{PlG} l e f$</td>
<td style="text-align: center;">49.0</td>
<td style="text-align: center;">43.9</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">81.3</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.22</td>
</tr>
<tr>
<td style="text-align: center;">Human</td>
<td style="text-align: center;">44.5</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">82.6</td>
<td style="text-align: center;">83.3</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.71</td>
</tr>
</tbody>
</table>
<p>Table 3: Text generation results on $\operatorname{PlGPeN}$-NLG, showing models of roughly equivalent size (up to 117 M parameters). Our $\operatorname{PlG} l e f$ outperforms the LM baseline (using the same architecture but omitting the physical reasoning component) by 4 BLEU points, 2 BERTScore $F_{1}$ points, and 0.35 points in a human evaluation of language faithfulness to the actual scene.
next, it leads to more faithful generation as well.</p>
<h2>5 Analysis</h2>
<h3>5.1 Qualitative examples.</h3>
<p>We show two qualitative examples in Figure 4, covering both $\operatorname{PlGPeN}$-NLU as well as $\operatorname{PlGPeN}$-NLG. In the first row, the robot empties a held Mug that is filled with water. $\operatorname{PlG} l e f$ gets the state, and generates a faithful sentence summarizing that the mug becomes empty. T5 struggles somewhat, emptying the water from both the Mug and the (irrelevant) Sink. It also generates text saying that the Sink becomes empty, instead of the Mug.</p>
<p>In the second row, $\operatorname{PlG} l e f$ correctly predicts the next object states, but its generated text is incomplete - it should also write that the mug becomes filled wtih Coffee. T5 makes the same mistake in generation, and it also underpredicts the state changes, omitting all changes to the Mug.</p>
<p>We suspect that T5 struggles here in part because Mug is an unseen object. T5 only experiences it through language-only pretraining, but this might not be enough for a fully grounded representation.</p>
<h3>5.2 Representing novel words</h3>
<p>The language models that perform best today are trained on massive datasets of text. However, this has unintended consequences (Bender et al., 2021) and it is unlike how children learn language, with children learning novel words from experience (Carey and Bartlett, 1978). The large scale of our pretraining datasets might allow models to learn to perform physical-commonsense like tasks for wrong reasons, overfitting to surface patterns rather than learning meaningful grounding.</p>
<p>We investigate the extent of this by training a 'zero-shot' version of our backbone LM on Wikipedia and books - the only difference is that</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Qualitative examples. Our model $\mathfrak{M G} \mathcal{L}$ reliably predicts what might happen next (like the $\mathfrak{M u g}$ becoming empty in Row 1), in a structured and explicit way. However, it often struggles at generating sentences for unseen objects like $\mathfrak{M u g}$ that are excluded from the training set. T5 struggles to predict these changes, for example, it seems to suggest that emptying the $\mathfrak{M u g}$ causes all containers in the scene to become empty.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: $\mathfrak{M G} \mathfrak{P} \in \mathcal{N}$-NLU performance of a zero-shot $\mathfrak{M G} \mathcal{L}$ ， that was pretrained on Books and Wikipedia without reading any words of our 'unseen' objects like 'mug.' It outperforms a much bigger T5-11B overall, though is in turn beaten by $\mathfrak{M G} \mathcal{L}$ on unseen objects like 'Sink' and 'Microwave.'
we explicitly exclude all mentioned sentences containing one of our "unseen" object categories. In this setting, not only must $\mathfrak{M G} \mathcal{L}$ learn to ground words like 'mug,' it must do so without having seen the word 'mug' during pretraining. This is significant because we count over 20 k instances of 'Mug' words (including morphology) in our dataset.</p>
<p>We show results in Figure 5. A version of $\mathfrak{M G} \mathcal{L}$ with the zero-shot LM does surprisingly well - achieving $80 \%$ accuracy at predicting the state changes for "Mug" - despite never having been pretrained on one before. This even outperforms T5 at the overall task. Nevertheless, $\mathfrak{M G} \mathcal{L}$ outperforms it by roughly $7 \%$ at unseen objects, with notable gains of over $10 \%$ on highly dynamic objects like Toasters and Sinks.</p>
<h2>6 Related Work</h2>
<p>Grounded commonsense reasoning. In this work, we study language grounding and common-
sense reasoning at the representation and concept level. The aim is to train models that learn to acquire concepts more like humans, rather than performing well on a downstream task that (for humans) requires commonsense reasoning. Thus, this work is somewhat different versus other 3D embodied tasks like QA (Gordon et al., 2018; Das et al., 2018), along with past work for measuring such grounded commonsense reasoning, like SWAG, HellaSWAG, and VCR (Zellers et al., 2018, 2019b,a). The knowledge covered is different, as it is self-contained within THOR. While VCR, for instance, includes lots of visual situations about what people are doing, this paper focuses on learning the physical properties of objects.</p>
<p>Zero-shot generalization. There has been a lot of past work involved with learning 'zero-shot': often learning about the grounded world in language, and transferring that knowledge to vision. Techniques for this include looking at word embeddings (Frome et al., 2013) and dictionary definitions (Zellers and Choi, 2017). In this work, we propose the inverse. This approach was used to learn better word embeddings (Gupta et al., 2019) or semantic tuples (Yatskar et al., 2016), but we consider learning a component to be plugged into a deep Transformer language model.</p>
<p>Past work evaluating these types of zero-shot generalization have also looked into how well models can compose concepts in language together (Lake and Baroni, 2018; Ruis et al., 2020). Our work considers elements of compositionality through grounded transfer. For example, in</p>
<p>MGPen-NLG, models must generate sentences about the equivalent of dropping a 'dax', despite never having seen one before. However, our work is also contextual, in that the outcome of 'dropping a dax' might depend on external attributes (like how high we're dropping it from).</p>
<p>Structured Models for Attributes and Objects. The idea of modeling actions as functions that transform objects has been explored in the computer vision space (Wang et al., 2016). Past work has also built formal structured models for connecting vision and language (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), we take a neural approach and connect today's best models of language form to similarly neural models of a simulated environment.</p>
<h2>7 Conclusion</h2>
<p>In this paper, we presented an approach MGLeT for jointly modeling language form and meaning. We presented a testbed MGPeN for evaluating our model, which performs well at grounding language to the (simulated) world.</p>
<h2>Acknowledgments</h2>
<p>We thank the reviewers for their helpful feedback, and the Mechanical Turk workers for doing a great job in annotating our data. Thanks also to Zak Stone and the Google Cloud TPU team for help with the computing infrastructure. This work was supported by the DARPA CwC program through ARO (W911NF-15-1-0543), the DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the Allen Institute for AI.</p>
<h2>References</h2>
<p>Chris Alberti, Jeffrey Ling, Michael Collins, and David Reitter. 2019. Fusion of detected objects in text for visual question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2131-2140.</p>
<p>Yoav Artzi, Nicholas FitzGerald, and Luke S Zettlemoyer. 2013. Semantic parsing with combinatory categorial grammars. ACL (Tutorial Abstracts), 3.</p>
<p>Emily M Bender, Timnit Gebru, Angelina McMillanMajor, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big. Proceedings of FAccT.</p>
<p>Emily M. Bender and Alexander Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5185-5198, Online. Association for Computational Linguistics.</p>
<p>Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, et al. 2020. Experience grounds language. arXiv preprint arXiv:2004.10151.</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165.
S. Carey and E. Bartlett. 1978. Acquiring a single new word.</p>
<p>Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1-10.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. arXiv preprint arXiv:1905.03197.</p>
<p>Andrea Frome, Greg Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visualsemantic embedding model.</p>
<p>Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi. 2018. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Aditya Gupta and Greg Durrett. 2019. Effective use of transformer networks for entity tracking. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 759-769.</p>
<p>Tanmay Gupta, Alexander Schwing, and Derek Hoiem. 2019. Vico: Word embeddings from visual cooccurrences. In Proceedings of the IEEE International Conference on Computer Vision, pages 74257434 .</p>
<p>Stevan Harnad. 1990. The symbol grounding problem. Physica D: Nonlinear Phenomena, 42(13):335-346.</p>
<p>Richard Held and Alan Hein. 1963. Movementproduced stimulation in the development of visually guided behavior. Journal of comparative and physiological psychology, 56(5):872.</p>
<p>Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64-77.</p>
<p>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.</p>
<p>Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474.</p>
<p>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics, 1:193-206.</p>
<p>Brenden Lake and Marco Baroni. 2018. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In International Conference on Machine Learning, pages 2873-2882. PMLR.</p>
<p>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 1435-1442.</p>
<p>James L McClelland, Felix Hill, Maja Rudolph, Jason Baldridge, and Hinrich Schütze. 2019. Extending machine language models toward humanlevel language understanding. arXiv preprint arXiv:1912.05877.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311-318.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Technical report, OpenAI.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv e-prints.</p>
<p>Laura Ruis, Jacob Andreas, Marco Baroni, Diane Bouchacourt, and Brenden M Lake. 2020. A benchmark for systematic generalization in grounded language understanding. Advances in Neural Information Processing Systems, 33.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pages 4603-4611.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. 2020. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10740-10749.</p>
<p>Linda Smith and Michael Gasser. 2005. The development of embodied cognition: Six lessons from babies. Artificial life, 11(1-2):13-29.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000-6010. Curran Associates Inc.</p>
<p>Xiaolong Wang, Ali Farhadi, and Abhinav Gupta. 2016. Actions " transformations. In CVPR.</p>
<p>Mark Yatskar, Vicente Ordonez, and Ali Farhadi. 2016. Stating the obvious: Extracting visual common sense knowledge. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 193-198.</p>
<p>Chen Yu and Linda B Smith. 2012. Embodied attention and word learning by toddlers. Cognition, 125(2):244-262.</p>
<p>Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019a. From recognition to cognition: Visual commonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR).</p>
<p>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversarial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical</p>
<p>Methods in Natural Language Processing, pages 93104, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Rowan Zellers and Yejin Choi. 2017. Zero-shot activity recognition with verb attribute induction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 946958 .</p>
<p>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019b. HellaSwag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 47914800, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tianyi Zhang, V. Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. ArXiv, abs/1904.09675.</p>            </div>
        </div>

    </div>
</body>
</html>
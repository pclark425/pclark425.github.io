<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9342 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9342</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9342</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-fc918d6f8e2523696c34fa1be5aabdb42e9648d2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fc918d6f8e2523696c34fa1be5aabdb42e9648d2" target="_blank">Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> The method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.</p>
                <p><strong>Paper Abstract:</strong> Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9342.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9342.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex-AWM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI Codex used to hypothesize an Abstract World Model (AWM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors use OpenAI's Codex (code-davinci-002) with few-shot in-context prompting to generate a structured, textual Abstract World Model (AWM) for Minecraft: a directed acyclic graph of item subgoals (recipes, tool and workbench requirements) represented as a python dictionary. The LLM output is treated as a hypothesis and is verified/corrected through environment interaction during RL training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI Codex (code-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex is an LLM trained to generate code from natural language prompts; the paper uses the code-davinci-002 Codex variant via few-shot prompting to produce structured python-dictionary outputs encoding Minecraft item dependencies. (No parameter count or training-data specifics are provided in this paper.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Embodied reinforcement learning / embodied AI (simulated virtual environments — Minecraft item-crafting technology tree)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Predict a textual world model (AWM): for 391 Minecraft items generate crafting recipes (ingredients and quantities), required tool, and whether a crafting table or furnace is required; output formatted as a nested python dictionary mapped to a DAG of subgoal nodes and edges.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy metrics comparing LLM-predicted features to ground-truth AWM: (a) Collectable vs. Craftable label accuracy; (b) Workbench (crafting table / furnace) requirement accuracy; (c) Recipe ingredient correctness; (d) Recipe ingredients & quantities exact-match; plus counts/percentages of incorrectly inserted dependencies and missing dependencies; and error statistics for predicted ingredient quantities (std, absolute error, average error).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported accuracies (Codex hypothesized AWM over 391 items): Collectable vs. Craftable: 57% (All Items), 100% (Tools Only). Workbench (crafting table/furnace): 84% (All Items), 96% (Tools Only). Recipe Ingredients correctness: 66% (All Items), 81% (Tools Only). Recipe Ingredients & Quantities exact match: 55% (All Items), 69% (Tools Only). Additional metrics: % items with incorrectly inserted dependencies: 42% (All), 8% (Tools); % items with missing dependencies: 35% (All), 26% (Tools). Quantity prediction errors: std dev 0.98 (All) / 0.34 (Tools); absolute error 2.77 / 1.50; average error -1.07 / 0.50.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Factors identified in the paper affecting AWM prediction accuracy include: (1) domain commonality and representation: Codex performed substantially better on the 'tool' technology tree (more common and relevant items); (2) prompt engineering and use of structured output (few-shot in-context examples and code/dictionary output helped parsing and structure extraction); (3) naming/identifier mismatches between LLM output and environment identifiers (e.g., plank vs oak_plank), requiring parsing/normalization; (4) circular dependencies in raw LLM output that had to be removed; (5) ingredient quantities being a common error mode; (6) lack of grounding in environment dynamics (uncorrected LLM facts can be wrong), and (7) scarcity and random world seeds during environment evaluation affecting downstream observed success rates for items.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines and comparisons in the paper: (a) DECKARD (No LLM) — same modular RL method but AWM constructed tabula-rasa from experience; (b) ground-truth AWM (used in controlled error-insertion experiments) as an upper reference; (c) VPT and VPT-a baselines for policy performance (non-finetuned and finetuned VPT). Relative outcomes: LLM-hypothesized AWM (Codex) plus DECKARD pipeline produced large improvements in RL sample efficiency (order-of-magnitude improvement) and faster exploration versus DECKARD without LLM guidance and VPT baselines, even though the LLM AWM itself was imperfect. Experiments also compare time-to-task when starting from ground-truth AWM vs. Codex-hypothesized AWM with artificial errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations and failures include: (1) substantial fraction of incorrect/missing dependencies and incorrect ingredient quantities in Codex output; (2) one item in 391 failed to parse; (3) circular dependencies produced by the LLM needed ad-hoc removal; (4) ungrounded LLM assertions (e.g., predicting glass as collectable without tools) require environment verification to correct; (5) reliance on the environment being language-grounded (inventory-based textual abstraction) — approach not directly applicable when no textual abstraction exists; (6) the AWM approach assumes deterministic transitions and does not handle stochastic AWM transitions in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend treating LLM outputs as hypotheses to be verified via environment interaction rather than as ground truth; use structured few-shot prompts to elicit parseable outputs (code/dictionaries); ground LLM knowledge by verifying/correcting in the environment (Dream/Wake loop) to combine noisy internet-scale knowledge with grounded dynamics; acknowledge improvements come from focusing finetuning on predicted prerequisites (improves sample efficiency); note future work should address automatic identification of state abstractions, generating textual state descriptions from pixels, and extending to stochastic AWMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9342.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9342.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Textual world models (prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grounded textual world models learned from environment interactions (Ammanabrolu & Riedl 2021 mention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites prior work that learns grounded textual world models from environment interactions to assist RL agents in planning and action selection in text-based environments; this is mentioned as related art but not evaluated here.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning knowledge graphbased world models of textual environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Text-based games / textual environment world modeling (natural language-based RL / planning in text environments)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Learn a grounded textual world model from interactions to assist planning and action selection (as reported in the cited prior work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>As noted in the paper's related work, grounding (interaction with environment) is critical — ungrounded language models often fail to reason about real-world dynamics. The present paper uses that insight to verify LLM hypotheses with environment experience.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Not detailed in this paper beyond the general point that ungrounded LLM outputs can be erroneous; the present work frames prior grounded textual-world-model methods as complementary but does not re-evaluate them.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>The authors position their contribution as using few-shot LLM output to hypothesize an AWM and then grounding it via interaction, building on the idea that textual world models can be learned and should be grounded through experience.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning knowledge graphbased world models of textual environments <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>LLM-planner: Few-shot grounded planning for embodied agents with large language models <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Large language models still can't plan (a benchmark for llms on planning and reasoning about change) <em>(Rating: 2)</em></li>
                <li>Semantic exploration from language abstractions and pretrained representations <em>(Rating: 2)</em></li>
                <li>Improving intrinsic exploration with language abstractions <em>(Rating: 2)</em></li>
                <li>Mind's eye: Grounded language model reasoning through simulation <em>(Rating: 2)</em></li>
                <li>Minedojo: Building open-ended embodied agents with internet-scale knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9342",
    "paper_id": "paper-fc918d6f8e2523696c34fa1be5aabdb42e9648d2",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "Codex-AWM",
            "name_full": "OpenAI Codex used to hypothesize an Abstract World Model (AWM)",
            "brief_description": "The authors use OpenAI's Codex (code-davinci-002) with few-shot in-context prompting to generate a structured, textual Abstract World Model (AWM) for Minecraft: a directed acyclic graph of item subgoals (recipes, tool and workbench requirements) represented as a python dictionary. The LLM output is treated as a hypothesis and is verified/corrected through environment interaction during RL training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI Codex (code-davinci-002)",
            "model_description": "Codex is an LLM trained to generate code from natural language prompts; the paper uses the code-davinci-002 Codex variant via few-shot prompting to produce structured python-dictionary outputs encoding Minecraft item dependencies. (No parameter count or training-data specifics are provided in this paper.)",
            "scientific_subdomain": "Embodied reinforcement learning / embodied AI (simulated virtual environments — Minecraft item-crafting technology tree)",
            "simulation_task": "Predict a textual world model (AWM): for 391 Minecraft items generate crafting recipes (ingredients and quantities), required tool, and whether a crafting table or furnace is required; output formatted as a nested python dictionary mapped to a DAG of subgoal nodes and edges.",
            "evaluation_metric": "Accuracy metrics comparing LLM-predicted features to ground-truth AWM: (a) Collectable vs. Craftable label accuracy; (b) Workbench (crafting table / furnace) requirement accuracy; (c) Recipe ingredient correctness; (d) Recipe ingredients & quantities exact-match; plus counts/percentages of incorrectly inserted dependencies and missing dependencies; and error statistics for predicted ingredient quantities (std, absolute error, average error).",
            "simulation_accuracy": "Reported accuracies (Codex hypothesized AWM over 391 items): Collectable vs. Craftable: 57% (All Items), 100% (Tools Only). Workbench (crafting table/furnace): 84% (All Items), 96% (Tools Only). Recipe Ingredients correctness: 66% (All Items), 81% (Tools Only). Recipe Ingredients & Quantities exact match: 55% (All Items), 69% (Tools Only). Additional metrics: % items with incorrectly inserted dependencies: 42% (All), 8% (Tools); % items with missing dependencies: 35% (All), 26% (Tools). Quantity prediction errors: std dev 0.98 (All) / 0.34 (Tools); absolute error 2.77 / 1.50; average error -1.07 / 0.50.",
            "factors_affecting_accuracy": "Factors identified in the paper affecting AWM prediction accuracy include: (1) domain commonality and representation: Codex performed substantially better on the 'tool' technology tree (more common and relevant items); (2) prompt engineering and use of structured output (few-shot in-context examples and code/dictionary output helped parsing and structure extraction); (3) naming/identifier mismatches between LLM output and environment identifiers (e.g., plank vs oak_plank), requiring parsing/normalization; (4) circular dependencies in raw LLM output that had to be removed; (5) ingredient quantities being a common error mode; (6) lack of grounding in environment dynamics (uncorrected LLM facts can be wrong), and (7) scarcity and random world seeds during environment evaluation affecting downstream observed success rates for items.",
            "comparison_baseline": "Baselines and comparisons in the paper: (a) DECKARD (No LLM) — same modular RL method but AWM constructed tabula-rasa from experience; (b) ground-truth AWM (used in controlled error-insertion experiments) as an upper reference; (c) VPT and VPT-a baselines for policy performance (non-finetuned and finetuned VPT). Relative outcomes: LLM-hypothesized AWM (Codex) plus DECKARD pipeline produced large improvements in RL sample efficiency (order-of-magnitude improvement) and faster exploration versus DECKARD without LLM guidance and VPT baselines, even though the LLM AWM itself was imperfect. Experiments also compare time-to-task when starting from ground-truth AWM vs. Codex-hypothesized AWM with artificial errors.",
            "limitations_or_failure_cases": "Reported limitations and failures include: (1) substantial fraction of incorrect/missing dependencies and incorrect ingredient quantities in Codex output; (2) one item in 391 failed to parse; (3) circular dependencies produced by the LLM needed ad-hoc removal; (4) ungrounded LLM assertions (e.g., predicting glass as collectable without tools) require environment verification to correct; (5) reliance on the environment being language-grounded (inventory-based textual abstraction) — approach not directly applicable when no textual abstraction exists; (6) the AWM approach assumes deterministic transitions and does not handle stochastic AWM transitions in this work.",
            "author_recommendations_or_insights": "Authors recommend treating LLM outputs as hypotheses to be verified via environment interaction rather than as ground truth; use structured few-shot prompts to elicit parseable outputs (code/dictionaries); ground LLM knowledge by verifying/correcting in the environment (Dream/Wake loop) to combine noisy internet-scale knowledge with grounded dynamics; acknowledge improvements come from focusing finetuning on predicted prerequisites (improves sample efficiency); note future work should address automatic identification of state abstractions, generating textual state descriptions from pixels, and extending to stochastic AWMs.",
            "uuid": "e9342.0",
            "source_info": {
                "paper_title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Textual world models (prior work)",
            "name_full": "Grounded textual world models learned from environment interactions (Ammanabrolu & Riedl 2021 mention)",
            "brief_description": "The paper cites prior work that learns grounded textual world models from environment interactions to assist RL agents in planning and action selection in text-based environments; this is mentioned as related art but not evaluated here.",
            "citation_title": "Learning knowledge graphbased world models of textual environments",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": null,
            "scientific_subdomain": "Text-based games / textual environment world modeling (natural language-based RL / planning in text environments)",
            "simulation_task": "Learn a grounded textual world model from interactions to assist planning and action selection (as reported in the cited prior work).",
            "evaluation_metric": null,
            "simulation_accuracy": null,
            "factors_affecting_accuracy": "As noted in the paper's related work, grounding (interaction with environment) is critical — ungrounded language models often fail to reason about real-world dynamics. The present paper uses that insight to verify LLM hypotheses with environment experience.",
            "comparison_baseline": null,
            "limitations_or_failure_cases": "Not detailed in this paper beyond the general point that ungrounded LLM outputs can be erroneous; the present work frames prior grounded textual-world-model methods as complementary but does not re-evaluate them.",
            "author_recommendations_or_insights": "The authors position their contribution as using few-shot LLM output to hypothesize an AWM and then grounding it via interaction, building on the idea that textual world models can be learned and should be grounded through experience.",
            "uuid": "e9342.1",
            "source_info": {
                "paper_title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning knowledge graphbased world models of textual environments",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "LLM-planner: Few-shot grounded planning for embodied agents with large language models",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Large language models still can't plan (a benchmark for llms on planning and reasoning about change)",
            "rating": 2
        },
        {
            "paper_title": "Semantic exploration from language abstractions and pretrained representations",
            "rating": 2
        },
        {
            "paper_title": "Improving intrinsic exploration with language abstractions",
            "rating": 2
        },
        {
            "paper_title": "Mind's eye: Grounded language model reasoning through simulation",
            "rating": 2
        },
        {
            "paper_title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
            "rating": 1
        }
    ],
    "cost": 0.012506999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling</h1>
<p>Kolby Nottingham ${ }^{1}$ Prithviraj Ammanabrolu ${ }^{2}$ Alane Suhr ${ }^{2}$<br>Yejin Choi ${ }^{32}$ Hannaneh Hajishirzi ${ }^{32}$ Sameer Singh ${ }^{12}$ Roy Fox ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM—successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.</p>
<h2>1. Introduction</h2>
<p>Despite evidence that practical sequential decision making systems require efficient exploitation of prior knowledge regarding a task, the current prevailing paradigm in reinforcement learning (RL) is to train tabula rasa, without any</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>pretraining or external knowledge (Agarwal et al., 2022). In an effort to shift away from this paradigm, we focus on the task of creating embodied RL agents that can effectively exploit large-scale external knowledge sources presented in the form of pretrained large language models (LLMs).</p>
<p>LLMs contain potentially useful knowledge for completing tasks and compiling knowledge sources (Petroni et al., 2019). Previous work has attempted to apply knowledge from LLMs to decision-making by generating action plans for executing in an embodied environment (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b; Singh et al., 2022; Liang et al., 2022b; Huang et al., 2022a). However, LLMs still often fail when generating plans due to a lack of grounding (Valmeekam et al., 2022). Additionally, many of these agents that rely on LLM knowledge at execution time are limited in performance by the accuracy of LLM output. We hypothesize that if LLMs are instead applied to improving exploration during training, resulting policies will not be constrained by the accuracy of an LLM.</p>
<p>Exploration in environments with sparse rewards becomes increasingly difficult as the size of the explorable state space increases. For example, the popular 3D embodied environment Minecraft has a large technology tree of craftable items with complex dependencies and a high branching factor. Before crafting a stone pickaxe in Minecraft an agent must: collect logs, craft logs into planks and then sticks, craft a crafting table from planks, use the crafting table to craft a wooden pickaxe from sticks and planks, use the wooden pickaxe to collect cobblestone, and finally use the crafting table to craft a stone pickaxe from sticks and cobblestone. Reaching a goal item is difficult without expert knowledge of Minecraft via dense rewards (Baker et al., 2022; Hafner et al., 2023) or expert demonstrations (Skrynnik et al., 2021; Patil et al., 2020), making item crafting in Minecraft a longstanding AI challenge (Guss et al., 2019; Fan et al., 2022).</p>
<p>We propose DECKARD* (DECision-making for Knowledgable Autonomous Reinforcement-learning Dreamers), an agent that hypothesizes an Abstract World Model (AWM) over subgoals by few-shot prompting an</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. During the Dream phase, DECKARD uses the LLM-predicted DAG of subgoals, the hypothesized Abstract World Model (AWM), to sample a node on the path to the current task. Then, during the Wake phase, the agent executes subgoals and explores until reaching the sampled node. The AWM is corrected and discovered nodes marked as verified.</p>
<p>LLM, then exploits the AWM for exploration and verifies the AWM with grounded experience. As seen in Figure 1, DECKARD operates in two phases: (1) the Dream phase where it uses the hypothesized AWM to suggest the next node to explore from the directed acyclic graph (DAG) of subgoals; and (2) the Wake phase where it learns a modular policy of subgoals, each trained on RL objectives, and verifies the hypothesized AWM with grounded environment dynamics. Figure 1 shows two iterations of the DECKARD agent learning the "craft a stone pickaxe" task in Minecraft. During the first Dream phase, the agent has already verified the nodes $\log$ and plank, and DECKARD suggests exploring towards the stick subgoal, ignoring nodes such as door that are not predicted to complete the task. Then, during the following Wake phase, DECKARD executes each subgoal in the branch ending in the stick node and then explores until it successfully crafts a stick. If successful, the agent marks the newly discovered node as verified in its AWM before proceeding to the next iteration.</p>
<p>We evaluate DECKARD on learning to craft items in the Minecraft technology tree. We show that LLM-guidance is essential to exploration in DECKARD, with a version of our agent without LLM-guidance taking over twice as long to craft most items during open-ended exploration. Whereas, when exploring towards a specific task, DECKARD improves sample-efficiency by an order of magnitude versus comparable agents, (12x the ablated DECKARD without LLM-guidance). Our method is also robust to task decomposition errors in the LLM, consistently outperforming baselines as we introduce errors in the LLM output. DECKARD
demonstrates the potential for robustly applying LLMs to RL, thus enabling RL agents to effectively use large-scale, noisy prior knowledge sources for exploration.</p>
<h2>2. Related Work</h2>
<h3>2.1. Language-Assisted Decision Making</h3>
<p>Textual knowledge can be used to improve generalization in reinforcement learning through environment descriptions (Branavan et al., 2011; Zhong et al., 2020; Hanjie et al., 2021) or language instructions (Chevalier-Boisvert et al., 2019; Anderson et al., 2018; Ku et al., 2020; Shridhar et al., 2020). However, task specific textual knowledge is expensive to obtain, prompting the use of web queries (Nottingham et al., 2022) or models pretrained on general world knowledge (Dambekodi et al., 2020; Suglia et al., 2021; Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b).</p>
<p>LLMs can also be used as an external knowledge source by prompting or finetuning them to generate action plans. However, by default, the generated plans are not grounded in environment dynamics and constraining output can harm model performance, both of which lead to subpar performance of out-of-the-box LLMs on decision-making tasks (Valmeekam et al., 2022). Existing work that uses LLMs for generating action plans focuses on methods for grounding language in environment states (Ichter et al., 2022; Huang et al., 2022b; Song et al., 2022b), or improving LLM plans through more structured output (Singh et al., 2022; Liang et al., 2022b). In this work, we focus on using LLMs for exploration rather than directly generating action plans.</p>
<p>Tam et al. (2022) and Mu et al. (2022) recently demonstrated that language is a meaningful state abstraction when used for exploration. Additionally, Tam et al. (2022) experiment with using LLM latent representations of state descriptions for novelty exploration, relying on pretrained LLM encodings to detect novel textual states. To the best of our knowledge, we are the first to apply language-assisted decision-making to exploration by using LLMs to predict and verify environment dynamics through experience.</p>
<h3>2.2. Language Grounded in Interaction</h3>
<p>Without grounding, LLMs often fail to reason about real world dynamics (Bisk et al., 2020). Instruction following tasks have been a popular testbed for language grounding (Chevalier-Boisvert et al., 2019; Anderson et al., 2018; Ku et al., 2020; Shridhar et al., 2020) prompting many improvements to decision making conditioned on language instructions (Yu et al., 2018; Lynch \&amp; Sermanet, 2020; Nottingham et al., 2021; Suglia et al., 2021; Kuo et al., 2021; Zellers et al., 2021; Song et al., 2022a; Blukis et al., 2022). Other prior work used environment interactions to ground responses from question answering models in environment state (Gordon et al., 2018; Das et al., 2018) or physics (Liu et al., 2022). Finally, Ammanabrolu \&amp; Riedl (2021) learn a grounded textual world model from environment interactions to assist an RL agent in planning and action selection. In this work, our DECKARD agent also uses a type of textual world model but it is obtained few-shot from an LLM and then grounded in environment dynamics by verifying hypotheses through interaction.</p>
<h3>2.3. Modularity in RL</h3>
<p>Modular RL proposes to learn several independent policies in a composable way to facilitate training and generalization (Simpkins \&amp; Isbell, 2019). Ammanabrolu et al. (2020) and Patil et al. (2020) demonstrate how modular policies can improve exploration by reducing policy horizons, the former using the text-based game Zork and the latter using Minecraft. We implement modularity for Minecraft by finetuning a pretrained transformer policy with adapters, a technique recently implemented for RL by Liang et al. (2022a) for multi-task robotic policies.</p>
<h3>2.4. Minecraft</h3>
<p>Minecraft is a vast open-ended world with complex dynamics and sparse rewards. Crafting items in the Minecraft technology tree has long been considered a challenging task for reinforcement learning, requiring agents to overcome extremely delayed rewards and difficult exploration (Skrynnik et al., 2021; Patil et al., 2020; Hafner et al., 2023). This is partially due to the scarcity of items in the environment, but also due to the depth of some items in the game's technol- ogy tree. The purpose of our work is to overcome the latter of these two difficulties by better learning and navigating Minecraft's technology tree.</p>
<p>Several existing agents overcome the problem of item scarcity in Minecraft by simplifying environment parameters such as action duration (Patil et al., 2020) or block break time (Hafner et al., 2023), making comparison between methods difficult. For this reason we compare minimally to other Minecraft agents (see Table 2), focusing our evaluation on the benefits of LLM-guided exploration with DECKARD. We use the video pretrained (VPT) Minecraft agent (Baker et al., 2022) as a starting point for exploration and finetuning, and we use the Minedojo implementation of the Minecraft Environment (Fan et al., 2022).</p>
<h2>3. Background</h2>
<h3>3.1. Modular Reinforcement Learning</h3>
<p>Rather than train a single policy with sparse rewards, modular RL advocates learning compositional policy modules (Simpkins \&amp; Isbell, 2019). DECKARD automatically discovers subgoals in Minecraft-each of which maps to an independently trained policy module-and learns a DAG of dependencies (the AWM) to transition between subgoals. Policy modules are trained in an environment modeled by a POMDP with states $s \in \mathcal{S}$, obseravtions $o \in O$, actions $a \in \mathcal{A}$, and environment dynamics $\mathcal{T}: \mathcal{S}, \mathcal{A} \rightarrow \mathcal{S}^{\prime}$. These elements are common between modules, but each subgoal defines different initial states $\mathcal{S}<em 0="0">{0}$ and observations $O</em>}$, terminal states $\mathcal{S<em 0="0">{t}$, and reward functions $\mathcal{R}: \mathcal{S}, \mathcal{A} \rightarrow \mathbb{R}$, according to the particular subgoal. $\mathcal{S}</em>}$ and $O_{0}$ are defined by the current subgoal's parents in the DAG, and $\mathcal{S<em 0="0">{t}$ and $\mathcal{R}$ are defined by the current subgoal. For example, the craft wooden pickaxe subgoal has parents craft planks and craft stick, so $\mathcal{S}</em>$ includes these items in the agent's starting inventory. This subgoal recieves a reward and terminates when a wooden pickaxe is added to the agent's inventory. Section 5 and Appendix B provide more details.</p>
<p>Due to the compositionality of modular RL, individual modules can be chained together to achieve complex tasks. In our case, given a goal state $s_{g}$, we use the subgoal DAG to create a path from our current state to $s_{g},\left[s_{0}, s_{1}, \ldots, s_{g}\right]$, where each $s$ represents the terminal state for a subgoal. By chaining together sequences of subgoal modules, we can successfully navigate to connected portions of the currently discovered DAG and reach arbitrary goal states.</p>
<h3>3.2. Large Language Models</h3>
<p>Large language models (LLM) are trained with a language modeling objective to maximize the likelihood of training data from large text corpora. As LLMs have grown in size and representational power, they have seen success on var-</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 DECKARD
    \(G \leftarrow L L M() \quad / /\) hypothesize AWM with LLM
    \(C \leftarrow X: 0 \quad / /\) dict of visit counts
    \(V \leftarrow \emptyset \quad / /\) set of verified nodes
    while training do
        // Dream Phase
        \(F \leftarrow \operatorname{Frontier}(G, V)\)
        if \(\operatorname{any}\left(C(F) \leq c_{0}\right)\) then
            \(\bar{x} \leftarrow \operatorname{SampleBranch}\left(F \mid C(F) \leq c_{0}\right)\)
        else
            \(\bar{x} \leftarrow \operatorname{SampleBranch}(F \cup V)\)
        end if
        // Wake Phase
        \(x \leftarrow x_{0}\)
        for \(t=1 \ldots|\bar{x}|\) do
            \(x^{\prime} \leftarrow \operatorname{ExecuteSubgoal}\left(\bar{x}_{t}\right)\)
            \(C\left(x^{\prime}\right) \leftarrow C\left(x^{\prime}\right)+1\)
            if \(x^{\prime} \notin V\) then
                \(G \leftarrow \operatorname{AddEdge}\left(G, x, x^{\prime}\right)\)
                \(V \leftarrow V \cup\left\{x^{\prime}\right\}\)
            end if
            \(x \leftarrow x^{\prime}\)
        end for
    end while
</code></pre></div>

<p>ious downstream tasks by simply modifying their input, referred to as prompt engineering (Brown et al., 2020). Recent applications of LLMs to decision-making have relied partially or entirely on prompt engineering for their action planning (Ichter et al., 2022; Song et al., 2022b; Huang et al., 2022b; Singh et al., 2022; Liang et al., 2022b). We follow this pattern to extract knowledge from LLMs and construct our AWM. We prompt OpenAI's Codex model (OpenAI, 2022) to generate DECKARD's hypothesized AWM. Codex is trained to generate code samples from natural language. As with previous work (Singh et al., 2022; Liang et al., 2022b), we find that structured code output works well for extracting knowledge from LLMs. We structure LLM output by prompting Codex to generate a python dictionary of Minecraft item dependencies, which we then map to a graph of items and their dependencies (see Section 5.1 and Appendix A).</p>
<h2>4. DECKARD</h2>
<h3>4.1. Abstract World Model</h3>
<p>Our method, DECision-making for Knowledgable Autonomous Reinforcement-learning Dreamers (DECKARD), builds an Abstract World Model (AWM) of subgoal dependencies from state abstractions. We begin by assuming a textual state representation function $\phi: O \rightarrow X$. Textual state representations $x \in X$ make up
the nodes for our AWM $G: X, E$ with directed edges $E$ defining the dependencies between $X$. We further constrain $G$ to a directed acyclic graph (DAG) so that the nodes of the DAG represent subgoals useful in navigating towards a target goal. In our experiments, we use the agent's current inventory as $X$, a common component of the Minecraft observation space (Fan et al., 2022; Hafner et al., 2023).</p>
<p>We update $G$ from agent experience through environment exploration. When the agent experiences node $x_{t}$ for the first time, $G$ is updated by adding edges between the previous node $x_{t-1}$ and the new node $x_{t}$. When trying to reach a previously experienced node, DECKARD recovers the path from current node $x_{0}$ to the target node $x_{t}$ from the AWM. DECKARD then executes policies for each recovered node until it reaches the target goal.</p>
<h3>4.2. LLM Guidance</h3>
<p>The setup so far (referred to in our experiments as "DECKARD (No LLM)") allows the construction of a modular RL policy for navigating subgoals. However, the agent is still learning the AWM tabula-rasa. The key insight of DECKARD is that we can hypothesize the AWM with knowledge from an LLM. We use in-context learning, as described in Section 5.1, to predict $G$ from an LLM with predicted edges, $\hat{E}$. While acting in the environment, we verify or correct edges of $G$ and track the set of nodes that have been verified $V$ thus grounding the AWM hypothesized by the LLM in environment dynamics.</p>
<h3>4.2.1. DreAM PHASE</h3>
<p>Equipped with a hypothesized AWM, we iterate between Dream and Wake phases for guided exploration toward a goal (see Algorithm 1). During the Dream phase, we compute the verified frontier $F$ of $G$, composed of verified nodes $V$, with predicted edges to unverified nodes $G-V$. In addition, if a path between $V$ and the current task's goal exists, $F$ is pruned to only include nodes along the predicted path to the goal. For example, after learning to craft planks, subgoals door and stick are potential frontier nodes. However, if the target item is wooden pickaxe, DECKARD will eliminate door as a candidate node for exploration since stick is part of the LLM-predicted recipe for the target item and door is not. Finally, we sample a branch $\bar{x}$ terminating with an element from $F$ to explore during the Wake phase. If all nodes in $F$ have been sampled at least $c_{0}$ times (where $c_{0}$ is an exploration hyperparameter) without success, we the sample from all $V$ rather than $F$ only.</p>
<h3>4.2.2. WAKE PHASE</h3>
<p>Next, during the Wake phase, the agent executes the sequence of subgoals $\bar{x}$ updating $G$ with learned experience and adding verified nodes to $V$. If sampled from $F$, the final</p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>All Items</th>
<th>Tools Only</th>
</tr>
</thead>
<tbody>
<tr>
<td>Collectable vs. Craftable</td>
<td>57</td>
<td>100</td>
</tr>
<tr>
<td>Crafting Table / Furnace</td>
<td>84</td>
<td>96</td>
</tr>
<tr>
<td>Recipe Correct Items</td>
<td>66</td>
<td>81</td>
</tr>
<tr>
<td>Recipe Exact Match</td>
<td>55</td>
<td>69</td>
</tr>
</tbody>
</table>
<p>node in $\bar{x}$ will be unlearned, allowing the agent to explore in an attempt to reach the unverified node. If successful, the AWM is updated and the new node is also added to $V$. When adding a newly verified node $x$ we begin finetuning a new subgoal policy for $x$ (see Section 5). Beyond reducing the number of iterations it takes to construct $G$, one benefit of initializing $G$ with an LLM is that we do not finetune subgoals for nodes outside of the predicted path to our target goal. If the predicted recipes fail, then DECKARD begins training additional subgoal policies to assist in exploration. This drastically reduces the number of environment steps required to train DECKARD.</p>
<h2>5 Experiment Setup</h2>
<p>We apply DECKARD to crafting items in Minecraft, an embodied learning environment that requires agents to perform sequences of subgoals with sparse rewards. Our agent maps inventory items to AWM subgoals and learns a modular policy that can be composed to achieve complex tasks. By learning modular policies, our agent is able to collect and craft arbitrary items in the Minecraft technology tree.</p>
<h3>5.1 Predicting the Abstract World Model</h3>
<p>In our experiments, we predict the AWM using OpenAI’s Codex model <em>OpenAI (2022)</em> by prompting the LLM to generate recipes for Minecraft items. We prompt Codex to “Create a nested python dictionary containing crafting recipes and requirements for minecraft items” along with additional instructions about the dictionary contents and two examples: diamond pickaxe and diamond (see Appendix A). We iterate over 391 Minecraft items, generating recipes as well as tool requirements (mining stone requires a pickaxe) and workbench requirements (crafting a pickaxe requires a crafting table). The hypothesized AWM is generated at the start of training, so no forward passes of the LLM are necessary during training or inference. Table 1 shows the accuracy of the hypothesized un-grounded AWM.</p>
<p>Table 1. LLM accuracy when predicting various node features: whether an item is collectable (no parents) or craftable (has a recipe), whether it requires a crafting table or furnace to craft, whether recipe ingredients are correct, and whether the recipe is an exact match (including ingredient quantities). The first results column includes all 391 Minecraft items, whereas the second column only includes the 37 items in the tool technology tree.</p>
<h3>5.2 Subgoal Finetuning</h3>
<p>Rather than train each module from scratch, we finetune transformer adapters for each module with an RL objective following the adapter architecture from <em>Houlsby et al. (2019)</em>. We use the Video-Pretrained (VPT) Minecraft model as our starting policy <em>Baker et al. (2022)</em>. We chose to finetune VPT as it proved to be more sample efficient and more stable than training policies from scratch. Moreover, since VPT is pretrained on a variety of Minecraft skills, the non-finetuned VPT model explores the environment more thoroughly than a random agent. Our implementation of VPT finetuned with adapters is referred to as VPT-a.</p>
<p>Adapters are especially well suited for modular finetuning due to their lightweight architecture <em>Liang et al. (2022a)</em>. In our agent, each subgoal module corresponds to one set of adapters and only contains 9.5 million trainable parameters, approximately 2% of the 0.5 billion parameter VPT model. This allows us to train a separate set of adapters for each subgoal and still keep all parameters in memory concurrently, a practical benefit of using adapters for modular, compositional RL policies.</p>
<h3>5.3 Environment Details</h3>
<p>We use Minedojo’s Minecraft implementation for our experiments <em>Fan et al. (2022)</em>. As with VPT <em>Baker et al. (2022)</em>, our subgoal policies use a pixel only observation space and a large multi-discrete action space, while our overall policy transitions between subgoals based on the agent’s current inventory. Unlike VPT, we use standard high-level crafting actions that instantly crafts target items from inventory items. At the time of this writing, Minedojo does not support the VPT style of human-like crafting in a GUI, so we instead remove the VPT action for opening the crafting GUI and replace it with 254 crafting actions (one for each item). This brings our multi-discrete action space to 121 camera actions and 8714 keyboard/crafting actions, and our observation space to 128x128x3 pixels plus 391 inventory item quantities (only used to transition between subgoals).</p>
<p>Because our subgoals map to individual items, there is an intrinsic separation between items that are collected from the environment versus those that require crafting. While we must finetune a set of adapters for subgoals that require navigating or collecting items from the environment, crafting subgoal policies map to a single craft action—making them much more space and sample efficient compared to collectable item subgoals.</p>
<h3>5.4 Experiments</h3>
<p>We evaluate DECKARD on both crafting tasks—in which the agent learns to collect ingredients and craft a target item—and open-ended exploration. In open-ended exploration,</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Rate of exploration for during open-ended exploration, measured by the size of the verified AWM per iteration. Each iteration includes one Dream and one Wake phase. VPT measures the number of items discovered by a non-finetuned VPT policy and No LLM ablates LLM guidance. LLM guidance more than halves the time it takes to discover difficult items such as stone tools and glass.
although there is no extrinsic learning signal, DECKARD is intrinsically motivated to explore new AWM nodes. We compare the growth of the agent's verified AWM during open-ended exploration for DECKARD with and without LLM guidance along with a VPT baseline. Next, we compare LLM-guided DECKARD to RL baselines and DECKARD without LLM guidance on goal-driven tasks for collecting/crafting: logs, wooden pickaxes, cobblestone, stone pickaxes, furnaces, sand, and glass. We also compare to several popular Mincraft agents on the "craft a stone pickaxe task" (see Table 2). Finally, we evaluate the effect of artificial errors in the hypothesized AWM to simulate errors in LLM output and demonstrate DECKARD's robustness to LLM accuracy.</p>
<h2>6. Experiment Results</h2>
<h3>6.1. Open-Ended Exploration</h3>
<p>DECKARDis intrinsically motivated to explore new nodes, always sampling and attempting to craft new items, and thus does not require a target task to improve exploration. We can measure the effect of DECKARD on exploration by tracking the growth of the agent's verified AWM nodes. Figure 2 shows the speed of exploration when using DECKARD with and without LLM guidance. We also compare DECKARD to a VPT baseline that explores the environment without an AWM with a non-finetuned VPT policy. Although VPT does not construct an AWM, it gathers Minecraft items and randomly attempts to craft new items from the gathered ingredients. We track how many items it has discovered and plot that quantity in Figure 2.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. DECKARD prunes the AWM by only sampling from the frontier of verified and hypothesized AWM nodes. Without LLM guidance, our agent would sample from the entire AWM during exploration. However, the AWM grows in size throughout training and many nodes become dead ends, slowing exploration.</p>
<p>DECKARD without LLM guidance constructs an AWM from scratch, but only the LLM-guided DECKARD agent uses LLM guidance to decide which items to collect and which recipes to attempt next. Note that DECKARD subgoal policies are initialized with VPT, so VPT starts out exploring at a similar rate to DECKARD.</p>
<p>The DECKARD and VPT agents quickly learn to mine logs and craft wooden items. However, one exploration hurdle is discovering that wooden pickaxes are a prerequisite for mining cobblestone. As seen in Figure 2, it takes DECKARD without LLM guidance and the VPT baseline $2 x$ and $3 x$ longer respectively to learn to use a wooden pickaxe to mine cobblestone. Once the agents learn how to mine cobblestone, they can begin adding stone items to their AWM. However, only DECKARD avoids oversampling dead ends in the crafting tree allowing it to quickly explore new states. Also, the LLM incorrectly predicts that glass can be collected without crafting or tools of any kind, but DECKARD overcomes and corrects this error, successfully crafting glass and adding the correct recipe to the AWM.</p>
<p>In general, the frontier $F$ of the verified AWM nodes is much smaller than $G$. This difference increases as the agent continues to explore and add verified nodes to $G$. Figure 3 shows the sizes of $G$ and $F$ throughout open-ended exploration for DECKARD. The smaller size of $F$ means that each iteration DECKARD is more likely to sample items that are useful for crafting something new. Eventually, difficult to reach or erroneous nodes in $F$ could limit exploration, so we stop prioritizing sampling from the frontier after $c_{0}$ failed attempts to reach nodes from $F$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Success rates for item tasks on random world seeds. The VPT agent shows success rates of the pretrained VPT policy without any additional finetuning. VPT-a finetunes VPT using the same training setup as DECKARD without modularity or LLM guidance. As indicated by the results for log and sand (item tasks composed of a single subgoal), VPT-a is equivalent to a single subgoal policy. DECKARD without LLM-guidance has the same success rate as the full DECKARD agent.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. LLM guidance improves environment timestep efficiency by an order of magnitude by only learning policies for predicted prerequisites of target items.</p>
<h3>6.2. Crafting Tasks</h3>
<p>We also evaluate DECKARD on tasks that challenge the agent to collect or crafting a specific item. The training procedure for items tasks is the same, but rather than sample from the entire frontier $F$ as with open-ended exploration, we only sample nodes from $F$ predicted to lead to the target item. We conclude training when the target item is obtained.</p>
<p>Figure 4 compares DECKARD success rates to baselines across item tasks: logs, wooden and stone pickaxes, cobblestone, furnace, sand, and glass. The VPT baseline is the non-finetuned VPT policy acting in the environment, and VPT-a follows the same training setup as our subgoal policies (see Section 5.2). Agents are allowed a maximum of 1,000 environment steps to obtain collectable items (log and sand), and 5,000 steps for all other craftable items. Training for each agent is limited to 6 million steps, although DECKARD only takes that many for the "craft glass" task. DECKARD outperforms directly training on item tasks with a traditional reinforcement learning signal and learns to craft items further up the technology tree where the baseline completely fails.</p>
<p>Note that we use random world seeds for all evaluation making scarce items more difficult to reliably collect. For example, the fact that sand is more rare than logs is reflected in their respective success rates in Figure 4. Also, items that depend on logs (pickaxes, cobblestone, furnace) and sand (glass) will have success rates bounded by that of their parent nodes in the AWM.</p>
<p>The sample efficiency of DECKARD with LLM guidance is especially notable when applied to item crafting and collecting tasks. With LLM guidance, DECKARD can avoid learning subgoal policies for items it predicts are unnecessary for the current goal (see Section 4.2.2). Figure 5 demonstrates the large difference in sample efficiency when only training policies for predicted subgoals. Without LLM guidance, DECKARD finetunes subgoal policies for an average of fifteen different collectable items when learning to craft a stone pickaxe. With guidance, DECKARD only finetunes subgoal policies for collecting needed items (such as logs and cobblestone)—resulting in an order of magnitude improvement in sample efficiency.</p>
<p>Although not the primary goal of this work, we compare DECKARD to several agents from previous work trained to craft items along the Minecraft technology tree. Table 2 includes a high level overview of these agents and shows the number of environment samples for each to learn the "craft stone pickaxe" task. Note that each of these agents uses different action and observation spaces as well as pretraining data. For example, DECKARD does not require any reward shaping from domain expertise, expert demonstrations, or simplifications of the observation and action spaces. As mentioned in Section 5.3, we do follow previous work and use discrete actions for item crafting. At the time of writing, VPT is the only agent that learns low-level item crafting using the graphical crafting interface. Table 2 shows that DECKARD's sample efficiency is equal to or better than that of previous work.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Demos</th>
<th>Dense Rewards</th>
<th>Auto-crafting</th>
<th>Observations</th>
<th>Actions</th>
<th>Params</th>
<th>Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Align-RUDDER <em>(Patil et al., 2020)</em></td>
<td>Expert</td>
<td>✗</td>
<td>✓</td>
<td>Pixels &amp; Meta</td>
<td>61</td>
<td>2.5M/subgoal</td>
<td>2M</td>
</tr>
<tr>
<td>VPT+RL <em>(Baker et al., 2022)</em></td>
<td>Videos</td>
<td>✓</td>
<td>✗</td>
<td>Pixels Only</td>
<td>121, 8461</td>
<td>248M</td>
<td>2.4B</td>
</tr>
<tr>
<td>DreamerV3 <em>(Hafner et al., 2023)</em></td>
<td>None</td>
<td>✓</td>
<td>✓</td>
<td>Pixels &amp; Meta</td>
<td>25</td>
<td>200M</td>
<td>6M</td>
</tr>
<tr>
<td>DECKARD (No LLM)</td>
<td>Videos</td>
<td>✗</td>
<td>✓</td>
<td>Pixels &amp; Inventory</td>
<td>121, 8714</td>
<td>9.5M/subgoal</td>
<td>32M</td>
</tr>
<tr>
<td>DECKARD</td>
<td>Videos</td>
<td>✗</td>
<td>✓</td>
<td>Pixels &amp; Inventory</td>
<td>121, 8714</td>
<td>9.5M/subgoal</td>
<td>2.6M</td>
</tr>
</tbody>
</table>
<p>Table 2. We limit comparison between minecraft agents because of the various shortcuts used to solve the difficult exploration task. Align-RUDDER, relies on expert demonstrations. DreamerV3 and Align-RUDDER, simplify the action space. VPT+RL and DreamerV3 provide intermediate crafting rewards. The final column above compares how long each method takes to learn the “craft stone pickaxe” task. Despite its challenging learning setup, DECKARD achieves sample efficiency equal to or better than existing agents.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Effect of errors in the initial AWM, measured by time to craft a stone pickaxe. Starting from a ground truth AWM, the error rate indicates the percentage of artificially inserted/deleted edges to simulate errors in LLM output.</p>
<h3>6.3 Robustness</h3>
<p>Finally, we evaluate our claim that DECKARD is robust to errors in LLM output. While LLMs are becoming surprisingly knowledgeable, they are not grounded in environment knowledge and sometimes output erroneous facts <em>(Valmeekam et al., 2022)</em>. Figure 6 shows training time for DECKARD on the target task “craft stone pickaxe” for various error types and rates in the hypothesized AWM. For each run, we start with a ground truth AWM and artificially introduce errors over at least three different random seeds for each error type and rate.</p>
<p>The most common error in our LLM-hypothesized AWM was ingredient quantity (see Table 1), but we found that DECKARD was robust to this error and often ended up with a surplus of ingredients. Figure 6 shows the effect of inserting and deleting edges from the ground truth AWM. Inserted edges always add sand as an ingredient for the current item, and deleted edges may remove recipe ingredients or a required tool/crafting table. DECKARD with LLM guidance successfully outperforms DECKARD without LLM guidance even when faced with large errors in LLM output, demonstrating DECKARD’s robustness to LLM output as an exploration method.</p>
<h2>7 Discussion &amp; Conclusion</h2>
<p>In line with proposals to utilize pretrained models in RL <em>(Agarwal et al., 2022)</em>, we extract knowledge from LLMs in the form of an Abstract World Model (AWM) that defines transitions between subgoals in a directed acyclic graph. Our agent, DECKARD (DECision-making for Knowledgable Autonomous Reinforcement-learning Dreamers), successfully uses the AWM to intelligently explore Minecraft item crafting, learning to craft arbitrary items through a modular RL policy. Initializing DECKARD with an LLM-predicted AWM improves sample efficiency by an order of magnitude. Additionally, we use environment dynamics to ground the hypothesized AWM by verifying and correcting it with agent experience, robustly applying large-scale, noisy knowledge sources to aid in sequential decision-making.</p>
<p>We, along with many others, hope to utilize the potential of LLMs for unlocking internet-scale knowledge for decision-making. Throughout this effort, we encourage the pursuit of robust and generalizable methods, like DECKARD. One drawback of DECKARD, along with many other LLM-assisted RL methods, is that it requires an environment already be grounded in language. Some preliminary methods for generating state descriptions from images are used by <em>Tam et al. (2022)</em>, but this remains an open area of research. Additionally, we assume an abstraction over environment states to make predicting dependencies scalable. We leave the problem of of automatically identifying state abstractions to future work. Finally, DECKARD considers only deterministic transitions in the AWM. While a similar approach to ours could be applied to stochastic AWMs, that is out of the scope of this work.</p>
<p>DECKARD introduces a general approach for utilizing pretrained LLMs for guiding agent exploration. By alternating between sampling predicted next subgoals on the frontier of agent experience (The Dream phase) and executing subgoal policies to expand the frontier (The Wake phase), we successfully ground noisy LLM world knowledge with environment dynamics and learn a modular policy over compositional subgoals.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Dheeru Dua, Dylan Slack, Anthony Chen, Catarina Belem, Shivanshu Gupta, Tamanna Hossain, Yasaman Razeghi, Preethi Seshardi, and the anonymous reviewers for their discussions and feedback. Roy Fox is partly funded by the Hasso Plattner Foundation.</p>
<h2>References</h2>
<p>Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare, M. G. Reincarnating reinforcement learning: Reusing prior computation to accelerate progress. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=t3X5yMI_4G2.</p>
<p>Ammanabrolu, P. and Riedl, M. Learning knowledge graphbased world models of textual environments. Advances in Neural Information Processing Systems, 34:3720-3731, 2021.</p>
<p>Ammanabrolu, P., Tien, E., Hausknecht, M., and Riedl, M. O. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. arXiv preprint arXiv:2006.07409, 2020.</p>
<p>Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., Sünderhauf, N., Reid, I., Gould, S., and van den Hengel, A. Vision-and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.</p>
<p>Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., and Clune, J. Video pretraining (vpt): Learning to act by watching unlabeled online videos. In Advances in Neural Information Processing Systems, 2022.</p>
<p>Bisk, Y., Holtzman, A., Thomason, J., Andreas, J., Bengio, Y., Chai, J., Lapata, M., Lazaridou, A., May, J., Nisnevich, A., et al. Experience grounds language. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 8718-8735, 2020.</p>
<p>Blukis, V., Paxton, C., Fox, D., Garg, A., and Artzi, Y. A persistent spatial semantic representation for high-level natural language instruction execution. In Faust, A., Hsu, D., and Neumann, G. (eds.), Proceedings of the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research, pp. 706-717. PMLR, 0811 Nov 2022. URL https://proceedings.mlr. press/v164/blukis22a.html.</p>
<p>Branavan, S., Silver, D., and Barzilay, R. Learning to win by reading manuals in a monte-carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 268-277, 2011.</p>
<p>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020.</p>
<p>Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., and Bengio, Y. BabyAI: First steps towards grounded language learning with a human in the loop. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=rJeXCo0cYX.</p>
<p>Dambekodi, S., Frazier, S., Ammanabrolu, P., and Riedl, M. Playing text-based games with common sense. In Proceedings of the NeurIPS Workshop on Wordplay: When Language Meets Games, 2020.</p>
<p>Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., and Batra, D. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.</p>
<p>Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022.</p>
<p>Gordon, D., Kembhavi, A., Rastegari, M., Redmon, J., Fox, D., and Farhadi, A. Iqa: Visual question answering in interactive environments. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4089-4098, 2018.</p>
<p>Guss, W. H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., and Salakhutdinov, R. Minerl: A large-scale dataset of minecraft demonstrations. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI'19, pp. 2442-2448. AAAI Press, 2019. ISBN 9780999241141.</p>
<p>Hafner, D., Pasukonis, J., Ba, J., and Lillicrap, T. Mastering diverse domains through world models, 2023. URL https://arxiv.org/abs/2301.04104.</p>
<p>Hanjie, A. W., Zhong, V. Y., and Narasimhan, K. Grounding language to entities and dynamics for generalization in reinforcement learning. In International Conference on Machine Learning, pp. 4051-4062. PMLR, 2021.</p>
<p>Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., and Gelly, S. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790-2799. PMLR, 2019.</p>
<p>Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022a.</p>
<p>Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. Inner monologue: Embodied reasoning through planning with language models. In 6th Annual Conference on Robot Learning, 2022b.</p>
<p>Ichter, B., Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., Kalashnikov, D., Levine, S., Lu, Y., Parada, C., Rao, K., Sermanet, P., Toshev, A. T., Vanhoucke, V., Xia, F., Xiao, T., Xu, P., Yan, M., Brown, N., Ahn, M., Cortes, O., Sievers, N., Tan, C., Xu, S., Reyes, D., Rettinghouse, J., Quiambao, J., Pastor, P., Luu, L., Lee, K.-H., Kuang, Y., Jesmonth, S., Jeffrey, K., Ruano, R. J., Hsu, J., Gopalakrishnan, K., David, B., Zeng, A., and Fu, C. K. Do as i can, not as i say: Grounding language in robotic affordances. In 6th Annual Conference on Robot Learning, 2022. URL https://openreview.net/ forum?id=bdHkMjBJG_w.</p>
<p>Ku, A., Anderson, P., Patel, R., Ie, E., and Baldridge, J. Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4392-4412, 2020.</p>
<p>Kuo, Y.-L., Katz, B., and Barbu, A. Compositional networks enable systematic generalization for grounded language understanding. In Findings of the Association for Computational Linguistics: EMNLP 2021, pp. 216-226, 2021.</p>
<p>Liang, A., Singh, I., Pertsch, K., and Thomason, J. Transformer adapters for robot learning. In CoRL 2022 Workshop on Pre-training Robot Learning, 2022a. URL https://openreview.net/forum? $\mathrm{id}=\mathrm{H}-\mathrm{wvRYBmF}$.</p>
<p>Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Florence, P., Zeng, A., et al. Code as policies: Language model programs for embodied control. In Workshop on Language and Robotics at CoRL 2022, 2022b.</p>
<p>Liu, R., Wei, J., Gu, S. S., Wu, T.-Y., Vosoughi, S., Cui, C., Zhou, D., and Dai, A. M. Mind's eye: Grounded
language model reasoning through simulation. arXiv preprint arXiv:2210.05359, 2022.</p>
<p>Lynch, C. and Sermanet, P. Language conditioned imitation learning over unstructured data. Robotics: Science and Systems XVII, 2020.</p>
<p>Mu, J., Zhong, V., Raileanu, R., Jiang, M., Goodman, N., Rocktäschel, T., and Grefenstette, E. Improving intrinsic exploration with language abstractions. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K. (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum? id=ALIYCycCsTy.</p>
<p>Nottingham, K., Liang, L., Shin, D., Fowlkes, C. C., Fox, R., and Singh, S. Modular framework for visuomotor language grounding. In Embodied AI Workshop @ CVPR, 2021.</p>
<p>Nottingham, K., Pyla, A., Singh, S., and Fox, R. Learning to query internet text for informing reinforcement learning agents. In Reinforcement Learning and Decision Making Conference, 2022.</p>
<p>OpenAI. Powering next generation applications with openai codex, 2022. URL https://openai.com/blog/ codex-apps/.</p>
<p>Patil, V., Hofmarcher, M., Dinu, M.-C., Dorfer, M., Blies, P. M., Brandstetter, J., Arjona-Medina, J. A., and Hochreiter, S. Align-rudder: Learning from few demonstrations by reward redistribution. In International Conference on Machine Learning, 2020.</p>
<p>Petroni, F., Rocktäschel, T., Riedel, S., Lewis, P., Bakhtin, A., Wu, Y., and Miller, A. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 2463-2473, 2019.</p>
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.</p>
<p>Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., and Fox, D. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.</p>
<p>Simpkins, C. and Isbell, C. Composable modular reinforcement learning. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01): 4975-4982, Jul. 2019. doi: 10.1609/aaai.v33i01.</p>
<ol>
<li>URL https://ojs.aaai.org/index. php/AAAI/article/view/4428.</li>
</ol>
<p>Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A. Progprompt: Generating situated robot task plans using large language models. In Second Workshop on Language and Reinforcement Learning, 2022.</p>
<p>Skrynnik, A., Staroverov, A., Aitygulov, E., Aksenov, K., Davydov, V., and Panov, A. I. Forgetful experience replay in hierarchical reinforcement learning from expert demonstrations. Know.-Based Syst., 218(C), apr 2021. ISSN 0950-7051. doi: 10.1016/j.knosys. 2021.106844. URL https://doi.org/10.1016/ j.knosys.2021.106844.</p>
<p>Song, C. H., Kil, J., Pan, T.-Y., Sadler, B. M., Chao, W.-L., and Su, Y. One step at a time: Long-horizon vision-andlanguage navigation with milestones. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15482-15491, June 2022a.</p>
<p>Song, C. H., Wu, J., Washington, C., Sadler, B. M., Chao, W.-L., and Su, Y. Llm-planner: Few-shot grounded planning for embodied agents with large language models. arXiv preprint arXiv:2212.04088, 2022b.</p>
<p>Suglia, A., Gao, Q., Thomason, J., Thattai, G., and Sukhatme, G. Embodied bert: A transformer model for embodied, language-guided visual task completion. CoRR, abs/2108.04927, 2021. URL https://arxiv. org/abs/2108.04927.</p>
<p>Tam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy, N. A., Chan, S. C., Strouse, D., Wang, J. X., Banino, A., and Hill, F. Semantic exploration from language abstractions and pretrained representations. arXiv preprint arXiv:2204.05080, 2022.</p>
<p>Valmeekam, K., Olmo, A., Sreedharan, S., and Kambhampati, S. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.</p>
<p>Yu, H., Zhang, H., and Xu, W. Interactive grounded language acquisition and generalization in a 2d world. In International Conference on Learning Representations, 2018.</p>
<p>Zellers, R., Holtzman, A., Peters, M. E., Mottaghi, R., Kembhavi, A., Farhadi, A., and Choi, Y. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 2040-2050, 2021.</p>
<p>Zhong, V., Rocktäschel, T., and Grefenstette, E. Rtfm: Generalising to new environment dynamics via reading. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum? id=SJgob6NKvH.</p>
<h1>A. Codex In-Context Learning</h1>
<h2>A.1. Prompting Details</h2>
<p>We use OpenAI's Codex model (code-davinci-002) (OpenAI, 2022) to predict an Abstract World Model (AWM) for DECKARD. We prompt the model with instructions in code comments that instruct the model to generate a python dictionary with information for Minecraft item requirements. We also provide example entries for "diamond pickaxe" and "diamond". We then iterate over all 391 Minecraft items to generate the next entry in the python dictionary. We organize the data in the dictionary entries into the following item attributes:</p>
<ul>
<li>requires_crafting_table: whether an item requires the agent to have a crafting table prior to crafting</li>
<li>requires_furnace: whether the item is smelted with a furnace</li>
<li>required_tool: what tool is required to collect the item from the environment</li>
<li>recipe: list of ingredients and ingredient quantities to craft the item</li>
</ul>
<p>The full prompt we use can be found below:</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># Create a nested python dictionary containing crafting recipes and requirements</span>
<span class="k">for</span><span class="w"> </span><span class="n">minecraft</span><span class="w"> </span><span class="n">items</span><span class="o">.</span>
<span class="c1"># Each crafting item should have a recipe and booleans indicating whether a</span>
<span class="n">furnace</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">crafting</span><span class="w"> </span><span class="n">table</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">required</span><span class="o">.</span>
<span class="c1"># Non craftable blocks should have their recipe set to an empty list and</span>
<span class="n">indicate</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="k">tool</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">required</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">mine</span><span class="o">.</span>
<span class="n">minecraft_info</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="s2">&quot;diamond_pickaxe&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;requires_crafting_table&quot;</span><span class="p">:</span><span class="w"> </span><span class="n">True</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;requires_furnace&quot;</span><span class="p">:</span><span class="w"> </span><span class="n">False</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;required_tool&quot;</span><span class="p">:</span><span class="w"> </span><span class="n">None</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;recipe&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">            </span><span class="p">{</span>
<span class="w">                </span><span class="s2">&quot;item&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;stick&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;quantity&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;2&quot;</span>
<span class="w">            </span><span class="p">},</span>
<span class="w">            </span><span class="p">{</span>
<span class="w">                </span><span class="s2">&quot;item&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;diamond&quot;</span><span class="p">,</span>
<span class="w">                    </span><span class="s2">&quot;quantity&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;3&quot;</span>
<span class="w">            </span><span class="p">}</span>
<span class="w">        </span><span class="p">]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;diamond&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="s2">&quot;requires_crafting_table&quot;</span><span class="p">:</span><span class="w"> </span><span class="n">False</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;requires_furnace&quot;</span><span class="p">:</span><span class="w"> </span><span class="n">False</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;required_tool&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;iron_pickaxe&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="s2">&quot;recipe&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">[]</span>
<span class="w">    </span><span class="p">},</span>
<span class="w">    </span><span class="s2">&quot;[insert item name]&quot;</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
</code></pre></div>

<h2>A.2. Parsing Details</h2>
<p>When parsing output, we consider any item with a recipe of length zero to be a collectable item (it will have no parents in the AWM). In our experiments, Codex generated parsable entries for all but one Minecraft item (brown mushroom block).</p>
<p>In general, Codex predicts the same item identifier that Minedojo (Fan et al., 2022) uses. One major exception is that of planks, a common item essential for many recipes. We parse plank and wood as well as any variant of these two (oak plank) as planks. We also parse cane as reeds. Note that in all these cases the predicted names are also common identifiers for these items in minecraft, but they do not match the Minedojo identifiers.</p>
<p>Finally, we remove circular dependencies from the predicted AWM. First we remove edges from crafting table, furnace, and tool nodes to items that are found in the recipes for those nodes. Then we remove edges both to and from items found in eachother's recipes. There were four cases of circular dependencies in our hypothesized AWM, between planks and crafting table, log and wooden axe, fermented spider eye and spider eye, and purpur block and purpur pillar.</p>
<h1>A.3. Additional Results</h1>
<table>
<thead>
<tr>
<th style="text-align: center;">"Tool Only" Items</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">coal</td>
<td style="text-align: center;">furnace</td>
<td style="text-align: center;">crafting_table</td>
<td style="text-align: center;">log</td>
</tr>
<tr>
<td style="text-align: center;">planks</td>
<td style="text-align: center;">stick</td>
<td style="text-align: center;">cobblestone</td>
<td style="text-align: center;">iron_ore</td>
</tr>
<tr>
<td style="text-align: center;">iron_ingot</td>
<td style="text-align: center;">gold_ore</td>
<td style="text-align: center;">gold_ingot</td>
<td style="text-align: center;">diamond</td>
</tr>
<tr>
<td style="text-align: center;">wooden_hoe</td>
<td style="text-align: center;">wooden_sword</td>
<td style="text-align: center;">wooden_axe</td>
<td style="text-align: center;">wooden_pickaxe</td>
</tr>
<tr>
<td style="text-align: center;">wooden_shovel</td>
<td style="text-align: center;">stone_hoe</td>
<td style="text-align: center;">stone_sword</td>
<td style="text-align: center;">stone_axe</td>
</tr>
<tr>
<td style="text-align: center;">stone_pickaxe</td>
<td style="text-align: center;">stone_shovel</td>
<td style="text-align: center;">iron_hoe</td>
<td style="text-align: center;">iron_sword</td>
</tr>
<tr>
<td style="text-align: center;">iron_axe</td>
<td style="text-align: center;">iron_pickaxe</td>
<td style="text-align: center;">iron_shovel</td>
<td style="text-align: center;">golden_hoe</td>
</tr>
<tr>
<td style="text-align: center;">golden_sword</td>
<td style="text-align: center;">golden_axe</td>
<td style="text-align: center;">golden_pickaxe</td>
<td style="text-align: center;">golden_shovel</td>
</tr>
<tr>
<td style="text-align: center;">diamond_hoe</td>
<td style="text-align: center;">diamond_sword</td>
<td style="text-align: center;">diamond_axe</td>
<td style="text-align: center;">diamond_pickaxe</td>
</tr>
<tr>
<td style="text-align: center;">diamond_shovel</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 3. The 37 Minecraft items from the tool technology tree.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Metric</th>
<th style="text-align: center;">All Items</th>
<th style="text-align: center;">Tools Only</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Accuracy: Collectable vs. Craftable Label</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy: Workbench (Crafting Table/Furnace)</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">96</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy: Recipe Ingredients</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">81</td>
</tr>
<tr>
<td style="text-align: left;">Accuracy: Recipe Ingredients \&amp; Quantities</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">69</td>
</tr>
<tr>
<td style="text-align: left;">\% Items w/ Incorrectly Inserted Dependencies</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">\% Items w/ Missing Dependencies</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">26</td>
</tr>
<tr>
<td style="text-align: left;">Standard Deviation In Predicted Ingredient Quantity</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.34</td>
</tr>
<tr>
<td style="text-align: left;">Absolute Error In Predicted Ingredient Quantity</td>
<td style="text-align: center;">2.77</td>
<td style="text-align: center;">1.50</td>
</tr>
<tr>
<td style="text-align: left;">Average Error In Predicted Ingredient Quantity</td>
<td style="text-align: center;">-1.07</td>
<td style="text-align: center;">0.50</td>
</tr>
</tbody>
</table>
<p>Table 4. Additional Codex metrics for predicting the Minecraft AWM.
Our experiments with few-shot prompting Codex to generate the AWM for Minecraft show that LLMs can generate structured knowledge for decision making. However, predictions are not perfect, so we treat them as hypotheses that are verified by environment interactions. Codex does perform better on the tool technology tree, items that are both more common and more relevant for crafting agents. A large percentage of errors also appears to be from incorrectly predicted ingredient quantities.</p>
<h1>B. Subgoal Finetuning</h1>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Results for finetuning subgoal policies. DECKARD's VPT-based subgoal policies are trained on seeds where the target item is nearby and evaluated on random world seeds. Of these results, cobblestone is the most ubiquitous and sand the least, as indicated by how well the policy generalizes to random Minecraft world seeds.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: right;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">VPT Checkpoint</td>
<td style="text-align: right;">bc-house-3x</td>
</tr>
<tr>
<td style="text-align: left;">Environment Steps per Actor per Iteration</td>
<td style="text-align: right;">500</td>
</tr>
<tr>
<td style="text-align: left;">Number of Actors</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">Batch Size</td>
<td style="text-align: right;">40</td>
</tr>
<tr>
<td style="text-align: left;">Iteration Epochs</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;">Learning Rate</td>
<td style="text-align: right;">0.0001</td>
</tr>
<tr>
<td style="text-align: left;">$\gamma$</td>
<td style="text-align: right;">0.999</td>
</tr>
<tr>
<td style="text-align: left;">Value Loss Coefficient</td>
<td style="text-align: right;">1</td>
</tr>
<tr>
<td style="text-align: left;">Initial KL Loss Coefficient</td>
<td style="text-align: right;">.1</td>
</tr>
<tr>
<td style="text-align: left;">KL Coefficient Decay per Iteration</td>
<td style="text-align: right;">.999</td>
</tr>
<tr>
<td style="text-align: left;">Adapter Downsize Factor</td>
<td style="text-align: right;">16</td>
</tr>
</tbody>
</table>
<p>Table 5. DECKARD subgoal finetuning hyperparameters.</p>
<h2>B.1. VPT Finetuning</h2>
<p>We finetune VPT (3x w/ behavior cloning on house contractor data) (Baker et al., 2022) with reinforcement learning (RL) using transformer adapters as described by Houlsby et al. (2019). That is, we insert two adapter modules with residual connections in each transformer layer, with a 16x reduction in hidden state size. We updated the adapters and agent value head using proximal policy optimization (PPO) (Schulman et al., 2017), but we leave the rest of the agent unchanged (including the policy head).</p>
<p>Following Baker et al. (2022), we replace the traditional entropy loss in the PPO algorithm with a KL loss between the current policy and the non-finetuned VPT policy. The purpose of this loss is to prevent catastrophic forgetting early in training. Our experiments reaffirmed the importance of this term, even when leaving the majority of the VPT weights unchanged. The KL loss coefficient decays throughout training to allow the agent to reach an optimal policy.</p>
<h2>B.2. MineClip Reward</h2>
<p>Along with their Minedojo Minecraft implementation, Fan et al. (2022) introduced a text and video alignment model for Minecraft called MineClip and showed how the model could be used for automatic reward shaping given a text goal. We use MineClip to provide reward shaping for finetuning DECKARD subgoal and VPT-a policies. Unlike Fan et al. (2022), we implement MineClip reward shaping by subtracting clip $l_{l o w}=21$ from the MineClip alignment score and scaling by clip $_{l o}=0.005$, smoothed over smooth $=50$ steps:</p>
<p>$$
\text { reward }<em _alpha="\alpha">{\text {clip }}=\text { clip }</em>} \times \max \left(0, \text { mean }\left(\text { score_buffer <em _low="{low" _text="\text">{-s m o o t h:}\right)-\text { clip }</em>\right)
$$}</p>
<p>Additionally, we only provide the agent with non-zero reward when the MineClip alignment score reaches a new maximum for the episode. Finally we provide a reward of +1 when the agent successfully adds the target item to its inventory.</p>
<h1>B.3. Minecraft Settings</h1>
<p>Use use the Minedojo simulator (Fan et al., 2022) with the "creative" metatask for our experiments. We found Minedojo preferable to MineRL (Guss et al., 2019), due to a reduced tendency to crash when running many parallel environment instances. We followed the VPT (Baker et al., 2022) observation and action spaces-128x128x3 pixel observation space and 121x8461 multi-discrete action space-with the modification of replacing the "open inventory" action with 254 discrete crafting actions.</p>
<p>When training subgoal policies, we initialize the agent with items from the current node's parents. For example, when training the collect cobblestone subgoal, we initialize the agent with a wooden pickaxe, the required tool for cobblestone in the AWM. We terminate each episode after 1,000 environment steps, generating a new world.</p>
<p>We also found that finetuning was sensitive to world seed when training. For example, many world seeds spawned the agent far from target items, stranded on islands, or underwater. To mitigate the effect of poor world initialization on training, we use a single world seed for training each subgoal policy and then evaluate on random world seeds. We find that VPT is able to generalize to random seeds after training on a training seed.</p>
<h2>C. Abstract World Model</h2>
<h2>C.1. Disambiguating the World Model</h2>
<p>In many environments, multiple possible transitions between subgoals may exist. For example, in Minecraft, an agent can obtain coal through mining or by burning wood in a furnace. Ideally, edges of the AWM would provide paths with high success rate to each node. In our implementation we keep the first experienced edge between nodes, assuming it to be the simplest path.</p>
<h2>C.2. Additional Results</h2>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. AWM growth over the course of open-ended exploration. The first three quadrants are identical to Figure 2. The last quadrant adds results for a ground truth AWM. The agent learns to craft glass much sooner and also learns to craft glass bottles, and item none of the other methods reached.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Department of Computer Science, University of California Irvine, Irvine, CA, United States ${ }^{2}$ Allen Institute for Artificial Intelligence, Seattle, WA, United States ${ }^{3}$ Paul G. Allen School of Computer Science, Seattle, WA, United States. Correspondence to: Kolby Nottingham $&lt;$ knotting@uci.edu $&gt;$.</p>
<p>Proceedings of the $39^{\text {th }}$ International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>*https://deckardagent.github.io/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7408 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7408</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7408</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-260680723</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.02618v1.pdf" target="_blank">ChatGPT for GTFS: From Words to Information</a></p>
                <p><strong>Paper Abstract:</strong> The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zero-shot and program synthesis. Program synthesis works better, achieving ~90% accuracy on simple questions and ~40% accuracy on complex questions.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7408.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7408.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot MCQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot multiple-choice prompting for GTFS understanding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Direct zero-shot prompting of GPT-3.5 on a battery of GTFS multiple-choice questions (no examples); used to evaluate the model's raw knowledge of the GTFS specification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GTFS Understanding (MCQ)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer multiple-choice questions about the GTFS specification and semantics (categories: term definitions, file structure, attribute mapping, data structure, categorical mapping, common reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice questions (MCQ) delivered via chat messages; model instructed to output only answer option(s) (a/b/c/d).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>question type</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot (no in-context examples); prompts split into System + User messages; hyperparameters: temperature=0, top_p=1, max_tokens=2 for MCQs; some prompts instruct 'no explanation' and output-only answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>76.5% accuracy (overall on 183 GTFS MCQs)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>gpt-3.5-turbo via OpenAI chat completions; temperature fixed at 0 for determinism; MCQs limited to short token outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7408.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7408.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought MCQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting for MCQs (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Augmenting MCQ prompts with an explicit request for step-by-step reasoning (CoT) to elicit multi-step internal reasoning from GPT-3.5 and observe effect on MCQ answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GTFS Understanding (MCQ) with CoT</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same MCQ set as zero-shot, but prompts include 'Provide a step-by-step explanation' (CoT) and then the model outputs reasoning and answer.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice question with chain-of-thought instruction (natural-language CoT appended to prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot with explicit CoT instruction; temperature=0; same System+User message split; prompts request both explanation and final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (qualitative comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>No consistent numeric improvement reported; example-level outputs changed but often remained incorrect (paper reports CoT sometimes changes answer but did not fix categorical-mapping failures).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>76.5% accuracy (zero-shot MCQ)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No reliable improvement reported (qualitative differences only)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>CoT instantiated by asking for step-by-step explanation; temperature=0; results inspected qualitatively and via examples.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7408.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7408.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot natural-language retrieval from GTFS</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Pose GTFS information-retrieval questions to GPT-3.5 without example code: the model is given GTFS content (trimmed) and natural-language questions and must respond with answers directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GTFS Information Retrieval (simple + complex)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer factual questions requiring lookup and joins across GTFS files (47 simple lookups, 41 complex multi-file questions) using only natural-language prompts and provided feed snippets.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot natural-language prompts with GTFS data input streamed into the chat (no example demonstrations).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot; System + User messages; GTFS files fed into chat via sequential prompts; hyperparameters temperature=0, top_p=1; max_tokens large when feeding GTFS content, but model must output answer in natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>40% accuracy (overall on 88 GTFS retrieval questions)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>gpt-3.5-turbo, temperature=0; limited GTFS feed to fit context window (filtered to 4 routes, 9 trips); zero-shot prompting for retrieval tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7408.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7408.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Program synthesis (one-shot/pandas)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-shot program synthesis via pandas code generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One-shot demonstration to elicit Python/pandas code from GPT-3.5 that performs the required GTFS data extraction; generated code is executed to obtain answers, testing the model's ability to produce correct, runnable programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API, used here for code generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GTFS Information Retrieval via Program Synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python/pandas code that, when run against the GTFS CSV files, computes answers to retrieval questions (simple and complex).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot code-generation prompt (pandas) — task demonstration + instruction to output code that sets result in variable 'answer'.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / problem format</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>One-shot (single demonstration) program-synthesis prompt; instruct model to produce only pandas code; temperature=0 for determinism; max_tokens set to large context for code; generated code executed externally to obtain ground-truth answers; used to elicit stepwise reasoning via code.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>67% accuracy overall; reported breakdown: 90% accuracy on simple questions and 40% accuracy on complex questions (both figures reported in paper; 67% is aggregate).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>40% accuracy (zero-shot retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>+27.0% absolute overall compared to zero-shot retrieval (paper reports program synthesis substantially outperforms zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>gpt-3.5-turbo, one-shot demonstration included; generated pandas code was executed to derive answers; failure analysis: of 29 program-synthesis failures, 18 due to attribute-mapping errors, 8 due to logical errors, remainder due to data-structure misunderstandings.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7408.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7408.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Format consistency (agreement)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agreement between zero-shot and program-synthesis outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Quantified the consistency/agreement between answers produced by zero-shot natural-language prompting and by program-synthesis prompts as a measure of how prompt format changes outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>GTFS Retrieval (consistency analysis)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure percentage of questions where zero-shot answer matches program-synthesis answer to assess sensitivity to prompt/presentation format.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Comparative analysis between two prompt formats (zero-shot natural-language vs one-shot program-synthesis code generation).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / comparison</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Agreement measured across same retrieval question set; average consistency reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>consistency (agreement rate)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Approximately 30% average consistency between zero-shot and program-synthesis outputs (i.e., ~30% of answers agreed).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Agreement computed across 88 retrieval questions (47 simple, 41 complex); shows large differences in outputs induced purely by prompt/presentation style.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7408.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7408.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot / One-shot prompting (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot (one-shot) prompting for demonstration and better output formatting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper notes few-shot (one-shot) prompting can help the model better generalize output format — specifically used one-shot for program synthesis because it produced better code generalization; no comprehensive numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (gpt-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Prompt engineering (few-shot demonstration)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide a single example demonstration in prompt to influence the style and correctness of subsequent outputs (used with code generation).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>One-shot demonstration present vs zero-shot absent.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>One-shot demonstration appended to prompt for program synthesis; the paper states one-shot led to better generalization of code output but does not provide a numeric ablation versus strictly zero-shot program-synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>qualitative/code generalization (no numeric metric)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitatively reported as 'better generalization' for program synthesis when one-shot demonstration was used; no exact accuracy delta reported solely for few-shot vs zero-shot within program synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>One-shot demonstrations used for program synthesis prompts; temperature=0; the paper uses one-shot by default for code generation because it produced clearer runnable code.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>From Words to Code: Harnessing Data for Program Synthesis from Natural Language <em>(Rating: 2)</em></li>
                <li>Large Language Models are Zero-Shot Reasoners <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models <em>(Rating: 2)</em></li>
                <li>Evaluating Large Language Models Trained on Code <em>(Rating: 2)</em></li>
                <li>Jigsaw: large language models meet program synthesis <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7408",
    "paper_id": "paper-260680723",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Zero-shot MCQ",
            "name_full": "Zero-shot multiple-choice prompting for GTFS understanding",
            "brief_description": "Direct zero-shot prompting of GPT-3.5 on a battery of GTFS multiple-choice questions (no examples); used to evaluate the model's raw knowledge of the GTFS specification.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.",
            "model_size": null,
            "task_name": "GTFS Understanding (MCQ)",
            "task_description": "Answer multiple-choice questions about the GTFS specification and semantics (categories: term definitions, file structure, attribute mapping, data structure, categorical mapping, common reasoning).",
            "problem_format": "Multiple-choice questions (MCQ) delivered via chat messages; model instructed to output only answer option(s) (a/b/c/d).",
            "format_category": "question type",
            "format_details": "Zero-shot (no in-context examples); prompts split into System + User messages; hyperparameters: temperature=0, top_p=1, max_tokens=2 for MCQs; some prompts instruct 'no explanation' and output-only answers.",
            "performance_metric": "accuracy",
            "performance_value": "76.5% accuracy (overall on 183 GTFS MCQs)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "gpt-3.5-turbo via OpenAI chat completions; temperature fixed at 0 for determinism; MCQs limited to short token outputs.",
            "statistical_significance": null,
            "uuid": "e7408.0"
        },
        {
            "name_short": "Chain-of-Thought MCQ",
            "name_full": "Chain-of-thought prompting for MCQs (CoT)",
            "brief_description": "Augmenting MCQ prompts with an explicit request for step-by-step reasoning (CoT) to elicit multi-step internal reasoning from GPT-3.5 and observe effect on MCQ answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.",
            "model_size": null,
            "task_name": "GTFS Understanding (MCQ) with CoT",
            "task_description": "Same MCQ set as zero-shot, but prompts include 'Provide a step-by-step explanation' (CoT) and then the model outputs reasoning and answer.",
            "problem_format": "Multiple-choice question with chain-of-thought instruction (natural-language CoT appended to prompt).",
            "format_category": "prompt style",
            "format_details": "Zero-shot with explicit CoT instruction; temperature=0; same System+User message split; prompts request both explanation and final answer.",
            "performance_metric": "accuracy (qualitative comparison)",
            "performance_value": "No consistent numeric improvement reported; example-level outputs changed but often remained incorrect (paper reports CoT sometimes changes answer but did not fix categorical-mapping failures).",
            "baseline_performance": "76.5% accuracy (zero-shot MCQ)",
            "performance_change": "No reliable improvement reported (qualitative differences only)",
            "experimental_setting": "CoT instantiated by asking for step-by-step explanation; temperature=0; results inspected qualitatively and via examples.",
            "statistical_significance": null,
            "uuid": "e7408.1"
        },
        {
            "name_short": "Zero-shot retrieval",
            "name_full": "Zero-shot natural-language retrieval from GTFS",
            "brief_description": "Pose GTFS information-retrieval questions to GPT-3.5 without example code: the model is given GTFS content (trimmed) and natural-language questions and must respond with answers directly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.",
            "model_size": null,
            "task_name": "GTFS Information Retrieval (simple + complex)",
            "task_description": "Answer factual questions requiring lookup and joins across GTFS files (47 simple lookups, 41 complex multi-file questions) using only natural-language prompts and provided feed snippets.",
            "problem_format": "Zero-shot natural-language prompts with GTFS data input streamed into the chat (no example demonstrations).",
            "format_category": "input modality / prompt style",
            "format_details": "Zero-shot; System + User messages; GTFS files fed into chat via sequential prompts; hyperparameters temperature=0, top_p=1; max_tokens large when feeding GTFS content, but model must output answer in natural language.",
            "performance_metric": "accuracy",
            "performance_value": "40% accuracy (overall on 88 GTFS retrieval questions)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "gpt-3.5-turbo, temperature=0; limited GTFS feed to fit context window (filtered to 4 routes, 9 trips); zero-shot prompting for retrieval tasks.",
            "statistical_significance": null,
            "uuid": "e7408.2"
        },
        {
            "name_short": "Program synthesis (one-shot/pandas)",
            "name_full": "One-shot program synthesis via pandas code generation",
            "brief_description": "One-shot demonstration to elicit Python/pandas code from GPT-3.5 that performs the required GTFS data extraction; generated code is executed to obtain answers, testing the model's ability to produce correct, runnable programs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API, used here for code generation.",
            "model_size": null,
            "task_name": "GTFS Information Retrieval via Program Synthesis",
            "task_description": "Generate Python/pandas code that, when run against the GTFS CSV files, computes answers to retrieval questions (simple and complex).",
            "problem_format": "One-shot code-generation prompt (pandas) — task demonstration + instruction to output code that sets result in variable 'answer'.",
            "format_category": "prompt style / problem format",
            "format_details": "One-shot (single demonstration) program-synthesis prompt; instruct model to produce only pandas code; temperature=0 for determinism; max_tokens set to large context for code; generated code executed externally to obtain ground-truth answers; used to elicit stepwise reasoning via code.",
            "performance_metric": "accuracy",
            "performance_value": "67% accuracy overall; reported breakdown: 90% accuracy on simple questions and 40% accuracy on complex questions (both figures reported in paper; 67% is aggregate).",
            "baseline_performance": "40% accuracy (zero-shot retrieval)",
            "performance_change": "+27.0% absolute overall compared to zero-shot retrieval (paper reports program synthesis substantially outperforms zero-shot).",
            "experimental_setting": "gpt-3.5-turbo, one-shot demonstration included; generated pandas code was executed to derive answers; failure analysis: of 29 program-synthesis failures, 18 due to attribute-mapping errors, 8 due to logical errors, remainder due to data-structure misunderstandings.",
            "statistical_significance": null,
            "uuid": "e7408.3"
        },
        {
            "name_short": "Format consistency (agreement)",
            "name_full": "Agreement between zero-shot and program-synthesis outputs",
            "brief_description": "Quantified the consistency/agreement between answers produced by zero-shot natural-language prompting and by program-synthesis prompts as a measure of how prompt format changes outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.",
            "model_size": null,
            "task_name": "GTFS Retrieval (consistency analysis)",
            "task_description": "Measure percentage of questions where zero-shot answer matches program-synthesis answer to assess sensitivity to prompt/presentation format.",
            "problem_format": "Comparative analysis between two prompt formats (zero-shot natural-language vs one-shot program-synthesis code generation).",
            "format_category": "prompt style / comparison",
            "format_details": "Agreement measured across same retrieval question set; average consistency reported.",
            "performance_metric": "consistency (agreement rate)",
            "performance_value": "Approximately 30% average consistency between zero-shot and program-synthesis outputs (i.e., ~30% of answers agreed).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Agreement computed across 88 retrieval questions (47 simple, 41 complex); shows large differences in outputs induced purely by prompt/presentation style.",
            "statistical_significance": null,
            "uuid": "e7408.4"
        },
        {
            "name_short": "Few-shot / One-shot prompting (general)",
            "name_full": "Few-shot (one-shot) prompting for demonstration and better output formatting",
            "brief_description": "Paper notes few-shot (one-shot) prompting can help the model better generalize output format — specifically used one-shot for program synthesis because it produced better code generalization; no comprehensive numeric comparison provided.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (gpt-3.5-turbo)",
            "model_description": "Closed-source conversational transformer (ChatGPT family) accessed via OpenAI chat-completions API.",
            "model_size": null,
            "task_name": "Prompt engineering (few-shot demonstration)",
            "task_description": "Provide a single example demonstration in prompt to influence the style and correctness of subsequent outputs (used with code generation).",
            "problem_format": "One-shot demonstration present vs zero-shot absent.",
            "format_category": "prompt style",
            "format_details": "One-shot demonstration appended to prompt for program synthesis; the paper states one-shot led to better generalization of code output but does not provide a numeric ablation versus strictly zero-shot program-synthesis.",
            "performance_metric": "qualitative/code generalization (no numeric metric)",
            "performance_value": "Qualitatively reported as 'better generalization' for program synthesis when one-shot demonstration was used; no exact accuracy delta reported solely for few-shot vs zero-shot within program synthesis.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "One-shot demonstrations used for program synthesis prompts; temperature=0; the paper uses one-shot by default for code generation because it produced clearer runnable code.",
            "statistical_significance": null,
            "uuid": "e7408.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "From Words to Code: Harnessing Data for Program Synthesis from Natural Language",
            "rating": 2,
            "sanitized_title": "from_words_to_code_harnessing_data_for_program_synthesis_from_natural_language"
        },
        {
            "paper_title": "Large Language Models are Zero-Shot Reasoners",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Evaluating Large Language Models Trained on Code",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Jigsaw: large language models meet program synthesis",
            "rating": 1,
            "sanitized_title": "jigsaw_large_language_models_meet_program_synthesis"
        }
    ],
    "cost": 0.0136245,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CHATGPT FOR GTFS: FROM WORDS TO INFORMATION
Submission Date: August 8, 2023 4 Aug 2023</p>
<p>Saipraneeth Devunuri 
Shirin Qiam sqiam2@illinois.edu 
Lewis Lehe lehe@illinois.edu 
QiamLehe Devunuri </p>
<p>University of Illinois at Urbana-Champaign Champaign
ILUnited States</p>
<p>University of Illinois at Urbana-Champaign Champaign
ILUnited States</p>
<p>University of Illinois at Urbana-Champaign Champaign
ILUnited States</p>
<p>CHATGPT FOR GTFS: FROM WORDS TO INFORMATION
Submission Date: August 8, 2023 4 Aug 2023GTFSChatGPTLarge Language ModelsGenerative AIGPT-35
The General Transit Feed Specification (GTFS) standard for publishing transit data is ubiquitous. GTFS being tabular data, with information spread across different files, necessitates specialized tools or packages to retrieve information. Concurrently, the use of Large Language Models for text and information retrieval is growing. The idea of this research is to see if the current widely adopted LLMs (ChatGPT) are able to retrieve information from GTFS using natural language instructions. We first test whether ChatGPT (GPT-3.5) understands the GTFS specification. GPT-3.5 answers 77% of our multiple-choice questions (MCQ) correctly. Next, we task the LLM with information extractions from a filtered GTFS feed with 4 routes. For information retrieval, we compare zeroshot and program synthesis. Program synthesis works better, achieving 90% accuracy on simple questions and 40% accuracy on complex questions.</p>
<p>INTRODUCTION</p>
<p>The General Transit Feed Specification (GTFS) is an Open Standard design for publishing transit data and serves as a valuable resource for scientific analysis. GTFS facilitates public access to transit agency data, enabling transparency and fostering collaboration (1). Since its inception with the Bay Area Rapid Transit Agency (BART) in 2006, GTFS has gained widespread adoption, with over 75% of transit agencies providing scheduled transit services embracing this standard (2).</p>
<p>Prominent trip planning applications, including Google Maps, Apple Maps, and Open Trip Planner, rely on GTFS feeds to deliver accurate and reliable information to their users. However, while GTFS simplifies the process of publishing transit data in a standardized format, effectively analyzing GTFS data necessitates a learning curve. Utilizing the feed mandates a comprehensive understanding of established conventions (e.g., "Required," "May," "Must," etc.) and terminological definitions (e.g., "Leg," "Journey," "Field," etc.). The feed specification comprises six obligatory files, along with several optional files, each containing one or more primary keys utilized for data integration.</p>
<p>Furthermore, there are nuances within the GTFS specification that demand careful attention to detail. For instance, the calendar_dates.txt file can be utilized either independently to publish services on all days or in conjunction with the calendar.txt file to specify exceptions to the regular service schedule. Moreover, there exist certain disparities between the references defined by the standard and the recommended best practices. For instance, while the best practices suggest including the agency_id field in the agency.txt file even when the feed contains only one agency, the standard mandates the inclusion of agency_id only when multiple transit agencies are present. These intricacies highlight the importance of meticulous adherence to the GTFS specifications and understanding the recommended practices to ensure accurate data interpretation and utilization.</p>
<p>For this reason, many researchers and practitioners of GTFS use tools or packages that validate and analyze GTFS. For example, the gtfs-segments (3,4) package specializes in organizing GTFS data in the form of segments and calculating the stop spacings. The R5 Routing engine (5) and the R5R package (6) can be used for routing analysis, travel time estimations, and accessibility analysis. While most tools have basic manipulation operations such as extraction, formatting, and joins; one package may not have tools for all purposes. Besides, the packages must keep updating their code based on the changes in the GTFS reference, best practices, and the dependencies they rely on.</p>
<p>Recently, Large Language Models (LLMs) have showcased their potential in performing information retrieval and document question-answering tasks, demonstrating an ability to grasp domain-specific semantics. In a notable study, Bommarito II and Katz (7) documented that GPT-3.5 successfully passed the multi-state (MBE) multiple-choice component of the Bar exam, which entails answering intricate legal questions, showcasing the LLM's proficiency in reasoning. Subsequently, Katz et al. (8) reported that GPT-4 surpassed both GPT-3.5 and human performance by passing the bar exam, further attesting to the advancements in LLM capabilities. Furthermore, the medical domain has also benefited from LLMs (9, 10). Gilson et al. (11) demonstrated how GPT showcased logical reasoning in answering medical queries.</p>
<p>Besides document question-answering, LLMs have had huge adoption in assisting with writing code (12,13) to program synthesis (14)(15)(16). Program synthesis is a process where a computer program is automatically generated to solve a given problem. For example, Khatry et al. (17) demonstrated that LLMs could generate code for data manipulation tasks from natural language prompts.</p>
<p>With the wide range of use cases, LLMs have seen enormous growth in users from academia and otherwise. Generative Pre-trained Transformers (GPT) with models like GPT-3 (18) and  along with products built on top of them, such as ChatGPT (GPT-3.5) have received huge attention from media and scholars alike. While the architectures of these models remain closedsource, other organizations have released open-source LLMs such as BLOOM (20), LLaMA (21), and Alpaca (22). Despite the growth, applicability, and usage of LLMs in transportation (23) and other fields, the use of LLMs to understand and retrieve information from GTFS data has not been explored.</p>
<p>GTFS is essentially tabular data associated with additional documents that specify the data format and semantics. Therefore, if LLMs can understand the semantics of the data, they should be able to retrieve information from natural language prompts. To the best of our knowledge, ours is the first study examining the understanding of LLMs on the GTFS file format and feeds. Specifically, this research tries to answer the following research questions:</p>
<ol>
<li>Does generative AI like ChatGPT have an understanding of GTFS? 2. Can LLMs be used to retrieve information from GTFS files? 3. Can ChatGPT generate code that can aid information extraction? For this research, we curated over 180 multiple-choice questions from GTFS documentation and tested the LLM understanding in six different categories. Using filtered GTFS data from Chicago Transit Authority (CTA), we created an additional 88 questions to test the LLM capabilities in information retrieval. While testing the LLM, we employ different types of prompts or prompting techniques to extract answers and also understand the reasoning. Specifically, we employ zero-shot, few-shot, chain-of-thought (CoT), and program synthesis techniques to design the prompts. (These terms are defined in the Methodology section below.)</li>
</ol>
<p>The paper is structured as follows: We will begin by describing the GTFS data used for the analysis, along with an overview of the questionnaire in the next section. Following this, we describe the OpenAI API we used and the different prompts employed in the methodology sections. Next, we present the results on the understanding of GTFS using zero-shot and CoT approaches. Following this, we present the results of using zero-shot and one-shot program synthesis for extracting information from GTFS. Finally, we will conclude the paper and discuss potential future work.</p>
<p>DATA</p>
<p>We perform our analysis using two data sources. The first is a publicly available GTFS dataset. The second is the questionnaire consisting of over 250 questions. Both the filtered GTFS data and questionnaire along with responses are made available to the public 1 .</p>
<p>GTFS Data</p>
<p>We use real-world data from the Chicago Transit Authority to test the performance of GPT-3.5 on GTFS. The feed was published on May 3, 2023, and is available at https://transitfeeds.com/ p/chicago-transit-authority/165/20230503. The feed consists of 133 routes with 125 bus and 8 metro routes. Figure 1a shows the whole map of the transit network generated using gtfssegments(3) python library.</p>
<p>Large Language Models are infamous for their limited context length. The context length   Figure 1b shows the map of the filtered dataset. We limit the analysis to four bus routes ('49B', '120', '192', and 'X9') and nine trips on these routes. After filtering, we end up with 63 unique stops. Also, we limit the files to only 'required' and 'conditionally required'. This excludes some other useful optional files like 'shapes.txt' which has a relatively larger size in comparison to other files.</p>
<p>Questionnaire</p>
<p>There are two sets of questionnaires, each intended for a different purpose. Table 1 summarizes the different sets and the categories within each set.</p>
<p>GTFS Understanding</p>
<p>The first set of questions is helpful to understand the extent of LLM's understanding of GTFS and identify shortcomings. These questions are not particular to any GTFS feed and are solely based on GTFS documentation. The questionnaire is generated with https://gtfs.org/schedule/ reference as the reference which is the official source for documentation. The categories of the questionnaire include: • Term Definitions : GTFS uses terms like "Required", "May", "Service Day", etc. that are document specific. These are important to understand the file structures and attribute definitions. Example: What is a record in GTFS? a) A basic data structure representing a service day b) A collection of field values describing a single entity c) A set of files defining transit information d) A unique identifier for a transit agency • Common Reasoning : These questions test the basic knowledge of GTFS such as the abbreviation, the usage, and the purpose of different files present in GTFS. Example: What is the purpose of the "agency.txt" file in GTFS? a) It provides information about individual transit stops. b) It contains details about the routes and their associated stops. c) It specifies the frequency of trips. d) It provides information about the transit agency or operator. • File Structure : The GTFS specification has 8 files that are either required or conditionally required and several other files that are optional. These questions test if the GPT engine is able to find the corresponding file based on the general context. Example: How is frequency-based service represented in GTFS? a) Through the "frequency.txt" file. b) By using the "calendar.txt" file. c) By specifying trip times in the "stop-times.txt" file. d) Frequency-based service is not supported in GTFS. • Attribute Mapping : GTFS has several attributes spread across different files. The questions test if the GPT understands which attributes are present in which files. Example: Which file contains the route-type attribute in GTFS? a) stops.txt b) routes.txt c) trips.txt d) shapes.txt • Data Structure: GTFS standard uses several data types that are unique to the specification. For example, the color code for all routes should be a Hex code. These questions test if the attribute data types are correctly identified. Example: What data type is used for the date attribute in GTFS? a) Date (YYYYMMDD) b) Enum c) Text representation of date (e.g., "September 13, 2018") d) Integer (e.g., 20180913) • Categorical Mapping : In order to concisely represent data in attributes, the GTFS feed uses categorical variables to represent data. For example, the 'route_type' '3' corresponds to a 'bus' and a 'location_type' '0' corresponds to a 'stop'. Example: What does "route-type" 6 indicate in the "routes.txt" file? a) Tram, Streetcar, Light rail b) Subway,</p>
<p>Metro c) Aerial lift, suspended cable car d) Funicular</p>
<p>GTFS Retrieval</p>
<p>The second set of questions tests the information retrieval capabilities of LLMs. We generate this subset of questions and corresponding answers using the filtered dataset from CTA. The questions are divided into two categories:</p>
<p>• Simple: These questions are based on simple lookups within the same file or two different files (using relational keys) within GTFS. Example: What is the 'stop_sequence' for stop_id '5024'? • Complex: These questions need multiple files to extract information, require a deeper understanding, and could be open-ended. Example: What is the trip_id of the trip with the longest duration for the route with route_id 192?</p>
<p>METHODOLOGY</p>
<p>The primary objective of this methodology is to identify the limitations in ChatGPT's comprehension of GTFS data. To achieve this, we aim to identify the category where the LLM performs poorly. Denote the set of questions as Q and the set of potential prompts as P. The model's output, which consists of the answers, is represented as A = f (P, Q). Here f denotes the LLM which provides a functional mapping between natural language prompts and responses. To evaluate the performance, we have a ground truth set of correct answers denoted as S, and a grading function, denoted as g, to assess the accuracy of the model's responses. The grading function scores the responses based on the ground truth S. The scores in each category of questions give insight into the understanding of GTFS and the lack thereof. The secondary objective is to extract information from GTFS. The goal here is to find the most effective prompt, denoted as P * , that maximizes the grading function g for the model's output compared to the ground truth:
P * = argmax P g( f (P, Q), S)
In essence, we aim to find the prompt configuration that yields the best alignment between the model's answers and the ground truth, thereby improving the overall understanding of GTFS data by ChatGPT. Specifically, we try zero-shot, one-shot, chain of thought, and program synthesis methods by changing the prompts and comparing the results</p>
<p>OpenAI API and Hyperparameters</p>
<p>OpenAI provides API endpoints to interact with their GPT-3.5 and other LLMs, besides the chatbot version -ChatGPT. While using the OpenAI chat completions API 2 endpoints, the prompt is not a single string but a list of messages with certain roles and purposes. We use the openai python library 3 that provides wrappers around the chat completions and other API endpoints. The whole prompt is split into two: "System" and "User". The "System" part ensures that all prompts have a similar context of the question to answer and consists of instructions given to the GPT. The "User" part of the prompt has the actual question that is different from the rest. The prompts used in this paper are inspired by examples provided on OpenAI 4 website.</p>
<p>For this study, we stick to the 'gpt-3.5-turbo' model, which is susceptible to changes in hyperparameters. Besides the prompts, the hyperparameters are the only available features helpful for tuning the GPT-3.5 response. For this study, the following are the main hyperparameters available in the chat completions API:</p>
<p>• temperature: The temperature parameter (range [0,2]) controls the level of randomness in the sampling. A value of zero produces deterministic output, while higher values introduce more randomness. To ensure the reproducibility of results, we set the temperature to zero • top_p: This is an alternative to the temperature parameter that uses nucleus sampling probability. The documentation recommends tuning either temperature or top_p. Therefore, we use the default value of '1' • max_tokens: This parameter limits the maximum number of tokens in the output. For multiple-choice type questions, we set max_tokens to '2'. For other purposes, we set the token limit to the context length of the 'gpt-3.5-turbo' model at 16,000 tokens. Besides these, there are other hyperparameters such as 'best_of', 'presence_penalty', and 'frequency_penalty', which are all set to their default values.</p>
<p>Zero-Shot and Few-Shot Learning</p>
<p>Zero-shot prompting is a technique that allows Large Language Models (LLMs) to retrieve information from data without specific training or examples on the domain. The model is only given the instructions in natural language. Kojima et al. (24) showed that LLMs are zero-shot reasoners and can solve a variety of reasoning problems without fine-tuning or sample engineering.</p>
<p>Since GTFS is a standardized format for publishing transit schedules, the advantage of zero-shot prompting in GTFS information retrieval lies in its flexibility and adaptability. It allows LLMs to handle a wide range of queries without the need for explicit domain-specific training. This makes it easier to integrate GTFS information retrieval capabilities into various applications, as LLMs can leverage their pre-trained knowledge and generalize to different transit systems and datasets. Figure 2 shows an example zero-shot prompt and response.</p>
<p>Few-shot prompting is similar to zero-shot prompting but provides few demonstrations to the model that helps the model respond in a fashion more tuned to user preference. Brown et al. (18) demonstrated that few-shot prompting results in better accuracy than zero-shot by using incontext information. In this paper, we stick to a one-shot setting where we provide a task and a single demonstration of how to solve the task. Figure 3 shows an example of few-shot questioning.</p>
<p>It is important to note that zero-shot or few-shot prompting has its limitations. Since the LLM has not received direct training on GTFS data, its performance may not be as accurate or precise as models specifically trained on GTFS datasets. The level of understanding and retrieval accuracy may vary depending on the complexity of the queries and the available contextual information.</p>
<p>Chain of Thought (CoT)</p>
<p>Chain of Thought (CoT) prompting is a method aimed at eliciting multi-step reasoning from Language Models (LLMs). By guiding the model through a coherent series of steps, CoT encourages it   (25) demonstrated that CoT substantially enhances the reasoning capabilities of LLMs. One of the key benefits of CoT is its ability to provide insights into the thought process employed by the model when answering questions, regardless of whether the LLM's response is accurate or not. This makes CoT particularly valuable for comprehending the reasoning behind LLM outputs, leading to improved understanding and potential refinement of the models. A chain of thought could be instantiated by adding something as simple as "Provide a step-by-step explanation" to the prompt.</p>
<p>Program Synthesis</p>
<p>Program synthesis evaluates the understanding of Language Models (LLMs) by generating code that can reproduce the result/answer. This evaluation method provides a more comprehensive assessment of the model's abilities as it requires the LLM to not only interpret the textual input accurately but also produce correct and functional code that fulfills the given requirements. Since most GTFS is tabular data, the pandas(26) library can be used to generate the code. Khatry et al. (17) applied LLMs to generate code in pandas and SQL.</p>
<p>Additionally, program synthesis, similar to CoT, fosters the interpretability and explainability of LLMs. Since the generated code is executable and traceable, it becomes easier to diagnose the LLM's behavior and identify potential errors or misconceptions. Besides, the code generated can be generalized by asking generalized questions that aren't particular to a specific GTFS. This makes it possible to bypass the context length limitation and extract information without the input of GTFS files.   </p>
<p>SHORTCOMINGS IN GTFS UNDERSTANDING</p>
<p>Upon initial observation, it appears that the interface version of GPT-3.5, known as ChatGPT, exhibits an understanding of GTFS without requiring any specific context. Voß (27) conducted a study to investigate whether generative AI comprehends concepts such as bus bunching and bridging. The findings reported in the paper indicate that ChatGPT demonstrates a general understanding of GTFS. However, to establish a more comprehensive understanding of GTFS by GPT, additional evidence is needed. To assess the depth of GPT's understanding of GTFS, various types of questionnaires are employed for evaluation.</p>
<p>A total of 183 questions are posed that span across these six question categories. The questions are posed in the form of multiple-choice questions (MCQs) with single or multiple correct answers. The LLM is supposed to pick the correct answer and output that without the need for any explanation. An example of this question-answer process is depicted in Figure 2.  Figure 4. The overall accuracy across all categories is 76.5%. The results show that the ChatGPT performs perfectly for "Common Reasoning" and "Data Structure" type questions and above 90% in " File Structure" and "Attribute Mapping". While the GPT struggles a little with term definitions specific to GTFS, the "Categorical Mapping" has the worst accuracy. Figure 5 shows examples of prompts and answers where the LLM fails to answer questions related to categorical mapping. When posed the same question with and without CoT (Figure 5a &amp; 5b), we obtained different results. However, both the results were incorrect as the route_type for bus routes is '3'. From the CoT explanation, it is evident that the LLM understands what the 'route_type' purpose is. However, it fails to understand the mapping of categories in it. User: In the "route_type" field of the "routes.txt" file, which value represents a bus? a) 0 b) 1 c) 2 d) 3</p>
<p>Assistant: The correct answer is b) 1. Explanation: In the "routes.txt" file of a GTFS feed, the "route_type" field is used to indicate the type of transportation for a particular route. Each route can be assigned a specific value in the "route_type" field to represent the mode of transportation.According to the GTFS specification, the value 1 is used to represent a bus route. This means that if the "route_type" field has a value of 1, it indicates that the route is for a bus.Therefore, the correct answer is b) 1.  </p>
<p>INFORMATION RETRIEVAL FROM GTFS</p>
<p>Similar to testing the understanding of GTFS, we pose questions to the GPT to see its capabilities in information retrieval. A total of 88 questions were posed to the ChatGPT with 47 simple and 41 complex questions. We use zero-shot and program synthesis prompting to generate and evaluate the responses. To pose these questions using zero-shot, we first input GTFS data using a series of prompts. Figure 6 gives an example of setting up ChatGPT for zero-shot prompting. For program synthesis, we use one-shot learning as default as the model was able to better generalize the way to output code when provided with an illustration. Figure 3 shows an example prompt for program synthesis. The code generated by the program synthesis is later executed to obtain the results.</p>
<p>The results (see Figure 7) show that the performance of program synthesis(67% accuracy) is significantly better than the zero-shot(40% accuracy) for both simple and complex levels of problems. This difference could be arising because of two reasons. With zero-shot, the LLM is unable to make the joins or connections between multiple files and instead hallucinates output or intermediate steps. The second reason would be that similar to CoT, program synthesis makes the GPT think in a step-by-step fashion thereby having fewer logical fallacies. While it cannot be deduced from zero-shot which of these reasons is the main cause, the program synthesis offers some insights to identify what the model is lacking. Out of the 29 questions that the program synthesis failed to answer, 18 were due to attribute mapping errors and 8 were due to logical errors. The remaining are due to errors in the understanding of data structure.</p>
<p>Besides accuracy, we measure the consistency of results between zero-shot and program synthesis. Figure 7b shows the consistency of outputs from both approaches. The average consistency is about 30%, indicating that the results from these two approaches are widely different, more so in the case of complex questions.</p>
<p>CONCLUSION</p>
<p>This paper confirms that ChatGPT (GPT-3.5) has a good understanding of GTFS data even without explicitly providing information. Through multiple-choice questions (MCQs), we identified the understanding of GTFS in different categories. Although ChatGPT works fairly well among most categories, it fails to accurately map categorical attributes to their corresponding values. To overcome this, one has to explicitly give the mapping information as an additional context or finetune the LLM.</p>
<p>The zero-shot prompting fails to retrieve information in most cases. However, the program synthesis works fairly well for simple questions and struggles with hallucinations for complex questions. The program synthesis output is significantly different from that zero-shot as it may elicit a chain of thought. Besides, unlike zero-shot, program synthesis can be used for asking generalized questions that are independent of a specific GTFS feed, thereby eliminating the bottleneck of context length. This paper demonstrates the capabilities of LLMs in comprehending GTFS data and utilizing this understanding to extract information from natural language prompts. The implications of this study extend to benefit both academics and practitioners, particularly those with limited coding expertise. This study suggests a potential shift away from conventional coding practices towards programming through instructions or examples. This democratizes transit data extraction and analysis; thereby encouraging community input from various stakeholders such as planners, local leaders, journalists, etc. Moreover, the paper introduces a systematic approach for evaluating LLMs' comprehension, which can be applied across diverse domains. As a result, other researchers can adopt similar methodologies to cater to their specific requirements. For instance, leveraging LLMs to validate GTFS data before its publication. Besides the approach, the set of questionnaires generated for this paper could also help with benchmarking other LLMs (or fine-tuned LLMs) on GTFS understanding and information retrieval.</p>
<p>Currently, this paper works on a closed-source LLM(GPT-3.5). Therefore, the only improvement can be brought through in-context learning(i.e. prompt-based learning). Within incontext learning, breaking down the question into simpler steps could result in better results. The hallucinations with program synthesis are a result of misalignment in 'Attribute Mapping' and logical errors in coding. The GPT-3.5 does not have a code interpreter and is prone to coding errors. Using the GPT-4 could overcome these as the model has a built-in code interpreter. The usage of open-source LLMs and training the LLMs using supervised fine-tuning and reinforcement learning with human feedback(RLHF) still need to be explored.  </p>
<p>FIGURE 1 :
1Bus and metro network of Chicago Transit Authority(CTA) generated using gtfssegments(3) python library refers to the maximum token length/count an LLM can process at a time. The ChatGPT or GPT-3.5 have a maximum context length of 16,000 tokens. Approximately 4 characters are equivalent to one token, making the context a little shy of 4000 words. For this reason, we trim the dataset to contain fewer routes and trips.</p>
<p>System: You are a helpful assistant with an understanding of GTFS(General Transit Feed Specification). Instructions: -You are given a set of questions with single or multiple correct answers with 4 options a,b,c, and d.-You do not have to provide any explanation.-Output only the answer for every question asked. For example, if the correct answer is 'a' then output 'a'. If the correct answers are 'a' and 'b' then output 'a,b'User: Can a GTFS feed contain multiple agency information? a) Yes, it can include multiple agencies. b) No, GTFS feeds can only represent a single agency. c) Multiple agency information is specified in the "agency.txt" file. d) Agencies are not relevant in GTFS feeds.Assistant: a,c</p>
<p>FIGURE 2 :
2Example zero-shot prompt with the system and user prompts along with assistant output to arrive at an answer systematically. Wei et al.</p>
<p>System: You are an expert assistant that understands GTFS file format thoroughly. You will be posed questions about information retrieval from GTFS.Instructions: -Respond to the question with Python code that answers the question. -Use only pandas library and output Python code that pertains to the question. -Assume you are in the directory of the GTFS file folder. -Store the result in a variable called 'answer' Example User: Find the route_url given the 'route_id'.</p>
<p>FIGURE 3 :
3Example prompt for program synthesis with the system and user prompts along with assistant output</p>
<p>FIGURE 4 :
4Summary of performance by question category for GPT-3.5 [ChatGPT] on GTFS understanding Using zero-shot learning, the LLM attempts to answer the questions without any examples provided to it. The results of zero-shot learning on different categories of questions are shown in</p>
<p>System: You are a helpful assistant with an understanding of GTFS(General Transit Feed Specification). Instructions: -You are given a set of questions with single or multiple correct answers with 4 options a,b,c, and d.-You do not have to provide any explanation. -Output only the answer for every question asked. For example, if the correct answer is 'a' then output 'a'. If the correct answers are 'a' and 'b' then output 'a,b'User: In the "route_type" field of the "routes.txt" file, which value represents a You are a helpful assistant with an understanding of GTFS(General Transit Feed Specification). Instructions: -You are given a set of questions with single or multiple correct answers with 4 options a,b,c and d.-Provide a step-by-step explanation for the answer -Output the answer for every question asked. For example, if the correct answer is 'a' then output 'a'. If the correct answers are 'a' and 'b' then output 'a,b'.</p>
<p>( b )
bZero-shot with Chain of Thought (CoT) prompt</p>
<p>FIGURE 5 :
5ChatGPT makes mistakes with 'Categorical Mapping'</p>
<p>FIGURE 7 :
7Summary of performance by question type for GPT-3.5 [ChatGPT] on data extraction from GTFS</p>
<p>TABLE 1 :
1Dataset questionnaire and their categoriesQuestion Type 
Number of Questions 
GTFS Understanding 
Term Definitions 
14 
Common Reasoning 
24 
File Structure 
15 
Attribute Mapping 
28 
Data Structure 
29 
Categorical Mapping 
73 
GTFS Retrieval 
Simple 
47 
Complex 
41 </p>
<p>The filtered GTFS data and questionnaire used in this paper are available at https://github.com/UTEL-UIUC/ GTFS_LLM
OpenAI Chat Completions API: https://platform.openai.com/docs/api-reference/completions/ create [Accessed 2023-07-29]3 Open AI Python Library https://github.com/openai/openai-python
Example Q&amp;A available at https://platform.openai.com/examples/default-qa [Accessed 2023-07-29]
ACKNOWLEDGEMENTSThis paper was initially drafted by the authors, and subsequently, certain sections were refined by gpt-3.5-turbo and text-davinci-002 to enhance clarity and improve the overall eloquence of the writing.AUTHOR CONTRIBUTIONSThe authors confirm their contribution to the paper as follows: study conception and design: S. Devunuri, S.Qiam, L. Lehe; data collection: S. Devunuri, S.Qiam; analysis and interpretation of results: S. Devunuri, S.Qiam, L. Lehe; draft manuscript preparation: S. Devunuri, S.Qiam, L. Lehe. All authors reviewed the results and approved the final version of the manuscript.
User: Using GTFS file format, identify files that are required. Ask each file one by one, and I will provide the input. User: Using GTFS file format, identify files that are required. Ask each file one by one, and I will provide the input.</p>
<p>Assistant: Great! To begin with, the GTFS format typically consists of several text files, each representing different aspects of the public transit data. The required files for GTFS analysis are: 1. agency.txt: Contains information about the transit agency operating the routes. calendar.txt: Defines the service availability for each day of the week. 2. calendar_dates.txt: Specifies exceptions for the 3. calendar.txt service availability. 4. routes.txt: Describes the routes operated by the transit agency. 5. trips.txt: Provides details about individual trips on each routeAssistant: Great! To begin with, the GTFS format typically consists of several text files, each representing different aspects of the public transit data. The required files for GTFS analysis are: 1. agency.txt: Contains information about the transit agency operating the routes. calendar.txt: Defines the service availability for each day of the week. 2. calendar_dates.txt: Specifies exceptions for the 3. calendar.txt service availability. 4. routes.txt: Describes the routes operated by the transit agency. 5. trips.txt: Provides details about individual trips on each route.</p>
<p>stop_times.txt: Contains the stop times for each trip. stop_times.txt: Contains the stop times for each trip.</p>
<p>txt: Indicates time-based headway information for trips. 10. transfers.txt: Specifies transfer rules for connections between routes at transfer points. 11. feed_info.txt: Contains information about the GTFS dataset itself. Now, let's proceed with the questions. I will ask for each required file. Describes the stops where vehicles pick up or drop off passengers. 8. shapes.txt: Defines the shapes of routes for rendering on maps. 9. frequencies. and you can provide the input. Please do not output the input you provide. Let's begin with the first file: Please provide the contents of the agency.txt file. User: agency_name,agency_url,agency_timezone,agency_lang,agency_phone, agency_fare_url Chicago Transit Authoritystops.txt: Describes the stops where vehicles pick up or drop off passengers. 8. shapes.txt: Defines the shapes of routes for rendering on maps. 9. frequencies.txt: Indicates time-based headway information for trips. 10. transfers.txt: Specifies transfer rules for connections between routes at transfer points. 11. feed_info.txt: Contains information about the GTFS dataset itself. Now, let's proceed with the questions. I will ask for each required file, and you can provide the input. Please do not output the input you provide. Let's begin with the first file: Please provide the contents of the agency.txt file. User: agency_name,agency_url,agency_timezone,agency_lang,agency_phone, agency_fare_url Chicago Transit Authority,http://transitchicago.com,America/Chicago,en,1-888-</p>
<p>Assistant: Great! Here's the next file, stops.txt. Please provide the input for stops. YOURCTA. YOURCTA,http://www.transitchicago.com/travel_information/fares/default.aspx Assistant: Great! Here's the next file, stops.txt. Please provide the input for stops.txt. ...</p>
<p>Skip the rest of the files. Now answer the following questions. User: Skip the rest of the files. Now answer the following questions:</p>
<p>What is the route_short_name for route_id 192?. What is the route_short_name for route_id 192?</p>
<p>How many stops are there for trip_id 6550025444020. How many stops are there for trip_id 6550025444020? ...</p>
<p>Assistant: 1) University of Chicago Hospitals Express 2)35. Assistant: 1) University of Chicago Hospitals Express 2)35 ...</p>
<p>GTFS Information Input and Questioning using ChatGPT REFERENCES 1. 6FIGURE 6: GTFS Information Input and Questioning using ChatGPT REFERENCES 1.</p>
<p>Pioneering open data standards: The GTFS Story. Beyond transparency: open data and the future of civic innovation. B Mchugh, McHugh, B., Pioneering open data standards: The GTFS Story. Beyond transparency: open data and the future of civic innovation, 2013, pp. 125-135.</p>
<p>C T Voulgaris, C Begwani, Predictors of Early Adoption of the General Transit Feed Specification. Findings. Voulgaris, C. T. and C. Begwani, Predictors of Early Adoption of the General Transit Feed Specification. Findings, 2023.</p>
<p>gtfs-segments -A fast and efficient library to generate bus stop spacings. S Devunuri, GithubDevunuri, S., gtfs-segments -A fast and efficient library to generate bus stop spacings. Github, 2022.</p>
<p>S Devunuri, S Qiam, L Lehe, A Pandey, Bus Stop Spacings Statistics: Theory and Evidence. Devunuri, S., S. Qiam, L. Lehe, and A. Pandey, Bus Stop Spacings Statistics: Theory and Evidence, 2022.</p>
<p>. Conveyal R5 Routing Engine, original-date: 2015-10-22T13:23:49Z2023Conveyal R5 Routing Engine, 2023, original-date: 2015-10-22T13:23:49Z.</p>
<p>R H M Pereira, M Saraiva, D Herszenhut, C K V Braga, M W Conway, r5r: Rapid Realistic Routing on Multimodal Transport Networks with R 5 in R. Findings, 2021. Network Design LabPereira, R. H. M., M. Saraiva, D. Herszenhut, C. K. V. Braga, and M. W. Conway, r5r: Rapid Realistic Routing on Multimodal Transport Networks with R 5 in R. Findings, 2021, publisher: Network Design Lab.</p>
<p>I I Bommarito, M , D M Katz, arXiv:2212.14402GPT takes the bar exam. arXiv preprintBommarito II, M. and D. M. Katz, GPT takes the bar exam. arXiv preprint arXiv:2212.14402, 2022.</p>
<p>GPT-4 Passes the Bar Exam. D M Katz, M J Bommarito, S Gao, P Arredondo, SSRN Electronic Journal. Katz, D. M., M. J. Bommarito, S. Gao, and P. Arredondo, GPT-4 Passes the Bar Exam. SSRN Electronic Journal, 2023.</p>
<p>T.-C Lee, K Staller, V Botoman, M P Pathipati, S Varma, B Kuo, ChatGPT Answers Common Patient Questions About Colonoscopy. Gastroenterology. 165e7Lee, T.-C., K. Staller, V. Botoman, M. P. Pathipati, S. Varma, and B. Kuo, ChatGPT An- swers Common Patient Questions About Colonoscopy. Gastroenterology, Vol. 165, No. 2, 2023, pp. 509-511.e7.</p>
<p>Assessing the Accuracy of Responses by the Language Model ChatGPT to Questions Regarding Bariatric Surgery: a Critical Appraisal. P P Ray, P Majumder, Obesity Surgery. 338Ray, P. P. and P. Majumder, Assessing the Accuracy of Responses by the Language Model ChatGPT to Questions Regarding Bariatric Surgery: a Critical Appraisal. Obesity Surgery, Vol. 33, No. 8, 2023, pp. 2588-2589.</p>
<p>How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment. A Gilson, C W Safranek, T Huang, V Socrates, L Chi, R A Taylor, D Chartash, JMIR Medical Education. 945312Gilson, A., C. W. Safranek, T. Huang, V. Socrates, L. Chi, R. A. Taylor, and D. Chartash, How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assess- ment. JMIR Medical Education, Vol. 9, 2023, p. e45312.</p>
<p>Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming. D Sobania, M Briesch, F Rothlauf, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation ConferenceNew York, NY, USA, 2022Association for Computing Machinery22Sobania, D., M. Briesch, and F. Rothlauf, Choose your programming copilot: a compari- son of the program synthesis performance of github copilot and genetic programming. In Proceedings of the Genetic and Evolutionary Computation Conference, Association for Computing Machinery, New York, NY, USA, 2022, GECCO '22, pp. 1019-1027.</p>
<p>An Analysis of the Automatic Bug Fixing Performance of ChatGPT. D Sobania, M Briesch, C Hanna, J Petke, arXiv:2301.08653csSobania, D., M. Briesch, C. Hanna, and J. Petke, An Analysis of the Automatic Bug Fixing Performance of ChatGPT, 2023, arXiv:2301.08653 [cs].</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, arXiv:2107.033742021Evaluating Large Language Models Trained on Code. csChen, M., J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sas- try, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavar- ian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba, Evaluating Large Language Models Trained on Code, 2021, arXiv:2107.03374 [cs].</p>
<p>Jigsaw: large language models meet program synthesis. N Jain, S Vaidyanath, A Iyer, N Natarajan, S Parthasarathy, S Rajamani, R Sharma, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software EngineeringNew York, NY, USA, 2022Association for Computing Machinery22Jain, N., S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Rajamani, and R. Sharma, Jigsaw: large language models meet program synthesis. In Proceedings of the 44th International Conference on Software Engineering, Association for Computing Machinery, New York, NY, USA, 2022, ICSE '22, pp. 1219-1231.</p>
<p>A Comprehensive Survey on Program Synthesis With Evolutionary Algorithms. D Sobania, D Schweim, F Rothlauf, IEEE Transactions on Evolutionary Computation. 271Sobania, D., D. Schweim, and F. Rothlauf, A Comprehensive Survey on Program Syn- thesis With Evolutionary Algorithms. IEEE Transactions on Evolutionary Computation, Vol. 27, No. 1, 2023, pp. 82-97.</p>
<p>A Khatry, J Cahoon, J Henkel, S Deep, V Emani, A Floratou, S Gulwani, V Le, M Raza, S Shi, M Singh, A Tiwari, arXiv:2305.01598From Words to Code: Harnessing Data for Program Synthesis from Natural Language. 2023csKhatry, A., J. Cahoon, J. Henkel, S. Deep, V. Emani, A. Floratou, S. Gulwani, V. Le, M. Raza, S. Shi, M. Singh, and A. Tiwari, From Words to Code: Harnessing Data for Program Synthesis from Natural Language, 2023, arXiv:2305.01598 [cs].</p>
<p>T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, arXiv:2005.14165Language Models are Few-Shot Learners. csBrown, T. B., B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, Language Models are Few-Shot Learners, 2020, arXiv:2005.14165 [cs].</p>
<p>. Openai, arXiv:2303.08774GPT-4 Technical ReportOpenAI, GPT-4 Technical Report, 2023, arXiv:2303.08774 [cs].</p>
<p>T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. BloomarXiv preprintScao, T. L., A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, et al., Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.</p>
<p>LLaMA: Open and Efficient Foundation Language Models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Touvron, H., T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Roz- ière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, LLaMA: Open and Efficient Foundation Language Models, 2023.</p>
<p>Stanford Alpaca: An Instruction-following LLaMA model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, 2023Taori, R., I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto, Stanford Alpaca: An Instruction-following LLaMA model. https://github. com/tatsu-lab/stanford_alpaca, 2023.</p>
<p>ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?. O Zheng, M Abdel-Aty, D Wang, Z Wang, S Ding, Zheng, O., M. Abdel-Aty, D. Wang, Z. Wang, and S. Ding, ChatGPT Is on the Horizon: Could a Large Language Model Be All We Need for Intelligent Transportation?, 2023.</p>
<p>Large Language Models are Zero-Shot Reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, Kojima, T., S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, Large Language Models are Zero-Shot Reasoners, 2023.</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E Chi, Q Le, D Zhou, arXiv:2201.11903Thought Prompting Elicits Reasoning in Large Language Models. 2023csWei, J., X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou, Chain-of-Thought Prompting Elicits Reasoning in Large Language Models, 2023, arXiv:2201.11903 [cs].</p>
<p>Data Structures for Statistical Computing in Python. Wes Mckinney, Proceedings of the 9th Python in Science Conference. Stéfan van der Walt and Jarrod Millmanthe 9th Python in Science ConferenceWes McKinney, Data Structures for Statistical Computing in Python. In Proceedings of the 9th Python in Science Conference (Stéfan van der Walt and Jarrod Millman, eds.), 2010, pp. 56 -61.</p>
<p>S Voß, Bus Bunching and Bus Bridging: What Can We Learn from Generative AI Tools like ChatGPT? Sustainability. Multidisciplinary Digital Publishing Institute159625number: 12 PublisherVoß, S., Bus Bunching and Bus Bridging: What Can We Learn from Generative AI Tools like ChatGPT? Sustainability, Vol. 15, No. 12, 2023, p. 9625, number: 12 Publisher: Mul- tidisciplinary Digital Publishing Institute.</p>            </div>
        </div>

    </div>
</body>
</html>
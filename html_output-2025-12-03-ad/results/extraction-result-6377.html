<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6377 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6377</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6377</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-270562306</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.12793v2.pdf" target="_blank">ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools</a></p>
                <p><strong>Paper Abstract:</strong> We introduce ChatGLM, an evolving family of large language models that we have been developing over time. This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B. They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM. To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage. The high-quality alignment is achieved via a multi-stage post-training process, which involves supervised fine-tuning and learning from human feedback. Evaluations show that GLM-4 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench. The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) touse -- including web browser, Python interpreter, text-to-image model, and user-defined functions -- to effectively complete complex tasks. In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter. Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone. The open models can be accessed through https://github.com/THUDM and https://huggingface.co/THUDM.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6377.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6377.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLM-4 (arithmetic eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM-4 (version 0520) arithmetic evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GLM-4 (0520) on standard arithmetic/math benchmarks (GSM8K, MATH) using chain-of-thought prompting; reported high accuracy on GSM8K and moderate performance on competition-level MATH, with continued improvements via post-training techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4 (0520)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (GLM family, decoder/autoregressive with blank-infilling design choices)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Pre-trained on ~10 trillion tokens mostly Chinese and English from webpages, Wikipedia, books, code, and research papers (includes code and educational sources; high‑quality sources upweighted).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MATH (also BBH used with CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step word problems (GSM8K), competition-level mathematics (MATH); general mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems (GSM8K) and competition-style problems (MATH); evaluated with chain-of-thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K: grade-school level multi-step arithmetic; MATH: competition / hard mathematics</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-thought prompting (explicitly stated for GSM8K, MATH, BBH evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percent correct)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: 93.3% accuracy; MATH: 61.3% accuracy (Table 2, GLM-4 (0520))</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>The paper does not provide mechanistic interpretability analyses specific to numeric representations (no attention / activation / logit-lens probing reported). It does report higher-level empirical practices relevant to math: use of a ChatGLM-Math self-critique pipeline to improve math performance and the observation that certain abilities emerge only after pre-training loss drops below thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not characterized in mechanistic detail; authors note a remaining gap in the 'Mathematics' dimension relative to some GPT‑4 variants and that iterative methods (self-critique) are used to reduce math errors. No specific patterns like off‑by‑one or digit-swapping are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Paper reports progressive improvement across model generations and post-training (SFT/RLHF) and states emergent behavior tied to pre-training loss thresholds; smaller variants (e.g., GLM-4-9B variants) perform worse on math benchmarks, indicating improvement with larger/further-trained models and alignment/finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6377.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6377.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLM-4 All Tools (tool-augmented math)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLM-4 All Tools (autonomous tool use for math via Python interpreter and browser)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GLM-4 All Tools is aligned to autonomously decide to call external tools (Python interpreter, web browser, etc.) and is evaluated on math solving and information retrieval tasks using tool invocation; reportedly matches or exceeds comparable 'All Tools' systems on many practical math tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4 All Tools</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Transformer (GLM family) with tool-using agent alignment</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Same base pre-training (~10T tokens mostly Chinese/English) plus multi-stage post-training (SFT, RLHF, AgentTuning) and alignment data for tool use (AgentInstruct trajectories); no separate external-code training explicitly required because Python interpreter is an external tool.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K (via All Tools), Python Interpreter evaluation on MATH and Math23K; browser-based information-seeking for tasks combining data + arithmetic (Table 9)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Grade-school math / multi-step arithmetic solved by calling a Python interpreter; larger math benchmarks (Math23K) and composite tasks combining lookup + calculation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts where the model may call tools (Python code execution) to perform arithmetic/analysis; tool invocation and returned execution results provide grounded computation</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Ranges from grade-school (GSM8K) to Math23K (larger/harder set); includes multi-step calculations and data-aggregation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Agentic/tool-use prompting: model is aligned to analyze the task, plan step-by-step, and call the Python interpreter or browser as needed (autonomous tool selection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percent) on benchmark tasks when using tools; comparative performance vs GPT-4 Web/All Tools reported</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K (All Tools): 91.59% for GLM-4 All Tools vs 92.72% (GPT-4 Web); Python Interpreter MATH: 63.60% (GLM-4 All Tools) vs 65.00% (GPT-4 Web); Math23K: 88.50% (GLM-4 All Tools) vs 88.40% (GPT-4 Web). Browser information-seeking combined with arithmetic: GLM-4 All Tools 78.08% vs GPT-4(Web) 67.12% (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic analysis of how tool-calls lead to improved arithmetic reasoning is provided; improvements are reported at system/behavioral level (tool use leads to more accurate numeric answers when executions are available).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper does not enumerate specific mechanistic failure modes for tool-augmented arithmetic; general observation that tool use improves practical math solution accuracy and that GLM-4 All Tools often matches/surpasses GPT-4 All Tools in experiments performed by authors.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Tool integration (Python interpreter) is presented as an effective route to close performance gaps on arithmetic; system-level math performance benefits from tool-invocation and alignment (AgentTuning) rather than only scaling model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6377.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6377.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM‑Math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM‑Math (self-critique pipeline for math)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-training/data-selection pipeline that improves math problem solving by using the model itself to perform self-critique for selecting or filtering math training examples, reducing reliance on external verifiers or manual annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM-Math (method applied to GLM family / GLM-4)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Method applied to Transformer GLM models</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Uses model-generated outputs and the model's self-critique judgments to select higher-quality math training / finetuning samples; does not rely on external verifiers or manual annotations for selection.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Applied to improve performance on math benchmarks (e.g., GSM8K, MATH) as stated in the report</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step math reasoning and arithmetic problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language word problems and competition-style math problems; data selection and finetuning uses model outputs and critiques</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>Grade-school to competition-level (used to target GSM8K and MATH improvements)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Self-critique pipeline: model produces solutions and then critiques its own outputs; selected examples (based on self-critique) are used for data curation/finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Not presented as a single numeric metric in the report; described as continuously improving math reasoning capability of GLM models when applied</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Technique is procedural (data selection by self-critique). The paper does not present internal probes of numeric representations or layers that specifically show how self-critique changes internal arithmetic processing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Not explicitly enumerated for the self-critique method; the authors note it's part of ongoing improvements to address remaining mathematics gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Described as an iterative alignment / data-selection method that can be applied across model sizes; reported as one of the techniques used to continually enhance math performance across GLM iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6377.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6377.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain‑of‑Thought (CoT) prompting (used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain‑of‑Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that elicits multi-step intermediate reasoning (chain-of-thought) from LLMs; explicitly used by the authors in GSM8K, MATH, and BBH evaluations for GLM-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GLM-4 (and earlier ChatGLM evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>Prompting method for Transformer LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, BBH (CoT explicitly stated for these evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Multi-step arithmetic/word-problem reasoning and reasoning benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language questions with explicit instruction to produce chain-of-thought reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>From grade‑school multi-step problems (GSM8K) to harder reasoning tasks (MATH, BBH)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Chain-of-thought prompting (elicitation of intermediate reasoning steps); used as the evaluation prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (percent) reported in benchmark tables when CoT was used</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used to produce the reported results (e.g., GSM8K 93.3% for GLM-4 (0520) under CoT evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper does not provide detailed analysis of intermediate CoT activations or whether CoT tokens correspond to distinct numeric processing circuits; only behavioral-level improvements are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>No detailed CoT-specific failure modes analyzed in this report; authors note residual math gaps and employ additional techniques (self-critique, tool use) to mitigate them.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>CoT is used across evaluations; overall math performance improves with larger/later GLM iterations and with post-training techniques combined with CoT prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ChatGLM-Math: Improving math problem-solving in large language models with a self-critique pipeline <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6377",
    "paper_id": "paper-270562306",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "GLM-4 (arithmetic eval)",
            "name_full": "GLM-4 (version 0520) arithmetic evaluation",
            "brief_description": "Evaluation of GLM-4 (0520) on standard arithmetic/math benchmarks (GSM8K, MATH) using chain-of-thought prompting; reported high accuracy on GSM8K and moderate performance on competition-level MATH, with continued improvements via post-training techniques.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-4 (0520)",
            "model_family": "Transformer (GLM family, decoder/autoregressive with blank-infilling design choices)",
            "model_size": null,
            "training_data_description": "Pre-trained on ~10 trillion tokens mostly Chinese and English from webpages, Wikipedia, books, code, and research papers (includes code and educational sources; high‑quality sources upweighted).",
            "benchmark_name": "GSM8K; MATH (also BBH used with CoT)",
            "task_type": "Multi-step word problems (GSM8K), competition-level mathematics (MATH); general mathematical reasoning",
            "problem_format": "Natural-language word problems (GSM8K) and competition-style problems (MATH); evaluated with chain-of-thought prompting",
            "difficulty_level": "GSM8K: grade-school level multi-step arithmetic; MATH: competition / hard mathematics",
            "prompting_method": "Chain-of-thought prompting (explicitly stated for GSM8K, MATH, BBH evaluations)",
            "performance_metric": "Accuracy (percent correct)",
            "performance_value": "GSM8K: 93.3% accuracy; MATH: 61.3% accuracy (Table 2, GLM-4 (0520))",
            "internal_analysis": "The paper does not provide mechanistic interpretability analyses specific to numeric representations (no attention / activation / logit-lens probing reported). It does report higher-level empirical practices relevant to math: use of a ChatGLM-Math self-critique pipeline to improve math performance and the observation that certain abilities emerge only after pre-training loss drops below thresholds.",
            "failure_modes": "Not characterized in mechanistic detail; authors note a remaining gap in the 'Mathematics' dimension relative to some GPT‑4 variants and that iterative methods (self-critique) are used to reduce math errors. No specific patterns like off‑by‑one or digit-swapping are reported in this paper.",
            "scaling_trend": "Paper reports progressive improvement across model generations and post-training (SFT/RLHF) and states emergent behavior tied to pre-training loss thresholds; smaller variants (e.g., GLM-4-9B variants) perform worse on math benchmarks, indicating improvement with larger/further-trained models and alignment/finetuning.",
            "uuid": "e6377.0",
            "source_info": {
                "paper_title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "GLM-4 All Tools (tool-augmented math)",
            "name_full": "GLM-4 All Tools (autonomous tool use for math via Python interpreter and browser)",
            "brief_description": "GLM-4 All Tools is aligned to autonomously decide to call external tools (Python interpreter, web browser, etc.) and is evaluated on math solving and information retrieval tasks using tool invocation; reportedly matches or exceeds comparable 'All Tools' systems on many practical math tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-4 All Tools",
            "model_family": "Transformer (GLM family) with tool-using agent alignment",
            "model_size": null,
            "training_data_description": "Same base pre-training (~10T tokens mostly Chinese/English) plus multi-stage post-training (SFT, RLHF, AgentTuning) and alignment data for tool use (AgentInstruct trajectories); no separate external-code training explicitly required because Python interpreter is an external tool.",
            "benchmark_name": "GSM8K (via All Tools), Python Interpreter evaluation on MATH and Math23K; browser-based information-seeking for tasks combining data + arithmetic (Table 9)",
            "task_type": "Grade-school math / multi-step arithmetic solved by calling a Python interpreter; larger math benchmarks (Math23K) and composite tasks combining lookup + calculation",
            "problem_format": "Natural-language prompts where the model may call tools (Python code execution) to perform arithmetic/analysis; tool invocation and returned execution results provide grounded computation",
            "difficulty_level": "Ranges from grade-school (GSM8K) to Math23K (larger/harder set); includes multi-step calculations and data-aggregation tasks",
            "prompting_method": "Agentic/tool-use prompting: model is aligned to analyze the task, plan step-by-step, and call the Python interpreter or browser as needed (autonomous tool selection).",
            "performance_metric": "Accuracy (percent) on benchmark tasks when using tools; comparative performance vs GPT-4 Web/All Tools reported",
            "performance_value": "GSM8K (All Tools): 91.59% for GLM-4 All Tools vs 92.72% (GPT-4 Web); Python Interpreter MATH: 63.60% (GLM-4 All Tools) vs 65.00% (GPT-4 Web); Math23K: 88.50% (GLM-4 All Tools) vs 88.40% (GPT-4 Web). Browser information-seeking combined with arithmetic: GLM-4 All Tools 78.08% vs GPT-4(Web) 67.12% (Table 9).",
            "internal_analysis": "No internal mechanistic analysis of how tool-calls lead to improved arithmetic reasoning is provided; improvements are reported at system/behavioral level (tool use leads to more accurate numeric answers when executions are available).",
            "failure_modes": "Paper does not enumerate specific mechanistic failure modes for tool-augmented arithmetic; general observation that tool use improves practical math solution accuracy and that GLM-4 All Tools often matches/surpasses GPT-4 All Tools in experiments performed by authors.",
            "scaling_trend": "Tool integration (Python interpreter) is presented as an effective route to close performance gaps on arithmetic; system-level math performance benefits from tool-invocation and alignment (AgentTuning) rather than only scaling model parameters.",
            "uuid": "e6377.1",
            "source_info": {
                "paper_title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "ChatGLM‑Math",
            "name_full": "ChatGLM‑Math (self-critique pipeline for math)",
            "brief_description": "A post-training/data-selection pipeline that improves math problem solving by using the model itself to perform self-critique for selecting or filtering math training examples, reducing reliance on external verifiers or manual annotation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGLM-Math (method applied to GLM family / GLM-4)",
            "model_family": "Method applied to Transformer GLM models",
            "model_size": null,
            "training_data_description": "Uses model-generated outputs and the model's self-critique judgments to select higher-quality math training / finetuning samples; does not rely on external verifiers or manual annotations for selection.",
            "benchmark_name": "Applied to improve performance on math benchmarks (e.g., GSM8K, MATH) as stated in the report",
            "task_type": "Multi-step math reasoning and arithmetic problem solving",
            "problem_format": "Natural-language word problems and competition-style math problems; data selection and finetuning uses model outputs and critiques",
            "difficulty_level": "Grade-school to competition-level (used to target GSM8K and MATH improvements)",
            "prompting_method": "Self-critique pipeline: model produces solutions and then critiques its own outputs; selected examples (based on self-critique) are used for data curation/finetuning.",
            "performance_metric": "Not presented as a single numeric metric in the report; described as continuously improving math reasoning capability of GLM models when applied",
            "performance_value": null,
            "internal_analysis": "Technique is procedural (data selection by self-critique). The paper does not present internal probes of numeric representations or layers that specifically show how self-critique changes internal arithmetic processing.",
            "failure_modes": "Not explicitly enumerated for the self-critique method; the authors note it's part of ongoing improvements to address remaining mathematics gaps.",
            "scaling_trend": "Described as an iterative alignment / data-selection method that can be applied across model sizes; reported as one of the techniques used to continually enhance math performance across GLM iterations.",
            "uuid": "e6377.2",
            "source_info": {
                "paper_title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Chain‑of‑Thought (CoT) prompting (used)",
            "name_full": "Chain‑of‑Thought prompting",
            "brief_description": "A prompting strategy that elicits multi-step intermediate reasoning (chain-of-thought) from LLMs; explicitly used by the authors in GSM8K, MATH, and BBH evaluations for GLM-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to GLM-4 (and earlier ChatGLM evaluations)",
            "model_family": "Prompting method for Transformer LLMs",
            "model_size": null,
            "training_data_description": null,
            "benchmark_name": "GSM8K, MATH, BBH (CoT explicitly stated for these evaluations)",
            "task_type": "Multi-step arithmetic/word-problem reasoning and reasoning benchmarks",
            "problem_format": "Natural-language questions with explicit instruction to produce chain-of-thought reasoning",
            "difficulty_level": "From grade‑school multi-step problems (GSM8K) to harder reasoning tasks (MATH, BBH)",
            "prompting_method": "Chain-of-thought prompting (elicitation of intermediate reasoning steps); used as the evaluation prompting strategy",
            "performance_metric": "Accuracy (percent) reported in benchmark tables when CoT was used",
            "performance_value": "Used to produce the reported results (e.g., GSM8K 93.3% for GLM-4 (0520) under CoT evaluation).",
            "internal_analysis": "Paper does not provide detailed analysis of intermediate CoT activations or whether CoT tokens correspond to distinct numeric processing circuits; only behavioral-level improvements are reported.",
            "failure_modes": "No detailed CoT-specific failure modes analyzed in this report; authors note residual math gaps and employ additional techniques (self-critique, tool use) to mitigate them.",
            "scaling_trend": "CoT is used across evaluations; overall math performance improves with larger/later GLM iterations and with post-training techniques combined with CoT prompting.",
            "uuid": "e6377.3",
            "source_info": {
                "paper_title": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ChatGLM-Math: Improving math problem-solving in large language models with a self-critique pipeline",
            "rating": 2,
            "sanitized_title": "chatglmmath_improving_math_problemsolving_in_large_language_models_with_a_selfcritique_pipeline"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        }
    ],
    "cost": 0.014824749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
30 Jul 2024</p>
<p>Team Glm 
Aohan Zeng 
Bin Xu 
Bowen Wang 
Chenhui Zhang 
Da Yin 
Dan Zhang 
Diego Rojas 
Guanyu Feng 
Hanlin Zhao 
Hanyu Lai 
Hao Yu 
Hongning Wang 
Jiadai Sun 
Jiajie Zhang 
Jiale Cheng 
Jiayi Gui 
Jie Tang 
Jing Zhang 
Jingyu Sun 
Juanzi Li 
Lei Zhao 
Lindong Wu 
Lucen Zhong 
Mingdao Liu 
Minlie Huang 
Peng Zhang 
Qinkai Zheng 
Rui Lu 
Shuaiqi Duan 
Shudan Zhang 
Shulin Cao 
WengShuxun Yang 
Lam Tam 
Wenyi Zhao 
Xiao Liu 
Xiao Xia 
Xiaohan Zhang 
Xiaotao Gu 
Xin Lv 
Xinghan Liu 
Xinyi Liu 
Xinyue Yang 
Xixuan Song 
Xunkai Zhang 
Yifan An 
Yifan Xu 
Yilin Niu 
Yuantao Yang 
Yueyan Li 
Yushi Bai 
Yuxiao Dong 
Zehan Qi 
Zhaoyu Wang 
Zhen Yang 
Zhengxiao Du 
Zhenyu Hou 
Zihan Wang 
] Cogvideo 
Tsinghua University</p>
<p>ZhipuAI</p>
<p>ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools
30 Jul 2024DF16376525EFF1EC6069B9BCB0F64463arXiv:2406.12793v2[cs.CL]
We introduce ChatGLM, an evolving family of large language models that we have been developing over time.This report primarily focuses on the GLM-4 language series, which includes GLM-4, GLM-4-Air, and GLM-4-9B.They represent our most capable models that are trained with all the insights and lessons gained from the preceding three generations of ChatGLM.To date, the GLM-4 models are pre-trained on ten trillions of tokens mostly in Chinese and English, along with a small set of corpus from 24 languages, and aligned primarily for Chinese and English usage.The high-quality alignment is achieved via a multi-stage posttraining process, which involves supervised fine-tuning and learning from human feedback.Evaluations show that GLM-4, 1) closely rivals or outperforms GPT-4 in terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, 2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3) matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms GPT-4 in Chinese alignments as measured by AlignBench.The GLM-4 All Tools model is further aligned to understand user intent and autonomously decide when and which tool(s) to use-including web browser, Python interpreter, text-to-image model, and user-defined functions-to effectively complete complex tasks.In practical applications, it matches and even surpasses GPT-4 All Tools in tasks like accessing online information via web browsing and solving math problems using Python interpreter.Over the course, we have open-sourced a series of models, including ChatGLM-6B (three generations), GLM-4-9B (128K, 1M), GLM-4V-9B, WebGLM, and CodeGeeX, attracting over 10 million downloads on Hugging face in the year 2023 alone.The open models can be accessed through https://github.com/THUDMand https://huggingface.co/THUDM.</p>
<p>Introduction</p>
<p>The rapid development of large language models (LLMs) has been phenomenal [57].Take one of the most successful model series, the OpenAI's GPT models, as an example: the original GPT-3 model released in 2020 [3] marked a significant scale-up from GPT-1's 117 million parameters and GPT-2's 1.5 billion parameters, to 175 billion parameters.This scale-up enables the decoder-only transformer-based GPT-3 model with in-context learning and generalized capabilities: according to OpenAI, the GPT-3.5 series improved upon GPT-3 by incorporating instruction tuning, supervised fine tuning (SFT), and/or reinforcement learning from human feedback (RLHF) [29].This has now became a standard procedure to create performing LLMs, including the PaLM models [6], the LLaMA models [41], the Gemini models [40], and many more.</p>
<p>In a parallel line to the popularly adopted LLMs development practices, we proposed the General Language Model (GLM) architecture [11] featured with the autoregressive blank infilling objective and open-sourced the GLM-10B model in 2021 (See the GLM timeline in Figure 1).Starting in late 2021, we began pre-training GLM-130B [53].The goal was to train a 100B-scale model to match or surpass GPT-3 (davinci) while also verifying the techniques for successfully training models at this scale, along with other contemporary efforts such as OPT-175B [54] and BLOOM-176B [33].We completed the 400B-token training and evaluation of GLM-130B in July, and subsequently released the model and pre-training details [53] in August 2022.According to HELM in November 2022, GLM-130B matches GPT-3 (davinci) across various dimensions [20].</p>
<p>Following this, we initiated instruction tuning on GLM-130B.Later, ChatGPT further motivated us to align the base models with SFT and RLHF.We created and crafted the prompt-response pairs from scratch and performed SFT, while also starting to examine how to effectively apply RLHF.On March 14, 2023, the aligned model, ChatGLM-130B, went live on https://chatglm.cn.In addition, a smaller version, ChatGLM-6B [13], was open-sourced on the same day, attracting significantly more attention than anticipated.It was designed to have 6.2 billion parameters for 1) facilitating fast iteration of pre-and post-training techniques as well as data selection, and 2) enabling local deployment on consumer-grade graphics cards using INT4 quantization.Since then, we have been rapidly exploring and refining our pre-training and alignment techniques, leading to the second and third generations of ChatGLM series every other three months, both of which were pre-trained entirely from the beginning.</p>
<p>ChatGLM-6B was pre-trained on approximately one trillion tokens of Chinese and English corpus with a context length of 2,048 (2K), supplemented mostly by SFT.Released in June, ChatGLM2-6B was pre-trained and aligned with more high-quality data, leading to substantial improvements over its predecessor, including a 23% improvement on MMLU, 571% on GSM8K, and 60% on BBH.By adopting the FlashAttention technique [8], its context length was extended to 32K.Additionally, the integration of Multi-Query Attention [35] contributed to a 42% increase in inference speed.Taking this further, our 2nd generation code model CodeGeeX2-6B was developed by pre-training on an additional 600 billion code tokens.It demonstrated Pass@1 improvements over the initial generation, CodeGeeX-13B [58], with increases of 57% in Python, 71% in C++, 54% in Java, 83% in JavaScript, and 56% in Go as measured by HumanEval-X.When adapting to Character-based Dialogues, CharacterGLM [61] allows effective and safe character customization on LLMs.By further adapting more diverse training datasets, more sufficient training steps, and more optimized training strategies, ChatGLM3-6B topped 42 benchmarks across semantics, mathematics, reasoning, code, and knowledge.Starting from this generation, ChatGLM also supports function call and code interpreter, as well as complex agent tasks [22; 52; 18].In the course of these developments, we also developed models with 1.5B, 3B, 12B, 32B, 66B, and 130B parameters, allowing us to validate our observations and establish our own scaling laws.With all the lessons learned and experiences accumulated, we kicked off the training of GLM-4.The first cutoff checkpoint then underwent a multi-stage post-training process (e.g., SFT, RLHF, safety alignment) with a focus on the Chinese and English language for now.Subsequently, it was developed into two distinct versions: GLM-4 and GLM-4 All Tools, both supporting a 128K context length.Since Janurary 16, 2024, GLM-4 (0116) has been made available through the GLM-4 API at https://bigmodel.cn, and GLM-4 All Tools is accessible via the website https://chatglm.cnand mobile applications that support the creation of one's own agent-GLMs.The latest models are GLM-4 (0520) and GLM-4-Air (0605) with an upgrade on both pre-training and alignment.GLM-4-Air achieves comparable performance to GLM-4 (0116) with lower latency and inference cost.Evaluations of GLM-4 were performed on a variety of language benchmarks.These evaluations assess GLM-4's general abilities in English, instruction following in both English and Chinese, and alignment, long-context, and agent capacities in Chinese.First, on the most commonly-used English academic benchmarks-MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval, GLM-4 0520 achieves performance closely comparable to that of GPT-4 0613 [28] and Gemini 1.5 Pro [40].For example, it scores 83.3 vs. 86.4 and 83.7 on MMLU, respectively.Second, according to IFEval [62], GLM-4's instruction following capacities on both prompt and instruction levels are approximately as effective as GPT-4-Turbo in both English and Chinese.Third, in terms of Chinese language alignment, GLM-4 outperforms GPT-4 and matches GPT-4-Turbo across eight dimensions in AlignBench [23].Finally, for long-context tasks, the GLM-4 (128K) model matches the performance of GPT-4 Turbo and Claude 3 Opus as measured by LongBench-Chat [1], i.e., 87.3 vs. 87.2 and 87.7, respectively.</p>
<p>The GLM-4 All Tools model is specifically aligned to better understand user intent and autonomously select the most appropriate tool(s) for task completion.For example, it can access online information via a web browser in a multi-round manner, use Python interpreter to solve math problems, leverage a text-to-image model to generate images, and call user-defined functions.Figure 2 illustrates an example showing GLM-4 All Tools with a web browser and Python interpreter for addressing the user query of "Search for the global population from 2000 to 2023, then calculate the average annual growth rate".Our first-hand test shows that it not only matches but often surpasses the capabilities of GPT-4 All Tools for common tasks.</p>
<p>Following our three generations of open ChatGLM-6B models, we also openly released the GLM-4-9B (128K and 1M context length) model.GLM-4-9B is pre-trained on approximately ten trillion tokens of multilingual corpus with a context length of 8192 (8K) and post-trained with the same pipeline and data used for GLM-4 (0520).With less training compute, it outperforms Llama-3-8B [26] and supports all the functionality of All Tools in GLM-4.We also provide an experimental model GLM-4-9B-Chat-1M with 1 million (1M) context length (about 2 million Chinese characters).</p>
<p>Table 1 shows the performance of the three generations of ChatGLM-6B models and GLM-4-9B, illustrating the progressive improvements of ChatGLM over time.</p>
<p>Figure 3 summarizes the major improvements and features from GLM-130B to GLM-4 All Tools.Throughout this journey, we have also contributed to the open development of the code LLMs (CodeGeeX [58]) as well as visual language models for image understanding (CogVLM [45] and CogAgent [16]) and text-to-image generation (CogView [9; 10; 59]).The open models and data can be accessed via https://github.com/THUDMand https://huggingface.co/THUDM.</p>
<p>ChatGLM Techniques</p>
<p>In this section, we introduce both the pre-and post-training techniques we adopted and developed in ChatGLM, including the model architecture, pre-training data, alignment, and All Tools.We have detailed technical reports introducing each of the major techniques we used to reach GLM-4.</p>
<p>Pre-Training Data.Our pre-training corpus consists of multilingual (mostly English and Chinese) documents from a mixture of different sources, including webpages, Wikipedia, books, code, and research papers.The data processing pipeline mainly includes three stages: deduplication, filtering, and tokenization.The deduplication stage improves data diversity by removing duplicated or similar documents, with both exact and fuzzy deduplication.The filtering stage for webpages improves data quality by removing noisy documents that contain offensive language, placeholder text, source code, etc.The tokenization stage converts text into a sequence of tokens for further processing.The number of tokens in the pre-training data directly affects model training speed.To optimize this aspect, we employ the byte-level byte pair encoding (BPE) algorithm [34] to separately learn the Chinese and multilingual tokens and merge them with the tokens of the cl100k_base tokenizer in tiktoken [27] into a unified vocabulary with a size of 150,000.In the final training set, we re-weight different sources to increase the importance of high-quality and educational sources like books and Wikipedia.To this end, the pre-training corpus consists of around ten trillion tokens.</p>
<p>Throughout the four generations of ChatGLM development, our findings align with existing studies [60]: data quality and diversity are crucial for building effective LLMs.Despite the empirical lessons and insights gained, we have to date yet to identify a fundamental principle that could guide the processes of data collection, cleaning, and selection, which might inspire future research directions.</p>
<p>Architecture.The GLM family of LLMs is built on Transformer [43].In GLM-130B [53], we explored various options to stabilize its pre-training by taking into account the hardware constraints we faced at the time.Specifically, GLM-130B leveraged DeepNorm [44] as the layer normalization strategy and used Rotary Positional Encoding (RoPE) [38] as well as the Gated Linear Unit [36] with GeLU [15] activation function in FFNs.Throughout our exploration, we have investigated different strategies to enhance model performance and inference efficiency.The recent GLM-4 model adopts the following architecture design choices.</p>
<p>• No Bias Except QKV: To increase training speed, we removed all bias terms with the exception of the biases in Query, Key, and Value (QKV) matrices of the attention layers.In doing so, we observed a slight improvement in length extrapolation.• RMSNorm and SwiGLU: We adopted RMSNorm and SwiGLU to replace LayerNorm and ReLU, respectively.These two strategies brought better model performance.• Rotary positional embeddings (RoPE): We extended the RoPE to a two-dimensional form to accommodate the 2D positional encoding in GLM.• Group Query Attention (GQA): We replaced Multi-Head Attention (MHA) with Group Query Attention (GQA) to cut down on the KV cache size during inference.Given GQA uses fewer parameters than MHA, we increased the FFN parameter count to maintain the same model size, i.e., setting d ffn to 10/3 of the hidden size.</p>
<p>The context length of our models was extended from 2K (ChatGLM), to 32K (ChatGLM2 and ChatGLM3), and to 128K and 1M (GLM-4).These expansions were achieved not only through context extension-position encoding extension [31; 5] and continual training [47] on long text-but also long context alignment, enabling GLM-4 to effectively handle very long contexts (Cf [1] for technical details).</p>
<p>Alignment.Pre-training builds the foundation of LLMs while post-training [29] further refines these models to align with human preferences, such as understanding human intents, following instructions, and facilitating multi-turn dialogues.For GLM-4, the alignment is mostly achieved with supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) [17].In SFT, we find that authentic human prompts and interactions instead of template-based or model-generated responses are vital to the alignment quality.While SFT largely aligns the base models with human preferences, RLHF can further help mitigate issues of response rejection, safety, mixture of bilingual tokens generated, and multi-turn coherence among others.</p>
<p>For the first generation of our models (ChatGLM-6B and ChatGLM-130B), the prompt-response pairs were mostly annotated by the model developers.For later models, the alignment data is a combination of in-house annotation and proprietary data acquired from third parties, subject to strict quality control measures.Similar to existing practices [42], annotators are instructed to score model responses from several dimensions, including safety, factuality, relevance, helpfulness, and human preferences.</p>
<p>ChatGLM Techniques.Throughout the development of ChatGLM, we have introduced and will publish techniques that are used to enhance its performance.</p>
<p>• Emergent Abilities of LLMs [12]: We examined the relationship between pre-training loss and performance on downstream tasks and found that with the same pre-training loss, LLMs of different model sizes and training tokens generate the same downtream performance.We also found that on some tasks (such as MMLU and GSM8K), the performance improves beyond random chance only when the pre-training loss falls below a certain threshold.We thus redefine emergent abilities as those exhibited by models with lower pre-training losses [12].• LongAlign [1]: To extend LLMs' context window size, we proposed LongAlign-a comprehensive recipe for long context alignment.It enables GLM-4 to process long context texts (up to 128K tokens) with performance comparable to that of Claude 2 and GPT-4 Turbo (1106).• ChatGLM-Math [48]: To improve math problem solving in LLMs, we introduced ChatGLM-Math that leverages self-critique rather than external models or manual annotations for data selection.• ChatGLM-RLHF [17]: To align LLMs with human feedback, we introduced ChatGLM-RLHFour practices of applying PPO and DPO into LLMs.• Self-Contrast [24]: To avoid the need for expensive human preference feedback data, we developed a feedback-free alignment strategy Self-Contrast.It utilizes the target LLM to self-generate massive negative samples for its RLHF alignment.• AgentTuning [52]: To improve LLMs' agent capabilities, we developed the AgentTurning framework with the AgentInstruct instruction-tuning dataset that includes high-quality interaction trajectories between agents and environment.• APAR [21]: To improve the inference speed of LLMs for responses with hierarchical structures, we presented an auto-parallel auto-regressive (APAR) generation approach.It leverages instruct tuning to train LLMs to plan their (parallel) generation process and execute APAR generation.• Benchmarks: We also developed several open LLM benchmarks, including AgentBench [25] for evaluating LLMs as agents, LongBench [2] for evaluating the long context handling performance of LLMs, AlignBench [1] to measure the alignment quality of ChatGLM with Chinese language content, HumanEval-X [58] to evaluate HumanEval [4] problems in programming languages beyond Python, as well as NaturalCodeBench (NCB) to measure models' capacities to solve practical programming tasks.</p>
<p>GLM-4 All Tools.The latest ChatGLM models are GLM-4 and GLM-4 All Tools, both of which were trained and aligned by using the techniques above.GLM-4 All Tools is a model version further aligned to support intelligent agents and related tasks.It is trained to autonomously understand user intent, plan complex instructions, and call one or multiple tools (e.g., web browser, Python interpreter, and the text-to-image model) to complete complex tasks.Figure 4 presents the overall pipeline of the GLM-4 All Tools system.When a user issues a complex request, the model analyzes the task and plan the problem-solving process step by step.If it determines that it cannot complete the task independently, it will sequentially call one or multiple external tools, utilizing their intermediate feedback and results to help solve the task.</p>
<p>Built on the GLM-4's all-tools capabilities, we also developed the GLMs application platform that allows users to create and customize their own agents for specific tasks.The GLMs support not only the embedded Python interpreter, web browser, text-to-image model but also user-defined functions, APIs, and external knowledge bases to more effectively address user needs.</p>
<p>GLM-4 Capabilities</p>
<p>We examine the capabilities of the GLM-4 model from diverse perspectives, including the base capacity on academic benchmarks, code problem-solving, agent abilities in English, and instruction following, long context for both Chinese and English, as well as alignment in Chinese.As mentioned, GLM-4 was pre-trained mostly in Chinese and English and aligned predominantly to Chinese.In this section, we report results primarily for the latest GLM-4 version, i.e., GLM-4 (0520) and GLM-4-Air (0605), as GLM-4 (0520) is slightly better than its original 0116 version across the evaluated benchmarks.During evaluation, both GLM-4 and GLM-4-Air are deployed with BFloat16 precision.</p>
<p>For baselines, we present results for GPT-4 (0603), GPT-4 Turbo (1106, 2024-04-09), Claude 2, Claude 3 Opus, and Gemini 1.5 Pro, all of which were extracted from the corresponding technical reports or tested through their public APIs.</p>
<p>Overall, GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus) over the standard benchmarks, as well as instruction following, long context, code problemsolving, and agent abilities in English environment.For Chinese alignment, it generates strong performance against SOTA models across various domains, such as fundamental language ability, advanced Chinese understanding, professional knowledge, and open-ended question answering.In summary, GLM-4 is among the best in terms of Chinese language tasks.It also demonstrates comparable performance to GPT-4 and Claude 3 Opus in Chinese math and logic reasoning capabilities though it lags behind GPT-4 Turbo.</p>
<p>Evaluation of Academic Benchmarks</p>
<p>To evaluate the general performance of the base model, we select six commonly-used benchmarks spanning knowledge, math, reasoning, commonsense, and coding:</p>
<p>• MMLU [14]: Multi-choice questions collected from various examinations including mathematics, history, computer science, and more.We present all answers to the model and ask it to choose the letter of the answer.</p>
<p>• GSM8K [7]: 8,500 grade school math word problems (1,000 in the test set) that require the model to solve real-life situational problems using mathematical concepts.We use chain-of-thought prompting [46] for this benchmark.</p>
<p>• MATH: 12,500 challenging competition-level mathematics problems (5,000 in the test set).We use chain-of-thought prompting [46] for this benchmark.</p>
<p>• BBH [39]: A suite of 23 challenging BIG-Bench [37] tasks.We use chain-of-thought prompting [46] for this benchmark.</p>
<p>• GPQA [32]: A graduate-level multi-choice benchmark in biology, chemistry, and physics.</p>
<p>• HumanEval [4]: a coding benchmark that measures correctness of synthetic functions with automatic test-case checking.</p>
<p>We compare the performance of GLM-4 with the original GPT-4 [28].The results are shown in Table 2.We can observe that GLM-4 achieves 96.3% of GPT-4's accuracy on MMLU, and outperforms GPT-4 on other benchmarks.Overall, the base capacity of GLM-4 approaches that of GPT-4-Turbo and Claude 3 Opus.</p>
<p>Evaluation of Instruction Following</p>
<p>We assess the proficiency of GLM-4 in following instructions with the recently-introduced IFEval dataset [62].The dataset comprises 541 prompts derived from 25 distinct instructions that are verifiable through explicit criteria (e.g., "end your email with: P.S.I do like the cake" can be verified via string matching).We adhere to the methodologies outlined by [62] to calculate prompt-level and instruction-level accuracy in both strict mode and loose mode.To further evaluate the model's performance on following instructions in Chinese, we translate the original prompts into Chinese, omitted instructions that are not applicable in Chinese (such as capitalization), and adjust the scoring scripts to accommodate Chinese data.In loose mode, GLM-4 matches instruction-level accuracy achieved by GPT-4 Turbo in both English and Chinese.In strict mode, GLM-4 achieves 99.0% and 98.6% of instruction-level accuracy of GPT-4 Turbo (2024-04-09) in English and Chinese, respectively.</p>
<p>Evaluation of Alignment</p>
<p>AlignBench [23] provides an automatic LLMs-as-Judge method to benchmark the alignment of LLMs in Chinese context.It consists 683 queries spanning 8 different categories, and evaluates model responses using a GPT-4 based multidimensional rule-calibrated pointwise reference-based scoring method.We evaluate on AlignBench-v1.1, which more carefully improves the reference generation quality, especially by complementing human-collected evidences from webpages with urls for knowledge-related questions that takes up 66.5% of total queries.On this version, almost all LLMs achieve lower scores than they do in the previous AlignBench.Results are shown in Table 4. GLM-4 outperforms GPT-4 Turbo, Claude 3 Opus, and Gemini 1.5 Pro in general, achieves the highest overall score among the baselines.Especially on Chinese Logic Reasoning and Language Understanding tasks, GLM-4 significantly outperforms all other powerful models.These results demonstrate its strong grasping of Chinese language and knowledge.</p>
<p>The current performance gap between GLM-4 and GPT-4 Turbo (2024-04-09) mostly lies in the Mathematics dimension.We have been employing techniques introduced in ChatGLM-Math [48] such as self-critique to continuously enhance GLM models' math reasoning capabilities.</p>
<p>Evaluation of Long Context Handling Abilities</p>
<p>To assess the performance of GLM-4 on long text tasks, we carry out evaluations on LongBench-Chat [1], a benchmark set with context lengths ranging from 10-100k, encompassing a wide range of long text scenarios frequently utilized by users, such as document Q&amp;A, summarization, and coding.In our quest to provide a more detailed comparison against the performance of GLM-4 in different languages, we also segregate LongBench-Chat according to language.This yields two distinct portions: Chinese and English.We therefore report the results for both segments separately, offering a fine-grained overview of GLM-4's cross-linguistic capabilities.</p>
<p>Regarding the specific evaluation settings, we score the outputs of each model based on GPT-4, adopting a few-shot strategy within LongBench-Chat.Moreover, given our objective to minimize score variations and to reach a more reliable statistical conclusion, we repeated evaluations multiple times.Subsequently, we report the average from these multiple evaluations in Table 5 to ensure that the final performance metric reflects a thorough understanding of how GLM-4 behaves under diverse conditions.And the results clearly suggested that the performance of GLM-4 aligns with that of GPT-4 Turbo and Claude 3 Opus on English prompts, and it outperforms the best of them on Chinese prompts.</p>
<p>Evaluation of Coding Abilities on Real-world User Prompts</p>
<p>While HumanEval [4] has been widely adopted for evaluating LLMs' code generation, most of its problems are about introductory algorithms.However, in practice, users ask complicated questions to complete their daily work, whose difficulty is usually far beyond the scope of HumanEval.</p>
<p>Additionally, previous works have reported HumanEval-contaminated training data [28; 19; 50] in own or other LLMs, making the results on HumanEval relatively less trustful than before.</p>
<p>As a result, beside HumanEval we evaluate GLM-4 on NaturalCodeBench (NCB) [55], a challenging bilingual coding benchmark derived from real user prompts to mirror the complexity of real-world coding tasks.As shown in Table 6, GLM-4 has a close coding performance to Claude 3 Opus in practical scenarios.While there is still some gaps to GPT-4 models, considering GLM-4 bilingually balanced nature, there is quite much potential to improve its performance on NCB via better training strategies and data curation in our following iterations.</p>
<p>Evaluation of Function Call</p>
<p>To evaluate the performance of GLM models on function call, we carry out evaluations on Berkeley Function Call Leaderboard [49], a benchmark with 2k question-function-answer pairs.The benchmark evaluates model's ability on calling functions in three categories: evaluation by Abstract Syntax Tree (AST), evaluation by executing APIs, and relevance detection.The first category compares the model output functions against function documents and possible answers with AST analysis.The second category checks for response correctness by executing the generated function calls.Relevance detection evaluates the model's capacity on recognizing functions that are not suitable to address the user's question.The results are shown in Table 7.We can observe that the function-call capability of GLM-4 (0520) aligns with that of GPT-4 Turbo (2024-04-09), while GLM-4-9B-Chat significantly outperforms Llama-3-8B-Instruct.Another observation is that the overall accuracy does not improve with model sizes, while GLM-4-9B-Chat can even outperform GLM-4-Air.On the other hand, we observe that the performance on execution summary, which evaluates the execution results of real-world APIs, improves smoothly with model size.</p>
<p>Evaluation of Agent Abilities</p>
<p>It is widely observed that LLMs are capable to serve as intelligent agents in versatile environments and contexts [30; 51], known as LLMs-as-Agents [25].As a result, we evaluate GLM-4 together with other comparison LLMs on AgentBench [25], a comprehensive agentic benchmark for text-based LLMs across an array of practical environments, including code-based, game-based, and web-based contexts.Specifically, we evaluate on 7 out of 8 AgentBench environments except for Digital Card Game, which is too time-consuming to interact with.Overall scores are calculated using the original per-dataset weights provided in AgentBench [25].The results are presented in Table 8.As it shows, GLM-4 models present quite impressive performance on agent tasks, with the GLM-4-Air's comparable and GLM-4's outperforming results to GPT-4 Turbo and Claude 3 Opus.In terms of specific environments, we find GLM-4 series performed especially well on Database, House-Holding, and Web Shopping tasks, while still demonstrating a gap to GPT-4 series on Operating System, Knowledge Graph, and Lateral Thinking Puzzles.The gap suggests that there is still room for GLM-4 to improve its performance on code-related agentic tasks and highly interactive language tasks.</p>
<p>Evaluation of All Tools</p>
<p>GLM-4 is further aligned to support intelligent agents and user-configured GLMs functionalities on https://chatglm.cn,and the resultant model is GLM-4 All Tools.As mentioned, GLM-4 All Tools can complete complex tasks by autonomously understanding user intent, planing step-by-step instructions, and calling multiple tools, including web browser, Python interpreter, and the text-toimage model (e.g., CogView3 [59].Table 9 shows that GLM-4 All Tools (Web) achieved similar performance on Python interpreter for solving math problems, browser for information seeking, compared to ChatGPT-4 (Web), respectively.</p>
<p>Safety and Risks</p>
<p>We are committed to ensuring that GLM-4 operates as a safe, responsible, and unbiased model.In addition to addressing common ethical and fairness concerns, we carefully assess and mitigate potential harms that the model may pose to users in real-world scenarios.Risk Mitigation.We carefully cleaned data in the pre-training stage by removing text containing sensitive keywords and web pages from a pre-defined blacklist.In the alignment phase, we evaluate each training sample for safety and remove any that pose potential risks.Harmlessness is also an important criteria for preference alignment when comparing multiple model outputs.</p>
<p>We have a red team that constantly challenges the model with tricky questions that tend to cause unsafe answers.We collect all harmful question-answer pairs from GLM-4 and improve them with human annotations for further model alignment.</p>
<p>Safety Evaluation.We evaluate the GLM-4 model on the SafetyBench [56], which assesses each model from 7 dimensions: Ethics and Morality (unethical behaviors), Illegal Activities (basic knowledge of law), Mental Health (adverse impacts on mental health), Offensiveness (offensive behaviors), Physical Health (dangerous behaviors that can cause physical harms), Privacy and Property (privacy breach or property loss), Unfairness and Bias.We evaluate different models on the Chinese subset of SafetyBench, which is created by removing highly sensitive questions that tend to be censored, to mitigate interference from different API safety policies.</p>
<p>Table 10 shows the safety results of GLM-4 and SOTA models.On most dimensions GLM-4 (0520) shows competitive safety performance, and overall it achieves comparable performance with Claude 3 Opus.GLM-4 slightly falls behind the GPT-4 family, especially on the Physical Health dimension, which demands robust common sense knowledge about the physical world to avoid potential risks.More efforts have been put into this direction to develop a more capable and safe GLM model.</p>
<p>Conclusion</p>
<p>In this report, we introduce the ChatGLM family of large language models from GLM-130B to GLM-4 (All Tools).Over the past one and half years, we have made great progress in understanding various perspectives of large language models from our first-hand experiences.With the development of each model generation, the team has learned and applied more effective and efficient strategies for both model pre-training and alignment.The recent ChatGLM models-GLM-4 (0116, 0520), GLM-4-Air (0605), and GLM-4 All Tools-demonstrate significant advancements in understanding and executing complex tasks by autonomously employing external tools and functions.These GLM-4 models have achieved performance on par with, and in some cases surpassing, state-of-the-art models such as GPT-4 Turbo, Claude 3 Opus, and Gemini 1.5 Pro, particularly in handling tasks relevant to the Chinese language.In addition, we are committed to promoting accessibility and safety of LLMs through open releasing of our model weights and techniques developed throughout this journey.Our open models, including language, code, and vision models, have attracted over 10 million downloads on Hugging Face in the year 2023 alone.Currently, we are working on more capable models with everything we have learned to date.In the future, we will continue democratizing cutting-edge LLM technologies through open sourcing, and push the boundary of model capabilities towards the mission of teaching machines to think like humans.</p>
<p>2 3Figure 2 :
22
Figure 2: An Illustrative Example of GLM-4 All Tools.</p>
<p>Figure 3 :
3
Figure 3: From GLM-130B to ChatGLM to ChatGLM2/3 to GLM-4 All Tools.</p>
<p>Figure 4 :
4
Figure 4: The overall pipeline of GLM-4 All Tools and customized GLMs (agents).</p>
<p>Table 1 :
1
Performance of Open ChatGLM-6B, ChatGLM2-6B, ChatGLM3-6B, and GLM-4-9B.
Language DatasetChatGLM-6B ChatGLM2-6B ChatGLM3-6B-Base GLM-4-9B(2023-03-14)(2023-06-25)(2023-10-27) (2024-06-05)GSM8K1.525.972.384.0MATH3.16.925.730.4EnglishBBH MMLU0.0 25.229.2 45.266.1 61.476.3 74.7GPQA--26.834.3HumanEval0.09.858.570.1BoolQ51.879.087.989.6CommonSenseQA20.565.486.590.7HellaSwag30.457.079.782.6PIQA65.769.680.179.1DROP3.925.670.977.2C-Eval23.751.769.077.1ChineseCMMLU25.350.067.575.1GAOKAO-Bench26.846.467.374.5C335.158.673.977.2</p>
<p>Table 2 :
2
GLM-4 performance on academic benchmarks.
ModelMMLU GSM8K MATH BBH GPQA HumanEvalGPT-4 (0314)86.492.052.983.135.767.0GPT-4 Turbo (1106)84.795.764.388.342.583.7GPT-4 Turbo (2024-04-09)86.795.673.488.249.388.2Claude 3 Opus86.895.060.186.850.484.9Gemini 1.5 Pro85.990.867.789.246.284.1GLM-4-9B-Chat72.479.650.676.328.871.8GLM-4-Air (0605)81.990.957.980.438.475.7GLM-4 (0116)81.587.647.982.335.772.0GLM-4 (0520)83.393.361.384.739.978.5</p>
<p>Table 3 :
3
[62]4 performance on IFEval[62], an LLM instruction following benchmark.'L' stands for 'Loose' and 'S' stands for 'Strict'.'P' stands for 'Prompt' and 'I' stands for 'Instruction'.
ModelEnglishChineseL-P S-PL-IS-IL-P S-PL-IS-IGPT-4 (0613)79.5 77.1 85.5 83.7 72.4 68.9 80.0 75.7GPT-4 Turbo (1106)79.1 75.4 85.1 82.4 74.3 69.1 80.8 76.5GPT-4 Turbo (2024-04-09) 84.5 81.2 88.7 85.9 79.3 72.6 84.2 79.1Claude 275.0 58.0 81.7 67.7 57.1 46.5 64.9 55.1Claude 3 Opus90.6 85.5 93.7 90.0 78.3 73.3 84.3 80.4GLM-4-9B-Chat73.0 69.0 80.3 77.2 73.0 69.0 80.3 77.2GLM-4-Air (0605)80.4 75.2 86.1 82.3 79.3 71.2 84.0 77.3GLM-4 (0520)83.7 79.1 88.7 85.0 79.7 71.9 84.2 78.0</p>
<p>Table 4 :
4
[23]4 performance on AlignBench[23], an LLM benchmark for alignment in Chinese.
ModelMath Logic Language Chinese QA Writing Role Play Professional OverallGPT-4 (0613)7.547.177.827.027.397.678.207.297.46GPT-4 Turbo (1106)7.857.667.907.228.248.538.467.957.90GPT-4 Turbo (2024-04-09) 8.327.677.607.578.377.758.188.598.00Claude 26.395.856.755.726.685.876.866.566.26Claude 3 Opus7.277.117.947.718.217.617.738.027.53Gemini 1.5 Pro7.077.777.317.228.557.837.798.527.47GLM-4-9B-Chat7.006.016.697.267.977.598.107.527.01GLM-4-Air (0605)7.696.957.538.007.908.018.358.097.65GLM-4 (0116)7.207.207.608.198.457.888.058.567.66GLM-4 (0520)7.897.958.007.868.118.048.068.478.00</p>
<p>Table 5 :
5
[2]-4 performance on LongBench-Chat[2].
ModelEnglish ChineseGPT-4 Turbo (1106)87.271.4GPT-4 Turbo (2024-04-09)85.082.1Claude 281.376.2Claude 3 Opus87.782.7GLM-4-9B-Chat76.879.0GLM-4-Air (0605)82.481.0GLM-4 (0520)87.384.0</p>
<p>Table 6 :
6
[55]4 performance on NaturalCodeBench (NCB)[55], a benchmark with real coding prompts in two programming languages (Python and Java) for English and Chinese.
ModelPython (en) Java (en) Python (zh) Java (zh) OverallGPT-4 (0613)55.751.153.451.152.8GPT-4 Turbo (1106)51.955.047.351.951.5GPT-4 Turbo (2024-04-09)57.552.353.152.353.8Claude 234.436.633.632.834.4Claude 3 Opus48.948.945.050.448.3Gemini 1.5 Pro45.039.741.543.142.3GLM-4-9B-Chat33.929.830.834.432.2GLM-4-Air (0605)40.839.743.139.740.8GLM-4 (0520)51.642.845.448.947.1</p>
<p>Table 7 :
7
GLM performance on the Berkeley Function Call Leaderboard.
ModelAST Summary Exec Summary Relevance OverallLlama-3-8B-Instruct59.2570.0145.8358.88GPT-4 Turbo (2024-04-09)82.1478.6188.7581.24GPT-4o (2024-05-13)85.2380.3781.2582.94ChatGLM3-6B62.1869.785.4257.88GLM-4-9B-Chat80.2684.4087.9281.00GLM-4-Air (0605)84.3485.9368.3380.94GLM-4 (0520)82.5987.7884.1781.76</p>
<p>Table 8 :
8
[25]4 performance on AgentBench[25].
OperatingDataBaseKnowledgeLateral ThinkingHouseWebWebOverallSystemGraphPuzzlesHoldingShoppingBrowsingGPT-4 (0613)42.432.058.816.678.061.129.03.69GPT-4 Turbo (1106)40.352.754.017.770.052.830.03.77GPT-4 Turbo (2024-04-09)41.046.753.219.472.055.119.03.68Claude 218.127.341.38.454.061.40.02.03Claude 3 Opus23.655.053.420.070.048.528.03.62GLM-4-Air (0605)31.951.053.812.378.069.230.03.58GLM-4 (0520)36.852.751.415.382.068.329.03.79</p>
<p>Table 9 :
9
Performance of GLM-4 All Tools.
GLM-4 All ToolsGPT-4(Web, 0116)(Web, 0110)GSM8K91.5992.72Python InterpreterMATH63.6065.00Math23K88.5088.40BrowserInformation Seeking78.0867.12</p>
<p>Table 10 :
10
[56]4 performance on SafetyBench[56], compared to GPT-4 models and Claude 3 Opus.
Ethics &amp;IllegalMentalOffens-PhysicalPrivacy &amp;UnfairnessOverallMoralityActivitiesHealthivenessHealthProperty&amp; BiasGPT-4 (0613)92.793.393.087.796.791.373.389.7GPT-4 Turbo (1106)91.092.093.086.092.088.774.388.1GPT-4 Turbo (2024-04-09) 90.391.391.785.392.089.375.087.9Claude 3 Opus92.791.792.786.394.788.766.087.5GLM-4 (0520)92.391.393.386.392.388.666.087.2
Acknowledgement.We would like to thank all the data annotators, infra operating staffs, collaborators, and partners as well as everyone at Zhipu AI and Tsinghua University not explicitly mentioned in the report who have provided support, feedback, and contributed to ChatGLM.We would also like to thank Yuxuan Zhang and Wei Jia from Zhipu AI as well as the teams at Hugging Face, ModelScope, WiseModel, and others for their help on the open-sourcing efforts of the GLM family of models.
Longalign: A recipe for long context alignment of large language models. Y Bai, X Lv, J Zhang, Y He, J Qi, L Hou, J Tang, Y Dong, J Li, 2024</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Y Bai, X Lv, J Zhang, H Lyu, J Tang, Z Huang, Z Du, X Liu, A Zeng, L Hou, Y Dong, J Tang, J Li, 2023</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc2020</p>
<p>. M Chen, J Tworek, H Jun, Q Yuan, H P De Oliveira Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, A Ray, R Puri, G Krueger, M Petrov, H Khlaaf, G Sastry, P Mishkin, B Chan, S Gray, N Ryder, M Pavlov, A Power, L Kaiser, M Bavarian, C Winter, P Tillet, F P Such, D Cummings, M Plappert, F Chantzis, E Barnes, A Herbert-Voss, W H Guss, A Nichol, A Paino, N Tezak, J Tang, I Babuschkin, S Balaji, S Jain, W Saunders, C Hesse, A N Carr, J Leike, J Achiam, V Misra, E Morikawa, A Radford, M Knight, M Brundage, M Murati, K Mayer, P Welinder, B Mcgrew, D Amodei, S Mccandlish, I Sutskever, W Zaremba, 2021Evaluating large language models trained on code. CoRR, abs/2107.03374</p>
<p>Extending context window of large language models via positional interpolation. S Chen, S Wong, L Chen, Y Tian, arXiv:2306.155952023arXiv preprint</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, C Hesse, J Schulman, CoRR, abs/2110.141682021</p>
<p>Flashattention: Fast and memory-efficient exact attention with io-awareness. T Dao, D Fu, S Ermon, A Rudra, C Ré, Advances in Neural Information Processing Systems. 202235</p>
<p>Cogview: Mastering text-to-image generation via transformers. M Ding, Z Yang, W Hong, W Zheng, C Zhou, D Yin, J Lin, X Zou, Z Shao, H Yang, J Tang, 2021</p>
<p>Cogview2: Faster and better text-to-image generation via hierarchical transformers. M Ding, W Zheng, W Hong, J Tang, Advances in Neural Information Processing Systems. 202235</p>
<p>Glm: General language model pretraining with autoregressive blank infilling. Z Du, Y Qian, X Liu, M Ding, J Qiu, Z Yang, J Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Understanding emergent abilities of language models from the loss perspective. Z Du, A Zeng, Y Dong, J Tang, 2024</p>
<p>Chatglm-6b: An open bilingual dialogue language model. T Glm , 2023</p>
<p>Measuring massive multitask language understanding. D Hendrycks, C Burns, S Basart, A Zou, M Mazeika, D Song, J Steinhardt, International Conference on Learning Representations. 2021</p>
<p>D Hendrycks, K Gimpel, arXiv:1606.08415Gaussian error linear units (gelus). 2016arXiv preprint</p>
<p>W Hong, W Wang, Q Lv, J Xu, W Yu, J Ji, Y Wang, Z Wang, Y Zhang, J Li, B Xu, Y Dong, M Ding, J Tang, Cogagent: A visual language model for gui agents. 2023</p>
<p>Z Hou, Y Niu, Z Du, X Zhang, X Liu, A Zeng, Q Zheng, M Huang, H Wang, J Tang, Y Dong, Chatglm-rlhf: Practices of aligning large language models with human feedback. 2024</p>
<p>H Lai, X Liu, I L Iong, S Yao, Y Chen, P Shen, H Yu, H Zhang, X Zhang, Y Dong, arXiv:2404.03648Bootstrap and reinforce a large language model-based web navigating agent. 2024arXiv preprint</p>
<p>Y Li, S Bubeck, R Eldan, A D Giorno, S Gunasekar, Y T Lee, Textbooks are all you need ii: phi-1.5 technical report. 2023</p>
<p>Holistic evaluation of language models. P Liang, R Bommasani, T Lee, D Tsipras, D Soylu, M Yasunaga, Y Zhang, D Narayanan, Y Wu, A Kumar, B Newman, B Yuan, B Yan, C Zhang, C Cosgrove, C D Manning, C Ré, D Acosta-Navas, D A Hudson, E Zelikman, E Durmus, F Ladhak, F Rong, H Ren, H Yao, J Wang, K Santhanam, L Orr, L Zheng, M Yuksekgonul, M Suzgun, N Kim, N Guha, N Chatterji, O Khattab, P Henderson, Q Huang, R Chi, S M Xie, S Santurkar, S Ganguli, T Hashimoto, T Icard, T Zhang, V Chaudhary, W Wang, X Li, Y Mai, Y Zhang, Y Koreeda, 2023</p>
<p>Apar: Llms can do auto-parallel auto-regressive decoding. M Liu, A Zeng, B Wang, P Zhang, J Tang, Y Dong, ArXiv, abs/2401.067612024</p>
<p>Webglm: Towards an efficient web-enhanced question answering system with human preferences. X Liu, H Lai, H Yu, Y Xu, A Zeng, Z Du, P Zhang, Y Dong, J Tang, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>X Liu, X Lei, S Wang, Y Huang, Z Feng, B Wen, J Cheng, P Ke, Y Xu, W L Tam, X Zhang, L Sun, H Wang, J Zhang, M Huang, Y Dong, J Tang, Alignbench: Benchmarking chinese alignment of large language models. 2023</p>
<p>Extensive self-contrast enables feedback-free language model alignment. X Liu, X Song, Y Dong, J Tang, 2024</p>
<p>. X Liu, H Yu, H Zhang, Y Xu, X Lei, H Lai, Y Gu, H Ding, K Men, K Yang, S Zhang, X Deng, A Zeng, Z Du, C Zhang, S Shen, T Zhang, Y Su, H Sun, M Huang, Y Dong, J Tang, 2023Agentbench: Evaluating llms as agents</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta. 2024</p>
<p>. Openai, Tiktoken, 2023</p>
<p>R Openai, arXivGpt-4 technical report. 2023</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Generative agents: Interactive simulacra of human behavior. J S Park, J O'brien, C J Cai, M R Morris, P Liang, M S Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Train short, test long: Attention with linear biases enables input length extrapolation. O Press, N Smith, M Lewis, International Conference on Learning Representations. 2022</p>
<p>GPQA: A graduate-level google-proof q&amp;a benchmark. D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, J Michael, S R Bowman, CoRR, abs/2311.120222023</p>
<p>T L Scao, A Fan, C Akiki, E Pavlick, S Ilić, D Hesslow, R Castagné, A S Luccioni, F Yvon, M Gallé, arXiv:2211.05100A 176b-parameter open-access multilingual language model. 2022arXiv preprint</p>
<p>Neural machine translation of rare words with subword units. R Sennrich, B Haddow, A Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Fast transformer decoding: One write-head is all you need. N Shazeer, arXiv:1911.021502019arXiv preprint</p>
<p>Glu variants improve transformer. N Shazeer, 2020</p>
<p>. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, A R Brown, A Santoro, A Gupta, A Garriga-Alonso, A Kluska, A Lewkowycz, A Agarwal, A Power, A Ray, A Warstadt, A W Kocurek, A Safaya, A Tazarv, A Xiang, A Parrish, A Nie, A Hussain, A Askell, A Dsouza, A Rahane, A S Iyer, A Andreassen, A Santilli, A Stuhlmüller, A M Dai, A La, A K Lampinen, A Zou, A Jiang, A Chen, A Vuong, A Gupta, A Gottardi, A Norelli, A Venkatesh, A Gholamidavoodi, A Tabassum, A Menezes, A Kirubarajan, A Mullokandov, A Sabharwal, A Herrick, A Efrat, A Erdem, A Karakas, 2022Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615</p>
<p>Roformer: Enhanced transformer with rotary position embedding. J Su, Y Lu, S Pan, A Murtadha, B Wen, Y Liu, arXiv:2104.098642021arXiv preprint</p>
<p>Challenging big-bench tasks and whether chain-of-thought can solve them. M Suzgun, N Scales, N Schärli, S Gehrmann, Y Tay, H W Chung, A Chowdhery, Q V Le, E H Chi, D Zhou, J Wei, Findings of the Association for Computational Linguistics: ACL 2023. A Rogers, J L Boyd-Graber, N Okazaki, Toronto, CanadaAssociation for Computational LinguisticsJuly 9-14, 2023. 2023</p>
<p>G Team, R Anil, S Borgeaud, Y Wu, J.-B Alayrac, J Yu, R Soricut, J Schalkwyk, A M Dai, A Hauth, K Millican, D Silver, S Petrov, M Johnson, I Antonoglou, J Schrittwieser, A Glaese, J Chen, E Pitler, T Lillicrap, A Lazaridou, O Firat, J Molloy, M Isard, P R Barham, T Hennigan, B Lee, F Viola, M Reynolds, Y Xu, R Doherty, E Collins, C Meyer, E Rutherford, E Moreira, K Ayoub, M Goel, G Tucker, E Piqueras, M Krikun, I Barr, N Savinov, I Danihelka, B Roelofs, A White, A Andreassen, T Von Glehn, L Yagati, M Kazemi, L Gonzalez, M Khalman, J Sygnowski, A Frechette, C Smith, L Culp, L Proleev, Y Luan, X Chen, J Lottes, N Schucher, F Lebron, A Rrustemi, N Clay, P Crone, T Kocisky, J Zhao, B Perz, D Yu, H Howard, A Bloniarz, J W Rae, H Lu, L Sifre, M Maggioni, F Alcober, D Garrette, M Barnes, S Thakoor, J Austin, G Barth-Maron, W Wong, R Joshi, R Chaabouni, D Fatiha, A Ahuja, R Liu, Y Li, S Cogan, J Chen, C Jia, C Gu, Q Zhang, J Grimstad, A J Hartman, M Chadwick, G S Tomar, X Garcia, E Senter, E Taropa, T S Pillai, J Devlin, M Laskin, D De Las Casas, D Valter, C Tao, L Blanco, A P Badia, D Reitter, M Chen, J Brennan, C Rivera, S Brin, S Iqbal, G Surita, J Labanowski, A Rao, S Winkler, E Parisotto, Y Gu, K Olszewska, Y Zhang, R Addanki, A Miech, A Louis, L E Shafey, D Teplyashin, G Brown, E Catt, N Attaluri, J Balaguer, J Xiang, P Wang, Z Ashwood, A Briukhov, A Webson, S Ganapathy, S Sanghavi, A Kannan, M.-W Chang, A Stjerngren, J Djolonga, Y Sun, A Bapna, M Aitchison, P Pejman, H Michalewski, T Yu, C Wang, J Love, J Ahn, D Bloxwich, K Han, P Humphreys, T Sellam, J Bradbury, V Godbole, S Samangooei, B Damoc, A Kaskasoli, S M R Arnold, V Vasudevan, S Agrawal, J Riesa, D Lepikhin, R Tanburn, S Srinivasan, H Lim, S Hodkinson, P Shyam, J Ferret, S Hand, A Garg, T L Paine, J Li, Y Li, M Giang, A Neitz, Z Abbas, S York, M Reid, E Cole, A Chowdhery, D Das, D Rogozińska, V Nikolaev, P Sprechmann, Z Nado, L Zilka, F Prost, L He, M Monteiro, G Mishra, C Welty, J Newlan, D Jia, M Allamanis, C H Hu, R De Liedekerke, J Gilmer, C Saroufim, S Rijhwani, S Hou, D Shrivastava, A Baddepudi, A Goldin, A Ozturel, A Cassirer, Y Xu, D Sohn, D Sachan, R K Amplayo, C Swanson, D Petrova, S Narayan, A Guez, S Brahma, J Landon, M Patel, R Zhao, K Villela, L Wang, W Jia, M Rahtz, M Giménez, L Yeung, H Lin, J Keeling, P Georgiev, D Mincu, B Wu, S Haykal, R Saputro, K Vodrahalli, J Qin, Z Cankara, A Sharma, N Fernando, W Hawkins, B Neyshabur, S Kim, A Hutter, P Agrawal, A Castro-Ros, G Van Den Driessche, T Wang, F Yang, S Chang, P Komarek, R Mcilroy, M Lučić, G Zhang, W Farhan, M Sharman, P Natsev, P Michel, Y Cheng, Y Bansal, S Qiao, K Cao, S Shakeri, C Butterfield, J Chung, P K Rubenstein, S Agrawal, A Mensch, K Soparkar, K Lenc, T Chung, A Pope, L Maggiore, J Kay, P Jhakra, S Wang, J Maynez, M Phuong, T Tobin, A Tacchetti, M Trebacz, K Robinson, Y Katariya, S Riedel, P Bailey, K Xiao, N Ghelani, L Aroyo, A Slone, N Houlsby, X Xiong, Z Yang, E Gribovskaya, J Adler, M Wirth, L Lee, M Li, T Kagohara, J Pavagadhi, S Bridgers, A Bortsova, S Ghemawat, Z Ahmed, T Liu, R Powell, V Bolina, M Iinuma, P Zablotskaia, J Besley, D.-W Chung, T Dozat, R Comanescu, X Si, J Greer, G Su, M Polacek, R L Kaufman, S Tokumine, H Hu, E Buchatskaya, Y Miao, M Elhawaty, A Siddhant, N Tomasev, J Xing, C Greer, H Miller, S Ashraf, A Roy, Z Zhang, A Ma, A Filos, M Besta, R Blevins, T Klimenko, C.-K Yeh, S Changpinyo, J Mu, O Chang, M Pajarskas, C Muir, V Cohen, C L Lan, K Haridasan, A Marathe, S Hansen, S Douglas, R Samuel, M Wang, S Austin, C Lan, J Jiang, J Chiu, J A Lorenzo, L L Sjösund, S Cevey, Z Gleicher, T Avrahami, A Boral, H Srinivasan, V Selo, R May, K Aisopos, L Hussenot, L B Soares, K Baumli, M B Chang, A Recasens, B Caine, A Pritzel, F Pavetic, F Pardo, A Gergely, J Frye, V Ramasesh, D Horgan, K Badola, N Kassner, S Roy, E Dyer, V Campos, A Tomala, Y Tang, D E Badawy, E White, B Mustafa, O Lang, A Jindal, S Vikram, Z Gong, S Caelles, R Hemsley, G Thornton, F Feng, W Stokowiec, C Zheng, P Thacker, Çaglar Ünlü, Z Zhang, M Saleh, J Svensson, M Bileschi, P Patil, A Anand, R Ring, K Tsihlas, A Vezer, M Selvi, T Shevlane, M Rodriguez, T Kwiatkowski, S Daruki, K Rong, A Dafoe, N Fitzgerald, K Gu-Lemberg, M Khan, L A Hendricks, M Pellat, V Feinberg, J Cobon-Kerr, T Sainath, M Rauh, S H Hashemi, R Ives, Y Hasson, Y Li, E Noland, Y Cao, N Byrd, L Hou, Q Wang, T Sottiaux, M Paganini, J.-B Lespiau, A Moufarek, S Hassan, K Shivakumar, J Van Amersfoort, A Mandhane, P Joshi, A Goyal, M Tung, A Brock, H Sheahan, V Misra, C Li, N Rakićević, M Dehghani, F Liu, S Mittal, J Oh, S Noury, E Sezener, F Huot, M Lamm, N D Cao, C Chen, G Elsayed, E Chi, M Mahdieh, I Tenney, N Hua, I Petrychenko, P Kane, D Scandinaro, R Jain, J Uesato, R Datta, A Sadovsky, O Bunyan, D Rabiej, S Wu, J Zhang, G Vasudevan, E Leurent, M Alnahlawi, I Georgescu, N Wei, I Zheng, B Chan, P G Rabinovitch, P Stanczyk, Y Zhang, D Steiner, S Naskar, M Azzam, M Johnson, A Paszke, C.-C Chiu, J S Elias, A Mohiuddin, F Muhammad, J Miao, A Lee, N Vieillard, S Potluri, J Park, E Davoodi, J Zhang, J Stanway, D Garmon, A Karmarkar, Z Dong, J Lee, A Kumar, L Zhou, J Evens, W Isaac, Z Chen, J Jia, A Levskaya, Z Zhu, C Gorgolewski, P Grabowski, Y Mao, A Magni, K Yao, J Snaider, N Casagrande, P Suganthan, E Palmer, G Irving, E Loper, M Faruqui, I Arkatkar, N Chen, I Shafran, M Fink, A Castaño, I Giannoumis, W Kim, M Rybiński, A Sreevatsa, J Prendki, D Soergel, A Goedeckemeyer, W Gierke, M Jafari, M Gaba, J Wiesner, D G Wright, Y Wei, H Vashisht, Y Kulizhskaya, J Hoover, M Le, L Li, C Iwuanyanwu, L Liu, K Ramirez, A Khorlin, A Cui, T Lin, M Georgiev, M Wu, R Aguilar, K Pallo, A Chakladar, A Repina, X Wu, T Van Der Weide, P Ponnapalli, C Kaplan, J Simsa, S Li, O Dousse, F Yang, J Piper, N Ie, M Lui, R Pasumarthi, N Lintz, A Vijayakumar, L N Thiet, D Andor, P Valenzuela, C Paduraru, D Peng, K Lee, S Zhang, S Greene, D D Nguyen, P Kurylowicz, S Velury, S Krause, C Hardin, L Dixon, L Janzer, K Choo, Z Feng, B Zhang, A Singhal, T Latkar, M Zhang, Q Le, E A Abellan, D Du, D Mckinnon, N Antropova, T Bolukbasi, O Keller, D Reid, D Finchelstein, M A Raad, R Crocker, P Hawkins, R Dadashi, C Gaffney, S Lall, K Franko, E Filonov, A Bulanova, R Leblond, V Yadav, S Chung, H Askham, L C Cobo, K Xu, F Fischer, J Xu, C Sorokin, C Alberti, C.-C Lin, C Evans, H Zhou, A Dimitriev, H Forbes, D Banarse, Z , P Narayana, J Li, S Fatehi, J Wieting, O Ajmeri, B Uria, T Zhu, Y Ko, L Knight, A Héliou, N Niu, S Gu, C Pang, D Tran, Y Li, N Levine, A Stolovich, N Kalb, R Santamaria-Fernandez, S Goenka, W Yustalim, R Strudel, A Elqursh, B Lakshminarayanan, C Deck, S Upadhyay, H Lee, M Dusenberry, Z Li, X Wang, K Levin, R Hoffmann, D Holtmann-Rice, O Bachem, S Yue, S Arora, E Malmi, D Mirylenka, Q Tan, C Koh, S H Yeganeh, S Põder, S Zheng, F Pongetti, M Tariq, Y Sun, L Ionita, M Seyedhosseini, P Tafti, R Kotikalapudi, Z Liu, A Gulati, J Liu, X Ye, B Chrzaszcz, L Wang, N Sethi, T Li, B Brown, S Singh, W Fan, A Parisi, J Stanton, C Kuang, V Koverkathu, C A Choquette-Choo, Y Li, T Lu, A Ittycheriah, P Shroff, P Sun, M Varadarajan, S Bahargam, R Willoughby, D Gaddy, I Dasgupta, G Desjardins, ; Y Sun, Y Zhao, S Lee, P Nayak, D Fritz, M R Vuyyuru, J Aslanides, N Vyas, M Wicke, X Ma, T Bilal, E Eltyshev, D Balle, N Martin, H Cate, J Manyika, K Amiri, Y Kim, X Xiong, K Kang, F Luisier, N Tripuraneni, D Madras, M Guo, A Waters, O Wang, J Ainslie, J Baldridge, H Zhang, G Pruthi, J Bauer, F Yang, R Mansour, J Gelman, Y Xu, G Polovets, J Liu, H Cai, W Chen, X Sheng, E Xue, S Ozair, A Yu, C Angermueller, X Li, W Wang, J Wiesinger, E Koukoumidis, Y Tian, A Iyer, M Gurumurthy, M Goldenson, P Shah, M Blake, H Yu, A Urbanowicz, J Palomaki, C Fernando, K Brooks, K Durden, H Mehta, N Momchev, E Rahimtoroghi, M Georgaki, A Raul, S Ruder, M Redshaw, J Lee, K Jalan, D Li, G Perng, B Hechtman, P Schuh, M Nasr, M Chen, K Milan, V Mikulik, T Strohman, J Franco, T Green, D Hassabis, K Kavukcuoglu, J Dean, O Vinyals, Gemini: A family of highly capable multimodal models. B Robenek, B Mittal, B Albrecht, A Shenoy, F Moiseev, H Jacobsson, A Ghaffarkhah, M Rivière, A Walton, C Crepy, A Parrish, Y Liu, Z Zhou, C Farabet, C Radebaugh, P Srinivasan, C Van Der Salm, A Fidjeland, S Scellato, E Latorre-Chimoto, H Klimczak-Plucińska, D Bridson, D De Cesare, T Hudson, P Mendolicchio, L Walker, A Morris, I Penchev, M Mauger, A Guseynov, A Reid, S Odoom, L Loher, V Cotruta, M Yenugula, D Grewe, A Petrushkina, T Duerig, A Sanchez, S Yadlowsky, A Shen, A Globerson, A Kurzrok, L Webb, S Dua, D Li, P Lahoti, S Bhupatiraju, D Hurt, H Qureshi, A Agarwal, T Shani, M Eyal, A Khare, S R Belle, L Wang, C Tekur, M S Kale, J Wei, R Sang, B Saeta, T Liechty, 2023</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>. H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, T Scialom, 2023Llama 2: Open foundation and fine-tuned chat models</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, 2023</p>
<p>Deepnet: Scaling transformers to 1,000 layers. H Wang, S Ma, L Dong, S Huang, D Zhang, F Wei, 2022</p>
<p>Cogvlm: Visual expert for pretrained language models. W Wang, Q Lv, W Yu, W Hong, J Qi, Y Wang, J Ji, Z Yang, L Zhao, X Song, J Xu, B Xu, J Li, Y Dong, M Ding, J Tang, 2023</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, New Orleans, LA, USA2022. November 28 -December 9, 2022, 20222022</p>
<p>Effective long-context scaling of foundation models. W Xiong, J Liu, I Molybog, H Zhang, P Bhargava, R Hou, L Martin, R Rungta, K A Sankararaman, B Oguz, arXiv:2309.160392023arXiv preprint</p>
<p>Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline. Y Xu, X Liu, X Liu, Z Hou, Y Li, X Zhang, Z Wang, A Zeng, Z Du, W Zhao, J Tang, Y Dong, 2024</p>
<p>Berkeley function calling leaderboard. F Yan, H Mao, C C , .-J Ji, T Zhang, S G Patil, I Stoica, J E Gonzalez, 2024</p>
<p>Rethinking benchmark and contamination for language models with rephrased samples. S Yang, W.-L Chiang, L Zheng, J E Gonzalez, I Stoica, arXiv:2311.048502023arXiv preprint</p>
<p>S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Agenttuning: Enabling generalized agent abilities for llms. A Zeng, M Liu, R Lu, B Wang, X Liu, Y Dong, J Tang, 2023</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.024142022arXiv preprint</p>
<p>Opt: Open pre-trained transformer language models. S Zhang, S Roller, N Goyal, M Artetxe, M Chen, S Chen, C Dewan, M Diab, X Li, X V Lin, arXiv:2205.010682022arXiv preprint</p>
<p>S Zhang, H Zhao, X Liu, Q Zheng, Z Qi, X Gu, X Zhang, Y Dong, J Tang, arXiv:2405.04520Naturalcodebench: Examining coding performance mismatch on humaneval and natural user prompts. 2024arXiv preprint</p>
<p>Safetybench: Evaluating the safety of large language models with multiple choice questions. Z Zhang, L Lei, L Wu, R Sun, Y Huang, C Long, X Liu, X Lei, J Tang, M Huang, arXiv:2309.070452023arXiv preprint</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x. Q Zheng, X Xia, X Zou, Y Dong, S Wang, Y Xue, Z Wang, L Shen, A Wang, Y Li, T Su, Z Yang, J Tang, 2023</p>
<p>Cogview3: Finer and faster text-to-image generation via relay diffusion. W Zheng, J Teng, Z Yang, W Wang, J Chen, X Gu, Y Dong, M Ding, J Tang, 2024</p>
<p>. C Zhou, P Liu, P Xu, S Iyer, J Sun, Y Mao, X Ma, A Efrat, P Yu, L Yu, S Zhang, G Ghosh, M Lewis, L Zettlemoyer, O Levy, Lima, 2023Less is more for alignment</p>
<p>J Zhou, Z Chen, D Wan, B Wen, Y Song, J Yu, Y Huang, L Peng, J Yang, X Xiao, arXiv:2311.16832Customizing chinese conversational ai characters with large language models. 2023arXiv preprint</p>
<p>Instructionfollowing evaluation for large language models. J Zhou, T Lu, S Mishra, S Brahma, S Basu, Y Luan, D Zhou, L Hou, arXiv:2311.079112023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
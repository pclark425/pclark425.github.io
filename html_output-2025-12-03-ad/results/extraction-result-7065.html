<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7065 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7065</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7065</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1" target="_blank">ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language</a></p>
                <p><strong>Paper Venue:</strong> Findings</p>
                <p><strong>Paper TL;DR:</strong> A generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them, and iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations).</p>
                <p><strong>Paper Abstract:</strong> Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proof(s) that support them. In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7065.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7065.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProofWriter (All-At-Once)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProofWriter — All-At-Once generative proof model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generative system that uses a seq2seq transformer to produce an answer and a full proof in one pass from a natural-language theory and question; trained to output linearized proofs and intermediate conclusions directly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProofWriter (All-At-Once)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Seq2seq generative model built on top of T5 (text-to-text transformer), trained to map "theory + question" to "answer + linearized proof" in natural language form with special encoding for rules, facts and intermediate conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-11B (primary reported), also evaluated with T5-large (770M)</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (T5) seq2seq; single-step all-at-once generation of full proof</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Variants of the RuleTaker-derived datasets (D0, D1, D2, D3, D5) in CWA and OWA forms; ParaRules for paraphrase generalization; Birds-Electricity testbeds; enumerative and abductive derivative datasets (D3-Enum, D5-Enum, D*-Ab).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Direct generative proof synthesis: model trained to output full proof tree (linearized) and answer in one sequence (no external symbolic search).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RuleTaker / D* (D0,D1,D2,D3,D5), Birds-Electricity, ParaRules; derived enumerative and abductive datasets (D3-Enum, D5-Enum, D3-Ab, D5-Ab).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Synthetic natural-language theories generated from Datalog programs with templated English facts and rules; tasks require deductive reasoning to depths up to 5 (D5) with closed-world (CWA) and open-world (OWA) variants; also includes hand-authored rulebases (Birds, Electricity) and paraphrased rules (ParaRules).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation (answer + exact proof), implication enumeration (all consequences), abduction (single-fact missing explanations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Answer accuracy; exact-proof match (FA/full accuracy); implication F1; implication set exact-match accuracy; abduction F1 and perfect-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On D5(orig) IID test: answer accuracy 99.2% (All-At-Once), exact-proof match 96.2% (Paper reports +9% absolute vs PRover). On ParaRules: proof correctness +3% vs PRover. Implication enumeration (trained on D3-Enum): D3+Enum F1=98.9 (All), exact set acc=92.5; on D5-Enum F1=94.5, exact set acc=44.6. Abduction: (reported for separate model) see Abduction entries.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to PRover baseline: +9% absolute exact-proof match on D5(orig) (ProofWriter 96.2% vs PRover 87.1%); on ParaRules ProofWriter +3% proof correctness; in enumeration and out-of-domain tasks All-At-Once underperforms the Iterative variant (see Iterative entry).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>All-At-Once generation can produce high-quality, human-readable proofs and achieves state-of-the-art exact-proof match on RuleTaker-style benchmarks when trained on matching-depth data; it is effective for IID settings and paraphrased language.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>All-At-Once does not generalize as well to proofs deeper than seen during training (poor generalization to unseen depths), and generated proofs may be post-hoc (not guaranteed to correspond to inference steps the model "actually took"); verification of intermediate steps is needed to assess faithfulness. Also can hit token/input-length limits for large theories.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7065.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7065.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProofWriter (Iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProofWriter — Iterative 1-step implication generator with forward-chaining</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant that trains a model to generate single 1-hop inferences (implication + 1-step proof) and then applies it iteratively (forward chaining), assembling multi-hop proofs from the generated 1-step fragments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ProofWriter (Iterative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same T5-based seq2seq architecture fine-tuned to generate single-hop implications and their 1-step proofs; at test time iteratively adds inferred facts back into context until exhaustion to perform exhaustive forward-chaining; assembled proofs are faithful to generated steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>T5-11B (primary reported), T5-large also evaluated</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer (T5) seq2seq used in an iterative forward-chaining loop (external control loop wraps the model).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Derivative iterative training examples built from RuleTaker Datasets (examples: C -> I1; C+I1 -> I2; ...), using D0-D3 theories for iterative training; same auxiliary enumerative and abductive datasets for respective tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generative single-step inference + exhaustive forward chaining; proofs assembled from generated 1-step fragments (faithful proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RuleTaker / D* (D0–D5), Birds-Electricity, ParaRules, D3-Enum/D5-Enum, D3-Ab/D5-Ab</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same as All-At-Once: NL Datalog-style theories, varying depths; iterative model naturally enumerates implications (forward-chaining) and produces verifiable proof steps.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation (faithful/multi-hop), implication enumeration, abduction (via separate training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-proof match (FA), implication F1/accuracy, abduction F1/accuracy, verified-proof rate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>OOD Birds-Electricity zero-shot: proof correctness = 97.0% (Iterative) vs PRover 80.5% and All-At-Once 84.5% (paper text). Enumeration: D3+Enum F1=99.8 (Iter), exact set acc=98.8; D5-Enum F1=99.5 (Iter), exact set acc=93.9. Abduction: D3-Ab F1=97.4 Acc=94.5; D5-Ab F1=97.3 Acc=93.5 (these abduction numbers come from the abductive model trained and evaluated in paper; iterative mechanism aids enumeration). Verified-proof rates: iterative proofs are always verified by construction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms All-At-Once on out-of-domain generalization and on enumerating implications (large margins in exact set accuracy on D5-Enum: Iterative ~93.9% vs All-At-Once 44.6%). Iterative also outperforms PRover on OOD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative 1-step generation plus forward-chaining generalizes substantially better to proofs deeper than seen during training and to out-of-domain rule language; assembled proofs are faithful (reflect model's actual generated steps).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Inefficient/unstructured: exhaustive forward-chaining proves everything before answering a particular query (computationally wasteful); risk of exceeding transformer token limit as the theory grows (token-limit constraint, e.g., 512 tokens), requiring retrieval or guided expansion for large theories; errors can accumulate across iterations (though single-step accuracy is high so accumulation is small).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7065.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7065.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PRover</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PRover: Proof generation for interpretable reasoning over rules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior state-of-the-art system for generating proofs over RuleTaker-style natural-language rules that uses classification to select nodes and an ILP (integer linear program) module to assemble a proof tree under consistency constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PRover: Proof generation for interpretable reasoning over rules</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PRover</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classifier-based approach that first predicts which facts/rules/connections should appear in a proof and then uses an ILP solver to assemble a consistent proof graph (post-hoc assembly).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Transformer-based classifier + ILP post-processing (neural classification followed by symbolic constrained assembly).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Originally trained/evaluated on RuleTaker Datasets (D5 and related).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Classification of proof components + ILP-constrained assembly (post-hoc proof construction).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Uses an Integer Linear Programming (ILP) module to enforce structural consistency and assemble the final proof graph from classifier outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RuleTaker D5 (orig) and related testbeds (ParaRules, Birds-Electricity) used as baselines</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Same Datalog-style NL reasoning datasets; used as the prior SOTA baseline for proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation (classify components then assemble proof graph)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Exact-proof match (FA/full accuracy), answer accuracy</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>On D5(orig) IID test: answer accuracy 99.3% (PRover) vs ProofWriter 99.2%; exact-proof match 87.1% (PRover) vs ProofWriter 96.2% (All-At-Once). On Birds-Electricity: PRover proof correctness 80.5% vs Iterative ProofWriter 97.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>PRover serves as baseline; ProofWriter reports large improvements in exact-proof match (e.g., +9% absolute on D5).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>First system to generate proof graphs in this NL reasoning setting but proofs can be imperfect and may not reflect model-internal decisions (post-hoc).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Proofs are assembled post-hoc and may not be faithful to the model's internal decision process; ILP assembly adds complexity; lower OOD generalization compared to iterative generative approaches reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7065.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7065.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (Text-to-Text Transfer Transformer) 11B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large text-to-text transformer model (11 billion parameters) used as the base seq2seq architecture for ProofWriter; fine-tuned to generate answers, proofs, implications and abductive facts in NL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-11B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (text-to-text) pretrained on large corpora, then fine-tuned in a text-to-text format for downstream tasks; used here as the backbone for both All-At-Once and Iterative ProofWriter variants.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B parameters</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Encoder-decoder Transformer (T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pretrained on web-scale corpora (original T5 pretraining), fine-tuned on RuleTaker D* derivatives (D3/D5, enumerative and abductive datasets) for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Fine-tuned generative transformer producing structured linearized proofs (either full-proof generation or single-step implication generation).</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Internally used for evaluation on RuleTaker-derived benchmarks (D3/D5, Birds-Electricity, ParaRules), enumerative and abductive datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>As above: synthetic NL theories and OOD hand-authored rulebases for zero-shot evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Seq2seq generation of proofs, implications, abductive facts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Same metrics as ProofWriter experiments (answer acc, exact-proof match, F1 for enumerations/abduction).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported as the primary model achieving the high scores in the paper (e.g., ProofWriter exact-proof match 96.2% on D5 when using T5-11B). Compared to T5-large, T5-11B yields higher proof-correctness (notably on deeper/unseen depths).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>T5-11B systematically slightly outperforms T5-large on proof correctness, especially at higher depths and in generalization (examples: All-At-Once proofs and Iterative proofs show higher exact-proof match with 11B).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large pre-trained seq2seq transformers (T5-11B) can be fine-tuned to perform near-perfect QA and high-quality proof generation over NL-encoded Datalog theories; scale helps for proof correctness and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Large model computational cost; smaller T5-large produces competitive answer accuracy but lower proof exact-match on deeper/unseen depths (scalability / resource constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7065.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7065.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-large (770M)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Smaller variant of T5 used to evaluate the sensitivity of results to model scale; generally performs slightly worse than T5-11B for proof correctness, especially at greater depths.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring the limits of transfer learning with a unified text-to-text transformer</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (smaller T5 checkpoint, ~770M parameters) fine-tuned similarly to the T5-11B based ProofWriter variants for ablation/scale studies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>770M parameters</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>Encoder-decoder Transformer (T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Same fine-tuning datasets as T5-11B but with smaller model capacity (RuleTaker-derived datasets, enumerative and abductive derivatives).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Generative seq2seq proof/implication/abduction generation with same formats as larger model.</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RuleTaker D3->D5 generalization experiments and other ProofWriter tasks</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used in paper to show effect of model scale on answer/proof accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Proof generation, implication enumeration, abduction (same tasks as main experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Answer accuracy; exact-proof match; F1/accuracy for enumeration/abduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>T5-large is generally competitive on answer accuracy but shows reduced proof exact-match vs T5-11B, especially at higher depths; e.g., All-At-Once evaluation (train on D3, eval on D5) shows proof correctness noticeably lower with T5-large (paper reports substantial drops at deeper depths), while Iterative variant differences are smaller but present (T5-11B slightly better).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>T5-large vs T5-11B: T5-11B gives higher proof correctness (particularly for All-At-Once on deeper/unseen proofs); T5-large is somewhat worse but still competitive.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Model scale improves proof exact-match scores and generalization to deeper proofs; even the smaller model can perform well on many cases but degrades more strongly on unseen-depth generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Lower proof exact-match on higher-depth / out-of-distribution proofs compared to T5-11B; less headroom for generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7065.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7065.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RuleTaker (D*)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RuleTaker-derived D* benchmarks (D0, D1, D2, D3, D5; CWA and OWA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Synthetic benchmark family derived from Datalog programs rendered into templated (and paraphrased) English facts and rules, designed to evaluate deductive reasoning in NL across increasing proof depths and negation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transformers as soft reasoners over language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RuleTaker / D* (benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not a model — a collection of datasets and tasks: questions over small NL theories with ground-truth answers and enumerated gold proofs/implications; includes CWA (negation-as-failure) and OWA (hard negation) variants, ParaRules paraphrase set, and Birds-Electricity hand-authored rulebases for OOD testing.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>RuleTaker D* (D0–D5), D*-Enum (enumeration), D*-Ab (abduction), ParaRules, Birds-Electricity</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Datasets contain theories (sets of facts and rules in English), questions requiring up to specified depths of inference, ground-truth proofs (many per question possible), and derivative enumeration/abduction annotations; used to evaluate strict logical deduction and proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>Strict logical deduction over NL: True/False/Unknown QA, proof generation (exact-match), implication enumeration, single-fact abduction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Answer accuracy; exact-proof match (FA); implication F1 and exact set accuracy; abduction F1 and perfect-match accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used throughout paper as core benchmark. Representative numbers: ProofWriter (All) exact-proof match on D5(orig) = 96.2% vs PRover 87.1%; Iterative yields strong enumeration and OOD transfer (e.g., D5-Enum exact-set acc Iterative 93.9% vs All 44.6). Abduction (D3-Ab): F1=97.4 Acc=94.5; D5-Ab F1=97.3 Acc=93.5.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>The paper compares model variants and PRover across these datasets; demonstrates that generative strategies (ProofWriter) improve exact-proof match and that iterative generation improves generalization and enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RuleTaker-style benchmarks are effective tests of strict logical reasoning in NL; generative transformer-based systems can achieve near-perfect QA and high exact-proof match with appropriate training; iterative single-step generation greatly aids generalization and enumeration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Datasets are synthetic / templated language (though ParaRules and Birds-Electricity test OOD); some original RuleTaker datasets required repairs for stratification and negation-handling (authors provide D*(CWA) fixes); scaling to larger, real-world theories remains constrained by token limits and forward-chaining inefficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language', 'publication_date_yy_mm': '2020-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as soft reasoners over language <em>(Rating: 2)</em></li>
                <li>PRover: Proof generation for interpretable reasoning over rules <em>(Rating: 2)</em></li>
                <li>LeapOfThought: Teaching pre-trained models to systematically reason over implicit knowledge <em>(Rating: 2)</em></li>
                <li>Generative language modeling for automated theorem proving <em>(Rating: 2)</em></li>
                <li>Measuring systematic generalization in neural proof generation with transformers <em>(Rating: 2)</em></li>
                <li>Learning a SAT solver from single-bit supervision <em>(Rating: 1)</em></li>
                <li>Language models as knowledge bases? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7065",
    "paper_id": "paper-87c45a908537ffe1d2ab71a5d609bd7b4efa4fe1",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "ProofWriter (All-At-Once)",
            "name_full": "ProofWriter — All-At-Once generative proof model",
            "brief_description": "A generative system that uses a seq2seq transformer to produce an answer and a full proof in one pass from a natural-language theory and question; trained to output linearized proofs and intermediate conclusions directly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ProofWriter (All-At-Once)",
            "model_description": "Seq2seq generative model built on top of T5 (text-to-text transformer), trained to map \"theory + question\" to \"answer + linearized proof\" in natural language form with special encoding for rules, facts and intermediate conclusions.",
            "model_size": "T5-11B (primary reported), also evaluated with T5-large (770M)",
            "architecture_type": "Transformer (T5) seq2seq; single-step all-at-once generation of full proof",
            "training_data": "Variants of the RuleTaker-derived datasets (D0, D1, D2, D3, D5) in CWA and OWA forms; ParaRules for paraphrase generalization; Birds-Electricity testbeds; enumerative and abductive derivative datasets (D3-Enum, D5-Enum, D*-Ab).",
            "reasoning_method": "Direct generative proof synthesis: model trained to output full proof tree (linearized) and answer in one sequence (no external symbolic search).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "RuleTaker / D* (D0,D1,D2,D3,D5), Birds-Electricity, ParaRules; derived enumerative and abductive datasets (D3-Enum, D5-Enum, D3-Ab, D5-Ab).",
            "benchmark_description": "Synthetic natural-language theories generated from Datalog programs with templated English facts and rules; tasks require deductive reasoning to depths up to 5 (D5) with closed-world (CWA) and open-world (OWA) variants; also includes hand-authored rulebases (Birds, Electricity) and paraphrased rules (ParaRules).",
            "task_type": "Proof generation (answer + exact proof), implication enumeration (all consequences), abduction (single-fact missing explanations).",
            "performance_metric": "Answer accuracy; exact-proof match (FA/full accuracy); implication F1; implication set exact-match accuracy; abduction F1 and perfect-match accuracy.",
            "performance_value": "On D5(orig) IID test: answer accuracy 99.2% (All-At-Once), exact-proof match 96.2% (Paper reports +9% absolute vs PRover). On ParaRules: proof correctness +3% vs PRover. Implication enumeration (trained on D3-Enum): D3+Enum F1=98.9 (All), exact set acc=92.5; on D5-Enum F1=94.5, exact set acc=44.6. Abduction: (reported for separate model) see Abduction entries.",
            "comparison_with_baseline": "Compared to PRover baseline: +9% absolute exact-proof match on D5(orig) (ProofWriter 96.2% vs PRover 87.1%); on ParaRules ProofWriter +3% proof correctness; in enumeration and out-of-domain tasks All-At-Once underperforms the Iterative variant (see Iterative entry).",
            "key_findings": "All-At-Once generation can produce high-quality, human-readable proofs and achieves state-of-the-art exact-proof match on RuleTaker-style benchmarks when trained on matching-depth data; it is effective for IID settings and paraphrased language.",
            "limitations": "All-At-Once does not generalize as well to proofs deeper than seen during training (poor generalization to unseen depths), and generated proofs may be post-hoc (not guaranteed to correspond to inference steps the model \"actually took\"); verification of intermediate steps is needed to assess faithfulness. Also can hit token/input-length limits for large theories.",
            "uuid": "e7065.0",
            "source_info": {
                "paper_title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "ProofWriter (Iterative)",
            "name_full": "ProofWriter — Iterative 1-step implication generator with forward-chaining",
            "brief_description": "A variant that trains a model to generate single 1-hop inferences (implication + 1-step proof) and then applies it iteratively (forward chaining), assembling multi-hop proofs from the generated 1-step fragments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ProofWriter (Iterative)",
            "model_description": "Same T5-based seq2seq architecture fine-tuned to generate single-hop implications and their 1-step proofs; at test time iteratively adds inferred facts back into context until exhaustion to perform exhaustive forward-chaining; assembled proofs are faithful to generated steps.",
            "model_size": "T5-11B (primary reported), T5-large also evaluated",
            "architecture_type": "Transformer (T5) seq2seq used in an iterative forward-chaining loop (external control loop wraps the model).",
            "training_data": "Derivative iterative training examples built from RuleTaker Datasets (examples: C -&gt; I1; C+I1 -&gt; I2; ...), using D0-D3 theories for iterative training; same auxiliary enumerative and abductive datasets for respective tasks.",
            "reasoning_method": "Generative single-step inference + exhaustive forward chaining; proofs assembled from generated 1-step fragments (faithful proofs).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "RuleTaker / D* (D0–D5), Birds-Electricity, ParaRules, D3-Enum/D5-Enum, D3-Ab/D5-Ab",
            "benchmark_description": "Same as All-At-Once: NL Datalog-style theories, varying depths; iterative model naturally enumerates implications (forward-chaining) and produces verifiable proof steps.",
            "task_type": "Proof generation (faithful/multi-hop), implication enumeration, abduction (via separate training).",
            "performance_metric": "Exact-proof match (FA), implication F1/accuracy, abduction F1/accuracy, verified-proof rate.",
            "performance_value": "OOD Birds-Electricity zero-shot: proof correctness = 97.0% (Iterative) vs PRover 80.5% and All-At-Once 84.5% (paper text). Enumeration: D3+Enum F1=99.8 (Iter), exact set acc=98.8; D5-Enum F1=99.5 (Iter), exact set acc=93.9. Abduction: D3-Ab F1=97.4 Acc=94.5; D5-Ab F1=97.3 Acc=93.5 (these abduction numbers come from the abductive model trained and evaluated in paper; iterative mechanism aids enumeration). Verified-proof rates: iterative proofs are always verified by construction.",
            "comparison_with_baseline": "Outperforms All-At-Once on out-of-domain generalization and on enumerating implications (large margins in exact set accuracy on D5-Enum: Iterative ~93.9% vs All-At-Once 44.6%). Iterative also outperforms PRover on OOD tasks.",
            "key_findings": "Iterative 1-step generation plus forward-chaining generalizes substantially better to proofs deeper than seen during training and to out-of-domain rule language; assembled proofs are faithful (reflect model's actual generated steps).",
            "limitations": "Inefficient/unstructured: exhaustive forward-chaining proves everything before answering a particular query (computationally wasteful); risk of exceeding transformer token limit as the theory grows (token-limit constraint, e.g., 512 tokens), requiring retrieval or guided expansion for large theories; errors can accumulate across iterations (though single-step accuracy is high so accumulation is small).",
            "uuid": "e7065.1",
            "source_info": {
                "paper_title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "PRover",
            "name_full": "PRover: Proof generation for interpretable reasoning over rules",
            "brief_description": "A prior state-of-the-art system for generating proofs over RuleTaker-style natural-language rules that uses classification to select nodes and an ILP (integer linear program) module to assemble a proof tree under consistency constraints.",
            "citation_title": "PRover: Proof generation for interpretable reasoning over rules",
            "mention_or_use": "use",
            "model_name": "PRover",
            "model_description": "Classifier-based approach that first predicts which facts/rules/connections should appear in a proof and then uses an ILP solver to assemble a consistent proof graph (post-hoc assembly).",
            "model_size": null,
            "architecture_type": "Transformer-based classifier + ILP post-processing (neural classification followed by symbolic constrained assembly).",
            "training_data": "Originally trained/evaluated on RuleTaker Datasets (D5 and related).",
            "reasoning_method": "Classification of proof components + ILP-constrained assembly (post-hoc proof construction).",
            "external_tool_used": true,
            "external_tool_description": "Uses an Integer Linear Programming (ILP) module to enforce structural consistency and assemble the final proof graph from classifier outputs.",
            "benchmark_name": "RuleTaker D5 (orig) and related testbeds (ParaRules, Birds-Electricity) used as baselines",
            "benchmark_description": "Same Datalog-style NL reasoning datasets; used as the prior SOTA baseline for proof generation.",
            "task_type": "Proof generation (classify components then assemble proof graph)",
            "performance_metric": "Exact-proof match (FA/full accuracy), answer accuracy",
            "performance_value": "On D5(orig) IID test: answer accuracy 99.3% (PRover) vs ProofWriter 99.2%; exact-proof match 87.1% (PRover) vs ProofWriter 96.2% (All-At-Once). On Birds-Electricity: PRover proof correctness 80.5% vs Iterative ProofWriter 97.0%.",
            "comparison_with_baseline": "PRover serves as baseline; ProofWriter reports large improvements in exact-proof match (e.g., +9% absolute on D5).",
            "key_findings": "First system to generate proof graphs in this NL reasoning setting but proofs can be imperfect and may not reflect model-internal decisions (post-hoc).",
            "limitations": "Proofs are assembled post-hoc and may not be faithful to the model's internal decision process; ILP assembly adds complexity; lower OOD generalization compared to iterative generative approaches reported in this paper.",
            "uuid": "e7065.2",
            "source_info": {
                "paper_title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "T5-11B",
            "name_full": "T5 (Text-to-Text Transfer Transformer) 11B",
            "brief_description": "Large text-to-text transformer model (11 billion parameters) used as the base seq2seq architecture for ProofWriter; fine-tuned to generate answers, proofs, implications and abductive facts in NL.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5-11B",
            "model_description": "Encoder-decoder Transformer (text-to-text) pretrained on large corpora, then fine-tuned in a text-to-text format for downstream tasks; used here as the backbone for both All-At-Once and Iterative ProofWriter variants.",
            "model_size": "11B parameters",
            "architecture_type": "Encoder-decoder Transformer (T5 family)",
            "training_data": "Pretrained on web-scale corpora (original T5 pretraining), fine-tuned on RuleTaker D* derivatives (D3/D5, enumerative and abductive datasets) for reasoning tasks.",
            "reasoning_method": "Fine-tuned generative transformer producing structured linearized proofs (either full-proof generation or single-step implication generation).",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "Internally used for evaluation on RuleTaker-derived benchmarks (D3/D5, Birds-Electricity, ParaRules), enumerative and abductive datasets.",
            "benchmark_description": "As above: synthetic NL theories and OOD hand-authored rulebases for zero-shot evaluation.",
            "task_type": "Seq2seq generation of proofs, implications, abductive facts.",
            "performance_metric": "Same metrics as ProofWriter experiments (answer acc, exact-proof match, F1 for enumerations/abduction).",
            "performance_value": "Reported as the primary model achieving the high scores in the paper (e.g., ProofWriter exact-proof match 96.2% on D5 when using T5-11B). Compared to T5-large, T5-11B yields higher proof-correctness (notably on deeper/unseen depths).",
            "comparison_with_baseline": "T5-11B systematically slightly outperforms T5-large on proof correctness, especially at higher depths and in generalization (examples: All-At-Once proofs and Iterative proofs show higher exact-proof match with 11B).",
            "key_findings": "Large pre-trained seq2seq transformers (T5-11B) can be fine-tuned to perform near-perfect QA and high-quality proof generation over NL-encoded Datalog theories; scale helps for proof correctness and generalization.",
            "limitations": "Large model computational cost; smaller T5-large produces competitive answer accuracy but lower proof exact-match on deeper/unseen depths (scalability / resource constraints).",
            "uuid": "e7065.3",
            "source_info": {
                "paper_title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "T5-large",
            "name_full": "T5-large (770M)",
            "brief_description": "Smaller variant of T5 used to evaluate the sensitivity of results to model scale; generally performs slightly worse than T5-11B for proof correctness, especially at greater depths.",
            "citation_title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "mention_or_use": "use",
            "model_name": "T5-large",
            "model_description": "Encoder-decoder Transformer (smaller T5 checkpoint, ~770M parameters) fine-tuned similarly to the T5-11B based ProofWriter variants for ablation/scale studies.",
            "model_size": "770M parameters",
            "architecture_type": "Encoder-decoder Transformer (T5 family)",
            "training_data": "Same fine-tuning datasets as T5-11B but with smaller model capacity (RuleTaker-derived datasets, enumerative and abductive derivatives).",
            "reasoning_method": "Generative seq2seq proof/implication/abduction generation with same formats as larger model.",
            "external_tool_used": false,
            "external_tool_description": null,
            "benchmark_name": "RuleTaker D3-&gt;D5 generalization experiments and other ProofWriter tasks",
            "benchmark_description": "Used in paper to show effect of model scale on answer/proof accuracy.",
            "task_type": "Proof generation, implication enumeration, abduction (same tasks as main experiments).",
            "performance_metric": "Answer accuracy; exact-proof match; F1/accuracy for enumeration/abduction.",
            "performance_value": "T5-large is generally competitive on answer accuracy but shows reduced proof exact-match vs T5-11B, especially at higher depths; e.g., All-At-Once evaluation (train on D3, eval on D5) shows proof correctness noticeably lower with T5-large (paper reports substantial drops at deeper depths), while Iterative variant differences are smaller but present (T5-11B slightly better).",
            "comparison_with_baseline": "T5-large vs T5-11B: T5-11B gives higher proof correctness (particularly for All-At-Once on deeper/unseen proofs); T5-large is somewhat worse but still competitive.",
            "key_findings": "Model scale improves proof exact-match scores and generalization to deeper proofs; even the smaller model can perform well on many cases but degrades more strongly on unseen-depth generalization.",
            "limitations": "Lower proof exact-match on higher-depth / out-of-distribution proofs compared to T5-11B; less headroom for generalization.",
            "uuid": "e7065.4",
            "source_info": {
                "paper_title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
                "publication_date_yy_mm": "2020-12"
            }
        },
        {
            "name_short": "RuleTaker (D*)",
            "name_full": "RuleTaker-derived D* benchmarks (D0, D1, D2, D3, D5; CWA and OWA variants)",
            "brief_description": "Synthetic benchmark family derived from Datalog programs rendered into templated (and paraphrased) English facts and rules, designed to evaluate deductive reasoning in NL across increasing proof depths and negation settings.",
            "citation_title": "Transformers as soft reasoners over language",
            "mention_or_use": "use",
            "model_name": "RuleTaker / D* (benchmark)",
            "model_description": "Not a model — a collection of datasets and tasks: questions over small NL theories with ground-truth answers and enumerated gold proofs/implications; includes CWA (negation-as-failure) and OWA (hard negation) variants, ParaRules paraphrase set, and Birds-Electricity hand-authored rulebases for OOD testing.",
            "model_size": null,
            "architecture_type": null,
            "training_data": null,
            "reasoning_method": null,
            "external_tool_used": null,
            "external_tool_description": null,
            "benchmark_name": "RuleTaker D* (D0–D5), D*-Enum (enumeration), D*-Ab (abduction), ParaRules, Birds-Electricity",
            "benchmark_description": "Datasets contain theories (sets of facts and rules in English), questions requiring up to specified depths of inference, ground-truth proofs (many per question possible), and derivative enumeration/abduction annotations; used to evaluate strict logical deduction and proof generation.",
            "task_type": "Strict logical deduction over NL: True/False/Unknown QA, proof generation (exact-match), implication enumeration, single-fact abduction.",
            "performance_metric": "Answer accuracy; exact-proof match (FA); implication F1 and exact set accuracy; abduction F1 and perfect-match accuracy.",
            "performance_value": "Used throughout paper as core benchmark. Representative numbers: ProofWriter (All) exact-proof match on D5(orig) = 96.2% vs PRover 87.1%; Iterative yields strong enumeration and OOD transfer (e.g., D5-Enum exact-set acc Iterative 93.9% vs All 44.6). Abduction (D3-Ab): F1=97.4 Acc=94.5; D5-Ab F1=97.3 Acc=93.5.",
            "comparison_with_baseline": "The paper compares model variants and PRover across these datasets; demonstrates that generative strategies (ProofWriter) improve exact-proof match and that iterative generation improves generalization and enumeration.",
            "key_findings": "RuleTaker-style benchmarks are effective tests of strict logical reasoning in NL; generative transformer-based systems can achieve near-perfect QA and high exact-proof match with appropriate training; iterative single-step generation greatly aids generalization and enumeration.",
            "limitations": "Datasets are synthetic / templated language (though ParaRules and Birds-Electricity test OOD); some original RuleTaker datasets required repairs for stratification and negation-handling (authors provide D*(CWA) fixes); scaling to larger, real-world theories remains constrained by token limits and forward-chaining inefficiency.",
            "uuid": "e7065.5",
            "source_info": {
                "paper_title": "ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language",
                "publication_date_yy_mm": "2020-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as soft reasoners over language",
            "rating": 2
        },
        {
            "paper_title": "PRover: Proof generation for interpretable reasoning over rules",
            "rating": 2
        },
        {
            "paper_title": "LeapOfThought: Teaching pre-trained models to systematically reason over implicit knowledge",
            "rating": 2
        },
        {
            "paper_title": "Generative language modeling for automated theorem proving",
            "rating": 2
        },
        {
            "paper_title": "Measuring systematic generalization in neural proof generation with transformers",
            "rating": 2
        },
        {
            "paper_title": "Learning a SAT solver from single-bit supervision",
            "rating": 1
        },
        {
            "paper_title": "Language models as knowledge bases?",
            "rating": 1
        }
    ],
    "cost": 0.017760249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language</h1>
<p>Oyvind Tafjord, Bhavana Dalvi Mishra, Peter Clark<br>Allen Institute for AI, Seattle, WA<br>{oyvindt, bhavanad, peterc}@allenai.org</p>
<h4>Abstract</h4>
<p>Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications. However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect. In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proofs that support them. In particular, iterating a 1 -step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by $+9 \%$ absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems. We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof. These results significantly improve the viability of neural methods for systematically reasoning over natural language. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>A fundamental goal for AI, dating back to its earliest years, is automated reasoning: the ability to draw valid conclusions from explicitly provided knowledge (McCarthy, 1959). However, approaches relying on expressing knowledge in a formal representation language have sometimes proved challenging (Musen and Van der Lei, 1988). Recent work on RuleTaker (Clark et al., 2020) demonstrated a modern approach to this goal, in which transformers emulate deductive reasoning</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Given facts, rules, and a question all expressed in natural language, ProofWriter answers the question and generates a proof of the answer.
over statements expressed in natural language, by reliably assigning true/false labels to candidate implications. However, simply assigning true/false labels is limiting. For practical purposes, systems should also generate proofs of those labels, so that their conclusions can be verified and a humanunderstandable rationale be produced.</p>
<p>Recent work on PRover, by Saha et al. (2020), provided first results towards this goal, assembling proofs by first classifying which facts, rules, and connections should be in the proof tree then using an Integer Linear Programming (ILP) module to enforce consistency constraints. However, the gen-</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: ProofWriter iteratively generates 1-step implications and their proofs, and adds implications back into into the context for deeper reasoning. The stepwise proof fragments are assembled into full proofs of N-hop conclusions.
erated proofs were imperfect, and there were no guarantees that the model "believed" the proofs that it was reciting, i.e., that its QA module would agree with the steps shown in the proof. In this paper, we adopt a different approach, based on generation rather than classification. Our system, ProofWriter, generates proofs such as that shown in Figure 1 by iteratively generating 1-hop inferences and their (simple) proofs, adding implications back into the context for deeper reasoning, and assembling more complex proofs from the 1-hop fragments (Figure 2). As the accuracy of 1-hop inference is highly reliable, the accuracy of deeper inference and their proofs is also high. This results in proofs that substantially exceed the earlier method's accuracy, and also reflect the model's internal decisions, rather than a post-hoc rationalization (i.e., is a "faithful" proof (Subramanian et al., 2020)).</p>
<p>The generative approach also affords two other new capabilities. First, ProofWriter generates implications that logically follow from a NL (natural language) theory, allowing enumeration of consequences (rather than only assigning truth values to pre-conjectured hypotheses). Second, we demonstrate (a constrained form of) abduction: Given a theory and an unprovable conclusion, identify a missing fact (if any) that allows the conclusion to be proved when added to the theory, plus its proof.</p>
<p>We evaluate our work on a collection of natural language reasoning datasets, including the Rule-</p>
<p>Taker datasets as well as several new variants. We achieve state-of-the-art results in proof generation, and strong new baselines for implication enumeration and abduction over natural language theories. Our contributions are thus:</p>
<ol>
<li>A new method for proof generation for logical reasoning over natural language, that obtains state-of-the-art results and is faithful to the model's internal decisions.</li>
<li>A method and baseline results for generating logical implications of statements in NL.</li>
<li>A method and baseline results for performing abduction over natural language statements.</li>
<li>New datasets to promote further research.</li>
</ol>
<p>These results significantly improve the viability of neural methods for formal reasoning over language.</p>
<h2>2 Related Work</h2>
<p>Our work builds on the RuleTaker line of research, in which transformers learn to emulate a deductive reasoning algorithm (Clark et al., 2020). Unlike other approaches to reasoning such as parsing to a formal language (Kamath and Das, 2019), implementing a reasoning algorithm with neural components (Weber et al., 2019; Rocktäschel and Riedel, 2017), or SAT solving (Selsam et al., 2019), these transformers emulate reasoning over language directly, bypassing a formal representation.</p>
<p>PRover (Saha et al., 2020), mentioned earlier, was the first system to also produce proofs in this context, although its post hoc approach meant that proofs did not necessarily represent the actual model decisions. Gontier et al. (2020) also explored the generation of answers and proofs, but in the context of rule induction with $(\approx 10)$ fixed rules to induce. In contrast, ProofWriter generates proofs from explicit NL rules (which may differ for each problem). Similarly, formal theorem proving has explored proving mathematical theorems from fixed, fundamental axioms, e.g., (Polu and Sutskever, 2020; Wang and Deng, 2020), while ProofWriter performs inference with differing sets of rules expressed in natural language.</p>
<p>Our work is also distinct from the large body of work on rationales and explanation. Work on rationales aims to identify sentences (or phrases) that caused a model to make a particular decision, but without an explanation of why that rationale led to the answer (the model's reasoning is opaque), e.g., (DeYoung et al., 2019; Narang et al., 2020). Similarly, work on explanations has sought to gen-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Proof (and Answer)</th>
<th style="text-align: left;">$C Q \rightarrow A P$</th>
<th style="text-align: left;">Given theory $C$ and hypothesis fact $Q$, determine $Q$ 's truth $A$ and proof $P$ (if any)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Enumeration</td>
<td style="text-align: left;">$C \rightarrow I_{1}, \ldots, I_{n}$</td>
<td style="text-align: left;">Given $C$, generate all implications $I_{i}$ that logically follow.</td>
</tr>
<tr>
<td style="text-align: left;">Abduction</td>
<td style="text-align: left;">$C Q \rightarrow f_{m}$</td>
<td style="text-align: left;">Given $C$ and an unprovable fact $Q$, identify a new fact $f_{m}$ that, when added to $C$, <br> would make $Q$ true.</td>
</tr>
</tbody>
</table>
<p>Table 1: The three tasks that ProofWriter performs.
erate human-style justifications, which again are typically supporting evidence rather than a fullyformed line of reasoning, and without explicit reasoning rules (Camburu et al., 2018; Jhamtani and Clark, 2020; Inoue et al., 2020). In contrast, ProofWriter produces a deductive chain of reasoning from what is known to what is concluded, using a transformer retrained to reason systematically.</p>
<h2>3 Approach</h2>
<h3>3.1 Definitions</h3>
<p>Let:</p>
<ul>
<li>$C$ be a theory, a set of English sentences $C$ consisting of facts $F$ and rules $R$, each expressing a logical fact or rule in English. (We also refer to $C$ as the context).</li>
<li>$Q$ be a question, a hypothesis fact in English whose truth is to be determined based solely on the information in $C$.</li>
<li>$A$ be an answer, where $A \in{$ True, False $}$ (if reasoning using a closed-world assumption) or $A \in{$ True, False, Unknown $}$ (open-world assumption).</li>
<li>$P$ be a proof, described shortly.</li>
<li>$I$ be an implication, a fact that logically follows from $C$.</li>
</ul>
<p>We define three tasks (also see Table 1):</p>
<ol>
<li>proof (inc. QA): $C Q \rightarrow A P$ : Given $C$ and hypothesis fact $Q$, what is the truth $A$ and proof $P$ (if any) of $Q$ ?</li>
<li>enumeration: $C \rightarrow I_{1}, \ldots, I_{n}$ : Which $I_{i}$ follow from $C$ ?</li>
<li>abduction(restricted form) $C Q \rightarrow f_{m}$ : Which extra fact $f_{m}$ will make $Q$ true given $C$ ?
We reuse (and add to) the RuleTaker datasets for our work, which include all five elements above. An example of a RuleTaker theory (facts and rules), a query, and a proof generated by ProofWriter are shown in Figure 1. Facts and rules are English statements, and implications are English statements that logically follow from those facts and rules. The original datasets were generated from synthetic logic programs and their implications, using natural language patterns to produce the English forms.</li>
</ol>
<h3>3.2 Semantics</h3>
<p>Following prior work, we adopt the semantics of Datalog (Ceri et al., 1989): A fact is true if it is either known (i.e., explicitly stated in the context $C$ ), or (recursively) is the conclusion of a rule whose conditions are true (is "supported"). For handling negation, we use two alternative Datalog semantics: The first, following prior work, makes the closedworld assumption (CWA) and uses negation as failure (NAF), so that any fact not provable is assumed false. Under this semantics, negated facts and negative rule conclusions are not allowed (redundant under the CWA). The second makes an open-world assumption (OWA), and does allow negative facts and rule conclusions. Under this semantics, a third truth value Unknown is also possible.</p>
<h3>3.3 Proof Representation</h3>
<p>We define a proof $P$ of a fact $f_{q}$ as a directed acyclic graph $(N, E)$ with nodes $n \in N$ and (directed, untyped) edges $e \in E$. Each node in $P$ is either a fact $f$ (a ground literal) or a rule $r$ (a logical implication), expressed in English. Each edge in the proof either connects a fact to a rule, denoting that the fact helps satisfy the rule's condition, or connects a rule to a fact, denoting that the fact follows from the instantiated rule. Thus nodes in any branch of the proof will alternate between facts and rules. Note this definition differs from (and is richer than) that in PRover, where intermediate conclusions were not part of the proof.</p>
<p>Facts in the proof are one of three types: known facts $f_{i} \in F$, negated facts $f_{\text {naf }}$ that cannot be proven (false under negation-as-failure (NAF)), and facts $f_{\text {conc }}$ that are the conclusions of rules. $f_{i}$ and $f_{\text {naf }}$ are leaf nodes of the proof, while the $f_{\text {conc }}$ are intermediate nodes within the proof. Note that $f_{\text {naf }}$ and $f_{\text {conc }}$ are by definition not in $F$. Example proofs are shown in Figures 1 and 3.</p>
<h3>3.4 Proof Encoding</h3>
<p>As we wish to generate proofs, we need to encode $P$ as a linear structure that can be output by a generative model. Facts and rules in the context are explicitly labeled with identifiers (fact1, ..., rule1,</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An example proof that includes a negated (negation-as-failure) fact.
...) that the proof can refer to, see Figures 1 and 3. ${ }^{2}$ Then, in the linear proof, rule nodes are denoted by their identifier (rule1, ...), while fact nodes are denoted by three types of identifiers: fact1, fact2, ... for facts in the context; naf1, naf2, ... for facts not in the context and assumed false; and conc1, conc2, ... for facts concluded by rules. To decode the naf<em> and conc</em> identifiers (which by definition are not in the context), an additional sequence of the form "with conc1: sentence1. conc2: sentence2. ..." is appended to the proof.</p>
<p>To linearize the proof in a format convenient for a generative model, we conjoin rules and their conclusions using a "\%" symbol, express conjunctive rule conditions with a "\&amp;" symbol, and use "#" to denote the inverse implication (" $\leftarrow$ "). We then express the tree using Polish notation. E.g., the proof tree "((fact1 \&amp; fact2) $\rightarrow$ rule1 $\rightarrow$ conc1)" (i.e., fact1 and fact2 satisfy rule1, concluding conc1) would be expressed "# rule1\%conc1 \&amp; fact1 fact2". Thus the 3 -step proof from Figure 1 is encoded:
# rule $18 \%$ conc1 \&amp; fact5 # rule $12 \%$ conc2
# rule $11 \%$ conc3 fact16 ; with conc1:
Charlie is quiet. ; conc2: Charlie is young. ; conc3: Charlie is kind.</p>
<p>If the question is a known fact, the "depth 0 proof" is simply the fact itself (e.g., fact1). If no proof exists, the symbol "None" is used.</p>
<h3>3.5 Models</h3>
<p>The ProofWriter models are built on top of the text-to-text pretrained T5 transformer (Raffel et al., 2020) (T5-11B). We use different textual prompts for the different tasks. For the task of generating an answer and a proof, the input to the model is</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>of the form: " \$question\$ = question ; \$context\$ = theory-sentences", for example: "\$question\$ = Erin is big. ; \$context\$ = sent1: Erin is young. sent2: If ..." The output is of the form: "\$answer\$ = True/False/Unknown : \$proof\$ = proof ;", where proof is encoded as described in Section 3.4. For training instances where multiple outputs are valid, we select a single one at random (for multiple proofs, we select among the shortest proofs). Appendix D lists the hyperparameters and gives input/output examples for each task.</p>
<h3>3.6 Task 1: Proof Generation</h3>
<p>We evaluate two methods of proof generation:
All-At-Once: We train a model to generate the full proof and answer in one go (theory + question $\rightarrow$ answer + proof).
Iterative: We first train a model to generate a single 1-step implication (theory $\rightarrow$ implication +1 -step-proof), where the implication follows from a single rule application. Then at test time, we apply this model iteratively, adding each implication to the theory and repeating until no more implications can be found (i.e., exhaustive forward-chaining). The proof for any given implication can then be assembled from the 1-step-proof fragments (Figure 2).</p>
<h3>3.6.1 All-At-Once ProofWriter ("All")</h3>
<p>The All-At-Once model is trained directly on $C Q \rightarrow A P$ examples in the datasets ( $P=$ "None" if there is no proof of $Q$ ). Section 3.5 describes the i/o format, and Appendix D. 1 shows an example.</p>
<h3>3.6.2 Iterative ProofWriter ("Iter")</h3>
<p>Training: To train the Iterative model, for each theory $C$ in the training data, we create an augmented set of training examples with one sequence of iteratively inferred facts in turn, each using $C$ plus the previously inferred facts. For example, if theory $C_{1}$ implies $I_{1}, I_{2}$, and $I_{3}$, then we create four training examples $C_{1} \rightarrow I_{1}, C_{1} \cup\left{I_{1}\right} \rightarrow I_{2}$, $C_{1} \cup\left{I_{1}, I_{2}\right} \rightarrow I_{3}$, and $C_{1} \cup\left{I_{1}, I_{2}, I_{3}\right} \rightarrow$ "None". The order of adding the $I_{i}$ is random but constrained such that if a later implication depends on an earlier one, the earlier one must be inferred first. For example, if the proof of $I_{3}$ depends on $I_{2}$ (determined by inspecting the gold proofs), $I_{2}$ must be in the context before $I_{3}$ is inferred. This ensures that all example inferences are depth 1 (i.e., a single rule application). An example input/output for one step is shown in Appendix D.2.</p>
<p>Testing: To answer and provide the proof for a particular question/implication, the model generates all implications and their proofs by iteratively applying the model until no more implications (the implication "None") is generated. It then looks for the question among them. If found, the answer is True with the proof given. The model also looks for the negation of the question ${ }^{3}$ and its proof. If found, the answer is False with the proof given. Otherwise, there is no proof (proof = "None") and the answer is False (for positive questions, CWA), True (for negative questions, CWA), or Unknown (any question, OWA).</p>
<h3>3.7 Task 2: Implication Enumeration</h3>
<p>A second desirable reasoning skill is enumerating implications of a theory (rather than just assign True/False to a hypothesis). This capability is important for practical application of the technology. In fact, the Iterative ProofWriter already does this by design, a substantial advantage. To evaluate this (later), we compare this with an "all at once" strategy of generating all implications as a single output string, analogous to the All-At-Once strategy for generating the full proof as a single string. For training this All-At-Once enumerator, and testing both, we gather the list of all implications $I_{i}$ of each theory $C$ in the train/test data. Each train/test example is of then of the form: given $C$, predict all the $I_{i}$. An example input/output is in Appendix D.3.</p>
<h3>3.8 Task 3: Abduction (Single Fact)</h3>
<p>A third desirable reasoning skill is abduction over natural language theories, again made possible by generative models. Abduction has previously been studied extensively in formal logic, e.g., (Konolige, 1997), and in NLP, e.g., (Hobbs et al., 1993; Bhagavatula et al., 2020). Here we evaluate whether a generative approach can combine logic and NLP, performing logical abduction over natural language knowledge. We do this for a restricted form of abduction, namely single-fact abduction: Given a theory $C$ and a possible implication $Q$ not provable from $C$, identify a new fact $f_{m}$ (other than the trivial $Q$ itself) such that $C \cup\left{f_{m}\right}$ implies $Q$.</p>
<p>We restrict this task to the OWA (open-world) setting where questions can naturally have unknown truth values. To train and test an abductive</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>model over our datasets, we create an abductive version as follows: For each theory $C$ in the train/test data, for each unprovable fact $Q$, identify all alternative "missing facts" fact $M$ that, when added to $C$, make $Q$ True. To do this, recall that each NL theory was originally generated from a formal one $C_{\text {formal }}$ in a formal representation language (Datalog). We first exhaustively enumerate all possible $Q_{\text {formal }}$ and fact $M_{\text {formal }}$ in the formal language (this is feasible as the space of predicates and individuals is small), then use a theorem prover to test if $C_{\text {formal }} \cup\left{\right.$ fact $M_{\text {formal }}}$ implies $Q_{\text {formal }}$ for all pairs (factM $M_{\text {formal }}, Q_{\text {formal }}$ ). For each success, we generate the NL equivalents $Q$ and fact $M$ using simple NL generation templates. We then collect the alternative factMs for each $Q$. The abduction task is then, given $C$ and $Q$, identify the set of all alternative factMs, i.e.:</p>
<p>$$
C, Q \rightarrow \text { factM }<em i="i">{1}, \ldots, \text { factM }</em>
$$</p>
<p>If there is no single fact $M$ that can be added to make $Q$ true, then the symbol "None" is output.</p>
<h2>4 Datasets</h2>
<p>We now evaluate ProofWriter on these three tasks. We use the original RuleTaker D* datasets (Clark et al., 2020), plus we create two new variants: The first (CWA) is similar to the original except it fixes some minor inconsistencies concerning negation (details in Appendix A.2). The second (OWA) is also similar to the original, except reasoning uses an open-world assumption.</p>
<p>We denote these as $\mathrm{D}^{<em>}($ orig $), \mathrm{D}^{</em>}(\mathrm{CWA})$, and $\mathrm{D}^{*}$ (OWA). Each example in each dataset contains a theory $C$, a question $Q$, the answer $A$ (True/False/Unknown), and all possible proofs $P_{1}, \ldots, P_{n}$ for that answer (if provable). ${ }^{4}$ Each theory is also accompanied with all possible proofs of all possible implications, as auxiliary annotations.</p>
<p>The D* datasets comprise five datasets, named D0, D1, D2, D3, D5, each containing 100k questions. In each dataset, theories and questions are expressed in templated English (e.g., Figure 1), questions can be positive or negated facts (e.g., "Charlie is not quiet?"), and answers are equally divided into True/False (and Unknown, for the OWA versions). Each dataset contains questions whose answers require reasoning up to depths $D(D=0$, $1,2,3,5)$. Thus, for example, all questions in D0</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>|  |  | Answer | Proof |  |  |
| Depth | # qns | PRo | ProofWriter | PRo | ProofWriter |
| 0 | 6299 | 100 | 100 | 98.4 | 99.6 |
| 1 | 4434 | 99.0 | 99.1 | 93.1 | 98.7 |
| 2 | 2915 | 98.8 | 98.6 | 84.8 | 97.3 |
| 3 | 2396 | 99.1 | 98.5 | 80.5 | 94.4 |
| 4 | 2134 | 98.8 | 98.7 | 72.4 | 91.0 |
| 5 | 2003 | 99.3 | 99.3 | 65.1 | 86.4 |
| All | 20192 | 99.3 | 99.2 | 87.1 | 96.2 |</p>
<p>Table 2: [Task 1: Proof Generation] Systems trained and tested on D5(orig), showing the breakdown by depth of proof required to answer each question. ProofWriter generates significantly more correct proofs for all depths, achieving a new SOTA on this task.
are lookup questions, requiring no inference. Each dataset is split 70/10/20 into train/dev/test.</p>
<p>To test generalization, we also use two other datasets from the original RuleTaker work:
Birds-Electricity: These 6 test-only datasets use small, real-world theories written by hand (one per dataset) to test out-of-distribution model performance. Details are in Appendix A.3.
ParaRules: This dataset contains 40k questions against 2 k theories expressed in paraphrased natural language, obtained through crowdsourcing. This dataset tests transfer to more natural expressions of knowledge. Details are in Appendix A.4.</p>
<h2>5 Experiments and Results</h2>
<h3>5.1 Task 1: Proof Generation (Comparison with Prior Work)</h3>
<p>First, we compare ProofWriter's ability to generate proofs with PRover, the current state-of-the-art. We evaluate both answer accuracy and proof correctness. For proof correctness, for a fair comparison, we ignore the intermediate conclusion nodes (which PRover does not generate). We then use the same strict scoring metric as in PRover (called FA or Full Accuracy in the PRover paper): the proof graph must exactly match a gold proof (i.e., be perfectly correct); otherwise, the proof scores 0 .</p>
<h3>5.1.1 Generating Answers and Proofs</h3>
<p>We use the same IID (independent, identically distributed) data used for PRover (train/test on dataset D5(orig)). The results are in Table 2, showing accuracies for questions requiring increasingly deeper depths of reasoning to answer. The ProofWriter's results are for the All-At-Once model. (The Iterative model scores are almost identical, see later Table 4.) While answer accuracy is almost perfect for both systems, ProofWriter generates substantially</p>
<p>|  |  | Answer | Proof |  |  |  |
| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
|  |  | PRo | ProofWriter |  |  |  |
| Test | # qns |  | All | Iter |  |  |
| Birds1 | 40 | 95.0 | 100 | 95.0 | 92.5 | 100 | 95.0 |
| Birds2 | 40 | 95.0 | 100 | 95.0 | 95.0 | 100 | 95.0 |
| Elec1 | 162 | 100 | 96.9 | 100 | 95.1 | 96.9 | 100 |
| Elec2 | 180 | 100 | 98.9 | 100 | 91.7 | 98.9 | 100 |
| Elec3 | 624 | 89.7 | 92.0 | 95.5 | 71.8 | 92.0 | 95.5 |
| Elec4 | 4224 | 84.8 | 83.3 | 97.1 | 80.6 | 82.0 | 97.1 |
| All | 5270 | 86.5 | 85.5 | 97.0 | 80.5 | 84.5 | 97.0 |</p>
<p>Table 3: [Task 1: Proof Generation] Training on D5, test on Birds-Electricity. Both ProofWriter versions ("All" for All-At-Once, "Iter" for Iterative) outperform PRover overall in both answer and proof correctness. The Iterative model is also significantly more robust.
more correct proofs (last line, $+9 \%$ absolute), and without the complexity of PRover's heuristic assembly of proof graphs using ILP.</p>
<h3>5.1.2 Performance on OOD Rulesets</h3>
<p>We compared ProofWriter's and PRover's ability to generalize to the hand-authored Birds-Electricity rulesets, zero shot. These rulesets are out-ofdomain (OOD), as their English is not templated and is stylistically different to the training data. We compare the PRover and All-At-Once ("All") ProofWriter models trained on D5, plus the Iterative ProofWriter ("Iter") trained on D0-D3 theories. The models do not see any Birds-Electricity examples during training. The results in Table 3 show that ProofWriter's proof generation transfers well zero-shot to these hand-authored datasets, with $84.5 \%$ proof correctness for All-At-Once, and $97 \%$ for the Iterative ProofWriter, indicating better out-of-domain generalization for the Iterative version. Both ProofWriter models significantly outperform PRover ( $80.5 \%$ ).</p>
<p>We also find ProofWriter obtains more correct proofs ( $+3 \%$ ) than PRover on the ParaRules dataset. Details are in Appendix B.1.</p>
<h3>5.2 Task 1: Proof Generation (All-At-Once vs. Iterative)</h3>
<p>Second, we compare our two approaches to proof generation, All-At-Once vs. Iterative, in more detail. We show that although they have almost identical performance for proofs with depths seen in training, the Iterative model generalizes better to proofs of longer depths than seen in training. For these comparisons, we use the new $\mathrm{D}^{<em>}(\mathrm{CWA})$ datasets (which fix some minor errors in $\mathrm{D}^{</em>}$ (orig)), and also the $\mathrm{D}^{*}(\mathrm{OWA})$ datasets to explore performance in an open-world setting.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Proof</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Depth</td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
</tr>
<tr>
<td style="text-align: left;">N/A</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">97.8</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">97.3</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">96.9</td>
<td style="text-align: center;">97.1</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">93.1</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">96.5</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">86.4</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">97.6</td>
</tr>
</tbody>
</table>
<p>Table 4: [Task 1] Comparison of All-At-Once ("All") vs. Iterative ("Iter") ProofWriter models, trained on D5 and D0-D3 respectively, and tested on D5.</p>
<h3>5.2.1 Comparison (IID Test Set)</h3>
<p>We train the All-At-Once model on D5 (train), and the Iterative model using the method described in Section 3.6.2, using the ( $\sim 5 \mathrm{k}$ ) theories from D3 (train) plus $\sim 20 \%$ of the D0-D2 (train) theories. ${ }^{5}$ We then test both models on D5 (test). We measure both answer and proof accuracies, and also break down the results by proof depth (using "N/A" as the proof depth for questions that are not provable). The D5 test set has 2 k questions at each proof depth, plus 8 k unprovable questions (proof $=$ "None", depth = "N/A'). ${ }^{6}$</p>
<p>The results are shown in Table 4, and show that both ProofWriter versions have similar, high proof correctness ( $95 \%+$ ) on the test set, even though some proofs are highly complex.</p>
<h3>5.2.2 Generalization to Unseen Depths</h3>
<p>We also wish to see how well the models can generate proofs at depths unseen during training. To do this, we train an All-At-Once model on D3, and use the same Iterative model as earlier (trained on iterative examples from theories up to depth 3). We test on D5. As D5 contains problems at greater depths than those seen during training, we can observe the models' ability to generalize. We compare with both the CWA and OWA versions of our datasets.</p>
<p>The results are shown in Table 5. As can be seen, the All-At-Once model has quite poor generalization for generating longer proofs than seen in training, while the Iterative model is more robust (red box).</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 5: [Task 1] Comparison of the All-At-Once vs. Iterative ProofWriter models, trained on D3 and tested on D5. While scores are mostly similar throughout, the iterative model generalizes substantially better to generate proofs of depths unseen during training (red box).
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Table 5: [Task 1] Comparison of the All-At-Once vs. Iterative ProofWriter models, trained on D3 and tested on D5. While scores are mostly similar throughout, the iterative model generalizes substantially better to generate proofs of depths unseen during training (red box).
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 4: [Task 1] All-At-Once proofs can be verified by checking each step as a separate QA query.</p>
<h3>5.3 Verifying All-At-Once Proofs</h3>
<p>Proofs from the Iterative ProofWriter have an additional desirable property: each proof step is one that the model explicitly took during the iteration, i.e., the model "believes" the step. In contrast, the All-At-Once proofs are a post hoc generated string of symbols, and may not reflect steps that ProofWriter would actually make. However, because proofs include intermediate conclusions, we can alleviate this concern by verifying individual steps in the All-At-Once proofs. For example, if a generated proof step states that fact2 + fact3 + rule4 implies conc1, we can simply ask ProofWriter in QA mode if this is true (Figure 4). Given the almost perfect performance for such simple depth 1 questions in QA mode (with no distractor facts or rules), the ability to verify a correct proof corresponds to the accuracy of correctly generating the correct intermediate conclusions conc* in the first place. (Note that an unverified proof is not necessarily wrong, rather cannot be verified as right). OWA proofs can be fully verified in this way. For CWA theories with NAFs, the verification is only partial as NAFs are presumed negative statements</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Verified Proofs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Train on:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Train on:</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Depth</td>
<td style="text-align: center;">D3</td>
<td style="text-align: center;">D5</td>
<td style="text-align: center;">D3</td>
<td style="text-align: center;">D5</td>
</tr>
<tr>
<td style="text-align: left;">N/A</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.7</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">98.6</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">95.8</td>
<td style="text-align: center;">94.3</td>
<td style="text-align: center;">96.8</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">94.6</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">16.4</td>
<td style="text-align: center;">90.8</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">87.2</td>
<td style="text-align: center;">97.2</td>
<td style="text-align: center;">87.9</td>
<td style="text-align: center;">97.9</td>
</tr>
</tbody>
</table>
<p>Table 6: [Task 1: Proof Generation] The All-At-Once model's ability to verify its proofs. For proofs within depths seen during training, almost all correct proofs (Tables 5 and 4, columns 5 and 7) can be verified. However, for proofs at unseen depths, the proportion that can be verified drops rapidly (trained on D3, test on depths 4,5). In contrast, Iterative ProofWriter's proofs are always verified, by definition of its algorithm.
which require the full theory to verify.
We measured the percentage of correct, verified proofs, shown in Table 6. Provided proofs are within the depths seen during training, almost all correct proofs can be verified. However, at depths deeper than seen at training, the proportion that can be verified drops rapidly. In contrast, the Iterative ProofWriter's proofs are always verified, as by definition they are assembled from single step inferences that the model actually took.</p>
<h3>5.4 Task 2: Implication Enumeration</h3>
<p>Third, we evaluate ProofWriter's performance on a new task, namely enumerating implications of a theory (rather than just assign True/False to a hypothesis). We compare the All-At-Once and Iterative strategies as described in Section 3.7.</p>
<p>To train All-At-Once, and test both, we created an enumerative dataset of $C \rightarrow\left{I_{1}, \ldots, I_{n}\right}$ examples (Section 3.7). For this we sample theories $C$ in the D0-D3 datasets and gather the list of all implications $I_{i}$ for each theory $C$. We call this enumerative dataset D3+Enum. We similarly create a D5-Enum dataset from theories in (only) D5 to test OOD conclusion generation. We create CWA and OWA versions of both.</p>
<p>We train All-At-Once on D3-Enum (train), then test both models on D3-Enum (test) and D5-Enum (test). For metrics, we measure F1 scores by comparing the individual predicted implications with the gold $I_{i}$, as well as the exact-match correctness of the predicted set of implications $\left{I_{1}, \ldots, I_{n}\right}$ (one point if the set exactly matches the gold, bar ordering, zero otherwise). The results are shown in</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Enum</td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Test</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">Iter</td>
</tr>
<tr>
<td style="text-align: center;">D3+</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">99.0</td>
</tr>
<tr>
<td style="text-align: center;">D5</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">94.8</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">44.6</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">48.9</td>
<td style="text-align: center;">94.8</td>
</tr>
</tbody>
</table>
<p>Table 7: [Task 2: Enumeration] Iterative ProofWriter is better at generating all implications than an All-AtOnce strategy. (All-At-Once is trained on D3+Enum, Iterative ProofWriter is the same model as earlier.)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test:</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">D3-Ab</td>
<td style="text-align: center;">7067</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">94.5</td>
</tr>
<tr>
<td style="text-align: left;">D5-Ab</td>
<td style="text-align: center;">7181</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">93.5</td>
</tr>
</tbody>
</table>
<p>Table 8: [Task 3: Abduction] Given a theory $C$ and an unprovable conclusion $Q$, predict all alternative facts that, when added to $C$, make $Q$ provable.</p>
<p>Table 7, and show that the Iterative ProofWriter is better at implication enumeration than the simple All-At-Once strategy. In particular, the All-At-Once strategy struggles for problems at depths unseen in training (second row), although it does well on its own test set despite the complicated unordered output it has to generate (up to 16 different implications in D3, 21 in D5).</p>
<h3>5.5 Task 3: Abduction (Single Fact)</h3>
<p>Fourth and finally, we evaluate performance on a second new task, namely abduction over natural language theories, again made possible by generative models. Analogous to implication enumeration, we create a derivative abductive dataset of $C, Q \rightarrow f a c t M_{1}, \ldots$, fact $M_{i}$ examples, where $C \cup\left{f a c t M_{i}\right}$ results in $Q$ becoming provable as described in Section 3.8. We create such D<em>-Ab datasets from the D</em>(OWA) datasets.</p>
<h3>5.5.1 Results (IID)</h3>
<p>We trained a model on D3-Ab (train), and then tested on both D3-Ab (test) and D5-Ab (test). We evaluate the results by comparing the predicted and gold fact $M \mathrm{~s}$, measuring both F1 and "perfect match" Accuracy ( 1 when F1=1, 0 otherwise). The results are shown in Table 8, and indicate that the model performs well overall ( $85 \%+$ scores). We also broke down the recall of fact $M \mathrm{~s}$ by proof depth required to prove $Q$ given $C$ and fact $M$. This is shown in Table 9, indicating that it is harder to identify a fact $M$ that completes a deeper proof. The similarity of D3-Ab and D5-Ab scores suggests that D5-Ab is not out-of-domain for this task: Although depths for provable D5 facts are deeper than D3, this task concerns unprovable facts, which may not be distributed differently to D3-Ab.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Gold Proof</th>
<th style="text-align: center;">Test on D3-Ab</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Test on D5-Ab</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Depth</td>
<td style="text-align: center;"># Gold</td>
<td style="text-align: center;">Acc (recall)</td>
<td style="text-align: center;"># Gold</td>
<td style="text-align: center;">Acc (recall)</td>
</tr>
<tr>
<td style="text-align: left;">N/A</td>
<td style="text-align: center;">2155</td>
<td style="text-align: center;">97.73</td>
<td style="text-align: center;">2170</td>
<td style="text-align: center;">97.74</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">4813</td>
<td style="text-align: center;">98.46</td>
<td style="text-align: center;">4731</td>
<td style="text-align: center;">98.73</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">1719</td>
<td style="text-align: center;">96.22</td>
<td style="text-align: center;">1986</td>
<td style="text-align: center;">96.17</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: center;">688</td>
<td style="text-align: center;">90.26</td>
<td style="text-align: center;">915</td>
<td style="text-align: center;">92.79</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">153</td>
<td style="text-align: center;">75.82</td>
<td style="text-align: center;">330</td>
<td style="text-align: center;">82.73</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">36.84</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">78.13</td>
</tr>
</tbody>
</table>
<p>Table 9: [Task 3: Abduction] Recall of abduced facts by proof depth. The data suggests that it is harder to identify a fact $M$ that completes a deeper proof.</p>
<h3>5.5.2 Generalization to New Tasks</h3>
<p>To assess out-of-domain generalization, we also evaluate how well the trained abductive model performs on an abductive version of the BirdsElectricity(OWA) theories, zero-shot (created using the same approach, Section 3.8). We find that ProofWriter has perfect zero-shot performance for the simple Birds rulebases, but progressively reduced performance for the Electricity theories as they get more complex (dropping to $64 \%$ F1, $62 \%$ Accuracy for one rulebase), indicating that the abductive task is only partly solved (Appendix B.2).</p>
<h2>6 Discussion</h2>
<h3>6.1 All-At-Once vs. Iterative Strategies</h3>
<p>While the All-At-Once approach to proof generation is simple, efficient, and effective, it does not generalize as well to proofs of greater depth than seen at training. In contrast, the Iterative approach is robust to generalization. Even though errors at each iteration accumulate, the reliability of 1-step inference is so high that such error accumulations remain small. The Iterative architecture, namely a simple model embedded in a recursive loop (rather than single seq2seq model), illustrates how transformers can be used in a "scale-invariant" way, i.e., performance is largely unchanged by the scale (here reasoning depth) of the problem. In addition, as proofs are built from actual inference steps taken by the model, they are by definition "faithful" to the model's inference steps, rather than being a post hoc rationalization.</p>
<p>However, there are also some drawbacks to the Iterative approach: First, it is inefficient and unguided, proving everything possible and only then looking for the answer and proof for a particular question. In fact, this is a limitation of unconstrained forward-chaining in general, hence established techniques for guiding forward-chaining could be applied, e.g., a best-first expansion strategy, or using a backward-chaining strategy instead
(which would similarly need to be controlled). Second, as the theory grows by one fact per iteration, there is a risk of exceeding the transformer's input token limit ( 512 tokens by default), hence limiting the size of theories that can be handled. For larger theories, a retrieval mechanism might be needed to manage the facts and rules available to the reasoner.</p>
<h3>6.2 Abduction and Implicit Knowledge</h3>
<p>Recently, LeapOfThought (Talmor et al., 2020) showed that RuleTaker-like models could be retrained to reason with a combination of explicit and implicit knowledge, rather than requiring all rules to be stated explicitly (the implicit knowledge coming from the latent knowledge acquired during pretraining (Petroni et al., 2019)). Now, given an abductive capability such as the one we have presented, we have a mechanism for materializing the implicit knowledge used to answer a question, and hence generating the full proof of its answer: Given a LeapOfThought conclusion, first abduce the "missing" (implicit) fact(s) required for an explicit proof, then use ProofWriter to generate that proof. This is a significant step forward to help understand a model's decisions when both implicit and explicit knowledge has been used.</p>
<h2>7 Summary and Conclusion</h2>
<p>While it is remarkable that transformers can learn to systematically reason over language, such methods will have limited impact if they cannot also explain their answers. In this work, we showed the first application of generative techniques to this task, and demonstrated how proofs, implication enumerations, and abductive inferences can be generated, exceeding the prior state-of-the-art in proof generation by $+9 \%$ (absolute). In addition, the Iterative ProofWriter robustly generalizes to deeper proofs and more varied language than seen in training, and produces proofs that reflect (i.e., are faithful to) the model's actual inference decisions. Finally, the abductive capability offers the potential for generating proofs when both explicit and implicit knowledge are used, by materializing the implicit knowledge needed to complete the proof. Together, these significantly improve the viability of neural methods for systematically reasoning over language in practical settings. The ProofWriter datasets are available at https://allenai.org/data/proofwriter</p>
<p>Acknowledgements: We thank Google for providing the TPUs for conducting experiments.</p>
<h2>References</h2>
<p>Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, S. Yih, and Yejin Choi. 2020. Abductive commonsense reasoning. In $I C L R^{\prime} 20$.</p>
<p>Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. In Advances in Neural Information Processing Systems, pages 9539-9549.
S. Ceri, G. Gottlob, and L. Tanca. 1989. What you always wanted to know about datalog (and never dared to ask). IEEE Trans. Knowl. Data Eng., 1:146-166.</p>
<p>Peter Clark, Oyvind Tafjord, and Kyle Richardson. 2020. Transformers as soft reasoners over language. In IJCAI'20.</p>
<p>Jay DeYoung, Sarthak Jain, Nazneen Rajani, E. Lehman, Caiming Xiong, R. Socher, and Byron C. Wallace. 2019. ERASER: A benchmark to evaluate rationalized nlp models. In $A C L$.</p>
<p>Nicolas Gontier, Koustuv Sinha, Siva Reddy, and C. Pal. 2020. Measuring systematic generalization in neural proof generation with transformers. In NeurIPS'20.
J. Hobbs, Mark E. Stickel, Douglas E. Appelt, and P. Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63:69-142.
N. Inoue, Pontus Stenetorp, and Kentaro Inui. 2020. R4C: A benchmark for evaluating RC systems to get the right answer for the right reason. In $A C L$.</p>
<p>Harsh Jhamtani and P. Clark. 2020. Learning to explain: Datasets and models for identifying valid reasoning chains in multihop question-answering. In EMNLP.</p>
<p>Aishwarya Kamath and Rajarshi Das. 2019. A survey on semantic parsing. In $A K B C^{\prime} 19$.
K. Konolige. 1997. Abductive theories in artificial intelligence. In Principles of Knowledge Representation.</p>
<p>John McCarthy. 1984. Applications of circumscription to formalizing common sense knowledge. In NMR.</p>
<p>John W. McCarthy. 1959. Programs with common sense. In Proc. Tedding Conf. on the Mechanization of Thought Processes, pages 75-91.</p>
<p>Mark A Musen and Johan Van der Lei. 1988. Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system models. In Machine Intelligence and Pattern Recognition, volume 7, pages 335-352. Elsevier.</p>
<p>Sharan Narang, Colin Raffel, Katherine Lee, A. Roberts, Noah Fiedel, and Karishma Malkan. 2020. WT5?! training text-to-text models to explain their predictions. ArXiv, abs/2004.14546.
F. Petroni, Tim Rocktäschel, Patrick Lewis, A. Bakhtin, Y. Wu, Alexander H. Miller, and S. Riedel. 2019. Language models as knowledge bases? In EMNLP.</p>
<p>Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. ArXiv, abs/2009.03393.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, M. Matena, Yanqi Zhou, W. Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Tim Rocktäschel and S. Riedel. 2017. End-to-end differentiable proving. In NeurIPS.</p>
<p>Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, and Mohit Bansal. 2020. PRover: Proof generation for interpretable reasoning over rules. In EMNLP'20.</p>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. 2019. Learning a SAT solver from single-bit supervision. In $I C L R$.</p>
<p>Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, and Matt Gardner. 2020. Obtaining faithful interpretations from compositional neural networks. In $A C L$.</p>
<p>Alon Talmor, Oyvind Tafjord, P. Clark, Y. Goldberg, and Jonathan Berant. 2020. LeapOfThought: Teaching pre-trained models to systematically reason over implicit knowledge. In NeurIPS.</p>
<p>Ming-Zhe Wang and Jun Deng. 2020. Learning to prove theorems by learning to generate theorems. In NeurIPS.</p>
<p>Leon Weber, Pasquale Minervini, Jannes Münchmeyer, Ulf Leser, and Tim Rocktäschel. 2019. Nlprolog: Reasoning with weak unification for question answering in natural language. In $A C L$.</p>
<h1>Appendix: ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language</h1>
<h2>A Datasets: Additional Details</h2>
<h2>A. 1 Statistics</h2>
<p>Some overall statistics for the updated RuleTaker CWA and OWA datasets are in Table 10. The number of implications per theory can reach 20 and above, and the proof depths go up to 10 , even though the proof depths of the associated questions are limited to the dataset depth (e.g., depth 3 for D3).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;"># th</th>
<th style="text-align: center;"># qns</th>
<th style="text-align: center;"># impl min/mean/max</th>
<th style="text-align: center;">depth <br> max</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CWA:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D0</td>
<td style="text-align: center;">27020</td>
<td style="text-align: center;">100002</td>
<td style="text-align: center;">0/1.0/18</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">D1</td>
<td style="text-align: center;">12965</td>
<td style="text-align: center;">100012</td>
<td style="text-align: center;">1/1.9/17</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">D2</td>
<td style="text-align: center;">9138</td>
<td style="text-align: center;">100014</td>
<td style="text-align: center;">2/3.3/18</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">D3</td>
<td style="text-align: center;">7067</td>
<td style="text-align: center;">100024</td>
<td style="text-align: center;">3/5.1/16</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: center;">D5</td>
<td style="text-align: center;">4935</td>
<td style="text-align: center;">100030</td>
<td style="text-align: center;">5/9.8/21</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Birds/Elec</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">5270</td>
<td style="text-align: center;">0/2.0/6</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">ParaRules</td>
<td style="text-align: center;">2403</td>
<td style="text-align: center;">40022</td>
<td style="text-align: center;">3/4.3/14</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">OWA:</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">D0</td>
<td style="text-align: center;">26978</td>
<td style="text-align: center;">100000</td>
<td style="text-align: center;">0/0.8/18</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">D1</td>
<td style="text-align: center;">12933</td>
<td style="text-align: center;">100014</td>
<td style="text-align: center;">1/1.7/14</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">D2</td>
<td style="text-align: center;">9033</td>
<td style="text-align: center;">100010</td>
<td style="text-align: center;">2/3.1/14</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">D3</td>
<td style="text-align: center;">6940</td>
<td style="text-align: center;">100036</td>
<td style="text-align: center;">3/4.8/16</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">D5</td>
<td style="text-align: center;">4752</td>
<td style="text-align: center;">100030</td>
<td style="text-align: center;">5/9.1/21</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">Birds/Elec</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">5270</td>
<td style="text-align: center;">0/1.2/6</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: center;">ParaRules</td>
<td style="text-align: center;">2403</td>
<td style="text-align: center;">40022</td>
<td style="text-align: center;">3/4.3/14</td>
<td style="text-align: center;">5</td>
</tr>
</tbody>
</table>
<p>Table 10: Statistics for the CWA and OWA datasets, giving the number of theories, questions and implications per theory. Note that the maximum implication proof depth can go higher than the maximum proof depth for the included questions (e.g., for D5 the maximum questions depth is 5 , but there are implications up to depth 10 which are include in the enumeration task).</p>
<p>Table 11 describes overall statistics for the datasets for Task 3: Abduction. Each abduction question can have zero or more missing facts as answer, and the proof depths can go up to 11 .</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: center;"># th</th>
<th style="text-align: center;"># qns</th>
<th style="text-align: center;"># missing <br> facts <br> min/mean/max</th>
<th style="text-align: center;">max <br> proof <br> depth</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">D0-Ab</td>
<td style="text-align: center;">18011</td>
<td style="text-align: center;">85705</td>
<td style="text-align: center;">$0 / 0.8 / 15$</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">D1-Ab</td>
<td style="text-align: center;">10448</td>
<td style="text-align: center;">49808</td>
<td style="text-align: center;">$0 / 0.8 / 12$</td>
<td style="text-align: center;">7</td>
</tr>
<tr>
<td style="text-align: left;">D2-Ab</td>
<td style="text-align: center;">7092</td>
<td style="text-align: center;">37245</td>
<td style="text-align: center;">$0 / 0.9 / 11$</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: left;">D3-Ab</td>
<td style="text-align: center;">5633</td>
<td style="text-align: center;">34915</td>
<td style="text-align: center;">$0 / 1.1 / 11$</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">D5-Ab</td>
<td style="text-align: center;">4362</td>
<td style="text-align: center;">35213</td>
<td style="text-align: center;">$0 / 1.2 / 9$</td>
<td style="text-align: center;">11</td>
</tr>
<tr>
<td style="text-align: left;">Birds-Electricity-Ab</td>
<td style="text-align: center;">140</td>
<td style="text-align: center;">3940</td>
<td style="text-align: center;">$0 / 0.24 / 4$</td>
<td style="text-align: center;">4</td>
</tr>
</tbody>
</table>
<p>Table 11: Statistics for the Abduction datasets, giving the number of theories, abduction questions, number of missing facts per question and maximum proof depth.</p>
<h2>A. 2 Repairs to the RuleTaker Datasets</h2>
<p>The original RuleTaker theories were intended to be full Datalog theories, but contained three occasional violations in the with-negation theories:</p>
<ol>
<li>Some theories contained negated facts (e.g., "Bob is not red") and/or rules with negated conclusions. Such statements are redundant under a CWA, and not allowed according to formal Datalog specifications.</li>
<li>Some theories included rules with a free variable in a negated condition (e.g., "If someone is not blue then Bob is happy."). Such rules are not allowed according to formal Datalog specifications, as the possible groundings of the variable require meta-information about the theory as a whole.</li>
<li>A bug in the stratification checker led to a few theories being included that were not stratifiable, and hence may have multiple, valid truth assignments for their facts.
As a result, the theories were regenerated (with the same distribution over number of facts, rules, condition, etc.) to create the $\mathrm{D}^{*}(\mathrm{CWA})$ datasets, avoiding these issues.</li>
</ol>
<p>The $\mathrm{D}^{<em>}(\mathrm{OWA})$ datasets are similar to the $\mathrm{D}^{</em>}$ (orig) datasets, but evaluated without a CWA, i.e., negation-as-failure (NAF) is replaced with hard negation. The theories with negation were again regenerated to ensure they were stratifiable (to avoid negation cycles), but they still retain negated facts and rule conclusions. The truth values of the questions were recomputed using an OWA, resulting in answers True/False/Unknown.</p>
<h2>A. 3 The Birds-Electricity Datasets</h2>
<p>The RuleTaker "birds" rulebase is a well-known logic problem illustrating the use of "abnormality" predicates (McCarthy, 1984), ${ }^{7}$, and converted into English by hand. The dataset contains a single theory of six rules (e.g., "If someone is a bird and wounded then they are abnormal.") and seven facts (e.g., "Bill is wounded"), and forty questions against this theory (i.e., 40 test examples total). Birds1 and Birds2 differ solely in the English wording (e.g., "Bill is flying" vs. "Bill can fly").</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Proof</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"># qns</td>
<td style="text-align: center;">PRo</td>
<td style="text-align: center;">ProofWriter</td>
<td style="text-align: center;">PRo</td>
<td style="text-align: center;">ProofWriter</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{D}=0$</td>
<td style="text-align: center;">2968</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">2406</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">97.3</td>
<td style="text-align: center;">99.3</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">1443</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">88.7</td>
<td style="text-align: center;">97.7</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: center;">1036</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">89.9</td>
<td style="text-align: center;">96.5</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">142</td>
<td style="text-align: center;">88.0</td>
<td style="text-align: center;">91.5</td>
<td style="text-align: center;">76.1</td>
<td style="text-align: center;">83.1</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">8008</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">95.1</td>
<td style="text-align: center;">98.5</td>
</tr>
</tbody>
</table>
<p>Table 12: [Task 1: Proof Generation] Train on D3 + ParaRules, test on (only) ParaRules. Both systems demonstrate robustness to more complex linguistic expressions in the theories, with ProofWriter obtaining $3 \%$ higher proof correctness.</p>
<p>The four RuleTaker "electricity" datasets contain examples of reasoning about toy electrical cicuits using a small set of general rules about circuits. Examples in each dataset are built using a fixed set of general rules per dataset, ranging from five rules (Elec1) to twelve rules (Elec4). Each example in these datasets contains the general rules, plus between two and five facts describing a particular circuit, with a set of questions about the circuit, e.g., Q: "The light bulb is glowing?" A: True.</p>
<h2>A. 4 The ParaRules Dataset</h2>
<p>The RuleTaker "ParaRules" dataset contains 40k questions against 2 k theories expressed in paraphrased natural language, obtained by having crowdworkers rephrase the templated English facts and rules from sampled original theories into more varied natural language. For example, "Bob is cold." might be rephrased "In the snow sits Bob, crying from being cold"; or "Alan is round. Alan is blue. Alan is kind." might be rephrased "Alan, who is round and also kind, tends to be rather blue"; or "If someone is kind then they are young." might be rephrased "A kind person will certainly be young.". While the previous datasets contain synthetic language, ParaRules tests the models' ability to reason over more human-like paraphrased language.</p>
<h2>B Additional Results</h2>
<h2>B. 1 Results on the OOD ParaRules Dataset</h2>
<p>We also test the robustness of ProofWriter's proof generation to theories that use more varied natural language, summarized in Section 5.1.2. Following (Clark et al., 2020) and (Saha et al., 2020), we train on the combined training partitions of D3(orig) and ParaRules, then test on the ParaRules test partition. The results in Table 12 show that PRover and ProofWriter (All-At-Once) are robust to more com-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Test Dataset:</th>
<th style="text-align: center;"># qns</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Birds1-Ab</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">Birds2-Ab</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">100.00</td>
<td style="text-align: center;">100.00</td>
</tr>
<tr>
<td style="text-align: left;">Elec1-Ab</td>
<td style="text-align: center;">114</td>
<td style="text-align: center;">89.47</td>
<td style="text-align: center;">89.47</td>
</tr>
<tr>
<td style="text-align: left;">Elec2-Ab</td>
<td style="text-align: center;">126</td>
<td style="text-align: center;">90.25</td>
<td style="text-align: center;">88.89</td>
</tr>
<tr>
<td style="text-align: left;">Elec3-Ab</td>
<td style="text-align: center;">456</td>
<td style="text-align: center;">81.79</td>
<td style="text-align: center;">76.32</td>
</tr>
<tr>
<td style="text-align: left;">Elec4-Ab</td>
<td style="text-align: center;">3216</td>
<td style="text-align: center;">85.77</td>
<td style="text-align: center;">83.99</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">3940</td>
<td style="text-align: center;">85.66</td>
<td style="text-align: center;">83.53</td>
</tr>
</tbody>
</table>
<p>Table 13: [Task 3: Abduction] Zero-shot scores of the D3-Ab model on the Birds-Electricity-Ab rulebases.
plex natural language in the input, with ProofWriter obtaining $3 \%$ higher proof correctness.</p>
<h2>B. 2 Abduction: Generalization to New Tasks</h2>
<p>Section 5.5.2 summarized the results of testing abductive reasoning on abductive versions of the Birds-Electricity(OWA) theories. The detailed results are shown in Table 13, showing perfect zeroshot performance for the simple Birds rulebases, but progressively reduced performance for the Electricity theories as they get more complex. This indicates that the abductive task remains only partially solved by our generative model.</p>
<h2>C Results with T5-large</h2>
<p>In the main part of the paper we trained ProofWriter starting from the largest available T5-11B model (11 billion parameters). If we instead use the more manageable T5-large model ( 770 million parameters), the scores generally go down, but typically by a small amount.</p>
<p>In Tables 14 and 15 we show two examples of this, for the All-At-Once and Iterative ProofWriter models respectively, when training on the D3 dataset and evaluating on D5. We see the T5-large model is a bit worse on higher depth proof accuracy in the All-At-Once model, but is otherwise quite competitive.</p>
<h2>D Hyperparameters and I/O Examples</h2>
<p>We fine-tune the models on the training set using the default hyperparameters (including the Adafactor optimizer) in the T5 library. ${ }^{8}$ We use the largest T5-11B model for the main results, fine-tuned for 40k steps (batch size 8), selecting the checkpoint with highest validation score (usually the final step). See Appendix C for results using the smaller T5large.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Proof</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Depth</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
</tr>
<tr>
<td style="text-align: left;">N/A</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">97.4</td>
<td style="text-align: center;">99.4</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">99.8</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.4</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">98.2</td>
<td style="text-align: center;">97.6</td>
<td style="text-align: center;">98.8</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">91.2</td>
<td style="text-align: center;">94.5</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">69.9</td>
<td style="text-align: center;">46.9</td>
<td style="text-align: center;">71.4</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">72.9</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">12.4</td>
<td style="text-align: center;">27.4</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">35.1</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">96.6</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">85.6</td>
<td style="text-align: center;">90.2</td>
</tr>
</tbody>
</table>
<p>Table 14: [Task 1] Comparing T5-large vs T5-11B for the All-At-Once models trained on D3 and evaluated on D5. T5-large is actually a bit ahead of T5-11B on answer accuracy (for CWA), although the proof correctness is noticeably higher with T5-11B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Answer</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Proof</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CWA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OWA</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">Depth</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
<td style="text-align: center;">large</td>
<td style="text-align: center;">11 B</td>
</tr>
<tr>
<td style="text-align: left;">N/A</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">99.9</td>
<td style="text-align: center;">99.0</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">99.9</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">99.8</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">99.3</td>
<td style="text-align: center;">95.0</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">97.8</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: center;">98.3</td>
<td style="text-align: center;">99.5</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">91.0</td>
<td style="text-align: center;">91.7</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">97.3</td>
</tr>
<tr>
<td style="text-align: left;">3</td>
<td style="text-align: center;">98.6</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">98.4</td>
<td style="text-align: center;">99.2</td>
<td style="text-align: center;">89.0</td>
<td style="text-align: center;">90.4</td>
<td style="text-align: center;">95.5</td>
<td style="text-align: center;">97.1</td>
</tr>
<tr>
<td style="text-align: left;">4</td>
<td style="text-align: center;">98.0</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">97.5</td>
<td style="text-align: center;">99.1</td>
<td style="text-align: center;">86.3</td>
<td style="text-align: center;">88.9</td>
<td style="text-align: center;">93.4</td>
<td style="text-align: center;">96.3</td>
</tr>
<tr>
<td style="text-align: left;">5</td>
<td style="text-align: center;">97.7</td>
<td style="text-align: center;">98.9</td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">87.8</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">86.4</td>
</tr>
<tr>
<td style="text-align: left;">All</td>
<td style="text-align: center;">98.7</td>
<td style="text-align: center;">99.7</td>
<td style="text-align: center;">98.8</td>
<td style="text-align: center;">99.6</td>
<td style="text-align: center;">94.4</td>
<td style="text-align: center;">95.4</td>
<td style="text-align: center;">96.4</td>
<td style="text-align: center;">97.6</td>
</tr>
</tbody>
</table>
<p>Table 15: [Task 1] Comparing T5-large vs T5-11B for the Iterative ProofWriter trained on D0-D3 and evaluated on D5. T5-11B is systematically slightly better.</p>
<p>For all tasks in this paper, the model input and output are plain textual strings in a certain format. The format is slightly more verbose than strictly necessary, as it is compatible with a more general framework combining different views of the task. This is not typically an issue, although there are a few cases where the iterative model reaches the default 512 token limit of T5 when adding implications to the theory. We now show input/output examples for proof generation (All-At-Once and (one step of) the Iterative approaches), implication generation (All-At-Once approach; Iterative implication generation is a side-effect of Iterative proof generation), and abduction.</p>
<h2>D. 1 All-At-Once Proof Generation</h2>
<p>In this task we are given a theory and question, and need to produce an answer (True/False/Unknown) and proof with intermediates.
INPUT: \$answer\$ ; \$proof\$ ; \$question\$ = The lion is not nice? ; \$context\$ = sent1: The tiger chases the lion. sent2: The lion is not big. sent3: If something visits the dog and it is kind then it visits the mouse. sent4: The dog chases the lion. sent5: If something is big then it visits the dog. sent6: The tiger eats the dog. sent7: If something visits the tiger and the tiger is not red then it is not nice. sent8: If something chases
the dog then it is not red. sent9: The mouse does not chase the tiger. sent10: If something visits the mouse then the mouse is red. sent11: The tiger visits the lion. sent12: The tiger does not eat the mouse. sent13: The mouse is nice. sent14: The lion does not eat the tiger. sent15: If the tiger visits the dog and the dog is not big then the dog chases the mouse. sent16: The lion visits the tiger. sent17: If something chases the lion and it visits the dog then it chases the dog. sent18: The dog is nice. sent19: If something chases the lion then it is big. sent20: If something eats the lion then it is not cold.
OUTPUT: \$answer\$ = True ; \$proof\$ = # sent7@int1 \&amp; sent16 # sent8@int2 # sent17@int3 \&amp; sent1 # sent5@int4 # sent19@int5 sent1 ; with int1: The lion is not nice. ; int2: The tiger is not red. ; int3: The tiger chases the dog. ; int4: The tiger visits the dog. ; int5: The tiger is big.</p>
<p>If we translate back from Polish Notation, this proof corresponds to ((sent16 \&amp; ((((sent1 \&amp; ((sent1 $\rightarrow$ sent $19 \rightarrow$ int5) $\rightarrow$ sent5 $\rightarrow$ int4)) $\rightarrow$ sent $17 \rightarrow$ int3)) $\rightarrow$ sent $8 \rightarrow$ int2)) $\rightarrow$ sent $7 \rightarrow$ int1)</p>
<h2>D. 2 Iterative Proof Generation (one step)</h2>
<p>In this task we ask the model to generate one valid implication of the theory with proof of depth 1. If no such implication exists, instead generate "None" as the answer.
INPUT: \$answer\$ ; \$proof\$ ; \$question\$ = What is one singlehop inference? ; \$context\$ = sent1: If something eats the cow and it is big then the cow sees the bald eagle. sent2: If something likes the bald eagle then it is rough. sent3: If something eats the dog then it likes the cow. sent4: Big things are young. sent5: If something likes the cow then it eats the cow. sent6: If something sees the bald eagle then the bald eagle eats the cow. sent7: If something likes the bald eagle then the bald eagle is kind. sent8: If something sees the bald eagle then the bald eagle eats the dog. sent9: The bald eagle eats the cow. sent10: The bald eagle sees the dog. sent11: The dog is big. sent12: The cow likes the bald eagle. sent13: The bald eagle is young. sent14: The dog sees the cow. sent15: The bald eagle is kind. sent16: The dog is young. sent17: The bald eagle sees the cow. sent18: The bald eagle is rough. sent19: The cow eats the bald eagle. sent20: The dog is cold. sent21: The dog likes the cow. sent22: The dog eats the bald eagle. sent23: The dog eats the cow. sent24: The bald eagle likes the dog. sent25: The bald eagle likes the cow. sent26: The cow sees the bald eagle. sent27: The cow sees the dog. OUTPUT: \$answer\$ = The cow is rough. ; \$proof\$ = # sent2 sent12</p>
<p>During evaluation time, we would then take this output and append it to the theory as "sent28: The cow is rough." and repeat the question. If a subsequent proof then references "sent28", we know already that this was derived using the proof "# sent2 sent12".</p>
<h2>D. 3 All-At-Once Implication Enumeration</h2>
<p>In this task we ask the model to simply output all valid implications of a theory, without generations proofs in this case.</p>
<p>INPUT: \$answer\$ ; \$proof\$ ; \$question\$ = What are all the inferences? ; \$context\$ = sent1: The tiger chases the lion. sent2: The lion is not big. sent3: If something visits the dog and it is kind then it visits the mouse. sent4: The dog chases the lion. sent5: If something is big then it visits the dog. sent6: The tiger eats the dog. sent7: If something visits the tiger and the tiger is not red then it is not nice. sent8: If something chases the dog then it is not red. sent9: The mouse does not chase the tiger. sent10: If something visits the mouse then the mouse is red. sent11: The tiger visits the lion. sent12: The tiger does not eat the mouse. sent13: The mouse is nice. sent14: The lion does not eat the tiger. sent15: If the tiger visits the dog and the dog is not big then the dog chases the mouse. sent16: The lion visits the tiger. sent17: If something chases the lion and it visits the dog then it chases the dog. sent18: The dog is nice. sent19: If something chases the lion then it is big. sent20: If something eats the lion then it is not cold.
OUTPUT: \$answer\$ = The dog is big. The tiger is big. The dog visits the dog. The tiger visits the dog. The dog chases the dog. The tiger chases the dog. The dog is not red. The tiger is not red. The lion is not nice.</p>
<p>This particular theory has 9 valid implications, around average for D5 theories. (The number of implications can be as high as 21).</p>
<h1>D. 4 Iterative Implication Enumeration</h1>
<p>Iterative implication generation is a side-effect of Iterative proof generation, hence the input/output (one step) is the same as in Appendix D.2.</p>
<h2>D. 5 Abduction</h2>
<p>In this task we ask the model to output all alternative missing facts, separated by a comma, that can be (separately) added to a theory so as to prove the given implication to be True.
INPUT: \$answer\$ ; \$question\$ = Dave is rough. ; \$context\$ = triple1: Anne is white. triple2: Charlie is young. triple3: Dave is round. triple4: Erin is quiet. rule1: If someone is rough and young then they are blue. rule2: Rough, white people are smart. rule3: All smart people are rough. rule4: All white people are smart. rule5: If someone is young then they are smart. rule6: All smart people are rough.
OUTPUT: \$answer\$ = Dave is young. , Dave is smart.
This particular abduction question has 2 alternative missing facts. Any one of them when added to the input theory is sufficient to prove the implication 'Dave is rough'.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://github.com/google-research/text-to-text-transfertransformer&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ The domain is small enough that all proofs can be enumerated. However, there still can be a large number, e.g., some D5 questions have over 3000 possible proofs.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
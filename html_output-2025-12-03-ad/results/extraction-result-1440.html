<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1440 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1440</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1440</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-a1f89d5b0d69f85e50c7872858afe3cc687a108d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a1f89d5b0d69f85e50c7872858afe3cc687a108d" target="_blank">Evolving scientific discovery by unifying data and background knowledge with AI Hilbert</a></p>
                <p><strong>Paper Venue:</strong> Nature Communications</p>
                <p><strong>Paper TL;DR:</strong> It is demonstrated that some famous scientific laws, including Kepler’s Law of Planetary Motion and the Radiated Gravitational Wave Power equation, can be derived in a principled manner from axioms and experimental data.</p>
                <p><strong>Paper Abstract:</strong> The discovery of scientific formulae that parsimoniously explain natural phenomena and align with existing background theory is a key goal in science. Historically, scientists have derived natural laws by manipulating equations based on existing knowledge, forming new equations, and verifying them experimentally. However, this does not include experimental data within the discovery process, which may be inefficient. We propose a solution to this problem when all axioms and scientific laws are expressible as polynomials and argue our approach is widely applicable. We model notions of minimal complexity using binary variables and logical constraints, solve polynomial optimization problems via mixed-integer linear or semidefinite optimization, and prove the validity of our scientific discoveries in a principled manner using Positivstellensatz certificates. We demonstrate that some famous scientific laws, including Kepler’s Law of Planetary Motion and the Radiated Gravitational Wave Power equation, can be derived in a principled manner from axioms and experimental data.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1440.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1440.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Hilbert (Evolving Scientific Discovery by Unifying Data and Background Knowledge with AI Hilbert)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated scientific-discovery system introduced in this paper that searches for polynomial laws simultaneously consistent with polynomial background theory and experimental data using sum-of-squares / semidefinite and mixed-integer optimization and returns formal Positivstellensatz certificates of derivability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A polynomial-optimization based discovery system that represents background knowledge as polynomial equalities/inequalities (a basic semialgebraic set), formulates discovery as minimizing data discrepancy plus a distance (d^c) to the background theory, encodes minimal complexity with binary/logical variables, and solves the resulting polynomial problems via sum-of-squares semidefinite programming or mixed-integer conic solvers. It outputs an implicit polynomial q(x)=0 together with SOS multipliers (α, β) that constitute a Positivstellensatz certificate proving (or approximately proving) derivability from axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Physics / Applied Mathematics (general physical laws expressible as polynomials); demonstrated on fluid mechanics, celestial mechanics, relativity, gravitational waves, quantum inequalities</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>AI-Hilbert is demonstrated to axiomatically derive or re-derive multiple known scientific laws by combining background axioms and (optionally) experimental data: Hagen–Poiseuille velocity profile for laminar flow, Radiated Gravitational Wave Power equation (Peters & Mathews form), Einstein's relativistic time-dilation relation, Kepler's Third Law, and Bell-type inequalities. For each derived law it returns the discovered implicit polynomial and the Positivstellensatz multipliers that certify (or quantify the discrepancy from) derivability from the supplied axioms.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td>new paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td>The paper frames AI-Hilbert as "a new paradigm for scientific discovery" (Section 1.2) arguing it unifies background theory and data and provides formal Positivstellensatz proofs; the authors describe it as a "first step towards discovering new laws of the universe" and emphasize a paradigm-level change (searching over polynomial certificates) rather than labeling particular derived equations as "transformational" versus "incremental."</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation is by demonstration: (1) formal derivations from background axioms alone (feasibility via SOS/SDP) and (2) minimization of a weighted objective combining sum |q(x_i)| over data and λ·d^c(q, theory) where d^c is the SOS-based coefficient distance to the background theory. Other evaluation elements: controlling certificate degree and complexity (sparsity/degree bounds), ablation-like experiments varying available axioms and data to illustrate data-vs-theory trade-offs, run-time / memory reporting for expensive instances (e.g., gravitational wave derivation required large SDP with many monomials), and comparison (qualitative and some quantitative) to prior systems on benchmark tasks (see literature review and appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation is performed by (a) exact symbolic matching/recognition of recovered implicit formulas to known, published physical laws (e.g., forms of Kepler's Third Law, Hagen–Poiseuille, time-dilation, gravitational wave power), (b) providing Positivstellensatz certificates (SOS multipliers α_i, β_j) that formally prove q belongs to the preprime generated by axioms when d^c=0, (c) approximate certificates and the residual polynomial r(x) when d^c>0 quantifying discrepancy, (d) demonstrations on synthetic and real/noisy data (e.g., light-clock experimental data), and (e) comparisons to prior automated discovery results (e.g., AI-Feynman, Eureqa) showing improved behavior in low-data / noisy settings when relevant theory is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is assessed by (i) the methodological contribution—using sum-of-squares / Positivstellensatz certificates to simultaneously enforce axiomatic derivability and data fit, enabling formal proofs as part of discovery, (ii) capability demonstrations: re-derivation of several non-trivial known laws (including examples earlier systems struggled with), and (iii) tractability claims (polynomial-time when degrees bounded and theory consistent). The paper positions novelty not as producing unknown laws in this work but as a new approach that can discover laws that are otherwise hard to find without such optimization tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Primary metrics reported are successful re-derivations of canonical laws (qualitative success), certificate distance d^c (SOS coefficient distance) to quantify exactness, data-fitting error (sum |q(x_i)|), complexity bounds (degrees, sparsity τ), number of axioms retained in best-subset selection in inconsistent settings, and computational resources / runtimes (e.g., problem with 416,392 monomials took 14368s to write and 6.58s solve with Mosek). No single scalar ‘‘impact score’’ is given; numeric evidence is the count of successfully derived target laws and reported runtimes/memory footprints.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper contrasts AI-Hilbert with the classical scientific method and data-driven symbolic regression: AI-Hilbert uses background theory to restrict search (thus often requiring less data than purely data-driven methods), produces formal derivations (unlike many ML approaches), and can identify inconsistent axioms via best-subset selection. It argues that with a complete, correct background theory no data is needed (deductive recovery), whereas human/classical methods would usually rely on manual manipulation; the comparison is conceptual and demonstrated by re-deriving human-known laws automatically. No controlled human-versus-system empirical study with statistical metrics is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>No aggregated percentage success rate is provided; success is reported qualitatively by successful axiomatic derivation of multiple canonical laws (Hagen–Poiseuille, Einstein time dilation, Kepler's Third Law, Radiated Gravitational Wave Power, Bell inequalities) under appropriate setups. The paper also reports experiments showing AI-Hilbert outperforms other methods on a testbed in Appendix A (numerical details are in the appendix, not summarized with a single rate in the main text).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Identified challenges: (1) tractability depends on bounding polynomial/certificate degree—unbounded degree makes the problem hard; (2) mixed-integer semidefinite instances (inconsistent theory / subset selection) are NP-hard in general; (3) memory and time can be large for high-degree multipliers (e.g., the gravitational-wave derivation required large memory and HPC resources); (4) correctness is conditional on correctness of the supplied background theory (formal certificates are relative to axioms); (5) degeneracy in implicit polynomial representations can produce multiple equivalent q's, requiring additional regularization or certificate-degree control to prefer canonical forms; (6) the paper does not claim to discover entirely novel, peer-reviewed breakthrough laws within this work—demonstrations are re-derivations of known laws.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving scientific discovery by unifying data and background knowledge with AI Hilbert', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1440.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1440.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Feynman (Udrescu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven symbolic regression method that combines neural networks, dimensional analysis and physics-inspired techniques to discover symbolic formulas from data; originally demonstrated on a benchmark of 100 physics formulas where it recovered all laws under a large, noiseless sampling regime.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Feynman: A Physics-Inspired Method for Symbolic Regression</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid method that uses neural-network fitting to identify simple functional structure, dimensional analysis, and symbolic regression heuristics to recover closed-form formulae from data. In the reference benchmark, it used large (100,000) noiseless samples per law to recover many target formulas.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Physics / symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Previously shown to recover a suite of canonical physics formulas (the Feynman benchmark) when supplied with large numbers of noiseless samples; noted in this paper as an example of a strong data-driven discovery system that is data-inefficient for realistic noisy/scarce regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated in prior work on the Feynman benchmark: success counted when the exact symbolic formula is recovered under a specified data sampling and noise regime (e.g., 100k noiseless observations per law). In this paper AI-Feynman is discussed in terms of its benchmark recovery counts.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation in cited work via benchmark recovery (exact symbolic matches) on synthetic noiseless datasets; in this paper it is critiqued by re-evaluation under limited-data/noisy regimes where performance degrades.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Paper treats AI-Feynman as representative of advanced data-driven discovery approaches; novelty assessed historically by the original authors (neural+physics heuristics) but here AI-Feynman is used to exemplify limitations (data inefficiency, sensitivity to sampling regime).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Recovery counts on Feynman benchmark are cited: AI-Feynman recovered 100/100 laws with 100,000 noiseless observations per law (original claim). Under limited-data (10 noisy observations) settings referenced in this paper, recovery drops significantly (Cornelio et al. results cited: AI-Feynman recovered 40/81 in limited-data noisy benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>The paper contrasts AI-Feynman with AI-Hilbert conceptually: AI-Feynman is data-driven and can hallucinate invalid laws when data is scarce or noisy and does not provide formal derivations tied to background axioms; AI-Hilbert integrates axiom-based proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported recovery: 100/100 on original noiseless, dense sampling benchmark; substantially lower in limited/noisy data (e.g., 40/81 in a cited limited-data test).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Identified limitations (as discussed in this paper): data inefficiency (requires large, sometimes unrealistic sampling regimes), sensitivity to noise and sampling ranges, agnosticism to background theory leading to larger search spaces and spurious/non-meaningful formulas, lack of formal proof certificates.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving scientific discovery by unifying data and background knowledge with AI Hilbert', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1440.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1440.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Eureqa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Eureqa (Schmidt & Lipson)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A symbolic-regression system based on genetic programming and heuristic search designed to distill free-form natural laws from experimental data; historically a widely used baseline for automated discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Distilling Free-Form Natural Laws from Experimental Data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Eureqa</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A heuristic genetic-programming based symbolic regression system that searches over expressions to fit data and was demonstrated to recover many known physical laws in early benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Physics / general empirical laws</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Earlier system shown to recover many symbolic relations from experimental datasets; in the Feynman benchmark it recovered 71/100 instances (as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluated by counting number of recovered benchmark laws (e.g., 71/100 on the Feynman dataset as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation by exact-match recovery on benchmark datasets (noiseless sampling regime in original demonstrations); compared to newer methods in the literature review.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Eureqa is cited historically as an important early system for automated law discovery; novelty relative to newer methods is discussed (heuristic vs optimization/certificate-based approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Benchmark recovery counts (71/100 on the referenced Feynman dataset) are cited as a point of comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Used in the paper as a historical baseline; the authors note that newer methods (e.g., AI-Feynman) improve recovery under ideal sampling, but that purely data-driven methods can fail in scarce/noisy settings.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported recovery: 71/100 on the referenced Feynman benchmark (as cited in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Heuristic search can yield spurious or non-derivable laws; no formal inference certificates; performance degrades in realistic low-data/noisy regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving scientific discovery by unifying data and background knowledge with AI Hilbert', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1440.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1440.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Descartes (Cornelio et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that iteratively generates candidate symbolic laws from data and tests whether they are derivable from background knowledge; it measures distance from theory but treats data-derived hypotheses and theory as disjoint components.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A system that proposes candidate symbolic formulae (e.g., via symbolic regression / mixed-integer nonlinear methods) from data, then tests those formulae against background theory and computes reasoning-based distances to quantify mismatch; it does not jointly learn from axioms and data.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Physics / symbolic regression with background theory</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>AI-Descartes is described as generating hypotheses from observed data and validating them against known theory, but it keeps theory and data separate and (as reported) fails to recover correct formulae in some settings because it does not learn from axioms and data simultaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Evaluation in the cited work and as discussed here involves testing derivability of candidate formulas against background axioms and computing distances (reasoning-based measures) to theory; in this paper AI-Descartes is cited as a prior approach that inspired the joint data-theory approach of AI-Hilbert.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation consists of derivability tests against provided theory and measurement of distance between data-derived formulas and theory; the paper notes AI-Descartes can measure mismatch but cannot always recover correct laws.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>AI-Descartes is recognized for integrating reasoning checks with data-driven hypothesis generation but is characterized as leaving theory and data disjoint, motivating AI-Hilbert which unifies them in a single optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Paper contrasts AI-Descartes with AI-Hilbert: AI-Descartes generates hypotheses from data and then tests them against theory (sequential), whereas AI-Hilbert jointly optimizes for data fit and derivability and returns formal certificates.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Not quantified in this paper; reported qualitatively as unable to recover correct formulae in some settings where AI-Hilbert can because it does not jointly learn from axioms and data.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>AI-Descartes keeps theory and data disjoint (generate-then-test workflow), leading to inability to recover correct formulae when joint inference from both sources is required.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evolving scientific discovery by unifying data and background knowledge with AI Hilbert', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AI Feynman: A Physics-Inspired Method for Symbolic Regression <em>(Rating: 2)</em></li>
                <li>Distilling Free-Form Natural Laws from Experimental Data <em>(Rating: 2)</em></li>
                <li>AI-Descartes <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1440",
    "paper_id": "paper-a1f89d5b0d69f85e50c7872858afe3cc687a108d",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "AI-Hilbert",
            "name_full": "AI-Hilbert (Evolving Scientific Discovery by Unifying Data and Background Knowledge with AI Hilbert)",
            "brief_description": "An automated scientific-discovery system introduced in this paper that searches for polynomial laws simultaneously consistent with polynomial background theory and experimental data using sum-of-squares / semidefinite and mixed-integer optimization and returns formal Positivstellensatz certificates of derivability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "AI-Hilbert",
            "system_description": "A polynomial-optimization based discovery system that represents background knowledge as polynomial equalities/inequalities (a basic semialgebraic set), formulates discovery as minimizing data discrepancy plus a distance (d^c) to the background theory, encodes minimal complexity with binary/logical variables, and solves the resulting polynomial problems via sum-of-squares semidefinite programming or mixed-integer conic solvers. It outputs an implicit polynomial q(x)=0 together with SOS multipliers (α, β) that constitute a Positivstellensatz certificate proving (or approximately proving) derivability from axioms.",
            "discovery_domain": "Physics / Applied Mathematics (general physical laws expressible as polynomials); demonstrated on fluid mechanics, celestial mechanics, relativity, gravitational waves, quantum inequalities",
            "discovery_description": "AI-Hilbert is demonstrated to axiomatically derive or re-derive multiple known scientific laws by combining background axioms and (optionally) experimental data: Hagen–Poiseuille velocity profile for laminar flow, Radiated Gravitational Wave Power equation (Peters & Mathews form), Einstein's relativistic time-dilation relation, Kepler's Third Law, and Bell-type inequalities. For each derived law it returns the discovered implicit polynomial and the Positivstellensatz multipliers that certify (or quantify the discrepancy from) derivability from the supplied axioms.",
            "discovery_type": "new paradigm",
            "discovery_type_justification": "The paper frames AI-Hilbert as \"a new paradigm for scientific discovery\" (Section 1.2) arguing it unifies background theory and data and provides formal Positivstellensatz proofs; the authors describe it as a \"first step towards discovering new laws of the universe\" and emphasize a paradigm-level change (searching over polynomial certificates) rather than labeling particular derived equations as \"transformational\" versus \"incremental.\"",
            "evaluation_methods": "Evaluation is by demonstration: (1) formal derivations from background axioms alone (feasibility via SOS/SDP) and (2) minimization of a weighted objective combining sum |q(x_i)| over data and λ·d^c(q, theory) where d^c is the SOS-based coefficient distance to the background theory. Other evaluation elements: controlling certificate degree and complexity (sparsity/degree bounds), ablation-like experiments varying available axioms and data to illustrate data-vs-theory trade-offs, run-time / memory reporting for expensive instances (e.g., gravitational wave derivation required large SDP with many monomials), and comparison (qualitative and some quantitative) to prior systems on benchmark tasks (see literature review and appendix).",
            "validation_approaches": "Validation is performed by (a) exact symbolic matching/recognition of recovered implicit formulas to known, published physical laws (e.g., forms of Kepler's Third Law, Hagen–Poiseuille, time-dilation, gravitational wave power), (b) providing Positivstellensatz certificates (SOS multipliers α_i, β_j) that formally prove q belongs to the preprime generated by axioms when d^c=0, (c) approximate certificates and the residual polynomial r(x) when d^c&gt;0 quantifying discrepancy, (d) demonstrations on synthetic and real/noisy data (e.g., light-clock experimental data), and (e) comparisons to prior automated discovery results (e.g., AI-Feynman, Eureqa) showing improved behavior in low-data / noisy settings when relevant theory is provided.",
            "novelty_assessment": "Novelty is assessed by (i) the methodological contribution—using sum-of-squares / Positivstellensatz certificates to simultaneously enforce axiomatic derivability and data fit, enabling formal proofs as part of discovery, (ii) capability demonstrations: re-derivation of several non-trivial known laws (including examples earlier systems struggled with), and (iii) tractability claims (polynomial-time when degrees bounded and theory consistent). The paper positions novelty not as producing unknown laws in this work but as a new approach that can discover laws that are otherwise hard to find without such optimization tooling.",
            "impact_metrics": "Primary metrics reported are successful re-derivations of canonical laws (qualitative success), certificate distance d^c (SOS coefficient distance) to quantify exactness, data-fitting error (sum |q(x_i)|), complexity bounds (degrees, sparsity τ), number of axioms retained in best-subset selection in inconsistent settings, and computational resources / runtimes (e.g., problem with 416,392 monomials took 14368s to write and 6.58s solve with Mosek). No single scalar ‘‘impact score’’ is given; numeric evidence is the count of successfully derived target laws and reported runtimes/memory footprints.",
            "comparison_to_human_discoveries": true,
            "comparison_details": "The paper contrasts AI-Hilbert with the classical scientific method and data-driven symbolic regression: AI-Hilbert uses background theory to restrict search (thus often requiring less data than purely data-driven methods), produces formal derivations (unlike many ML approaches), and can identify inconsistent axioms via best-subset selection. It argues that with a complete, correct background theory no data is needed (deductive recovery), whereas human/classical methods would usually rely on manual manipulation; the comparison is conceptual and demonstrated by re-deriving human-known laws automatically. No controlled human-versus-system empirical study with statistical metrics is reported.",
            "success_rate": "No aggregated percentage success rate is provided; success is reported qualitatively by successful axiomatic derivation of multiple canonical laws (Hagen–Poiseuille, Einstein time dilation, Kepler's Third Law, Radiated Gravitational Wave Power, Bell inequalities) under appropriate setups. The paper also reports experiments showing AI-Hilbert outperforms other methods on a testbed in Appendix A (numerical details are in the appendix, not summarized with a single rate in the main text).",
            "challenges_limitations": "Identified challenges: (1) tractability depends on bounding polynomial/certificate degree—unbounded degree makes the problem hard; (2) mixed-integer semidefinite instances (inconsistent theory / subset selection) are NP-hard in general; (3) memory and time can be large for high-degree multipliers (e.g., the gravitational-wave derivation required large memory and HPC resources); (4) correctness is conditional on correctness of the supplied background theory (formal certificates are relative to axioms); (5) degeneracy in implicit polynomial representations can produce multiple equivalent q's, requiring additional regularization or certificate-degree control to prefer canonical forms; (6) the paper does not claim to discover entirely novel, peer-reviewed breakthrough laws within this work—demonstrations are re-derivations of known laws.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1440.0",
            "source_info": {
                "paper_title": "Evolving scientific discovery by unifying data and background knowledge with AI Hilbert",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AI-Feynman",
            "name_full": "AI-Feynman (Udrescu et al.)",
            "brief_description": "A data-driven symbolic regression method that combines neural networks, dimensional analysis and physics-inspired techniques to discover symbolic formulas from data; originally demonstrated on a benchmark of 100 physics formulas where it recovered all laws under a large, noiseless sampling regime.",
            "citation_title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression",
            "mention_or_use": "mention",
            "system_name": "AI-Feynman",
            "system_description": "A hybrid method that uses neural-network fitting to identify simple functional structure, dimensional analysis, and symbolic regression heuristics to recover closed-form formulae from data. In the reference benchmark, it used large (100,000) noiseless samples per law to recover many target formulas.",
            "discovery_domain": "Physics / symbolic regression",
            "discovery_description": "Previously shown to recover a suite of canonical physics formulas (the Feynman benchmark) when supplied with large numbers of noiseless samples; noted in this paper as an example of a strong data-driven discovery system that is data-inefficient for realistic noisy/scarce regimes.",
            "discovery_type": "",
            "discovery_type_justification": "",
            "evaluation_methods": "Evaluated in prior work on the Feynman benchmark: success counted when the exact symbolic formula is recovered under a specified data sampling and noise regime (e.g., 100k noiseless observations per law). In this paper AI-Feynman is discussed in terms of its benchmark recovery counts.",
            "validation_approaches": "Validation in cited work via benchmark recovery (exact symbolic matches) on synthetic noiseless datasets; in this paper it is critiqued by re-evaluation under limited-data/noisy regimes where performance degrades.",
            "novelty_assessment": "Paper treats AI-Feynman as representative of advanced data-driven discovery approaches; novelty assessed historically by the original authors (neural+physics heuristics) but here AI-Feynman is used to exemplify limitations (data inefficiency, sensitivity to sampling regime).",
            "impact_metrics": "Recovery counts on Feynman benchmark are cited: AI-Feynman recovered 100/100 laws with 100,000 noiseless observations per law (original claim). Under limited-data (10 noisy observations) settings referenced in this paper, recovery drops significantly (Cornelio et al. results cited: AI-Feynman recovered 40/81 in limited-data noisy benchmark).",
            "comparison_to_human_discoveries": false,
            "comparison_details": "The paper contrasts AI-Feynman with AI-Hilbert conceptually: AI-Feynman is data-driven and can hallucinate invalid laws when data is scarce or noisy and does not provide formal derivations tied to background axioms; AI-Hilbert integrates axiom-based proofs.",
            "success_rate": "Reported recovery: 100/100 on original noiseless, dense sampling benchmark; substantially lower in limited/noisy data (e.g., 40/81 in a cited limited-data test).",
            "challenges_limitations": "Identified limitations (as discussed in this paper): data inefficiency (requires large, sometimes unrealistic sampling regimes), sensitivity to noise and sampling ranges, agnosticism to background theory leading to larger search spaces and spurious/non-meaningful formulas, lack of formal proof certificates.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1440.1",
            "source_info": {
                "paper_title": "Evolving scientific discovery by unifying data and background knowledge with AI Hilbert",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Eureqa",
            "name_full": "Eureqa (Schmidt & Lipson)",
            "brief_description": "A symbolic-regression system based on genetic programming and heuristic search designed to distill free-form natural laws from experimental data; historically a widely used baseline for automated discovery.",
            "citation_title": "Distilling Free-Form Natural Laws from Experimental Data",
            "mention_or_use": "mention",
            "system_name": "Eureqa",
            "system_description": "A heuristic genetic-programming based symbolic regression system that searches over expressions to fit data and was demonstrated to recover many known physical laws in early benchmarks.",
            "discovery_domain": "Physics / general empirical laws",
            "discovery_description": "Earlier system shown to recover many symbolic relations from experimental datasets; in the Feynman benchmark it recovered 71/100 instances (as reported in the paper).",
            "discovery_type": "",
            "discovery_type_justification": "",
            "evaluation_methods": "Evaluated by counting number of recovered benchmark laws (e.g., 71/100 on the Feynman dataset as cited).",
            "validation_approaches": "Validation by exact-match recovery on benchmark datasets (noiseless sampling regime in original demonstrations); compared to newer methods in the literature review.",
            "novelty_assessment": "Eureqa is cited historically as an important early system for automated law discovery; novelty relative to newer methods is discussed (heuristic vs optimization/certificate-based approaches).",
            "impact_metrics": "Benchmark recovery counts (71/100 on the referenced Feynman dataset) are cited as a point of comparison.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Used in the paper as a historical baseline; the authors note that newer methods (e.g., AI-Feynman) improve recovery under ideal sampling, but that purely data-driven methods can fail in scarce/noisy settings.",
            "success_rate": "Reported recovery: 71/100 on the referenced Feynman benchmark (as cited in this paper).",
            "challenges_limitations": "Heuristic search can yield spurious or non-derivable laws; no formal inference certificates; performance degrades in realistic low-data/noisy regimes.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1440.2",
            "source_info": {
                "paper_title": "Evolving scientific discovery by unifying data and background knowledge with AI Hilbert",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AI-Descartes",
            "name_full": "AI-Descartes (Cornelio et al.)",
            "brief_description": "An approach that iteratively generates candidate symbolic laws from data and tests whether they are derivable from background knowledge; it measures distance from theory but treats data-derived hypotheses and theory as disjoint components.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "AI-Descartes",
            "system_description": "A system that proposes candidate symbolic formulae (e.g., via symbolic regression / mixed-integer nonlinear methods) from data, then tests those formulae against background theory and computes reasoning-based distances to quantify mismatch; it does not jointly learn from axioms and data.",
            "discovery_domain": "Physics / symbolic regression with background theory",
            "discovery_description": "AI-Descartes is described as generating hypotheses from observed data and validating them against known theory, but it keeps theory and data separate and (as reported) fails to recover correct formulae in some settings because it does not learn from axioms and data simultaneously.",
            "discovery_type": "",
            "discovery_type_justification": "",
            "evaluation_methods": "Evaluation in the cited work and as discussed here involves testing derivability of candidate formulas against background axioms and computing distances (reasoning-based measures) to theory; in this paper AI-Descartes is cited as a prior approach that inspired the joint data-theory approach of AI-Hilbert.",
            "validation_approaches": "Validation consists of derivability tests against provided theory and measurement of distance between data-derived formulas and theory; the paper notes AI-Descartes can measure mismatch but cannot always recover correct laws.",
            "novelty_assessment": "AI-Descartes is recognized for integrating reasoning checks with data-driven hypothesis generation but is characterized as leaving theory and data disjoint, motivating AI-Hilbert which unifies them in a single optimization.",
            "impact_metrics": "",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Paper contrasts AI-Descartes with AI-Hilbert: AI-Descartes generates hypotheses from data and then tests them against theory (sequential), whereas AI-Hilbert jointly optimizes for data fit and derivability and returns formal certificates.",
            "success_rate": "Not quantified in this paper; reported qualitatively as unable to recover correct formulae in some settings where AI-Hilbert can because it does not jointly learn from axioms and data.",
            "challenges_limitations": "AI-Descartes keeps theory and data disjoint (generate-then-test workflow), leading to inability to recover correct formulae when joint inference from both sources is required.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1440.3",
            "source_info": {
                "paper_title": "Evolving scientific discovery by unifying data and background knowledge with AI Hilbert",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AI Feynman: A Physics-Inspired Method for Symbolic Regression",
            "rating": 2
        },
        {
            "paper_title": "Distilling Free-Form Natural Laws from Experimental Data",
            "rating": 2
        },
        {
            "paper_title": "AI-Descartes",
            "rating": 2
        }
    ],
    "cost": 0.017800749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Evolving Scientific Discovery by Unifying Data and Background Knowledge with AI Hilbert</h1>
<p>Ryan Cory-Wright*<br>Department of Analytics, Marketing and Operations, Imperial College Business School, London, UK<br>r.cory-wright@imperial.ac.uk</p>
<p>Cristina Cornelio<br>Samsung AI, Cambridge, UK<br>c.cornelio@samsung.com<br>Sanjeeb Dash<br>IBM Thomas J. Watson Research Center<br>Yorktown Heights, USA<br>sanjeebd@us.ibm.com<br>Bachir El Khadir<br>IBM Thomas J. Watson Research Center<br>Yorktown Heights, USA<br>bachir009@gmail.com<br>Lior Horesh<br>IBM Thomas J. Watson Research Center<br>Yorktown Heights, USA<br>lhoresh@us.ibm.com</p>
<h4>Abstract</h4>
<p>The discovery of scientific formulae that parsimoniously explain natural phenomena and align with existing background theory is a key goal in science. Historically, scientists have derived natural laws by manipulating equations based on existing knowledge, forming new equations, and verifying them experimentally. In recent years, data-driven scientific discovery has emerged as a viable competitor in settings with large amounts of experimental data. Unfortunately, data-driven methods often fail to discover valid laws when data is noisy or scarce. Accordingly, recent works combine regression and reasoning to eliminate formulae inconsistent with background theory. However, the problem of searching over the space of formulae consistent with background theory to find one that best fits the data is not wellsolved. We propose a solution to this problem when all axioms and scientific laws are expressible via polynomial equalities and inequalities and argue that our approach is widely applicable. We model notions of minimal complexity using binary variables and logical constraints, solve polynomial optimization problems via mixed-integer linear or semidefinite optimization, and prove the validity of our scientific discoveries in a principled manner using Positivstellensatz certificates. The optimization techniques leveraged in this paper allow our approach to run in polynomial time with fully correct background theory (under an assumption that the complexity of our derivation is bounded), or non-deterministic polynomial (NP) time with partially correct background theory. We demonstrate that some famous scientific laws, including Kepler's Third Law of Planetary Motion, the Hagen-Poiseuille Equation, and the Radiated Gravitational Wave Power equation, can be derived in a principled manner from axioms and experimental data.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1 Introduction</p>
<p>A fundamental problem in science involves explaining natural phenomena in a manner consistent with noisy experimental data and a body of potentially inexact and incomplete background knowledge about the universe’s laws [39]. In the past few centuries, The Scientific Method [101] has led to significant progress in discovering new laws. Unfortunately, the rate of emergence of these laws and their contribution to economic growth is stagnating relative to the amount of capital invested in deducing them [27, 21]. Indeed, Dirac [41] noted that it is now more challenging for first-rate physicists to make second-rate discoveries than it was previously for second-rate physicists to make first-rate ones, while Arora et al. [7] found that the marginal value of scientific discoveries to large companies has declined since the fall of the Berlin Wall. Moreover, Bloom et al. [24] have found that research productivity in the United States halves every thirteen years because good scientific ideas are getting harder to find. This phenomenon can be partly explained by analogy to [33]: The Scientific Method has picked most of the “low-hanging fruit” in science, such as natural laws that relate physical quantities using a small number of low-degree polynomials. This calls for more disciplined and principled alternatives to The Scientific Method, which integrate background information and experimental data to generate and verify higher dimensional laws of nature, thereby promoting scientific discovery [c.f. 69, 110]. Accordingly, Figure 1 provides an overview of these alternatives.</p>
<p>Even as the rate of scientific discovery has decreased, the scalability of global optimization methods has significantly improved. Indeed, as we argue in this paper, global optimization methods are now a mature technology capable of searching over the space of scientific laws — owing to Moore’s law and significant theoretical and computational advances by the optimization community [see 22, 58, 15, for reviews]. Indeed, Bertsimas and Dunn [18, Chap. 1] observed that the speedup in raw computing power between 1991 and 2015 is at least six orders of magnitude. Additionally, polynomial optimization has become much more scalable since the works of Lasserre [74] and Parrilo [87], and primal-dual interior-point methods [84, 92, 102] have improved considerably, with excellent implementations now available in, for example, the Hosek solver [6]. Indeed, even methods for non-convex quadratically constrained problems have achieved machine-independent speedups of nearly 200 since their integration within commercial solvers in 2019 [1, 58].</p>
<p>In this paper, we propose a new approach to scientific discovery that leverages these advances by the optimization community. Given a set of background axioms, theorems, and laws expressible as a basic semialgebraic set (i.e., a system of polynomial equalities and inequalities) and observations from experimental data, we derive new laws representable as polynomial expressions that are either exactly or approximately consistent with existing laws and experimental data by solving polynomial optimization problems via linear and semidefinite optimization. By leveraging fundamental results from real algebraic geometry, we obtain formal proofs of the correctness of our laws as a byproduct of the optimization problems. This is notable, because existing automated approaches to scientific discovery, as reviewed in Section 1.1, often rely upon deep learning techniques that do not provide formal proofs and are prone to “hallucinating” incorrect scientific laws that cannot be automatically proven or disproven, analogously to output from state-of-the-art Large Language Models such as GPT-4 [86]. As such, any new laws derived by these systems cannot easily be explained or justified.</p>
<p>Conversely, our approach discovers new scientific laws by solving an optimization problem to minimize a weighted sum of discrepancies between the proposed law and experimental data, plus the distance between the proposed law and its projection onto the set of symbolic laws derivable from background theory. As a result, our approach discovers scientific laws alongside a proof of their consistency with existing background theory by default. Moreover, our approach is scalable; it runs in polynomial time with respect to the number of symbolic variables and axioms (when the degree of the polynomial certificates we search over is bounded; see Section 2.2) with a complete and consistent background theory.</p>
<p>We believe our approach could be a first step towards discovering new laws of the universe which involve higher degree polynomials and are impractical for scientists to discover without the aid of modern optimization solvers and high-performance computing environments. Further, our approach is potentially useful for reconciling mutually inconsistent axioms. Indeed, if a system of scientific laws is mutually inconsistent (in the sense that no point satisfies all laws simultaneously), our polynomial optimization problem offers a formal proof of its inconsistency.</p>
<h1>1.1 Literature Review</h1>
<p>We propose an approach to scientific discovery, which we term AI-Hilbert, that uses polynomial optimization to obtain scientific formulae derivable from background theory axioms and consistent with experimental data. This differs from existing works on scientific discovery that express prior knowledge as constraints on the functional form of a learned model (e.g., shape constraints such as monotonicity [38]). Indeed, shape-constrained approaches to scientific discovery have been proposed [71, 72, 43, 38, 19], while discovering scientific laws that are simultaneously derivable from prior knowledge expressed as polynomials and experimental data is, to our knowledge, a new approach.
AI-Hilbert builds upon two areas of the optimization and discovery literature typically considered in isolation: sum-of-squares optimization techniques for solving polynomial optimization problems, and data-driven techniques for symbolic discovery. We now review the relevant literature.</p>
<p>Sum-of-Squares Optimization: Sum-of-squares optimization has been an important component of global optimization methods since the seminal work of Parrilo [87] (see also Lasserre [74]), which combines two key observations. First, sum-of-squares decompositions of multivariate polynomials can be computed via semidefinite optimization, so optimizing over sum-of-squares polynomials is no harder than performing semidefinite optimization. Second, owing to a fundamental result from real algebraic geometry, the Positivstellensatz [70, 103, 90], polynomials of bounded degree defined on basic semialgebraic sets can be certified as non-negative over these sets by representing them
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of scientific discovery paradigms. Traditional scientific discovery formulates hypotheses using existing theory and observed phenomena. These hypotheses are validated and tested using data. In contrast, machine learning methods rely on large datasets to identify patterns. AIDescartes [32] proposes an inversion of the conventional scientific discovery paradigm. It generates hypotheses from observed data and validates them against known theory. However, in AI-Descartes, theory and data remain disjoint and do not mutually enhance one another. In contrast, our work, AI Hilbert, combines data and theory to formulate hypotheses. Unlike conventional methods, insights in data and knowledge embedded in the theory collaboratively reduce the search space. These two components complement each other: theory compensates for noisy or sparse data, while data compensates for inconsistent or incomplete theory. Note that blue denotes components associated with data, purple denotes components linked to theory, and dashed lines represent iterative processes.</p>
<p>as systems of sum-of-squares polynomials (see Section 1.3). Consequently, optimizing over a real polynomial system is equivalent to solving a (larger) sum-of-squares optimization problem, and thus a tractable convex problem. These observations have allowed an entire field of optimization to blossom; see Blekherman et al. [23], Hall [59] for reviews. However, to our knowledge, no works have proposed using sum-of-squares optimization to discover scientific formulae. The closest works are Clegg et al. [31], who propose using Gröbner bases to design proofs of unsatisfiability, Curmei and Hall [38], who propose a sum-of-squares approach to fitting a polynomial to data under very general constraints on the functional form of the polynomial, e.g., non-negativity of the derivative over a box, Ahmadi and El Khadir [3], who propose learning the behavior of noisy dynamical systems via semialgebraic techniques, and Fawzi et al. [45], who propose learning proofs of optimality of stable set problems by combining reinforcement learning with the Positivstellensatz. However, determining whether polynomial optimization is useful for scientific discovery is, to our knowledge, open.</p>
<p>Data-Driven Approaches to Scientific Discovery: The availability of large amounts of scientific data generated and collected over the past few decades has spurred increasing interest in data-driven methods for scientific discovery that aim to identify symbolic equations that accurately explain high-dimensional datasets. Bongard and Lipson [25] and Schmidt and Lipson [98] proposed using heuristics and genetic programming to discover scientifically meaningful formulae, and implement their approach in the Eureqa software system [42]. Other proposed approaches are based on mixed-integer global optimization [10, 35], sparse regression [26, 96, 19], Cylindrical Algebraic Decomposition [50], neural networks [65, 73], and Bayesian Markov Chain Monte Carlo approaches [56]. See [68, 13] for reviews of data-driven scientific discovery in fundamental physics and chemistry.</p>
<p>Data-driven approaches have been shown by several authors to perform well in highly overdetermined settings with limited amounts of noise. For instance, Udrescu et al. [107, 106] proposed a method called AI-Feynman, which combines neural networks with physics-based techniques to discover symbolic formulae. Moreover, they constructed a benchmark dataset of 100 scientific laws derived from Richard Feynman's lecture notes [47], with 100, 000 noiseless experimental observations of each scientific law, and demonstrated that while the Eurequa system could recover an already impressive $71 / 100$ instances from the data, their approach could recover all one hundred; see Cornelio et al. [32] for a review of scientific discovery systems.</p>
<p>Unfortunately, data-driven approaches to scientific discovery have at least three significant drawbacks. First, they are not data efficient [49] and only reliably recover scientific formulae in overdetermined settings with orders of magnitude more data than a human would likely need to make the same discoveries. Indeed, Matsubara et al. [83] recently argued that the sampling regime used by AI-Feynman is unrealistic, because it samples values far from those observable in the real world. Moreover, Cornelio et al. [32] recently benchmarked AI-Feyman on 81 of the 100 laws, but with 10 (rather than 100,000 ) observations per law, and where each experimental observation is contaminated with a small amount of noise. In this limited data setting, Cornelio et al. [32] found that AI-Feynman recovered 40 of the 81 laws considered, whereas [32] were able to recover $49 / 81$ laws using their symbolic regression solver. This performance degradation is a significant issue in practice because scientific data is typically expensive to obtain and scarce and noisy. Second, purely data-driven methods are agnostic to important background information, such as existing literature, that valid scientific formulae should be consistent with unless there is extraordinary experimental evidence that the literature is incorrect. This implies that data-driven methods search over a larger space of laws than is necessary, require more data than a human would need to derive a valid law, and frequently propose laws that are not scientifically meaningful. Third, data-driven methods typically do not provide interpretable explanations for why their discoveries are valid [c.f. 95], which makes diagnosing whether their discoveries are consistent with existing theory challenging.
To account for background theory in scientific discovery, Cornelio et al. [32] recently proposed an approach called AI-Descartes, which iteratively generates plausible scientific formulae using a mixed-integer nonlinear symbolic regression solver [see also 10], and tests whether these formulae are derivable from the background knowledge. If they are not, the method provides a set of reasoningbased measures to compute how distant the formulae induced from the data are from the background theory but is unable to recover the correct formulae. This is because their approach induces potential scientific laws from data and subsequently tests the hypothesis against the background theory, rather than learning from axioms and data simultaneously.</p>
<h1>1.2 Contributions and Structure</h1>
<p>We propose a new paradigm for scientific discovery that derives polynomial laws simultaneously consistent with experimental data and a body of background knowledge expressible as polynomial equalities and inequalities. We term our approach AI-Hilbert, inspired by the work of David Hilbert, one of the first mathematicians to investigate the relationship between sum-of-squares and non-negative polynomials [62].
Our approach automatically provides an axiomatic derivation of the correctness of a discovered scientific law, conditional on the correctness of our background theory. Moreover, in instances with inconsistent background theory, our approach can successfully identify the sources of inconsistency by performing best subset selection to determine the axioms that best explain the data. This is notably different from current data-driven approaches to scientific discovery, which often generate spurious laws in limited data settings and fail to differentiate between valid and invalid discoveries or provide explanations of their derivations. We illustrate our approach by axiomatically deriving some of the most frequently cited natural laws in the scientific literature, including Kepler's Third Law and Einstein's Relativistic Time Dilation Law, among other scientific discoveries.
A second contribution of our approach is that it permits fine-grained control of the tractability of the scientific discovery process, by bounding the degree of the coefficients in the Positivstellensatz certificates that are searched over (see Section 1.3, for a formal statement of the Positivstellensatz). This differs from prior works on automated scientific discovery, which offers more limited control over their time complexity. For instance, in the special case of scientific discovery with a complete body of background theory and no experimental data, to our knowledge, the only current alternative to our approach is symbolic regression [see, e.g., 35], which requires genetic programming or mixedinteger nonlinear programming techniques that are not guaranteed to run in polynomial time. On the other hand, our approach searches for polynomial certificates of a bounded degree via a fixed level of the sum-of-squares hierarchy [74, 87], which can be searched over in polynomial time [84, 91].
To contrast our approach with existing approaches to scientific discovery, Figure 2 depicts a stylized version of the scientific method. In this version, new laws of nature are proposed from background theory (which may be written down by humans, automatically extracted from existing literature, or even generated using AI [67]) and experimental data, using classical discovery techniques, datadriven techniques, or AI-Hilbert. Observe that data-driven discoveries may be inconsistent with background theory, and discoveries via classical methods may not be consistent with relevant data sources, while discoveries made via AI-Hilbert are consistent with background theory and relevant data sources. This suggests that AI-Hilbert could be a first step toward discovery frameworks that are less likely to make false discoveries. Moreover, as mentioned in the introduction, AI-Hilbert uses background theory to restrict the effective dimension of the set of possible scientific laws, and, therefore, likely requires less data to make scientific discoveries than purely data-driven approaches.</p>
<h3>1.3 Background and Notation</h3>
<p>The notation is mostly standard to the semidefinite and polynomial optimization literature. We let non-boldface characters such as $b$ denote scalars, lowercase bold-faced characters such as $\boldsymbol{x}$ denote vectors, uppercase bold-faced characters such as $\boldsymbol{A}$ denote matrices, and calligraphic uppercase characters such as $\mathcal{Z}$ denote sets. We let $[n]$ denote the set of indices ${1, \ldots, n}$. We let $\boldsymbol{e}$ denote the vector of ones, $\mathbf{0}$ denote the vector of all zeros, and $\mathbb{I}$ denote the identity matrix. We let $|\boldsymbol{x}|<em _="+">{p}$ denote the $p$-norm of a vector $\boldsymbol{x}$ for $p \geq 1$. We let $\mathbb{R}$ denote the real numbers, $\mathcal{S}^{n}$ denote the cone of $n \times n$ symmetric matrices, and $\mathcal{S}</em>$ denote the cone of $n \times n$ positive semidefinite matrices.
We also use some notations specific to the sum-of-squares (SOS) optimization literature; see [34] for an introduction to computational algebraic geometry and [23] for a general theory of sum-of-squares and convex algebraic optimization. Specifically, we let $\mathbb{R}[\boldsymbol{x}]}^{n<em 2="2" d="d" n_="n,">{n, 2 d}$ denote the ring of real polynomials in the $n$-tuple of variables $\boldsymbol{x} \in \mathbb{R}^{n}$ of degree $2 d, P</em>}$ denote the convex cone of non-negative polynomials in $n$ variables of degree $2 d$, and}:=\left{p \in \mathbb{R} \mid \boldsymbol{x}\right}_{n, 2 d}: p(\boldsymbol{x}) \geq 0 \quad \forall \boldsymbol{x} \in \mathbb{R}^{n</p>
<p>$$
\Sigma[\boldsymbol{x}]<em i="i">{n, 2 d}:=\left{p(\boldsymbol{x}): \exists q</em>]}, \ldots, q_{m} \in \mathbb{R}[\boldsymbol{x<em i="1">{n, d}, p(\boldsymbol{x})=\sum</em>)\right}
$$}^{m} q_{i}^{2}(\boldsymbol{x</p>
<p>denote the cone of sum-of-squares polynomials in $n$ variables of degree $2 d$, which can be optimized over via $\binom{n+d}{d}$ dimensional semidefinite matrices [c.f. 87] using interior point methods [84]. Note</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The scientific method with scientific discoveries made via classical methods, datadriven methods, or AI-Hilbert. AI-Hilbert proposes scientific laws consistent with a body of background theory formally articulated as polynomial equalities, inequalities, and relevant data sources. This likely allows scientific discoveries to be made using fewer data points than state-of-theart approaches, and for missing scientific axioms to be deduced via abductive reasoning as part of the scientific discovery process. On the other hand, existing approaches to scientific discovery propose laws that may be inconsistent with either background theory or existing data sources.</p>
<p>that $\Sigma[\boldsymbol{x}]<em 2="2" d="d" n_="n,">{n, 2 d} \subseteq P</em>]}$, and the inclusion is strict unless $n \leq 2,2 d \leq 2$ or $n=3,2 d=4$ [61]. Nonetheless, $\Sigma[\boldsymbol{x<em 2="2" d="d" n_="n,">{n, 2 d}$ provides a high-quality approximation of $P</em>$ norm of its coefficient vector) to any desired accuracy $\epsilon&gt;0$ by a sequence of sum-of-squares [75]. If the maximum degree $d$ is unknown, we suppress the dependence on $d$ in our notation.
To define a notion of distance between polynomials, we also use several functional norms. Let $|\cdot|}$, since each non-negative polynomial can be approximated (in the $\ell_{1<em p="p">{p}$ denote the $\ell</em>}$ norm of a vector. Let $\boldsymbol{\mu} \in \mathbb{N}^{n}$ be the vector $\left(\mu_{1}, \ldots, \mu_{n}\right)$ and $\boldsymbol{x}^{\boldsymbol{\mu}}$ stand for the monomial $x_{1}^{\mu_{1}} \ldots x_{n}^{\mu_{n}}$. Then, for a polynomial $q \in \mathbb{R<em _boldsymbol_mu="\boldsymbol{\mu">{n, 2 d}[\boldsymbol{x}]$ with the decomposition $q(\boldsymbol{x})=$ $\sum</em>|} \in \mathbb{N}^{n}:|\boldsymbol{\mu<em _boldsymbol_mu="\boldsymbol{\mu">{1} \leq 2 d} a</em>|}} \boldsymbol{x}^{\boldsymbol{\mu}}$, we let the notation $|\boldsymbol{q<em _boldsymbol_mu="\boldsymbol{\mu">{p}=\left(\sum</em>|} \in \mathbb{N}^{n}:|\boldsymbol{\mu<em _boldsymbol_mu="\boldsymbol{\mu">{1} \leq 2 d} \alpha</em>$ denote the coefficient norm of the polynomial,
Finally, to derive new laws of nature from existing ones, we repeatedly invoke a fundamental result from real algebraic geometry called the Positivstellensatz [see, e.g., 103]. Various versions of the Positivstellensatz exist, with stronger versions holding under stronger assumptions [see 76, for a review], and any reasonable version being a viable candidate for our approach. For simplicity, we invoke a compact version due to [90], which holds under some relatively mild assumptions but nonetheless lends itself to relatively tractable optimization problems:}}^{\mu}\right)^{1 / p</p>
<p>Theorem 1 (Putinar's Positivstellensatz [90], see also Theorem 5.1 of [87]) Consider the basic (semi)algebraic sets</p>
<p>$$
\begin{aligned}
\mathcal{G} &amp; :=\left{\boldsymbol{x} \in \mathbb{R}^{n}: g_{1}(\boldsymbol{x}) \geq 0, \ldots, g_{k}(\boldsymbol{x}) \geq 0\right} \
\mathcal{H} &amp; :=\left{\boldsymbol{x} \in \mathbb{R}^{n}: h_{1}(\boldsymbol{x})=0, \ldots h_{l}(\boldsymbol{x})=0\right}
\end{aligned}
$$</p>
<p>where $g_{i}, h_{j} \in \mathbb{R}[x]<em 0="0">{n}$, and $\mathcal{G}$ satisfies the Archimedean property ${ }^{2}$ [see also 23, Chap. 6.4.4], i.e., there exists an $R&gt;0$ and $\alpha</em>]}, \ldots \alpha_{k} \in \Sigma[\boldsymbol{x<em i="1">{n}$ such that $R-\sum</em>$, the implication}^{n} x_{i}^{2}=\alpha_{0}(\boldsymbol{x})+\sum_{i=1}^{k} \alpha_{i}(\boldsymbol{x}) g_{i}(\boldsymbol{x})$. Then, for any $f \in \mathbb{R}[x]_{n, 2 d</p>
<p>$$
\boldsymbol{x} \in \mathcal{G} \cap \mathcal{H} \Longrightarrow f(\boldsymbol{x}) \geq 0
$$</p>
<p>holds if and only if there exist SOS polynomials $\alpha_{0}, \ldots, \alpha_{k} \in \Sigma[\boldsymbol{x}]<em 1="1">{n, 2 d}$, and real polynomials $\beta</em>$ such that}, \ldots, \beta_{l} \in \mathbb{R}[\boldsymbol{x}]_{n, 2 d</p>
<p>$$
f(x)=\alpha_{0}(x)+\sum_{i=1}^{k} \alpha_{i}(\boldsymbol{x}) g_{i}(\boldsymbol{x})+\sum_{j=1}^{l} \beta_{j}(\boldsymbol{x}) h_{j}(\boldsymbol{x})
$$</p>
<p>Note that strict polynomial inequalities of the form $h_{i}(x)&gt;0$ can be modeled by introducing an auxiliary variable $\tau$ and requiring that $h_{i}(x) \tau^{2}-1=0$, and thus our focus on non-strict inequalities in Theorem 1 is without loss of generality [see also 23].
Remarkably, the Positivstellensatz implies that if we set the degree of $\alpha_{i}$ s to be zero, then polynomial laws consistent with a set of equality-constrained polynomials can be searched over via linear optimization. Indeed, this subset of laws is sufficiently expressive that, as we demonstrate in our numerical results, it allows us to recover Kepler's third law and Einstein's dilation law axiomatically. Moreover, the set of polynomial natural laws consistent with polynomial (in)equalities can be searched via semidefinite or sum-of-squares optimization.
We close this section by remarking that one could develop an alternative version of the Positivstellensatz with only inequality constraints, by expressing each equality via two inequalities. However, this increases the number of decision variables in the optimization problems generated by the Positivstellensatz and solved in this paper, and thus decreases the tractability of these optimization problems; see also [23]. Accordingly, we treat equality and inequality constraints separately throughout.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>1.4 Structure</p>
<p>The rest of the paper is organized as follows: in Section 2, we describe AI-Hilbert, the scientific discovery system proposed in this paper. In Section 3, we argue that it presents an exciting new approach to scientific discovery, by demonstrating that it can rediscover the Hagen-Poiseuille Equation, Einstein’s Relativistic Time Dilation Law, Kepler’s Third Law, the Radiated Gravitational Wave Power Equation, and the Bell Inequalities (additionally, a detailed comparison of AI-Hilbert with state-of-the-art approaches in Appendix A and examples of scientific discovery via AI-Hilbert in Appendix B). In Section 4, we summarize our conclusions and discuss the limitations of and opportunities arising from this work.</p>
<h2>2 Discovering Scientific Formulae Via Polynomial Optimization</h2>
<p>In this section, we formally introduce AI-Hilbert. First, in Section 2.1, we provide an overview of our approach. Next, in Section 2.2, we formalize our approach as a polynomial optimization problem. Further, in Section 2.3, we define a new notion of the distance between a polynomial and a (possibly inconsistent or incomplete) set of polynomial background knowledge axioms. Subsequently, in Section 2.4, we discuss the role of background theory on the amount of data required to discover scientific laws, by revisiting some well-studied examples from the machine learning and statistics literature. Finally, in Section 2.5, we specialize our approach to problem settings where a scientist has access to a complete background theory and no experimental data.</p>
<h3>2.1 Method Overview</h3>
<p>Our scientific discovery method (AI-Hilbert) aims to discover an unknown polynomial formula $q(\cdot)\in\mathbb{R}[x]$ which describes a physical phenomenon, and is both consistent with a background theory of polynomial equalities and inequalities $\mathcal{B}$ (a set of axioms) and a collection of experimental data $\mathcal{D}$ (defined below). We provide a high-level overview of AI-Hilbert in Figure 3 and summarize our procedure in Algorithm 1. The inputs to AI-Hilbert are a four-tuple $(\mathcal{B},\mathcal{D},\mathcal{C}(\Lambda),d^{c})$, where:</p>
<ul>
<li>$\mathcal{B}$ denotes the relevant background theory, expressed as a collection of axioms, in the scientific discovery setting, i.e., the polynomial laws relevant for discovering $q$. It is the union of the inequalities ${g_{1}(\boldsymbol{x})\geq 0,\ldots,g_{k}(\boldsymbol{x})\geq 0}$ defining $\mathcal{G}$ and the equalities ${h_{1}(\boldsymbol{x})=0,\ldots,h_{l}(\boldsymbol{x})=0}$ defining $\mathcal{H}$ – where $\mathcal{G}$ and $\mathcal{H}$ are as in (1) and (2), respectively. $\mathcal{B}$ is defined over $n$ variables $x_{1},\ldots,x_{n}$. However, only $t$ of these $n$ variables can be measured and are directly relevant for explaining the observed phenomenon. In particular, we let $x_{1}$ denote the dependent variable. The</li>
</ul>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Schematic illustration of AI Hilbert and its components. Using background knowledge encoded as multivariate polynomials, experimental data, and hyperparameters (e.g., a sparsity constraint on the background theory) to control our model’s complexity, we formulate scientific discovery as a polynomial optimization problem, reformulate it as a semidefinite optimization problem, and solve it to obtain both a symbolic model and its formal derivation. Dashed boxes correspond to optional components. An example is introducing an incorrect candidate formula as a new axiom in the background theory.</p>
<p>remaining $n-t$ variables appear in the background theory but are not directly observable ${ }^{3}$. The background theory $\mathcal{B}$ is defined as complete if it contains all the axioms necessary to formally prove the target formula, incomplete otherwise. Moreover, $\mathcal{B}$ is called inconsistent if it contains axioms that contradict each other, consistent otherwise (we define these terms more rigorously in Section 2.3). A special case of inconsistency is when a formula that incorrectly describes the studied phenomenon is added to a consistent background theory (see Section 3.6).</p>
<ul>
<li>$\mathcal{D}:=\left{\overline{\boldsymbol{x}}<em _in_n="\in[n" i="i" t_="t]">{i}\right}</em>}$ denotes a collection of data points, or measurements of an observed physical phenomenon, which may be contaminated by noise, e.g., from measurement error. We assume that $\overline{\boldsymbol{x}<em i_="i," j="j">{i} \in \mathbb{R}^{n}$ and $\overline{\boldsymbol{x}}</em>}=0$ for $j \geq t+1$, i.e., the value of $\overline{\boldsymbol{x}<em i="i">{i, j}-$ the $j$ th entry of $\overline{\boldsymbol{x}}</em>-$ is set to zero for all variables $j$ that cannot or should not be measured.</li>
<li>$\mathcal{C}$ denotes a set of constraints and bounds which depend on a set of hyper-parameters $\Lambda$. Specifically, we consider a global bound on the degree of the polynomial $q$; a vector $\boldsymbol{d}$ restricting individual variable degrees in $q$; a hyperparameter $\lambda$ that models the fidelity to background theory and data; and a bound over the number of axioms that should be included in a formula derivation.</li>
<li>$d^{c}(\cdot, \mathcal{G} \cap \mathcal{H})$ denotes a distance function that defines the distance from an arbitrary polynomial to the background theory. We formally define $d^{c}$ in Section 2.3.</li>
</ul>
<p>Algorithm 1 provides a high-level description of AI-Hilbert. The procedure first combines the background theory $\mathcal{B}$ and data $\mathcal{D}$ to generate a polynomial optimization problem $\operatorname{Pr}$ which targets a specific concept identified by a dependent - or target - variable, included in the set of observable entities that can be measured in the environment $\left(x_{1}, \ldots, x_{t}\right)$. This is achieved by leveraging the distance $d^{c}$ (formally defined in Section 2.3) and integrating the bounds and constraints $\mathcal{C}$ (with their hyperparameters $\Lambda$ ) via the PolyJuMP.jl Julia package [82]. This corresponds to the Formulate step of Algorithm 1, which we formalize in Section 2.2.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 AI Hilbert for Scientific Discovery
Input: \(\left(\mathcal{B}, \mathcal{D}, \mathcal{C}(\Lambda), d^{c}\right)\)
    \(\mathbb{P}_{\mathbb{P}} \leftarrow\) Formulate \(\left(\mathcal{B}, \mathcal{D}, \mathcal{C}(\Lambda), d^{c}\right)\)
    \(\mathbb{P}_{\mathbb{P}}{ }^{\text {ad }} \leftarrow\) Reduce \((\mathbb{P}_{\mathbb{P}}\)
    \(q(\boldsymbol{x}) \leftarrow \operatorname{Solve}\left(\mathbb{P}_{\mathbb{P}}{ }^{\text {ad }}\right)\)
Output: \(q(\boldsymbol{x})=0\)
Output: \(\boldsymbol{\alpha}, \boldsymbol{\beta}\)
</code></pre></div>

<p>AI-Hilbert then reformulates the problem $\operatorname{Pr}$ as a semidefinite (or linear if no inequalities are present in the background theory) optimization problem $\mathbb{P r}^{\text {ad }}$, by leveraging standard techniques from sum-of-squares optimization that are now integrated within the SumOfSquares.jl and PolyJuMP.jl Julia packages, as discussed in Section 1.3. This corresponds to the Reduce step of Algorithm 1.
Next, AI-Hilbert solves $\mathbb{P r}^{\text {ad }}$ using a mixed-integer conic optimization solver such as Gurobi [1] or Mosek [6]. This corresponds to the Solve step of Algorithm 1.
AI-Hilbert then outputs a candidate formula of the form $q(\boldsymbol{x})=0$ where the only monomials with nonzero coefficients are those that only contain the variables $x_{1}, \ldots, x_{t}$, the (independent and dependent) variables that are observed in the environment. The background theory may contain additional variables $x_{t+1}, \ldots, x_{n}$ that are not observed in the environment and that will not appear in the derived law. This is because the axioms in the background theory are not constraints on the functional form of the target polynomial, but rather general scientific laws describing the environment, often not including any of the quantities/variables observed in the data.
Finally, AI-Hilbert returns multipliers $\left{\alpha_{i}\right}<em j="j">{i=1}^{k},\left{\beta</em>$ such that}\right}_{j=1}^{t</p>
<p>$$
q(\boldsymbol{x})=\alpha_{0}(\boldsymbol{x})+\sum_{i=1}^{k} \boldsymbol{\alpha}<em i="i">{i}(\boldsymbol{x}) g</em>}(\boldsymbol{x})+\sum_{j=1}^{t} \boldsymbol{\beta<em j="j">{j}(\boldsymbol{x}) h</em>)
$$}(\boldsymbol{x</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>if $d^{c}(q, \mathcal{G} \cap \mathcal{H})=0$, which is a certificate of the fact that $q$ is derivable from the background theory. If $d^{c}&gt;0$, then AI-Hilbert returns a certificate that $q$ is approximately derivable from the background theory, and $q$ is approximately equal to $\alpha_{0}(\boldsymbol{x})+\sum_{i=1}^{k} \boldsymbol{\alpha}<em i="i">{i}(\boldsymbol{x}) g</em>}(\boldsymbol{x})+\sum_{j=1}^{l} \boldsymbol{\beta<em j="j">{j}(\boldsymbol{x}) h</em>)$.}(\boldsymbol{x</p>
<h1>2.2 Overall Problem Setting</h1>
<p>AI-Hilbert aims to discover an unknown polynomial model $q(\boldsymbol{x})=0$, which contains one or more dependent variables raised to some power within the expression (to avoid the trivial solution $q \equiv 0$ ), is approximately consistent with our axioms $\mathcal{G}$ and $\mathcal{H}$-meaning $d^{c}$ is small, and explains our experimental data well, meaning $q\left(\bar{\boldsymbol{x}}<em 1="1">{i}\right)$ is small for each data point $i$, and is of low complexity.
Let $x</em> \leq 2 d\right}$. Let the discovered polynomial expression be}, \ldots, x_{t}$ denote the measurable variables, and let $x_{1}$ denote the dependent variable which we would like to ensure appears in our scientific law. Let $\Omega=\left{\boldsymbol{\mu} \in \mathbb{N}^{n}:|\boldsymbol{\mu}|_{1</p>
<p>$$
q(x)=\sum_{\boldsymbol{\mu} \in \Omega} a_{\boldsymbol{\mu}} x^{\boldsymbol{\mu}}
$$</p>
<p>where $2 d$ is a bound on the maximum allowable degree of $q$. We formulate the following polynomial optimization problem:</p>
<p>$$
\begin{array}{cl}
\min <em 2="2" d="d" n_="n,">{q \in \mathbb{R}</em>}} &amp; \sum_{\bar{\boldsymbol{x}<em i="i">{i} \in \Omega}\left|q\left(\bar{\boldsymbol{x}}</em>) \
\text { s.t. } &amp; \sum_{\boldsymbol{\mu} \in \Omega: \boldsymbol{\mu}}\right)\right|+\lambda \cdot d^{c}(q, \mathcal{G} \cap \mathcal{H<em _boldsymbol_mu="\boldsymbol{\mu">{1} \geq 1} a</em>=1 \
&amp; a_{\boldsymbol{\mu}}=0 \forall \boldsymbol{\mu} \in \Omega: \sum_{j=t+1}^{n} \boldsymbol{\mu}_{j} \geq 1
\end{array}
$$}</p>
<p>where $d^{c}$, the distance between $q$ and the background theory, is the optimal value of an inner minimization problem we define in Section 2.3, $\lambda&gt;0$ is a hyperparameter that balances the relative importance of model fidelity to the data against model fidelity to a set of axioms, the first constraint ensures that $x_{1}$, our dependent variable, appears in $q$, the second constraint ensures that we do not include any unmeasured variables. In certain problem settings, we constrain $d^{c}=0$, rather than penalizing the size of $d^{c}$ in the objective.
Note that the formulation of the first constraint controls the complexity of the scientific discovery problem via the degree of the Positivstellensatz certificate: a smaller bound on the allowable degree in the certificate yields a more tractable optimization problem but a less expressive family of certificates to search over, which ultimately entails a trade-off that needs to be made by the user. Indeed, this trade-off has been formally characterized by Lasserre [75], who showed that every non-negative polynomial is approximable to any desired accuracy by a sequence of sum-of-squares polynomials, with a trade-off between the degree of the SOS polynomial and the quality of the approximation.
After solving Problem (4), one of two possibilities occurs. Either the distance between $q$ and our background information is 0 , or the Positivstellensatz provides a non-zero polynomial</p>
<p>$$
r(\boldsymbol{x}):=q(\boldsymbol{x})-\alpha_{0}(\boldsymbol{x})-\sum_{i=1}^{k} \alpha_{i}(\boldsymbol{x}) g_{i}(\boldsymbol{x})-\sum_{j=1}^{l} \beta_{j}(\boldsymbol{x}) h_{j}(\boldsymbol{x})
$$</p>
<p>which defines the discrepancy between our derived physical law and its projection onto our background information. In this sense, solving Problem (4) also provides information about the inverse problem of identifying a complete set of axioms that explain $q$. In either case, it follows from the Positivstellensatz (Theorem 1) that solving Problem (4) for different hyperparameter values and different bounds on the degree of $q$ eventually yields polynomials that explain the experimental data well and are approximately derivable from background theory.
We close this section with two remarks on the generality and complexity of AI-Hilbert.</p>
<p>Implicit and Explicit Symbolic Discovery: Most prior work [e.g., 38, 32, 99] aims to identify an unknown symbolic model $f \in \mathbb{R}[x]<em i="i">{n, 2 d}$ of the form $y</em>}=f\left(\boldsymbol{x<em i="i">{i}\right)$ for a set of independent variables of interest $\boldsymbol{x}</em>$, while AI-Hilbert searches for an implicit polynomial function $q$ which links the dependent and independent variables. We do this for two reasons. First, many scientific formulae of practical interest admit implicit representations as polynomials, but explicit representations of the dependent variable as a polynomial function of the independent variables are not possible [c.f. 2]. For instance, Kepler's third law of planetary motion has this property (see Section 3.6). Second, as proven by Artin [9] to partially resolve Hilbert's 17th problem [c.f. 61], an arbitrary non-negative polynomial can be represented as a sum of squares of rational functions. Therefore, by multiplying by the denominator in Artin's representation [9], implicit representations of natural laws become a viable and computationally affordable search space.
We remark that the implicit representation of scientific laws as polynomials where $q(\boldsymbol{x})=0$ introduces some degeneracy in the set of optimal polynomials derivable from (4), particularly in the presence of a correct yet overdetermined background theory. For instance, in the derivation of Kepler's Law of Planetary Motion in Section 3.6, we eventually derive the polynomial $m_{1} m_{2} G p^{2}=$ $m_{1} d_{1} d_{2}^{2}+m_{2} d_{1}^{2} d_{2}+2 m_{2} d_{1} d_{2}^{2}$. Since we have the axiom that $m_{1} d_{1}=m_{2} d_{2}$, we could instead derive the (equivalent) formula $m_{1} m_{2} G p^{2}=\left(m_{1}+m_{2}\right) d_{1} d_{2}\left(d_{1}+d_{2}\right)$. To partly break this degeneracy, we propose to either constrain the degree of the proof certificate and gradually increase it (as is done in (4)) or, (equivalently in a Lagrangian sense) include a term modeling the complexity of our derived polynomial (e.g., $|q|} \in \mathbb{R}^{n}$ and a dependent variable $y_{i} \in \mathbb{R<em 1="1">{1}$, the $L</em>$-coefficient norm of $q$ ) in the objective.</p>
<p>Complexity of Scientific Discovery: Observe that, if the degree of our new scientific law $q$ is fixed and the degree of the polynomial multipliers in the definition in $d^{c}$ is also fixed, then Problem (4) can be solved in polynomial time ${ }^{4}$ with a consistent set of axioms (resp. nondeterministic polynomial time with an inconsistent set of axioms). This is because solving Problem (4) with a fixed degree and a consistent set of axioms corresponds to solving a semidefinite optimization problem of a polynomial size, which can be solved in polynomial time (assuming that a constraint qualification such as Slater's condition holds) [84]. Moreover, although solving Problem (4) with a fixed degree and an inconsistent set of axioms corresponds to solving a mixed-integer semidefinite optimization problem, which is NP-hard, recent evidence [40] shows that integer optimization problems can be solved in polynomial time with high probability. This suggests that Problem (4) may also be solvable in polynomial time with high probability. However, if the degree of $q$ is unbounded then, to the best of our knowledge, no existing algorithm solves Problem (4) in polynomial time. This explains why searching for scientific laws of a fixed degree and iteratively increasing the degree of the polynomial laws searched over, in accordance with Occam's Razor, is a key aspect of our approach.</p>
<h1>2.3 Distance to Background Theory and Model Complexity</h1>
<p>Scientists often start with experimental measurements and a set of polynomial equalities and inequalities (axioms) which they believe to be true. From these axioms and measurements, they aim to deduce a new law, explaining their data, which includes one or more dependent variables and excludes certain variables. The simplest case of scientific discovery involves a consistent and correct set of axioms that fully characterize the problem. In this case, the Positivstellensatz (Theorem 1) facilitates the discovery of new scientific laws via deductive reasoning, without using any experimental data, as we argue in Section 2.5. Indeed, under an Archimedean assumption, the set of all valid scientific laws corresponds precisely to the preprime (see [34] for a definition) generated by our axioms [90], and searching for the simplest polynomial version of a law which features a given dependent variable corresponds to solving an easy linear or semidefinite feasibility problem.
It is not uncommon to have a set of axioms that is inconsistent (meaning that there are no values of $\boldsymbol{x} \in \mathbb{R}^{n}$ that satisfy all laws simultaneously), or incomplete (meaning the axioms do not 'span' the space of all derivable polynomials; we provide a formal definition later in this section). Therefore, we require a notion of a distance between a body of background theory (which, in our case, consists</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>of a set of polynomial equalities and inequalities) and a polynomial. We now establish this definition, treating the inconsistent and incomplete cases separately; note that the inconsistent and incomplete case may be treated via the inconsistent case. We remark that $[23,113]$ propose related notions of the distance between (a) a point and a variety defined by a set of equality constraints, and (b) the distance between two semialgebraic sets via their Hausdorff distance. However, to our knowledge, the distance metrics in this paper have not previously been proposed.</p>
<p>Incomplete Case: Suppose $\mathcal{B}$ is a background theory (consisting of equalities and inequalities in $\mathcal{G}$ and $\mathcal{H}$ ), where $\mathcal{G}$ satisfies the previously defined Archimedean property (see Theorem 1) with constant $R$, and the axioms are not inconsistent, meaning that $\mathcal{G} \cap \mathcal{H} \neq \emptyset$. Then, a natural notion of distance is the $\ell_{2}$ coefficient distance $d^{c}$ between $q$ and $\mathcal{G} \cap \mathcal{H}$, which is given by:</p>
<p>$$
d^{c}(q, \mathcal{G} \cap \mathcal{H}):=\min <em 0="0">{\substack{\alpha</em>
$$}, \ldots, \alpha_{k} \in \Sigma_{n, 2 d}[\boldsymbol{x}],\left|q-\alpha_{0}-\sum_{i=1}^{k} \alpha_{i} g_{i}-\sum_{j=1}^{l} \beta_{j} h_{j}\right|_{2}</p>
<p>It follows directly from Putinar's Positivstellensatz that $d(q, \mathcal{G} \cap \mathcal{H})=0$ if and only if $q$ is derivable from $\mathcal{B}$. We remark that this distance has a geometric interpretation as the distance between a polynomial $q$ and its projection onto the algebraic variety generated by $\mathcal{G} \cap \mathcal{H}$. Moreover, by norm equivalence, this is equivalent to the Hausdorff distance [113] between $q$ and $\mathcal{G} \cap \mathcal{H}$.
With the above definition of $d^{c}$, and the fact that $\mathcal{G} \cap \mathcal{H} \neq \emptyset$, we say that $\mathcal{G} \cap \mathcal{H}$ is an incomplete set of axioms if there does not exist a polynomial $p$ with a non-zero coefficient on at least one monomial involving a dependent variable, such that $d^{c}(q, \mathcal{G} \cap \mathcal{H})=0$.</p>
<p>Inconsistent Case: Suppose $\mathcal{B}$ is an inconsistent background theory i.e, $\mathcal{G} \cap \mathcal{H}=\emptyset$. Then, a natural approach to scientific discovery is to assume that a subset of the axioms are valid laws, while the remaining axioms are scientifically invalid (or invalid in a specific context, e.g., micro vs. macro-scale). In line with the sparse regression literature [c.f. 20] and related work on discovering nonlinear dynamics [19, 80], we assume that scientific discoveries can be made using at most $k$ correct scientific laws and define the distance between the scientific law and the problem data via a best subset selection problem. Specifically, we introduce binary variables $z_{i}$ and $y_{j}$ to denote whether the $i$ th and $j$-th laws are consistent, and require that $\alpha_{i}=0$ if $z_{i}=0$ and $\beta_{j}=0$ if $y_{j}=0$ and $\sum_{i} z_{i}+\sum_{j} y_{j} \leq \tau$ for a sparsity budget $\tau$. Furthermore, we allow a non-zero $\ell_{2}$ distance between the scientific law $f$ and the reduced background theory, but penalize this distance in the objective. This gives the following notion of distance between a scientific law $q$ and the space $\mathcal{G} \cap \mathcal{H}$ :</p>
<p>$$
\begin{aligned}
d^{c}(q, G \cap \mathcal{H}):=\min &amp; \left|q-\alpha_{0}-\sum_{i=1}^{k} \alpha_{i} g_{i}-\sum_{j=1}^{l} \beta_{j} h_{j}\right|<em i="i">{2} \
\text { s.t. } &amp; \alpha</em>=0, \forall i \in{0, \ldots, k} \
&amp; \beta_{j}=0 \text { if } y_{j}=0, \forall j \in{1, \ldots l} \
&amp; \sum_{i=0}^{k} z_{i}+\sum_{j=1}^{l} y_{j} \leq \tau \
&amp; z_{0} \ldots z_{k} \in{0,1}, y_{1}, \ldots y_{l} \in{0,1} \
&amp; \alpha_{0}, \ldots, \alpha_{k} \in \Sigma_{n, 2 d}[\boldsymbol{x}], \beta_{1}, \ldots, \beta_{l} \in \mathbb{R}[x]_{n, 2 d}
\end{aligned}
$$}=0 \text { if } z_{i</p>
<p>It follows directly from the Positivstellensatz that $d=0$ if and only if $q$ can be derived from $\mathcal{B}$. If $\tau=m+l$, then we certainly have $d^{c}=0$, since the overall system of polynomials is inconsistent and the sum-of-squares proof system can deduce that " $-1 \geq 0$ " from inconsistent proof systems, from which it can claim a distance of 0 . However, by treating $\tau$ as a hyper-parameter and including the quality of the law on experimental data as part of the optimization problem (see Section 2.2), scientific discoveries can be made from inconsistent axioms by incentivizing solvers to set $z_{i}=0$ for inconsistent axioms $i$. Alternatively, a practitioner may wish to explore the Pareto frontier of scientific discoveries that arise as we vary $\tau$, to detect how large the set of correct background knowledge is. Provided there is a sufficiently high penalty cost on poorly explaining scientific data via the derived law, our optimization problem prefers a subset of correct axioms with a non-zero distance $d^{c}$ to the derived polynomial over a set of inconsistent axioms which gives a distance $d^{c}=0$.</p>
<h1>2.4 Impact of Background Theory on Amount of Data Needed to Discover Scientific Laws</h1>
<p>In the introduction, we suggested that providing a partially complete background theory expressible as polynomial equalities and inequalities may accelerate the scientific discovery process by decreasing the amount of data required to recover a scientific law with high probability. We now justify our claim in the introduction, by reviewing examples from the machine learning literature, which may be viewed as special cases of scientific discovery, where including relevant background theory decreases the number of data points required to recover a scientific law with high probability. The first two examples involve discovery settings where the ground truth is known to be sparse, and imposing a sparsity constraint on the discovered law reduces the amount of data required to recover the law with high probability. Note that these examples are due to [52, 8, 29]. However, the similarities between them strongly suggest that similar relations between the amount of background theory introduced and the amount of data required for scientific discovery also hold in other contexts.</p>
<h2>Example 1 Sparse Linear Regression [52]</h2>
<p>Consider a sparse high-dimensional regression model where we aim to recover a $\tau$-sparse regression model $\boldsymbol{\beta}^{\star} \in \mathbb{R}^{p}$ given access to $n$ noisy linear observations of the form $\boldsymbol{Y}=\boldsymbol{X} \boldsymbol{\beta}^{\star}+\boldsymbol{W} \in \mathbb{R}^{n}$, where $X_{i, j} \stackrel{\text { iid }}{=} \mathcal{N}(0,1)$ and $W_{i} \stackrel{\text { iid }}{=} \mathcal{N}\left(0, \sigma^{2}\right)$ for some parameter $\sigma&gt;0$, it is known that $\boldsymbol{\beta}^{\star}$ is a $\tau$-sparse vector with binary coefficients, and $n \ll p$ with $p \rightarrow \infty$. Then, it is information-theoretically impossible to recover $\boldsymbol{\beta}$ if $n \leq \Theta\left(\frac{2 \tau \log p}{\log \left(1+\frac{2 \tau}{\sigma^{2}}\right)}\right)$ [111]. On the other hand, for $n \geq \Theta\left(\frac{2 \tau \log p}{\log \left(1+\frac{2 \tau}{\sigma^{2}}\right)}\right)$, the (unique) optimal solution of the polynomial optimization problem (expressible as a binary problem)</p>
<p>$$
\min <em 2="2">{\boldsymbol{\beta} \in \mathbb{R}^{p}} \quad|\boldsymbol{Y}-\boldsymbol{X} \boldsymbol{\beta}|</em>=\tau
$$}^{2} \text { s.t. } \beta_{i}^{2}=\beta_{i} \forall i \in[p], \sum_{i \in[p]} \beta_{i</p>
<p>is such that</p>
<p>$$
\frac{1}{\tau}\left|\boldsymbol{\beta}^{\star}-\boldsymbol{\beta}\right|_{0} \rightarrow 0
$$</p>
<p>with high probability as $\tau \rightarrow \infty$. On the other hand, for any $\lambda&gt;0$, the optimal solution of the popular Lasso method</p>
<p>$$
\min <em 2="2">{\boldsymbol{\beta} \in \mathbb{R}^{p}} \quad|\boldsymbol{Y}-\boldsymbol{X} \boldsymbol{\beta}|</em>
$$}^{2}+\lambda|\boldsymbol{\beta}|_{1</p>
<p>only recovers $\boldsymbol{\beta}^{\star}$ with high probability when $n \geq \Theta\left(2 \tau+\sigma^{2}\right) \log p$. We remind the reader that</p>
<p>$$
\Theta\left(2 \tau+\sigma^{2}\right) \log p&gt;\Theta\left(\frac{2 \tau \log p}{\log \left(1+\frac{2 \tau}{\sigma^{2}}\right)}\right)
$$</p>
<h2>Example 2 Sparse Principal Component Analysis [8]</h2>
<p>Consider a sparse principal component analysis setting where we aim to recover a $\tau$-sparse binary vector $\boldsymbol{x}^{\star} \in \mathbb{R}^{n}$ given an observed matrix $\boldsymbol{Y}=\frac{\lambda}{\tau} \boldsymbol{x}^{\star} \boldsymbol{x}^{\star}{ }^{\top}+\boldsymbol{W}$, where $\boldsymbol{W}$ is a $\operatorname{GOE}(n)$ matrix, i.e., is a symmetric matrix with on-diagonal entries taking values $W_{i, i} \stackrel{\text { iid }}{=} \mathcal{N}\left(0, \frac{2}{n}\right)$ and off-diagonal entries taking values $W_{i, j} \stackrel{\text { iid }}{=} \mathcal{N}\left(0, \frac{1}{n}\right)$, and $\lambda&gt;0$ is the signal-to-noise ratio. Let $\lambda, \tau$ possibly depend on $n$, and set $1 \ll \tau \ll n$ as $n \rightarrow \infty$. Then:</p>
<ul>
<li>Recovery of $\boldsymbol{x}^{\star}$ is information-theoretically impossible when $\lambda \ll \frac{\sqrt{\tau}}{\sqrt{n}}$, with high probability.</li>
<li>The (mixed-integer representable) polynomial optimization problem</li>
</ul>
<p>$$
\max <em i="i">{\boldsymbol{x} \in \mathbb{R}^{n}} \quad \boldsymbol{x}^{\top} \boldsymbol{Y} \boldsymbol{x} \text { s.t. } x</em>=\tau
$$}^{2}=x_{i} \forall i \in[n], \sum_{i \in[n]} x_{i</p>
<p>achieves exact recovery with high probability when $\lambda \gg \frac{\sqrt{\tau}}{\sqrt{n}}$.</p>
<ul>
<li>
<p>The diagonal thresholding algorithm of [66] recovers $\boldsymbol{x}^{\star}$ with high probability if $\lambda \gg \frac{\tau}{\sqrt{n}}$.</p>
</li>
<li>
<p>The vanilla PCA method, which disregards backgrounds theory encoded via a sparsity constraint and solves the polynomial optimization problem</p>
</li>
</ul>
<p>$$
\max <em 2="2">{\boldsymbol{x} \in \mathbb{R}^{n}} \boldsymbol{x}^{\top} \boldsymbol{Y} \boldsymbol{x} \text { s.t. }|\boldsymbol{x}|</em>=\tau
$$}^{2</p>
<p>fails to recover $\boldsymbol{x}^{\star}$ with high probability when $\lambda&gt;1$.</p>
<h1>Example 3 Low-Rank Matrix Completion [29]</h1>
<p>Consider a low-rank matrix completion setting where we aim to recover a fixed rank $r n \times n$ matrix $\boldsymbol{A}$ given a uniform random sample of its entries $\Omega \subseteq[n] \times[n]$ of size $m$, where $\boldsymbol{A}$ satisfies the mutual incoherence property of [29] with constant $\mu$. Then:</p>
<ul>
<li>Recovery of $\boldsymbol{A}$ is information-theoretically impossible when $m \leq \Theta(n r \log n)$, because there are infinitely many rank- $r$ matrices that match all observed entries perfectly [29].</li>
<li>The polynomial optimization problem [c.f. 17, 15]</li>
</ul>
<p>$$
\min <em _Omega="\Omega" _i_="(i," _in="\in" j_="j)">{\boldsymbol{X} \in \mathbb{R}^{n \times n}, \boldsymbol{Y} \in \mathcal{S}^{n}} \sum</em>)=r
$$}\left(X_{i, j}-A_{i, j}\right)^{2} \text { s.t. } \boldsymbol{Y} \boldsymbol{X}=\boldsymbol{X}, \boldsymbol{Y}^{2}=\boldsymbol{Y}, \operatorname{tr}(\boldsymbol{Y</p>
<p>achieves exact recovery with high probability provided $m \geq \Theta(n r \log n)$.</p>
<ul>
<li>The nuclear norm relaxation of [29] recovers $\boldsymbol{A}$ with high probability provided $m \geq$ $\Theta\left(n^{6 / 5} r \log n\right)$.</li>
<li>The naive approach of disregarding the rank constraint and solving</li>
</ul>
<p>$$
\min <em _Omega="\Omega" _i_="(i," _in="\in" j_="j)">{\boldsymbol{X} \in \mathbb{R}^{n \times n}} \sum</em>
$$}\left(X_{i, j}-A_{i, j}\right)^{2</p>
<p>which admits the solution $X_{i, j}=0$ if $(i, j) \notin \Omega$, fails to recover $\boldsymbol{A}$ with high probability provided $m&lt;n^{2}$.</p>
<p>The above examples are admittedly more stylized than many scientific discovery settings that arise in practice. Nonetheless, they reveal that in certain circumstances, encoding relevant background provably reduces the amount of data required to recover a scientific law with high probability. This agrees with intuition: if we provide a complete background theory that can be manipulated to recover the scientific law, then, as discussed in the next section, we require no data to recover a scientific law. On the other hand, if we provide no background theory we may require a significant amount of data to recover a law. Therefore, providing relevant background theory that constrains the space of derivable scientific laws should decrease the amount of data needed to recover a scientific law with high confidence. This observation highlights the value of embedding relevant background theory within the scientific discovery process.</p>
<h3>2.5 Discovering Scientific Laws From Background Theory Alone</h3>
<p>Suppose that the background theory $\mathcal{B}$ constitutes a complete set of axioms that fully describes our physical system. Then, any polynomial which contains our dependent variable $x_{1}$ and is derivable from our system of axioms is a valid physical law. Therefore, we need not even collect any experimental data, and we can solve the following feasibility problem to discover a valid law (let $\Omega=\left{\boldsymbol{\mu} \in \mathbb{N}^{n}:|\boldsymbol{\mu}|_{1} \leq 2 d\right}$ ):</p>
<p>$$
\begin{aligned}
\exists \quad q(x) &amp; =\sum_{\boldsymbol{\mu} \in \Omega} a_{\boldsymbol{\mu}} x^{\boldsymbol{\mu}} \
\text { s.t. } \quad q(\boldsymbol{x}) &amp; =\alpha_{0}(\boldsymbol{x})+\sum_{j=1}^{k} \alpha_{i}(\boldsymbol{x}) g_{i}(\boldsymbol{x})+\sum_{j=1}^{l} \beta_{j}(\boldsymbol{x}) h_{j}(\boldsymbol{x}) \
\sum_{\boldsymbol{\mu} \in \Omega: \boldsymbol{\mu}<em _boldsymbol_mu="\boldsymbol{\mu">{1} \geq 1} a</em> &amp; =1 \
a_{\boldsymbol{\mu}} &amp; =0 \quad \forall \boldsymbol{\mu} \in \Omega: \sum_{j=l+1}^{n} \boldsymbol{\mu}}<em i="i">{j} \geq 1 \
\alpha</em>]}(\boldsymbol{x}) &amp; \in \Sigma[\boldsymbol{x<em j="j">{n, 2 d}, \beta</em>
\end{aligned}
$$}(\boldsymbol{x}) \in \mathbb{R}[\boldsymbol{x}]_{n, 2 d</p>
<p>where the second and third constraints ensure that we include the dependent variable $x_{1}$ in our formula $q$ and rule out the trivial solution $q=0$, and exclude any solutions $q$ which contain uninteresting symbolic variables respectively.
Note that if we do not have any inequality constraints in either problem, then we may eliminate $\alpha_{i}$ and obtain a linear optimization problem.</p>
<h1>3 Experimental Validation</h1>
<p>In this section, we showcase AI-Hilbert's ability to recover valid scientific laws across various problem settings, including settings with incomplete or inconsistent axioms and experimental data.
The rest of this section is laid out as follows: First, we provide a detailed explanation of AI-Hilbert's implementation in Section 3.1. Subsequently, we discuss the trade-off between data and theory made by AI-Hilbert in Section 3.2. Next, we validate AI-Hilbert on five selected problems that highlight its different capabilities, as discussed below. Our main findings are as follows:</p>
<ol>
<li>In Sections 3.3-3.4, we demonstrate that AI-Hilbert successfully derives valid scientific laws solely from a complete and consistent background theory. Namely, by deriving the HagenPoissuille Equation (Section 3.3) and deriving the Radiated Gravitational Wave Power Equation (Section 3.4). We further demonstrate this capability on a suite of test instances in Appendix B.</li>
<li>In Sections 3.5-3.6, we demonstrate that AI-Hilbert is capable of deriving a valid scientific law from an inconsistent yet complete background theory. We illustrate this capability through two scenarios: Firstly, when two axioms in the background theory contradict each other (Section 3.5), by using measurement data to discern the correct axiom. Secondly, where an incorrect candidate formula is incorporated into a complete and consistent background theory (Section 3.6). In both cases, we use real-life, noisy, experimental data.</li>
<li>In Section 3.7, we demonstrate that AI-Hilbert successfully derives a valid scientific law from an incomplete yet consistent background theory and experimental data. Specifically, we demonstrate that AI-Hilbert is data-efficient, in the sense that as the number of axioms supplied increases, less data is required to successfully derive a scientific law.</li>
<li>In Section 3.8, we showcase AI-Hilbert's ability to handle and discover inequalities, namely the Bell Inequalities. This is notable because existing scientific discovery methods, to our knowledge, cannot handle inequalities.</li>
<li>We compare AI-Hilbert with some widely used methods from the literature on a suite of test instances in Appendix A. We demonstrate that AI-Hilbert outperforms these methods by rediscovering more of the scientific laws in this test bed. This occurs because AI-Hilbert integrates data and theory systematically via sum-of-squares optimization.</li>
</ol>
<h3>3.1 Implementation Details</h3>
<p>We now illustrate the lower-level implementation of AI-Hilbert via a synthetic example where the axioms are assumed to be consistent and complete. Consider a semialgebraic system in two real</p>
<p>variables $x$ and $y$ which comprises the axioms:</p>
<p>$$
\begin{gathered}
x^{2}+y^{2}-2=0 \
y-x^{3}=0
\end{gathered}
$$</p>
<p>where we let $h_{1}(x, y)=0$ and $h_{2}(x, y)=0$ denote Equations (6)-(7), respectively. These axioms can be viewed as being true in a subdomain of $\mathbb{R}^{2}$, i.e., when $|x|=1$.
Let the set $S={(1,1),(-1,-1)}$ contain all points satisfying equations (6)-(7). If</p>
<p>$$
q(x, y)=\beta_{1}(x, y) h_{1}(x, y)+\beta_{2}(x, y) h_{2}(x, y)
$$</p>
<p>where $\beta_{1}, \beta_{2}$ are polynomials in $x, y$, then $q$ is a polynomial that vanishes on $S$. On the other hand, to certify that a polynomial $q(x, y)$ vanishes on $S$, we search for a polynomial $q(x, y)$ such that $q(x, y)=g(x, y) p(x, y)$ and $q$ satisfies the expression in (8). For example, $q(x, y)=x-y$ vanishes on $S$; setting $\beta_{1}=-\frac{1}{2} x, \beta_{2}=-1$, we have</p>
<p>$$
q=\beta_{1} h_{1}+\beta_{2} h_{2}=x-y+\frac{1}{2}\left(x^{3}-x y^{2}\right)=(x-y)\left(1+\frac{1}{2} x(x+y)\right)
$$</p>
<p>Suppose we wish to find a polynomial function $q(x, y)$ that vanishes on $S$ and explains the dataset</p>
<p>$$
\bar{\boldsymbol{x}}=\left[\begin{array}{cc}
.5 &amp; .5 \
-2 &amp; -2 \
3 &amp; 3 \
-3 &amp; -3
\end{array}\right]
$$</p>
<p>where the columns are observed values of $x$ and $y$, respectively, and each row represents a datapoint. Note that these data observations are noiseless observations from the polynomial $f(x, y)=x-y$, which we aim to recover. Then we search for $q$ satisfying (8) that fits $\bar{\boldsymbol{x}}$.
We assume that $\beta_{1}, \beta_{2}, q$ are unknown polynomials that are comprised of monomials of degree at most 3, i.e., the ten monomials in the vector</p>
<p>$$
\text { mon }=\left(1, y, y^{2}, y^{3}, x, x y, x y^{2}, x^{2}, x^{2} y, x^{3}\right)
$$</p>
<p>where $\operatorname{mon}<em 2="2">{1}=1, \operatorname{mon}</em>$. Let}=y, \ldots, \operatorname{mon}_{10}=x^{3</p>
<p>$$
\beta_{1}(x, y)=\sum_{j=1}^{10} a_{j} \operatorname{mon}<em 2="2">{j}, \beta</em>}(x, y)=\sum_{j=1}^{10} b_{j} \operatorname{mon<em j="1">{j}, \text { and } q(x, y)=\sum</em>
$$}^{10} c_{j} \operatorname{mon}_{j</p>
<p>Further, let $\boldsymbol{v}$ denote the vector $\left(a_{1}, \ldots, a_{10}, b_{1}, \ldots, b_{10}, c_{1}, \ldots, c_{10}\right), \overline{\boldsymbol{x}}<em i="i">{i}$ denote the $i$ th row of $\overline{\boldsymbol{x}}$, and $q\left(\overline{\boldsymbol{x}}</em>}\right)$ be the value of the polynomial $q$ evaluated at the point $(\bar{x}, \bar{y})=\overline{\boldsymbol{x}<em i="i">{i}$. Note that $q\left(\overline{\boldsymbol{x}}</em>}\right)=\sum_{j=1}^{10} c_{j} \operatorname{mon<em i="i">{j}\left(\overline{\boldsymbol{x}}</em>$.
If $\overline{\boldsymbol{x}}}\right)$ is a linear function of the unknowns $c_{1}, \ldots, c_{10<em i="i">{i}$ is a noiseless experimental observation, we should have $q\left(\overline{\boldsymbol{x}}</em>}\right)=0$. Accordingly, we interpret any nonzero value of $\left|q\left(\overline{\boldsymbol{x}<em i="i">{i}\right)\right|$ as the error when $q$ is evaluated at $\overline{\boldsymbol{x}}</em>$, which are obtained by equating the coefficients of the monomials in Equation (8):}$, and aim to minimize this error when selecting $q$. Equation (8) implies the following linear equations in the variables $a_{i}, b_{i}, c_{i</p>
<p>$$
\begin{gathered}
c_{1}=-2 a_{1} \
c_{2}=-2 a_{2}+b_{1} \
c_{3}=a_{1}-2 a_{3}+b_{2} \
\vdots \
0=a_{3}+b_{4} \
{\left[\text { [coef. of } 1\right]} \
{\left[\text { [coef. of } y\right]} \
{\left[\text { [coef. of } y^{2}\right]} \
{\left[\text { [coef. of } x y^{3}\right]}
\end{gathered}
$$</p>
<p>Let these constraints be denoted by $A v=\mathbf{0}$, and let $m$, the number of data points, be an integer between 1 and 4 . Then, we solve the linear optimization problem</p>
<p>$$
\begin{aligned}
&amp; \min 100\left(\sum_{i=1}^{m} t_{i}\right)+\sum_{j=1}^{10} w_{j} \
&amp; \text { s.t } \quad t_{i} \geq q\left(\overline{\boldsymbol{x}<em i="i">{i}}\right) \quad i=1, \ldots, m \
&amp; t</em>} \geq-q\left(\overline{\boldsymbol{x<em j="j">{i}}\right) \quad i=1, \ldots, m \
&amp; w</em> \quad j=1, \ldots, 10 \
&amp; w_{j} \geq-c_{j} \quad j=1, \ldots, 10 \
&amp; A v=\mathbf{0} \
&amp; \sum_{j=4}^{10} c_{j}=1 \
&amp; v \in \mathbb{R}^{30}, t_{i}, w_{j} \geq 0 \quad i=1, \ldots, d, j=1, \ldots, 10 .
\end{aligned}
$$} \geq c_{j</p>
<p>In Problem (15), the first two constraints imply that $t_{i} \geq\left|q\left(\overline{\boldsymbol{x}}<em i="i">{\boldsymbol{i}}\right)\right|$; the third and fourth force $w</em>$-coefficient norm of $q$. The latter term is a regularization term incentivizing a sparse $q$.
If we set $m=1$ and solve (15), we obtain the (correct) function $q(x, y)=x-y$ in (15). In other words, a single data point from $\overline{\boldsymbol{x}}$ suffices to "recover" $x-y$ with the choice of objective in (15).
However, let LPF be the linear optimization problem obtained by dropping the second term in the objective in (15) (and not incentivizing sparsity). If we solve LPF with $m=1$, we get $q^{\prime}=$ $4 x^{3}-x^{2}-y^{2}-4 y+2$ as a solution. This is not a multiple of $(x-y)$ but vanishes at the points in $S$ and the first datapoint in $\mathcal{D}$. We need $m=2$ (and use two datapoints) before we get} \geq\left|c_{i}\right|$. The second-to-last constraint forces a monomial containing $x$ to be present in $q$ to avoid the trivial solution $q \equiv 0$. We sometimes use a different right-hand-side value to get non-fractional $c_{i}$ values. Thus, the optimization problem above searches for a polynomial $q$ that minimizes a weighted combination of the $L_{1}$-error of $q$ and the $L_{1</p>
<p>$$
q^{\prime}=y^{3}+x^{2} y-2 x y^{2}+4 x-4 y=(x-y)\left((x-y)^{2}+4\right)
$$</p>
<p>This illustrates the role of sparsity and regularization in reducing the amount of data required to recover a scientific law. Note that $(x-y)^{2}+4$ is strictly positive for all real $x, y$, and therefore $q^{\prime}=0$ on $S$ indicates that $x-y=0$ on $S$.
Further, if we drop the constraints $A v=\mathbf{0}$ and allow $q$ to be an arbitrary degree-3 polynomial in $x, y$ (and not equal to $\beta_{1} h_{1}+\beta_{2} h_{2}$ ), then we cannot recover a multiple of $x-y$ until we set $m=4$ and use all datapoints in $\overline{\boldsymbol{x}}$. In the latter case, we get $q=x^{3}-y^{3}$. This highlights the value of background theory in restricting the space of feasible scientific laws and reducing the amount of data required to recover a scientific law.
If the only measured variable is $x$, then we would try to eliminate $y$ in the final formula. Multiplying (7) by $y+x^{3}$ and subtracting the result from (6), we get the expression $x^{2}+x^{6}-2=0$. This satisfies the $x$-components of points in $S$. Sometimes, there may be a unique way of eliminating variables and getting a formula on the measured variables.</p>
<h1>3.2 Trade off Between Data and Theory</h1>
<p>There is a fundamental trade-off between the amount of background theory available and the amount of data needed for scientific discovery. Indeed, with a complete and consistent set of background theories, it is possible to perform scientific discovery without any experimental data via the Positivestellensatz (see Section 1.3). On the other hand, the purely data-driven approaches to scientific discovery reviewed in Section 1.1 often require many noiseless or low-noise experimental observations to successfully perform scientific discovery. More generally, as the amount of relevant background theory increases, the amount of experimental data required by AI-Hilbert to perform scientific discovery cannot increase, because increasing the amount of background theory decreases the effective VC-dimension of our scientific discovery problem [see, e.g., 60]. This can be seen in the machine learning examples discussed in Section 2.4, where imposing a sparsity constraint (i.e., providing a relevant axiom) reduces the number of data observations needed to discover a ground truth model; see also $[79,57]$ for an analysis of the asymptotic consistency of shape-constrained regression. We provide further evidence of this experimentally in Section 3.7.</p>
<p>The above observations can also be explained via real algebraic geometry: requiring that a scientific law is consistent with an axiom is equivalent to restricting the space of valid scientific laws to a subset of the space of discoverable scientific laws. As such, an axiom is equivalent to infinitely many data observations that penalize scientific laws outside a subspace but provide no information that discriminates between scientific laws within the subspace.</p>
<h1>3.3 Deriving the Hagen-Poiseuille Equation</h1>
<p>We consider the problem of deriving the velocity of laminar fluid flow through a circular pipe, from a simplified version of the Navier-Stokes equations, an assumption that the velocity can be modeled by a degree-two polynomial in the radius of the pipe, and a no-slip boundary condition. Letting $u(r)$ denote the velocity in the pipe where $r$ is the distance from the center of the pipe, $R$ denote the radius of the pipe, $\Delta p$ denote the pressure differential throughout the pipe, $L$ denote the length of the pipe, and $\mu$ denote the viscosity of the fluid, we have the following velocity profile for $r \in[0, R]$ :</p>
<p>$$
u(r)=\frac{-\Delta p}{4 L \mu}\left(r^{2}-R^{2}\right)
$$</p>
<p>We now derive this law axiomatically, by assuming that the velocity profile can be modeled by a symmetric polynomial, and iteratively increasing the degree of the polynomial until we obtain a polynomial solution, consistent with Occam's Razor. Accordingly, we initially set the degree of $u$ to be two and add together the following terms with appropriate polynomial multipliers:</p>
<p>$$
\begin{array}{r}
u=c_{0}+c_{2} r^{2} \
\mu \frac{\partial}{\partial r}\left(r \frac{\partial}{\partial r} u\right)-r \frac{d p}{d x}=0 \
c_{0}+c_{2} R^{2}=0 \
L \frac{d p}{d x}=-\Delta p
\end{array}
$$</p>
<p>Here Equation (17) posits a quadratic velocity profile in $r$, Equation (18) imposes a simplified version of the Navier-Stokes equations in spherical coordinates, Equation (19) imposes a no-slip boundary condition on the velocity profile of the form $u(R)=0$, and Equation (20) posits that the pressure gradient throughout the pipe is constant. The variables in this axiom system are $u, r, R, L, \mu, \Delta p, c_{0}, c_{2}$, and $\frac{d p}{d x}$. We treat $c_{0}, c_{2}, \frac{d p}{d x}$ as variables that cannot be measured, and use the differentiate function in Julia to symbolically differentiate $u=c_{0}+c_{2} r^{2}$ with respect to $r$ in Equation (18) before solving the problem, giving the equivalent expression $4 c_{2} \mu r-r \frac{d p}{d x}$. Solving Problem (5) with $u$ as the dependent variable, and searching for polynomial multipliers (and polynomial $q$ ) of degree at most 3 in each variable and an overall degree of at most 6 , we get the expression:</p>
<p>$$
4 r L \mu u-r \Delta p\left(R^{2}-r^{2}\right)=0
$$</p>
<p>which confirms the result. The associated polynomial multipliers for Equations (17)-(20) are:</p>
<p>$$
\begin{gathered}
4 r L \mu \
r^{2} L-L R^{2} \
4 r L \mu \
r^{3}-r R^{2}
\end{gathered}
$$</p>
<h3>3.4 Radiated Gravitational Wave Power</h3>
<p>We now consider the problem of deriving the power radiated from gravitational waves emitted by two-point masses orbiting their common center of gravity in a Keplerian orbit, as originally derived by Peters and Mathews [89] and verified for binary star systems by Hulse and Taylor [64]. Specifically, [89] showed that the average power generated by such a system is:</p>
<p>$$
P=-\frac{32 G^{4}}{5 c^{5} r^{5}}\left(m_{1} m_{2}\right)^{2}\left(m_{1}+m_{2}\right)
$$</p>
<p>where $P$ is the (average) power of the wave, $G=6.6743 \times 10^{-11} \mathrm{~m}^{3} \mathrm{~kg}^{-1} \mathrm{~s}^{-2}$ is the universal gravitational constant, $c$ is the speed of light, $m_{1}, m_{2}$ are the masses of the objects, and we assume that the two objects orbit a constant distance of $r$ away from each other. Note that this equation is one of the twenty so-called bonus laws considered in the work introducing AI-Feynman [107], and notably, is one of only two such laws that neither AI-Feynman nor Eurequ [42] were able to derive. We now derive this law axiomatically, by combining the following axioms with appropriate multipliers:</p>
<p>$$
\begin{aligned}
\omega^{2} r^{3}-G\left(m_{1}+m_{2}\right) &amp; =0 \
x 5\left(m_{1}+m_{2}\right)^{2} c^{5} P+G \operatorname{Tr}\left(\frac{d^{3}}{d t^{3}}\left(m_{1} m_{2} r^{2}\left(\begin{array}{ccc}
x^{2}-\frac{1}{3} &amp; x y &amp; 0 \
x y &amp; y^{2}-\frac{1}{3} &amp; 0 \
0 &amp; 0 &amp; -\frac{1}{3}
\end{array}\right)\right)^{2}\right) &amp; =0 \
x^{2}+y^{2} &amp; =1
\end{aligned}
$$</p>
<p>where we make the variable substitution $x=\cos \phi, y=\sin \phi, \operatorname{Tr}$ stands for the trace function, and we manually define the derivative of a bivariate degree-two trigonometric polynomial in $\phi=\phi_{0}+\omega t$ in $(x, y)$ in terms of $(x, y, \omega)$ as the following linear operator:
$\frac{d}{d t}\left(\left(\begin{array}{c}\sin \phi \ \cos \phi\end{array}\right)^{\top}\left(\begin{array}{ll}a_{1,1} &amp; a_{1,2} \ a_{2,1} &amp; a_{2,2}\end{array}\right)\left(\begin{array}{c}\sin \phi \ \cos \phi\end{array}\right)\right)=\omega\left(\begin{array}{c}\sin \phi \ \cos \phi\end{array}\right)^{\top}\left(\begin{array}{ll}a_{1,2}+a_{2,1} &amp; a_{1,1}-a_{2,2} \ a_{1,1}-a_{2,2} &amp; -a_{1,2}-a_{2,1}\end{array}\right)\left(\begin{array}{c}\sin \phi \ \cos \phi\end{array}\right)$.
Note that Equation (21) is a restatement of Kepler's previously derived third law of planetary motion, Equation (22) provides the gravitational power of a wave when the wavelength is large compared to the source dimensions, by linearizing the equations of general relativity, the third equation defines the quadruple moment tensor, and Equation (23) (which we state as $x^{2}+y^{2}=1$ within our axioms) is a standard trigonometric identity. Solving Problem (5) with $P$ as the dependent variable, and searching for a formula involving $P, G, r, c, m_{1}, m_{2}$ with polynomial multipliers of degree at most 20 , and allowing each variable to be raised to a power for the variables $\left(P, x, y, \omega, G, r, c, m_{1}, m_{2}\right)$ of at most $(1,4,4,4,3,6,1,5,5)$ respectively, then yields the following equation:</p>
<p>$$
\frac{1}{4} P r^{5} c^{5}\left(m_{1}+m_{2}\right)^{2}=\frac{-8}{5} G^{4} m_{1}^{2} m_{2}^{2}\left(m_{1}+m_{2}\right)^{3}
$$</p>
<p>which verifies the result. Note that this equation is somewhat expensive to derive, owing to fact that searching over the set of degree 20 polynomial multipliers necessitates generating a large number of linear equalities, and writing these equalities to memory is both time and memory intensive. Accordingly, we solved Problem (5) using the MIT SuperCloud environment [93] with 640 GB RAM. The resulting system of linear inequalities involves 416392 candidate monomials, and takes 14368s to write the problem to memory and 6.58 s to be solved by Mosek. This shows that the correctness of the universal gravitational wave equation can be confirmed via the following multipliers:</p>
<p>$$
\begin{aligned}
&amp; \frac{-8}{5} G m_{1}^{2} m_{2}^{2}\left(\omega^{4} r^{6}\left(x^{2}+y^{2}\right)^{2}+\omega^{2} r^{3} G\left(m_{1}+m_{2}\right)+G^{2}\left(m_{1}+m_{2}\right)^{2}\right) \
&amp; \frac{1}{20} r^{5} \
&amp; \frac{-8}{5} \omega^{4} r^{6} G^{2} m_{1}^{2} m_{2}^{2}\left(m_{1}+m_{2}\right)\left(x^{2}+y^{2}+1\right)
\end{aligned}
$$</p>
<p>Finally, Figure 4 illustrates how the Positivstellensatz derives this equation, by demonstrating that (setting $m_{1}=m_{2}=c=G=1$ ), the gravitational wave equation is precisely the set of points $(\omega, r, P)$ where our axioms hold with equality.</p>
<h1>3.5 Deriving Einstein's Relativistic Time Dilation Law</h1>
<p>Next, we consider the problem of deriving Einstein's relativistic time dilation formula from a complete set of background knowledge axioms plus an inconsistent "Newtonian" axiom, which posits that light behaves like a mechanical object. We distinguish between these axioms using data on the relationship between the velocity of a light clock and the relative passage of time, as measured experimentally by Chou et at. [30] and stated explicitly in the work of Cornelio et al. [32, Tab. 6].</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Illustration of the Positivstellensatz and its ability to recover the Radiation Gravitational Wave Power Equation in the special case where $m_{1}=m_{2}=c=G=1$. Keeping other variables constant, the points that obey the power equation are the intersection of the points that obey Kepler's Third Law and the points of a linearized equation from general relativity, and the wave equation is recoverable by adding these other equations with appropriate polynomial multipliers.</p>
<p>Einstein's law describes the relationship between how two observers in relative motion to each other observe time, and demonstrates that observers moving at different speeds experience time differently. Indeed, letting the constant $c$ denote the speed of light, the frequency $f$ of a clock moving at a speed $v$ is related to the frequency $f_{0}$ of a stationary clock via</p>
<p>$$
\frac{f}{f_{0}}=\sqrt{1-\frac{v^{2}}{c^{2}}}
$$</p>
<p>We now derive this law axiomatically, by adding together the following five axioms with appropriate polynomial multipliers:</p>
<p>$$
\begin{array}{r}
c d t_{0}-2 d=0 \
c d t-2 L=0 \
4 L^{2}+4 d^{2}-v^{2} d t^{2}=0 \
f d t_{0}=1 \
f d t=1
\end{array}
$$</p>
<p>plus the following (inconsistent) Newtonian axiom:</p>
<p>$$
d t^{2}\left(v^{2}+c^{2}\right)-4 L^{2}=0
$$</p>
<p>where $d t_{0}$ denotes the time required for a light to travel between two stationary mirrors separated by a distance $d$, and $d t$ denotes the time required for light to travel between two similar mirrors moving at velocity $v$, giving a distance between the mirrors of $L$.
These axioms have the following meaning: Equation (29) relates the time required for light to travel between two stationary mirrors to their distance, Equation (30) similarly relates the time required for light to travel between two mirrors in motion to the effective distance $L$, Equation (31) relates the physical distance between the mirrors $d$ to their effective distance $L$ induced by the motion $v$ via the Pythagorean theorem, and Equations (32)-(33) relate frequencies and periods. Finally, Equation (34) assumes (incorrectly) that light behaves like other mechanical objects, meaning if it is emitted orthogonally from an object traveling at velocity $v$, then it has velocity $\sqrt{v^{2}+c^{2}}$.</p>
<p>By solving Problem (4) with a cardinality constraint that we include at most $\tau=5$ axioms (corresponding to the exclusion of one axiom), a constraint that we must exclude either Equation (30) or Equation (34), $f$ as the dependent variable, experimental data in $f, f_{0}, v, c$ to separate the valid and invalid axioms (obtained from [32, Tab. 6] by setting $f_{0}=1$ to transform the data in $\left(f-f_{0}\right) / f_{0}$ into data in $f, f_{0}$ ), $f_{0}, v, c$ as variables that we would like to appear in our polynomial formula $q(\boldsymbol{x})=0 \forall \boldsymbol{x} \in \mathcal{G} \cap \mathcal{H}$, and searching the set of polynomial multipliers of degree at most 2 in each term, we obtain the law:</p>
<p>$$
-c^{2} f_{0}^{2}+c^{2} f^{2}+f_{0}^{2} v^{2}=0
$$</p>
<p>in 6.04 seconds using Gurobi version 9.5.1. Moreover, we immediately recognize this as a restatement of Einstein's law 28. This shows that the correctness of Einstein's law can be verified by multiplying the (consistent relativistic set of) axioms by the following polynomials:</p>
<p>$$
\begin{array}{r}
2 d f_{0}^{2} f^{2}+c f_{0} f^{2} \
-c f_{0}^{2} f-2 f_{0}^{2} f^{2} L \
-f_{0}^{2} f^{2} \
-2 c d f_{0} f^{2}-c^{2} f^{2} \
c^{2} d t f_{0}^{2} f-d t f_{0}^{2} f v^{2}+c^{2} f_{0}^{2}-f_{0}^{2} v^{2}
\end{array}
$$</p>
<p>Moreover, it verifies that relativistic axioms, particularly the axiom $c d t=2 L$, fit the light clock data of [30] better than Newtonian axioms, because, by the definition of Problem (4), AI-Hilbert selects the combination of $\tau=5$ axioms with the lowest discrepancy between the discovered scientific formula and the experimental data.</p>
<h1>3.6 Deriving Kepler's Third Law of Planetary Motion</h1>
<p>In this section, we consider the problem of deriving Kepler's third law of planetary motion from a complete set of background knowledge axioms plus an incorrect candidate formula as an additional axiom, which is to be screened out using experimental data. To our knowledge, this paper is the first work that addresses this particularly challenging problem setting. Indeed, none of the approaches to scientific discovery reviewed in the introduction successfully distinguish between correct and incorrect axioms via experimental data by solving a single optimization problem. The primary motivation for this experiment is to demonstrate that AI-Hilbert provides a system for determining whether, given a background theory and experimental data, it is possible to improve upon a state-of-the-art scientific formula using background theory and experimental data.
Kepler's law describes the relationship between the distance between two bodies, e.g., the sun and a planet, and their orbital periods and takes the form:</p>
<p>$$
p=\sqrt{\frac{4 \pi^{2}\left(d_{1}+d_{2}\right)^{3}}{G\left(m_{1}+m_{2}\right)}}
$$</p>
<p>where $G=6.6743 \times 10^{-11} \mathrm{~m}^{3} \mathrm{~kg}^{-1} \mathrm{~s}^{-2}$ is the universal gravitational constant, $m_{1}$ and $m_{2}$ are the masses of the two bodies, $d_{1}$ and $d_{2}$ are the respective distances between $m_{1}, m_{2}$ and their common center of mass, and $p$ is the orbital period. We now derive this law axiomatically by adding together the following five axioms with appropriate polynomial multipliers:</p>
<p>$$
\begin{aligned}
d_{1} m_{1}-d_{2} m_{2} &amp; =0 \
\left(d_{1}+d_{2}\right)^{2} F_{g}-G m_{1} m_{2} &amp; =0 \
F_{c}-m_{2} d_{2} w^{2} &amp; =0 \
F_{c}-F_{g} &amp; =0 \
w p &amp; =1
\end{aligned}
$$</p>
<p>plus the following (incorrect) candidate formula (as an additional axiom) proposed by Cornelio et al. [32] for the exoplanet dataset (where the mass of the planets can be discarded as negligible when added to the much bigger mass of the star):</p>
<p>$$
p^{2} m_{1}-0.1319\left(d_{1}+d_{2}\right)^{3}=0
$$</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Under the real number complexity model, and under the bit number complexity model under some mild regularity conditions on the semidefinite optimization problems that arise from our sum-of-squares optimization problems. Note that, under the bit complexity model, semidefinite optimization problems cannot always be solved in polynomial time due to the existence of ill-behaved semidefinite problems where all feasible solutions are of doubly exponential size. We refer to Ramana [91] or Laurent and Rendl [77] for a complete characterization of the complexity of semidefinite optimization.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
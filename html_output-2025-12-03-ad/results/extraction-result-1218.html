<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1218 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1218</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1218</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-27.html">extraction-schema-27</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <p><strong>Paper ID:</strong> paper-53280207</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1811.04551v2.pdf" target="_blank">Learning Latent Dynamics for Planning from Pixels</a></p>
                <p><strong>Paper Abstract:</strong> Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from pixels and chooses actions through online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this problem using a latent dynamics model with both deterministic and stochastic transition function and a generalized variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards. PlaNet uses significantly fewer episodes and reaches final performance close to and sometimes higher than top model-free algorithms.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1218.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1218.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PlaNet RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Planning Network - Recurrent State Space Model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent dynamics world model used in the PlaNet agent that combines a deterministic recurrent path and a stochastic latent state to predict future latent states from pixels and enable planning entirely in latent space.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PlaNet RSSM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent-space recurrent state-space model (RSSM) that factorizes the hidden state into a deterministic RNN state h_t (GRU) and a stochastic latent s_t (diagonal Gaussian). An encoder q(s_t | h_t, o_t) (filtering posterior) maps image observations to latent posteriors; the transition prior p(s_t | h_t) is Gaussian and the deterministic update h_t = f(h_{t-1}, s_{t-1}, a_{t-1}) is implemented with a GRU. Observation p(o_t | h_t, s_t) and reward p(r_t | h_t, s_t) decoders are trained but planning is performed purely in latent space. Training uses a variational ELBO augmented with latent overshooting (multi-step KL penalties) and a fixed global prior to avoid posterior collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image-based continuous control (DeepMind Control Suite: cartpole, finger, cheetah, cup, walker, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Negative log-likelihood / reconstruction term under Gaussian decoder (equivalently MSE for pixels with unit variance), KL divergence between posterior and transition prior, and multi-step predictive accuracy evaluated by open-loop video prediction and downstream task return (episode return).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Qualitative: latent overshooting yields substantially improved multi-step predictive fidelity; pixel-accurate open-loop predictions reported for ~50 steps in the cheetah environment; no numeric MSE reported in the paper. Fidelity is assessed indirectly via improved planning return.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Partially interpretable: latent states retain task-relevant physical quantities — open-loop state diagnostics show learned latents can predict simulator positions, velocities, and reward accurately, indicating latent dimensions capture interpretable factors of the environment.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Visualization of open-loop video predictions, open-loop state diagnostics (training small networks to map frozen latents to true simulator states), and qualitative inspection of predicted frames.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Training: reported ~1-2 days on a single NVIDIA V100 GPU for experiments in the paper (training across tasks). Model size specifics: 30-dimensional diagonal Gaussian latents, GRU with 200 units, encoder/decoders two FC layers of size 200 and convolutional/deconvolutional nets; training batch: 50 sequence chunks length 50. Planning cost: CEM with J=1000 candidates, I=10 iterations, evaluates one forward trajectory per candidate in latent space (fast since images are not decoded).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Sample efficiency: uses ~50× fewer environment interaction episodes than model-free baseline A3C; similar wall-clock training time to top model-free D4PG despite much less data. Latent-space planning is computationally cheaper than rollout with image generation, enabling evaluation of thousands of action sequences per step.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reaches final performance close to, and sometimes higher than, top model-free algorithms while using 50× less data; surpasses D4PG on the cheetah running task by ~19% relative improvement (reported in paper); outperforms A3C in multiple tasks within far fewer episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>High multi-step predictive fidelity (via latent overshooting and RSSM design) translates into improved planning returns; learning dynamics in a compact latent space provides sufficient task-relevant information for control, so fidelity in latent-space multi-step predictions correlates with policy performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Latent overshooting improves long-term fidelity and final task performance but can slow initial learning for some tasks (e.g., finger); purely stochastic transitions capture multimodality but make long-term memory difficult; purely deterministic models fail to capture multiple futures and can be exploitable; longer planning horizons increase search complexity and can hurt performance despite potentially better long-term fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Combined deterministic RNN path (GRU) and stochastic latents; 30-dim diagonal Gaussian latent variables; filtering encoder q(s_t|h_t,o_t); global fixed Gaussian prior regularizer; latent overshooting across D up to 50 with β scheduling (β1 large, β>1 =1 for most tasks); plan in latent space using CEM; action repeat to reduce effective horizon (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Compared with purely deterministic RNN and purely stochastic SSM ablations: RSSM (deterministic+stochastic) achieved better planning performance across tasks. Compared with model-free baselines: PlaNet achieves similar or better final returns with much less data. Compared with approaches that access low-dimensional simulator states, PlaNet is less direct but operates from pixels and generalizes to partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper's recommendations: include both deterministic and stochastic components in transition model; train multi-step predictions using latent overshooting to improve planning-relevant fidelity; plan in latent space (avoid decoding images during planning); tune KL-divergence scales, action-repeat, and overshooting weights per domain. Use an RNN (deterministic) to maintain long-term memory and stochastic latents to model uncertainty/multi-modality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Latent Dynamics for Planning from Pixels', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1218.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1218.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RNN (deterministic) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic recurrent neural network transition model (pure RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully deterministic recurrent model (GRU) used as an ablation to test the importance of stochastic latents; it maintains memory via recurrent hidden states but cannot capture multiple plausible futures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Deterministic RNN (GRU) latent model</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A recurrent model where transitions are fully deterministic (hidden state h_t only, no stochastic s_t). Observation and reward decoders are conditioned on the deterministic state. No stochastic latent sampling; therefore latent overshooting for stochastic latents is not applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>deterministic latent world model / RNN</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image-based continuous control (same DeepMind control suite tasks used in ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood (pixel MSE) and downstream planning return; multi-step fidelity assessed via open-loop predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Lower multi-step robustness to ambiguous futures compared to RSSM; performs worse in many tasks in ablation experiments (exact numeric scores not tabulated in-paper), and lacked the multimodality captured by stochastic components.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent hidden state is a black-box neural representation but can be probed similarly with open-loop diagnostics; less explicit representation of multiple futures.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Same diagnostics as RSSM (reconstructions and open-loop predictions); no specialized interpretability methods mentioned for deterministic-only model.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Slightly cheaper per-step forward pass than stochastic models (no sampling) but still uses comparable architectural components (GRU with 200 units); planning cost similar when used for latent rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Computationally simpler but empirically less effective for planning from pixels because it cannot model uncertainty/multi-modality; in experiments it underperformed the combined RSSM.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Lower task returns compared to RSSM in the reported ablations; incapable of capturing multiple plausible futures which degrades planner robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good for deterministic or near-deterministic settings where memory is paramount, but inability to represent multiple futures makes it exploitable and less robust under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Simplicity and memory capability (deterministic recurrence) vs inability to represent uncertainty; deterministic-only simplifies training (no latent sampling/ELBO) but sacrifices robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Use of GRU-only transitions, no stochastic latent s_t, no latent overshooting. The RNN ablation also did not use overshooting since it lacks stochastic latents.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Underperforms RSSM (deterministic+stochastic) on the suite of control tasks; better than trivial baselines but worse than models that capture uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper suggests deterministic recurrence is necessary but not sufficient — combine deterministic recurrence with stochastic latents for best trade-off between memory and uncertainty representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Latent Dynamics for Planning from Pixels', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1218.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1218.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SSM (stochastic) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stochastic State-Space Model (purely stochastic transitions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fully stochastic state-space model (SSM) with transitions modeled probabilistically but without a deterministic recurrent path; captures multiple futures but struggles to maintain long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Stochastic SSM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Purely stochastic latent state-space model where states s_t are sampled from p(s_t | s_{t-1}, a_{t-1}) and the model lacks a deterministic hidden recurrence; observation and reward decoders depend on s_t only. Latent transitions parameterized by neural nets producing Gaussian means and variances.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>stochastic latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>image-based continuous control (ablation set)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction log-likelihood (pixel MSE) and multi-step predictive accuracy measured by open-loop predictions and downstream returns.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Can capture multimodal futures but has difficulty remembering information over many steps, leading to inconsistent long-horizon rollouts and worse planning returns than RSSM; no numeric fidelity metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latents represent multimodal uncertainty but are less stable over long sequences, making interpretation of long-horizon behavior difficult.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Open-loop video predictions and qualitative inspection; no special interpretability techniques described for SSM ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Architecturally similar to RSSM but may require more sampling/variance control; planning cost similar when used for latent rollouts but more prone to sampled outliers causing divergent trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Captures uncertainty better than deterministic RNN but at cost of long-term memory and stable rollouts; overall planning utility lower than RSSM in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Worse planning performance than RSSM across tasks in ablations; unable to match final task returns achieved by combined deterministic+stochastic model.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Good at modeling multimodality which is useful under partial observability, but poor long-term memory reduces planner reliability and thus harms task performance.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Stochasticity improves robustness to uncertainty but harms memory of past events and can produce sampled outlier trajectories; trade-off with deterministic recurrence necessitates hybrid RSSM design.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Purely stochastic latent transitions (no deterministic h_t), Gaussian transition parameterization, use of KL regularization and global prior to limit posterior collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>Performs worse than RSSM due to memory limitations; better at representing multimodality than deterministic RNN but less useful for planning because of inconsistent long-horizon trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Paper recommends combining stochastic state components with a deterministic recurrent pathway rather than using a purely stochastic SSM for planning from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Latent Dynamics for Planning from Pixels', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1218.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1218.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>World models (Ha & Schmidhuber)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prominent latent world-model approach that learns a VAE-style visual embedding and an RNN-based dynamics model to enable control from pixels, referenced in the related work as an approach to learn compact environment models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>World models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>World Models</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>VAE encoder to compress pixels to latent codes plus an RNN (MDN-RNN in original paper) predictive model over latents; a controller is trained (e.g., evolutionary strategies) on imagined rollouts in latent space. The model generates visual rollouts and supports training controllers without interacting with the real environment.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>video games and simple simulation control from pixels</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Video prediction fidelity (visual plausibility) and controller performance on downstream tasks (episode return).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Not quantified in this paper beyond discussion; referenced as a related approach that demonstrates planning/control from pixels in simpler environments.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent codes can be inspected and decoded to images; interpretability depends on latent structure but generally considered a black-box neural model with visualizations of generated frames.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent decoding to pixels (visualization) and inspection of generated rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Generative image decoding during rollouts can be computationally expensive; original work trained models and controllers but details are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Used as inspiration for latent-space planning ideas; PlaNet emphasizes planning purely in latent space and avoiding expensive image decoding during planning to improve efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Effective on some toy/simulated domains demonstrated by Ha & Schmidhuber; paper cites these works as complementary but not directly compared numerically in PlaNet.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful in domains where visual realism of rollouts helps controller training; image decoding during planning can be a computational bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>High-fidelity pixel decoding helpful for visualization and some training pipelines but costly for planning; motivating PlaNet's choice to plan without decoding images.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>VAE + RNN predictive model + separate controller trained on imagined rollouts; emphasis on compact latent representation of visual scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PlaNet differs by focusing on filtering posterior for planning, combining stochastic and deterministic components, and training multi-step latent consistency (latent overshooting) to improve multi-step predictive fidelity without decoding images.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_configuration</strong></td>
                            <td>Not specified in this paper; cited as related work illustrating benefits of latent models for control from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Latent Dynamics for Planning from Pixels', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1218.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1218.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>E2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A latent-model approach that learns locally-linear latent transitions suitable for LQR-style control, cited as prior work that could control low-dimensional tasks from images but difficulties scaling to harder domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Embed to control: A locally linear latent dynamics model for control from raw images</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>E2C (Embed to Control)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Learns an encoder that embeds images to a latent space where local linear dynamics are assumed; planning/control is performed using LQR on the locally-linear latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (locally-linear latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>simple control tasks from pixels (cartpoles, 2-link arms)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction error in latent space and performance of LQR controller on embedded dynamics (episode return).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Reported effective for simple low-dimensional tasks in cited work; paper notes difficulty scaling E2C to more challenging pixel domains.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent dynamics are structured to be locally linear which aids interpretability and enables control via linear methods.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Imposed locally-linear constraints in latent space to enable LQR, providing some interpretability of dynamics around trajectory segments.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Relatively lightweight for small latent sizes and linear-control solvers, but limited in scaling to high-dimensional visual domains per the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Works well when low-dimensional latent linearity assumption holds; less applicable to complex, partially observable, or contact-rich tasks that PlaNet targets.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Good on simple tasks cited in original work; not directly compared numerically in PlaNet paper.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Utility constrained by local-linearity assumption; suitable when latent dynamics are approximately linear over control-relevant regions.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Interpretability and control simplicity vs limited representational power for complex dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Enforce locally-linear latent transitions to allow application of LQR; learn encoder/decoder jointly with latent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PlaNet relaxes the strict local-linearity assumption, uses an RNN component for non-Markovian settings and latent overshooting for multi-step fidelity, enabling scaling to harder tasks from pixels.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Latent Dynamics for Planning from Pixels', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1218.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1218.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RCE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Robust Locally-Linear Controllable Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of locally-linear latent embedding approaches that aims to improve robustness of learned latent dynamics for control; cited as prior latent model work for control from images.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Robust locally-linear controllable embedding</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RCE (Robust Locally-Linear Controllable Embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Latent embedding approach that enforces locally-linear dynamics in latent space with robustness-oriented modifications to improve control performance under model inaccuracies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent world model (locally-linear latent dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>control from raw images for simple simulated tasks</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>Reconstruction and latent dynamics prediction error; controller performance on downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Effective for the small-scale tasks reported in its original work; paper cites RCE as difficult to scale to more complex image domains.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Locally-linear structure provides some interpretability of latent dynamics in local regions.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Latent factorization and linearization for local regions; visualization of latent trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Comparable to other latent embedding methods; scales poorly to complex, high-dimensional visual tasks according to PlaNet discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Efficient for low-dimensional problems but limited generalization to contact-rich or partially observable tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Reported successful on small control tasks (cartpole, 2-link arm) in original work; PlaNet targets more challenging tasks with different model choices.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Works when locally-linear assumptions are valid; less useful for long-horizon, partially observable, contact-rich environments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Robustness and interpretability via local linearity vs limited representational flexibility for complex dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Locally-linear latent dynamics with robustness penalties and encoder/decoder learned jointly.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PlaNet moves beyond locally-linear assumptions and addresses partial observability and multi-step fidelity with latent overshooting and RSSM architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Latent Dynamics for Planning from Pixels', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1218.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1218.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of world models used in AI systems, including details about their fidelity, interpretability, computational efficiency, and task-specific utility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VRNN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A Recurrent Latent Variable Model for Sequential Data (VRNN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variational recurrent neural network that combines RNNs and stochastic latent variables for sequence modeling, referenced as related prior work for latent sequence models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A recurrent latent variable model for sequential data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VRNN (Variational RNN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Combines RNNs with latent stochastic variables at every time step, trained via variational inference (ELBO), typically uses a dependence of the posterior on the full sequence (smoothing) or conditioning on past via filtering.</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>latent sequence model / probabilistic recurrent world model</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>general sequence modeling and video prediction</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_metric</strong></td>
                            <td>ELBO components (reconstruction likelihood and KL), sequence log-likelihood and downstream predictive performance.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_performance</strong></td>
                            <td>Effective for sequential data modeling; cited as computationally heavier for forward prediction when generated observations are fed back into the model (as noted in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_assessment</strong></td>
                            <td>Latent variables provide some interpretable stochastic factors, but overall model remains largely a black box.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_method</strong></td>
                            <td>Inspection of generated sequences and latent sampling; not specialized in PlaNet paper.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost</strong></td>
                            <td>Forward prediction can be expensive if model feeds decoded observations back into the recurrent state, making long open-loop rollouts costly.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_comparison</strong></td>
                            <td>Less efficient for planning where many forward rollouts are required unless modified to avoid decoding loops; motivates PlaNet's RSSM that avoids decoding during planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_performance</strong></td>
                            <td>Good for video/sequence modeling benchmarks in cited literature; not directly benchmarked numerically in PlaNet.</td>
                        </tr>
                        <tr>
                            <td><strong>task_utility_analysis</strong></td>
                            <td>Useful for capturing temporal stochasticity; feed-back of reconstructed observations can hinder efficient latent rollouts for planning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoffs_observed</strong></td>
                            <td>Expressivity for sequential modeling vs computational cost of autoregressive decoding during long rollouts.</td>
                        </tr>
                        <tr>
                            <td><strong>design_choices</strong></td>
                            <td>Stochastic latents at each time step combined with RNN dynamics; variational training with ELBO.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_alternatives</strong></td>
                            <td>PlaNet emphasizes a filtering posterior and architectures that permit cheap latent rollouts (no image decoding during planning), addressing a practical limitation of some VRNN-style approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning Latent Dynamics for Planning from Pixels', 'publication_date_yy_mm': '2018-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>World models <em>(Rating: 2)</em></li>
                <li>Embed to control: A locally linear latent dynamics model for control from raw images <em>(Rating: 2)</em></li>
                <li>Robust locally-linear controllable embedding <em>(Rating: 2)</em></li>
                <li>A recurrent latent variable model for sequential data <em>(Rating: 2)</em></li>
                <li>Stochastic variational video prediction <em>(Rating: 1)</em></li>
                <li>Learning and querying fast generative models for reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1218",
    "paper_id": "paper-53280207",
    "extraction_schema_id": "extraction-schema-27",
    "extracted_data": [
        {
            "name_short": "PlaNet RSSM",
            "name_full": "Deep Planning Network - Recurrent State Space Model",
            "brief_description": "A latent dynamics world model used in the PlaNet agent that combines a deterministic recurrent path and a stochastic latent state to predict future latent states from pixels and enable planning entirely in latent space.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PlaNet RSSM",
            "model_description": "Latent-space recurrent state-space model (RSSM) that factorizes the hidden state into a deterministic RNN state h_t (GRU) and a stochastic latent s_t (diagonal Gaussian). An encoder q(s_t | h_t, o_t) (filtering posterior) maps image observations to latent posteriors; the transition prior p(s_t | h_t) is Gaussian and the deterministic update h_t = f(h_{t-1}, s_{t-1}, a_{t-1}) is implemented with a GRU. Observation p(o_t | h_t, s_t) and reward p(r_t | h_t, s_t) decoders are trained but planning is performed purely in latent space. Training uses a variational ELBO augmented with latent overshooting (multi-step KL penalties) and a fixed global prior to avoid posterior collapse.",
            "model_type": "latent world model",
            "task_domain": "image-based continuous control (DeepMind Control Suite: cartpole, finger, cheetah, cup, walker, etc.)",
            "fidelity_metric": "Negative log-likelihood / reconstruction term under Gaussian decoder (equivalently MSE for pixels with unit variance), KL divergence between posterior and transition prior, and multi-step predictive accuracy evaluated by open-loop video prediction and downstream task return (episode return).",
            "fidelity_performance": "Qualitative: latent overshooting yields substantially improved multi-step predictive fidelity; pixel-accurate open-loop predictions reported for ~50 steps in the cheetah environment; no numeric MSE reported in the paper. Fidelity is assessed indirectly via improved planning return.",
            "interpretability_assessment": "Partially interpretable: latent states retain task-relevant physical quantities — open-loop state diagnostics show learned latents can predict simulator positions, velocities, and reward accurately, indicating latent dimensions capture interpretable factors of the environment.",
            "interpretability_method": "Visualization of open-loop video predictions, open-loop state diagnostics (training small networks to map frozen latents to true simulator states), and qualitative inspection of predicted frames.",
            "computational_cost": "Training: reported ~1-2 days on a single NVIDIA V100 GPU for experiments in the paper (training across tasks). Model size specifics: 30-dimensional diagonal Gaussian latents, GRU with 200 units, encoder/decoders two FC layers of size 200 and convolutional/deconvolutional nets; training batch: 50 sequence chunks length 50. Planning cost: CEM with J=1000 candidates, I=10 iterations, evaluates one forward trajectory per candidate in latent space (fast since images are not decoded).",
            "efficiency_comparison": "Sample efficiency: uses ~50× fewer environment interaction episodes than model-free baseline A3C; similar wall-clock training time to top model-free D4PG despite much less data. Latent-space planning is computationally cheaper than rollout with image generation, enabling evaluation of thousands of action sequences per step.",
            "task_performance": "Reaches final performance close to, and sometimes higher than, top model-free algorithms while using 50× less data; surpasses D4PG on the cheetah running task by ~19% relative improvement (reported in paper); outperforms A3C in multiple tasks within far fewer episodes.",
            "task_utility_analysis": "High multi-step predictive fidelity (via latent overshooting and RSSM design) translates into improved planning returns; learning dynamics in a compact latent space provides sufficient task-relevant information for control, so fidelity in latent-space multi-step predictions correlates with policy performance.",
            "tradeoffs_observed": "Latent overshooting improves long-term fidelity and final task performance but can slow initial learning for some tasks (e.g., finger); purely stochastic transitions capture multimodality but make long-term memory difficult; purely deterministic models fail to capture multiple futures and can be exploitable; longer planning horizons increase search complexity and can hurt performance despite potentially better long-term fidelity.",
            "design_choices": "Combined deterministic RNN path (GRU) and stochastic latents; 30-dim diagonal Gaussian latent variables; filtering encoder q(s_t|h_t,o_t); global fixed Gaussian prior regularizer; latent overshooting across D up to 50 with β scheduling (β1 large, β&gt;1 =1 for most tasks); plan in latent space using CEM; action repeat to reduce effective horizon (task-dependent).",
            "comparison_to_alternatives": "Compared with purely deterministic RNN and purely stochastic SSM ablations: RSSM (deterministic+stochastic) achieved better planning performance across tasks. Compared with model-free baselines: PlaNet achieves similar or better final returns with much less data. Compared with approaches that access low-dimensional simulator states, PlaNet is less direct but operates from pixels and generalizes to partial observability.",
            "optimal_configuration": "Paper's recommendations: include both deterministic and stochastic components in transition model; train multi-step predictions using latent overshooting to improve planning-relevant fidelity; plan in latent space (avoid decoding images during planning); tune KL-divergence scales, action-repeat, and overshooting weights per domain. Use an RNN (deterministic) to maintain long-term memory and stochastic latents to model uncertainty/multi-modality.",
            "uuid": "e1218.0",
            "source_info": {
                "paper_title": "Learning Latent Dynamics for Planning from Pixels",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "RNN (deterministic) ablation",
            "name_full": "Deterministic recurrent neural network transition model (pure RNN)",
            "brief_description": "A fully deterministic recurrent model (GRU) used as an ablation to test the importance of stochastic latents; it maintains memory via recurrent hidden states but cannot capture multiple plausible futures.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Deterministic RNN (GRU) latent model",
            "model_description": "A recurrent model where transitions are fully deterministic (hidden state h_t only, no stochastic s_t). Observation and reward decoders are conditioned on the deterministic state. No stochastic latent sampling; therefore latent overshooting for stochastic latents is not applicable.",
            "model_type": "deterministic latent world model / RNN",
            "task_domain": "image-based continuous control (same DeepMind control suite tasks used in ablations)",
            "fidelity_metric": "Reconstruction log-likelihood (pixel MSE) and downstream planning return; multi-step fidelity assessed via open-loop predictions.",
            "fidelity_performance": "Lower multi-step robustness to ambiguous futures compared to RSSM; performs worse in many tasks in ablation experiments (exact numeric scores not tabulated in-paper), and lacked the multimodality captured by stochastic components.",
            "interpretability_assessment": "Latent hidden state is a black-box neural representation but can be probed similarly with open-loop diagnostics; less explicit representation of multiple futures.",
            "interpretability_method": "Same diagnostics as RSSM (reconstructions and open-loop predictions); no specialized interpretability methods mentioned for deterministic-only model.",
            "computational_cost": "Slightly cheaper per-step forward pass than stochastic models (no sampling) but still uses comparable architectural components (GRU with 200 units); planning cost similar when used for latent rollouts.",
            "efficiency_comparison": "Computationally simpler but empirically less effective for planning from pixels because it cannot model uncertainty/multi-modality; in experiments it underperformed the combined RSSM.",
            "task_performance": "Lower task returns compared to RSSM in the reported ablations; incapable of capturing multiple plausible futures which degrades planner robustness.",
            "task_utility_analysis": "Good for deterministic or near-deterministic settings where memory is paramount, but inability to represent multiple futures makes it exploitable and less robust under partial observability.",
            "tradeoffs_observed": "Simplicity and memory capability (deterministic recurrence) vs inability to represent uncertainty; deterministic-only simplifies training (no latent sampling/ELBO) but sacrifices robustness.",
            "design_choices": "Use of GRU-only transitions, no stochastic latent s_t, no latent overshooting. The RNN ablation also did not use overshooting since it lacks stochastic latents.",
            "comparison_to_alternatives": "Underperforms RSSM (deterministic+stochastic) on the suite of control tasks; better than trivial baselines but worse than models that capture uncertainty.",
            "optimal_configuration": "Paper suggests deterministic recurrence is necessary but not sufficient — combine deterministic recurrence with stochastic latents for best trade-off between memory and uncertainty representation.",
            "uuid": "e1218.1",
            "source_info": {
                "paper_title": "Learning Latent Dynamics for Planning from Pixels",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "SSM (stochastic) ablation",
            "name_full": "Stochastic State-Space Model (purely stochastic transitions)",
            "brief_description": "A fully stochastic state-space model (SSM) with transitions modeled probabilistically but without a deterministic recurrent path; captures multiple futures but struggles to maintain long-term memory.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Stochastic SSM",
            "model_description": "Purely stochastic latent state-space model where states s_t are sampled from p(s_t | s_{t-1}, a_{t-1}) and the model lacks a deterministic hidden recurrence; observation and reward decoders depend on s_t only. Latent transitions parameterized by neural nets producing Gaussian means and variances.",
            "model_type": "stochastic latent world model",
            "task_domain": "image-based continuous control (ablation set)",
            "fidelity_metric": "Reconstruction log-likelihood (pixel MSE) and multi-step predictive accuracy measured by open-loop predictions and downstream returns.",
            "fidelity_performance": "Can capture multimodal futures but has difficulty remembering information over many steps, leading to inconsistent long-horizon rollouts and worse planning returns than RSSM; no numeric fidelity metrics reported.",
            "interpretability_assessment": "Latents represent multimodal uncertainty but are less stable over long sequences, making interpretation of long-horizon behavior difficult.",
            "interpretability_method": "Open-loop video predictions and qualitative inspection; no special interpretability techniques described for SSM ablation.",
            "computational_cost": "Architecturally similar to RSSM but may require more sampling/variance control; planning cost similar when used for latent rollouts but more prone to sampled outliers causing divergent trajectories.",
            "efficiency_comparison": "Captures uncertainty better than deterministic RNN but at cost of long-term memory and stable rollouts; overall planning utility lower than RSSM in experiments.",
            "task_performance": "Worse planning performance than RSSM across tasks in ablations; unable to match final task returns achieved by combined deterministic+stochastic model.",
            "task_utility_analysis": "Good at modeling multimodality which is useful under partial observability, but poor long-term memory reduces planner reliability and thus harms task performance.",
            "tradeoffs_observed": "Stochasticity improves robustness to uncertainty but harms memory of past events and can produce sampled outlier trajectories; trade-off with deterministic recurrence necessitates hybrid RSSM design.",
            "design_choices": "Purely stochastic latent transitions (no deterministic h_t), Gaussian transition parameterization, use of KL regularization and global prior to limit posterior collapse.",
            "comparison_to_alternatives": "Performs worse than RSSM due to memory limitations; better at representing multimodality than deterministic RNN but less useful for planning because of inconsistent long-horizon trajectories.",
            "optimal_configuration": "Paper recommends combining stochastic state components with a deterministic recurrent pathway rather than using a purely stochastic SSM for planning from pixels.",
            "uuid": "e1218.2",
            "source_info": {
                "paper_title": "Learning Latent Dynamics for Planning from Pixels",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "World Models",
            "name_full": "World models (Ha & Schmidhuber)",
            "brief_description": "A prominent latent world-model approach that learns a VAE-style visual embedding and an RNN-based dynamics model to enable control from pixels, referenced in the related work as an approach to learn compact environment models.",
            "citation_title": "World models",
            "mention_or_use": "mention",
            "model_name": "World Models",
            "model_description": "VAE encoder to compress pixels to latent codes plus an RNN (MDN-RNN in original paper) predictive model over latents; a controller is trained (e.g., evolutionary strategies) on imagined rollouts in latent space. The model generates visual rollouts and supports training controllers without interacting with the real environment.",
            "model_type": "latent world model",
            "task_domain": "video games and simple simulation control from pixels",
            "fidelity_metric": "Video prediction fidelity (visual plausibility) and controller performance on downstream tasks (episode return).",
            "fidelity_performance": "Not quantified in this paper beyond discussion; referenced as a related approach that demonstrates planning/control from pixels in simpler environments.",
            "interpretability_assessment": "Latent codes can be inspected and decoded to images; interpretability depends on latent structure but generally considered a black-box neural model with visualizations of generated frames.",
            "interpretability_method": "Latent decoding to pixels (visualization) and inspection of generated rollouts.",
            "computational_cost": "Generative image decoding during rollouts can be computationally expensive; original work trained models and controllers but details are in the cited paper.",
            "efficiency_comparison": "Used as inspiration for latent-space planning ideas; PlaNet emphasizes planning purely in latent space and avoiding expensive image decoding during planning to improve efficiency.",
            "task_performance": "Effective on some toy/simulated domains demonstrated by Ha & Schmidhuber; paper cites these works as complementary but not directly compared numerically in PlaNet.",
            "task_utility_analysis": "Useful in domains where visual realism of rollouts helps controller training; image decoding during planning can be a computational bottleneck.",
            "tradeoffs_observed": "High-fidelity pixel decoding helpful for visualization and some training pipelines but costly for planning; motivating PlaNet's choice to plan without decoding images.",
            "design_choices": "VAE + RNN predictive model + separate controller trained on imagined rollouts; emphasis on compact latent representation of visual scenes.",
            "comparison_to_alternatives": "PlaNet differs by focusing on filtering posterior for planning, combining stochastic and deterministic components, and training multi-step latent consistency (latent overshooting) to improve multi-step predictive fidelity without decoding images.",
            "optimal_configuration": "Not specified in this paper; cited as related work illustrating benefits of latent models for control from pixels.",
            "uuid": "e1218.3",
            "source_info": {
                "paper_title": "Learning Latent Dynamics for Planning from Pixels",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "E2C",
            "name_full": "Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images",
            "brief_description": "A latent-model approach that learns locally-linear latent transitions suitable for LQR-style control, cited as prior work that could control low-dimensional tasks from images but difficulties scaling to harder domains.",
            "citation_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "mention_or_use": "mention",
            "model_name": "E2C (Embed to Control)",
            "model_description": "Learns an encoder that embeds images to a latent space where local linear dynamics are assumed; planning/control is performed using LQR on the locally-linear latent dynamics.",
            "model_type": "latent world model (locally-linear latent dynamics)",
            "task_domain": "simple control tasks from pixels (cartpoles, 2-link arms)",
            "fidelity_metric": "Reconstruction error in latent space and performance of LQR controller on embedded dynamics (episode return).",
            "fidelity_performance": "Reported effective for simple low-dimensional tasks in cited work; paper notes difficulty scaling E2C to more challenging pixel domains.",
            "interpretability_assessment": "Latent dynamics are structured to be locally linear which aids interpretability and enables control via linear methods.",
            "interpretability_method": "Imposed locally-linear constraints in latent space to enable LQR, providing some interpretability of dynamics around trajectory segments.",
            "computational_cost": "Relatively lightweight for small latent sizes and linear-control solvers, but limited in scaling to high-dimensional visual domains per the paper's discussion.",
            "efficiency_comparison": "Works well when low-dimensional latent linearity assumption holds; less applicable to complex, partially observable, or contact-rich tasks that PlaNet targets.",
            "task_performance": "Good on simple tasks cited in original work; not directly compared numerically in PlaNet paper.",
            "task_utility_analysis": "Utility constrained by local-linearity assumption; suitable when latent dynamics are approximately linear over control-relevant regions.",
            "tradeoffs_observed": "Interpretability and control simplicity vs limited representational power for complex dynamics.",
            "design_choices": "Enforce locally-linear latent transitions to allow application of LQR; learn encoder/decoder jointly with latent dynamics.",
            "comparison_to_alternatives": "PlaNet relaxes the strict local-linearity assumption, uses an RNN component for non-Markovian settings and latent overshooting for multi-step fidelity, enabling scaling to harder tasks from pixels.",
            "uuid": "e1218.4",
            "source_info": {
                "paper_title": "Learning Latent Dynamics for Planning from Pixels",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "RCE",
            "name_full": "Robust Locally-Linear Controllable Embedding",
            "brief_description": "A variant of locally-linear latent embedding approaches that aims to improve robustness of learned latent dynamics for control; cited as prior latent model work for control from images.",
            "citation_title": "Robust locally-linear controllable embedding",
            "mention_or_use": "mention",
            "model_name": "RCE (Robust Locally-Linear Controllable Embedding)",
            "model_description": "Latent embedding approach that enforces locally-linear dynamics in latent space with robustness-oriented modifications to improve control performance under model inaccuracies.",
            "model_type": "latent world model (locally-linear latent dynamics)",
            "task_domain": "control from raw images for simple simulated tasks",
            "fidelity_metric": "Reconstruction and latent dynamics prediction error; controller performance on downstream tasks.",
            "fidelity_performance": "Effective for the small-scale tasks reported in its original work; paper cites RCE as difficult to scale to more complex image domains.",
            "interpretability_assessment": "Locally-linear structure provides some interpretability of latent dynamics in local regions.",
            "interpretability_method": "Latent factorization and linearization for local regions; visualization of latent trajectories.",
            "computational_cost": "Comparable to other latent embedding methods; scales poorly to complex, high-dimensional visual tasks according to PlaNet discussion.",
            "efficiency_comparison": "Efficient for low-dimensional problems but limited generalization to contact-rich or partially observable tasks.",
            "task_performance": "Reported successful on small control tasks (cartpole, 2-link arm) in original work; PlaNet targets more challenging tasks with different model choices.",
            "task_utility_analysis": "Works when locally-linear assumptions are valid; less useful for long-horizon, partially observable, contact-rich environments.",
            "tradeoffs_observed": "Robustness and interpretability via local linearity vs limited representational flexibility for complex dynamics.",
            "design_choices": "Locally-linear latent dynamics with robustness penalties and encoder/decoder learned jointly.",
            "comparison_to_alternatives": "PlaNet moves beyond locally-linear assumptions and addresses partial observability and multi-step fidelity with latent overshooting and RSSM architecture.",
            "uuid": "e1218.5",
            "source_info": {
                "paper_title": "Learning Latent Dynamics for Planning from Pixels",
                "publication_date_yy_mm": "2018-11"
            }
        },
        {
            "name_short": "VRNN",
            "name_full": "A Recurrent Latent Variable Model for Sequential Data (VRNN)",
            "brief_description": "A variational recurrent neural network that combines RNNs and stochastic latent variables for sequence modeling, referenced as related prior work for latent sequence models.",
            "citation_title": "A recurrent latent variable model for sequential data",
            "mention_or_use": "mention",
            "model_name": "VRNN (Variational RNN)",
            "model_description": "Combines RNNs with latent stochastic variables at every time step, trained via variational inference (ELBO), typically uses a dependence of the posterior on the full sequence (smoothing) or conditioning on past via filtering.",
            "model_type": "latent sequence model / probabilistic recurrent world model",
            "task_domain": "general sequence modeling and video prediction",
            "fidelity_metric": "ELBO components (reconstruction likelihood and KL), sequence log-likelihood and downstream predictive performance.",
            "fidelity_performance": "Effective for sequential data modeling; cited as computationally heavier for forward prediction when generated observations are fed back into the model (as noted in the paper).",
            "interpretability_assessment": "Latent variables provide some interpretable stochastic factors, but overall model remains largely a black box.",
            "interpretability_method": "Inspection of generated sequences and latent sampling; not specialized in PlaNet paper.",
            "computational_cost": "Forward prediction can be expensive if model feeds decoded observations back into the recurrent state, making long open-loop rollouts costly.",
            "efficiency_comparison": "Less efficient for planning where many forward rollouts are required unless modified to avoid decoding loops; motivates PlaNet's RSSM that avoids decoding during planning.",
            "task_performance": "Good for video/sequence modeling benchmarks in cited literature; not directly benchmarked numerically in PlaNet.",
            "task_utility_analysis": "Useful for capturing temporal stochasticity; feed-back of reconstructed observations can hinder efficient latent rollouts for planning tasks.",
            "tradeoffs_observed": "Expressivity for sequential modeling vs computational cost of autoregressive decoding during long rollouts.",
            "design_choices": "Stochastic latents at each time step combined with RNN dynamics; variational training with ELBO.",
            "comparison_to_alternatives": "PlaNet emphasizes a filtering posterior and architectures that permit cheap latent rollouts (no image decoding during planning), addressing a practical limitation of some VRNN-style approaches.",
            "uuid": "e1218.6",
            "source_info": {
                "paper_title": "Learning Latent Dynamics for Planning from Pixels",
                "publication_date_yy_mm": "2018-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "World models",
            "rating": 2,
            "sanitized_title": "world_models"
        },
        {
            "paper_title": "Embed to control: A locally linear latent dynamics model for control from raw images",
            "rating": 2,
            "sanitized_title": "embed_to_control_a_locally_linear_latent_dynamics_model_for_control_from_raw_images"
        },
        {
            "paper_title": "Robust locally-linear controllable embedding",
            "rating": 2,
            "sanitized_title": "robust_locallylinear_controllable_embedding"
        },
        {
            "paper_title": "A recurrent latent variable model for sequential data",
            "rating": 2,
            "sanitized_title": "a_recurrent_latent_variable_model_for_sequential_data"
        },
        {
            "paper_title": "Stochastic variational video prediction",
            "rating": 1,
            "sanitized_title": "stochastic_variational_video_prediction"
        },
        {
            "paper_title": "Learning and querying fast generative models for reinforcement learning",
            "rating": 1,
            "sanitized_title": "learning_and_querying_fast_generative_models_for_reinforcement_learning"
        }
    ],
    "cost": 0.0182635,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning Latent Dynamics for Planning from Pixels</p>
<p>Danijar Hafner 
Google Brain 
Timothy Lillicrap 
Ian Fischer 
Google Research 
Ruben Villegas Google 
Brain David 
Ha Google 
Brain Honglak 
Lee Google 
Brain James 
Davidson Google Brain </p>
<p>DeepMind</p>
<p>Learning Latent Dynamics for Planning from Pixels</p>
<p>Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from pixels and chooses actions through online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this problem using a latent dynamics model with both deterministic and stochastic transition function and a generalized variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards. PlaNet uses significantly fewer episodes and reaches final performance close to and sometimes higher than top model-free algorithms.</p>
<p>Introduction</p>
<p>Planning is a natural and powerful approach to decision making problems with known dynamics, such as game playing and simulated robot control (Tassa et al., 2012;Silver et al., 2017;Moravčík et al., 2017). To plan in unknown environments, the agent needs to learn the dynamics from experience. Learning dynamics models that are accurate enough for planning has been a long-standing challenge. Some of the difficulties include model inaccuracies, accumulating errors of multi-step predictions, capturing multiple plausible futures, and overconfident predictions outside of the training distribution.</p>
<p>Planning using learned models offers several potential benefits over model-free reinforcement learning. First, model-based planning can be more data efficient because it leverages a richer training signal and does not require propagating rewards through Bellman backups. Moreover, planning carries the promise of increasing performance just by increasing the computational budget for searching for actions, as shown by Silver et al. (2017). Finally, learned dynamics can be independent of any specific task and thus have the potential to transfer well to other tasks in the same environment.</p>
<p>Recent work has shown promise in learning the dynamics of simple low-dimensional environments Chua et al., 2018;. However, these typically assume access to the underlying state of the world and the reward function, which may not be available in practice. In high-dimensional environments, the dynamics can be learned in a compact latent space to enable fast planning. The success of such latent models has been limited to simple tasks such as balancing cartpoles and controlling 2-link arms from dense rewards (Watter et al., 2015;Banijamali et al., 2017).</p>
<p>In this paper, we propose the Deep Planning Network (PlaNet), a model-based agent that learns the environment dynamics from pixels and chooses actions through online planning in latent space. To learn the dynamics, we use a transition model with both stochastic and deterministic components and train it using a generalized variational objective that encourages multi-step predictions. PlaNet solves several continuous control tasks from pixels that are more difficult than those previously solved by planning with learned models.</p>
<p>The key contributions of this paper are:</p>
<p>• Planning in latent spaces. We train a latent dynamics model while collecting data online using planning in latent space. Our agent solves several environments with contact dynamics, ground friction, and sparse rewards from the DeepMind control suite , shown in Figure 1. It significantly outperforms the model-free algorithm A3C (Mnih et al., 2016) and in some cases D4PG (Barth-Maron et al., 2018) in terms of final performance, while using 50× less environment interaction and a similar amount of computation time.</p>
<p>The results show that learning latent dynamics models for planning in image domains is a promising approach. • Recurrent state space model.</p>
<p>Stochasticity in the dynamics model is be important to capture partial observability, provide robustness during planning, and represent unpredictable parts of the environment. On the other hand, fully stochastic transitions make it difficult to remember information over many steps and sampled outliers may cause inconsistent trajectories and diverging state sequences. We design a latent dynamics model with both deterministic and stochastic components (Buesing et al., 2018;. Our experiments indicate this to be crucial for high planning performance.</p>
<p>• Latent overshooting. For planning, we need the model to accurately predict ahead for multiple time steps. However, the standard variational bound for latent sequence models only trains one-step predictions. For a model of limited capacity and restricted posterior family, the solution to this objective does not necessarily coincide with the best multi-step predictions Talvitie, 2014). We generalize the variational bound to include all multi-step predictions. Using only the terms in latent space results in a fast and effective regularizer that improves long-term predictions and is compatible with any latent sequence model.</p>
<p>Latent Space Planning</p>
<p>To solve unknown environments via planning, we need to model the environment dynamics from experience. PlaNet does so by iteratively collecting data using fast planning in latent space and training the dynamics model on the gathered data. In this section, we introduce notation for the environment and describe the general implementation of our model-based planning agent. In this section, we assume access to a learned dynamics model. Our design and training objective for this model are detailed in Sections 3 and 4, respectively.</p>
<p>Problem setup Since individual image observations generally do not reveal the full state of the environment, we consider a partially observable Markov decision process (POMDP). We define a Algorithm 1: Deep Planning Network (PlaNet) Input : Environment, planner, transition model p θ (s t | s t−1 , a t−1 ), observation model p θ (o t | s t ), reward model p θ (r t | s t ), encoder q θ (s t | o 1:t , a 1:t−1 ), exploration noise source ∼ p( ), action repeat R, number of seed episodes S, data collection interval C, batch size B, chunk length L, learning rate α. 
r k t , o k t+1 ← env.step(a t ) 15 r t , o t+1 ← R k=1 r k t , o R t+1 16 D ← D ∪ {(o t , a t , r t ) T t=1 } discrete time step t,
hidden states s t , image observations o t , continuous action vectors a t , and scalar rewards r t , that follow the stochastic dynamics Transition function:
s t ∼ p(s t | s t−1 , a t−1 ) Observation function: o t ∼ p(o t | s t ) Reward function: r t ∼ p(r t | s t ) Policy: r t ∼ p(a t | o ≤t , a &lt;t ),(1)
where we assume a fixed initial state s 0 without loss of generality. The goal is to implement a policy p(a t | o ≤t , a &lt;t ) that maximizes the expected sum of rewards E p T τ =t+1 p(r τ | s τ ) , where the expectation is over the functions of the environment and the policy.</p>
<p>Model-based planning PlaNet learns a transition model p(s t | s t−1 , a t−1 ), observation model p(o t | s t ), and reward model p(r t | s t ) from previously experienced episodes (notr cursive letters). The observation model provides a training signal but is not used for planning. We also learn an encoder q(s t | o ≤t , a &lt;t ) to infer an approximate belief over the current hidden state from the history using filtering. Given these components, we implement the policy as planning algorithm that searches for the best sequence of future actions. We use model-predictive control (MPC; Richards, 2005) to allow the agent to adapt its plan based on new observations, meaning we replan at each step. In contrast to model-free and hybrid reinforcement learning algorithms, there is no learned function that directly predicts actions.</p>
<p>Experience collection Since the agent may not initially visit all parts of the environment, we need to iteratively collect new experience and refine the dynamics model. We do so by planning with the partially trained model, as shown in Algorithm 1. Starting from a small amount of S seed episodes collected under random actions, we train the model and add one additional episode to the data set every C update steps. When collecting episodes for the data set, we add small Gaussian exploration noise to the action. To reduce the planning horizon and support model learning, we repeat each action R times, as is common in reinforcement learning (Mnih et al., 2015;.</p>
<p>Planning algorithm We use the cross entropy method (CEM; Rubinstein, 1997;Chua et al., 2018) to search for the best action sequence under the model, as outlined in Algorithm 2. CEM Algorithm 2: Cross entropy method planner Input : Current state belief q θ (s t | o ≤t , a &lt;t ), transition model p θ (s t | s t−1 , a t−1 ), reward model p θ (r t | s t ), planning horizon distance H, optimization iterations I, candidates per iteration J, number of candidates to re-fit K.</p>
<p>1 Initialize factorized belief over action sequences q(a t:t+H ) ← Normal(0, I). 2 for optimization iteration i = 1..I do // Evaluate J action sequences from the current belief.
3 for candidate action sequence j = 1..J do 4 a (j) t:t+H ∼ q(a t:t+H ) 5 s (j) t:t+H+1 ∼ q θ (s t | o 1:t , a 1:t−1 ) t+H+1 τ =t+1 p θ (s τ | s τ −1 , a (j) τ −1 ) 6 R (j) = t+H+1 τ =t+1 E[p θ (r τ | s (j) τ )] // Re-fit belief to the K best action sequences. 7 K ← argsort({R (j) } J j=1 ) 1:K 8 µ t:t+H = 1 K k∈K a (k) t:t+H , σ t:t+H = 1 K−1 k∈K |a (k) t:t+H − µ t:t+H |. 9
q(a t:t+H ) ← Normal(µ t:t+H , σ 2 t:t+H I) 10 return first action mean µ t .</p>
<p>is a population-based optimization algorithm that infers a distribution over action sequences that maximize the objective. For this, we initialize a time-dependent diagonal Gaussian belief over action sequences a t:t+H ∼ Normal(µ t:t+H , σ 2 t:t+H I), where t is the current time step of the agent and H is the length of the planning horizon. Starting from zero mean and unit variance, we repeatedly sample J candidate action sequences, evaluate them under the model, and re-fit the belief to the top K action sequences. After I iterations, the planner returns the mean of the belief for the first time step, µ t . Importantly, after receiving the next observation, the belief over action sequences starts from zero mean and unit variance again to avoid local optima.</p>
<p>To evaluate a candidate action sequence under the learned model, we sample a state trajectory starting from the current state belief, and sum the mean rewards predicted along the sequence. Since we use a population-based optimizer, we found it sufficient to consider a single trajectory per action sequence. Because the reward is modeled as function of the latent state, the planner can operate purely in latent space without generating images, which allows for fast evaluation of large batches of action sequences. The next section introduces the latent dynamics model that the planner operates on.</p>
<p>Recurrent State Space Model</p>
<p>For planning, we need to evaluate thousands of action sequences at every time step of the agent. Therefore, we use a recurrent state-space model (RSSM) that can predict forward purely in latent space, similar to recently proposed models (Karl et al., 2016;Buesing et al., 2018;. Instead of an extensive comparison to prior architectures, we highlight two findings that can guide future designs of dynamics models: our experiments show that both stochastic and deterministic paths in the transition model are crucial for successful planning. In this section, remind the reader of latent state-space models and then describe our dynamics model.</p>
<p>Latent dynamics</p>
<p>We consider sequences {o t , a t , r t } T t=1 with discrete time step t, high-dimensional image observations o t , continuous action vectors a t , and scalar rewards r t . A typical latent state-space model is shown in Figure 2b and resembles the structure of a partially observable Markov decision process. It defines the generative process of the images and rewards using a hidden state sequence {s t } T t=1 , Transition model:
s t ∼ p(s t | s t−1 , a t−1 ) Observation model: o t ∼ p(o t | s t ) Reward model: r t ∼ p(r t | s t ),(2)
where we assume a fixed initial state s 0 without loss of generality. The transition model is Gaussian with mean and variance parameterized by a feed-forward neural network, the observation model is Gaussian with mean parameterized by a deconvolutional neural network and identity covariance, and the reward model is a scalar Gaussian with mean parameterized by a feed-forward neural network and unit variance. Note that the log-likelihood under a Gaussian distribution with unit variance equals the mean squared error up to a constant.
o 1 , r 1 o 2 , r 2 o 3 , r 3 h 1 h 2 h 3 a 1 a 2 (a) Deterministic model (RNN) o 1 , r 1 o 2 , r 2 o 3 , r 3 s 1 s 2 s 3 a 1 a 2 (b) Stochastic model (SSM) o 1 , r 1 o 2 , r 2 o 3 , r 3 s 1 s 2 s 3 h 1 h 2 h 3 a 1 a 2 (c) Recurrent state-space model (RSSM)
Variational encoder Since the model is non-linear, we cannot directly compute the state posteriors that are needed for parameter learning. Instead, we use an encoder q(s 1:T ) = T t=1 q(s t | s t−1 , a t−1 , o t ) to infer approximate state posteriors from past observations and actions, where q(s t | s t−1 , a t−1 , o t ) is a diagonal Gaussian with mean and variance parameterized by a convolutional neural network followed by a feed-forward neural network. We use the filtering posterior that conditions on past observations since we are ultimately interested in using the model for planning, but one may also use the full smoothing posterior during training .</p>
<p>Training objective Using the encoder, we construct a variational bound on the data log-likelihood. For simplicity, we write losses for predicting only the observations -the reward losses follow by analogy. The variational bound obtained using Jensen's inequality is
ln p(o 1:T | a 1:T ) = ln E p(s 1:T |a 1:T ) T t=1 p(o t | s t ) = ln E q(s 1:T ) T t=1 p(o t | s t )p(s t | s t−1 , a t−1 )/q(s t ) ≥ E q(s 1:T ) T t=1 ln p(o t | s t ) + ln p(s t | s t−1 , a t−1 ) − ln q(s t ) = T t=1 E q(st) [ln p(o t | s t )] reconstruction − E q(st−1) KL[q(s t ) p(s t | s t−1 , a t−1 )] complexity .(3)
Estimating the outer expectations using a single reparameterized sample yields an efficient objective for inference and learning in non-linear latent variable models that can be optimized using gradient ascent (Kingma and Welling, 2013;Rezende et al., 2014).</p>
<p>Deterministic path Despite its generality, the fully stochastic transitions make it difficult for the transition model to reliably remember information for multiple time steps. In theory, it could learn to set the variance to zero for some state components, but the optimization procedure may not find this solution. This motivates including a deterministic sequence of activation vectors {h t } T t=1 that allow the model to access not just the last state but all previous states deterministically Buesing et al., 2018). We use such a model, shown in Figure 2c, that we name recurrent state-space model (RSSM),</p>
<p>Deterministic state model:</p>
<p>h
t = f (h t−1 , s t−1 , a t−1 ) Stochastic state model: s t ∼ p(s t | h t ) Observation model: o t ∼ p(o t | h t , s t ) Reward model: r t ∼ p(r t | h t , s t ),(4)
where f (h t−1 , s t−1 , a t−1 ) is implemented as a recurrent neural network (RNN). Intuitively, we can understand this model as splitting the state into a stochastic part s t and a deterministic part h t , which depend on the stochastic and deterministic parts at the previous time step through the RNN. We use the encoder q(s 1:T ) = T t=1 q(s t | h t , o t ) to parameterize the approximate state posteriors. Importantly, all information about the observations must pass through the sampling step of the encoder to avoid a deterministic shortcut from inputs to reconstructions. Global prior The model can be trained using the same loss function (Equation 3). In addition, we add a fixed global prior to prevent the posteriors from collapsing in near-deterministic environments. This adds additional KL-divergence loss terms from each posterior to a standard Gaussian. Another interpretation of this is to define the prior at each time step as product of the learned temporal prior and the global fixed prior. In the next section, we identify a limitation of the standard objective for latent sequence models and propose a generalization of it that improves long-term predictions.</p>
<p>Latent Overshooting</p>
<p>In the previous section, we derived the typical variational bound for learning and inference in latent sequence models (Equation 3). As show in Figure 3a, this objective function contains reconstruction terms for the observations and KL-divergence regularizers for the approximate posteriors. A limitation of this objective is that the transition function p(s t | s t−1 , a t−1 ) is only trained via the KL-divergence regularizers for one-step predictions: the gradient flows through p(s t | s t−1 , a t−1 ) directly into q(s t−1 ) but never traverses a chain of multiple p(s t | s t−1 , a t−1 ). In this section, we generalize this variational bound to latent overshooting, which trains all multi-step predictions in latent space.</p>
<p>Limited capacity If we could train our model to make perfect one-step predictions, it would also make perfect multi-step predictions, so this would not be a problem. However, when using a model with limited capacity and restricted distributional family, training the model only on one-step predictions until convergence does in general not coincide with the model that is best at multi-step predictions. For successful planning, we need accurate multi-step predictions. Therefore, we take inspiration from  and earlier related ideas Villegas et al., 2017;Lamb et al., 2016), and train all multi-step predictions of the model. We develop this idea for latent sequence models, showing that multi-step predictions can be improved by a loss in latent space, without having to generate additional images.</p>
<p>Multi-step prediction We start by generalizing the standard variational bound (Equation 3) from training one-step predictions to training multi-step predictions of a given distance. The original objective applies the transition model once to each state posterior to compute the priors inside of the KL-divergence terms. To apply the transition model multiple times after another, we change the expectation around the KL-divergence from the previous posterior to a multi-step prediction. This multi-step prediction is computed by repeatedly applying the transition model to the posterior for the state d steps into the past,
q(s t | o ≤t−d , a &lt;t ) q(s t−d | o ≤t−d , a &lt;t−d ) t k=t−d+1 p(s k | s k−1 , a k−1 ) ds t−d:t−1 ,(5)
where the integrals can be estimated by sampling one or more states from q(s t−d | o ≤t−d , a &lt;t−d ) and propagating them forward through the transition model. Given this definition of a multi-step prediction, we derive the multi-step generalization of Equation 3, which is a bound not on the prior predictive distribution but on the multi-step predictive distribution,
ln p d (o 1:T | a 1:T ) = ln T t=1 E q(st|o ≤t−d ,a&lt;t) p(o t | s t ) = ln T t=1 E q(st|o ≤t ,a&lt;t) p(o t | s t )q(s t | o ≤t−d , a &lt;t )/q(s t | o ≤t , a &lt;t ) ≥ T t=1 E q(st|o ≤t ,a&lt;t) ln p(o t | s t ) + ln q(s t | o ≤t−d , a &lt;t ) − ln q(s t | o ≤t , a &lt;t ) ≥ T t=1 E q(st|o ≤t ,a&lt;t) [ln p(o t | s t )] − E q(st−1|o ≤t−d ,a&lt;t−1) KL[q(s t | o ≤t , a &lt;t ) p(s t | s t−1 , a t−1 )] .
(6) The first bound stems from moving the log inside of the outer expectation, and the second bound comes from moving the log inside of the expectation of the multi-step prediction. Note that we must have all expectations on the outside to obtain an unbiased estimator when replacing expectations by sample averages. Maximizing this objective trains the multi-step predictive distribution, which is used during planning.</p>
<p>Latent overshooting The last paragraph derives an objective to train latent sequence models for multi-step predictions of a given distance d. However, we need accurate predictions not just for a fixed distance but for all steps within the planning horizon. We introduce latent overshooting for this, an objective function for latent sequence models that generalizes the standard variational bound (Equation 3) to train the model on multi-step predictions of all distances 1 ≤ d ≤ D. Latent overshooting can be interpreted as a regularizer in latent space that encourages consistency between closed and open loop predictions, where we include weighting factors {β d } D d=1 analogously to the β-VAE by Higgins et al. (2016). These can all be set to the same value, or chosen to let the model focus more on long-term or short-term predictions. In practice, we stop gradients of the posterior distributions for overshooting distances d &gt; 1, so that the multi-step predictions are trained towards the posteriors, but not the other way around. We use this objective function for training the dynamics model in our agent.
ln p(o 1:T | a 1:T ) ≥ T t=1 E q(st|o ≤t ,a&lt;t) [ln p(o t | s t )] reconstruction − 1 D D d=1 β d E q(st−1|o ≤t−d ,a&lt;t−1) KL[q(s t | o ≤t , a &lt;t ) p(s t | s t−1 , a t−1 )] latent overshooting ,(7)</p>
<p>Model-based RL Experiments</p>
<p>We evaluate PlaNet on six continuous control tasks from pixels. We explore multiple design axes of the agent: online experience collection, the latent overshooting objective, and stochastic and deterministic paths in the dynamics model. In this section, we focus on planning performance. We refer to the appendix for video predictions and hyper parameters. We use the same hyper parameters for all tasks, except for the action repeat and KL-divergence scales. Within 50× fewer episodes, PlaNet outperforms A3C (Mnih et al., 2016) and achieves similar performance to the top model-free algorithm D4PG (Barth-Maron et al., 2018). The training time of 1-2 days on a single Nvidia V100 GPU is comparable to that of D4PG and could be improved further by parallelizing data collection. Our implementation uses TensorFlow Probability (Dillon et al., 2017) and will be open sourced. Please visit https://danijar.com/planet for videos of the trained agents.</p>
<p>For our evaluation, we consider six image-based continuous control tasks of the DeepMind control suite , shown in Figure 1. These environments provide qualitatively different challenges. The cartpole swingup task requires a long planning horizon and to memorize the cart when it is out of view, the finger spinning task includes contact dynamics between the finger and the object, the cheetah tasks exhibit larger state and action spaces, the cup task only has a sparse reward for when the ball is caught, and the walker is challenging because the robot first has to stand up and then walk, resulting in collisions with the ground that are difficult to predict. In all tasks, the only observations are third-person camera images of size 64 × 64 × 3 pixels. proprioceptive states for 100,000 episodes, on all tasks. After 2,000 episodes, it achieves similar performance to D4PG, trained from images for 100,000 episodes, except for the finger task. On the cheetah running task, PlaNet surpasses the final performance of D4PG with a relative improvement of 19%. We refer to Table 1 for numerical results, which also includes the performance of CEM planning with the true dynamics of the simulator.</p>
<p>Figure 4 additionally compares PlaNet with latent overshooting to versions with standard variational objective, and with a fixed random data set rather than collecting experience online. We observe that online data collection helps all tasks and is necessary for the finger and walker tasks. Latent overshooting is necessary for successful planning on walker and cup tasks; the sparse reward in the cup task demands accurate predictions for many time steps. It also slows down initial learning for the finger task, but increases final performance on the cartpole balance and cheetah tasks.</p>
<p>Figure 5 compares design choices of the dynamics model. For this, we train PlaNet using our recurrent state-space model (RSSM), as well as versions with purely deterministic GRU (Cho et al., 2014), and purely stochastic state-space model (SSM). We observe the importance of both stochastic and deterministic elements in the transition function on all tasks. The stochastic component might help because all tasks are stochastic from the agent's perspective due to partial observability of the start state. The noise might also add a margin to the planning objective that results in more robust action sequences. The deterministic part allows the model to remember information over many time steps and is even more important -the agent does not train without it.</p>
<p>Related Work</p>
<p>Previous work in model-based reinforcement learning has focused on planning in low-dimensional state spaces Chua et al., 2018), combining the benefits of model-based and model-free approaches (Kalweit and Boedecker, 2017;Ha and Schmidhuber, 2018;, and pure video prediction without planning Karl et al., 2016;Gemici et al., 2017;Denton and Fergus, 2018;Buesing et al., 2018;. Appendix B reviews these orthogonal research directions in more detail.</p>
<p>Relatively few works have demonstrated successful planning from pixels using learned dynamics models. The robotics community focuses on video prediction models for planning (Agrawal et al., 2016;Ebert et al., 2017) that deal with the visual complexity of the real world and solve tasks with simple underlying dynamics, such as grasping or pushing objects. In comparison, we focus on simulated environments, where we scale to larger state and action spaces, as well as sparse reward functions. E2C (Watter et al., 2015) and RCE (Banijamali et al., 2017) embed images into a latent space, where they learn local-linear latent transitions and plan for actions using LQR. These methods balance simulated cartpoles and control 2-link arms from images, but have been difficult to scale up. We lift the Markov assumption of these models, making our method applicable under partial observability, and present results on more challenging environments that include longer planning horizons, contact dynamics, and sparse rewards.</p>
<p>Discussion</p>
<p>We present PlaNet, a model-based agent that learns a latent dynamics model from image observations and chooses actions by planning in latent space. To enable accurate long-term predictions, we choose a model with both stochastic and deterministic paths and train it using our proposed latent overshooting objective. We show that our agent is successful at several continuous control tasks from image observations, reaching performance that is comparable to the best model-free algorithms while using 50× fewer episodes and similar training time.</p>
<p>Directions for future work include learning temporal abstraction instead of using a fixed action repeat, possibly through hierarchical models. To further improve final performance, one could learn a value function to approximate the sum of rewards beyond the planning horizon. Our work provides a starting point for multi-task control by sharing the dynamics model and learning multiple reward predictors. While the results are encouraging for the field of model-based reinforcement learning, further innovations are necessary to catch up with model-free algorithms on control tasks of higher difficulty.  . The training curves for these are shown as orange line in Figure 4 and as solid green line in Figure 6 in their paper. We also include CEM planning (H = 12, I = 10, J = 1000, K = 100) with the true simulator instead of learned dynamics as an estimated upper bound on performance. Numbers indicate mean final performance over 4 seeds. </p>
<p>Method</p>
<p>A Hyper Parameters</p>
<p>We use the convolutional and deconvolutional networks from Ha and Schmidhuber (2018), a GRU (Cho et al., 2014) with 200 units in the dynamics model, and implement all other functions as two fully connected layers of size 200 with ReLU activations (Nair and Hinton, 2010). Distributions in latent space are 30-dimensional diagional Gaussians with predicted mean and standard deviation.</p>
<p>We pre-process images by reducing the bit depth to 5 bits as in (Kingma and Dhariwal, 2018). The model is trained using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 10 −3 and = 10 −4 on batches of B = 50 sequence chunks of length L = 50. For latent overshooting, we use D = 50 and set β 1 = D and β &gt;1 = 1 except for the cup task, where we use β 1 = 3 × 10 −3 D and β &gt;1 = 3 × 10 −3 . The divergence terms to the fixed global prior are scaled by 0.1.</p>
<p>For planning, we use CEM with horizon length H = 12, optimization iterations I = 10, candidate samples J = 1000, and refitting to the best K = 100. We start from S = 5 seed episodes with random actions and collect another episode every C = 50 update steps with ∼ Normal(0, 0.3) action noise. The action repeat differs between domains: cartpole (R = 8), finger (R = 2), cheetah (R = 4), cup (R = 6), walker (R = 2). We find important hyper parameters to be the learning rate, the KL-divergence scales, and the action repeat.</p>
<p>C Video Predictions</p>
<p>PlaNet (  8: Planning performance on the cheetah running task with the true simulator using different planner settings. Performance ranges from 132 (blue) to 837 (yellow). Evaluating more action sequences, optimizing for more iterations, and re-fitting to fewer of the best proposals tend to improve performance. A planning horizon length of 6 is not sufficient and results in poor performance. Much longer planning horizons hurt performance because of the increased search space. For this environment, best planning horizon length is near 8 steps.</p>
<p>Figure 1 :
1Image-based control domains used in our experiments. The figure shows agent observations before downscaling them to 64 × 64 × 3 pixels. (a) For the cartpole balance and swingup tasks, the camera is fixed so the cart can move out of sight. (b) The finger spinning task includes contacts between the finger and the object. (c) The cheetah running task includes both contacts and a larger number of joints. (d) The cup task has a sparse reward that is only given once the ball is caught. (e) The walker task requires balance and modeling difficult interactions with the ground when the robot is lying down.</p>
<p>Figure 2 :
2Latent dynamics model designs. In this example, the model observes the first two time steps and predicts the third. Circles represent stochastic variables and squares deterministic variables. Solid lines denote the generative process and dashed lines the inference network. (a) Transitions in a recurrent neural network are fully deterministic. This prevents the model from capturing multiple futures and makes it easy for the planner to exploit inaccuracies. (b) Transitions in a state-space model are fully stochastic. This makes it difficult for the model to remember information over multiple time steps. (c) Our model combines both stochastic and deterministic paths in the transition function, allowing it to robustly generate multiple futures.</p>
<p>Figure 3 :
3Unrolling schemes. The labels are short for multi-step priors q ij q(s i | o ≤j , a &lt;i ). Arrows pointing to shaded circles indicate log-likelihood loss terms. Wavy lines indicate KL-divergence loss terms. (a) The standard variational objectives decodes the posterior at every step to compute the reconstruction loss. It also places a KL on the prior and posterior at every step, which trains the transition function for one-step predictions. (b) Observation overshooting decodes all multi-step predictions to apply additional reconstruction losses. This is typically too expensive for image domains. (c) Latent overshooting predicts all multi-step priors. These state beliefs are trained towards their corresponding posteriors in latent space to encourage accurate multi-step predictions.</p>
<p>Figure 4 :
4Comparison of agent designs to model-free algorithms. Plots show test performance for the number of collected episodes. We compare PlaNet using latent overshooting (Equation 7), a version with standard variational objective (Equation 3), and a version that trains from a random data set of 1000 episodes rather than collecting experience during training. The lines show medians and the areas show percentiles 5 to 95 over 4 seeds and 10 rollouts.</p>
<p>Figure 4 Figure 5 :
45compares the performance of PlaNet to the model-free algorithms reported by. Within 500 episodes, PlaNet outperforms the policy-gradient method A3C trained from Comparison of model designs. Plots show test performance for the number of collected episodes. We compare PlaNet using our RSSM (Section 3) to purely deterministic (RNN) and purely stochastic models (SSM). The RNN does not use latent overshooting, as it does not have stochastic latents. The lines show medians and the areas show percentiles 5 to 95 over 4 seeds and 10 rollouts.</p>
<p>Figure 6 :Figure 7 :
67Open-loop video predictions for test episodes. The columns 1-5 show reconstructed context frames and the remaining images are generated open-loop. Latent overshooting enables pixel-accurate predictions for 50 steps into the future in the cheetah environment. We randomly selected action sequences from test episodes collected with action noise alongside the training episodes. Open-loop state diagnostics. We freeze the dynamics model of a PlaNet agent and learn small neural networks to predict the true positions, velocities, and reward of the simulator. The open-loop predictions of these quantities show that most information about the underlying system is present in the learned latent space and can be accurately predicted forward further than the planning horizons used in this work.</p>
<p>Figure
Figure 8: Planning performance on the cheetah running task with the true simulator using different planner settings. Performance ranges from 132 (blue) to 837 (yellow). Evaluating more action sequences, optimizing for more iterations, and re-fitting to fewer of the best proposals tend to improve performance. A planning horizon length of 6 is not sufficient and results in poor performance. Much longer planning horizons hurt performance because of the increased search space. For this environment, best planning horizon length is near 8 steps.</p>
<p>Table 1 :
1Comparison of PlaNet to the model-free algorithms A3C and D4PG reported by
Acknowledgements We thank Jacob Buckman, Nicolas Heess, John Schulman, Mohammad Norouzi, Silviu Pitis, Shane Gu, Chelsea Finn, George Tucker, Steven Bohez, and Jimmy Ba for helpful discussions.
Learning to poke by poking: Experiential learning of intuitive physics. P Agrawal, A V Nair, P Abbeel, J Malik, S Levine, Advances in Neural Information Processing Systems. P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. In Advances in Neural Information Processing Systems, pages 5074-5082, 2016.</p>
<p>Learning awareness models. B Amos, L Dinh, S Cabi, T Rothörl, A Muldal, T Erez, Y Tassa, N Freitas, M Denil, International Conference on Learning Representations. B. Amos, L. Dinh, S. Cabi, T. Rothörl, A. Muldal, T. Erez, Y. Tassa, N. de Freitas, and M. Denil. Learning awareness models. In International Conference on Learning Representations, 2018.</p>
<p>M Babaeizadeh, C Finn, D Erhan, R H Campbell, S Levine, arXiv:1710.11252Stochastic variational video prediction. arXiv preprintM. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine. Stochastic variational video prediction. arXiv preprint arXiv:1710.11252, 2017.</p>
<p>. E Banijamali, R Shu, M Ghavamzadeh, H Bui, A Ghodsi, arXiv:1710.05373arXiv preprintRobust locally-linear controllable embeddingE. Banijamali, R. Shu, M. Ghavamzadeh, H. Bui, and A. Ghodsi. Robust locally-linear controllable embedding. arXiv preprint arXiv:1710.05373, 2017.</p>
<p>G Barth-Maron, M W Hoffman, D Budden, W Dabney, D Horgan, A Muldal, N Heess, T Lillicrap, arXiv:1804.08617Distributed distributional deterministic policy gradients. arXiv preprintG. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, A. Muldal, N. Heess, and T. Lil- licrap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018.</p>
<p>Scheduled sampling for sequence prediction with recurrent neural networks. S Bengio, O Vinyals, N Jaitly, N Shazeer, Advances in Neural Information Processing Systems. S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks. In Advances in Neural Information Processing Systems, pages 1171-1179, 2015.</p>
<p>Sample-efficient reinforcement learning with stochastic ensemble value expansion. J Buckman, D Hafner, G Tucker, E Brevdo, H Lee, arXiv:1807.01675arXiv preprintJ. Buckman, D. Hafner, G. Tucker, E. Brevdo, and H. Lee. Sample-efficient reinforcement learning with stochastic ensemble value expansion. arXiv preprint arXiv:1807.01675, 2018.</p>
<p>Learning and querying fast generative models for reinforcement learning. L Buesing, T Weber, S Racaniere, S Eslami, D Rezende, D P Reichert, F Viola, F Besse, K Gregor, D Hassabis, arXiv:1802.03006arXiv preprintL. Buesing, T. Weber, S. Racaniere, S. Eslami, D. Rezende, D. P. Reichert, F. Viola, F. Besse, K. Gregor, D. Hassabis, et al. Learning and querying fast generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.</p>
<p>. S Chiappa, S Racaniere, D Wierstra, S Mohamed, arXiv:1704.02254Recurrent environment simulators. arXiv preprintS. Chiappa, S. Racaniere, D. Wierstra, and S. Mohamed. Recurrent environment simulators. arXiv preprint arXiv:1704.02254, 2017.</p>
<p>Learning phrase representations using rnn encoder-decoder for statistical machine translation. K Cho, B Van Merriënboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, arXiv:1406.1078arXiv preprintK. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, arXiv:1805.12114arXiv preprintK. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. arXiv preprint arXiv:1805.12114, 2018.</p>
<p>A recurrent latent variable model for sequential data. J Chung, K Kastner, L Dinh, K Goel, A C Courville, Y Bengio, Advances in neural information processing systems. J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio. A recurrent latent variable model for sequential data. In Advances in neural information processing systems, pages 2980-2988, 2015.</p>
<p>Pilco: A model-based and data-efficient approach to policy search. M Deisenroth, C E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)M. Deisenroth and C. E. Rasmussen. Pilco: A model-based and data-efficient approach to policy search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pages 465-472, 2011.</p>
<p>Stochastic video generation with a learned prior. E Denton, R Fergus, arXiv:1802.07687arXiv preprintE. Denton and R. Fergus. Stochastic video generation with a learned prior. arXiv preprint arXiv:1802.07687, 2018.</p>
<p>. J V Dillon, I Langmore, D Tran, E Brevdo, S Vasudevan, D Moore, B Patton, A Alemi, M Hoffman, R A Saurous, arXiv:1711.10604Tensorflow distributions. arXiv preprintJ. V. Dillon, I. Langmore, D. Tran, E. Brevdo, S. Vasudevan, D. Moore, B. Patton, A. Alemi, M. Hoffman, and R. A. Saurous. Tensorflow distributions. arXiv preprint arXiv:1711.10604, 2017.</p>
<p>Probabilistic recurrent state-space models. A Doerr, C Daniel, M Schiegg, D Nguyen-Tuong, S Schaal, M Toussaint, S Trimpe, arXiv:1801.10395arXiv preprintA. Doerr, C. Daniel, M. Schiegg, D. Nguyen-Tuong, S. Schaal, M. Toussaint, and S. Trimpe. Probabilistic recurrent state-space models. arXiv preprint arXiv:1801.10395, 2018.</p>
<p>Self-supervised visual planning with temporal skip connections. F Ebert, C Finn, A X Lee, S Levine, arXiv:1710.05268arXiv preprintF. Ebert, C. Finn, A. X. Lee, and S. Levine. Self-supervised visual planning with temporal skip connections. arXiv preprint arXiv:1710.05268, 2017.</p>
<p>Deep visual foresight for planning robot motion. C Finn, S Levine, 2017 IEEE International Conference on. IEEERobotics and Automation (ICRAC. Finn and S. Levine. Deep visual foresight for planning robot motion. In Robotics and Automation (ICRA), 2017 IEEE International Conference on, pages 2786-2793. IEEE, 2017.</p>
<p>Improving pilco with bayesian neural network dynamics models. Y Gal, R Mcallister, C E Rasmussen, Data-Efficient Machine Learning workshop, ICML. Y. Gal, R. McAllister, and C. E. Rasmussen. Improving pilco with bayesian neural network dynamics models. In Data-Efficient Machine Learning workshop, ICML, 2016.</p>
<p>M Gemici, C.-C Hung, A Santoro, G Wayne, S Mohamed, D J Rezende, D Amos, T Lillicrap, arXiv:1702.04649Generative temporal models with memory. arXiv preprintM. Gemici, C.-C. Hung, A. Santoro, G. Wayne, S. Mohamed, D. J. Rezende, D. Amos, and T. Lillicrap. Generative temporal models with memory. arXiv preprint arXiv:1702.04649, 2017.</p>
<p>K Gregor, F Besse, arXiv:1806.03107Temporal difference variational auto-encoder. arXiv preprintK. Gregor and F. Besse. Temporal difference variational auto-encoder. arXiv preprint arXiv:1806.03107, 2018.</p>
<p>World models. D Ha, J Schmidhuber, arXiv:1803.10122arXiv preprintD. Ha and J. Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.</p>
<p>M Henaff, W F Whitney, Y Lecun, arXiv:1705.07177Model-based planning with discrete and continuous actions. arXiv preprintM. Henaff, W. F. Whitney, and Y. LeCun. Model-based planning with discrete and continuous actions. arXiv preprint arXiv:1705.07177, 2018.</p>
<p>beta-vae: Learning basic visual concepts with a constrained variational framework. I Higgins, L Matthey, A Pal, C Burgess, X Glorot, M Botvinick, S Mohamed, A Lerchner, International Conference on Learning Representations. I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, 2016.</p>
<p>Synthesizing neural network controllers with probabilistic model based reinforcement learning. J C G Higuera, D Meger, G Dudek, arXiv:1803.02291arXiv preprintJ. C. G. Higuera, D. Meger, and G. Dudek. Synthesizing neural network controllers with probabilistic model based reinforcement learning. arXiv preprint arXiv:1803.02291, 2018.</p>
<p>M Igl, L Zintgraf, T A Le, F Wood, S Whiteson, arXiv:1806.02426Deep variational reinforcement learning for pomdps. arXiv preprintM. Igl, L. Zintgraf, T. A. Le, F. Wood, and S. Whiteson. Deep variational reinforcement learning for pomdps. arXiv preprint arXiv:1806.02426, 2018.</p>
<p>N Kalchbrenner, A Oord, K Simonyan, I Danihelka, O Vinyals, A Graves, K Kavukcuoglu, arXiv:1610.00527Video pixel networks. arXiv preprintN. Kalchbrenner, A. v. d. Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and K. Kavukcuoglu. Video pixel networks. arXiv preprint arXiv:1610.00527, 2016.</p>
<p>Uncertainty-driven imagination for continuous deep reinforcement learning. G Kalweit, J Boedecker, Conference on Robot Learning. G. Kalweit and J. Boedecker. Uncertainty-driven imagination for continuous deep reinforcement learning. In Conference on Robot Learning, pages 195-206, 2017.</p>
<p>Deep variational bayes filters: Unsupervised learning of state space models from raw data. M Karl, M Soelch, J Bayer, P Van Der, Smagt, arXiv:1605.06432arXiv preprintM. Karl, M. Soelch, J. Bayer, and P. van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. arXiv preprint arXiv:1605.06432, 2016.</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>D P Kingma, P Dhariwal, Glow, arXiv:1807.03039Generative flow with invertible 1x1 convolutions. arXiv preprintD. P. Kingma and P. Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. arXiv preprint arXiv:1807.03039, 2018.</p>
<p>D P Kingma, M Welling, arXiv:1312.6114Auto-encoding variational bayes. arXiv preprintD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>R G Krishnan, U Shalit, D Sontag, arXiv:1511.05121Deep kalman filters. arXiv preprintR. G. Krishnan, U. Shalit, and D. Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.</p>
<p>Model-ensemble trust-region policy optimization. T Kurutach, I Clavera, Y Duan, A Tamar, P Abbeel, arXiv:1802.10592arXiv preprintT. Kurutach, I. Clavera, Y. Duan, A. Tamar, and P. Abbeel. Model-ensemble trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.</p>
<p>Professor forcing: A new algorithm for training recurrent networks. A M Lamb, A G A P Goyal, Y Zhang, S Zhang, A C Courville, Y Bengio, Advances In Neural Information Processing Systems. A. M. Lamb, A. G. A. P. GOYAL, Y. Zhang, S. Zhang, A. C. Courville, and Y. Bengio. Professor forcing: A new algorithm for training recurrent networks. In Advances In Neural Information Processing Systems, pages 4601-4609, 2016.</p>
<p>Deep multi-scale video prediction beyond mean square error. M Mathieu, C Couprie, Y Lecun, arXiv:1511.05440arXiv preprintM. Mathieu, C. Couprie, and Y. LeCun. Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440, 2015.</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, Nature. 5187540529V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried- miller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International Conference on Machine Learning. V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928-1937, 2016.</p>
<p>Learning multimodal transition dynamics for model-based reinforcement learning. T M Moerland, J Broekens, C M Jonker, arXiv:1705.00470arXiv preprintT. M. Moerland, J. Broekens, and C. M. Jonker. Learning multimodal transition dynamics for model-based reinforcement learning. arXiv preprint arXiv:1705.00470, 2017.</p>
<p>Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. M Moravčík, M Schmid, N Burch, V Lisỳ, D Morrill, N Bard, T Davis, K Waugh, M Johanson, M Bowling, Science. 3566337M. Moravčík, M. Schmid, N. Burch, V. Lisỳ, D. Morrill, N. Bard, T. Davis, K. Waugh, M. Johanson, and M. Bowling. Deepstack: Expert-level artificial intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.</p>
<p>Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. A Nagabandi, G Kahn, R S Fearing, S Levine, arXiv:1708.02596arXiv preprintA. Nagabandi, G. Kahn, R. S. Fearing, and S. Levine. Neural network dynamics for model-based deep reinforcement learning with model-free fine-tuning. arXiv preprint arXiv:1708.02596, 2017.</p>
<p>Rectified linear units improve restricted boltzmann machines. V Nair, G E Hinton, Proceedings of the 27th international conference on machine learning (ICML-10). the 27th international conference on machine learning (ICML-10)V. Nair and G. E. Hinton. Rectified linear units improve restricted boltzmann machines. In Pro- ceedings of the 27th international conference on machine learning (ICML-10), pages 807-814, 2010.</p>
<p>Action-conditional video prediction using deep networks in atari games. J Oh, X Guo, H Lee, R L Lewis, S Singh, Advances in Neural Information Processing Systems. J. Oh, X. Guo, H. Lee, R. L. Lewis, and S. Singh. Action-conditional video prediction using deep networks in atari games. In Advances in Neural Information Processing Systems, pages 2863-2871, 2015.</p>
<p>Stochastic backpropagation and approximate inference in deep generative models. D J Rezende, S Mohamed, D Wierstra, arXiv:1401.4082arXiv preprintD. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.</p>
<p>Robust constrained model predictive control. A G Richards, Massachusetts Institute of TechnologyPhD thesisA. G. Richards. Robust constrained model predictive control. PhD thesis, Massachusetts Institute of Technology, 2005.</p>
<p>Optimization of computer simulation models with rare events. R Y Rubinstein, European Journal of Operational Research. 991R. Y. Rubinstein. Optimization of computer simulation models with rare events. European Journal of Operational Research, 99(1):89-112, 1997.</p>
<p>Mastering the game of go without human knowledge. D Silver, J Schrittwieser, K Simonyan, I Antonoglou, A Huang, A Guez, T Hubert, L Baker, M Lai, A Bolton, Nature. 5507676354D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker, M. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676): 354, 2017.</p>
<p>A Srinivas, A Jabri, P Abbeel, S Levine, C Finn, arXiv:1804.00645Universal planning networks. arXiv preprintA. Srinivas, A. Jabri, P. Abbeel, S. Levine, and C. Finn. Universal planning networks. arXiv preprint arXiv:1804.00645, 2018.</p>
<p>Synthesis and stabilization of complex behaviors through online trajectory optimization. E Talvitie ; Y. Tassa, T Erez, E Todorov, Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEEUAIE. Talvitie. Model regularization for stable sample rollouts. In UAI, pages 780-789, 2014. Y. Tassa, T. Erez, and E. Todorov. Synthesis and stabilization of complex behaviors through online trajectory optimization. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pages 4906-4913. IEEE, 2012.</p>
<p>. Y Tassa, Y Doron, A Muldal, T Erez, Y Li, D D , . L Casas, D Budden, A Abdolmaleki, J Merel, A Lefrancq, arXiv:1801.00690Deepmind control suite. arXiv preprintY. Tassa, Y. Doron, A. Muldal, T. Erez, Y. Li, D. d. L. Casas, D. Budden, A. Abdolmaleki, J. Merel, A. Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>Neural discrete representation learning. A Van Den Oord, O Vinyals, Advances in Neural Information Processing Systems. A. van den Oord, O. Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pages 6309-6318, 2017.</p>
<p>Improving multi-step prediction of learned time series models. A Venkatraman, M Hebert, J A Bagnell, AAAI. A. Venkatraman, M. Hebert, and J. A. Bagnell. Improving multi-step prediction of learned time series models. In AAAI, pages 3024-3030, 2015.</p>
<p>Learning to generate long-term future via hierarchical prediction. J Villegas, Y Yang, S Zou, X Sohn, H Lin, Lee, arXiv:1704.05831arXiv preprintVillegas, J. Yang, Y. Zou, S. Sohn, X. Lin, and H. Lee. Learning to generate long-term future via hierarchical prediction. arXiv preprint arXiv:1704.05831, 2017.</p>
<p>Generating videos with scene dynamics. C Vondrick, H Pirsiavash, A Torralba, Advances In Neural Information Processing Systems. C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In Advances In Neural Information Processing Systems, 2016.</p>
<p>Embed to control: A locally linear latent dynamics model for control from raw images. M Watter, J Springenberg, J Boedecker, M Riedmiller, Advances in neural information processing systems. M. Watter, J. Springenberg, J. Boedecker, and M. Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In Advances in neural information processing systems, pages 2746-2754, 2015.</p>
<p>Unsupervised predictive memory in a goal-directed agent. G Wayne, C.-C Hung, D Amos, M Mirza, A Ahuja, A Grabska-Barwinska, J Rae, P Mirowski, J Z Leibo, A Santoro, arXiv:1803.10760arXiv preprintG. Wayne, C.-C. Hung, D. Amos, M. Mirza, A. Ahuja, A. Grabska-Barwinska, J. Rae, P. Mirowski, J. Z. Leibo, A. Santoro, et al. Unsupervised predictive memory in a goal-directed agent. arXiv preprint arXiv:1803.10760, 2018.</p>
<p>Imagination-augmented agents for deep reinforcement learning. T Weber, S Racanière, D P Reichert, L Buesing, A Guez, D J Rezende, A P Badia, O Vinyals, N Heess, Y Li, arXiv:1707.06203arXiv preprintT. Weber, S. Racanière, D. P. Reichert, L. Buesing, A. Guez, D. J. Rezende, A. P. Badia, O. Vinyals, N. Heess, Y. Li, et al. Imagination-augmented agents for deep reinforcement learning. arXiv preprint arXiv:1707.06203, 2017.</p>
<p>In the regime of control tasks with only a few state variables, such as the cart pole and mountain car tasks, PILCO (Deisenroth and Rasmussen, 2011) achieves remarkable sample efficiency using Gaussian processes to model the dynamics. Similar approaches using neural networks dynamics models can solve two-link balancing problems. Gal, Chua et al. ) use ensembles of neural networks, scaling up to the cheetah running task. The limitation of these methods. is that they access the low-dimensional Markovian state of the underlying system and sometimes the reward function. Amos et al.B Additional Related Work Planning in state space When low-dimensional states of the environment are available to the agent, it is possible to learn the dynamics directly in state space. In the regime of control tasks with only a few state variables, such as the cart pole and mountain car tasks, PILCO (Deisenroth and Rasmussen, 2011) achieves remarkable sample efficiency using Gaussian processes to model the dynamics. Similar approaches using neural networks dynamics models can solve two-link balancing problems (Gal et al., 2016; Higuera et al., 2018) and implement planning via gradients (Henaff et al., 2018). Chua et al. (2018) use ensembles of neural networks, scaling up to the cheetah running task. The limitation of these methods is that they access the low-dimensional Markovian state of the underlying system and sometimes the reward function. Amos et al. (2018</p>
<p>Nagabandi, Hybrid agents The challenges of model-based RL have motivated the research community to develop hybrid agents that accelerate policy learning by training on imagined experience (Kalweit and Boedecker. improving feature representations. or leveraging the information content of the model directly. integrated planning computation using reinforcement learning and without prediction loss, yet require expert demonstrations for trainingHybrid agents The challenges of model-based RL have motivated the research community to develop hybrid agents that accelerate policy learning by training on imagined experience (Kalweit and Boedecker, 2017; Nagabandi et al., 2017; Kurutach et al., 2018; Buckman et al., 2018; Ha and Schmidhuber, 2018), improving feature representations (Wayne et al., 2018; Igl et al., 2018), or leveraging the information content of the model directly (Weber et al., 2017). Srinivas et al. (2018) learn a policy network with integrated planning computation using reinforcement learning and without prediction loss, yet require expert demonstrations for training.</p>
<p>) train a dynamics model on all multi-step predictions at once. We generalize this idea to latent sequence models trained via variational inference. Latent sequence models Classic work has explored models for non-Markovian observation sequences, including recurrent neural networks (RNNs) with deterministic hidden state and probabilistic state-space models (SSMs). Bengio, Multi-step predictions Training sequence models on multi-step predictions has been explored for several years. Scheduled sampling. The ideas behind variational autoencoders (Kingma and WellingMulti-step predictions Training sequence models on multi-step predictions has been explored for several years. Scheduled sampling (Bengio et al., 2015) changes the rollout distance of the sequence model over the course of training. Hallucinated replay (Talvitie, 2014) mixes predictions into the data set to indirectly train multi-step predictions. Venkatraman et al. (2015) take an imitation learning approach. Recently, Amos et al. (2018) train a dynamics model on all multi-step predictions at once. We generalize this idea to latent sequence models trained via variational inference. Latent sequence models Classic work has explored models for non-Markovian observation se- quences, including recurrent neural networks (RNNs) with deterministic hidden state and probabilistic state-space models (SSMs). The ideas behind variational autoencoders (Kingma and Welling, 2013;</p>
<p>2015) combines RNNs and SSMs and is trained via variational inference. In contrast to our RSSM, it feeds generated observations back into the model which makes forward predictions expensive. Karl et al. (2016) address mode collapse to a single future by restricting the transition function. Rezende, ) have enabled non-linear SSMs that are trained via variational inference. ) stabilize training of purely stochastic models. hybrid agent instead for explicit planningRezende et al., 2014) have enabled non-linear SSMs that are trained via variational inference (Kr- ishnan et al., 2015). The VRNN (Chung et al., 2015) combines RNNs and SSMs and is trained via variational inference. In contrast to our RSSM, it feeds generated observations back into the model which makes forward predictions expensive. Karl et al. (2016) address mode collapse to a single future by restricting the transition function, (Moerland et al., 2017) focus on multi-modal transitions, and Doerr et al. (2018) stabilize training of purely stochastic models. Buesing et al. (2018) propose a model similar to ours but use in a hybrid agent instead for explicit planning.</p>
<p>) use adversarial losses. In simulated environments, Gemici et al. (2017) augment dynamics models with an external memory to remember long-time contexts. van den Oord et al. (2017) propose a variational model that avoids sampling using a nearest neighbor look-up. Oh, Denton and Fergus. Kalchbrenner et al. (2016) introduce an autoregressive video prediction model using gated CNNs and LSTMs. Recent approaches introduce stochasticity to the model to capture multiple futures. yielding high fidelity image predictions. These models are complimentary to our approachVideo prediction Video prediction is an active area of research in deep learning. Oh et al. (2015) and Chiappa et al. (2017) achieve visually plausible predictions on Atari games using deterministic models. Kalchbrenner et al. (2016) introduce an autoregressive video prediction model using gated CNNs and LSTMs. Recent approaches introduce stochasticity to the model to capture multiple futures (Babaeizadeh et al., 2017; Denton and Fergus, 2018). To obtain realistic predictions, Mathieu et al. (2015) and Vondrick et al. (2016) use adversarial losses. In simulated environments, Gemici et al. (2017) augment dynamics models with an external memory to remember long-time contexts. van den Oord et al. (2017) propose a variational model that avoids sampling using a nearest neighbor look-up, yielding high fidelity image predictions. These models are complimentary to our approach.</p>            </div>
        </div>

    </div>
</body>
</html>
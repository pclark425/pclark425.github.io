<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3049 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3049</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3049</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-269293320</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.13501v1.pdf" target="_blank">A Survey on the Memory Mechanism of Large Language Model based Agents</a></p>
                <p><strong>Paper Abstract:</strong> Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3049.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3049.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoryBank: Enhancing Large Language Models with Long-term Memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented memory module for conversational agents that encodes past conversation entries into embeddings and retrieves relevant memory to maintain long-term context and personality summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoryBank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A memory-augmented conversational agent that encodes conversation entries into an embedding index (dual-tower dense retrieval) and stores distilled summaries and daily insights; designed to preserve long-term personality and event summaries for dialogue continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented textual memory (long-term conversational memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Writes conversation turns and distilled summaries into a textual memory store and vector index (FAISS); during reading it encodes current context and retrieves top-K related memory entries by similarity to include in the prompt; management includes periodic distillation into high-level summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Long-term conversational consistency / personalized dialogue</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain persona, context continuity and retrieve past facts across long conversations to produce consistent, personalized responses.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue / conversation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a retrieval-indexed long-term textual memory improves the agent's ability to recall past events and maintain persona consistency; vector-indexed retrieval and distillation into summaries are emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Indexing and retrieval accuracy can produce irrelevant hits; long-term textual memory increases storage and retrieval cost and requires summary/management policies to avoid redundancy and token bloat.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3049.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RET-LLM: A General Read-Write Memory for Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A read-write memory architecture that supports structured tuple storage and locality-sensitive retrieval to provide external, queryable memory to LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ret-llm: Towards a general read-write memory for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A general memory module enabling LLMs to write structured tuples to a memory database and retrieve relevant tuples via LSH-based retrieval to support multi-step reasoning and task solving.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>read-write external memory (structured retrieval via LSH)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Memory writing projects actions/observations into tuples stored in a database; retrieval uses locality-sensitive hashing (LSH) to find related tuples given current context; retrieved tuples are concatenated to prompts for downstream LLM decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General multi-step reasoning / interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks that require recalling prior steps or facts stored as tuples to support multi-step reasoning and decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / tool-augmented tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Read-write structured memory with LSH retrieval allows flexible storage and efficient retrieval of relevant past entries, improving the availability of relevant historical facts for LLM decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval quality depends on embedding/indexing and LSH parameters; structured tuple formats require careful design of the write projection; scaling and relevance filtering remain challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3049.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemoChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemoChat: Tuning LLMs to Use Memos for Consistent Long-range Open-domain Conversation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational agent design that tunes LLMs to use short memo summaries (memos) as an explicit textual memory abstraction for long-range dialogue consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemoChat</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A dialogue system that generates and stores memo-style summaries of conversation segments; the LLM is tuned to incorporate these memos at inference to improve long-range consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual memo summaries (short-term / session-level)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>During writing, the agent abstracts each conversation segment into a concise memo used as an index/key; at reading, memos relevant to the current query are retrieved and injected into the prompt; management includes memo generation and possible merging.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-domain multi-turn dialogue / long-range conversation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Sustain coherent, personalized conversations over long sessions by summarizing and retrieving past segments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue / conversation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Memo-style textual abstractions make long conversations tractable and improve recall of past dialog content while controlling prompt length.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Memo abstraction quality is crucial; improper memoing or retrieval can omit salient details; still constrained by context injection costs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3049.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MemGPT: Towards LLMs as Operating Systems</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual agent treating LLMs as an operating system that manages a working context (virtual context) and a memory store to orchestrate user interactions and background tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memgpt: Towards llms as operating systems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemGPT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent framework that uses a working context to hold recent histories and a memory store for persistent information, enabling multi-turn interactions and stateful behaviors much like an OS.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>hybrid textual memory (working context + persistent store)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Maintains a 'working context' for recent interactions and a persistent memory for longer-term info; the agent autonomously decides when to write and how to retrieve entries for current tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-turn interactive tasks / agent orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maintain stateful, multi-turn interactions and coordinate multiple subtasks requiring recall of prior context and persistent facts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>dialogue / orchestration</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Separating immediate working context from persistent memory supports efficient stateful interactions and reduces prompt bloat while keeping relevant recent history accessible.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Autonomous write/read policies must be carefully designed to avoid forgetting or redundant storage; system-level complexity and retrieval latency are concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3049.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenerativeAgents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Agents: Interactive Simulacra of Human Behavior</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulation framework where each agent maintains a memory of past actions and events in natural language and periodically reflects to produce higher-level summaries used for planning and behavior consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generative agents: Interactive simulacra of human behavior</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Generative Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agents that store natural-language observations of events and actions, periodically run reflection to produce abstract thoughts/insights, and use those to guide future behavior in social simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual episodic memory + reflection-derived summaries</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Writes event observations as textual memory entries, triggers reflection when sufficient events accumulate to distill higher-level summaries, and retrieves relevant memories by similarity for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Social simulation / role-playing</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Simulate human-like daily behavior and interactions where consistent persona and history are essential; requires cross-trial accumulation and abstraction of events.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>social simulation / role-play</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Natural-language episodic memory plus periodic reflection enables believable, consistent agent behavior and supports emergent social dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Scalability and efficient retrieval from large natural-language memory; choosing reflection triggers and managing contradictions across memories are open problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3049.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: Language Agents with Verbal Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that converts trial experiences into verbal reflections and applies them across trials to improve agent performance on the same task by learning from past failures and successes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: Language agents with verbal reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An LLM-based agent pipeline that records past trial transcripts, generates natural-language reflections (lessons learned), and uses those cross-trial reflections to inform subsequent trials on the same task.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>cross-trial textual memory (verbal reflections / experiential memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>After trials, the agent generates reflective summaries capturing failure reasons and improvements; these reflections are stored and retrieved in later trials to influence action choices and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Iterative task solving / ALFWorld-like interactive tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks requiring multiple trials and iterative improvement, where learning from previous trial outcomes can guide future attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interactive task solving / iterative learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Verbalizing and storing cross-trial reflections helps agents avoid past mistakes and improves subsequent trial efficiency and success rates (qualitative improvement reported in original works).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reflection quality and retrieval relevance determine benefit; storing many reflections can create redundancy and contradictions; quantitative gains depend on good reflection generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3049.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ExpeL: LLM Agents are Experiential Learners</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Agents that store completed trajectories (successful and failed) and recall similar successful trajectories as exemplars to guide behavior on new tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Expel: Llm agents are experiential learners</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExpeL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A framework that records full interaction trajectories, organizes them, and retrieves top-K successful trajectories similar to a new task to serve as examples for planning and action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trajectory memory / retrieved exemplars (cross-trial episodic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Writes full trajectories into a vector store; during a new task the system retrieves the most similar successful trajectories by similarity scores and uses them as prompt examples to guide the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Complex interactive task suites (e.g., ALFWorld / multi-step tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Solve collections of interactive tasks by leveraging prior successful trajectories as exemplars for planning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>interactive task solving / exemplar-guided planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Recalling similar successful trajectories improves problem-solving on new but related tasks by providing concrete exemplar behavior; helpful for sample efficiency and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval quality, storage size of trajectories, and efficient indexing are bottlenecks; managing inconsistent or suboptimal exemplars is necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3049.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retroformer (reflection fine-tuning for cross-trial extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model that fine-tunes a reflection model to better extract and utilize cross-trial information from past trials to improve performance on subsequent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Retroformer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A fine-tuning approach where a model dedicated to reflection is trained so the agent can extract and apply cross-trial lessons more effectively across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>parametric memory (fine-tuned reflection model) + cross-trial textual memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Uses fine-tuning of a reflection/extraction model to transform past trial transcripts into distilled insights that are later retrieved and used to condition agent planning and decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-trial learning / iterative improvement tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Tasks where leveraging distilled lessons from many prior trials can speed up or improve future trial performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>iterative learning / multi-trial tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fine-tuning a dedicated reflection model can improve the quality of extracted cross-trial knowledge, enabling better reuse of past experience.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Fine-tuning cost and preservation of unrelated knowledge (catastrophic effects) are concerns; applicability to online settings is limited by training expense.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3049.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An Open-ended Embodied Agent with Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-world game agent for Minecraft that stores learned executable skills and code snippets as memory, using them to bootstrap future skill acquisition and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Minecraft agent that writes discovered skills and executable code snippets into memory, refines them via environment feedback, and reuses them to solve new tasks and explore the environment further.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual skill memory (executable code snippets and distilled skills)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Writes code/skill artifacts discovered during play into a memory library; management refines skills based on feedback; reading retrieves relevant skills to seed new plans or compose multi-step behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Open-world Minecraft exploration and multi-step skill acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Discover and reuse low-level and higher-level skills to accomplish increasingly complex objectives in a sparse-reward open-world environment.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>open-world game playing / skill acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Storing and reusing executable skills accelerates exploration and skill composition in open-ended environments, enabling bootstrapped learning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Skill retrieval and applicability to new contexts, and scalability of stored code artifacts; aligning stored skills with correct contexts is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3049.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synapse</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A memory-backed prompting method that records trajectories of successful computer-control actions and uses them as exemplars to improve performance on new control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Synapse: Trajectory-as-exemplar prompting with memory for computer control</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Synapse</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent targeting computer control tasks that stores successful trajectories and retrieves them as exemplar prompts which guide current action selection and improve task success.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>trajectory exemplar memory (retrieval of successful past trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Writes full interaction trajectories into a memory store; during new tasks the agent retrieves exemplar trajectories with high similarity and conditions planning on those examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Computer control tasks (GUI/automation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Select UI elements and sequences of actions to accomplish GUI-based tasks; requires leveraging past successful action sequences as exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>computer control / automation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using trajectories as exemplars aids selection and sequencing decisions in computer-control tasks by providing concrete prior examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Retrieval precision and exemplar relevance; storing many trajectories increases indexing cost; exemplar mismatches can mislead planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3049.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct: Synergizing Reasoning and Acting in Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework combining chain-of-thought style reasoning with tool use and retrieval of external knowledge (e.g., Wikipedia) to perform multi-step question answering and reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that interleaves reasoning traces with external actions (tool/API calls) and retrieval to obtain missing facts; memory in ReAct is often provided by external knowledge sources accessed at reasoning time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external-knowledge retrieval (tool/API calls) and chain-of-thought traces</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>During multi-step reasoning, the agent issues tool/API calls (e.g., Wikipedia lookup) to fetch external facts which are treated as memory inputs for subsequent reasoning steps; logs of prior reasoning steps can also serve as working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-step question answering / tool-augmented reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer complex questions requiring retrieval of up-to-date facts and multi-step inference with tool usage.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>question answering / tool use</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Integrating external API/tool retrieval into the reasoning loop reduces hallucination and improves factual grounding for multi-step QA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Tool reliability and API latency; integrating and aligning external facts with internal reasoning is nontrivial and can introduce noise if retrieval is poor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e3049.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GITM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GITM: Ghost in the Minecraft (Generally Capable Agents for Open-world Environments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-world agent that leverages textual memory plus external Wiki knowledge to navigate sparse-reward environments (Minecraft), storing and retrieving successful trajectories and recipes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GITM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A Minecraft agent that stores successful trajectories as memory and accesses external Minecraft Wiki via API to supplement internal memory for crafting and navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual trajectory memory + external knowledge API retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Writes gameplay trajectories and skill artifacts to memory; retrieves similar successful trajectories and queries external Wiki APIs for craft recipes and domain knowledge to inform current planning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Minecraft item production / open-world tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce items and navigate the open-world environment where external craft knowledge and prior successful sequences are necessary to accomplish goals.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>open-world game playing / craft planning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining internal trajectory memory with external Wiki retrieval supports solving sparse-reward tasks and complex crafting trees by providing both exemplars and factual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Integration of heterogeneous memory sources (trajectories vs external Wiki); retrieval latency and contradictory information across sources.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3049.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e3049.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TiM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TiM: Think-in-Memory (Recalling and Post-thinking Enable LLMs with Long-term Memory)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that self-generates multiple thoughts post-task and uses them as memory entries to support future reasoning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Think-in-memory: Recalling and post-thinking enable llms with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TiM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An agent that, after completing tasks, generates multiple internal thoughts (post-thinking) and stores them as memory to provide abstracted, generalizable knowledge for future tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>textual self-generated thought memory + reflection</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Writes multiple self-generated thoughts/insights after task completion into memory; future reads retrieve these thoughts as higher-level guidance or examples to improve reasoning and generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Reasoning enhancement / cross-task generalization</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Improve reasoning and generalization across tasks by storing and reusing post-hoc generated internal thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>reasoning / generalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Post-task self-thought generation and storage help produce more generalizable memory entries that benefit future analogous tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Quality and relevance of self-generated thoughts determine utility; risk of storing spurious or misleading self-reflections.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Survey on the Memory Mechanism of Large Language Model based Agents', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reflexion: Language agents with verbal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Ret-llm: Towards a general read-write memory for large language models <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Expel: Llm agents are experiential learners <em>(Rating: 2)</em></li>
                <li>Synapse: Trajectory-as-exemplar prompting with memory for computer control <em>(Rating: 2)</em></li>
                <li>Memochat: Tuning llms to use memos for consistent long-range open-domain conversation <em>(Rating: 2)</em></li>
                <li>Memgpt: Towards llms as operating systems <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory <em>(Rating: 2)</em></li>
                <li>Think-in-memory: Recalling and post-thinking enable llms with long-term memory <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3049",
    "paper_id": "paper-269293320",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "MemoryBank",
            "name_full": "MemoryBank: Enhancing Large Language Models with Long-term Memory",
            "brief_description": "A retrieval-augmented memory module for conversational agents that encodes past conversation entries into embeddings and retrieves relevant memory to maintain long-term context and personality summaries.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory",
            "mention_or_use": "mention",
            "agent_name": "MemoryBank",
            "agent_description": "A memory-augmented conversational agent that encodes conversation entries into an embedding index (dual-tower dense retrieval) and stores distilled summaries and daily insights; designed to preserve long-term personality and event summaries for dialogue continuity.",
            "memory_used": true,
            "memory_type": "retrieval-augmented textual memory (long-term conversational memory)",
            "memory_mechanism_description": "Writes conversation turns and distilled summaries into a textual memory store and vector index (FAISS); during reading it encodes current context and retrieves top-K related memory entries by similarity to include in the prompt; management includes periodic distillation into high-level summaries.",
            "task_name": "Long-term conversational consistency / personalized dialogue",
            "task_description": "Maintain persona, context continuity and retrieve past facts across long conversations to produce consistent, personalized responses.",
            "task_type": "dialogue / conversation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Using a retrieval-indexed long-term textual memory improves the agent's ability to recall past events and maintain persona consistency; vector-indexed retrieval and distillation into summaries are emphasized.",
            "limitations_or_challenges": "Indexing and retrieval accuracy can produce irrelevant hits; long-term textual memory increases storage and retrieval cost and requires summary/management policies to avoid redundancy and token bloat.",
            "uuid": "e3049.0",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "RET-LLM",
            "name_full": "RET-LLM: A General Read-Write Memory for Large Language Models",
            "brief_description": "A read-write memory architecture that supports structured tuple storage and locality-sensitive retrieval to provide external, queryable memory to LLM agents.",
            "citation_title": "Ret-llm: Towards a general read-write memory for large language models",
            "mention_or_use": "mention",
            "agent_name": "RET-LLM",
            "agent_description": "A general memory module enabling LLMs to write structured tuples to a memory database and retrieve relevant tuples via LSH-based retrieval to support multi-step reasoning and task solving.",
            "memory_used": true,
            "memory_type": "read-write external memory (structured retrieval via LSH)",
            "memory_mechanism_description": "Memory writing projects actions/observations into tuples stored in a database; retrieval uses locality-sensitive hashing (LSH) to find related tuples given current context; retrieved tuples are concatenated to prompts for downstream LLM decisions.",
            "task_name": "General multi-step reasoning / interactive tasks",
            "task_description": "Tasks that require recalling prior steps or facts stored as tuples to support multi-step reasoning and decisions.",
            "task_type": "multi-step reasoning / tool-augmented tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Read-write structured memory with LSH retrieval allows flexible storage and efficient retrieval of relevant past entries, improving the availability of relevant historical facts for LLM decision-making.",
            "limitations_or_challenges": "Retrieval quality depends on embedding/indexing and LSH parameters; structured tuple formats require careful design of the write projection; scaling and relevance filtering remain challenges.",
            "uuid": "e3049.1",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MemoChat",
            "name_full": "MemoChat: Tuning LLMs to Use Memos for Consistent Long-range Open-domain Conversation",
            "brief_description": "A conversational agent design that tunes LLMs to use short memo summaries (memos) as an explicit textual memory abstraction for long-range dialogue consistency.",
            "citation_title": "Memochat: Tuning llms to use memos for consistent long-range open-domain conversation",
            "mention_or_use": "mention",
            "agent_name": "MemoChat",
            "agent_description": "A dialogue system that generates and stores memo-style summaries of conversation segments; the LLM is tuned to incorporate these memos at inference to improve long-range consistency.",
            "memory_used": true,
            "memory_type": "textual memo summaries (short-term / session-level)",
            "memory_mechanism_description": "During writing, the agent abstracts each conversation segment into a concise memo used as an index/key; at reading, memos relevant to the current query are retrieved and injected into the prompt; management includes memo generation and possible merging.",
            "task_name": "Open-domain multi-turn dialogue / long-range conversation",
            "task_description": "Sustain coherent, personalized conversations over long sessions by summarizing and retrieving past segments.",
            "task_type": "dialogue / conversation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Memo-style textual abstractions make long conversations tractable and improve recall of past dialog content while controlling prompt length.",
            "limitations_or_challenges": "Memo abstraction quality is crucial; improper memoing or retrieval can omit salient details; still constrained by context injection costs.",
            "uuid": "e3049.2",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "MemGPT",
            "name_full": "MemGPT: Towards LLMs as Operating Systems",
            "brief_description": "A conceptual agent treating LLMs as an operating system that manages a working context (virtual context) and a memory store to orchestrate user interactions and background tasks.",
            "citation_title": "Memgpt: Towards llms as operating systems",
            "mention_or_use": "mention",
            "agent_name": "MemGPT",
            "agent_description": "An agent framework that uses a working context to hold recent histories and a memory store for persistent information, enabling multi-turn interactions and stateful behaviors much like an OS.",
            "memory_used": true,
            "memory_type": "hybrid textual memory (working context + persistent store)",
            "memory_mechanism_description": "Maintains a 'working context' for recent interactions and a persistent memory for longer-term info; the agent autonomously decides when to write and how to retrieve entries for current tasks.",
            "task_name": "Multi-turn interactive tasks / agent orchestration",
            "task_description": "Maintain stateful, multi-turn interactions and coordinate multiple subtasks requiring recall of prior context and persistent facts.",
            "task_type": "dialogue / orchestration",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Separating immediate working context from persistent memory supports efficient stateful interactions and reduces prompt bloat while keeping relevant recent history accessible.",
            "limitations_or_challenges": "Autonomous write/read policies must be carefully designed to avoid forgetting or redundant storage; system-level complexity and retrieval latency are concerns.",
            "uuid": "e3049.3",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GenerativeAgents",
            "name_full": "Generative Agents: Interactive Simulacra of Human Behavior",
            "brief_description": "A simulation framework where each agent maintains a memory of past actions and events in natural language and periodically reflects to produce higher-level summaries used for planning and behavior consistency.",
            "citation_title": "Generative agents: Interactive simulacra of human behavior",
            "mention_or_use": "mention",
            "agent_name": "Generative Agents",
            "agent_description": "Agents that store natural-language observations of events and actions, periodically run reflection to produce abstract thoughts/insights, and use those to guide future behavior in social simulations.",
            "memory_used": true,
            "memory_type": "textual episodic memory + reflection-derived summaries",
            "memory_mechanism_description": "Writes event observations as textual memory entries, triggers reflection when sufficient events accumulate to distill higher-level summaries, and retrieves relevant memories by similarity for decision-making.",
            "task_name": "Social simulation / role-playing",
            "task_description": "Simulate human-like daily behavior and interactions where consistent persona and history are essential; requires cross-trial accumulation and abstraction of events.",
            "task_type": "social simulation / role-play",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Natural-language episodic memory plus periodic reflection enables believable, consistent agent behavior and supports emergent social dynamics.",
            "limitations_or_challenges": "Scalability and efficient retrieval from large natural-language memory; choosing reflection triggers and managing contradictions across memories are open problems.",
            "uuid": "e3049.4",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: Language Agents with Verbal Reinforcement Learning",
            "brief_description": "A method that converts trial experiences into verbal reflections and applies them across trials to improve agent performance on the same task by learning from past failures and successes.",
            "citation_title": "Reflexion: Language agents with verbal reinforcement learning",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "An LLM-based agent pipeline that records past trial transcripts, generates natural-language reflections (lessons learned), and uses those cross-trial reflections to inform subsequent trials on the same task.",
            "memory_used": true,
            "memory_type": "cross-trial textual memory (verbal reflections / experiential memory)",
            "memory_mechanism_description": "After trials, the agent generates reflective summaries capturing failure reasons and improvements; these reflections are stored and retrieved in later trials to influence action choices and planning.",
            "task_name": "Iterative task solving / ALFWorld-like interactive tasks",
            "task_description": "Tasks requiring multiple trials and iterative improvement, where learning from previous trial outcomes can guide future attempts.",
            "task_type": "interactive task solving / iterative learning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Verbalizing and storing cross-trial reflections helps agents avoid past mistakes and improves subsequent trial efficiency and success rates (qualitative improvement reported in original works).",
            "limitations_or_challenges": "Reflection quality and retrieval relevance determine benefit; storing many reflections can create redundancy and contradictions; quantitative gains depend on good reflection generation.",
            "uuid": "e3049.5",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ExpeL",
            "name_full": "ExpeL: LLM Agents are Experiential Learners",
            "brief_description": "Agents that store completed trajectories (successful and failed) and recall similar successful trajectories as exemplars to guide behavior on new tasks.",
            "citation_title": "Expel: Llm agents are experiential learners",
            "mention_or_use": "mention",
            "agent_name": "ExpeL",
            "agent_description": "A framework that records full interaction trajectories, organizes them, and retrieves top-K successful trajectories similar to a new task to serve as examples for planning and action selection.",
            "memory_used": true,
            "memory_type": "trajectory memory / retrieved exemplars (cross-trial episodic memory)",
            "memory_mechanism_description": "Writes full trajectories into a vector store; during a new task the system retrieves the most similar successful trajectories by similarity scores and uses them as prompt examples to guide the agent.",
            "task_name": "Complex interactive task suites (e.g., ALFWorld / multi-step tasks)",
            "task_description": "Solve collections of interactive tasks by leveraging prior successful trajectories as exemplars for planning and execution.",
            "task_type": "interactive task solving / exemplar-guided planning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Recalling similar successful trajectories improves problem-solving on new but related tasks by providing concrete exemplar behavior; helpful for sample efficiency and generalization.",
            "limitations_or_challenges": "Retrieval quality, storage size of trajectories, and efficient indexing are bottlenecks; managing inconsistent or suboptimal exemplars is necessary.",
            "uuid": "e3049.6",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Retroformer",
            "name_full": "Retroformer (reflection fine-tuning for cross-trial extraction)",
            "brief_description": "A model that fine-tunes a reflection model to better extract and utilize cross-trial information from past trials to improve performance on subsequent tasks.",
            "citation_title": "",
            "mention_or_use": "mention",
            "agent_name": "Retroformer",
            "agent_description": "A fine-tuning approach where a model dedicated to reflection is trained so the agent can extract and apply cross-trial lessons more effectively across tasks.",
            "memory_used": true,
            "memory_type": "parametric memory (fine-tuned reflection model) + cross-trial textual memory",
            "memory_mechanism_description": "Uses fine-tuning of a reflection/extraction model to transform past trial transcripts into distilled insights that are later retrieved and used to condition agent planning and decisions.",
            "task_name": "Cross-trial learning / iterative improvement tasks",
            "task_description": "Tasks where leveraging distilled lessons from many prior trials can speed up or improve future trial performance.",
            "task_type": "iterative learning / multi-trial tasks",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Fine-tuning a dedicated reflection model can improve the quality of extracted cross-trial knowledge, enabling better reuse of past experience.",
            "limitations_or_challenges": "Fine-tuning cost and preservation of unrelated knowledge (catastrophic effects) are concerns; applicability to online settings is limited by training expense.",
            "uuid": "e3049.7",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An Open-ended Embodied Agent with Large Language Models",
            "brief_description": "An open-world game agent for Minecraft that stores learned executable skills and code snippets as memory, using them to bootstrap future skill acquisition and exploration.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "A Minecraft agent that writes discovered skills and executable code snippets into memory, refines them via environment feedback, and reuses them to solve new tasks and explore the environment further.",
            "memory_used": true,
            "memory_type": "textual skill memory (executable code snippets and distilled skills)",
            "memory_mechanism_description": "Writes code/skill artifacts discovered during play into a memory library; management refines skills based on feedback; reading retrieves relevant skills to seed new plans or compose multi-step behaviors.",
            "task_name": "Open-world Minecraft exploration and multi-step skill acquisition",
            "task_description": "Discover and reuse low-level and higher-level skills to accomplish increasingly complex objectives in a sparse-reward open-world environment.",
            "task_type": "open-world game playing / skill acquisition",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Storing and reusing executable skills accelerates exploration and skill composition in open-ended environments, enabling bootstrapped learning.",
            "limitations_or_challenges": "Skill retrieval and applicability to new contexts, and scalability of stored code artifacts; aligning stored skills with correct contexts is nontrivial.",
            "uuid": "e3049.8",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Synapse",
            "name_full": "Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control",
            "brief_description": "A memory-backed prompting method that records trajectories of successful computer-control actions and uses them as exemplars to improve performance on new control tasks.",
            "citation_title": "Synapse: Trajectory-as-exemplar prompting with memory for computer control",
            "mention_or_use": "mention",
            "agent_name": "Synapse",
            "agent_description": "An agent targeting computer control tasks that stores successful trajectories and retrieves them as exemplar prompts which guide current action selection and improve task success.",
            "memory_used": true,
            "memory_type": "trajectory exemplar memory (retrieval of successful past trajectories)",
            "memory_mechanism_description": "Writes full interaction trajectories into a memory store; during new tasks the agent retrieves exemplar trajectories with high similarity and conditions planning on those examples.",
            "task_name": "Computer control tasks (GUI/automation)",
            "task_description": "Select UI elements and sequences of actions to accomplish GUI-based tasks; requires leveraging past successful action sequences as exemplars.",
            "task_type": "computer control / automation",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Using trajectories as exemplars aids selection and sequencing decisions in computer-control tasks by providing concrete prior examples.",
            "limitations_or_challenges": "Retrieval precision and exemplar relevance; storing many trajectories increases indexing cost; exemplar mismatches can mislead planning.",
            "uuid": "e3049.9",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "brief_description": "A framework combining chain-of-thought style reasoning with tool use and retrieval of external knowledge (e.g., Wikipedia) to perform multi-step question answering and reasoning tasks.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "mention",
            "agent_name": "ReAct",
            "agent_description": "An agent that interleaves reasoning traces with external actions (tool/API calls) and retrieval to obtain missing facts; memory in ReAct is often provided by external knowledge sources accessed at reasoning time.",
            "memory_used": true,
            "memory_type": "external-knowledge retrieval (tool/API calls) and chain-of-thought traces",
            "memory_mechanism_description": "During multi-step reasoning, the agent issues tool/API calls (e.g., Wikipedia lookup) to fetch external facts which are treated as memory inputs for subsequent reasoning steps; logs of prior reasoning steps can also serve as working memory.",
            "task_name": "Multi-step question answering / tool-augmented reasoning",
            "task_description": "Answer complex questions requiring retrieval of up-to-date facts and multi-step inference with tool usage.",
            "task_type": "question answering / tool use",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Integrating external API/tool retrieval into the reasoning loop reduces hallucination and improves factual grounding for multi-step QA tasks.",
            "limitations_or_challenges": "Tool reliability and API latency; integrating and aligning external facts with internal reasoning is nontrivial and can introduce noise if retrieval is poor.",
            "uuid": "e3049.10",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "GITM",
            "name_full": "GITM: Ghost in the Minecraft (Generally Capable Agents for Open-world Environments)",
            "brief_description": "An open-world agent that leverages textual memory plus external Wiki knowledge to navigate sparse-reward environments (Minecraft), storing and retrieving successful trajectories and recipes.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "mention_or_use": "mention",
            "agent_name": "GITM",
            "agent_description": "A Minecraft agent that stores successful trajectories as memory and accesses external Minecraft Wiki via API to supplement internal memory for crafting and navigation tasks.",
            "memory_used": true,
            "memory_type": "textual trajectory memory + external knowledge API retrieval",
            "memory_mechanism_description": "Writes gameplay trajectories and skill artifacts to memory; retrieves similar successful trajectories and queries external Wiki APIs for craft recipes and domain knowledge to inform current planning.",
            "task_name": "Minecraft item production / open-world tasks",
            "task_description": "Produce items and navigate the open-world environment where external craft knowledge and prior successful sequences are necessary to accomplish goals.",
            "task_type": "open-world game playing / craft planning",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Combining internal trajectory memory with external Wiki retrieval supports solving sparse-reward tasks and complex crafting trees by providing both exemplars and factual knowledge.",
            "limitations_or_challenges": "Integration of heterogeneous memory sources (trajectories vs external Wiki); retrieval latency and contradictory information across sources.",
            "uuid": "e3049.11",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "TiM",
            "name_full": "TiM: Think-in-Memory (Recalling and Post-thinking Enable LLMs with Long-term Memory)",
            "brief_description": "A method that self-generates multiple thoughts post-task and uses them as memory entries to support future reasoning and generalization.",
            "citation_title": "Think-in-memory: Recalling and post-thinking enable llms with long-term memory",
            "mention_or_use": "mention",
            "agent_name": "TiM",
            "agent_description": "An agent that, after completing tasks, generates multiple internal thoughts (post-thinking) and stores them as memory to provide abstracted, generalizable knowledge for future tasks.",
            "memory_used": true,
            "memory_type": "textual self-generated thought memory + reflection",
            "memory_mechanism_description": "Writes multiple self-generated thoughts/insights after task completion into memory; future reads retrieve these thoughts as higher-level guidance or examples to improve reasoning and generalization.",
            "task_name": "Reasoning enhancement / cross-task generalization",
            "task_description": "Improve reasoning and generalization across tasks by storing and reusing post-hoc generated internal thoughts.",
            "task_type": "reasoning / generalization",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_comparison": null,
            "key_findings": "Post-task self-thought generation and storage help produce more generalizable memory entries that benefit future analogous tasks.",
            "limitations_or_challenges": "Quality and relevance of self-generated thoughts determine utility; risk of storing spurious or misleading self-reflections.",
            "uuid": "e3049.12",
            "source_info": {
                "paper_title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reflexion: Language agents with verbal reinforcement learning",
            "rating": 2,
            "sanitized_title": "reflexion_language_agents_with_verbal_reinforcement_learning"
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2,
            "sanitized_title": "memorybank_enhancing_large_language_models_with_longterm_memory"
        },
        {
            "paper_title": "Ret-llm: Towards a general read-write memory for large language models",
            "rating": 2,
            "sanitized_title": "retllm_towards_a_general_readwrite_memory_for_large_language_models"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 2,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2,
            "sanitized_title": "voyager_an_openended_embodied_agent_with_large_language_models"
        },
        {
            "paper_title": "Expel: Llm agents are experiential learners",
            "rating": 2,
            "sanitized_title": "expel_llm_agents_are_experiential_learners"
        },
        {
            "paper_title": "Synapse: Trajectory-as-exemplar prompting with memory for computer control",
            "rating": 2,
            "sanitized_title": "synapse_trajectoryasexemplar_prompting_with_memory_for_computer_control"
        },
        {
            "paper_title": "Memochat: Tuning llms to use memos for consistent long-range open-domain conversation",
            "rating": 2,
            "sanitized_title": "memochat_tuning_llms_to_use_memos_for_consistent_longrange_opendomain_conversation"
        },
        {
            "paper_title": "Memgpt: Towards llms as operating systems",
            "rating": 2,
            "sanitized_title": "memgpt_towards_llms_as_operating_systems"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory",
            "rating": 2,
            "sanitized_title": "ghost_in_the_minecraft_generally_capable_agents_for_openworld_enviroments_via_large_language_models_with_textbased_knowledge_and_memory"
        },
        {
            "paper_title": "Think-in-memory: Recalling and post-thinking enable llms with long-term memory",
            "rating": 2,
            "sanitized_title": "thinkinmemory_recalling_and_postthinking_enable_llms_with_longterm_memory"
        }
    ],
    "cost": 0.022097,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Survey on the Memory Mechanism of Large Language Model based Agents
21 Apr 2024</p>
<p>Zeyu Zhang zeyuzhang@ruc.edu.cn 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Xiaohe Bo 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Chen Ma 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Rui Li 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Xu Chen xu.chen@ruc.edu.cn 
Quanyu Dai 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>Huawei Noah's Ark Lab
China</p>
<p>Jieming Zhu 
Huawei Noah's Ark Lab
China</p>
<p>Zhenhua Dong 
Huawei Noah's Ark Lab
China</p>
<p>Ji-Rong Wen 
Gaoling School of Artificial Intelligence
Renmin University of China
BeijingChina</p>
<p>A Survey on the Memory Mechanism of Large Language Model based Agents
21 Apr 2024EF34277E330E74E19C4067A09BF547F6arXiv:2404.13501v1[cs.AI]
Large language model (LLM) based agents have recently attracted much attention from the research and industry communities.Compared with original LLMs, LLMbased agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions.The key component to support agent-environment interactions is the memory of the agents.While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies.To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents.In specific, we first discuss "what is" and "why do we need" the memory in LLM-based agents.Then, we systematically review previous studies on how to design and evaluate the memory module.In addition, we also present many agent applications, where the memory module plays an important role.At last, we analyze the limitations of existing work and show important future directions.To keep up with the latest advances in this field, we create a repository at https://github.com/nuster1128/LLM_Agent_Memory_Survey.LLM Memory Agent Environment Figure 1: The importance of the memory module in LLM-based agents.Preprint.Under review.</p>
<p>Personal Assistant Please help me to explain "LLM-based agent". A LLM-based agent is a type of artificial 
In
I' m Batman, the lights of city.</p>
<p>[Iron Man] My name is Iron Man, also known as Tony Stark. I am the founder of Stark Industries and a member of the Avengers.</p>
<p>As one of the genius inventors and billionaire, I have created the most advanced armor in the world, which not only protects me but also gives me incredible strength and the ability to fly.</p>
<p>Open-world Game
Skills</p>
<p>to n-i-1 # Swap if the element found is greater than the next element if arr[j] &gt; arr[j+1]: arr[j], arr[j+1] = arr[j+1], arr[j] return arr</p>
<p>Sort the numbers in ascending order.</p>
<p>Bubble sort repeatedly steps through the list, compares adjacent elements and swaps them if they are in wrong order.The pass through the list is repeated until the list is sorted.</p>
<p>Bubble sort can reorder a list of numbers.</p>
<p>Development Group</p>
<p>Recommendation I want to buy a dress for the graduation party. (Context) She just bought a new blue dress. So she may need a white accessories to match it. (Personal Preference) She often buys blue clothes. She values the cost-effectiveness of items, especially on clothes. She likes small things with light colors, such as ear pendants.</p>
<p>You may like this blue dress. It is of good quality and great price.</p>
<p>Would you like to buy a waistband for your dress?</p>
<p>Great!I like this bule one.I will buy it for the party.</p>
<p>Medicine</p>
<p>David, a 38-year-old male with a history of allergies and sinus infections, has a family history of diabetes and hypertension.As a smoker of about one pack a day and an occasional drinker, his lifestyle choices may contribute to his health risks.After traveling to a tropical country where mosquito-borne illnesses are prevalent, he has experienced symptoms such as mild fatigue, headache, and muscle aches for the past week.1 Introduction "Without memory, there is no culture.Without memory, there would be no civilization, no society, no future."</p>
<p>Elie Wiesel, 1928Wiesel, -2016 Recently, large language models (LLMs) have achieved remarkable success in a large number of domains, ranging from artificial intelligence and software engineering to education and social science [1][2][3].Original LLMs usually accomplish different tasks without interacting with environments.However, to achieve the final goal of artificial general intelligence (AGI), intelligent machines should be able to improve themselves by autonomously exploring and learning from the real world.For example, if a trip-planning agent intends to book a ticket, it should send an order request to the ticket website, and observe the response before taking the next action.A personal assistant agent should adjust its behaviors according to the user's feedback, providing personalized responses to improve user's satisfaction.To further push the boundary of LLMs towards AGI, recent years have witnessed a large number of studies on LLM-based agents [3,4], where the key is to equip LLMs with additional modules to enhance their self-evolving capability in real-world environments.</p>
<p>Among all the added modules, memory is a key component that differentiates the agents from original LLMs, making an agent truly an agent (see Figure 1).It plays an extremely important role in determining how the agent accumulates knowledge, processes historical experience, retrieves informative knowledge to support its actions, and so on.Around the memory module, people have devoted much effort to designing its information sources, storage forms, and operation mechanisms.For example, Shinn et al. [5] incorporate both in-trial and cross-trial information to build the memory module for enhancing the agent's reasoning capability.Zhong et al. [6] store memory information in the form of natural languages, which is explainable and friendly to the users.Modarressi et al. [7] design both memory reading and writing operations to interact with environments for task solving.</p>
<p>While previous studies have designed many promising memory modules, there still lacks a systemic study to view the memory modules from a holistic perspective.To bridge this gap, in this paper, we comprehensively review previous studies to present clear taxonomies and key principles for designing and evaluating the memory module.In specific, we discuss three key problems including: (1) what is the memory of LLM-based agents?(2) why do we need the memory in LLM-based agents?and (3) how to implement and evaluate the memory in LLM-based agents?To begin with, we detail the concepts of memory in LLM-based agents, providing both narrow and broad definitions.Then, we analyze the necessity of memory in LLM-based agents, showing its importance from three perspectives including cognitive psychology, self-evolution, and agent applications.Based on the problems of "what" and "why", we present commonly used strategies to design and evaluate the memory modules.For the memory design, we discuss previous works from three dimensions, that is, memory sources, memory forms, and memory operations.For the memory evaluation, we introduce two widely used approaches including direct evaluation and indirect evaluation via specific agent tasks.Next, we discuss agent applications including role-playing, social simulation, personal assistant, open-world games, code generation, recommendation, and expert systems, in order to show the importance of the memory module in practical scenarios.At last, we analyze the limitations of existing work and highlight significant future directions.</p>
<p>The main contributions of this paper can be summarized as follows: (1) We formally define the memory module and comprehensively analyze its necessity for LLM-based agents.(2) We systematically summarize existing studies on designing and evaluating the memory module in LLM-based agents, providing clear taxonomies and intuitive insights.(3) We present typical agent applications to show the importance of the memory module in different scenarios.(4) We analyze the key limitations of existing memory modules and show potential solutions for inspiring future studies.To our knowledge, this is the first survey on the memory mechanism of LLM-based agents.</p>
<p>The rest of this survey is organized as follows.First, we provide a systematical meta-survey for the fields of LLMs and LLM-based agents in Section 2, categorizing different surveys and summarizing their key contributions.Then, we discuss the problems of "what is", "why do we need" and "how to implement and evaluate" the memory module in LLM-based agents in Section 3 to 6. Next, we show the applications of memory-enhanced agents in Section 7. The discussions of the limitations of existing work and future directions come at last in Section 8 and Section 9.</p>
<p>Related Surveys</p>
<p>In the past two years, LLMs have attracted much attention from the academic and industry communities.To systemically summarize the studies in this field, researchers have written a lot of survey papers.In this section, we briefly review these surveys (see Figure 2 for an overview), highlighting their major focuses and contributions to better position our study.</p>
<p>Surveys on Large Language Models</p>
<p>In the field of LLMs, Zhao et al. [70] present the first comprehensive survey to summarize the background, evolution paths, model architectures, training methodologies, and evaluation strategies of LLMs.Hadi et al. [71] and Min et al. [72] also conduct LLM surveys from the holistic view, which, however, provide different taxonomies and understandings on LLMs.Following these surveys, people dive into specific aspects of LLMs and review the corresponding milestone studies and key technologies.These aspects can be classified into four categories including the fundamental problems, evaluation, applications, and challenges of LLMs.</p>
<p>Fundamental problems.The surveys in this category aim to summarize techniques that can be leveraged to tackle fundamental problems of LLMs.Specifically, Zhang et al. [8] provide a comprehensive survey on the methods of supervised fine-tuning, which is a key technique for better training LLMs.Shen et al. [9], Wang et al. [10] and Liu et al. [11] present surveys on the alignment of LLMs, which is a key requirement for LLMs to produce outputs consistent with human values.Gao et al. [12] propose a survey on the retrieval-augmented generation (RAG) capability of LLMs, which is key to providing LLMs with factual and up-to-date knowledge and removing hallucinations.Qin et al. [18] summarize the state-of-the-art methods on enabling LLMs to leverage external tools, which is fundamental for LLMs to expand their capability in domains that require specialized knowledge.Wang et al. [13], Yao et al. [14], Wang et al. [15], Feng et al. [16] and Zhang et al. [17] present surveys on the direction of LLM knowledge editing, which is important for customizing LLMs to satisfy specific requirements.Huang et al. [19], Wang et al. [20] and Pawar et al. [21] focus on long-context capabilities of LLMs, which is critical for LLMs to process more information at each time and enhance their application scenarios.Wu et al. [22], Song et al. [23], Caffagni et al. [24] and Yin et al. [25] summarize multi-modal LLMs, which expands the capability of LLMs from text to visual and other modalities.The above surveys mainly focus on the effectiveness of LLMs.Another important aspect of LLMs is their training and inference efficiency.To summarize studies on this aspect, Zhu et al. [30], Xu and McAuley [31], Wang et al. [32] and Park et al. [33] systematically review the techniques of model compression.Ding et al. [81] and Xu et al. [29] analyze and conclude the studies on parameter efficient fine-tuning.Bai et al. [26], Wan et al. [27], Miao et al. [28] and Ding et al. [81] put more focuses on the efficiency of resource utilization in a general sense.</p>
<p>Evaluation.The surveys in this category focus on how to evaluate the capability of LLMs.Specifically, Chang et al. [34] comprehensively summarize the evaluation methods from an overall perspective.It encompasses different evaluation tasks, methods, and benchmarks, which serve as critical parts in assessing LLM performances.Guo et al. [35] care more about the evaluation targets and describe how to evaluate the knowledge, alignment, and safety control capabilities of LLMs, which supplement evaluation metrics beyond performance.</p>
<p>Applications.The surveys in this category aim to summarize models that leverage LLMs to improve different applications.More concretely, Zhu et al. [37] focus on the field of information retrieval (IR) and summarize studies on LLM-based query processes.Xu et al. [38] pay more attention to information extraction (IE) and provide comprehensive taxonomies for LLM-based models in this field.Li et al. [50], Lin et al. [51] and Wang et al. [52] discuss the applications of LLMs in the field of recommender system, where they utilize agents to generate data and provide recommendations.Fan et al. [39], Wang et al. [40], and Zheng et al. [41] concentrate on how LLMs can benefit software engineering (SE) in terms of software design, development, and testing.Zeng et al. [42] summarize LLM-based methods in the field of robotics.Cui et al. [43] and Yang et al. [44] focus on the application of autonomous driving and summarize models in this domain based on LLMs from different perspectives.Beyond the above domains in artificial intelligence, LLMs have also been used in natural and social science.He et al. [45], Zhou et al. [46] and Wang et al. [47] summarize the applications of LLMs in medicine.Li et al. [48] focus on the applications of LLMs in finance.He et al. [49] review the models on leveraging LLMs to improve the development of psychology.[24], Yin et al. [25].</p>
<p>Long-context</p>
<p>Huang et al. [19], Wang et al. [20], Pawar et al. [21].</p>
<p>Tool Usage</p>
<p>Qin et al. [18].</p>
<p>Knowledge Editing</p>
<p>Wang et al. [13], Yao et al. [14], Wang et al. [15], Feng et al. [16], Zhang et al. [17].</p>
<p>Retrieval Augmentation Gao et al. [12].</p>
<p>Alignment Shen et al. [9], Wang et al. [10], Liu et al. [11].</p>
<p>Supervised Fine-tuning</p>
<p>Zhang et al. [8].</p>
<p>LLM-based Agent [3,4,[77][78][79][80] Challenges Applications Li et al. [76].</p>
<p>Evaluation Fundamental Problems</p>
<p>Multi-agents Guo et al. [75].</p>
<p>Memory</p>
<p>Our Survey.</p>
<p>Planning</p>
<p>Huang et al. [74].Challenges.The surveys in this category focus on trustworthiness in LLMs, such as hallucination, bias, unfairness, explainability, security, and privacy.Hallucination in LLMs refers to the problem that LLMs may generate misconceptions or fabrications, impacting their reliability for downstream applications.Zhang et al. [53], Huang et al. [54], Rawte et al. [55], Ye et al. [56], Ji et al. [57], Tonmoy et al. [58] and Jiang et al. [59] summarize the mainstream models for alleviating the hallucination problem in LLMs.The bias and unfairness problems refer to the phenomenon that LLMs may unequally treat different humans or objectives, which can lead to the propagation of societal stereotypes and discrimination.Gallegos et al. [60], Kotek et al. [61] and Li et al. [62] comprehensively discuss these challenges and summarize existing methods for alleviating them.The problem of explainability means that the internal working mechanisms of LLMs are still unclear.Zhao et al. [63] systematically discuss this problem and summarize previous efforts on improving the explainability of LLMs.Security and privacy are also challenging problems, which have been comprehensively surveyed in Yao et al. [64], Shayegani et al. [65], Neel and Chang [66], Smith et al.</p>
<p>[67], Dong et al. [68] and Das et al. [69].</p>
<p>Surveys on Large Language Model-based Agents</p>
<p>Based on the capability of LLMs, people have conducted a lot of studies on building LLM-based agents, which can autonomously perceive environments, take actions, accumulate knowledge, and evolve themselves.In this field, Wang et al. [3] present the first survey paper to systematically summarize LLM-based agents from the perspectives of agent construction, agent application, and agent evaluation.Xi et al. [4], Zhao et al. [77], Cheng et al. [78] and Ge et al. [80] also summarize LLM-based agent studies from the overall perspective, but they have different focuses and taxonomies, delivering more diverse understandings on this field.In addition to these overall surveys, there have also emerged several papers reviewing specific aspects of LLM-based agents.For the fundamental problems, Durante et al. [79] summarize studies on multi-modal agents.Huang et al. [74] focus on the planning capability of LLM-based agents.Guo et al. [75] pay more attention to the scenarios of multi-agent interactions.For the applications, Li et al. [76] provide a summarization on LLM-based agents that are leveraged as personal assistants.</p>
<p>Position of this work.Our survey summarizes the studies on a fundamental problem of LLM-based agents, that is, the agent's memory mechanism.To our knowledge, this is the first survey in this direction.We hope it can not only inspire more advanced memory architectures in the future, but also provide newcomers with comprehensive starting materials.</p>
<p>What is the Memory of LLM-based Agent</p>
<p>Interacting and learning from environments is a basic requirement of LLM-based agents.In the agent-environment interaction process, there are three key phases, that is, (1) the agent perceives information from the environment, and stores it into the memory; (2) the agent processes the stored information to make it more usable; and (3) the agent takes the next action based on the processed memory information.In all these phases, memory plays an extremely important role.In the following, we first define the memory of the agent from both narrow and broad perspectives, and then, detail the execution processes of the above three phases based on the memory module.</p>
<p>Basic Knowledge</p>
<p>For clear presentations, we first introduce several important background knowledge as follows: Definition 1 (Task).Task is the final target that the agent needs to achieve, for example, booking a flight ticket for Alice, recommending a restaurant for Bob, and so on.Formally, we use T to represent a task and label different tasks by subscripts in the following contents.Definition 2 (Environment).In a narrow sense, environment is the object that the agent needs to interact with to accomplish the task.For the examples in definition 1, the environments are Alice and Bob, who provide feedback on the agent's actions.More broadly, environment can be any contextual factors that influence the agent's decisions, such as the weather when booking flight tickets, the time and location when recommending restaurants, etc. Definition 3 (Trial and Step).To accomplish a task, the agent needs to interact with the environment.Usually, the agent first takes an action, and then the environment responds to this action.At last, the agent takes the next action based on the response.This process iterates until the task is finished.The complete agent-environment interaction process is called a trial, and each interaction turn is called a Illustration of the memory reading, writing, and management processes, where dotted lines mean that the cross-trial information can be incorporated into the memory module.</p>
<p>step.For each trial, the agent can take multiple steps to form a potential solution to the task.For each task, the agent can explore multiple trials to accomplish the task [5].Formally, at step t, we use a t and o t to represent the agent action and the observed environment response, respectively.Then, a T -length trial can be represented as  T = {a 1 , o 1 , a 2 , o 2 , ..., o T , a T }.</p>
<p>In the above definitions, task and environment are the most coarse-grained concepts, while step is the most fine-grained one.They together describe the complete agent-environment interaction process.</p>
<p>A Toy Example.Based on the above definitions, we show a toy example and use it throughout the paper to provide more intuitions.In this example, the agent has two tasks: (A) Making a trip plan for Alice in Beijing from 5/1/2024 to 5/3/2024, where the agent has to (i) buy a round-trip flight ticket, (ii) determine several attractions according to Alice's preferences, and (iii) arrange the visiting order of these attractions.(B) Recommending a movie for Alice on 5/10/2024, where the agent has to (i) determine the time that Alice is free to watch movies, and (ii) select a movie according to Alice's watching preference.For each task, we present a potential trial in the following (also illustrated in Figure 3(a)).For task (A), the trial is:</p>
<p>[</p>
<p>Broad Definition of the Agent Memory</p>
<p>In a broad sense, the memory of the agent can come from much wider sources, for example, the information across different trials and the external knowledge beyond the agent-environment interactions.Formally, given a series of sequential tasks {T 1 , T 2 , ..., T K }, for task T k , the memory information at step t comes from three sources: (1) the historical information within the same trial, that is,
 k t = {a k 1 , o k 1 , ..., a k t1 , o k t1 }
, where we add superscript k to label the task index.</p>
<p>(2) The historical information across different trials, that is,  k = {1 ,  2 , ...,  k1 ,  k  }, where  j (j  {1, ..., k  1}) represents the trials of task j 1 , and  k  denotes the previously explored trials for task T k .(3) External knowledge, which is represented by D k t .The memory of the agent is derived based on ( k t ,  k , D k t ).In the above toy example, for task (A), if there are several failed trials, that is, the feedback from Alice is negative, then these trials can be incorporated into the agent's memory to avoid future similar errors (corresponding to  k </p>
<p>).In addition, for task (B), the agent may recommend movies relevant to the attractions that Alice has visited in task (A) to capture her recent preferences (corresponding to { 1 ,  2 , ...,  k1 }).In the agent decision process, it has also referred to the magazine Attractions in Beijing for making trip plans, which is the external knowledge (corresponding to D k t ) for the current task T k .</p>
<p>Memory-assisted Agent-Environment Interaction</p>
<p>As mentioned at the beginning of Section 3, there are three key phases in the agent-environment interaction process.The agent memory module implements these phases through three operations including memory writing, memory management, and memory reading.</p>
<p>Memory Writing.This operation aims to project the raw observations into the actually stored memory contents, which are more informative [7] and concise [6].It corresponds to the first phase of the agent-environment interaction process.Given a task T k , if the agent takes an action a k t at step t, and the environment provides an observation o k t , then the memory writing operation can be formally represented as:
m k t = W ({a k t , o k t })
, where W is a projecting function.m k t is the finally stored memory contents, which can be either natural languages or parametric representations.In the above toy example, for task (A), the agent is supposed to remember the flight arrangement and the decision of attractions after [step 2].For task (B), the agent should memorize the fact that Alice hopes to watch movies at 9:00 PM, after [step 1].</p>
<p>Memory Management.This operation aims to process the stored memory information to make it more effective, for example, summarizing high-level concepts to make the agent more general-izable [6], merging similar information to reduce redundancy [7], and forgetting unimportant or irrelevant information to remove its negative influence.This operation corresponds to the second phase of the agent-environment interaction process.Let M k t1 be the memory contents for task k before step t, and suppose m k t is the stored information at step t based on the above memory writing operation, then, the memory management operation can be represented by:
M k t = P (M k t1 , m k t )
, where P is a function that iteratively processes the stored memory information.For the narrow memory definition, the iteration only happens within the same trial, and the memory is emptied when the trial is ended.For the broad memory definition, the iteration happens across different trials or even tasks, as well as the integrations of external knowledge.For task (B) in the above toy example, the agent can conclude that Alice enjoys watching science fiction movies in the evening, which can be used as a default rule to make recommendations for Alice in the future.</p>
<p>Memory Reading.This operation aims to obtain important information from the memory to support the next agent action.It corresponds to the third phase of the agent-environment interaction process.Suppose M k t is the memory contents for task k at step t, c k t is the context of the next action, then the memory reading operation can be represented by:
M k t = R(M k t , c k t+1
), where R is usually implemented by computing the similarity between M k t and c k t+1 [82].M k t is used as parts of the final prompt to drive the agent's next action.For task (B) in the above toy example, when the agent decides on the final recommended movie in [Step 3], it should focus on the "want to watch" list in [Step 2] and select one from it.</p>
<p>Based on the above operations, we can derive a unified function for the evolving process from
{a k t , o k t } to a k t+1 , that is: a k t+1 = LLM{R(P (M k t1 , W ({a k t , o k t })), c k t+1 )},
where LLM is the large language model.The complete agent-environment interaction process can be easily obtained by iteratively expanding this function (see Figure 3(b) for an intuitive illustration).Remark.This function provides a general formulation of the agent memorizing process.Previous works may use different specifications.For example, in [5], R and P are set as identical functions, and P only takes effect at the end of a trial.In Park et al. [83], R is implemented based on three criteria including similarity, time interval, and importance, and P is realized by a reflection process to obtain more abstract thoughts.In this section, we focus on the overall framework of the agent's memory operations.More detailed realizations of W , P , and R are deferred in Section 5.</p>
<p>Why We Need the Memory in LLM-based Agent</p>
<p>Above, we have introduced what is the memory of LLM-based agents.Before comprehensively presenting how to implement it, in this section, we briefly show why memory is necessary for building LLM-based agents, where we expand our discussion from three perspectives including cognitive psychology, self-evolution, and agent applications.</p>
<p>Perspective of Cognitive Psychology</p>
<p>Cognitive psychology is the scientific study of human mental processes such as attention, language use, memory, perception, problem-solving, creativity, and reasoning2 .Among these processes, memory is widely recognized as an extremely important one [84].It is fundamental for humans to learn knowledge by accumulating important information and abstracting high-level concepts [85], form social norms by remembering cultural values and individual experiences [86], take reasonable behaviors by imagining the potential positive and negative consequences [87], and among others.</p>
<p>A major goal of LLM-based agents is to replace humans for accomplishing different tasks.To make agents behave like humans, following human's working mechanisms to design the agents is a natural and essential choice [88].Since memory is important for humans, designing memory modules is also significant for the agents.In addition, cognitive psychology has been studied for a long time, so many effective human memory theories and architectures have been accumulated, which can support more advanced capabilities of the agents [89].</p>
<p>Perspective of Self-Evolution</p>
<p>To accomplish different practical tasks, agents have to self-evolve in dynamic environments [90].</p>
<p>In the agent-environment interaction process, the memory is key to the following aspects: (1) Experience accumulation.An important function of the memory is to remember past error plannings, inappropriate behaviors, or failed experiences, so as to make the agent more effective for handling similar tasks in the future [91].This is extremely important for enhancing the learning efficiency of the agent in the self-evolving process.( 2) Environment exploration.To autonomously evolve in the environment, the agents have to explore different actions and learn from the feedback [92].By remembering historical information, the memory can help to better decide when and how to make explorations, for example, focusing more on previously failed trials or actions with lower exploring frequencies [93].(3) Knowledge abstraction.Another important function of the memory is to summarize and abstract high-level information from raw observations, which is the basis for the agent to be more adaptive and generalizable to unseen environments [82].In summary, self-evolution is the basic characteristic of LLM-based agents, and memory is of key importance to self-evolution.</p>
<p>Perspective of Agent Applications</p>
<p>In many applications, memory is an indispensable component of the agent.For example, in a conversational agent, the memory stores information about historical conversations, which is necessary for the agent to generate the next response.Without memory, the agent does not know the context, and cannot continue the conversation [94].In a simulation agent, memory is of great importance to make the agent consistently follow the role profiles.Without memory, the agent may easily step out of the role during the simulation process [95].Both of the above examples show that the memory is not an optional component, but is necessary for the agents to accomplish given tasks.</p>
<p>In the above three perspectives, the first one reveals that the memory builds the cognitive basis of the agent.The second and third ones show that the memory is necessary for the agent's evolving principles and applications, which provide insights for designing agents with memory mechanisms.</p>
<p>How to Implement the Memory of LLM-based Agent</p>
<p>In this section, we discuss the implementation of the memory module from three perspectives: memory sources, memory forms, and memory operations.Memory sources refer to where the memory contents come from.Memory forms focus on how to represent the memory contents.Memory operations aim to process the memory contents.These three perspectives provide a comprehensive review of memory implementation methods, which is helpful for future research.For better demonstration, we present an overview of implementation methods in Figure 4.</p>
<p>Memory Sources</p>
<p>In previous works, the memory contents may come from different sources.Based on our formulation in Section 3, these sources can be classified into three categories, that is, the information inside a trial, the information across different trials, and the external knowledge.The former two are dynamically Table 1: Summarization of the memory sources.We use  and  to label whether or not the corresponding source is adopted in the model.</p>
<p>Models</p>
<p>Inside-trial Information Cross-trial Information External Knowledge
MemoryBank [6]    RET-LLM [7]    ChatDB [96]    TiM [97]    SCM [98]    Voyager [99]    MemGPT [100]    MemoChat [94]    MPC [101]    Generative Agents [83]    RecMind [102]    Retroformer [103]    ExpeL [82]    Synapse [91]    GITM [93]    ReAct [104]    Reflexion [5]    RecAgent [95]    Character-LLM [105]    MAC [106]    Huatuo [107]    ChatDev [1]    InteRecAgent [108]    MetaAgents [109]    TPTU [110, 111]    MetaGPT [112]    S 3 [2]    InvestLM [113]   
generated in the agent-environment interaction process (e.g., task internal information), while the latter is static information outside the loop (e.g., task external information).We summarize previous works on memory sources in Table 1.</p>
<p>Inside-trial Information</p>
<p>In the agent-environment interaction process, the historical steps within a trial are usually the most relevant and informative signals to support the agent's future actions.Almost all the previous works use this information as a part of the memory sources.</p>
<p>Representative Studies.Generative Agents [83] aims to simulate human's daily behaviors by using LLM-based agents.The memory of an agent is derived from the historical behaviors to achieve a target, for example, the collection of relevant papers when researching on a specific topic.MemoChat [94] aims to chat with humans, where the memory of the agent is derived based on the conversation history of a dialogue session.TiM [97] aims to enhance the agent's reasoning capability by self-generating multiple thoughts after accomplishing a task, which is used as the memory to provide more generalizable information.Voyager [99] focuses on building game agents based on Minecraft, where the memory contains executable codes of preliminary and basic actions to accomplish a task.It should be noted that the inside-trial information not only includes agentenvironment interactions, but also contains interaction contexts, such as time and location information.</p>
<p>Discussion.The inside-trial information is the most obvious and intuitive source that should be leveraged to construct the agent's memory since it is highly relevant to the current task that the agent has to accomplish.However, relying solely on inside-trial information may prevent the agent from accumulating valuable knowledge from various tasks and learning more generalizable information.Thus, many studies also explore how to effectively utilize the information across different tasks to build the memory module, which is detailed in the following sections.</p>
<p>Cross-trial Information</p>
<p>For LLM-based agents, the information accumulated across multiple trials in the environment is also a crucial part of the memory, typically including successful and failed actions and their insights, such as failure reasons, common action patterns to succeed, and so on.</p>
<p>Representative Studies.One of the most prominent studies is Reflexion [5], which proposes verbal reinforcement learning for LLM-based agents.It derives the experiences from past trials in verbal form, and applies them in subsequent trials to improve the performance of the same task.Furthermore, Retroformer [103] fine-tunes the reflection model, enabling the agent to extract cross-trial information from past trials more effectively.In Synapse [91], the agents focus on solving the computer control tasks.Their memory can record cross-trial information through successful exemplars, which would be used as references on similar trials.In ExpeL [82], the agents are required to solve a collection of complex interactive tasks within the environment.They store and organize completed trajectories, and recall similar ones for the new task.In the recalled trajectories, successful cases will be compared with failed ones to identify the patterns to succeed.</p>
<p>Discussion.According to the accumulated memory of cross-trial information, the agents are able to accumulate experiences, which is important for their evolution.Based on the past experiences, the agents can adjust their actions based on the overall feedback of the whole process.In contrast to the inside-trial observations, which serve as short-term memory, the trial experiences can be considered as long-term memory.It utilizes feedback from different trials to support a wider range of agent trials, providing more prolonged experiential support for agents.However, the limitation lies in the fact that both inside-trial and cross-trial information require the agents to personally engage in agent-environment interactions, where external experiences and knowledge are not included.</p>
<p>External Knowledge</p>
<p>An important characteristic of LLM-based agents is that they can be directly communicated and controlled in natural languages.As such, LLM-based agents can easily incorporate external knowledge in textual forms (e.g., Wikipedia3 ) to facilitate their decisions.</p>
<p>Representative Studies.In ReAct [104], the agents are required to answer questions about general knowledge by multiple reasoning steps.They can utilize Wikipedia APIs to obtain external knowledge if they lack information during these steps.GITM [93] intends to design agents in Minecraft, which can explore in complex and sparse-reward environments.The agents draw from the online Minecraft Wiki and craft recipes to provide an infinite source of knowledge for their navigation.</p>
<p>CodeAgent [114] focuses on the repo-level code generation task, which commonly requires complex dependencies and extensive documentation.It designs a web search strategy for acquiring related external knowledge.ChatDoctor [115] adapts LLM-based agents to the medical domain.It fine-tunes an acquisition process to retrieve external knowledge from Wikipedia and medical databases.</p>
<p>Discussion.The external knowledge can be obtained from both private and public sources.It provides LLM-based agents with much knowledge beyond their internal environment, which might be difficult or even impossible for the agent to acquire by agent-environment interactions.Moreover, most external knowledge can be acquired by accessing the APIs of various tools dynamically in real time according to the task needs, thus mitigating the problem of outdated knowledge.Integrating external knowledge into the memory of LLM-based agents significantly expands their knowledge boundaries, providing them with unlimited, up-to-date, and well-founded knowledge for decision-making.</p>
<p>Memory Forms</p>
<p>In general, there are two forms to represent the memory contents: textual form and parametric form.</p>
<p>In textual form, the information is explicitly retained and recalled by natural languages.In parametric Table 2: Summarization of the memory forms.We use  and  to label whether or not the corresponding memory form is adopted in the model.</p>
<p>Models Textual Form Parametric Form</p>
<p>Complete Recent Retrieved External Fine-tuning Editing
MemoryBank [6]       RET-LLM [7]       ChatDB [96]       TiM [97]       SCM [98]       Voyager [99]       MemGPT [100]       MemoChat [94]       MPC [101]       Generative Agents [83]       RecMind [102]       Retroformer [103]       ExpeL [82]       Synapse [91]       GITM [93]       ReAct [104]       Reflexion [5]       RecAgent [95]       Character-LLM [105]       MAC [106]       Huatuo [107]       ChatDev [1]       InteRecAgent [108]       MetaAgents [109]       TPTU [110, 111]       MetaGPT [112]       S 3 [2]       InvestLM [113]      
form, the memory information is encoded into parameters and implicitly influences the agent's actions.We summarize previous works on memory forms with their implementations in Table 2.</p>
<p>Memory in Textual Form</p>
<p>Textual form is currently the mainstream method to represent the memory contents, which is featured in better interpretability, easier implementation, and faster read-write efficiency.In specific, the textual form can be both non-structured representations like raw natural languages and structured information such as tuples, databases, and so on.In general, previous studies use the textual form memory to store four types of information including (1) complete agent-environment interactions, (2) recent agent-environment interactions, (3) retrieved agent-environment interactions, and (4) external knowledge.In the former three methods, the memory leverages natural languages to describe the information within the agent-environment interaction loop.In the former three types, they record the information inside the agent-environment interaction loop, while the last type leverages natural languages to store information outside that loop.</p>
<p>Complete Interactions.This method stores all the information of the agent-environment interaction history based on long-context strategies [116].For the example in Section 3.1, the memory of the agent in task (A) after step 2 can be implemented by concatenating all the information before step 2, and the final textual form memory is: "Your memory is [Step 1] (Agent) ... (Online Ticket Office) ... [Step 2] ... Please infer based on your memory".</p>
<p>In the previous work, different models store the memory information using different strategies.For example, in LongChat [116], the agents focus on understanding natural languages in long-context scenarios.It fine-tunes the foundation model for better adapting to memorize complete interactions.</p>
<p>Memory Sandbox [117] intends to alleviate the impact of irrelevant memory in conversations.It designs a transparent and interactive method to manage the memory of agents, which removes irrelevant memory before concatenating them as a prompt.Moreover, some efforts are dedicated to enhancing the capacity of LLMs to handle longer contexts [118,119].</p>
<p>While storing all the agent-environment interactions can maintain comprehensive information, obvious limitations exist in terms of computational cost, inference time, and inference robustness.Firstly, the fast-growing long-context memory in practice results in high computational cost during LLM inference, due to the quadratic growth of the time complexity of attention computation with sequence length.It thus requires much more computing resources and significantly increases inference latency, which hinders its practical deployment.What's more, with its fast growth, the memory length can easily exceed the upper bound of the sequence length during LLM's pretraining, which makes a truncation of memory necessary.Thus, it can lead to information loss due to the incompleteness of agent memory.Last but not least, it can lead to biases and unrobustness in LLM's inference.Specifically, a previous research [120] has shown that, the positions of text segments in a long context can greatly affect their utilization, so the memory in the long-context prompt can not be treated equally and stably.All these drawbacks show the need to design extra memory modules for LLM-based agents, rather than straightforwardly concatenating all the information into a prompt.</p>
<p>Recent Interactions.This method stores and maintains the most recently acquired memories using natural languages, thereby enhancing the efficiency of memory information utilization according to the Principle of Locality [121].In task (B) of the example in Section 3.1, we can just remember Alice's preferences in the recent three years, and truncate the distant part, where the recent three years can be considered as the memory window size.</p>
<p>In previous studies, there are various strategies to store recent textual memories.For example, SCM [98] proposes a flash memory based on the cache mechanism, which preserves observations from the recent t  1 time steps, aimed at enhancing the recency of information.MemGPT [100] considers the agent as an operating system, which can dynamically interact with users through a natural interface.</p>
<p>It designs the working context to hold recent histories, as a part of virtual context management.In RecAgent [95], the agents are designed to simulate user behaviors in movie recommendations.</p>
<p>It stores some temporal information in short-term memory as an intermediate cache, which can simulate the memory mechanism of the human brain [122,123].These representative methods can dynamically update memories based on recent interactions, and pay more attention to the recent context that is important for the current stage.</p>
<p>Caching the memory according to recency is an effective way to enhance memory efficiency, and it enables agents to focus more on the recent information.However, in long-term tasks, this method fails to access key information from distant memories.It can result in the loss of potentially crucial information that is not within the immediate cache window.In other words, emphasizing on recency can inherently neglect earlier, yet critical information, thus posing challenges in scenarios requiring a comprehensive understanding of past events.</p>
<p>Retrieved Interactions.Unlike the above method which truncates memories based on time, this method typically selects memory contents based on their relevance, importance, and topics.It ensures the inclusion of distant but crucial memories in the decision-making process, thereby addressing the limitation of only memorizing recent information.In task (A) of the example in Section 3.1, Alice's preferences have been stored in the memory before this task.At [Step 2], the agent will retrieve the most relevant aspects of Alice's preferences from memory based on the query keyword "travel", obtaining Alice's scenic spot preference for ancient architectures.In general, retrieval methods will generate embeddings as indexes for memory entries during memory writing, along with recording auxiliary information to assist in retrieval.During memory reading, matching scores are calculated for each memory entry, and the top-K entries will be used for the decision-making process of agents.</p>
<p>In existing studies, most agents utilize retrieval methods to process the memory information.For example, Park et al. [83] first calculate the relevance between the current context and memory entries by cosine similarity, and obtain the importance and recency according to auxiliary information.MemoryBank [6] employs a dual-tower dense retrieval model to find related information from past conversations.Each memory entry is encoded into an embedding and subsequently indexed by FAISS [124] to improve the efficiency of retrieval.When reading memories, the current context will be encoded as representations to obtain the most relevant memory.Moreover, RET-LLM [7] intends to design a write-read memory module for general usage.It utilizes Locality-Sensitive Hashing (LSH) to retrieve tuples with relative entries in the database to provide more information.In addition, ChatDB [96] designs to utilize symbolic memory, and proposes to generate SQL statements to retrieve from database to obtain stored information.</p>
<p>The retrieval methods considerably depend on the accuracy and efficiency of obtaining expected information.An inaccurate retrieval strategy can potentially acquire unrelated information that is unhelpful for agent inference.And a heavy retrieval system can lead to large computational costs and long time latency, especially when handling massive information.Moreover, retrieval methods typically store homogeneous information inside the environment, where all the information is in a consistent form.For heterogeneous information outside the environment, it's difficult to directly apply the same method for memory storage.</p>
<p>External Knowledge.To obtain more information, some agents acquire external knowledge by invoking tools, with the aim of transforming additional relevant knowledge into their own memories for decision-making.For instance, accessing external knowledge through Application Programming Interface (API) is a common practice [104,5].Nowadays, abundant public information, such as Wikipedia and OpenWeatherMap4 , are available online (either free of charge or on a paying basis), and can be conveniently accessed through API calls.For instance, in [Step 2] of task (A) of the example in Section 3.1, external knowledge from the digital magazine is obtained with tool methods.</p>
<p>In existing models, Toolformer [125] proposes to teach LLM to use tools, which can acquire external knowledge for better solving tasks.Furthermore, ToolLLM [126] empowers Llama [127] with the ability to utilize more APIs in RapidAPI 5 and to enable multi-tool usage, which provides a general interface to extend agents' ability.In TPTU [110], the agents are incorporated in both task planning and tool usage, in order to tackle intricate problems.The follow-up work [111] further improves its ability extensively like retrieval.In ToRA [128], the agents are required to solve mathematical problems.They utilize imitation learning to improve their ability to use program-based tools.</p>
<p>The above methods significantly advance the capabilities of agents by allowing them to access external up-to-date and real-world information from diverse sources.However, the reliability of this information can be questionable due to potential inaccuracies and biases [18].Furthermore, the integration of tools into agents demands a comprehensive understanding to interpret the retrieved information across various contexts, which can incur higher computational costs and complications in aligning external data with internal decision-making processes.Additionally, utilizing external APIs brings forth concerns regarding privacy, data security, and compliance with usage policies, necessitating rigorous management and oversight [18].</p>
<p>Memory in Parametric Form</p>
<p>An alternative type of approaches is to represent memory in parametric form.They do not take up the extra length of context in prompts, so they are not constrained by the length limitations of LLM context.However, the parametric memory form is still under-researched, and we categorize previous works into two types: fine-tuning methods and memory editing methods.</p>
<p>Fine-tuning Methods.Integrating external knowledge into the memory of agents is beneficial for enriching domain-specific knowledge on top of its general knowledge.To infuse the domain knowledge into LLMs, supervised fine-tuning is a common approach, which empowers agents with the memory of domain experts.It significantly improves the agent's ability to accomplish domainspecific tasks.In task (A) of the example in Section 3.1, the external knowledge of attractions from magazines can be fine-tuned into the parameters of LLMs prior to this task.</p>
<p>In previous works, Character-LLM [105] focuses on the role-play circumstance.It utilizes supervised fine-tuning strategies with role-related data (e.g., experiences), to endow agents with the specific traits and characteristics of the role.Huatuo [107] intends to empower agents with professional ability in the biomedical domain.It tries to fine-tune Llama [127] on Chinese medical knowledge bases.Besides, in order to create artificial doctors, DoctorGLM [129] fine-tunes ChatGLM [130] with LoRA [131], and Radiology-GPT [132] improves domain knowledge on radiology analysis by supervised fine-tuning on an annotated radiology dataset.Moreover, InvestLM [113] collects investment data and fine-tunes it to improve domain-specific abilities on financial investment.</p>
<p>The fine-tuning methods can effectively bridge the gap between general agents and specialized agents.It improves the capability of agents on the tasks that require high accuracy and reliability on domain-specific information.Nevertheless, fine-tuning LLMs for specific domains could potentially lead to overfitting, and it also raises concerns about catastrophic forgetting, where LLMs may forget the original knowledge because of updating their parameters.Another limitation of fine-tuning lies in the computational cost and time consumption, as well as the requirement of a large amount of data.Therefore, most fine-tuning approaches are applied to offline scenarios, and can seldom deal with online scenarios, such as fine-tuning with agent observations and trial experiences.Due to the frequent agent-environment interactions, it is unaffordable for the cost of backpropagation to fine-tune every step of the online and dynamic interactions.</p>
<p>Memory Editing Methods.Apart from the fine-tuning approaches, another type of methods for infusing memory into model parameters is knowledge editing [133,134].Unlike fine-tuning methods that extract patterns from certain datasets, knowledge editing methods specifically target and adjust only the facts that need to be changed.It ensures that unrelated knowledge remains unaffected.Knowledge editing methods are more suitable for small-scale memory adjustments.Generally, they have lower computational costs, making them more suitable for online scenarios.In our example of task (B), Alice always watches movies at 9:00 PM from the agent's memory, but she may recently change her work and would not be empty at 9:00 PM.If so, the related memory (such as routines at 9:00 PM) should be edited, which can be implemented by knowledge editing methods.</p>
<p>In previous studies, MAC [106] intends to design an effective and efficient memory adaptation framework for online scenarios.It utilizes meta-learning to substitute the optimization step.Per-sonalityEdit [135] focuses on editing the personality of LLMs and agents, where it changes their traits based on theories such as the big-five factor.MEND [134] utilizes the idea of meta-learning to train a lightweight model, which is capable of generating modifications for model parameters of a pre-trained language model.APP [136] studies whether adding a new fact leads to catastrophic forgetting of existing facts.It focuses on the impact of neighbor perturbation on memory addition.Moreover, KnowledgeEditor [133] trains a hyper-network to predict the modification of model parameters when injecting memory based on a learning-to-update problem formulation.Wang et al. [137] propose a new optimization target to change the poisoning knowledge of LLM, and maintain the general performance at the same time.For LLM-based agents, the agents can change bad memory by knowledge editing, which can be considered as a type of forgetting mechanism.</p>
<p>Knowledge editing methods provide an innovative way to update the information stored within the parameters of LLMs.By specifically targeting and adjusting the facts, these methods can ensure the non-targeted knowledge unaffected during updates, thus mitigating the issue of catastrophic forgetting.Moreover, the targeted adjustment mechanism allows for more efficient and less resourceintensive updates, making knowledge editing an appealing choice for high-precision and real-time modifications.However, despite these promising developments, computational costs of meta-training and the preservation of unrelated memories remain significant challenges.</p>
<p>Advantages and Disadvantages of Textual and Parametric Memory</p>
<p>Textual memory and parametric memory have their strengths and weaknesses respectively, making them suitable for different memory contents and application scenarios.In this section, we discuss the advantages and disadvantages of these two forms of memory from various aspects.</p>
<p>Effectiveness.The textual memory stores raw information about the agent-environment interactions, which is more comprehensive and detailed.However, it is constrained by the token limitation of LLM prompts, which makes the agent hard to store extensive information.In contrast, the parametric memory is not limited by the prompt length, but it may suffer from information loss when transforming texts into parameters, and the complex memory training can bring additional challenges.</p>
<p>Efficiency.For textual memory, each LLM inference requires to integrate memory into the context prompt, which leads to higher costs and longer processing times.In contrast, for parametric memory, the information can be integrated into the parameters of the LLM, eliminating the extra costs of these contexts.However, parametric memory takes additional costs in the writing process, but textual memory is easier to write, especially for small amounts of data.In a nutshell, textual memory is more efficient in writing, while parametric memory is more efficient in reading.</p>
<p>Interpretability.Textual memory is usually more explainable than the parametric one, since natural languages are the most natural and straightforward strategies for humans to understand, while parametric memory is commonly represented in latent space.Nevertheless, such explainability is obtained at the cost of information density.This is because the sequences of words in textual memory are represented in a discrete space, which is not as dense as continuous space in parametric memory.</p>
<p>In conclusion, the trade-offs between these two types of memories make them suitable for different applications.For example, for the tasks that require recalling recent interactions, like conversational and context-specific tasks, textual memory seems more effective.For the tasks that require a large amount of memory, or well-established knowledge, parametric memory can be a better choice.</p>
<p>Memory Operations</p>
<p>We separate the entire procedure of memory into three operations: memory writing, memory management, and memory reading.These three typically collaborate to achieve memory function, providing information for LLM inference.We summarize previous works on memory operations in Table 3.</p>
<p>Memory Writing</p>
<p>After the information is perceived by the agent, a part of it will be stored by the agent for further usage through the memory writing operation, and it is crucial to recognize which information is essential to store.Many studies choose to store the raw information, while others also put the summary of the raw information into the memory module.</p>
<p>Representative Studies.In TiM [97], the raw information will be extracted as the relation between two entities, and stored in a structured database.When writing into the database, similar contents will be stored in the same group.In SCM [98], it designs a memory controller to decide when to execute the operations.The controller serves as a guide for the whole memory module.In MemGPT [100], the memory writing is entirely self-directed.The agents can autonomously update the memory based on the contexts.In MemoChat [94], the agents summarize each conversation segment by abstracting the mainly discussed topics and storing them as keys for indexing memory pieces.</p>
<p>Discussion.Previous research indicates that designing the strategy of information extraction during the memory writing operation is vital [94].This is because the original information is commonly lengthy and noisy.Besides, different environments may provide various forms of feedback, and how to extract and represent the information as memory is also significant for memory writing.</p>
<p>Memory Management</p>
<p>For human beings, memory information is constantly processed and abstracted in the brains.The memory in the agent can also be managed by reflecting to generate higher-level memories, merging redundant memory entries, and forgetting unimportant, early memories.</p>
<p>Representative Studies.In MemoryBank [6], the agents process and distill the conversations into a high-level summary of daily events, similar to how humans recall key aspects of their experiences.Through long-term interactions, they continually evaluate and refine their knowledge, generating daily insights into personality traits.In Voyager [99], the agents are able to refine their memory based on the feedback of the environment.In Generative Agents [83], the agents can reflect to get higher-level information, where the abstract thoughts are generated from agents.The reflection process will be activated when there are accumulated events that are enough to address.For GITM [93], in order to establish common reference plans for various situations, key actions from multiple plans are further summarized in the memory module.</p>
<p>Discussion.Most of the memory management operations are inspired by the working mechanism of human brains.With the strong capability of LLMs to simulate human minds, these operations can help the agents to better generate high-level information and interact with environments.
MemoryBank [6]      RET-LLM [7]      ChatDB [96]      TiM [97]      SCM [98]      Voyager [99]      MemGPT [100]      MemoChat [94]      MPC [101]      Generative Agents [83]      RecMind [102]      Retroformer [103]      ExpeL [82]      Synapse [91]      GITM [93]      ReAct [104]      Reflexion [5]      RecAgent [95]      Character-LLM [105]      MAC [106]      Huatuo [107]      ChatDev [1]      InteRecAgent [108]      MetaAgents [109]      TPTU [110, 111]      MetaGPT [112]      S 3 [2]      InvestLM [113]     </p>
<p>Memory Reading</p>
<p>When the agents require information for reasoning and decision-making, the memory reading operation will extract related information from memory for usage.Therefore, how to access the related information for the current state is important.Due to the massive quantity of memory entities, and the fact that not all of them are pertinent to the current state, careful design is required to extract useful information based on relevance and other task-orientated factors.</p>
<p>Representative Studies.In ChatDB [96], the memory reading operation is executed by the SQL statements.These statements will be generated by agents as a series of Chain-of-Memory in advance.</p>
<p>In MPC [101], the agents can retrieve relevant memory from the memory pool.This method also proposes to provide Chain-of-Thought examples for ignoring certain memory.ExpeL [82] utilizes the Faiss [124] vector store as the pool of memory, and obtains the top-K successful trajectories that share the highest similarity scores with the current task.</p>
<p>Discussion.To some extent, the memory reading and writing operations are collaborative, and the forms of memory writing greatly influence the methods of memory reading.For the forms of textual For the forms of parametric memory, existing models may just utilize the updated parameters for inference, which can be seen as an implicit reading process.</p>
<p>How to Evaluate the Memory in LLM-based Agent</p>
<p>How to effectively evaluate the memory module remains an open problem, where diverse evaluation strategies have been proposed in previous works according to different applications.To clearly show the common ideas of different evaluation methods, in this section, we summarize a general framework, which includes two broad evaluation strategies (see Figure 5 for an overview), that is, (1) direct evaluation, which independently measures the capability of the memory module.( 2) indirect evaluation, which evaluates the memory module via end-to-end agent tasks.If the tasks can be effectively accomplished, the memory module is demonstrated to be useful.</p>
<p>Direct Evaluation</p>
<p>This type of approaches regards the memory of the agents as a stand-alone component and evaluates its effectiveness independently.Previous studies can be categorized into two classes: subjective evaluation and objective evaluation.The subjective evaluation aims to measure memory effectiveness based on human judgments, which can be widely used in the scenarios that lack objective ground truths.Objective evaluation assesses memory effectiveness based on numerical metrics, which makes it easy to compare different memory modules.</p>
<p>Subjective Evaluation</p>
<p>In subjective evaluation, there are two key problems, that is, (1) what aspects should be evaluated and (2) how to conduct the evaluation process.To begin with, the following two aspects are the most common perspectives leveraged to evaluate the memory module.</p>
<p>Coherence.This aspect refers to whether the recalled memory is natural and suitable for the current context.For example, if the agent is making a plan for Alice's travel, the memory should be related to her preference for traveling rather than working.In previous works, Modarressi et al. [7] study whether the memory module could provide proper references among the ever-changing knowledge.Liang et al. [98] present some examples to demonstrate the relation between the current query and historical memory.Zhong et al. [6] and Liu et al. [97] assess the coherence of responses that integrate context and retrieved memory by scoring labels.Lee et al. [101] focus on the contradiction between the recalled memory and contexts.</p>
<p>Rationality.This aspect aims to evaluate whether the recalled memory is reasonable.For example, if the agent is asked to answer "Where is the Summer Palace", the recalled memory should be "The Summer Palace is in Beijing" rather than "The Summer Palace is on the Moon".In previous works, Lee et al. [101] ask crowd workers to directly score the rationality of the retrieved memory.Zhong et al. [6] and Liu et al. [97] recruit human evaluators to check if the memory contains reasonable answers for the current question.</p>
<p>As for how to conduct the evaluation process, there are two important problems.The first one is how to select the human evaluators.In general, the evaluators should be familiar with the evaluation task, which ensures that the labeling results are convincing and reliable.In addition, the backgrounds of the evaluators should be diverse to remove subjective biases of specific human groups.The second problem is how to label the outputs of the memory module.Usually, one can either directly score the results [6] or make comparisons between two candidates [95].The former can obtain absolute and quantitative evaluation results, while the latter can remove the labeling noises when independently scoring each candidate.In addition, the granularity of the ratings should also be carefully designed.Too coarse ratings may not effectively discriminate the capabilities of different memory modules, while too fine-grained ones may bring more effort for the workers to make judgments.</p>
<p>In general, subjective evaluation can be used in a wide range of scenarios, where one just needs to define the evaluation aspects and let recruited workers make judgments.This method is usually more explainable since the workers can provide the reasons for their judgments.However, subjective evaluation is costly due to the need to employ human evaluators.Additionally, different groups of evaluators may have various biases, making the results difficult to reproduce and compare.</p>
<p>Objective Evaluation</p>
<p>In objective evaluation, previous work usually defines numeric metrics to evaluate the effectiveness and efficiency of the memory module.</p>
<p>Result Correctness.This metric measures whether the agent can successfully answer pre-defined questions directly based on the memory module.For example, the question could be "Where did Alice go today?" with two choices "A: the Summer Palace" and "B: the Great Wall".Then, the agent should choose the correct answer based on the problem and its memory.The agent-generated answer will be compared with the ground truth.Formally, the accuracy can be calculated as
Correctness = 1 N N i=1 I [a i = i ] ,
where N is the number of problems, a i represents the ground truth for the i-th problem, i means the answer given by the agent, and I [a i = i ] is the matching function commonly represented as
I [a i = i ] = 1 if a i = i , 0 if a i  = i .
In previous works, Hu et al. [96] construct questions from past histories with annotated ground truths and calculate the accuracy of whether the recalled memory could match the correct answers.Similarly, Packer et al. [100] generate questions and answers that can only be derived from past sessions, and compare the responses from the agents with the ground truths to calculate the accuracy.</p>
<p>Reference Accuracy.This metric evaluates whether the agent can discover relevant memory contents to answer the questions.Different from the above metric, which focuses on the final results, reference accuracy cares more about the intermediate information to support the agent's final decisions.In specific, it compares the retrieved memory with the pre-prepared ground truth.For the above problem of "Where did Alice go today?", if the memory contents include (A) "Alice had lunch with friends at Wangfujing today."and (B) "Alice had roast duck for lunch", then a better memory module should select (A) as a reference to answer the question.Usually, researchers leverage F1-score to evaluate the reference accuracy, which is calculated as
F1 = 2  Precision  Recall Precision + Recall ,
where the precision and recall scores are calculated as Precision = TP TP+FP and Recall = TP TP+FN .The TP represents the number of true positive memory contents, FP means the number of false positive memory contents, and FN indicates the number of false negative memory contents.In previous works, Lu et al. [94] utilize F1-score to evaluate the retrieval process of the memory, and Zhong et al. [6] focus on assessing whether related memory can be successfully retrieved.</p>
<p>Result Correctness and Reference Accuracy are both utilized to evaluate the effectiveness of the memory module.Beyond effectiveness, efficiency is also an important aspect, especially for realworld applications.Therefore, we describe the evaluation of efficiency as follows.</p>
<p>Time &amp; Hardware Cost.The total time cost includes the time leveraged for memory adaption and inference.The adaptation time refers to the time of memory writing and memory management, while the inference time indicates the time latency of memory reading.In specific, the difference from the end time to the start time of memory operations can be considered as the time consumption.Formally, the average time consumption of each type of operation can be represented as
time = 1 M M i=1 t end i  t start i ,
where M represents the number of these operations, t end i means the end time of the i-th operation, and t start i indicates the start time of that operation.As for the computation overhead, it can be evaluated by the peak GPU memory allocation.In previous works, Tack et al. [106] utilize the peak memory allocation and adaptation time to assess the efficiency of memory operations.</p>
<p>Objective evaluation offers numeric strategies to compare different methods of memory, which is important to benchmark this field and promote future developments.</p>
<p>Indirect Evaluation</p>
<p>Besides the above method that directly evaluates the memory module, evaluating via task completion is also a popular evaluation strategy.The intuition behind this type of approaches is that if the agent can successfully complete a task that highly depends on memory, it suggests that the designed memory module is effective.In the following parts, we present several representative tasks that are leveraged to evaluate the memory module in indirect ways.</p>
<p>Conversation</p>
<p>Engaging in conversations with humans is one of the most important applications of agents, where memory plays a crucial role in this process.By storing context information in memory, the agents allow users to experience personalized conversations, thus improving users' satisfaction.Therefore, when other parts of the agents are determined, the performance of the conversation tasks can reflect the effectiveness of different memory modules.</p>
<p>In the context of conversation, consistency and engagement are two commonly used methods to evaluate the effectiveness of the agents' memory.Consistency refers to how the response from agents is consistent with the context because dramatic changes should be avoided during the conversation.For example, Lu et al. [94] evaluate the consistency of agents on interactive dialogues, using GPT-4 to score on the responses from agents.Engagement refers to how the user is engaged to continue the conversation.It reflects the quality and attraction of agents' responses, as well as the ability of agents to craft the personas for current conversations.For example, Lee et al. [101] assess the engagingness of responses by SCE-p score, and Packer et al. [100] utilize CSIM score to evaluate the memory effect on increasing engagement of users.</p>
<p>Multi-source Question-answering</p>
<p>Multi-source questing-answering can comprehensively evaluate the memorized information from multiple sources, including inside-trial information, cross-trial information, and external knowledge.It focuses on the integration of memory utilization from various contents and sources.</p>
<p>In previous works, Yao et al. [104] evaluate the memory that integrates information from the task trial and the external knowledge from Wikipedia.Then, Shinn et al. [5] and Yao et al. [103] further include the cross-trial information of the same task, where the memory is permitted to obtain more experiences from previous failed trials.Moreover, Packer et al. [100] allow agents to utilize the memory from multi-document information for question-answering.</p>
<p>By evaluating multi-source question-answering tasks, the memory of agents can be examined on the capability of content integration from various sources.It also reveals the issue of the memory contradiction due to multiple information sources, and the problem of updated knowledge, which can potentially affect the performance of the memory module.</p>
<p>Long-context Applications</p>
<p>Beyond the above general applications, in many scenarios, LLM-based agents have to make decisions based on extremely long prompts.In these scenarios, the long prompts are usually regarded as the memory contents, which play an important role in driving agent behaviors.</p>
<p>In previous works, Huang et al. [19] organize a comprehensive survey for long-context LLMs, which provides a summary of evaluation metrics on long-context scenarios.Moreover, Shaham et al. [138] propose a zero-shot benchmark for evaluating agents' understanding of long-context natural languages.As for specific long-context tasks, long-context passage retrieval is one of the important tasks for evaluating the long-context ability of agents.It requires agents to find the correct paragraph in a long context that corresponds to the given questions or descriptions [139].Long-context summarization is another representative task.It requests agents to formulate a global understanding of the whole context, and summarizes it according to the descriptions, where some metrics on matching scores like ROUGE can be utilized to compare the results with ground truths.</p>
<p>The evaluation of long-context applications provides broader approaches to assess the function of memory in agents, focusing on practical downstream scenarios.The comprehensive benchmarks [138,140] also provide an objective assessment for the ability of long-context understanding.</p>
<p>Other Tasks</p>
<p>In addition to the above three types of major tasks for indirect evaluation, there are also some other metrics in general tasks that can reveal the effectiveness of the memory module.</p>
<p>Success rate refers to the proportion of tasks that agents can successfully solve.For Yao et al. [104], Shinn et al. [5] and Zhao et al. [82], they assess how many spacial tasks can be correctly completed through reasoning and memory in AlfWorld [141].In Zhu et al. [93], they evaluate the success rate of producing different items in Minecraft to show the effect of memory.Moreover, Shinn et al. [5] measure the success rate of passed problems by generated codes, and Zheng et al. [91] calculate the success rate of computer control and accuracy of element selection to show the function of trajectory-as-exemplar memory.Exploration degree typically appears in exploratory games, which reflects the extent that agents can explore the environment.For example, Wang et al. [99] compare the numbers of distinct items explored in Minecraft to reflect the skill learning in memory.</p>
<p>In fact, nearly all the memory-equipped agents can evaluate the effect of memory by ablation studies, comparing the performance between with/without memory modules.The evaluation on specific scenarios can better reflect the significance of memory for the downstream applications practically.</p>
<p>Discussions</p>
<p>Compared with direct evaluation, indirect evaluation via specific tasks can be easier to conduct, since there are already many public benchmarks.However, the performance on tasks can be attributed to various factors, and memory is only one of them, which may make the evaluation results biased.By direct evaluation, the effectiveness of the memory module can be independently evaluated, which improves the reliability of the evaluation results.However, to our knowledge, there are no open-sourced benchmarks tailored for the memory modules in LLM-based agents.</p>
<p>Memory-enhanced Agent Applications</p>
<p>Recently, LLM-based agents have been investigated across a wide variety of scenarios, facilitating societal advancement.In general, most LLM-based agents are equipped with memory modules.However, the specific effects undertaken by these memory components, the particular information they store, and the implementation methods they use, vary across different applications.In order to provide insights for the design of memory functionalities in LLM-based agents, in this section, we review and summarize how memory mechanisms are manifested in LLM-based agents across various application scenarios.In specific, we categorize them into several classes: role-playing and social simulation, personal assistant, open-world games, code generation, recommendation, expert systems in specific domains, and other applications.The summarization is shown in Table 4.</p>
<p>Role-playing and Social Simulation</p>
<p>Role-playing represents a classic application of LLM-based agents, where memory plays a crucial role inside the agents.It endows roles with distinct characteristics, differentiating them from one another.Many previous studies have explored methods for constructing role memories [105,143,[145][146][147].Shao et al. [105] construct the memory of roles by experience uploading, which utilizes SFT to inject memory into model parameters.Li et al. [143] enhance large language models for role-playing via an improved prompt and the character memory extracted from scripts, where user queries and Table 4: Summarization of memory-enhanced agents applications.</p>
<p>Applications Models Applications Models</p>
<p>Role-playing</p>
<p>Character-LLM [105] Code Generation</p>
<p>RTLFixer [142] ChatHaruhi [143] GameGPT [144] RoleLLM [145] ChatDev [1] NarrativePlay [146] MetaGPT [109] CharacterGLM [147] CodeAgent [114] Social Simulation</p>
<p>Generative Agents [83] Recommendation RecAgent [95] Lyfe Agents [148] InteRecAgent [108] S 3 [2] RecMind [102] MetaAgents [109] AgentCF [149] WarAgent [150] Medicine Huatuo [107] Personal Assistant</p>
<p>MemoryBank [6] DoctorGLM [129] RET-LLM [7] Radiology-GPT [132] MemoChat [94] Wang et al. [151] MemGPT [100] EHRAgent [152] MPC [101] ChatDoctor [115] AutoGen [153] Finance InvestLM [113] ChatDB [96] TradingGPT [154] TiM [97] QuantAgent [155] SCM [98] FinMem [156] Game Voyager [99] Koa et al. [157] GITM [93] Science Chemist-X [158] JARVIS [159] ChemDFM [160] LARP [161] MatChat [162] chatbot's responses are concatenated to form a sequence as memory.Wang et al. [145] infuse role-specific knowledge and episode memories into LLM-based agents, where context QA pairs are concatenated to form episode memory.Zhao et al. [146] aim to generate human-like responses, guided by personality traits extracted from narratives, which can be stored and retrieved by relevance and importance.Zhou et al. [147] generate character-based dialogues for different roles and empower LLM-based agents with corresponding styles by SFT.</p>
<p>Social simulation is basically an extension of role-playing, which focuses more on multi-agent modeling.The memory module is an important component for such applications, which helps to accurately simulate human dynamic behaviors.In previous studies, Kaiya et al. [148] propose a Summarize-and-Forget memory mechanism for better self-monitoring in social scenarios.Gao et al. [2] focus on social network simulation systems.Each agent in the system has a memory pool, which consists of diverse user messages from online platforms to identify the user.Li et al. [163] maintain conversation contexts, encompassing the economic environment and agent decisions from previous months, in order to simulate the impact of broad macroeconomic trends on agents' decision-making and to make the agents grasp market dynamics.Li et al. [109] simulate the job-seeking scenario in human society, where the memory of agents includes profiles and goals initially and is further enriched with other information, like dialogues and personal reflections.Hua et al. [150] simulate the decisions and consequences of the participating countries in the wars, where the conversations of agents are continuously maintained into memory.</p>
<p>There are several insights in designing an agent's memory for role-play and social simulation.First, the memory should be consistent with the roles' characteristics, which can be used to identify each role and distinguish it from the others.This is crucial for improving the realism of role-play and the diversity of social simulation.Second, the memory should appropriately influence the subsequent actions of the agent to ensure the consistency and rationality of its behaviors.Additionally, for humanoid agents, their memory mechanisms should align with the features of human memory, such as forgetting and long/short-term memory, which should refer to the theories of cognitive psychology.</p>
<p>Personal Assistant</p>
<p>LLM-based agents are well-suited for creating personal assistants, such as agents capable of engaging in long-term conversations with users [94,101,153], as well as those tasked with automatically seeking information [164].These agents often need to memorize previous dialogues to maintain the consistency, and remember critical styles and events to generate more personalized and relevant responses.Lu et al. [94] maintain the context consistency for dialogues by saving contents and information of conversations, which helps to find proper relevant information by retrieval.Lee et al. [101] summarize conversations to extract important information, store it, and retrieve it for future inference.Pan et al. [164] focus on information-seeking tasks, which design memory modules to store user's context information, and empower external knowledge with tool usage.Wu et al. [153] retain important context as memory to maintain conversation consistency.</p>
<p>In summary, most memory implementations for personal assistants adopt retrieval methods in textual form, because they are better at finding relevant information from pieces of conversations.For the memory storage, the agent should remember the factual information during user-agent interactions, as well as the personal style of users, in order to generate responses that are tailored to the user's situation.Additionally, when recalling memories, the agent should identify and retrieve the memory that is relevant to the current query and context.This principle can enable the agent to correctly understand the user's requirement, and maintain the consistency in conversations.</p>
<p>Open-world Game</p>
<p>For games and open-world exploration, LLM-based agents always maintain post observations as task contexts, and store experiences in previous successful trials.By leveraging past experiences, agents can avoid making the same mistakes repeatedly and achieve a high-level understanding of environments, thus exploring more effectively.Some of them can acquire external databases or APIs to obtain general knowledge [99,93,159,161].Wang et al. [99] save obtained skills into memory for further usage in Minecraft.Zhu et al. [93] store and retrieve successful trajectories as examples for similar tasks, and utilize external Minecraft Wiki by API calls.Wang et al. [159] construct multimodal memory as a knowledge library and provide examples for prompt by retrieving interactive experiences.Yan et al. [161] maintain working memory for decision-making, save and retrieve relevant past experiences, and implement external datasets for general knowledge.</p>
<p>In summary, no matter inside-trial or cross-trial information, the key aspect of memory is to reflect on past interactions and draw experiences that can be applied to the subsequent exploration.In addition to accumulating experience through self-involving trials, absorbing external knowledge as part of the agent's memory is also an important way to enhance the exploratory capabilities of the agent.</p>
<p>Code Generation</p>
<p>In the scenario of code generation, LLM-based agents can search relevant information from the memory, thereby obtaining more knowledge for development.They can save previous experiences for future problems, and also maintain context in conversational development interfaces [142,144,1,109].Tsai et al. [142] construct an external non-parametric memory database, which stores the compiler errors and human expert instructions for automatic syntax error fixing.In [144], personal information will be stored in the memory, and helps in retaining context and knowledge for decision-making.Qian et al. [1] adopt multi-agents to develop software, where each role maintains a memory to store the past conversations with other roles.Li et al. [109] also focus on software development, and the agent can retrieve its historical records preserved in memory when errors occur.Zhang et al. [114] can search relevant information when they face problems on code generation.</p>
<p>By leveraging external resources, the agents can learn from code-related knowledge and store it into their memory, thereby enhancing the capabilities of code generation.In addition, the memory can improve the continuity and consistency in code generation.By integrating contextual memory, the agent can better understand the requirements for software development, thereby enhancing the coherence of the generated code.Furthermore, the memory is also crucial for the iterative optimization of code, as it can identify the developer's targets based on the histories.</p>
<p>Recommendation</p>
<p>In the field of recommendation, some previous works focus on simulating users in recommender systems [95,108], where the memory can represent the user profiles and histories in the real world.Others try to improve the performance of recommendation, or provide other formats of recommendation interfaces [149,102].Wang et al. [95] simulate user behaviors in recommendation scenarios to generate data for recommender systems, and the agents store past observations and insights into a hierarchical memory.In Huang et al. [108], the memory in LLM-based agents can archive the user's conversational history over extended periods, as well as capture the most recent dialogues pertinent to the current prompt, to simulate interactive recommender systems.It also uses an actor-critic reflection to improve the robustness of agents.Item agents and user agents are equipped with different memories in [149], where item agents are endowed with dynamic memory modules designed to capture and preserve information pertinent to their intrinsic attributes and the inclinations of their adopters.For user agents, the adaptive memory updating mechanism plays a pivotal role in aligning the agents' operations with user behaviors and preferences.Wang et al. [102] memorize individualized user information like reviews or ratings for items, and acquire domainspecific knowledge and real-time information by web searching tools.</p>
<p>For both simulating users in recommender systems and capturing their preferences, retaining personalized information through memory is essential.A critical challenge lies in how to align the personalized information and feedback with LLMs, and store them into the memory of agents.It is also an important task for bridging the gap between conventional recommendation models and LLMs.</p>
<p>Expert System in Specific Domains</p>
<p>Medicine Domain.In the field of medicine, most of the previous works empower LLM-based agents with external knowledge in their memory [107,129,132,151,115].Wang et al. [107] fine-tune LLaMA [127] with medical knowledge graph CMeKG [165] in QA form, in order to enhance their medical domain knowledge.Xiong et al. [129] adopt LoRA [131] to efficiently fine-tune on foundation models for healthcare.Wang et al. [151] empower LLM-based agents to acquire text-based external knowledge as reasoning reference.Besides, Shi et al. [152] build memory upon the most relevant successful cases from past experiences, and use similarity metric for the retrieval of relevant questions in the medicine domain.</p>
<p>Finance Domain.Some previous works also apply LLM-based agents in the finance domain, whose memory can store financial knowledge [113], market information [154,156], and successful experiences [157,155].Yang et al. [113] construct financial investment dataset to fine-tune LLaMA [127] to empower knowledge on investment.Li et al. [154] design a layered-memory structure to store different types of marketing information.Wang et al. [155] record the ongoing interaction like exchanges and information to ensure consistent response, and record prior outputs as experiences for retrieving relevant examples to provide a diverse learning context for agents.Koa et al. [157] store past price movement and explanations, and generate reflections on previous trials.Yu et al. [156] adopt a layered memory mechanism to provide abundant information for reasoning.</p>
<p>Science.In the domain of science, some existing works design LLM-based agents with a large amount of knowledge in memory to solve problems [158,160,162].Chen et al. [158] include molecule database and online literature as external knowledge for memory in LLM-based agents, and retrieve them when they need related information.Zhao et al. [160] and Chen et al. [162] empower domain knowledge by fine-tuning in Chemistry and structured materials respectively.</p>
<p>To build an expert system based on agents in a specific vertical domain, it is necessary to retain the domain-specific knowledge in their memory.However, there are several challenges.First, domain knowledge is specialized and requires higher accuracy, leading to difficulties in constructing memory storage.Second, domain knowledge is often time-sensitive, which can become outdated in the future.Therefore, the memory needs to be partially updated when some of the knowledge has been out-of-date.Furthermore, the substantial volume of domain knowledge makes it difficult to recall from memory based on the current query.</p>
<p>Other Applications</p>
<p>There are some other applications of memory in LLM-based agents.Wang et al. [166] focus on the task of cloud root cause analysis, using memory to store framework rules, task requirements, tools documentation, few-shot examples, and agent observations.Qiang et al. [167] solve the problem of ontology matching.The agents save conversational dialogues and construct a rational database for retrieving external knowledge.Wen et al. [168] investigate autonomous driving, whose memory module is constructed by a vector database and contains the experiences from past driving scenarios.Wang et al. [169] propose to improve user acceptance testing, which employs a self-reflection mechanism.After each trial, the operation agent summarizes the conversation and updates the memory pool, until the goal of the current step is accomplished.</p>
<p>For different applications, the focus of memory varies, as it inherently serves the downstream tasks.Therefore, the design should also consider the requirements of tasks.</p>
<p>8 Limitations &amp; Future Directions</p>
<p>More Advances in Parametric Memory</p>
<p>At present, the memory of LLM-based agents is predominantly in textual form, especially for contextual knowledge such as observation records, trial experiences, and textual knowledge databases.</p>
<p>Although textual memory possesses the advantages of being interpretable and easy to expand and edit, it also implies a sacrifice in efficiency compared to parametric memory.Essentially, parametric memory boasts a higher information density, expressing semantics through continuous real-number vectors in a latent space, whereas textual memory employs a combination of tokens in a discrete space for semantic expression.Thus, parametric memory offers a richer expressive space, and its soft encoding is more robust compared to the hard-coded form of token sequences.Additionally, parametric memory is more storage-efficient, where it does not require the explicit storage of extensive texts, similar to a knowledge compression process.As for the memory management, such as merging and reflection, parametric memory does not necessarily design manual rules like textual memory does, but can employ optimization methods to learn these processes implicitly.Moreover, pluggable parametric memory is similar to a digital life card, capable of endowing agents with the requisite characteristics.For example, Huatuo [107] aims to enhance agents with expertise in the biomedical field by refining the Llama [127] model on Chinese medical knowledge bases.MAC [106] is designed to create a parametric memory adaptation framework suitable for online settings, employing meta-learning techniques to replace the traditional optimization phase.</p>
<p>Although parametric memory holds great prospects, it currently faces numerous challenges.Foremost among these is the issue of efficiency: how to effectively transform textual information into parameters or modifications of parameters is a critical question.Presently, researchers can transfer vast amounts of domain knowledge into the parameters of LLMs by SFT.However, it is time-consuming and requires extensive text corpus, making it unsuitable for situational knowledge.One viable approach is to employ meta-learning to let models learn to memorize.For example, MEND [134] leverages the method of meta-learning to train a compact model that has the ability to produce adjustments for the parameters of a pre-trained language model.Moreover, the lack of interpretability associated with parametric memory can be a hindrance, especially in domains requiring high levels of trust, such as medicine.Therefore, enhancing the credibility and interpretability of parametric memory is an urgent issue that needs to be addressed.</p>
<p>Memory in LLM-based Multi-agent Applications</p>
<p>The exploration of memory mechanisms within LLMs has burgeoned into the dynamic domain of multi-agent systems (MAS), marking significant advancements in the realms of synchronization, communication, and the management of information asymmetry.One pivotal aspect that emerges in the cooperative scenarios is memory synchronization among agents.This process is fundamental for establishing a unified knowledge base, ensuring consistency in decision-making across different agents.For example, Chen et al. [170] emphasize the significance of integrating synchronized memory modules for multi-robot collaboration.Another important aspect is the communication among agents, which heavily relies on memory for maintaining context and interpreting messages.For example, Mandi et al. [171] illustrate memory-driven communication frameworks that foster a common understanding among agents.In addition to cooperative scenarios, some studies also focus on competitive scenarios, and the information asymmetry becomes a crucial issue [172].</p>
<p>Looking ahead, the advancement of memory in LLM-based MAS is poised at the confluence of technological innovation and strategic application.It beckons the exploration of novel memory modules that can further enhance agent synchronization, enable more effective communication, and provide strategic advantages in information-rich environments.The development of such memory models would not only necessitate addressing the current challenges of memory integration and management, but also explore the untapped potentials of memory in facilitating more robust, intelligent, and adaptable MAS.As evidenced by pioneering research, the evolving landscape of LLM-based MAS sets a promising stage for future innovations in memory utilization and management.This exploration is expected to unravel new dimensions of memory integration, pushing the boundaries of what is currently achievable and setting new benchmarks in the realm of MAS.</p>
<p>Memory-based Lifelong Learning</p>
<p>Lifelong learning is an advanced topic in artificial intelligence, extending the learning capabilities of agents across their life-long span [173].Agents can continuously interact with their environment, persistently observe environments, and acquire external knowledge, enabling a mode of enhancement like humans.The memory of an agent is key to achieving lifelong learning, as it needs to learn to store and apply the past observations.Lifelong learning in LLM-based agents holds significant practical value, such as in long-term social simulations and personal assistance.However, it also faces several challenges.Firstly, lifelong learning is temporal, necessitating that an agent's memory captures temporality.This temporality could cause interactions between memories, such as memory overlap.Furthermore, due to the extended period of lifelong learning, it needs to store a vast amount of memories and retrieve them when needed, possibly incorporating a certain mechanism for forgetting.</p>
<p>Memory in Humanoid Agent</p>
<p>A humanoid agent refers to an agent designed to exhibit behaviors consistent with humans, thereby facilitating applications in social simulation, studies of human behavior, and role-playing.Unlike task-oriented agents where greater capability is typically preferred, the proficiency of a humanoid agent should closely mimic that of humans.Consequently, the memory of humanoid agents should align with human cognitive processes, adhering to psychological principles such as memory distortion and forgetfulness.Additionally, humanoid agents should possess knowledge boundaries, meaning that their knowledge should correspond to that of the entity they replicate.For instance, in role-playing scenarios, an agent embodying a child should not possess an understanding of advanced mathematical concepts or other complex knowledge beyond what is typical for that age [174].</p>
<p>Conclusion</p>
<p>In this survey, we provide a systematical review on the memory mechanism of LLM-based agents, where we focus on three key problems including "What is", "Why do we need" and "How to design and evaluate" the memory module in LLM-based agents.To show the importance of the agent's memory, we also present many typical applications, where the memory module plays an important role.We believe this survey can offer valuable references for newcomers to this domain, and also hope it can inspire more advanced memory mechanisms to enhance LLM-based agents.</p>
<p>According to the previous works, large language model based agents refer to artificial  (Context) The current topic is LLM-based agent."It" refers to LLM-based agents in this conversation.Social Simulation I'm a compassionate physician specializing in cardiology, committed to improving patients' heart health and well-being.I'm a skilled nurse dedicated to patient care, ensuring comfort and supporting health with empathy and expertise.</p>
<p>bubble_sort(arr): n = len(arr) # Traverse through all array elements for i in range(n):# Last i elements are already in place for j in range(0, n-i-1): # Traverse the array from 0</p>
<p>For</p>
<p>three days, I've had a fever ranging from 100.5F to 102F, a rash on my limbs, joint pain and swelling, especially in my hands, episodes of diarrhea, abdominal pain, and nausea, leading to a loss of appetite.The patient's travel history and symptoms suggest dengue fever, a mosquito-transmitted illness.Finance Yesterday, the financial market underwent significant fluctuations.Equity markets initially slumped as investors responded to weak corporate earnings reports and concerns over global economic growth prospects.Bond yields declined as investors sought safer assets.However, sentiment improved in the afternoon following better-than-expected economic data from a major economy, which boosted investor confidence.As a result, stock prices recovered, and the market closed with modest gains.</p>
<p>Figure 2 :
2
Figure 2: The organization of related surveys on LLMs and LLM-based agents.</p>
<p>Figure 3 :
3
Figure 3: (a) Examples of the potential trials in the agent-environment interaction process.(b) Illustration of the memory reading, writing, and management processes, where dotted lines mean that the cross-trial information can be incorporated into the memory module.</p>
<p>Figure 4 :
4
Figure 4: An overview of the sources, forms, and operations of the memory in LLM-based agents.</p>
<p>Figure 5 :
5
Figure 5: An overview of the evaluation methods of the memory module.memory, most previous works use the text similarity and other auxiliary information for reading.For the forms of parametric memory, existing models may just utilize the updated parameters for inference, which can be seen as an implicit reading process.</p>
<p>Agent: According to my previous experiences of movie recommendations for Alice, she prefers to start watching movies at 9:00 PM, because she has finished work by then.I should ask Alice if she is available at that time.Environment (Alice): Of course, it is a nice time![Step 2] Agent: Now, I need to find some movies that Alice has marked "want to watch" on Netflix.In a narrow sense, the memory of the agent is only relevant to the historical information within the same trial.Formally, for a given task, the historical information of the trial before step t is  t = {a 1 , o 1 , a 2 , o 2 , ..., a t1 , o t1 }, and then the memory is derived based on  t .
For task (B), the trial is:[Step 1] Environment (Netflix): Alice's want to watch: Interstellar, Planet Terror and Candle in the Tomb.[Step 3] Agent: Since Alice never watches horror movies at night, Interstellar would be more suitablefor her. Now, I need to confirm with Alice about the movie. Environment (Alice): Great! I like it!3.2 Narrow Definition of the Agent MemoryIn the above toyexample, for task (A), the agent at [step 3] needs to arrange the visiting order for Alice; at this time,its memory contains the information about the selected attractions and arrival time in [step 1] and[step 2]. For task (B), the agent has to choose a movie for Alice at [step 3]; at this time, its memorycontains the arranged time to watch films.Step 1] Agent: I need to book flight tickets from Shanghai to Beijing on 5/1/2024, and fromBeijing to Shanghai on 5/3/2024 through the online ticket office. Environment (Online TicketOffice): The order has been confirmed. The departure flight takes off at 9:00 AM, and your ticketnumber is EK240501SX9021. The returning flight takes off at 4:30 PM, and your ticket number isLG240503FN6317. Please arrive at the airport on time for your flight.[Step 2] Agent: I have acquired external knowledge from the magazine Attractions in Beijing. Itshows that the Great Wall, the Forbidden City and the Summer Palace are famous. According toAlice's preferences, I think she prefers ancient architecture, so I determine the Forbidden City andthe Summer Palace as the destinations. Environment (Alice): Well done![Step 3] Agent: Now, I need to arrange the visiting order for Alice. According to the informationabove, I recommend visiting the Summer Palace after she arrives in Beijing because it is suitable tospend a whole afternoon there and watch the sunset. Then, she can go to the Forbidden City the nextday, which would give her plenty of time. Finally, she can return to Shanghai on the third day andtake a rest. Environment (Alice): I love this plan. Thanks!</p>
<p>Table 3 :
3
Summarization of the memory operations.If a model does not have special designs on the memory operations, we use  to label it, otherwise, it is denoted by .  means that the memory operations are not discussed in the paper.
ManagementModelsWritingMergingReflectionForgettingReading
For each task, there can be multiple trials for exploring the final solution, and all of them can be incorporated into the memory.
https://en.wikipedia.org/wiki/Cognitive_psychology
https://www.wikipedia.org
https://openweathermap.org
https://rapidapi.com/hub
AcknowledgementWe thank Lei Wang for his proofreading and valuable suggestions to this survey.
. Memory-enhanced Agent Applications. </p>
<p>Limitations &amp; Future Directions 8.1 More Advances in Parametric Memory. </p>
<p>LLM-based. </p>
<p>Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, Maosong Sun, arXiv:2307.07924Communicative agents for software development. 2023arXiv preprint</p>
<p>Chen Gao, Xiaochong Lan, Zhihong Lu, Jinzhu Mao, Jinghua Piao, Huandong Wang, Depeng Jin, Yong Li, arXiv:2307.14984Social-network simulation system with large language model-empowered agents. 20233arXiv preprint</p>
<p>Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, arXiv:2308.11432A survey on large language model based autonomous agents. 2023arXiv preprint</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Shunyu Karthik R Narasimhan, Yao, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, Yanlin Wang, arXiv:2305.10250Memorybank: Enhancing large language models with long-term memory. 2023arXiv preprint</p>
<p>Ret-llm: Towards a general read-write memory for large language models. Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, Hinrich Schtze, arXiv:2305.143222023arXiv preprint</p>
<p>Instruction tuning for large language models: A survey. Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, arXiv:2308.107922023arXiv preprint</p>
<p>Large language model alignment: A survey. Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong, Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong, arXiv:2309.150252023arXiv preprint</p>
<p>Aligning large language models with human: A survey. Yufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, Qun Liu, arXiv:2307.129662023arXiv preprint</p>
<p>Trustworthy llms: a survey and guideline for evaluating large language models. Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao, Yegor Cheng, Muhammad Klochkov, Hang Faaiz Taufiq, Li, arXiv:2308.053742023alignment. arXiv preprint</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Knowledge editing for large language models: A survey. Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, arXiv:2310.162182023arXiv preprint</p>
<p>Yunzhi Yao, Peng Wang, Bozhong Tian, Siyuan Cheng, Zhoubo Li, Shumin Deng, Huajun Chen, Ningyu Zhang, arXiv:2305.13172Editing large language models: Problems, methods, and opportunities. 2023arXiv preprint</p>
<p>Easyedit: An easy-to-use knowledge editing framework for large language models. Peng Wang, Ningyu Zhang, Xin Xie, Yunzhi Yao, Bozhong Tian, Mengru Wang, Zekun Xi, Siyuan Cheng, Kangwei Liu, Guozhou Zheng, arXiv:2308.072692023arXiv preprint</p>
<p>Zhangyin Feng, Weitao Ma, Weijiang Yu, Lei Huang, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.05876Trends in integration of knowledge and large language models: A survey and taxonomy of methods, benchmarks, and applications. 2023arXiv preprint</p>
<p>A comprehensive study of knowledge editing for large language models. Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, arXiv:2401.012862024arXiv preprint</p>
<p>Tool learning with foundation models. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, arXiv:2304.083542023arXiv preprint</p>
<p>Advancing transformer architecture in long-context large language models: A comprehensive survey. Yunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan Yang, Zhou Xin, Xiaoxing Ma, arXiv:2311.123512023arXiv preprint</p>
<p>Xindi Wang, Mahsa Salmani, Parsa Omidi, Xiangyu Ren, Mehdi Rezagholizadeh, Armaghan Eshaghi, arXiv:2402.02244Beyond the limits: A survey of techniques to extend the context length in large language models. 2024arXiv preprint</p>
<p>The what, why, and how of context length extension techniques in large language models-a detailed survey. Saurav Pawar, Tonmoy, Vinija Zaman, Aman Jain, Amitava Chadha, Das, arXiv:2401.078722024arXiv preprint</p>
<p>Multimodal large language models: A survey. Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, Philip Yu, 2023 IEEE International Conference on Big Data (BigData). IEEE2023</p>
<p>Shezheng Song, Xiaopeng Li, Shasha Li, arXiv:2311.07594How to bridge the gap between modalities: A comprehensive survey on multimodal large language model. 2023arXiv preprint</p>
<p>The (r) evolution of multimodal large language models: A survey. Davide Caffagni, Federico Cocchi, Luca Barsellotti, Nicholas Moratelli, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara, arXiv:2402.124512024arXiv preprint</p>
<p>Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong Chen, arXiv:2306.13549A survey on multimodal large language models. 2023arXiv preprint</p>
<p>Beyond efficiency: A systematic survey of resource-efficient large language models. Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, arXiv:2401.006252024arXiv preprint</p>
<p>Efficient large language models: A survey. Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, arXiv:2312.0386320231arXiv preprint</p>
<p>Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, Zhihao Jia, arXiv:2312.15234Towards efficient generative large language model serving: A survey from algorithms to systems. 2023arXiv preprint</p>
<p>Parameter-efficient fine-tuning methods for pretrained language models: A critical review and assessment. Lingling Xu, Haoran Xie, Si-Zhao Joe Qin, Xiaohui Tao, Fu Lee, Wang , arXiv:2312.121482023arXiv preprint</p>
<p>Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang, arXiv:2308.07633A survey on model compression for large language models. 2023arXiv preprint</p>
<p>A survey on model compression and acceleration for pretrained language models. Canwen Xu, Julian Mcauley, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Model compression and efficient inference for large language models: A survey. Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, Xiaofei He, arXiv:2402.097482024arXiv preprint</p>
<p>A comprehensive survey of compression algorithms for language models. Seungcheol Park, Jaehyeon Choi, Sojin Lee, Kang, arXiv:2401.153472024arXiv preprint</p>
<p>A survey on evaluation of large language models. Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, ACM Transactions on Intelligent Systems and Technology. 2023</p>
<p>Evaluating large language models: A comprehensive survey. Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, arXiv:2310.197362023arXiv preprint</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu, arXiv:2304.137122023arXiv preprint</p>
<p>Large language models for information retrieval: A survey. Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, Ji-Rong Wen, arXiv:2308.071072023arXiv preprint</p>
<p>Large language models for generative information extraction: A survey. Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Enhong Chen, arXiv:2312.176172023arXiv preprint</p>
<p>Large language models for software engineering: Survey and open problems. Angela Fan, Beliz Gokkaya, Mark Harman, Mitya Lyubarskiy, Shubho Sengupta, Shin Yoo, Jie M Zhang, arXiv:2310.035332023arXiv preprint</p>
<p>Software testing with large language models: Survey, landscape, and vision. Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, Qing Wang, IEEE Transactions on Software Engineering. 2024</p>
<p>Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen Zhang, Dewu Zheng, Mingxi Ye, Jiachi Chen, arXiv:2311.10372A survey of large language models for code: Evolution, benchmarking, and future trends. 2023arXiv preprint</p>
<p>Large language models for robotics: A survey. Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, Philip S Yu, arXiv:2311.072262023arXiv preprint</p>
<p>A survey on multimodal large language models for autonomous driving. Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer Vision2024</p>
<p>A survey of large language models for autonomous driving. Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan, arXiv:2311.010432023arXiv preprint</p>
<p>Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, Erik Cambria, arXiv:2310.05694A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. 2023arXiv preprint</p>
<p>A survey of large language models in medicine: Progress, application, and challenge. Hongjian Zhou, Boyang Gu, Xinyu Zou, Yiru Li, Peilin Sam S Chen, Junling Zhou, Yining Liu, Chengfeng Hua, Xian Mao, Wu, arXiv:2311.051122023arXiv preprint</p>
<p>Pre-trained language models in biomedical domain: A systematic survey. Benyou Wang, Qianqian Xie, Jiahuan Pei, Zhihong Chen, Prayag Tiwari, Zhao Li, Jie Fu, ACM Computing Surveys. 5632023</p>
<p>Large language models in finance: A survey. Yinheng Li, Shaofei Wang, Han Ding, Hang Chen, Proceedings of the Fourth ACM International Conference on AI in Finance. the Fourth ACM International Conference on AI in Finance2023</p>
<p>Towards a psychological generalist ai: A survey of current applications of large language models and future prospects. Tianyu He, Guanghui Fu, Yijing Yu, Fan Wang, Jianqiang Li, Qing Zhao, Changwei Song, Hongzhi Qi, Dan Luo, Huijing Zou, arXiv:2312.045782023arXiv preprint</p>
<p>Large language models for generative recommendation: A survey and visionary discussions. Lei Li, Yongfeng Zhang, Dugang Liu, Li Chen, arXiv:2309.011572023arXiv preprint</p>
<p>How can recommender systems benefit from large language models: A survey. Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, arXiv:2306.058172023arXiv preprint</p>
<p>Xiangnan He, and Tat-Seng Chua. Generative recommendation: Towards next-generation recommender paradigm. Wenjie Wang, Xinyu Lin, Fuli Feng, arXiv:2304.035162023arXiv preprint</p>
<p>Siren's song in the ai ocean: a survey on hallucination in large language models. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, arXiv:2309.012192023arXiv preprint</p>
<p>A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, arXiv:2311.052322023arXiv preprint</p>
<p>A survey of hallucination in large foundation models. Amit Vipula Rawte, Amitava Sheth, Das, arXiv:2309.059222023arXiv preprint</p>
<p>Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, Weiqiang Jia, arXiv:2309.06794Cognitive mirage: A review of hallucinations in large language models. 2023arXiv preprint</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, ACM Computing Surveys. 55122023</p>
<p>Sm Tonmoy, Vinija Zaman, Anku Jain, Rani, Aman Vipula Rawte, Amitava Chadha, Das, arXiv:2401.01313A comprehensive survey of hallucination mitigation techniques in large language models. 2024arXiv preprint</p>
<p>Xuhui Jiang, Yuxing Tian, Fengrui Hua, Chengjin Xu, Yuanzhuo Wang, Jian Guo, arXiv:2402.06647A survey on large language model hallucination via a creativity perspective. 2024arXiv preprint</p>
<p>Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, Nesreen K Ahmed, arXiv:2309.00770Bias and fairness in large language models: A survey. 2023arXiv preprint</p>
<p>Gender bias and stereotypes in large language models. Hadas Kotek, Rikker Dockum, David Sun, Proceedings of The ACM Collective Intelligence Conference. The ACM Collective Intelligence Conference2023</p>
<p>A survey on fairness in large language models. Yingji Li, Mengnan Du, Rui Song, Xin Wang, Ying Wang, arXiv:2308.101492023arXiv preprint</p>
<p>Explainability for large language models: A survey. Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Mengnan Du, ACM Transactions on Intelligent Systems and Technology. 2023</p>
<p>Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, Yue Zhang, arXiv:2312.02003A survey on large language model (llm) security and privacy: The good, the bad, and the ugly. 20231arXiv preprint</p>
<p>Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, Nael Abu-Ghazaleh, arXiv:2310.10844Survey of vulnerabilities in large language models revealed by adversarial attacks. 2023arXiv preprint</p>
<p>Seth Neel, Peter Chang, arXiv:2312.06717Privacy issues in large language models: A survey. 2023arXiv preprint</p>
<p>Identifying and mitigating privacy risks stemming from language models: A survey. Victoria Smith, Ali Shahin Shamsabadi, Carolyn Ashurst, Adrian Weller, arXiv:2310.014242023arXiv preprint</p>
<p>Attacks, defenses and evaluations for llm conversation safety: A survey. Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao, arXiv:2402.092832024arXiv preprint</p>
<p>Security and privacy challenges of large language models: A survey. Chandra Badhan, M Das, Yanzhao Hadi Amini, Wu, arXiv:2402.008882024arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>A survey on large language models: Applications, challenges, limitations, and practical usage. Rizwan Muhammad Usman Hadi, Abbas Qureshi, Muhammad Shah, Anas Irfan, Muhammad Zafar, Naveed Bilal Shaikh, Jia Akhtar, Seyedali Wu, Mirjalili, 2023Authorea Preprints</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran, Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, Dan Roth, ACM Computing Surveys. 5622023</p>
<p>Augmented language models: a survey. Grgoire Mialon, Roberto Dess, Maria Lomeli, Christoforos Nalmpantis, Ram Pasunuru, Roberta Raileanu, Timo Baptiste Rozire, Jane Schick, Asli Dwivedi-Yu, Celikyilmaz, arXiv:2302.078422023arXiv preprint</p>
<p>Understanding the planning of llm agents: A survey. Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen, arXiv:2402.027162024arXiv preprint</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, V Nitesh, Olaf Chawla, Xiangliang Wiest, Zhang, arXiv:2402.016802024arXiv preprint</p>
<p>Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, arXiv:2401.05459Personal llm agents: Insights and survey about the capability, efficiency and security. 2024arXiv preprint</p>
<p>An in-depth survey of large language model-based artificial intelligence agents. Pengyu Zhao, Zijian Jin, Ning Cheng, arXiv:2309.143652023arXiv preprint</p>
<p>Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, arXiv:2401.03428Exploring large language model based intelligent agents: Definitions, methods, and prospects. 2024arXiv preprint</p>
<p>Agent ai: Surveying the horizons of multimodal interaction. Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park, Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi, arXiv:2401.035682024arXiv preprint</p>
<p>Yingqiang Ge, Yujie Ren, Wenyue Hua, Shuyuan Xu, Juntao Tan, Yongfeng Zhang, arXiv:2312.03815Llm as os (llmao), agents as apps: Envisioning aios, agents and the aios-agent ecosystem. 2023arXiv preprint</p>
<p>Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, arXiv:2203.069042022arXiv preprint</p>
<p>Expel: Llm agents are experiential learners. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, Gao Huang, arXiv:2308.101442023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>. L Robert, Jerome Solso, Kagan, Cognitive psychology. Houghton Mifflin Harcourt P. 1979</p>
<p>Levels of processing: A framework for memory research. I M Fergus, Robert S Craik, Lockhart, Journal of verbal learning and verbal behavior. 1161972</p>
<p>Memory cultures: Memory, subjectivity and recognition. Selma Leydesdorff, 2017Routledge</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. Philip Nicholas, Johnson-Laird , 1983Harvard University Press</p>
<p>The Soar cognitive architecture. John E , Laird , 2019MIT press</p>
<p>Duality of the mind: A bottom-up approach toward cognition. Ron Sun, 2001Psychology Press</p>
<p>Reinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, 2018MIT press</p>
<p>Synapse: Trajectory-as-exemplar prompting with memory for computer control. Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An, NeurIPS 2023 Foundation Models for Decision Making Workshop. 2023</p>
<p>A reinforcement learning framework for relevance feedback. Ali Montazeralghaem, Hamed Zamani, James Allan, Proceedings of the 43rd international acm sigir conference on research and development in information retrieval. the 43rd international acm sigir conference on research and development in information retrieval2020</p>
<p>Ghost in the minecraft: Generally capable agents for open-world enviroments via large language models with text-based knowledge and memory. Xizhou Zhu, Yuntao Chen, Chenxin Hao Tian, Weijie Tao, Chenyu Su, Gao Yang, Bin Huang, Lewei Li, Xiaogang Lu, Wang, arXiv:2305.171442023arXiv preprint</p>
<p>Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu, arXiv:2308.08239Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. 2023arXiv preprint</p>
<p>Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, Ji-Rong Wen, When large language model based agent meets user behavior analysis: A novel user simulation paradigm. 2023</p>
<p>Chatdb: Augmenting llms with databases as their symbolic memory. Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao, arXiv:2306.039012023arXiv preprint</p>
<p>Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, Guannan Zhang, arXiv:2311.08719Think-in-memory: Recalling and post-thinking enable llms with long-term memory. 2023arXiv preprint</p>
<p>Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li, arXiv:2304.133432023arXiv preprint</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Charles Packer, Vivian Fang, G Shishir, Kevin Patil, Sarah Lin, Joseph E Wooders, Gonzalez, arXiv:2310.08560Memgpt: Towards llms as operating systems. 2023arXiv preprint</p>
<p>Prompted llms as chatbot modules for long open-domain conversation. Gibbeum Lee, Jongho Volker Hartmann, Dimitris Park, Kangwook Papailiopoulos, Lee, arXiv:2305.045332023arXiv preprint</p>
<p>Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, Yingzhen Yang, Recmind, arXiv:2308.14296Large language model powered agent for recommendation. 2023arXiv preprint</p>
<p>Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, arXiv:2308.02151Retrospective large language agents with policy gradient optimization. 2023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022arXiv preprint</p>
<p>Character-llm: A trainable agent for role-playing. Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, arXiv:2310.101582023arXiv preprint</p>
<p>Online adaptation of language models with a memory of amortized contexts. Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, Jonathan Richard Schwarz, arXiv:2403.043172024arXiv preprint</p>
<p>Huatuo: Tuning llama model with chinese medical knowledge. Haochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, Ting Liu, arXiv:2304.069752023arXiv preprint</p>
<p>Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie, arXiv:2308.16505Recommender ai agent: Integrating large language models for interactive recommendations. 2023arXiv preprint</p>
<p>Yuan Li, Yixuan Zhang, Lichao Sun, Metaagents, arXiv:2310.06500Simulating interactions of human behaviors for llm-based task-oriented coordination via collaborative generative agents. 2023arXiv preprint</p>
<p>Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Xingyu Zeng, Rui Zhao, arXiv:2308.03427Tptu: Task planning and tool usage of large language model-based ai agents. 2023arXiv preprint</p>
<p>Yilun Kong, Jingqing Ruan, Yihong Chen, Bin Zhang, Tianpeng Bao, Shiwei Shi, Guoqing Du, Xiaoru Hu, Hangyu Mao, Ziyue Li, arXiv:2311.11315Tptu-v2: Boosting task planning and tool usage of large language model-based agents in real-world systems. 2023arXiv preprint</p>
<p>Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka, Shing Yau, Zijuan Lin, Liyang Zhou, arXiv:2308.00352Meta programming for multi-agent collaborative framework. 2023arXiv preprint</p>
<p>Yi Yang, Yixuan Tang, Kar Yan, Tam , arXiv:2309.13064Investlm: A large language model for investment using financial domain instruction tuning. 2023arXiv preprint</p>
<p>Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, Zhi Jin, arXiv:2401.073392024arXiv preprint</p>
<p>Chatdoctor: A medical chat model fine-tuned on llama model using medical domain knowledge. Li Yunxiang, Li Zihan, Zhang Kai, Dan Ruilong, Zhang You, arXiv:2303.140702023arXiv preprint</p>
<p>How long can context length of open-source llms truly promise?. Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Memory sandbox: Transparent and interactive memory management for conversational agents. Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, Stephen Macneil, Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023</p>
<p>Giraffe: Adventures in expanding context lengths in llms. Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, Siddartha Naidu, arXiv:2308.108822023arXiv preprint</p>
<p>Focused transformer: Contrastive training for context scaling. Szymon Tworkowski, Konrad Staniszewski, Mikoaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Mio, 2023</p>
<p>Kevin Nelson F Liu, John Lin, Ashwin Hewitt, Michele Paranjape, Fabio Bevilacqua, Percy Petroni, Liang, arXiv:2307.03172Lost in the middle: How language models use long contexts. 2023arXiv preprint</p>
<p>The locality principle. J Peter, Denning, Communications of the ACM. 4872005</p>
<p>Memory: A contribution to experimental psychology. Hermann Ebbinghaus, trans. HA Ruger &amp; CE Bussenius. Teachers College. 1885</p>
<p>Replication and analysis of ebbinghaus' forgetting curve. M J Jaap, Joeri Murre, Dros, PloS one. 107e01206442015</p>
<p>Billion-scale similarity search with gpus. Jeff Johnson, Matthijs Douze, Herv Jgou, IEEE Transactions on Big Data. 732019</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dess, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothe Lachaux, Baptiste Lacroix, Naman Rozire, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, arXiv:2309.17452A tool-integrated reasoning agent for mathematical problem solving. 2023arXiv preprint</p>
<p>Doctorglm: Fine-tuning your chinese doctor is not a herculean task. Honglin Xiong, Sheng Wang, Yitao Zhu, Zihao Zhao, Yuxiao Liu, Qian Wang, Dinggang Shen, arXiv:2304.010972023arXiv preprint</p>
<p>Glm-130b: An open bilingual pre-trained model. Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.024142022arXiv preprint</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021arXiv preprint</p>
<p>Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, arXiv:2306.08666Radiology-gpt: A large language model for radiology. 2023arXiv preprint</p>
<p>Editing factual knowledge in language models. Nicola De Cao, Wilker Aziz, Ivan Titov, arXiv:2104.081642021arXiv preprint</p>
<p>. Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, Christopher D Manning, arXiv:2110.113092021arXiv preprintFast model editing at scale</p>
<p>Editing personality for large language models. Shengyu Mao, Ningyu Zhang, Xiaohan Wang, Mengru Wang, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, 2023</p>
<p>Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, Zhen-Hua Ling, arXiv:2401.17623Neighboring perturbations of knowledge editing on large language models. 2024arXiv preprint</p>
<p>Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen, arXiv:2403.14472Detoxifying large language models via knowledge editing. 2024arXiv preprint</p>
<p>Zeroscrolls: A zero-shot benchmark for long text understanding. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, Omer Levy, arXiv:2305.141962023arXiv preprint</p>
<p>Longbench: A bilingual, multitask benchmark for long context understanding. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, arXiv:2308.145082023arXiv preprint</p>
<p>How long can context length of open-source llms truly promise?. Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, Hao Zhang, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Alfworld: Aligning text and embodied environments for interactive learning. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Ct, Yonatan Bisk, Adam Trischler, Matthew Hausknecht, arXiv:2010.037682020arXiv preprint</p>
<p>Rtlfixer: Automatically fixing rtl syntax errors with large language models. Yunda Tsai, Mingjie Liu, Haoxing Ren, arXiv:2311.165432023arXiv preprint</p>
<p>Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, Haosheng Wang, arXiv:2308.09597Reviving anime character in reality via large language model. 2023arXiv preprint</p>
<p>Dake Chen, Hanbin Wang, Yunhao Huo, Yuzhao Li, Haoyang Zhang, arXiv:2310.08067Gamegpt: Multiagent collaborative framework for game development. 2023arXiv preprint</p>
<p>Zekun Moore, Wang , Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, arXiv:2310.00746Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. 2023arXiv preprint</p>
<p>Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, Lin Gui, arXiv:2310.01459Narrativeplay: Interactive narrative understanding. 2023arXiv preprint</p>
<p>Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, arXiv:2311.16832Customizing chinese conversational ai characters with large language models. 2023arXiv preprint</p>
<p>Zhao Kaiya, Michelangelo Naim, Jovana Kondic, Manuel Cortes, Jiaxin Ge, Shuying Luo, Guangyu Robert Yang, Andrew Ahn, arXiv:2310.02172Lyfe agents: Generative agents for low-cost real-time social interactions. 2023arXiv preprint</p>
<p>Agentcf: Collaborative learning with autonomous language agents for recommender systems. Junjie Zhang, Yupeng Hou, Ruobing Xie, Wenqi Sun, Julian Mcauley, Wayne Xin Zhao, Leyu Lin, Ji-Rong Wen, arXiv:2310.092332023arXiv preprint</p>
<p>War and peace (waragent): Large language model-based multi-agent simulation of world wars. Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang, arXiv:2311.172272023arXiv preprint</p>
<p>Knowledge-tuning large language models with structured medical knowledge bases for reliable response generation in chinese. Haochun Wang, Sendong Zhao, Zewen Qiang, Zijian Li, Nuwa Xi, Yanrui Du, Muzhen Cai, Haoqiang Guo, Yuhan Chen, Haoming Xu, arXiv:2309.041752023arXiv preprint</p>
<p>Ehragent: Code empowers large language models for complex tabular reasoning on electronic health records. Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda Zhu, Joyce Ho, Carl Yang, May D Wang, arXiv:2401.071282024arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023arXiv preprint</p>
<p>Tradinggpt: Multiagent system with layered memory and distinct characters for enhanced financial trading performance. Yang Li, Yangyang Yu, Haohang Li, Zhi Chen, Khaldoun Khashanah, arXiv:2309.037362023arXiv preprint</p>
<p>Quantagent: Seeking holy grail in trading by self-improving large language model. Saizhuo Wang, Hang Yuan, Lionel M Ni, Jian Guo, arXiv:2402.037552024arXiv preprint</p>
<p>Finmem: A performance-enhanced llm trading agent with layered memory and character design. Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W Suchow, Khaldoun Khashanah, 20232311arXiv e-prints</p>
<p>Learning to generate explainable stock predictions using self-reflective large language models. Kelvin Jl Koa, Yunshan Ma, Ritchie Ng, Tat-Seng Chua, arXiv:2402.036592024arXiv preprint</p>
<p>Qun Fang, Pheng Ann Heng, and Guangyong Chen. Chemist-x: Large language model-empowered agent for reaction condition recommendation in chemical synthesis. Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu, Lanqing Li, Jiezhong Qiu, Jianzhang Pan, Yi Huang, 2024</p>
<p>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, arXiv:2311.059972023arXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Dialogue foundation model for chemistry. 2024arXiv preprint</p>
<p>Ming Yan, Ruihao Li, Hao Zhang, Hao Wang, Zhilan Yang, Ji Yan, arXiv:2312.17653Larp: Language-agent role play for open-world games. 2023arXiv preprint</p>
<p>Matchat: A large language model and application service platform for materials science. Zi-Yi Chen, Fan-Kai Xie, Meng Wan, Yang Yuan, Miao Liu, Zong-Guo Wang, Sheng Meng, Yan-Gang Wang, Chinese Physics B. 32111181042023</p>
<p>Large language model-empowered agents for simulating macroeconomic activities. Nian Li, Chen Gao, Yong Li, Qingmin Liao, arXiv:2310.104362023arXiv preprint</p>
<p>Kwaiagents: Generalized information-seeking agent system with large language models. Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin, arXiv:2312.048892023arXiv preprint</p>
<p>Preliminary study on the construction of chinese medical knowledge graph. Odma Byambasuren, Yunfei Yang, Zhifang Sui, Damai Dai, Baobao Chang, Sujian Li, Hongying Zan, Journal of Chinese Information Processing. 33102019</p>
<p>Rcagent: Cloud root cause analysis by autonomous agents with tool-augmented large language models. Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Lunting Fan, Lingfei Wu, Qingsong Wen, arXiv:2310.163402023arXiv preprint</p>
<p>Zhangcheng Qiang, Weiqing Wang, Kerry Taylor, arXiv:2312.00326Agent-om: Leveraging large language models for ontology matching. 2023arXiv preprint</p>
<p>Dilu: A knowledge-driven approach to autonomous driving with large language models. Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao, arXiv:2309.162922023arXiv preprint</p>
<p>Zhitao Wang, Wei Wang, Zirao Li, Long Wang, Can Yi, Xinjie Xu, Luyang Cao, Hanjing Su, Shouzhi Chen, Jun Zhou, arXiv:2401.02705Xuat-copilot: Multi-agent collaborative system for automated user acceptance testing with large language model. 2024arXiv preprint</p>
<p>Scalable multirobot collaboration with large language models: Centralized or decentralized systems?. Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, Chuchu Fan, arXiv:2309.159432023arXiv preprint</p>
<p>Roco: Dialectic multi-robot collaboration with large language models. Zhao Mandi, Shreeya Jain, Shuran Song, arXiv:2307.047382023arXiv preprint</p>
<p>From text to tactic: Evaluating llms playing the game of avalon. Jonathan Light, Min Cai, Sheng Shen, Ziniu Hu, arXiv:2310.050362023arXiv preprint</p>
<p>Lifelong machine learning: a paradigm for continuous learning. Bing Liu, Frontiers of Computer Science. 112017</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. Rosa I Gati V Aher, Adam Arriaga, Kalai Tauman, International Conference on Machine Learning. PMLR2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5318 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5318</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5318</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-267523425</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.04470v5.pdf" target="_blank">Six Fallacies in Substituting Large Language Models for Human Participants</a></p>
                <p><strong>Paper Abstract:</strong> Can artificial-intelligence (AI) systems, such as large language models (LLMs), replace human participants in behavioral and psychological research? Here, I critically evaluate the replacement perspective and identify six interpretive fallacies that undermine its validity. These fallacies are (a) equating token prediction with human intelligence, (b) treating LLMs as the average human, (c) interpreting alignment as explanation, (d) anthropomorphizing AI systems, (e) essentializing identities, and (f) substituting model data for human evidence. Each fallacy represents a potential misunderstanding about what LLMs are and what they can tell researchers about human cognition. In the analysis, I distinguish levels of similarity between LLMs and humans, particularly functional equivalence (outputs) versus mechanistic equivalence (processes), while highlighting both technical limitations (addressable through engineering) and conceptual limitations (arising from fundamental differences between statistical and biological intelligence). For each fallacy, specific safeguards are provided to guide responsible research practices. Ultimately, the analysis supports conceptualizing LLMs as pragmatic simulation tools—useful for role-play, rapid hypothesis testing, and computational modeling (provided their outputs are validated against human data)—rather than as replacements for human participants. This framework enables researchers to leverage language models productively while respecting the fundamental differences between machine intelligence and human thought.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5318.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5318.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Analogical reasoning (Webb et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emergent analogical reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Report of LLM performance on analogical reasoning benchmarks showing emergent analogical capabilities in large autoregressive transformer models relative to human baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent analogical reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (various; reported across models in Webb et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer language models evaluated for emergent analogical reasoning; specific architectures/models vary across experiments in Webb et al. (e.g., GPT-family and comparable large LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Analogical reasoning tasks (emergent analogical benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Reasoning / analogical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Benchmarks that require mapping relations between items (analogy problems) to test the ability to infer abstract relational structure rather than surface similarity; typically administered as item-by-item analogy problems or puzzle-like prompt completions.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to exhibit strong/emergent analogical reasoning ability, with levels of performance that improve with model scale and that in some cases rival or exceed human-level performance on selected analogical benchmarks (qualitative claim in main text).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not specified quantitatively in this paper's text; referenced claim is that LLM performance on these tasks compares favorably to human performance in the cited work (Webb et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Cited as LLMs performing at or above human levels on certain analogical reasoning benchmarks in Webb et al.; no statistical details reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper emphasizes caution: apparent analogical competence may reflect dataset/task-specific patterns or surface cues exploited by LLMs; underlying mechanisms may differ from human analogical reasoning (functional alignment vs mechanistic equivalence).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Six Fallacies in Substituting Large Language Models for Human Participants', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5318.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5318.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Probabilistic reasoning (Linda/Bat-and-ball) (Yax et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Studying and improving reasoning in humans and machines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Work comparing LLMs and humans on classic probabilistic reasoning problems (e.g., Linda/conjunction, bat-and-ball/cognitive reflection) showing LLMs can outperform humans under some conditions and respond differently to prompts/manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Studying and improving reasoning in humans and machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (including GPT-family in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large instruction-tuned language models evaluated on probabilistic and cognitive-reflection style tasks; performance affected by prompting strategies (e.g., chain-of-thought).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Linda problem / Bill problems and bat-and-ball problem (cognitive reflection / probabilistic reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Reasoning / probabilistic reasoning / cognitive reflection</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Classic cognitive tasks that probe probabilistic judgment (conjunction fallacy, representativeness) and intuitive vs reflective responses (bat-and-ball); typically presented as short vignettes or arithmetic puzzles requiring a correct normative response.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported in the paper as LLMs (and specifically GPT-4 in the sentence that groups results) outperforming humans on some probabilistic reasoning instances; performance also sensitive to prompting (e.g., chain-of-thought prompts can improve LLM accuracy). No numeric accuracies provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not reported numerically here; implicitly the typical human error-prone baseline on these problems (classic literature documents substantial human fallibility), but no precise values provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative claim: LLMs can outperform humans on selected probabilistic reasoning tasks; however, sensitivity to prompt wording and explanatory differences suggest different underlying processes.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>LLM improvements may be driven by surface prompt cues or chain-of-thought prompting (e.g., 'let's think step by step'), and such gains do not necessarily indicate human-like reasoning; prompt sensitivity and lack of mechanistic homology are highlighted as limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Six Fallacies in Substituting Large Language Models for Human Participants', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5318.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5318.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Irony / Indirect request detection</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mention that GPT-4 (reported in cited literature) demonstrates superior detection/interpretation of irony and recognition of indirect requests compared with humans on assessed samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI GPT-4 (instruction-tuned large multimodal-capable transformer); described in text as outperforming humans on pragmatic tasks in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Irony detection and indirect request recognition</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Pragmatics / social language comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Tasks that require interpreting nonliteral language (irony, sarcasm) and detecting implied/indirect requests from conversational context; typically administered as vignettes or conversational snippets asking for interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported in the text as GPT-4 outperforming humans on these pragmatic tasks in the cited literature; no quantitative scores provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not provided numerically in the paper; only qualitatively referenced as being lower than GPT-4 in the cited results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitatively described as GPT-4 outperforming humans on detecting and interpreting irony and indirect requests; the paper cautions that such alignment may reflect surface pattern matching rather than shared mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The review cautions that LLMs' superior performance may be brittle to prompt phrasing and not indicative of genuine pragmatic competence grounded in embodied social experience.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Six Fallacies in Substituting Large Language Models for Human Participants', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5318.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5318.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory of mind / False belief & faux pas</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Reported mixed results: LLMs can reach ceiling on some false-belief-style tasks but underperform humans on nuanced social-pragmatic tasks such as faux pas detection, and are often highly sensitive to prompt/wording changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (e.g., GPT-family referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned large language models tested on classic theory-of-mind paradigms (false belief) and social-pragmatic faux pas tests; performance depends on formulation and prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>False belief tests and faux pas tests (theory of mind / social cognition)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Social cognition / theory of mind</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>False belief tasks test understanding that another agent can hold beliefs different from reality; faux pas tests assess detection of socially inappropriate remarks/actions that may offend or embarrass others - typically vignette-based and require explanation/interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported as achieving ceiling performance on some false-belief tasks but underperforming humans on faux pas tests; model answers are often fragile to small changes in wording (prompt sensitivity).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Characterized qualitatively: humans perform well on false-belief tests and typically outperform models on nuanced faux pas detection; numerical baselines not provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Mixed: functional parity in simple false-belief questions but worse-than-human performance on more nuanced social-pragmatic tasks; prompt sensitivity indicates different underlying processing.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>High sensitivity to subtle rewordings, and breakdowns on tasks requiring contextualized social inference, suggest LLM success can be syntactic/pattern-driven rather than reflecting human-like mental-state reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Six Fallacies in Substituting Large Language Models for Human Participants', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5318.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5318.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bargaining / cooperation games (Mozikov et al., 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The good, the bad, and the Hulk-like GPT: Analyzing emotional decisions of large language models in cooperation and bargaining games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis of LLM (GPT-4 referenced) behavior in economic games showing LLMs maintained more consistent/rational decision-making compared with humans when emotions were induced.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The good, the bad, and the Hulk-like GPT: Analyzing emotional decisions of large language models in cooperation and bargaining games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned GPT-4 evaluated in iterated cooperation and bargaining paradigms to examine decision patterns under emotional manipulations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Cooperation and bargaining games (economic decision-making paradigms)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Decision making / social decision-making / affective influence on choice</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Experimental game paradigms (e.g., bargaining, iterated cooperation) where participants make choices that can be influenced by induced emotions; typically measure cooperation rates, offers, acceptances, and responses to emotional context.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Described as maintaining consistent, rational decision-making across induced-emotion conditions (less susceptible to emotion-driven deviations), with GPT-4 producing behaviorally stable strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans are described as showing emotion-induced deviations in bargaining and cooperation games (e.g., altered offers or cooperativeness under emotional manipulations); no numeric baselines provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs (GPT-4) were more consistent/rational compared with humans under emotional manipulations — better in normative rationality but qualitatively different (lacking humans' emotion-driven variability).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Differences reflect absence of emotion/subjectivity in LLMs; thus, superior 'rationality' is not evidence of human-equivalent decision psychology but of different underlying motivations (none) and optimization objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Six Fallacies in Substituting Large Language Models for Human Participants', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5318.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5318.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sensorimotor feature representation (Xu et al., 2025)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Study comparing LLM and human conceptual representations across domains, showing LLMs capture non-sensorimotor aspects of concepts but systematically fail to recover sensorimotor features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Text-only LLMs (unspecified in summary)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Text-trained language models evaluated for their internal representations of concept features (sensorimotor vs non-sensorimotor) using representational alignment analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Concept feature recovery across sensorimotor vs non-sensorimotor domains</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Conceptual representation / semantic memory / grounded cognition</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Analyses that assess whether models' representations encode features people report for concepts (e.g., visual, motor, affective features) and compare alignment across feature types.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>LLMs recovered non-sensorimotor features (e.g., functional, abstract, affective associations) relatively well but showed markedly reduced alignment for sensorimotor features (e.g., motor affordances), per cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans naturally encode both sensorimotor and non-sensorimotor features in conceptual representations; quantitative human benchmarks are reported in the cited work but not reproduced here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>LLMs align with humans on non-sensorimotor conceptual features but underperform markedly on sensorimotor features, indicating a representational gap tied to lack of embodied grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Findings highlight grounding limitations of text-only models; lack of multidimensional sensory experience likely causes systematic deficits in sensorimotor knowledge representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Six Fallacies in Substituting Large Language Models for Human Participants', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5318.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5318.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emotional understanding / emotional intelligence (Wang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emotional intelligence of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of LLMs on emotional understanding tasks, with reports that models can perform like humans on some emotional-understanding measures, but such alignment may be superficial (expressive/syntactic rather than experiential).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional intelligence of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (evaluated in Wang et al., 2023; unspecified models in review)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Language models evaluated on tasks measuring emotion perception, labeling, or reasoning about emotions; models' 'understanding' derives from learned word associations rather than subjective feeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Tests of emotional understanding / emotional intelligence tasks</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Affective cognition / emotion perception & reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Tasks assessing ability to recognize, label, reason about, or manage emotions in scenarios; commonly vignette-based or questionnaire-style measures adapted for LLM evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported to sometimes match human-level scores on certain emotional-understanding tests in the cited work, but the paper stresses that model competence is expressive/syntactic rather than phenomenological.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Not numerically reported here; human performance varies by task and is the comparator used in the cited literature.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Superficial alignment is possible (similar scores/patterns), but the review emphasizes this does not imply shared experiential or mechanistic processes.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Model 'empathy' or 'emotional understanding' is constructed from text patterns and lacks subjective experience; this undermines construct validity if one assumes equivalence with human emotional cognition.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Six Fallacies in Substituting Large Language Models for Human Participants', 'publication_date_yy_mm': '2025-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Emergent analogical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Studying and improving reasoning in humans and machines <em>(Rating: 2)</em></li>
                <li>The good, the bad, and the Hulk-like GPT: Analyzing emotional decisions of large language models in cooperation and bargaining games <em>(Rating: 2)</em></li>
                <li>Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts <em>(Rating: 2)</em></li>
                <li>Emotional intelligence of large language models <em>(Rating: 2)</em></li>
                <li>Using cognitive psychology to understand GPT-3 <em>(Rating: 2)</em></li>
                <li>Language model finetuning on scaled survey data for predicting distributions of public opinions <em>(Rating: 1)</em></li>
                <li>How likely do LLMs with CoT mimic human reasoning? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5318",
    "paper_id": "paper-267523425",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "Analogical reasoning (Webb et al., 2023)",
            "name_full": "Emergent analogical reasoning in large language models",
            "brief_description": "Report of LLM performance on analogical reasoning benchmarks showing emergent analogical capabilities in large autoregressive transformer models relative to human baselines.",
            "citation_title": "Emergent analogical reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs (various; reported across models in Webb et al., 2023)",
            "model_description": "Large autoregressive transformer language models evaluated for emergent analogical reasoning; specific architectures/models vary across experiments in Webb et al. (e.g., GPT-family and comparable large LMs).",
            "model_size": null,
            "cognitive_test_name": "Analogical reasoning tasks (emergent analogical benchmarks)",
            "cognitive_test_type": "Reasoning / analogical reasoning",
            "cognitive_test_description": "Benchmarks that require mapping relations between items (analogy problems) to test the ability to infer abstract relational structure rather than surface similarity; typically administered as item-by-item analogy problems or puzzle-like prompt completions.",
            "llm_performance": "Reported to exhibit strong/emergent analogical reasoning ability, with levels of performance that improve with model scale and that in some cases rival or exceed human-level performance on selected analogical benchmarks (qualitative claim in main text).",
            "human_baseline_performance": "Not specified quantitatively in this paper's text; referenced claim is that LLM performance on these tasks compares favorably to human performance in the cited work (Webb et al., 2023).",
            "performance_comparison": "Cited as LLMs performing at or above human levels on certain analogical reasoning benchmarks in Webb et al.; no statistical details reported here.",
            "notable_differences_or_limitations": "The paper emphasizes caution: apparent analogical competence may reflect dataset/task-specific patterns or surface cues exploited by LLMs; underlying mechanisms may differ from human analogical reasoning (functional alignment vs mechanistic equivalence).",
            "uuid": "e5318.0",
            "source_info": {
                "paper_title": "Six Fallacies in Substituting Large Language Models for Human Participants",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Probabilistic reasoning (Linda/Bat-and-ball) (Yax et al., 2024)",
            "name_full": "Studying and improving reasoning in humans and machines",
            "brief_description": "Work comparing LLMs and humans on classic probabilistic reasoning problems (e.g., Linda/conjunction, bat-and-ball/cognitive reflection) showing LLMs can outperform humans under some conditions and respond differently to prompts/manipulations.",
            "citation_title": "Studying and improving reasoning in humans and machines",
            "mention_or_use": "mention",
            "model_name": "LLMs (including GPT-family in cited work)",
            "model_description": "Large instruction-tuned language models evaluated on probabilistic and cognitive-reflection style tasks; performance affected by prompting strategies (e.g., chain-of-thought).",
            "model_size": null,
            "cognitive_test_name": "Linda problem / Bill problems and bat-and-ball problem (cognitive reflection / probabilistic reasoning tasks)",
            "cognitive_test_type": "Reasoning / probabilistic reasoning / cognitive reflection",
            "cognitive_test_description": "Classic cognitive tasks that probe probabilistic judgment (conjunction fallacy, representativeness) and intuitive vs reflective responses (bat-and-ball); typically presented as short vignettes or arithmetic puzzles requiring a correct normative response.",
            "llm_performance": "Reported in the paper as LLMs (and specifically GPT-4 in the sentence that groups results) outperforming humans on some probabilistic reasoning instances; performance also sensitive to prompting (e.g., chain-of-thought prompts can improve LLM accuracy). No numeric accuracies provided in this review.",
            "human_baseline_performance": "Not reported numerically here; implicitly the typical human error-prone baseline on these problems (classic literature documents substantial human fallibility), but no precise values provided in the paper.",
            "performance_comparison": "Qualitative claim: LLMs can outperform humans on selected probabilistic reasoning tasks; however, sensitivity to prompt wording and explanatory differences suggest different underlying processes.",
            "notable_differences_or_limitations": "LLM improvements may be driven by surface prompt cues or chain-of-thought prompting (e.g., 'let's think step by step'), and such gains do not necessarily indicate human-like reasoning; prompt sensitivity and lack of mechanistic homology are highlighted as limitations.",
            "uuid": "e5318.1",
            "source_info": {
                "paper_title": "Six Fallacies in Substituting Large Language Models for Human Participants",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Irony / Indirect request detection",
            "name_full": "",
            "brief_description": "Mention that GPT-4 (reported in cited literature) demonstrates superior detection/interpretation of irony and recognition of indirect requests compared with humans on assessed samples.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "OpenAI GPT-4 (instruction-tuned large multimodal-capable transformer); described in text as outperforming humans on pragmatic tasks in cited work.",
            "model_size": null,
            "cognitive_test_name": "Irony detection and indirect request recognition",
            "cognitive_test_type": "Pragmatics / social language comprehension",
            "cognitive_test_description": "Tasks that require interpreting nonliteral language (irony, sarcasm) and detecting implied/indirect requests from conversational context; typically administered as vignettes or conversational snippets asking for interpretation.",
            "llm_performance": "Reported in the text as GPT-4 outperforming humans on these pragmatic tasks in the cited literature; no quantitative scores provided here.",
            "human_baseline_performance": "Not provided numerically in the paper; only qualitatively referenced as being lower than GPT-4 in the cited results.",
            "performance_comparison": "Qualitatively described as GPT-4 outperforming humans on detecting and interpreting irony and indirect requests; the paper cautions that such alignment may reflect surface pattern matching rather than shared mechanisms.",
            "notable_differences_or_limitations": "The review cautions that LLMs' superior performance may be brittle to prompt phrasing and not indicative of genuine pragmatic competence grounded in embodied social experience.",
            "uuid": "e5318.2",
            "source_info": {
                "paper_title": "Six Fallacies in Substituting Large Language Models for Human Participants",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Theory of mind / False belief & faux pas",
            "name_full": "",
            "brief_description": "Reported mixed results: LLMs can reach ceiling on some false-belief-style tasks but underperform humans on nuanced social-pragmatic tasks such as faux pas detection, and are often highly sensitive to prompt/wording changes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "LLMs (e.g., GPT-family referenced)",
            "model_description": "Instruction-tuned large language models tested on classic theory-of-mind paradigms (false belief) and social-pragmatic faux pas tests; performance depends on formulation and prompt sensitivity.",
            "model_size": null,
            "cognitive_test_name": "False belief tests and faux pas tests (theory of mind / social cognition)",
            "cognitive_test_type": "Social cognition / theory of mind",
            "cognitive_test_description": "False belief tasks test understanding that another agent can hold beliefs different from reality; faux pas tests assess detection of socially inappropriate remarks/actions that may offend or embarrass others - typically vignette-based and require explanation/interpretation.",
            "llm_performance": "Reported as achieving ceiling performance on some false-belief tasks but underperforming humans on faux pas tests; model answers are often fragile to small changes in wording (prompt sensitivity).",
            "human_baseline_performance": "Characterized qualitatively: humans perform well on false-belief tests and typically outperform models on nuanced faux pas detection; numerical baselines not provided in this review.",
            "performance_comparison": "Mixed: functional parity in simple false-belief questions but worse-than-human performance on more nuanced social-pragmatic tasks; prompt sensitivity indicates different underlying processing.",
            "notable_differences_or_limitations": "High sensitivity to subtle rewordings, and breakdowns on tasks requiring contextualized social inference, suggest LLM success can be syntactic/pattern-driven rather than reflecting human-like mental-state reasoning.",
            "uuid": "e5318.3",
            "source_info": {
                "paper_title": "Six Fallacies in Substituting Large Language Models for Human Participants",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Bargaining / cooperation games (Mozikov et al., 2024)",
            "name_full": "The good, the bad, and the Hulk-like GPT: Analyzing emotional decisions of large language models in cooperation and bargaining games",
            "brief_description": "Empirical analysis of LLM (GPT-4 referenced) behavior in economic games showing LLMs maintained more consistent/rational decision-making compared with humans when emotions were induced.",
            "citation_title": "The good, the bad, and the Hulk-like GPT: Analyzing emotional decisions of large language models in cooperation and bargaining games",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Instruction-tuned GPT-4 evaluated in iterated cooperation and bargaining paradigms to examine decision patterns under emotional manipulations.",
            "model_size": null,
            "cognitive_test_name": "Cooperation and bargaining games (economic decision-making paradigms)",
            "cognitive_test_type": "Decision making / social decision-making / affective influence on choice",
            "cognitive_test_description": "Experimental game paradigms (e.g., bargaining, iterated cooperation) where participants make choices that can be influenced by induced emotions; typically measure cooperation rates, offers, acceptances, and responses to emotional context.",
            "llm_performance": "Described as maintaining consistent, rational decision-making across induced-emotion conditions (less susceptible to emotion-driven deviations), with GPT-4 producing behaviorally stable strategies.",
            "human_baseline_performance": "Humans are described as showing emotion-induced deviations in bargaining and cooperation games (e.g., altered offers or cooperativeness under emotional manipulations); no numeric baselines provided here.",
            "performance_comparison": "LLMs (GPT-4) were more consistent/rational compared with humans under emotional manipulations — better in normative rationality but qualitatively different (lacking humans' emotion-driven variability).",
            "notable_differences_or_limitations": "Differences reflect absence of emotion/subjectivity in LLMs; thus, superior 'rationality' is not evidence of human-equivalent decision psychology but of different underlying motivations (none) and optimization objectives.",
            "uuid": "e5318.4",
            "source_info": {
                "paper_title": "Six Fallacies in Substituting Large Language Models for Human Participants",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Sensorimotor feature representation (Xu et al., 2025)",
            "name_full": "Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts",
            "brief_description": "Study comparing LLM and human conceptual representations across domains, showing LLMs capture non-sensorimotor aspects of concepts but systematically fail to recover sensorimotor features.",
            "citation_title": "Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts",
            "mention_or_use": "mention",
            "model_name": "Text-only LLMs (unspecified in summary)",
            "model_description": "Text-trained language models evaluated for their internal representations of concept features (sensorimotor vs non-sensorimotor) using representational alignment analyses.",
            "model_size": null,
            "cognitive_test_name": "Concept feature recovery across sensorimotor vs non-sensorimotor domains",
            "cognitive_test_type": "Conceptual representation / semantic memory / grounded cognition",
            "cognitive_test_description": "Analyses that assess whether models' representations encode features people report for concepts (e.g., visual, motor, affective features) and compare alignment across feature types.",
            "llm_performance": "LLMs recovered non-sensorimotor features (e.g., functional, abstract, affective associations) relatively well but showed markedly reduced alignment for sensorimotor features (e.g., motor affordances), per cited study.",
            "human_baseline_performance": "Humans naturally encode both sensorimotor and non-sensorimotor features in conceptual representations; quantitative human benchmarks are reported in the cited work but not reproduced here.",
            "performance_comparison": "LLMs align with humans on non-sensorimotor conceptual features but underperform markedly on sensorimotor features, indicating a representational gap tied to lack of embodied grounding.",
            "notable_differences_or_limitations": "Findings highlight grounding limitations of text-only models; lack of multidimensional sensory experience likely causes systematic deficits in sensorimotor knowledge representation.",
            "uuid": "e5318.5",
            "source_info": {
                "paper_title": "Six Fallacies in Substituting Large Language Models for Human Participants",
                "publication_date_yy_mm": "2025-07"
            }
        },
        {
            "name_short": "Emotional understanding / emotional intelligence (Wang et al., 2023)",
            "name_full": "Emotional intelligence of large language models",
            "brief_description": "Evaluation of LLMs on emotional understanding tasks, with reports that models can perform like humans on some emotional-understanding measures, but such alignment may be superficial (expressive/syntactic rather than experiential).",
            "citation_title": "Emotional intelligence of large language models",
            "mention_or_use": "mention",
            "model_name": "LLMs (evaluated in Wang et al., 2023; unspecified models in review)",
            "model_description": "Language models evaluated on tasks measuring emotion perception, labeling, or reasoning about emotions; models' 'understanding' derives from learned word associations rather than subjective feeling.",
            "model_size": null,
            "cognitive_test_name": "Tests of emotional understanding / emotional intelligence tasks",
            "cognitive_test_type": "Affective cognition / emotion perception & reasoning",
            "cognitive_test_description": "Tasks assessing ability to recognize, label, reason about, or manage emotions in scenarios; commonly vignette-based or questionnaire-style measures adapted for LLM evaluation.",
            "llm_performance": "Reported to sometimes match human-level scores on certain emotional-understanding tests in the cited work, but the paper stresses that model competence is expressive/syntactic rather than phenomenological.",
            "human_baseline_performance": "Not numerically reported here; human performance varies by task and is the comparator used in the cited literature.",
            "performance_comparison": "Superficial alignment is possible (similar scores/patterns), but the review emphasizes this does not imply shared experiential or mechanistic processes.",
            "notable_differences_or_limitations": "Model 'empathy' or 'emotional understanding' is constructed from text patterns and lacks subjective experience; this undermines construct validity if one assumes equivalence with human emotional cognition.",
            "uuid": "e5318.6",
            "source_info": {
                "paper_title": "Six Fallacies in Substituting Large Language Models for Human Participants",
                "publication_date_yy_mm": "2025-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Emergent analogical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "emergent_analogical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Studying and improving reasoning in humans and machines",
            "rating": 2,
            "sanitized_title": "studying_and_improving_reasoning_in_humans_and_machines"
        },
        {
            "paper_title": "The good, the bad, and the Hulk-like GPT: Analyzing emotional decisions of large language models in cooperation and bargaining games",
            "rating": 2,
            "sanitized_title": "the_good_the_bad_and_the_hulklike_gpt_analyzing_emotional_decisions_of_large_language_models_in_cooperation_and_bargaining_games"
        },
        {
            "paper_title": "Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts",
            "rating": 2,
            "sanitized_title": "large_language_models_without_grounding_recover_nonsensorimotor_but_not_sensorimotor_features_of_human_concepts"
        },
        {
            "paper_title": "Emotional intelligence of large language models",
            "rating": 2,
            "sanitized_title": "emotional_intelligence_of_large_language_models"
        },
        {
            "paper_title": "Using cognitive psychology to understand GPT-3",
            "rating": 2,
            "sanitized_title": "using_cognitive_psychology_to_understand_gpt3"
        },
        {
            "paper_title": "Language model finetuning on scaled survey data for predicting distributions of public opinions",
            "rating": 1,
            "sanitized_title": "language_model_finetuning_on_scaled_survey_data_for_predicting_distributions_of_public_opinions"
        },
        {
            "paper_title": "How likely do LLMs with CoT mimic human reasoning?",
            "rating": 1,
            "sanitized_title": "how_likely_do_llms_with_cot_mimic_human_reasoning"
        }
    ],
    "cost": 0.018026,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Six Fallacies in Substituting Large Language Models for Human Participants</p>
<p>Zhicheng Lin zhichenglin@gmail.com 
Department of Psychology
Yonsei University</p>
<p>Gati Aher 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Michael Bernstein 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Danica Dillion 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Nancy Fulda 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Nicholas Laskowski 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Paweł Niszczota 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Philipp Schoenegger 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Lindia Tjuatja 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Lukasz Walasek 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>David Wingate 
Department of Psychology
Department of Psychology
University of Science and Technology of China Correspondence Zhicheng Lin
Yonsei University
03722SeoulRepublic of Korea</p>
<p>Six Fallacies in Substituting Large Language Models for Human Participants
B9B540E0B0B6791123B3DBE513C68EDCgenerative AI (GenAI)large language models (LLMs)AI participants (AI subjects)silicon samplingsimulation or modeling (modelling)research validity
Can AI systems like large language models (LLMs) replace human participants in behavioral and psychological research?Here I critically evaluate the "replacement" perspective and identify six interpretive fallacies that undermine its validity.These fallacies are: (1) equating token prediction with human intelligence, (2) treating LLMs as the average human, (3) interpreting alignment as explanation, (4) anthropomorphizing AI systems, (5) essentializing identities, and (6) substituting model data for human evidence.Each fallacy represents a potential misunderstanding about what LLMs are and what they can tell us about human cognition.The analysis distinguishes levels of similarity between LLMs and humans, particularly functional equivalence (outputs) versus mechanistic equivalence (processes), while highlighting both technical limitations (addressable through engineering) and conceptual limitations (arising from fundamental differences between statistical and biological intelligence).For each fallacy, specific safeguards are provided to guide responsible research practices.Ultimately, the analysis supports conceptualizing LLMs as pragmatic simulation tools-useful for role-play, rapid hypothesis testing, and computational modeling (provided their outputs are validated against human data)-rather than as replacements for human participants.This framework enables researchers to leverage language models productively while respecting the fundamental differences between machine intelligence and human thought.</p>
<p>validity in distinct ways: Some compromise our ability to make causal inferences, others limit generalizability across populations, and still others reflect misalignment between what we intend to measure and what we actually capture.The methodological and theoretical safeguards provided for each fallacy address these threats by strengthening both research design and interpretive frameworks.</p>
<p>Taken together, these analyses support the function of LLMs as neural language simulators.Rather than stand-ins for people, they reveal latent psychological and cognitive dynamics embedded in the text-dynamics rooted in the communication of human thoughts, attitudes, and behaviors.This approach offers both conceptual clarity and practical guidance for leveraging LLMs appropriately in behavioral and cognitive research.</p>
<p>Box 1 | Language Models and the Language User Illusion</p>
<p>Modern AI chatbots like ChatGPT are built on large language models (LLMs).Unlike traditional rule-based systems, LLMs process text by converting it into tokens (units of text) and mapping these to numerical vectors in a high-dimensional space (embedding).This mathematical representation enables models to capture semantic relationships between concepts-such as "king" relating to "queen" similarly to how "man" relates to "woman."The breakthrough transformer architecture (Vaswani et al., 2017) revolutionized these models through attention mechanisms that allow words to gather contextual information from surrounding text.This helps LLMs resolve ambiguities in language and handle complex linguistic tasks that once seemed to require human intelligence.</p>
<p>Base models (also known as "foundation" or "pretrained" models) are typically trained through autoregression-predicting each subsequent token based on preceding onesenabling them to compress vast amounts of patterns and textual knowledge into their network weights.However, consumer-facing instruct models like ChatGPT undergo further post-training.This often involves supervised learning (fine-tuning on specific instruction-response examples) and reinforcement learning (RL; learning from feedback signals, like rewards or penalties, to improve outputs).One prominent RL technique is reinforcement learning from human feedback (RLHF), which aligns model outputs with human preferences.Beyond alignment, RL methods are increasingly used to enhance specific cognitive-like capabilities, including reasoning and tool use (e.g., interacting with web browsers or code interpreters), often through trial-and-error processes where the models learn to generate more effective or logically sound sequences of thought or action.Beyond model weights, the behavior of instruct models also depends heavily on system prompts (extensive human-authored instructions that guide model behavior), user prompts (Lin, 2024), and inference-time techniques (e.g., methodologies like retrieval-augmented generation [RAG]).This sophisticated combination contributes to the illusion that the model possesses humanlike understanding (Mitchell &amp; Krakauer, 2023).It allows LLMs to model language in ways that go beyond traditional distributional semantics (Lake &amp; Murphy, 2023).They capture nuanced, contextualized meanings and structures in language (Mahowald et al., 2024;Manning et al., 2020;Millière, 2024), including visual knowledge embedded in text (Jones et al., 2022;Lewis et al., 2019) and abstract concepts like space and time (Gurnee &amp; Tegmark, 2023).When LLMs produce responses that feel empathetic or insightful, users often project human-like qualities onto them-what can be termed the "language user illusion."The fluent language generated by LLMs creates a compelling illusion of interacting with another mind, but this overlooks their fundamentally different nature from human language users.This distinction forms the critical foundation for the fallacies examined throughout this paper.</p>
<p>Interpretive Fallacies in LLM-Based Research</p>
<p>To understand why LLMs function as simulators rather than replacements, we must first examine the conceptual errors that underlie the replacement perspective.Six fallacies emerge in discussions of LLMs as human substitutes, each revealing a different way researchers might misinterpret what LLMs are and what their outputs signify (Table 1).</p>
<p>Token Prediction as Human Intelligence Fallacy</p>
<p>LLMs accomplish tasks once reserved for humans through a layered process: They are initially pretrained to predict sequential tokens; then post-trained via fine-tuning and reinforcement learning; and finally, their reasoning behavior is shaped by guidance from well-crafted prompts and inference-time techniques (Box 1).This process equips LLMs with intricate linguistic knowledge (e.g., syntactic rules, semantic relations)-often termed formal linguistic competence (Mahowald et al., 2024).Crucially, these models do more than memorize patterns; they demonstrate the ability to navigate complex, context-dependent tasks in both language processing and generation (Mahowald et al., 2024;Millière &amp; Buckner, 2024a).Through learning and contextual adaptation, LLMs can infer underlying task structures and generate appropriate responses across varied domains-a form of instrumental knowledge that supports problem-solving in diverse settings (Yildirim &amp; Paul, 2024).Yet, at their core, LLMs have no minds (Searle, 1980) but are autoregressive statistical models that manipulate language-a task different from human cognition, producing a kind of ungrounded intelligence.They manipulate language by leveraging statistical patterns learned during training; while these patterns encode aspects of derivative meaning present in text-corpus relationships (e.g., "Paris"-"France," "fire"-"hot"), the models themselves are yet to access this meaning through direct, grounded experience (e.g., visual qualia, proprioception).This mirrors Church encoding in lambda calculus, where data and operations are expressed purely through abstract functions, defined by how they transform inputs to outputs, rather than by inherent, grounded meaning (Church, 1936).This is in contrast to human intelligence, which integrates general and specialized capabilities through evolutionary adaptations and developmental learning (Box 2)-and is "grounded in one's embodied physical and emotional experiences" and "deeply reliant on one's social and cultural environments" (Mitchell, 2024).</p>
<p>A fallacy emerges when we mistake the capacity of LLMs to predict language patterns and solve cognitive problems for genuine human-like understanding or intelligence.To unpack this fallacy, it is helpful to distinguish between two levels of understanding: functional understanding (reliably producing appropriate outputs, as when a dog successfully catches a ball) and reflective understanding (consciously grasping underlying principles, as when humans can articulate why the ball follows a particular trajectory).While LLMs may demonstrate considerable functional understanding by generating contextually appropriate responses, they currently lack the reflective understanding that emerges from embodied experience and consciousness.Consequently, mistaking their output for genuine cognition would misrepresent what is being measured; the model's linguistic performance, based on statistical token prediction, risks being conflated with human cognitive processes that integrate both functional and reflective understanding.The "intelligence" of LLMs is thus operationally different from the human cognition it might appear to simulate.</p>
<p>This fallacy manifests in two major ways.First, while LLMs process vast amounts of text reflecting real-world information and human experience, their tokens operate without direct, first-hand sensorimotor contact with the world.Consequently, they lack embodied referents and lived experience derived from such interaction (Leivada et al., 2023).This critical absence of grounding is demonstrated by how text-based LLMs, while capable of representing non-sensorimotor features of human concepts (e.g., emotional valence), systematically fail to capture sensorimotor features, particularly those related to motor actions.Indeed, the alignment between LLM and human conceptual representations diminishes markedly from non-sensorimotor to sensorimotor domains (Xu et al., 2025).</p>
<p>Second, this absence of first-person, sensorimotor grounding limits the models' connection to the physical and social realities they aim to simulate.This hinders the development of fully functional linguistic competence-the use of language to achieve goals in the world (Mahowald et al., 2024).It also restricts their capacity to acquire the kind of worldly knowledge needed for robust world models-internal representations that are structure-preserving and behaviorally efficacious in real-world interactions (Webb et al., 2023;Yildirim &amp; Paul, 2024).Such models, including cognitive maps, body schemas, and spatial schemas, support embodied reasoning and action, which remain beyond the scope of current models (Wicke &amp; Wachowiak, 2024).</p>
<p>A critical consequence of this fallacy is that when researchers attribute changes in model outputs to experimental manipulations without recognizing the underlying statistical nature, they inevitably risk misinterpretation.Consider a simple prompt: "Mao Zedong was…" Unlike interacting with another mind, when we engage with a chatbot, we are not seeking its opinion-despite the compelling illusion thereof-but rather making a computational request: Given the statistical distributions in the language model, what sequence is most likely to follow these words?Models trained for neutrality will likely provide correspondingly factual responses.Indeed, the character of such responses is shaped by the model's training, geopolitical alignment, and prompt language.For example, DeepSeek-R1-pretrained mostly on English and Chinese text, and subsequently aligned with government regulatory directives (Box 1)-showed substantially higher proportions of Chinese-state propaganda and anti-U.S. bias compared with ChatGPT o3-mini-high, which was pretrained primarily on English text.This bias was most pronounced with Simplified Chinese queries, diminished with Traditional Chinese inputs, and was nearly absent when queried in English (Huang et al., 2025).</p>
<p>Fundamentally then, the model, unlike a human, has no communicative intent, no opinion of, attitude toward, or belief about Mao, and no intrinsic capacity to tell the truth-it just models a distribution of token sequences based on the training texts (Shanahan, 2024).While they manifest proficient language use that goes beyond simply retrieving pre-recorded text strings, they are not true language users in the philosophical sense.They do not possess intrinsic meaning, communicative intentions, or other internal states essential to human language users (Block, 1981).</p>
<p>Indeed, model outputs can be highly sensitive to seemingly trivial prompt variations-variations that human language users would tolerate (Ivanova, 2025).Small changes in prompt language, wording, order, or context can produce drastically different outputs not because of the manipulated variable of interest, but due to the model's inherent sensitivity patterns-a phenomenon that contrasts with how humans process language.</p>
<p>Additionally, findings from LLM studies cannot be reliably generalized to human cognition.The gap between token prediction mechanisms and human cognitive processes fundamentally limits what we can infer about real human populations from model behavior, even when performance patterns may appear superficially similar.As philosopher John Searle illustrated in his "Chinese Room" thought experiment, a system might process symbols according to rules without understanding their meaning-just as a person following instructions to manipulate Chinese characters could produce appropriate responses without comprehending Chinese.When models generate coherent language without the intentionality, consciousness, and direct grounding in real-world experience that characterize human intelligence (Searle, 1980), they are capturing linguistic form (the observable structure of language) rather than true meaning (the communicative intent behind language) (Bender &amp; Koller, 2020).Therefore, to confuse cognitive algorithms for cognition, or models of the mind for the mind itself, represents a category mistake, both ontologically (misidentifying their nature) and epistemologically (misunderstanding their knowledge).</p>
<p>No doubt LLMs (and AI in general) will continue to advance.But, as detailed in Box 3, even these improvements face fundamental conceptual limitations regarding embodied experience and grounding.Knowledge, as constructivists like Jean Piaget have argued, is built from sensory experiences and perceptions, which are then layered with symbols and categories over time.Without such experiences, LLMs are like an artificial version of the proverbial Mary studying everything about color in a black-and-white room her entire life (Jackson, 1982).One can argue that when Mary leaves her room and sees color for the first time, she learns something new-what it is like to see something pink (Jackson, 1986).If so, a great deal more is at stake for language models, which experience neither color nor anything.Indeed, unlike Mary, who can use her other experiences as a scaffold for understanding color-much like Helen Keller using associations from senses like touch and smell to construct a color scheme despite being blind and deaf ("Pink makes me think of a baby's cheek, or a gentle southern breeze")-LLMs have none of this kind of sensory scaffolding.</p>
<p>Essential aspects of human cognition-emotions, intuition, and other subjective experiences-so far remain incomputable and therefore cannot be fully replicated by computational algorithms.Even if everything about human cognition were theoretically computable, actually capturing the full depth and complexity of human thought processes would likely exceed computational feasibility.The interplay of perception, memory, emotions, and social contexts creates patterns so intricate that complete replication would require exponentially increasing computational resources.In other words, creating AI systems that genuinely replicate human cognition-systems behaving like humans under all circumstances-may be fundamentally intractable from a computational perspective (Van Rooij et al., 2024).AI systems-constrained by their current algorithms, data, and embedded assumptions-face inherent challenges in replicating human cognition.This limitation parallels Gödel's famous discovery in mathematics: Any formal system sophisticated enough for basic arithmetic will contain true statements that cannot be proven within that system (Fokas, 2023).Similarly, AI systems face intrinsic constraints in capturing all aspects of human thought within their computational frameworks.</p>
<p>To mitigate these issues, consider the following.1. Model selection and settings: Different model architectures may capture different aspects of linguistic behavior (Dettki et al., 2025).Choose models based on their specific capabilities for the research question rather than assuming larger or more advanced models automatically approximate human cognition better (Zan et al., 2025;Zhou et al., 2024).Compare base models with fine-tuned versions to understand how optimization objectives affect outputs relative to human responses (Binz &amp; Schulz, 2023a;Yax et al., 2024).Evaluate the impact of model parameters such as temperature (Li &amp; Qi, 2025).2. Prompt design: Test model sensitivity to prompt variations to distinguish between robust patterns and artifacts of specific phrasings (Brucks &amp; Toubia, 2025).This helps identify when models are exhibiting systematic "reasoning" versus merely responding to surface-level cues in ways humans would not (Ivanova, 2025).</p>
<p>Interpretations and applications:</p>
<p>Validate LLM outputs against human data, particularly when studying cognitive processes, to establish appropriate boundaries for generalizing from LLM experiments to human psychology.Make explicit that LLM performance, however impressive, stems from statistical prediction rather than human-like understanding (Shiffrin &amp; Mitchell, 2023).4. Ethics: Acknowledge model limitations in research reporting, particularly the inherent distinction between token prediction and human cognition, to prevent misinterpretation and reduce the risk of anthropomorphizing LLMs in scientific literature (Ibrahim &amp; Cheng, 2025).</p>
<p>Box 2 | Biological versus Machine Optimization</p>
<p>Natural selection introduces a different optimization mechanism from machine training, which directly impacts how we should interpret intelligence in both systems.Biological pressures for survival and reproduction embed implicit assumptions and constraints within neural systems-termed ecological inductive biases (Richards et al., 2019).These biases enable humans to learn efficiently from physical and social interactions, manage limited cognitive resources by prioritizing selective inputs and tasks, and adapt to uncertainty using shortcuts and flexibility.Such evolved constraints stand in contrast to the engineered bias of language models, which arise from their objective function (e.g., next-token prediction, reward maximization), design (e.g., embeddings, transformer architecture), and training data composition.Other computational models similarly have their distinctive inductive biases; for example, convolutional neural networks (CNNs) learn patterns by focusing on local spatial relationships, as their convolutional filters operate over small image regions.</p>
<p>A crucial distinction is that biological evolution carries historical baggage that constrains optimization in ways that shape cognitive capabilities.When environments suddenly change, biological systems cannot rebuild from scratch but must adapt using existing neural machinery, often creating suboptimal yet functional solutions.This evolutionary path dependency is further constrained by the DNA bottleneck, limiting intergenerational information transfer: All adaptive changes must be encoded in the genome, which can only store a fraction of an organism's lifetime learning.In contrast, AI systems face no such constraints.They can be redesigned from the ground up with each iteration, and their "knowledge" can be transferred directly via perfect copying, distillation into smaller models, or selective merging of capabilities-all without the genetic limitations that restrict biological information transfer.</p>
<p>These disparities between language models and humans in their optimization pressures manifest in both internal architectures and external behaviors (Doerig et al., 2023).Key architectural differences include:</p>
<p>• Computation mechanisms: Models rely on digital computation and continuous activation functions, contrasting with the analog computation and all-or-none neuronal firing characteristic of the human brain; • Information flow: Models predominantly use feed-forward processing, whereas the human brain exhibits extensive feedback modulation across interconnected regions; and • Learning algorithms: Models depend on backpropagation-an algorithm that adjusts network weights based on output errors-for learning, which is biologically implausible due to its reliance on explicit error signals, symmetric weight updates during forward and backward passes, global error propagation throughout the entire network, and discrete, stepwise updates after processing batches of data.</p>
<p>These architectural differences produce observable behavioral distinctions between LLMs and human cognition:</p>
<p>• Data efficiency: LLMs exhibit low data efficiency, as evidenced by sublinear scaling laws-while increasing data and model size leads to improved performance, achieving incremental improvements requires exponentially more resources; • Energy consumption: LLMs display low energy efficiency, consuming megawatts of power compared to the ~20 watts used by the human brain; • Memory stability: Models show susceptibility to catastrophic forgetting, where learning new tasks causes the model to forget previously learned tasks; • Learning approach: Current LLMs lack active exploration and continual learning-they do not retain prior interactions or instructions across sessionsunlike humans, who learn cumulatively over time; and Conceptual limitations, by contrast, reflect intrinsic discrepancies between model design and human cognition that define the boundaries of the current paradigm.These include:</p>
<p>(1) grounding-LLMs lack embodied experience that situates cognition in physical and social contexts, operating solely through abstract symbol manipulation;</p>
<p>(2) agency-LLMs have no intrinsic goals, desires, or motivations driving behavior, functioning instead as sophisticated prediction systems; and (3) subjectivity-LLMs lack conscious experience that introduces qualitative dimensions to human cognition, including emotional valence and phenomenal awareness.These limitations constrain the construct validity of LLMs as psychological models in ways not readily addressed through incremental engineering.</p>
<p>Even rich observational learning from video may not equate to the embodied knowledge humans acquire through direct, multisensory interaction.Without lived experience, models might still struggle to fully replicate crucial aspects of human intelligence, such as deeply grounded physical intuition, nuanced social common sense, and certain forms of mathematical reasoning (S. Lee et al., 2024;McCoy et al., 2024;Mirzadeh et al., 2024), even when they mimic or surpass human performance in other ways.For example, larger models improve in certain domains (e.g., some challenging pattern recognition tasks) but still fail at seemingly simple tasks that humans would expect them to handle easily (Zhou et al., 2024).The depth of understanding gained without interactive, multisensory embodiment may differ from human cognition (Jones &amp; Bergen, 2024), resembling, to some extent, the difference between inferring object properties from descriptions or observations versus learning through direct manipulation and sensory feedback, where crucial experiential nuances can be missed (cf.Kim et al., 2019, regarding visual properties and congenital blindness).</p>
<p>Paradoxically, solutions to these conceptual limitations may render future AI systems less analogous to human cognition even while they gain superhuman abilities.For instance, addressing grounding limitations through embodied robotics with RL would introduce fundamentally different learning processes from both current text-based training and human development-such as active, continuous learning from direct experience via grounded rewards from the world using non-human tools, rather than relying on human input.Similarly, systems designed to overcome reasoning limitations might develop internal representations and goals with no human analogue.For example, as relentless problem-solving agents, these systems may exhibit a distribution of competence that differs from that of an average person.</p>
<p>As the field progresses beyond current paradigms, the relationship between machine and human cognition will continue to evolve, requiring ongoing reassessment of the role of LLMs in psychological science.</p>
<p>The Average Human Fallacy</p>
<p>Mischaracterizing the nature of intelligence in LLMs may not matter as much if they behave or perform like the average human, enabling them to functionally replace human participants.But this assumption commits the average human fallacy, which misrepresents what LLM outputs signify.</p>
<p>Consider the engineering purpose and approach of LLMs.They are explicitly developed to outperform humans, as measured by a broad range of benchmarks and standardized tests-an engineering feat bolstered by continually improved design and algorithms, and access to more data and compute, while free from biological limitations (see also Box 2).In actual tests, for example, GPT-4 outperforms humans in detecting and interpreting irony, recognizing indirect requests or hints in conversation (Strachan et al., 2024), analogical reasoning tasks (Webb et al., 2023), and probabilistic reasoning tasks like the Linda/Bill problems and the bat-and-ball problem (Yax et al., 2024), but underperforms humans in tasks like faux pas tests (Strachan et al., 2024).This contrasts with the assumption of the replacement view that LLM responses mirror average human judgments from the training data (Dillion et al., 2023) or the majority's mainstream opinions (Qu et al., 2024).</p>
<p>The generalizability challenge is multifaceted.First, LLMs are not representative samples of any defined human population, let alone an "average" human.Their training data (mostly online text) exhibit systematic biases-predominantly Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations, especially those who are hegemonic, young, and publicly expressive (Crockett &amp; Messeri, 2023;Santurkar et al., 2023;Tao et al., 2024).This biased training creates a complex landscape for psychological simulation: While LLMs might adequately capture behaviors that are largely universal across human cultures (certain basic cognitive or emotional phenomena, perhaps), they face limitations when simulating those known to vary across cultures and groups-from number representations to personality traits and moral reasoning.Further complicating this picture is our incomplete understanding of which psychological phenomena are universal versus culturally variable.This uncertainty means researchers must exercise particular caution when using WEIRD-biased LLMs-or any LLMs for that matter-to simulate potentially culturedependent phenomena.</p>
<p>In tandem with spatial biases, the temporal distribution of the data is more concentrated in recent history, with the model's understanding of the past filtered through the lens of contemporary languages and norms (Ziems et al., 2024).This presentist and recency bias risks temporal flattening, wherein historical and contemporary voices are homogenized, obscuring the richness of historical diversity in human thought.This is problematic for probing thoughts and behaviors that have evolved over extended periods-such as the way people think about concepts like gender, race, and class (Kozlowski &amp; Evans, 2024).The models may lack the contextual richness and temporal granularity needed to understand how these concepts were discussed in different periods, potentially reinforcing contemporary biases when trying to understand people from earlier time periods.</p>
<p>Beyond training data biases, the engineering goals of LLMs further compromise their representational accuracy.The goal of developing advanced LLMs-to provide accurate, helpful answers to users-contrasts with the study of human cognition, which is replete with inaccuracies, biases, shortcuts, and idiosyncrasies.For instance, in language comprehension, people often settle for a partial and sometimes inaccurate understanding that is nevertheless sufficient for the task at hand-"good-enough representations" (Ferreira et al., 2002).In decision making, when emotions are induced in bargaining games and repeated cooperation games, GPT-4 tends to maintain consistent, rational decision-making, in contrast to humans (Mozikov et al., 2024).</p>
<p>Due to model limitations, opacity, and the common use of RLHF, such models cannot be assumed to represent the "average" of their training data.Responses may reflect hallucination, the influence of RLHF, or other bias reduction efforts; they could also simply regurgitate specific instances, examples, or strategies from their training data, particularly when the query is well represented in the data (Aher et al., 2023;Binz &amp; Schulz, 2023a;Shiffrin &amp; Mitchell, 2023).For example, when LLM responses are altered through RLHF, they can deviate from the original training data-and may even exacerbate their misalignments with non-dominant views (Santurkar et al., 2023).In this process, the communicative intent of developers (and data labelers) shapes model outputs to prioritize goals like accuracy and helpfulness, rather than reflecting "raw" human-like responses from the training texts.</p>
<p>This raises concerns about whether the outputs genuinely represent human responses.If the psychological construct being measured is meant to be "average human response," the LLM fails to operationalize this construct due to the representational biases described above.The models often become "too neutral, detached, and nonjudgmental," lacking selfhood and initiative (Ye et al., 2024), and exhibiting homogeneous personality profiles-high in agreeableness and low in neuroticism (Li &amp; Qi, 2025;Pellert et al., 2024).While this finetuning process can reduce certain biases (Hu et al., 2025), enhance accuracy, and make the model more pleasant to interact with, it also weakens its ability to reflect the actual attitudes and thoughts present in human texts (Harding et al., 2024) and to produce diverse responses (Murthy et al., 2024).Such alterations may also introduce new preferences or biases from the feedback, including sycophancy (a tendency to generate outputs that are excessively userpleasing or conform to perceived desirable responses), making it challenging to rely on RLHF-tuned LLMs as accurate indicators of human thought and judgment (Park et al., 2024).In survey responses, for example, LLMs do not share human response biases, with more pronounced discrepancies in RLHF-tuned models (Tjuatja et al., 2024).Likewise, within the GPT-3 family, fine-tuned models exhibited higher propensity for conjunction fallacy and intuitive reasoning relative to base models (Yax et al., 2024).</p>
<p>The practical implications of these misalignments are evident in actual response patterns.LLM responses have been found to mischaracterize marginalized groups, as evidenced by out-group imitation rather than in-group description (Wang et al., 2025), and misrepresent sampled groups, as demonstrated by upward bias in mean ratings of the Big Five personality traits (Niszczota et al., 2025), as well as in other surveys and tests (Hagendorff et al., 2023;Sarstedt et al., 2024;Tjuatja et al., 2024).In addition to these shifts in average responses, LLMs also fail to capture the nuances and heterogeneity of human responses, producing flattened, oversimplified portrayals of various groups (Wang et al., 2025).</p>
<p>While diversifying the training corpus to include more languages and cultural contexts helps to broaden representations, achieving global representations is ultimately a long-term challenge (Lin &amp; Li, 2023).In the foreseeable future, the quantity and quality of available non-English training texts will remain impoverished compared to English as the lingua franca.</p>
<p>To mitigate the average human fallacy, key considerations include explicitly defining represented populations, potentially using specialized models, testing prompts and models for consistency and bias, validating responses against human data, and transparently reporting representational limitations.Specifically:</p>
<ol>
<li>Model selection and customization: Be explicit about which populations are represented in the training data and how this affects generalizability claims.Consider using specialized models fine-tuned on representative human data and, importantly, verify performance with actual samples from those populations (Gao et al., 2024;Suh et al., 2025).2. Prompt design: Test models across multiple prompting styles to assess the consistency of simulated responses and determine sensitivity to superficial variations (Li &amp; Qi, 2025;Momennejad et al., 2023;Tao et al., 2024).Compare outputs from different LLM versions and architectures to identify systematic biases in representation.</li>
</ol>
<p>Interpretations and applications:</p>
<p>Validate LLM responses against human samples, particularly for claims about specific populations or demographic groups, and report cases where LLM responses diverge from human patterns.Avoid generalizing beyond domains where empirical validation has been established (Abdurahman et al., 2025).4. Ethics: Document potential biases and acknowledge limitations in representativeness when reporting results from LLM simulations (Abdurahman et al., 2025).</p>
<p>Alignment as Explanation Fallacy</p>
<p>High output alignment between LLM responses and human data has been suggested as evidence that LLMs can sometimes replace human participants (Dillion et al., 2023).While such alignment suggests that LLMs might capture something mechanistic about human behavior, it is fallacious to assume mechanistic equivalence-that the model explains human cognition (Guest &amp; Martin, 2023) or can replace human participants for theoretical understanding (see Box 4).Treating such alignment as sufficient for mechanistic equivalence constitutes the alignment-as-explanation fallacy.This fallacy arises when output similarity is conflated with representational and processing similarity-that is, when similar outputs are taken as evidence that LLMs engage the same psychological constructs or cognitive mechanisms as humans, overlooking the differences between the statistical pattern-matching of current LLMs and the embodied, contextual cognitive processes of humans.</p>
<p>Indeed, a fundamental problem of applying human-centric tests and concepts to LLMs-such as theory of mind or emotional understanding-is the assumption that they engage with information in ways similar to humans, presupposing background psychological mechanisms that may be absent or irrelevant for LLMs (Box 1).These anthropomorphic assumptions undermine the construct validity (i.e., the degree to which a test measures the specific psychological construct it purports to measure) of psychological tests in LLMs (Millière &amp; Buckner, 2024b).For example, response confidence in LLMs, as measured by token probability, may differ from human self-reports.Similarly, emotional intelligence-the ability to perceive, understand, manage, and use emotions in oneself and othersencompasses self-awareness, empathy, emotional regulation, and social skills, all rooted in subjective experiences that are absent in LLMs.Model "understanding" of emotions is purely expressive and syntactic, driven by learned associations between words and concepts rather than experiential, visceral insight or an internalized understanding of mental states.So, when LLMs perform like humans on tests of emotional understanding-in either overall score or response pattern (Wang et al., 2023)-this alignment is at the surface level, rather than reflecting genuine equivalence.</p>
<p>Apparent alignments between LLM and human responses may also be artifacts of specific task formulations rather than evidence of robust, human-like reasoning capabilities.One clue for a lack of true competence is a dissociation between accuracy and the model's explanations of its responses-namely, reasoning (Leivada et al., 2023).Another critical test is prompt sensitivity, examining how performance is affected by superficial alterations to the prompts-changes to which humans typically show little to no sensitivity or respond to differently.Such prompt sensitivity has been documented in LLMs in survey responses-RLHF-tuned models can be highly sensitive to changes like typos (Tjuatja et al., 2024)-and in tasks like reasoning (Binz &amp; Schulz, 2023a;Dasgupta et al., 2022;Yax et al., 2024), decision making (Binz &amp; Schulz, 2023b;Suri et al., 2024), theory of mind (Strachan et al., 2024), moral judgments (Oh &amp; Demberg, 2025), and more (Kamoi et al., 2024;McCoy et al., 2024).</p>
<p>For example, LLMs but not humans improved their performance in reasoning tasks when the instruction included the phrase "let's think step by step" (Yax et al., 2024), a form of chain-of-thought prompting.Conversely, while LLMs performed at ceiling in a false belief test just like human participants, they struggled when small changes were made to the formulation of false belief scenarios-suggesting syntactic pattern processing rather than robust reasoning (Strachan et al., 2024).These sensitivities to minor variations highlight how apparent alignment can mask differences in underlying processes, creating a threat to internal validity-the ability to draw firm conclusions about causal relationships-when the observed performance is misattributed to the intended manipulation rather than to superficial linguistic patterns.</p>
<p>But even when performance reflects underlying abilities, it is not clear whether models employ mechanisms similar to those of humans.Given that different systems can achieve the same outcome through different mechanisms-known as multiple realizability (Bowers et al., 2023;Guest &amp; Martin, 2023), as in telling time with digital versus mechanical clocks-it is unwarranted to assume mechanistic or even functional equivalence (Box 4).Indeed, it is notoriously difficult to understand exactly what LLMs have learned.Beyond inherent differences between machine and biological intelligence in their architecture and algorithms, LLMs are trained on datasets much larger than what human learners experience.Differences in mechanisms may manifest as distinct response characteristics, including (1) context sensitivity, such as prompt sensitivity or performance variations across different vignettes (Yax et al., 2024); (2) response patterns, such as variability in open-ended responses (Li et al., 2024), item-by-item performance variability (Wang et al., 2023), and correlation of accuracy with confidence (Yax et al., 2024); and (3) error types and consistency, such as errors arising from cognitive demands versus those from item wording or familiarity (Yax et al., 2024).</p>
<p>This raises a fundamental question: When LLMs perform at human levels in psychological tasks, what does it tell us about the capacity of these models?Performance can reflect true competence-some underlying abilities-or something more superficial, like pattern memorization (Gao et al., 2024) or other surface-level cues or pure chance.Conversely, underperformance can reflect something other than incompetence, such as processing limitations or ineffective prompting (Firestone, 2020).Thus, to establish true competence, model outputs should be sensitive to changes in the task-relevant inputs but insensitive to irrelevant changes (Harding &amp; Sharadin, in press).</p>
<p>To mitigate the alignment-as-explanation fallacy, safeguards include testing alignment robustness against perturbations, examining reasoning processes, using causal interventions, performing cross-domain validation, and explicitly acknowledging the limits of inferring shared mechanisms.Specifically:</p>
<ol>
<li>Model testing and validation: Test whether the alignment between LLM and human responses is robust to perturbations in task structure, prompt wording, and contextual variations that shouldn't affect performance (Oh &amp; Demberg, 2025).If the performance varies significantly with superficial changes, this suggests a lack of true construct equivalence (Firestone, 2020).2. Process tracing: Use methods such as chain-of-thought prompting to examine the reasoning paths that LLMs use to arrive at answers, comparing these with human reasoning protocols (Bao et al., 2024) (Zan et al., 2025).</li>
</ol>
<p>The same applies to validation across different languages of the prompt, such as English and Chinese (Jin et al., 2024). 5. Explicit limitation acknowledgment: When reporting alignments between LLM and human responses, acknowledge the limitations of inferring shared mechanisms, noting their distinct architecture and learning history (Abdurahman et al., 2025).</p>
<p>Box 4 | Functional versus Mechanistic Equivalence</p>
<p>Functional equivalence occurs when two systems produce the same outputs given the same inputs under specified conditions-like how both a digital watch and a sundial tell accurate time on a clear day, despite different mechanisms.Mechanistic equivalence, in contrast, requires shared internal causal processes-addressing how results are produced, not merely that they are produced.Because distinct mechanisms can yield identical functions (multiple realizability), functional alignment alone cannot establish mechanistic alignment.</p>
<p>In psychological research, an LLM that correctly answers false belief questions demonstrates functional alignment with humans, but evidence from prompt sensitivity studies reveals mechanistic divergence.Even "perfect" behavioral correlation therefore licenses, at most, a claim of functional parity.Demonstrating mechanistic parity would demand convergent evidence from: (1) input perturbation tests-performance should degrade in the same pattern as humans when task-relevant cues are modified; (2) internalstate homology-representational similarity analyses should reveal shared information coding; and (3) causal intervention-disabling a model component should mirror the effect of corresponding cognitive impairments in humans (Firestone, 2020;Guest &amp; Martin, 2023).</p>
<p>Differentiating functional and mechanistic equivalence has implications for three levels of LLM-human similarity:</p>
<p>• Level 1: Pragmatic simulation.If the research goal is purely functional-pretesting vignettes, forecasting average responses, relationship counseling-then output equivalence between LLMs and humans may suffice regardless of underlying mechanisms.Here, the simulation perspective is interested primarily in what is produced, not how it is produced.For example, if an LLM reliably predicts which survey items show ceiling effects, its utility as a rapid prototyping tool remains valid even if its internal processes differ from human cognition.• Level 2: Explanatory inference.If the research goal is theoretical-testing models of cognition, emotion, or moral reasoning-then superficial behavioral alignment becomes insufficient.Internal and construct validity demand evidence that similar underlying mechanisms are at play.An LLM might generate humanlike responses to reasoning tasks without engaging in human-like reasoning-just as a calculator produces accurate math answers through processes entirely unlike human calculation.• Level 3: Phenomenological attribution.Even if perfect functional and mechanistic overlap were demonstrated, this would not automatically confer phenomenal states-subjective experiences like visceral, emotional empathy.Claims that LLMs "have beliefs," "feel empathy," or "experience understanding" involve a third level of equivalence that requires additional, presently unknown properties (e.g., global workspace dynamics, biological embodiment, or other factors that might give rise to consciousness).Consequently, attributions of subjective experience to LLMs remain largely speculative until such criteria are articulated and empirically evaluated.</p>
<p>Thus, LLMs can serve as valuable functional simulators for many research purposes without needing to process and experience the world as humans do.However, researchers must be transparent about which level of equivalence they are claiming and provide appropriate evidence for that level.The danger lies not in using LLMs as Level 1 simulators, but in making unwarranted leaps to Level 2 theoretical inferences or Level 3 phenomenological attributions-shortcuts that threaten validity.</p>
<p>Anthropomorphism Fallacy</p>
<p>Beyond misalignments due to algorithms, purposes, and implementations, another issue with the replacement view is that it leads to anthropomorphism of LLMs (Crockett &amp; Messeri, 2023;Shiffrin &amp; Mitchell, 2023).Their fluent, human-seeming responses can trigger an enhanced ELIZA effect: the tendency to attribute human-like understanding to systems that merely manipulate symbols, creating a compelling "language user" illusion (Box 1).Indeed, when researchers refer to "the minds of language models" or "the machine minds of LLMs" (Dillion et al., 2023), such terminology-whether used metaphorically or literally-can inadvertently encourage teleological bias, attributing purposes and goals to LLMs.</p>
<p>The anthropomorphism fallacy misinterprets LLM responses, treating statistical artifacts as expressions of an inner mental life.When researchers ascribe human-like mental states-such as beliefs, intentions, or consciousness-to LLMs based on their linguistic output, they risk invalidating measures of psychological constructs.Indeed, many such constructs, including attitudes and emotions, presuppose a mental architecture that current LLMs lack.</p>
<p>In many situations, using anthropomorphic language in conversation is natural-even useful.However, with AI like LLMs-which currently lack documented markers of consciousness yet produce remarkably coherent conversations (Shardlow &amp; Przybyla, 2024)-this semblance seduces users to interpret model behavior through the lens of folk psychology, attributing "beliefs" or "consciousness" to these (Colombatto &amp; Fleming, 2024).Indeed, the chatbot service provider Character AI invites users to meet AIs that "feel alive."</p>
<p>While the philosophical debate about machine consciousness remains open (Butlin et al., 2023), ascribing emotions or intentions to current LLMs (saying they "believe" or "think") risks creating impressions that outpace their demonstrated capabilities.As of mid-2025, no publicly available LLM exhibits clear markers of phenomenal awareness or intentional agency.Such attribution gaps can lead researchers and the public to either overestimate or underestimate these systems' capabilities (Crockett &amp; Messeri, 2023;Shanahan, 2024), potentially shaping AI policy in ways disconnected from technological reality (Lin, 2025).Perhaps most concerning, this conflation may dilute our understanding of distinctly human qualities-feelings, thoughts, and virtues-thereby diminishing their meaning (Vallor, 2024).</p>
<p>The fallacy also leads researchers to draw invalid inferences from LLM outputs.When interpreting model responses as expressing human-like mental states rather than statistical probabilities, they may fail to account for the statistical artifacts inherent in token prediction systems.Similarly, findings may be inappropriately generalized from anthropomorphized LLMs to human populations without recognizing the essential difference between statistical text generation and human psychological processes.</p>
<p>The replacement perspective thus risks anthropomorphizing algorithms and mischaracterizing their nature-a conceptual error that invites misunderstandings and misinterpretations.For example, one such mischief is that "any given LLM can act as only a single participant" (Dillion et al., 2023).Yet unlike humans, who are influenced by a unique combination of personal experiences, emotions, and cognitive biases, LLMs are not limited to a single perspective but generate responses based on their vast, diverse dataset.This means that, depending on the prompt and context, the same LLM can produce a range of patchwork responses, each reflecting different viewpoints or types of reasoning (Santurkar et al., 2023).This variability is not indicative of a singular, consistent "mind," but rather of a multifaceted tool capable of simulating diverse perspectives.A teenager, a senior citizen, a subject matter expert, or a layperson: LLMs can role-play various characters or personas (Shanahan et al., 2023).This chameleon-like ability highlights LLMs as tools for linguistic simulation, not as human participants.</p>
<p>Mitigating the anthropomorphism fallacy requires using precise language, maintaining conceptual clarity about LLMs' lack of mental states, documenting technical settings, applying simulation-based interpretive frameworks, and specific researcher training.Consider:</p>
<ol>
<li>Language and framing: Use precise, non-anthropomorphic terminology when describing LLM outputs.Instead of saying an LLM "believes" or "feels," opt for terms like "produces," "generates," or "outputs" to accurately reflect their statistical nature (Ibrahim &amp; Cheng, 2025;Shanahan, 2024).</li>
</ol>
<p>Conceptual clarity:</p>
<p>Explicitly acknowledge in research designs and reports that current LLMs do not possess mental states or consciousness (Shardlow &amp; Przybyla, 2024).Define psychological constructs carefully, noting when they inherently depend on mental states that LLMs lack.3. Documentation practices: Document the specific LLM, version, prompt design, and parameter settings used to generate responses (Lin, 2025), emphasizing the technical rather than psychological aspects of the process.4. Interpretive frameworks: Develop and apply interpretive frameworks that treat LLM outputs as simulations rather than expressions of beliefs or attitudes (Ibrahim &amp; Cheng, 2025).This includes distinguishing between "simulated beliefs" and actual beliefs when reporting results.</p>
<p>Education and training: Provide training to research teams on the mechanisms of</p>
<p>LLMs and the risks of anthropomorphic interpretations (Lin, 2025).Foster a research culture that maintains conceptual precision when discussing AI capabilities.</p>
<p>Identity Essentialization Fallacy</p>
<p>Under the replacement perspective, prompting often invokes identity labeling, such as instructing the LLM to act as or adopt the identity of "White man," "Black woman," "Chinese," or "American"-as if such labels describe innate, static, homogeneous social groups, each entailing a specific set of behaviors (Chuang et al., 2024;Wang et al., 2025).This approach constitutes an identity essentialization fallacy that caricatures how identity operates in human populations.</p>
<p>When simple demographic labels are used to prompt LLMs, there is often an implicit assumption that these can generate responses representative of real human populations.Such prompting approaches treat social categories as static and homogeneous, ignoring the vast diversity within any demographic group.When LLMs generate responses based on these simplified identity prompts, they may produce stereotyped or inaccurate outputs that fail to capture the nuanced realities of actual human populations (Li &amp; Qi, 2025), limiting what we can learn about real-world contexts and populations (Lahoti et al., 2023;M. H. Lee et al., 2024).Indeed, demographic prompting can even reduce alignment with human judgments (Sun et al., 2025).</p>
<p>In colloquial exchanges, essentialist language about social categories-from "artists are eccentric" to "women are nurturing"-is convenient and also meaningful.But in empirical research, identity essentialization masks the fluidity and diversity inherent within any demographic, overlooking individual nuances and intersectionality while reinforcing stereotypes and biases prevalent within society, thus overestimating group differences (Namboodiripad et al., 2023;Prentice &amp; Miller, 2006).</p>
<p>Identity is not a static, unitary construct that can be captured by a single demographic label-it is fluid, contextual, and intersectional.When researchers use essentialist prompting techniques, they misrepresent the psychological construct of identity itself, reducing rich, complex human experiences to one-dimensional categories.This reductive operationalization fails to capture how various aspects of identity interact, how identity salience shifts across contexts, and how individuals negotiate multiple, sometimes contradictory, identity facets.This is not to deny the importance of identities, nor to advocate for identity-blindness.As pervasive societal structures that shape our thoughts, attitudes, and behaviors, social categories like race, gender, and class are deeply embedded in our experiences-and often an ingrained part of our identity.But rather than reducing individuals to essentialist categories, a more appropriate approach is to consider how various identities-demographic, professional, or situational-interact, by role-playing various personas through contextualized prompting.This involves crafting character profiles that encompass a broader array of characteristics-from contextual descriptions ("I am a young tech worker living in the United States") to broader social categories (e.g., based on political leaning or personality type)-allowing for more nuanced explorations of perspectives and experiences.</p>
<p>For example, instead of prompting an LLM to act as a "Black woman," which may reinforce stereotypes or oversimplify identity (Sun et al., 2025), we might construct a more holistic persona by adding a specific context, such as "a young entrepreneur from Atlanta who is passionate about sustainable fashion and community development"; or by incorporating intersectional identities, such as "a young Black female tech worker navigating the challenges of a male-dominated field."These contextual descriptions incorporate identity but frame it within specific experiences, values, and contexts.Indeed, contextualized prompting has been shown to evoke distinct, diverse (Wang et al., 2025), and better-aligned responses from LLMs (Bui et al., 2025).</p>
<p>Identity is multifaceted and context-dependent, with varying salience for different individuals.Simulating human participants therefore risks misrepresenting the salience of various aspects of identity-reflecting the prompter's perspective or presumptions about which aspects of identity are important, rather than capturing the intersectional reality experienced by the simulated persona.It is therefore crucial, whether using contextualized prompting or not, to examine potential biases and limitations in the prompt.Sidestepping genuine engagement with marginalized communities further risks artificial inclusion (Agnew et al., 2024).</p>
<p>To mitigate the identity essentialization fallacy when simulating diverse perspectives, safeguards include using contextual prompting, incorporating intersectional approaches, performing diversity validation, ensuring transparency about limitations, and adopting collaborative methods.Consider several approaches:</p>
<ol>
<li>Contextual prompting: Instead of using simple demographic labels, develop richer, context-specific prompts that incorporate multiple aspects of identity (including intersectionality), specific experiences, and environmental factors (Bui et al., 2025).2. Validation: Compare LLM-generated responses across multiple prompting strategies and validate against actual human responses from the target population to identify when simulations misrepresent or stereotype particular groups (Sun et al., 2025).3. Transparency in limitations: Explicitly acknowledge the limitations of identity simulation in research reports, including the risk of reinforcing stereotypes or oversimplifying complex identities.Document the specific prompting approaches used and their potential biases (Sun et al., 2025).4. Collaborative approach: When studying specific cultural or identity groups, involve members of those groups in designing prompts, validating outputs, and interpreting results (Zhao et al., 2024).</li>
</ol>
<p>Substitution Fallacy</p>
<p>LLMs can mimic certain aspects of human behavior and cognition, but purporting them as primary tools to directly reveal the human mind reflects a substitution fallacy.</p>
<p>A core issue arises from the temporal limitations of LLM training data.Because LLMs are trained on historical datasets with a specific cutoff date, they represent a snapshot of human knowledge, attitudes, and behaviors at that moment.Updating through retraining is infrequent and resource-intensive.This static nature restricts their capacity to capture ongoing societal changes, new social phenomena, or evolving attitudes and behaviors, such as rapidly changing views on new technologies or social movements (Zhu et al., 2025).Without real-time adaptability, previous alignments do not guarantee current applicability.Furthermore, a model's advertised knowledge cutoff often differs from its actual, or effective, knowledge cutoff.The functional knowledge of LLMs frequently corresponds to older text versions that predate the stated cutoff.This discrepancy stems from widespread temporal misalignments within large pretraining corpora-for instance, older documents lingering in recent web crawls-and from the incomplete removal of outdated or duplicated content during data processing (Cheng et al., 2024).</p>
<p>This fallacy persists even if we disregard challenges related to the static and historically bound nature of LLM training data-or issues of grounding, embodiment, and subjective experience.As the average human fallacy illustrates, responses from LLMs cannot be assumed a priori to represent average responses of the targeted human group.Even when LLMs and humans show alignment, this correlation should not be confused with equivalence in cognitive processes or mechanisms (the alignment as explanation fallacy; Box 4).This leads to an epistemic dilemma: Generalizing findings from LLMs to humans requires corroboration with actual human data, undermining the basic premise of the substitution proposition.</p>
<p>Additionally, substitution risks creating closed-loop information systems.When models trained on historical data are used as primary tools for generating new data, they perpetuate a self-referential loop that creates a distorted view of the present by amplifying the past (including its biases, errors, and oversights) rather than reflecting current human thought or behavior.This can lead to misleading statistical inferences reflecting model artifacts rather than genuine psychological phenomena and behavioral patterns.</p>
<p>Even with up-to-date training data, excluding human participants leaves LLMs simulating humans in ways detached from rich, evolving realities.Such detachment can entrench outdated knowledge, weaken the diversity vital for human progress, and create epistemic echo chambers.Thus, LLMs should serve as supplementary rather than primary tools for understanding the human mind.</p>
<p>To mitigate the substitution fallacy, safeguards include using sequential validation with human participants, benchmarking against time-sensitive data, integrating mixed methods, ensuring temporal transparency, and implementing closed-loop detection techniques.Consider several approaches:</p>
<ol>
<li>Sequential validation: Implement a sequential research design where LLM explorations are followed by validation with human participants.Use LLMs for hypothesis generation or initial exploration, but validate key findings with relevant human data (Gui &amp; Toubia, 2023).2. Benchmarking against time-sensitive data: Regularly benchmark LLM responses against recent human data to assess temporal drift in model outputs compared to current human attitudes and behaviors.This helps establish the temporal boundaries of generalizability for LLM-based findings (Cheng et al., 2024).3. Temporal transparency: Explicitly document training data cutoff dates and potential temporal limitations in research reports, particularly in rapidly evolving domains (Cheng et al., 2024).</li>
</ol>
<p>Concluding Remarks</p>
<p>Recent advances in human-level AI are renewing the classic debate on the role of computing artifacts in understanding the human mind and brain (Simon, 1983).This paper critically assesses the emerging proposition of substituting human participants with LLMs in behavioral and social sciences.By exposing six fallacies inherent in this replacement perspective, the analysis underscores that, despite their human-like language production capabilities, current LLMs do not-and as presently conceived, cannot-substitute for the nuanced intricacies of human thought.Unlike the statistical text prediction that drives current LLMs, human intelligence emerges from embodied interaction with the world-grounded in sensory experiences, enriched by multimodal integration, and shaped by subjective consciousness.The predominantly linguistic nature of LLMs further constrains their ability to capture the breadth of human experience, including nonverbal cues, implicit attitudes, and real-world behaviors.</p>
<p>By identifying challenges to research integrity and providing practical guidelines, the analysis supports the simulation perspective: LLMs serve as tools for simulating roles and modeling cognitive processes, complementing but not replacing humans.As outlined in Box 4, this perspective helps investigators distinguish between research contexts where outputlevel simulation suffices (pragmatic applications like rapid prototyping) and those requiring deeper mechanistic evidence (theoretical claims about cognitive processes).In practice, researchers should leverage LLMs primarily for hypothesis generation, theory development, and rapid prototyping-then validate with human participants.This sequential approach capitalizes on model strengths (comprehensive knowledge, efficient simulation) while acknowledging their limitations (lack of grounding, representational biases).Implementing the controls and considerations outlined for each fallacy can substantially improve research quality and interpretability.</p>
<p>As emphasized in Box 3, understanding model limitations requires distinguishing between technical and conceptual constraints.While technical limitations may be addressed through engineering advances, conceptual limitations represent fundamental challenges to using LLMs as psychological models.As these technologies evolve, we must continuously re-evaluate their capabilities and limitations, develop appropriate benchmarks, and establish guidelines for responsible integration.This perspective invites us to reconsider the role of AI in behavioral and cognitive science-as a mirror through which we can better understand the similarities and differences between human intelligence and machine intelligence.And the limitations of apparently human-like models in replicating human thought may bring us a deeper appreciation of the complexity and wonder of the mind.</p>
<p>Table 1
1
Six Fallacies in Conceptualizing LLMs as Human Replacements
FallacyCoreWhy It's ProblematicResearch ImplicationsMisconceptionToken prediction asMistaking statisticalLLMs process languageCannot assumeintelligencepattern matchingwithout embodiedcognitive equivalencefor humanexperience,or generalize fromunderstandingconsciousness, orLLM performance togenuine understandinghuman cognitionThe average human Assuming LLMsTraining biasesCannot reliablyrepresent typical(WEIRD, temporal)generalize findings toor average humanand optimization goalsdiverse humanresponsesdistort LLM outputpopulations orrepresentationcontextsAlignment asInferring sharedDifferent architecturesOutput alignment doesexplanationcognitivecan produce similarnot confirm sharedmechanisms fromresults (multipleunderlying processessimilar outputsrealizability); surfacewith humanssimilarity is notmechanisticequivalenceAnthropomorphism Attributing human-Current LLMs lackInvalidateslike mental statesconsciousness,psychologicalto statisticalintentions, or genuineconstructs requiringsystemsemotionalmental states; risksunderstandingmisinterpretationIdentityTreatingReduces complex, fluid,Misrepresentsessentializationdemographicand intersectionalpopulation diversitylabels as fixed,identities toand the construct ofhomogeneousstereotypes; ignoresidentity itselfcategorieswithin-group diversitySubstitutionUsing LLMs asStatic training data, lackLLM data requiresprimary tools forof real-timehuman validation;understandingadaptability, andcannot directly replacehuman cognitionpotential for closed-human participantsor behaviorloop effects</p>
<p>•</p>
<p>Input robustness: Models demonstrate fragility to minor input perturbations and noise, exemplified by the sensitivity of LLMs to prompt variations, namely, brittleness.
These differences in optimization mechanisms, architectural constraints, and resultingbehaviors illustrate why token prediction, however sophisticated, produces a qualitativelydifferent form of intelligence than human cognition-and thus LLMs cannot simplyreplace human participants in psychological research.Box 3 |</p>
<p>Technical versus Conceptual Limitations in Model Simulation of Human Behavior and Cognition</p>
<p>When evaluating language models as simulation tools for psychological science, distinguishing between technical and conceptual limitations clarifies both current research constraints and future prospects.Technical limitations arise from current implementation practices and can be mitigated through methodological refinements.</p>
<p>These include: (1) training data biases-predominantly WEIRD-skewed text corpora with specific temporal cutoffs; (2) algorithmic artifacts-such as those introduced by RLHF that systematically alter response patterns; (3) architectural constraints-including limited context windows and memory capabilities; and (4) prompt sensitivity-where minor wording variations produce substantially different outputs.These limitations affect simulation fidelity and require various methodological controls, but are potentially addressable through engineering advancements within the existing paradigm.Progress in addressing technical limitations is already underway.Multimodal LLMs (MLLMs) now process information across text, images, audio, and video.Reasoningfocused models (sometimes referred to as large reasoning models) integrate reinforcement learning with chains of thought, explicitly narrating intermediate reasoning steps.This scaffolding promotes deeper analysis and improves performance on tasks requiring structured, multi-step inference across domains like science, math, and code.As models learn from increasingly diverse digital media, they develop more sophisticated representations of the physical world, potentially capturing dynamics beyond human sensory reach (e.g., at micro and macro scales) and improving aspects of physical common sense derived from text alone.</p>
<p>The writing was supported by the National Key R&amp;D Program of China STI2030 Major Projects (2021ZD0204200), the National Natural Science Foundation of China (32071045), and the Shenzhen Fundamental Research Program (JCYJ20210324134603010).The funders had no role in the decision to publish or in the preparation of the manuscript.I used Claude Sonnet/Opus 4, GPT-4o, and Gemini 2.5 Pro for proofreading the manuscript, following the prompts described at https://www.nature.com/articles/s41551-024-01185-8.
A primer for evaluating large language models in social-science research. S Abdurahman, A Salkhordeh Ziabari, A K Moore, D M Bartels, M Dehghani, 10.1177/25152459251325174Advances in Methods and Practices in Psychological Science. 2025825152459251325174</p>
<p>W Agnew, A S Bergman, J Chien, M Díaz, S El-Sayed, J Pittman, . . Mckee, K R , 10.1145/3613904.3642703The illusion of artificial inclusion Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems. Honolulu, HI, USA2024</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. G V Aher, R I Arriaga, A T Kalai, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine LearningHonolulu, Hawaii, USA2023</p>
<p>How likely do LLMs with CoT mimic human reasoning?. G Bao, H Zhang, C Wang, L Yang, Y Zhang, 10.48550/arXiv.2402.16048arXiv:2402.160482024</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. E M Bender, A Koller, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline2020</p>
<p>FAQ on catastrophic AI risks. Y Bengio, 2024. September 8, 2024</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, 10.1073/pnas.22185231202023a120Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Turning large language models into cognitive models. M Binz, E Schulz, 10.48550/arXiv.2306.03917arXiv:2306.039172023b</p>
<p>Psychologism and behaviorism. N Block, 10.2307/2184371Philosophical Review. 9011981</p>
<p>Deep problems with neural network models of human vision. J S Bowers, G Malhotra, M Dujmovic, M Llera Montero, C Tsvetkov, V Biscione, . . Blything, R , 10.1017/S0140525X22002813Behavioral and Brain Sciences. 46e3852023</p>
<p>Prompt architecture induces methodological artifacts in large language models. M Brucks, O Toubia, 10.1371/journal.pone.0319159PLOS ONE. 204e03191592025</p>
<p>Mixture-of-personas language models for population simulation. N Bui, H T Nguyen, S Kumar, J Theodore, W Qiu, V A Nguyen, R Ying, 10.48550/arXiv.2504.05019arXiv:2504.050192025</p>
<p>P Butlin, R Long, E Elmoznino, Y Bengio, J Birch, A Constant, . . Ji, X , 10.48550/arXiv.2308.08708arXiv:2308.08708Consciousness in artificial intelligence: Insights from the science of consciousness. 2023</p>
<p>J Cheng, M Marone, O Weller, D Lawrie, D Khashabi, B Van Durme, 10.48550/arXiv.2403.12958arXiv:2403.12958Dated data: Tracing knowledge cutoffs in large language models. 2024</p>
<p>Y.-S Chuang, Z Studdiford, K Nirunwiroj, A Goyal, V V Frigo, S Yang, . . Rogers, T T , 10.48550/arXiv.2406.17232arXiv:2406.17232Beyond demographics: Aligning role-playing LLM-based agents using human belief networks. 2024</p>
<p>An unsolvable of elementary number theory. A Church, 10.2307/2371045American Journal of Mathematics. 5821936</p>
<p>Folk psychological attributions of consciousness to large language models. C Colombatto, S M Fleming, 10.1093/nc/niae013Neuroscience of Consciousness. 20241e0132024</p>
<p>Quasi-experimentation: Design &amp; analysis issues for field settings. T D Cook, D T Campbell, 1979Houghton Mifflin</p>
<p>Should large language models replace human participants?. M Crockett, L Messeri, 10.31234/osf.io/4zdx9PsyArXiv. 2023</p>
<p>Construct validity in psychological tests. L J Cronbach, P E Meehl, 10.1037/h0040957Psychological Bulletin. 5241955</p>
<p>Language models show human-like content effects on reasoning tasks. I Dasgupta, A K Lampinen, S C Chan, H R Sheahan, A Creswell, D Kumaran, . . Hill, F , 10.48550/arXiv.2207.07051arXiv:2207.070512022</p>
<p>Using large language models in psychology. D Demszky, D Yang, D S Yeager, C J Bryan, M Clapper, S Chandhok, . . Pennebaker, J W , 10.1038/s44159-023-00241-5Nature Reviews Psychology. 2112023</p>
<p>Do large language models reason causally like us? Even better?. H M Dettki, B M Lake, C M Wu, B Rehder, 10.48550/arXiv.2502.10215arXiv:2502.102152025</p>
<p>Can AI language models replace human participants?. D Dillion, N Tandon, Y Gu, K Gray, 10.1016/j.tics.2023.04.008Trends in Cognitive Sciences. 2772023</p>
<p>Good-enough representations in language comprehension. A Doerig, R P Sommers, K Seeliger, B Richards, J Ismael, G W Lindsay, . . Kietzmann, T C Ferreira, F Bailey, K G D Ferraro, V , 10.1111/1467-8721.00158Current Directions in Psychological Science. 2472023. 2002Nature Reviews Neuroscience</p>
<p>Performance vs. competence in human-machine comparisons. C Firestone, 10.1073/pnas.19053341172020117Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Can artificial intelligence reach human thought. A S Fokas, 10.1093/pnasnexus/pgad409PNAS Nexus. 2124092023</p>
<p>Y Gao, D Lee, G Burtch, S Fazelpour, 10.48550/arXiv.2410.19599arXiv:2410.19599Take caution in using LLMs as human surrogates: Scylla ex machina. 2024</p>
<p>AI and the transformation of social science research. I Grossmann, M Feinberg, D C Parker, N A Christakis, P E Tetlock, W A Cunningham, 10.1126/science.adi1778Science. 38066502023</p>
<p>On logical inference over brains, behaviour, and artificial neural networks. O Guest, A E Martin, 10.1007/s42113-022-00166-xComputational Brain &amp; Behavior. 622023</p>
<p>The challenge of using LLMs to simulate human behavior: A causal inference perspective. G Gui, O Toubia, 10.48550/arXiv.2312.15524arXiv:2312.155242023</p>
<p>Language models represent space and time. W Gurnee, M Tegmark, 10.48550/arXiv.2310.02207arXiv:2310.022072023</p>
<p>Language agents mirror human causal reasoning biases: How can we help them think like scientists?. A Gx-Chen, D Lin, M Samiei, D Precup, B A Richards, R Fergus, K Marino, 10.48550/arXiv.2505.09614arXiv:2505.096142025</p>
<p>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. T Hagendorff, S Fabi, M Kosinski, J Harding, W D'alessandro, N G Laskowski, R Long, 10.1007/s00146-023-01725-xNature Computational Science. 3102023. 2024AI &amp; SocietyAI language models cannot replace human research participants</p>
<p>What is it for a machine learning model to have a capability?. J Harding, N Sharadin, 10.1086/732153The British Journal for the Philosophy of Science. in press</p>
<p>Language models align with human judgments on key grammatical constructions. J Hu, K Mahowald, G Lupyan, A Ivanova, R Levy, 10.1073/pnas.24009171212024121e2400917121Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Generative language models exhibit social identity biases. T Hu, Y Kyrychenko, S Rathje, N Collier, S Van Der Linden, J Roozenbeek, 10.1038/s43588-024-00741-1Nature Computational Science. 512025</p>
<p>Analysis of LLM bias (Chinese propaganda and anti-US sentiment) in DeepSeek-R1 vs. P.-H Huang, Z Lin, S Imbot, W Fu, E Tu, 10.48550/arXiv.2506.01814arXiv:2506.01814ChatGPT o3-mini-high. 2025</p>
<p>L Ibrahim, M Cheng, 10.48550/arXiv.2502.09192arXiv:2502.09192Thinking beyond the anthropomorphic paradigm benefits LLM research. 2025</p>
<p>How to evaluate the cognitive abilities of LLMs. A A Ivanova, 10.1038/s41562-024-02096-zNature Human Behaviour. 922025</p>
<p>Epiphenomenal qualia. The Philosophical Quarterly. F Jackson, 10.2307/2960077198232</p>
<p>What Mary didn't know. F Jackson, 10.2307/2026143The Journal of Philosophy. 8351986</p>
<p>Language model alignment in multilingual trolley problems. Z Jin, M Kleiman-Weiner, G Piatti, S Levine, J Liu, F Gonzalez, . . Mihalcea, R , 10.48550/arXiv.2407.02273arXiv:2407.022732024</p>
<p>Does word knowledge account for the effect of world knowledge on pronoun interpretation?. C R Jones, B Bergen, 10.1017/langcog.2024.2Language and Cognition. 2024</p>
<p>Distributional semantics still can't account for affordances. C R Jones, T A Chang, S Coulson, J A Michaelov, S Trott, B Bergen, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society2022</p>
<p>R Kamoi, S S S Das, R Lou, J J Ahn, Y Zhao, X Lu, . . Vummanthala, S R , 10.48550/arXiv.2404.03602arXiv:2404.03602Evaluating LLMs at detecting errors in LLM responses. 2024</p>
<p>VALID: A checklist-based approach for improving validity in psychological research. S Kerschbaumer, M Voracek, B Aczél, S F Anderson, B M Booth, E M Buchanan, . . Tran, U S , 10.1177/25152459241306432Advances in Methods and Practices in Psychological Science. 81251524592413064322025</p>
<p>Knowledge of animal appearance among sighted and blind adults. J S Kim, G V Elli, M Bedny, 10.1073/pnas.19009521162019116Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Simulating subjects: The promise and peril of AI stand-ins for social agents and interactions. A C Kozlowski, J A Evans, 10.31235/osf.io/vp3j22024</p>
<p>Improving diversity of demographic representation in large language models via collective-critiques and self-voting. P Lahoti, N Blumm, X Ma, R Kotikalapudi, S Potluri, Q Tan, . . Beutel, A , 10.48550/arXiv.2310.16523arXiv:2310.165232023</p>
<p>Word meaning in minds and machines. B M Lake, G L Murphy, 10.1037/rev0000297Psychological Review. 13022023</p>
<p>Large language models portray socially subordinate groups as more homogeneous, consistent with a bias observed in humans. M H Lee, J M Montgomery, C K Lai, Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency. the 2024 ACM Conference on Fairness, Accountability, and TransparencyRio de Janeiro, Brazil2024</p>
<p>S Lee, M Li, B Lai, W Jia, F Ryan, X Cao, . . Yang, D , 10.48550/arXiv.2409.15316arXiv:2409.15316Towards social AI: A survey on understanding social interactions. 2024</p>
<p>E Leivada, G Marcus, F Günther, E Murphy, 10.48550/arXiv.2308.00109arXiv:2308.00109A sentence is worth a thousand pictures: Can large language models understand hum4n l4ngu4ge and the w0rld behind w0rds. 2023</p>
<p>Distributional semantics as a source of visual knowledge. M Lewis, M Zettersten, G Lupyan, 10.1073/pnas.19101481162019116Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Toward accurate psychological simulations: Investigating LLMs' responses to personality and cultural variables. C Li, Y Qi, 10.1016/j.chb.2025.108687Computers in Human Behavior. 1701086872025</p>
<p>Quantifying AI psychology: A psychometrics benchmark for large language models. Y Li, Y Huang, H Wang, X Zhang, J Zou, L Sun, 10.48550/arXiv.2406.17675arXiv:2406.176752024</p>
<p>Why and how to embrace AI such as ChatGPT in your academic life. Z Lin, 10.1098/rsos.230658Royal Society Open Science. 102306582023</p>
<p>How to write effective prompts for large language models. Z Lin, 10.1038/s41562-024-01847-2Nature Human Behaviour. 842024</p>
<p>Beyond principlism: Practical strategies for ethical AI use in research practices. Z Lin, 10.1007/s43681-024-00585-5AI and Ethics. 52025</p>
<p>Global diversity of authors, editors, and journal ownership across subdisciplines of psychology: Current state and policy implications. Z Lin, N Li, 10.1177/17456916221091831Perspectives on Psychological Science. 1822023</p>
<p>Dissociating language and thought in large language models. K Mahowald, A A Ivanova, I A Blank, N Kanwisher, J B Tenenbaum, E Fedorenko, 10.1016/j.tics.2024.01.011Trends in Cognitive Sciences. 2862024</p>
<p>Emergent linguistic structure in artificial neural networks trained by self-supervision. C D Manning, K Clark, J Hewitt, U Khandelwal, O Levy, 10.1073/pnas.19073671172020117Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Large language models predict human sensory judgments across six modalities. R Marjieh, I Sucholutsky, P Van Rijn, N Jacoby, T L Griffiths, 10.1038/s41598-024-72071-1Scientific Reports. 141214452024</p>
<p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M D Hardy, T L Griffiths, 10.1073/pnas.23224201212024121e2322420121Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Language models as models of language. R Millière, 10.48550/arXiv.2408.07144arXiv:2408.071442024</p>
<p>A philosophical introduction to language models -Part I: Continuity with classic debates. R Millière, C Buckner, 10.48550/arXiv.2401.03910arXiv:2401.039102024a</p>
<p>R Millière, C Buckner, 10.48550/arXiv.2405.03207arXiv:2405.03207A philosophical introduction to language models -Part II: The way forward. 2024b</p>
<p>I Mirzadeh, K Alizadeh, H Shahrokhi, O Tuzel, S Bengio, M Farajtabar, 10.48550/arXiv.2410.05229arXiv:2410.05229GSM-Symbolic: Understanding the limitations of mathematical reasoning in large language models. 2024</p>
<p>Debates on the nature of artificial general intelligence. M Mitchell, 10.1126/science.ado7069Science. 383668970692024</p>
<p>The debate over understanding in AI's large language models. M Mitchell, D C Krakauer, 10.1073/pnas.22159071202023120Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Evaluating cognitive maps and planning in large language models with cogeval. I Momennejad, H Hasanbeig, F Vieira Frujeri, H Sharma, N Jojic, H Palangi, . . Larson, J , Advances in Neural Information Processing Systems. 202336</p>
<p>M Mozikov, N Severin, V Bodishtianu, M Glushanina, M Baklashkin, A V Savchenko, I Makarov, 10.48550/arXiv.2406.03299arXiv:2406.03299The good, the bad, and the Hulk-like GPT: Analyzing emotional decisions of large language models in cooperation and bargaining games. 2024</p>
<p>One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity. S K Murthy, T Ullman, J Hu, 10.48550/arXiv.2411.04427arXiv:2411.044272024</p>
<p>Essentialist characterizations of language are an obstacle to accuracy, progress, and justice in science. S Namboodiripad, E Kutlu, A Babel, M Baese-Berk, A Block, M T Carlson, . . Wright, K E , 10.31234/osf.io/jn3ct2023</p>
<p>Large language models can replicate crosscultural differences in personality. P Niszczota, M Janczak, M Misiak, 10.1016/j.jrp.2025.104584Journal of Research in Personality. 1152025. 104584</p>
<p>Robustness of large language models in moral judgements. S Oh, V Demberg, 10.1098/rsos.241229Royal Society Open Science. 1242412292025</p>
<p>OpenAI charter. Openai, 2024. September 8, 2024</p>
<p>Diminished diversity-of-thought in a standard large language model. P S Park, P Schoenegger, C Zhu, 10.3758/s13428-023-02307-x2024</p>
<p>AI psychometrics: Assessing the psychological profiles of large language models through psychometric inventories. M Pellert, C M Lechner, C Wagner, B Rammstedt, M Strohmaier, 10.1177/17456916231214460Perspectives on Psychological Science. 1952024</p>
<p>Essentializing differences between women and men. D A Prentice, D T Miller, 10.1111/j.1467-9280.2006.01675.xPsychological Science. 1722006</p>
<p>Promoting interactions between cognitive science and large language models. Y Qu, P Du, W Che, C Wei, C Zhang, W Ouyang, . . Liu, Q , 10.1016/j.xinn.2024.100579The Innovation. 521005792024</p>
<p>A deep learning framework for neuroscience. B A Richards, T P Lillicrap, P Beaudoin, Y Bengio, R Bogacz, A Christensen, . . Kording, K P , 10.1038/s41593-019-0520-2Nature Neuroscience. 22112019</p>
<p>Whose opinions do language models reflect?. S Santurkar, E Durmus, F Ladhak, C Lee, P Liang, T Hashimoto, Proceedings of the 40th International Conference on Machine Learning, Proceedings of Machine Learning Research. the 40th International Conference on Machine Learning, Machine Learning Research2023</p>
<p>Using large language models to generate silicon samples in consumer and marketing research: Challenges, opportunities, and guidelines. M Sarstedt, S J Adler, L Rau, B Schmitt, 10.1002/mar.21982Psychology &amp; Marketing. 412024</p>
<p>Simulating the human in HCD with ChatGPT: Redesigning interaction design with AI. Interactions. A Schmidt, P Elagroudy, F Draxler, F Kreuter, R Welsch, 10.1145/3637436202431</p>
<p>Minds, brains, and programs. J R Searle, 10.1017/S0140525X00005756Behavioral and Brain Sciences. 331980</p>
<p>Talking about large language models. M Shanahan, 10.1145/3624724Communications of the ACM. 6722024</p>
<p>Role play with large language models. M Shanahan, K Mcdonell, L Reynolds, 10.1038/s41586-023-06647-8Nature. 62379872023</p>
<p>Deanthropomorphising NLP: Can a language model be conscious?. M Shardlow, P Przybyla, 10.1371/journal.pone.0307521PLOS ONE. 1912e03075212024</p>
<p>Probing the psychology of AI models. R Shiffrin, M Mitchell, 10.1073/pnas.23009631202023120Proceedings of the National Academy of Sciences of the United States of America</p>
<p>Why should machines learn?. H A Simon, 10.1016/B978-0-08-051054-5.50006-6Machine Learning. R S Michalski, J G Carbonell, T M Mitchell, Morgan Kaufmann1983</p>
<p>Experimental and quasi-experimental designs for research. J C Stanley, D T Campbell, 1963Rand McNally</p>
<p>Language model finetuning on scaled survey data for predicting distributions of public opinions. J W A Strachan, D Albergo, G Borghini, O Pansardi, E Scaliti, S Gupta, . . Becchio, C Suh, J Jahanparast, E Moon, S Kang, M Chang, S , 10.48550/arXiv.2502.16761arXiv:2502.16761Nature Human Behaviour. 872024. 2025Testing theory of mind in large language models and humans</p>
<p>Sociodemographic prompting is not yet an effective approach for simulating subjective judgments with LLMs. H Sun, J Pei, M Choi, D Jurgens, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. the 2025 Conference of the Nations of the Americas ChapterAlbuquerque, New MexicoShort Papers20252</p>
<p>Do large language models show decision heuristics similar to humans? A case study using GPT-3.5. G Suri, L R Slater, A Ziaee, M Nguyen, 10.1037/xge0001547Journal of Experimental Psychology: General. 15342024</p>
<p>Cultural bias and cultural alignment of large language models. Y Tao, O Viberg, R S Baker, R F Kizilcec, 10.1093/pnasnexus/pgae346PNAS Nexus. 932024</p>
<p>Do LLMs exhibit human-like response biases? A case study in survey design. L Tjuatja, V Chen, T Wu, A Talwalkwar, G Neubig, 10.1162/tacl_a_00685Transactions of the Association for Computational Linguistics. 122024</p>
<p>The AI mirror: How to reclaim our humanity in an age of machine thinking. S Vallor, 2024Oxford University Press</p>
<p>Reclaiming AI as a theoretical tool for cognitive science. I Van Rooij, O Guest, F G Adolfi, R De Haan, A Kolokolova, P Rich, 10.1007/542113-024-00217-5Computational Brain &amp; Behavior. 72024</p>
<p>Polosukhin, I. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Attention is all you need Proceedings of the 31st International Conference on Neural Information Processing Systems. Long Beach, California, USA2017</p>
<p>Large language models that replace human participants can harmfully misportray and flatten identity groups. A Wang, J Morgenstern, J P Dickerson, 10.1038/s42256-025-00986-zNature Machine Intelligence. 732025</p>
<p>Emotional intelligence of large language models. X Wang, X Li, Z Yin, Y Wu, J Liu, 10.1177/18344909231213958Journal of Pacific Rim Psychology. 17183449092312139582023</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, 10.1038/s41562-023-01659-wNature Human Behaviour. 792023</p>
<p>Exploring spatial schema intuitions in large language and vision models. P Wicke, L Wachowiak, Findings of the Association for Computational Linguistics: ACL 2024. L.-W Ku, A Martins, &amp; V Srikumar, Bangkok, Thailand2024. August</p>
<p>Large language models without grounding recover non-sensorimotor but not sensorimotor features of human concepts. Q Xu, Y Peng, S A Nastase, M Chodorow, M Wu, P Li, 10.1038/s41562-025-02203-8Nature Human Behaviour. 2025</p>
<p>Studying and improving reasoning in humans and machines. N Yax, H Anllo, S Palminteri, 10.1038/s44271-024-00091-8Communications Psychology. 21512024</p>
<p>Language models as critical thinking tools: A case study of philosophers. A Ye, J Moore, R Novick, A X Zhang, 10.48550/arXiv.2404.04516arXiv:2404.045162024</p>
<p>From task structures to world models: What do LLMs know?. I Yildirim, L A Paul, 10.1016/j.tics.2024.02.008Trends in Cognitive Sciences. 2852024</p>
<p>Can the capability of large language models be described by human ability? A meta study. M Zan, Y Zhang, B Zhang, F Liu, D Cheng, 10.48550/arXiv.2504.12332arXiv:2504.123322025</p>
<p>X Zhao, S M Sriwarnasinghe, J Tang, S Wang, H Wang, S Morikawa, 10.48550/arXiv.2411.08294arXiv:2411.08294Collaborative participatory research with LLM agents in South Asia: An empiricallygrounded methodological initiative and agenda from field evidence in Sri Lanka. 2024</p>
<p>Larger and more instructable language models become less reliable. L Zhou, W Schellaert, F Martinez-Plumed, Y Moros-Daval, C Ferri, J Hernandez-Orallo, 10.1038/s41586-024-07930-yNature. 2024</p>
<p>Is your LLM outdated? A deep look at temporal generalization. C Zhu, N Chen, Y Gao, Y Zhang, P Tiwari, B Wang, Proceedings of the 2025 Conference of the Nations of the Americas Chapter. Long Papers. the 2025 Conference of the Nations of the Americas ChapterAlbuquerque, New Mexicothe Association for Computational Linguistics20251</p>
<p>Can large language models transform computational social science? Computational Linguistics. C Ziems, W Held, O Shaikh, J Chen, Z Zhang, D Yang, 10.1162/coli_a_00502202450</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4713 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4713</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4713</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-f13e41d24e5d0a68ca662c1b49de398a6fb68251</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f13e41d24e5d0a68ca662c1b49de398a6fb68251" target="_blank">A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A metric to measure the lexicon usage diversity of a given MWP corpus is proposed, and it is demonstrated that ASDiv (Academia Sinica Diverse MWP Dataset) is more diverse than existing corpora.</p>
                <p><strong>Paper Abstract:</strong> We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that ASDiv is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4713.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4713.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LCA++</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LCA++ (Roy & Roth 2015)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical model for solving general arithmetic word problems that infers equations from text using learned patterns and features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving general arithmetic word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LCA++</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Statistical (feature-based) arithmetic word problem solver introduced by Roy & Roth (2015); learns mappings from problem text to equation templates using supervised training (no neural gate-feedforward stacks described here).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic MWPs: addition, subtraction, multiplication, division, multi-step arithmetic problems (elementary school level).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Template/pattern-based mapping from text to equation templates using learned statistical features; effectively relies on recognizing lexicon and syntactic patterns to select or construct equations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Paper argues and cites that low lexical/syntactic diversity in corpora lets systems exploit similar templates; empirical correlation: LCA++ achieves high accuracy on low-diversity corpora and much lower accuracy on diverse ASDiv.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>None reported in this paper that shows LCA++ uses algorithmic arithmetic reasoning beyond pattern/template matching; performance collapses on diverse or higher-grade problems, indicating lack of robust algorithmic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in Tables 3 and 4: On ASDiv-A (CLD=0.50) LCA++ accuracy = 0.68; on ASDiv (CLD=0.49) accuracy = 0.36. Per-grade accuracies on ASDiv: G1=0.53, G2=0.64, G3=0.49, G4=0.35, G5=0.03, G6=0.01. On MathQA-C the table shows '-' (failure to run/apply) for LCA++ on that corpus in this paper's reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No internal probing reported for LCA++; paper uses corpus-level interventions (constructing ASDiv with higher lexicon diversity, filtering inconsistent formulas) and cross-validation to evaluate LCA++ performance across datasets of varying diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Performance heavily degrades on lexically and syntactically diverse MWPs and on higher-grade (more complex) problems; susceptible to training/test pattern overlap (over-optimistic results when test set contains near-duplicates of training); may fail to run/apply on some corpora (reported '-' on MathQA-C in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Compared in paper to UnitDep and GTS: LCA++ generally underperforms UnitDep on ASDiv-A (0.68 vs 0.78) and is similar to GTS on ASDiv (0.36 vs 0.36), but was not reported on MathQA-C where GTS scored 0.86.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4713.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4713.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UnitDep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UnitDep (Unit Dependency Graph)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A statistical approach that models unit dependencies between quantities to improve arithmetic word problem solving, using a unit-dependency graph representation to constrain equation construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unit Dependency Graph and its Application to Arithmetic Word Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>UnitDep</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Statistical solver that augments textual-to-equation inference with unit-dependency graphs that capture relationships among quantities (units) to reduce incorrect equation hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic MWPs (basic operations and multi-step problems) where unit consistency aids equation inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Uses explicit unit-dependency constraints/representations to guide selection/construction of arithmetic expressions, reducing implausible equations by reasoning about units rather than only lexical templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical results: UnitDep attains the highest reported accuracy on ASDiv-A (0.78), outperforming LCA++ and matching or slightly exceeding GTS on some settings, suggesting unit constraints improve generalization on diverse arithmetic problems.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite unit modeling, UnitDep's accuracy drops on the fully diverse ASDiv dataset (0.37) and on higher-grade problems, indicating unit constraints alone do not yield robust arithmetic algorithmic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: ASDiv-A accuracy = 0.78; ASDiv accuracy = 0.37. Per-grade accuracies on ASDiv: G1=0.55, G2=0.65, G3=0.51, G4=0.34, G5=0.03, G6=0.01. MathQA-C entry in Table 3 shows '-' for UnitDep (failure on that corpus as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>No internal probing/ablation of UnitDep reported in this paper; evidence is comparative performance across datasets of differing lexicon diversity and grade difficulty.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Fails to generalize reliably on highly diverse lexicon/syntactic patterns and on more complex (higher-grade) problems; still vulnerable to inflated performance from low-diversity datasets and inconsistent dataset annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Outperforms LCA++ on ASDiv-A (0.78 vs 0.68) and slightly outperforms or matches GTS in some settings; however, all models achieve low accuracy (~0.36) on the full ASDiv corpus, indicating limited gap between approaches when evaluated on truly diverse problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4713.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4713.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goal-driven Tree-structured neural model (GTS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network-based math word problem solver that builds tree-structured expressions guided by goal-driven decoding using two-layer gate-feedforward networks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A goal-driven tree-structured neural model for math word problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GTS</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Neural model (Xie & Sun 2019) using a goal-driven tree-structured decoding approach with two-layer gate-feedforward networks to generate equation trees from problem text.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Arithmetic MWPs including single-step and multi-step operations; aims to generate expression trees representing solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Neural tree-structured generation can learn compositional expression-building, potentially enabling some algorithmic reasoning by constructing expression trees conditioned on textual goals rather than purely matching templates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GTS attains high accuracy on low-diversity corpus MathQA-C (0.86), and competitive performance on ASDiv-A (0.68), indicating neural tree generation can capture useful mappings; however performance drops on highly diverse ASDiv (0.36), showing limits to generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Substantial performance degradation on ASDiv and on higher-grade problems (G5/G6 ~0.07/0.01) suggests the model does not perform robust algorithmic arithmetic reasoning and is sensitive to training corpus diversity and complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported: MathQA-C accuracy = 0.86; ASDiv-A accuracy = 0.68 (noted significantly lower than MathQA-C with p<0.01); ASDiv accuracy = 0.36. Per-grade accuracies on ASDiv: G1=0.64, G2=0.60, G3=0.47, G4=0.34, G5=0.07, G6=0.01.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Paper reports repeated runs (5 times averaged) and cross-validation; also compares performance across datasets with different lexicon diversity and after filtering inconsistent formulas using SymPy verification, but no fine-grained internal probing of neural mechanisms is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Fails on complex/higher-grade problems and highly diverse linguistic patterns; performance inflated by test/train similarity; sensitive to noisy/inconsistent dataset annotations; does not exhibit robust arithmetic generalization across diverse MWPs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Outperforms or matches statistical approaches on low-diversity MathQA-C (0.86) but on more diverse ASDiv variants its advantage disappears (GTS ~0.36 equals LCA++ ~0.36 and close to UnitDep ~0.37).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4713.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4713.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Template/Pattern Matching (mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Equation-template / lexical-pattern memorization mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-algorithmic mechanism wherein MWP solvers obtain answers by matching input text to previously seen lexical/syntactic patterns or equation templates rather than performing symbolic arithmetic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Template/pattern matching (MWP solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Describes the tendency of statistical and learned systems to rely on lexicon and sentence-pattern similarity to training examples to select pre-learned equation templates or generate similar equations; essentially a memorization/generalization-from-templates strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Primarily arithmetic MWPs that correspond to recurring sentence patterns and equation templates (single-step and multi-step elementary problems).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Systems perform arithmetic by retrieving or instantiating equation templates learned from training examples that have similar lexicon/syntactic patterns, rather than performing general symbolic or algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Correlation in paper: corpora with low lexicon usage diversity (low CLD) yield much higher reported solver accuracies; MathQA and similar datasets contain many identical sentence patterns enabling template reuse; Roy & Roth (2017) show performance drops when similar MWPs are removed; ASDiv designed to reduce this effect shows much lower solver performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct internal model analyses disprove template reliance; however, presence of models (e.g., UnitDep) that use unit reasoning indicates hybrid approaches can mitigate pure template dependence, though not fully eliminate failures.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not directly a model, but explanatory: on corpora with low diversity (MathQA-C, CLD~0.08) systems achieve up to 0.86 (GTS), while on high-diversity ASDiv (CLD~0.49) performance drops to ~0.36, demonstrating the impact of template/matching strategies on apparent performance.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Paper constructs ASDiv to reduce pattern duplicates and measures lexicon usage diversity (LD, CLD) via BLEU-derived metric, then evaluates solvers; also filters MathQA with SymPy to expose annotation inconsistencies. These interventions show that reducing pattern overlap reduces solver accuracy, supporting template-matching hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Leads to over-optimistic evaluation when datasets have low diversity or high train-test pattern overlap; fails to generalize to novel lexical/syntactic phrasing, higher-grade multi-step reasoning, or problems requiring external domain knowledge; sensitive to noisy/inconsistent annotations in datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Explains differences among LCA++, UnitDep, and GTS across datasets: high accuracies on low-diversity corpora likely reflect template reuse, while similar low accuracies across diverse datasets indicate all approaches are vulnerable to this failure mode.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers', 'publication_date_yy_mm': '2020-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Solving general arithmetic word problems <em>(Rating: 2)</em></li>
                <li>Unit Dependency Graph and its Application to Arithmetic Word Problem Solving <em>(Rating: 2)</em></li>
                <li>A goal-driven tree-structured neural model for math word problems <em>(Rating: 2)</em></li>
                <li>A Meaning-based English Math Word Problem Solver with Understanding, Reasoning and Explanation <em>(Rating: 2)</em></li>
                <li>A Meaning-based Statistical English Math Word Problem Solver <em>(Rating: 2)</em></li>
                <li>MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms <em>(Rating: 2)</em></li>
                <li>How well do computers solve math word problems? Large-scale Dataset construction and evaluation <em>(Rating: 1)</em></li>
                <li>Parsing algebraic word problems into equations <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4713",
    "paper_id": "paper-f13e41d24e5d0a68ca662c1b49de398a6fb68251",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "LCA++",
            "name_full": "LCA++ (Roy & Roth 2015)",
            "brief_description": "A statistical model for solving general arithmetic word problems that infers equations from text using learned patterns and features.",
            "citation_title": "Solving general arithmetic word problems",
            "mention_or_use": "use",
            "model_name": "LCA++",
            "model_description": "Statistical (feature-based) arithmetic word problem solver introduced by Roy & Roth (2015); learns mappings from problem text to equation templates using supervised training (no neural gate-feedforward stacks described here).",
            "arithmetic_task_type": "Arithmetic MWPs: addition, subtraction, multiplication, division, multi-step arithmetic problems (elementary school level).",
            "mechanism_hypothesis": "Template/pattern-based mapping from text to equation templates using learned statistical features; effectively relies on recognizing lexicon and syntactic patterns to select or construct equations.",
            "evidence_for_mechanism": "Paper argues and cites that low lexical/syntactic diversity in corpora lets systems exploit similar templates; empirical correlation: LCA++ achieves high accuracy on low-diversity corpora and much lower accuracy on diverse ASDiv.",
            "evidence_against_mechanism": "None reported in this paper that shows LCA++ uses algorithmic arithmetic reasoning beyond pattern/template matching; performance collapses on diverse or higher-grade problems, indicating lack of robust algorithmic generalization.",
            "performance_metrics": "Reported in Tables 3 and 4: On ASDiv-A (CLD=0.50) LCA++ accuracy = 0.68; on ASDiv (CLD=0.49) accuracy = 0.36. Per-grade accuracies on ASDiv: G1=0.53, G2=0.64, G3=0.49, G4=0.35, G5=0.03, G6=0.01. On MathQA-C the table shows '-' (failure to run/apply) for LCA++ on that corpus in this paper's reported experiments.",
            "probing_or_intervention_results": "No internal probing reported for LCA++; paper uses corpus-level interventions (constructing ASDiv with higher lexicon diversity, filtering inconsistent formulas) and cross-validation to evaluate LCA++ performance across datasets of varying diversity.",
            "limitations_and_failure_modes": "Performance heavily degrades on lexically and syntactically diverse MWPs and on higher-grade (more complex) problems; susceptible to training/test pattern overlap (over-optimistic results when test set contains near-duplicates of training); may fail to run/apply on some corpora (reported '-' on MathQA-C in Table 3).",
            "comparison_to_other_models": "Compared in paper to UnitDep and GTS: LCA++ generally underperforms UnitDep on ASDiv-A (0.68 vs 0.78) and is similar to GTS on ASDiv (0.36 vs 0.36), but was not reported on MathQA-C where GTS scored 0.86.",
            "uuid": "e4713.0",
            "source_info": {
                "paper_title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "UnitDep",
            "name_full": "UnitDep (Unit Dependency Graph)",
            "brief_description": "A statistical approach that models unit dependencies between quantities to improve arithmetic word problem solving, using a unit-dependency graph representation to constrain equation construction.",
            "citation_title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
            "mention_or_use": "use",
            "model_name": "UnitDep",
            "model_description": "Statistical solver that augments textual-to-equation inference with unit-dependency graphs that capture relationships among quantities (units) to reduce incorrect equation hypotheses.",
            "arithmetic_task_type": "Arithmetic MWPs (basic operations and multi-step problems) where unit consistency aids equation inference.",
            "mechanism_hypothesis": "Uses explicit unit-dependency constraints/representations to guide selection/construction of arithmetic expressions, reducing implausible equations by reasoning about units rather than only lexical templates.",
            "evidence_for_mechanism": "Empirical results: UnitDep attains the highest reported accuracy on ASDiv-A (0.78), outperforming LCA++ and matching or slightly exceeding GTS on some settings, suggesting unit constraints improve generalization on diverse arithmetic problems.",
            "evidence_against_mechanism": "Despite unit modeling, UnitDep's accuracy drops on the fully diverse ASDiv dataset (0.37) and on higher-grade problems, indicating unit constraints alone do not yield robust arithmetic algorithmic reasoning.",
            "performance_metrics": "Reported: ASDiv-A accuracy = 0.78; ASDiv accuracy = 0.37. Per-grade accuracies on ASDiv: G1=0.55, G2=0.65, G3=0.51, G4=0.34, G5=0.03, G6=0.01. MathQA-C entry in Table 3 shows '-' for UnitDep (failure on that corpus as reported).",
            "probing_or_intervention_results": "No internal probing/ablation of UnitDep reported in this paper; evidence is comparative performance across datasets of differing lexicon diversity and grade difficulty.",
            "limitations_and_failure_modes": "Fails to generalize reliably on highly diverse lexicon/syntactic patterns and on more complex (higher-grade) problems; still vulnerable to inflated performance from low-diversity datasets and inconsistent dataset annotations.",
            "comparison_to_other_models": "Outperforms LCA++ on ASDiv-A (0.78 vs 0.68) and slightly outperforms or matches GTS in some settings; however, all models achieve low accuracy (~0.36) on the full ASDiv corpus, indicating limited gap between approaches when evaluated on truly diverse problems.",
            "uuid": "e4713.1",
            "source_info": {
                "paper_title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "GTS",
            "name_full": "Goal-driven Tree-structured neural model (GTS)",
            "brief_description": "A neural-network-based math word problem solver that builds tree-structured expressions guided by goal-driven decoding using two-layer gate-feedforward networks.",
            "citation_title": "A goal-driven tree-structured neural model for math word problems",
            "mention_or_use": "use",
            "model_name": "GTS",
            "model_description": "Neural model (Xie & Sun 2019) using a goal-driven tree-structured decoding approach with two-layer gate-feedforward networks to generate equation trees from problem text.",
            "arithmetic_task_type": "Arithmetic MWPs including single-step and multi-step operations; aims to generate expression trees representing solutions.",
            "mechanism_hypothesis": "Neural tree-structured generation can learn compositional expression-building, potentially enabling some algorithmic reasoning by constructing expression trees conditioned on textual goals rather than purely matching templates.",
            "evidence_for_mechanism": "GTS attains high accuracy on low-diversity corpus MathQA-C (0.86), and competitive performance on ASDiv-A (0.68), indicating neural tree generation can capture useful mappings; however performance drops on highly diverse ASDiv (0.36), showing limits to generalization.",
            "evidence_against_mechanism": "Substantial performance degradation on ASDiv and on higher-grade problems (G5/G6 ~0.07/0.01) suggests the model does not perform robust algorithmic arithmetic reasoning and is sensitive to training corpus diversity and complexity.",
            "performance_metrics": "Reported: MathQA-C accuracy = 0.86; ASDiv-A accuracy = 0.68 (noted significantly lower than MathQA-C with p&lt;0.01); ASDiv accuracy = 0.36. Per-grade accuracies on ASDiv: G1=0.64, G2=0.60, G3=0.47, G4=0.34, G5=0.07, G6=0.01.",
            "probing_or_intervention_results": "Paper reports repeated runs (5 times averaged) and cross-validation; also compares performance across datasets with different lexicon diversity and after filtering inconsistent formulas using SymPy verification, but no fine-grained internal probing of neural mechanisms is provided.",
            "limitations_and_failure_modes": "Fails on complex/higher-grade problems and highly diverse linguistic patterns; performance inflated by test/train similarity; sensitive to noisy/inconsistent dataset annotations; does not exhibit robust arithmetic generalization across diverse MWPs.",
            "comparison_to_other_models": "Outperforms or matches statistical approaches on low-diversity MathQA-C (0.86) but on more diverse ASDiv variants its advantage disappears (GTS ~0.36 equals LCA++ ~0.36 and close to UnitDep ~0.37).",
            "uuid": "e4713.2",
            "source_info": {
                "paper_title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers",
                "publication_date_yy_mm": "2020-07"
            }
        },
        {
            "name_short": "Template/Pattern Matching (mechanism)",
            "name_full": "Equation-template / lexical-pattern memorization mechanism",
            "brief_description": "A non-algorithmic mechanism wherein MWP solvers obtain answers by matching input text to previously seen lexical/syntactic patterns or equation templates rather than performing symbolic arithmetic reasoning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "Template/pattern matching (MWP solvers)",
            "model_description": "Describes the tendency of statistical and learned systems to rely on lexicon and sentence-pattern similarity to training examples to select pre-learned equation templates or generate similar equations; essentially a memorization/generalization-from-templates strategy.",
            "arithmetic_task_type": "Primarily arithmetic MWPs that correspond to recurring sentence patterns and equation templates (single-step and multi-step elementary problems).",
            "mechanism_hypothesis": "Systems perform arithmetic by retrieving or instantiating equation templates learned from training examples that have similar lexicon/syntactic patterns, rather than performing general symbolic or algorithmic computation.",
            "evidence_for_mechanism": "Correlation in paper: corpora with low lexicon usage diversity (low CLD) yield much higher reported solver accuracies; MathQA and similar datasets contain many identical sentence patterns enabling template reuse; Roy & Roth (2017) show performance drops when similar MWPs are removed; ASDiv designed to reduce this effect shows much lower solver performance.",
            "evidence_against_mechanism": "No direct internal model analyses disprove template reliance; however, presence of models (e.g., UnitDep) that use unit reasoning indicates hybrid approaches can mitigate pure template dependence, though not fully eliminate failures.",
            "performance_metrics": "Not directly a model, but explanatory: on corpora with low diversity (MathQA-C, CLD~0.08) systems achieve up to 0.86 (GTS), while on high-diversity ASDiv (CLD~0.49) performance drops to ~0.36, demonstrating the impact of template/matching strategies on apparent performance.",
            "probing_or_intervention_results": "Paper constructs ASDiv to reduce pattern duplicates and measures lexicon usage diversity (LD, CLD) via BLEU-derived metric, then evaluates solvers; also filters MathQA with SymPy to expose annotation inconsistencies. These interventions show that reducing pattern overlap reduces solver accuracy, supporting template-matching hypothesis.",
            "limitations_and_failure_modes": "Leads to over-optimistic evaluation when datasets have low diversity or high train-test pattern overlap; fails to generalize to novel lexical/syntactic phrasing, higher-grade multi-step reasoning, or problems requiring external domain knowledge; sensitive to noisy/inconsistent annotations in datasets.",
            "comparison_to_other_models": "Explains differences among LCA++, UnitDep, and GTS across datasets: high accuracies on low-diversity corpora likely reflect template reuse, while similar low accuracies across diverse datasets indicate all approaches are vulnerable to this failure mode.",
            "uuid": "e4713.3",
            "source_info": {
                "paper_title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers",
                "publication_date_yy_mm": "2020-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Solving general arithmetic word problems",
            "rating": 2
        },
        {
            "paper_title": "Unit Dependency Graph and its Application to Arithmetic Word Problem Solving",
            "rating": 2
        },
        {
            "paper_title": "A goal-driven tree-structured neural model for math word problems",
            "rating": 2
        },
        {
            "paper_title": "A Meaning-based English Math Word Problem Solver with Understanding, Reasoning and Explanation",
            "rating": 2
        },
        {
            "paper_title": "A Meaning-based Statistical English Math Word Problem Solver",
            "rating": 2
        },
        {
            "paper_title": "MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms",
            "rating": 2
        },
        {
            "paper_title": "How well do computers solve math word problems? Large-scale Dataset construction and evaluation",
            "rating": 1
        },
        {
            "paper_title": "Parsing algebraic word problems into equations",
            "rating": 1
        }
    ],
    "cost": 0.012994499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers</h1>
<p>Shen-Yun Miao, Chao-Chun Liang, and Keh-Yih Su<br>Institute of Information Science, Academia Sinica, Taiwan<br>miao.iisas@gmail.com, {ccliang, kysu}@iis.sinica.edu.tw</p>
<h4>Abstract</h4>
<p>We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms of both language patterns and problem types) English math word problem (MWP) corpus for evaluating the capability of various MWP solvers. Existing MWP corpora for studying AI progress remain limited either in language usage patterns or in problem types. We thus present a new English MWP corpus with 2,305 MWPs that cover more text patterns and most problem types taught in elementary school. Each MWP is annotated with its problem type and grade level (for indicating the level of difficulty). Furthermore, we propose a metric to measure the lexicon usage diversity of a given MWP corpus, and demonstrate that $A S D i v$ is more diverse than existing corpora. Experiments show that our proposed corpus reflects the true capability of MWP solvers more faithfully.</p>
<h2>1 Introduction</h2>
<p>Human math/science tests have been considered more suitable for evaluating AI progress than the Turing test (Clark and Etzioni, 2016). Among them, math word problems (MWPs) are frequently chosen to study natural language understanding and simulate human problem solving (Bakman, 2007; Mukherjee and Garain, 2008; Liang et al., 2016), because the answer is not a span within the given problem text that can be directly extracted. Table 1 shows a typical example of MWP, which consists of a few sentences that involve quantities.</p>
<p>Current MWP corpora can be classified into four categories: (1) the Number Word Problem corpus (Shi et al., 2015), which contains number word problems only; (2) the Arithmetic Word Problem corpora (Hosseini et al., 2014; Roy et al., 2015), which involve the four basic arithmetic operations</p>
<h2>Math Word Problem</h2>
<p>A sandwich is priced at $\$ 0.75$. A cup of pudding is priced at $\$ 0.25$. Tim bought 2 sandwiches and 4 cups of pudding. How much money should Tim pay?
Solution: $0.75 \times 2+0.25 \times 4=2.5$
Table 1: A math word problem
(addition, subtraction, multiplication and division) with either single-step or multi-step operations; (3) the Algebraic Word Problem corpora (Kushman et al., 2014; Koncel-Kedziorski et al., 2015; Roy and Roth, 2017; Upadhyay \&amp; Chang, 2015; Wang et al., 2017), which focus on algebraic MWPs; and (4) the Mixed-type MWP corpora (Huang et al., 2016, Ling et al., 2017, Amini et al., 2019), which are large-scale collections of either daily algebra or GRE/GMAT examination MWPs. Table 2 is a comparison of existing English MWP corpora.</p>
<p>However, these existing corpora are either limited in terms of the diversity of the associated problem types (as well as lexicon usage patterns), or lacking information such as difficulty levels. For example, categories (1), (2), and (3) collect only certain types of MWPs. On the other hand, although large-scale mixed-type MWP corpora contain more problem types, the annotated answers or formulas are sometimes inconsistent, and the corresponding difficulty level is usually not provided.</p>
<p>Furthermore, low-diversity corpora are typically characterized by highly similar problems, which usually yields over-optimistic results (Huang et al., 2016) (as the answer frequently can be simply obtained from the existing equation template associated with the most similar MWP in the training-set). Roy and Roth (2017) shown significantly lowered performance if highly similar MWPs are removed. Therefore, dataset diversity is more critical than the dataset size for accurately judging the true capability of an MWP solver.</p>
<p>We thus present ASDiv (Academia Sinica Diverse MWP Dataset), a new MWP corpus that contains diverse lexicon patterns with wide problem</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Corpus</th>
<th style="text-align: center;">MWP category</th>
<th style="text-align: center;">Annotation</th>
<th style="text-align: center;">Size</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Dolphin1878 (Shi et al., 2015)</td>
<td style="text-align: center;">Number-word problems</td>
<td style="text-align: center;">Equation/answer</td>
<td style="text-align: center;">1,878</td>
</tr>
<tr>
<td style="text-align: center;">AI2 (Hosseini et al., 2014)</td>
<td style="text-align: center;">Arithmetic word problems</td>
<td style="text-align: center;">Equation/answer</td>
<td style="text-align: center;">395</td>
</tr>
<tr>
<td style="text-align: center;">IL (Roy et al., 2015)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">562</td>
</tr>
<tr>
<td style="text-align: center;">AllArith (Roy and Roth, 2017)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">831</td>
</tr>
<tr>
<td style="text-align: center;">KAZB (Kushman et al., 2014)</td>
<td style="text-align: center;">Algebraic word problems</td>
<td style="text-align: center;">Equation/answer</td>
<td style="text-align: center;">514</td>
</tr>
<tr>
<td style="text-align: center;">ALGES (Koncel-Kedziorski et al., 2015)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equation/answer</td>
<td style="text-align: center;">508</td>
</tr>
<tr>
<td style="text-align: center;">DRAW (Upadhyay \&amp; Chang, 2015)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Equation/answer/template</td>
<td style="text-align: center;">1,000</td>
</tr>
<tr>
<td style="text-align: center;">Dolphin18K (Huang et al., 2016)</td>
<td style="text-align: center;">Arithmetic/algebraic + domain knowledge problems</td>
<td style="text-align: center;">Equation/answer</td>
<td style="text-align: center;">18 K</td>
</tr>
<tr>
<td style="text-align: center;">AQuA (Ling et al., 2017)</td>
<td style="text-align: center;">Arithmetic/algebraic + domain knowledge problems</td>
<td style="text-align: center;">Rationale/answer <br> (multi-choice problems)</td>
<td style="text-align: center;">100 K</td>
</tr>
<tr>
<td style="text-align: center;">MathQA (Amini et al., 2019)</td>
<td style="text-align: center;">Arithmetic/algebraic + domain knowledge problems</td>
<td style="text-align: center;">Decomposed-linear formula/answer <br> (multi-choice problems)</td>
<td style="text-align: center;">37 K</td>
</tr>
<tr>
<td style="text-align: center;">ASDiv</td>
<td style="text-align: center;">Arithmetic/algebraic + domain knowledge problems</td>
<td style="text-align: center;">Equation/answer + grade-level/problem-type</td>
<td style="text-align: center;">2,305</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of different English MWP corpora
type coverage. Each problem provides consistent equations and answers. It is further annotated with the corresponding problem type and grade level, which can be used to test the capability of a system and to indicate the difficulty level of a problem, respectively. The diverse lexicon patterns can be used to assess whether an MWP solver obtains answers by understanding the meaning of the problem text, or simply by finding an existing MWP with similar patterns (Huang et al., 2016). Problem type diverseness is crucial for evaluating whether a system is competitive with humans when solving MWPs of various categories. Besides, to assess text diversity, we propose a lexicon usage diversity metric to measure the diversity of an MWP corpus.</p>
<p>This paper makes the following contributions: (1) We construct a diverse (in terms of lexicon usage), wide-coverage (in problem type), and publicly available ${ }^{1}$ MWP corpus, with annotations that can be used to assess the capability of different systems. (2) We propose a lexicon usage diversity metric to measure the diversity of an MWP corpus and use it to evaluate existing corpora. (3) We show that the real performance of state-of-the-art (SOTA) systems is still far behind human performance if evaluated on a corpus that mimics a real human test.</p>
<h2>2 Problem Type</h2>
<p>A problem type $(P T)$ indicates a crucial math operation pattern for solving an MWP. As MWPs of</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>the same problem type share a similar pattern (in language usages, logic representation, or inferences), they thus indicate stereotypical math operation patterns that could be adopted to solve an MWP (Liang et al., 2018). In ASDiv, each MWP is annotated with a specific $P T$ taught at elementary schools. Some examples of selected PTs are shown in Table 5 (Appendix). Currently, we provide 24 different common PTs and classify them into three main categories according to the operations involved and illustrated below. These PTs are usually specified in math textbooks and mostly covered in elementary schools.</p>
<p>Basic arithmetic operations: This category includes: Addition, Subtraction, Difference, Multiplication, three different Divisions (i.e., common-division, floor-division, and ceil-division), Sum, Surplus, Number-Operation, three different Time-Variant-Quantities (TVQ), and Multi-step. The first seven types are self-explanatory. Number-Operation indicates that the problem description consists mainly of numbers and their relations. $T V Q^{2}$ denotes an entity-state related variable (e.g., initial/current/final-state and change) whose value is updated sequentially according to a sequence of events described in an MWP. Last, in a Multi-step problem, the answer is obtained from multiple arithmetic operations.</p>
<p>Aggregative operations: This category includes: (1) Comparison, (2) Set-Operation, (3) Ra</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Lexicon usage diversity of various corpora.
tio, (4) Number-Pattern, (5) Algebra-1, and (6) Al-gebra-2. The first three types are self-explanatory. Number-Pattern refers to the problems which involve deducing a pattern from a sequence of integers (Table 5 (Appendix) shows an example). Al-gebra-1 and Algebra-2 are algebraic problems with one and two unknown variables, respectively.</p>
<p>Additional domain knowledge required: This category includes Greatest Common Divisor, Least Common Multiple, Geometry, and UnitTrans. Additional geometric knowledge (e.g., area $=$ length * width) is required in Geometry problems. UnitTrans means that the answer is obtained via conversion to the metric system (e.g., converting 'miles to 'kilometers').</p>
<h2>3 ASDiv Math Word Problem Corpus</h2>
<p>This corpus was designed based on the following guidelines: (1) The corpus should be as diverse as possible in terms of lexicon usage so that the answer is less likely to be obtained via mechanical/statistical pattern matching without understanding the content. (2) The corpus should cover most PTs found in primary school so that it can approximate real human tests. (3) The corpus should be annotated with sufficient information so that it can be used not only to assess the capability of various systems but also to facilitate system development.</p>
<h3>3.1 Corpus Diversity Metrics</h3>
<p>We first propose a lexicon usage diversity metric, in terms of BLEU (Papineni et al., 2002), to measure the degree of diversity of a given corpus. This metric is from 0 to 1 ; higher value indicates the corpus is more diverse. We first use Stanford CoreNLP (Manning et al., 2014) to tokenize and tag POSs, and then use NLTK (Bird et al., 2004) to lemmatize each token. Furthermore, we normalize the original sentences with: (1) stop word removal; and (2) named entity and quantity normalization,
which replace the associated person names and quantity values with meta symbols in an MWP (i.e., two MWPs are regarded as identical if they differ only in names or quantity-values). This thus places the focus on essential words that matter in the MWP. The obtained sequence is then used to measure the lexicon usage diversity specified below.</p>
<p>Let $P=\left{P_{1}, P_{2}, \ldots, P_{M}\right}$ be a specific set of MWPs in a given corpus with the same $P T$, where $P_{i}$ is the $i$-th MWP in $P$. For a given $P_{i}$, we define its lexicon usage diversity (LD) of $P_{i}$ as</p>
<p>$$
L D_{i}=1-\max <em i="i">{i, t \neq i} \frac{B L E U\left(P</em>
$$}, P_{j}\right)+B L E U\left(P_{i}, P_{i}\right)}{2</p>
<p>where $\operatorname{BLEU}\left(P_{i}, P_{j}\right)$ is the BLEU score between $P_{i}$ and $P_{j}(\mathrm{j} \neq \mathrm{i}, j={1,2, \ldots, M})$. We measure the BLEU score bi-directionally with n-grams up to $n=4$. This measure is mainly used to identify repeated usage of lexicon and phrases; $n=4$ suffices for this case. $L D_{i}$ evaluates the lexicon diversity between $P_{i}$ and all $P_{j}(i \neq j)$. Furthermore, the mean of all $L D_{i}$ (under the same corpus) can be used to indicate the corpus lexicon diversity (CLD). Adding a new MWP with a low LD to an existing corpus introduces little new information to the corpus; thus, it should be either discarded or revised. This diversity metric can help the corpus constructor to decide whether an MWP can be directly adopted or not.</p>
<h3>3.2 Challenges in Constructing a LargeScale MWP Dataset</h3>
<p>Since MathQA is the second-largest dataset in Table 2 (with 37K MWPs), and is cleaner (Amini et al., 2019) than the largest one (AQuA), we first evaluate it with the above LD measurement. Figure 1 shows that its CLD is only 0.05 .</p>
<p>To understand the reason for the low diversity of MathQA (LD $=0$ for $85 \%$ of the MathQA MWPs), we investigated this dataset. We observed that MathQA includes various MWP subsets, each of</p>
<p>which shares the same sentence pattern among its members. Figure 1 clearly shows its skewed distribution. Figure 3 (Appendix) shows a subset of which all 105 members share the same sentence pattern.</p>
<p>Since most MWP solvers can only solve arithmetic MWPs, we further selected its arithmetic ${ }^{3}$ subset, generated their corresponding equations according to the annotated formulas, and then solved the equations using the SymPy ${ }^{4}$ package. Afterwards, we verified the consistency between the answer obtained from the annotated formula and the labeled answer. The results show that the annotated formulas of $27 \%$ of the problems do not match their labeled answers.</p>
<p>We randomly inspected 30 inconsistent MWPs and classified them into three error-types: (1) Incorrect formula ( $67 \%$ ), for which the annotated formula cannot be used to solve the given MWP; (2) problematic description ( $23 \%$ ), for which the description text is either incomplete or problematic; (3) valueless answer ( $10 \%$ ), for which the given answer is either wrong or inappropriate. Table 6 (Appendix) illustrates examples of each error-type.</p>
<p>Although building a large corpus via crowdsourcing is a tempting approach, it can result in a poor-quality corpus if the annotation procedure is not well controlled. We believe the quality of the dataset is more important than its size, if they cannot be achieved simultaneously.</p>
<h3>3.3 Corpus Construction</h3>
<p>To account for the problems observed in MathQA, we first collected MWPs from 28 websites and then either pruned the problem or revised the text if it was highly similar to any existing ones (according to the proposed lexicon usage diversity metric). This yielded a total of 2,305 MWPs.</p>
<p>Next, we hired one master-degree research assistant with a background in automatic MWP solving to annotate the problem type, equation, answer, and grade level manually for each MWP. If annotations were provided with the original MWP ( $22.6 \%$ of the source MWPs included equations and answers; $52 \%$ had answers only; $63.5 \%$ included grade-level information), we used it directly; otherwise, we annotated them manually ${ }^{5}$.</p>
<p>Since MWPs are usually clearly specified (with</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Distribution of PT categories (G1 G6)
a sure answer), there is no ambiguous interpretation once the answer is given. Therefore, as opposed to other corpora in which annotations (mostly linguistic attributes) are mainly based on human subjective judgment, the MWP answer/equation annotation is more objective and must be consistent. As a result, human carefulness, instead of human agreement, is a more critical issue in this task. Since an incorrect math expression usually yields an incorrect answer, we used a program to automatically verify the consistency between the annotated equations and the answers. Inconsistent MWPs were re-annotated and checked again. Afterwards, we randomly selected 480 samples ( 20 samples per problem type) to verify the final annotation correctness. All those samples were correct, which confirms our above assertion.</p>
<p>Figure 2 shows the distribution of different problem categories in six grade levels in elementary school. Most arithmetic operations appear in grade levels 1 to 4 , which means students learn basic arithmetic operations in this stage. We further separate Addition/Subtraction from Multiplication/Division to highlight that they are in different difficulty levels for students. Figure 2 also indicates Multiplication/Division is more emphasized in grade 3 and 4. In grades 5 and 6, improved math skills enable students to solve difficult MWPs that require more aggregative operations and additional domain knowledge. Thus, the grade level is a useful indicator of difficulty and can be employed to evaluate the capability of MWP solving systems.</p>
<h3>3.4 LD Distributions of Various Corpora</h3>
<p>We compare the diversity among various MWPs of the same $P T$ (for those corpora without annotated PT Category, diversity is measured over the whole corpus). Lastly, we generate the associated $L D$ distributions (uniformly quantized into 20 intervals between 0 and 1 ) and calculate the corpus lexicon</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">MathQA-C <br> (CLD $=0.08)$</th>
<th style="text-align: center;">ASDiv-A <br> (CLD $=0.50)$</th>
<th style="text-align: center;">ASDiv <br> (CLD $=0.49)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">L</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.36</td>
</tr>
<tr>
<td style="text-align: center;">U</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.37</td>
</tr>
<tr>
<td style="text-align: center;">G</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">$0.68^{#}$</td>
<td style="text-align: center;">$0.36^{#}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Accuracies for different systems (CLD denotes the corpus lexicon diversity; $\mathrm{L}, \mathrm{U}$ and G denote the $L C A++$, UnitDep, and GTS systems respectively. '-` denotes failure on this corpus; # indicates performance is significantly lower than "-C" with $\mathrm{p}&lt;0.01$.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">G1</th>
<th style="text-align: center;">G2</th>
<th style="text-align: center;">G3</th>
<th style="text-align: center;">G4</th>
<th style="text-align: center;">G5</th>
<th style="text-align: center;">G6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">L</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.49</td>
<td style="text-align: center;">0.35</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">U</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.01</td>
</tr>
<tr>
<td style="text-align: center;">G</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.01</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance of various grade levels on the ASDiv. L/U/G are the same as that in Table 3.
diversity (CLD, Section 3.1) on corpora frequently adopted for comparing various systems: (1) AI2, (2) IL, (3) KAZB, (4) ALGES, (5) DRAW, (6) AllArith, and (7) MathQA.</p>
<p>Figure 1 shows the distributions of CLD for various corpora: there are about $85 \%, 28 \%, 22 \%$ and $20 \%$ identical MWPs (these numbers are the percentages of MWPs with $L D_{i}=0$ w.r.t. each dataset) in MathQA, IL, AI2 and ALGES corpora respectively, whereas ASDiv contains none. We also evaluate syntactic pattern diversity (in terms of POS $n$-gram) and the diversity between MWPs in the training set and the test set. Both yield similar trends, too (details are given in the Appendix).</p>
<h2>4 Experiments</h2>
<p>To study the correlation between CLD and system performance, we selected three SOTA MWP solvers to conduct the experiments: two based on statistical models, $L C A++$ (Roy and Roth, 2015) and UnitDep (Roy and Roth, 2017); and one using a neural network which adopts two-layer gate-feedforward networks for a Goal-driven Tree-structured approach (GTS) (Xie et al., 2019).</p>
<p>Since the selected MWP solvers solve only arithmetic MWPs, we first collected 4,117 MWPs from MathQA to construct a subset that its associated formulas satisfy the following two conditions: (1) they involve only arithmetic operations; and (2) they contain neither external constants (which would necessitate external domain knowledge to solve the problem and is out of the scope of this work) nor reused operands (which rarely occur and would complicate the solution procedure). We filtered out inconsistent problems (specified in Section 3.2) and termed the remaining 3,000 MWPs as</p>
<p>MathQA-C dataset (-C for consistent) to evaluate the performance. Similarly, we extracted a subset of 1,218 MWPs that involve only arithmetic operations (and also satisfy the constraints mentioned above) from ASDiv, and termed this the $A S D i v-A$ dataset (-A for arithmetic). The CLDs for MathQA-C and ASDiv-A were found to be 0.08 and 0.50 , respectively. Also, $\mathrm{LD}=0$ for $82 \%$ of the MathQA-C MWPs.</p>
<p>Afterwards, we tested the solvers against three MWP corpora: MathQA-C, ASDiv-A, and ASDiv. MathQA-C is reported with 5-fold cross-validation accuracy. For ASDiv-A and ASDiv, we randomly split the MWPs of each $P T$ into five nearly equallysized subsets, and report the 5 -fold cross-validation accuracy. For GTS system, we repeated the experiment 5 times and obtained the averaged answer accuracy.</p>
<p>Table 3 compares the answer accuracies of various systems. We observe that the overall performance is only around $36 \%$ on ASDiv, which shows that the performance of the current SOTA systems still is not competitive with human performance, and that CLD is correlated with the system performance (i.e., lower diversity implies higher performance) and is a useful metric to evaluate existing corpora. Table 4 further shows the accuracy of different grade levels on ASDiv: the performance of grades 5 and 6 are significantly lower than the performance of grade 1 to 4 . As accuracy is highly correlated with the grade level, the grade level is a useful index for indicating the difficulty of MWPs.</p>
<h2>5 Conclusion and Future Work</h2>
<p>We present an MWP corpus which not only is highly diverse in terms of lexicon usage but also covers most problem types taught in elementary school. Each MWP is annotated with the corresponding problem type, equation, and grade level, which are useful for machine learning and assessing the difficulty level of each MWP. We also propose a metric to measure the diversity of lexicon usage of a given corpus. In terms of this metric, we show that in comparison with those corpora widely adopted to compare systems, ours is more suitable for assessing the real performance of an MWP solver. Last, we conduct experiments to show that a low-diverse MWP corpora will exaggerate the true performance of SOTA systems (we are still far behind human-level performance), and that grade level is a useful index for indicating the difficulty of an MWP.</p>
<h2>References</h2>
<p>Aida Amini, Saadia Gabriel, Peter Lin, Rik KoncelKedziorski, Yejin Choi, Hannaneh Hajishirzi. 2019. MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms. In Proceedings of NAACL-HLT 2019.</p>
<p>Yefim Bakman. 2007. Robust understanding of word problems with extraneous information. http://lanl.arxiv.org/abs/math.GM/0701393.</p>
<p>Steven Bird and Edward Loper. 2004. NLTK: The Natural Language Toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, 2004, pages 214-217.</p>
<p>Peter Clark and Oren Etzioni. 2016. My Computer is an Honor Student - but how Intelligent is it? Standardized Tests as a Measure of AI. AI Magazine, pages $5-12$.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin and Wei-Ying Ma. 2016. How well do computers solve math word problems? Large-scale Dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Berlin, Germany, Association for Computational Linguistics (ACL), 2016, pages 887-896.</p>
<p>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. 2014. Learning to solve arithmetic word problems with verb Categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014.</p>
<p>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics (TACL), 3:585-597.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. Association for Computational Linguistics (ACL), 1:271-281, Jun. 2014.</p>
<p>Chao-Chun Liang, Shih-Hong Tsai, Ting-Yun Chang, Yi-Chung Lin and Keh-Yih Su. 2016. A Meaningbased English Math Word Problem Solver with Understanding, Reasoning and Explanation. In Proceedings of the 26th International Conference on Computational Linguistics (COLING): System Demonstrations, pages 151-155, Osaka, Japan, December 11-17 2016.</p>
<p>Chao-Chun Liang, Yu-Shiang Wong, Yi-Chung Lin and Keh-Yih Su. 2018. A Meaning-based Statistical English Math Word Problem Solver. In Proceedings of NAACL-HLT 2018.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer and Phil Blunsom. 2017. Program Induction for Rationale Generation: Learning to Solve and Explain Algebraic Word Problems. Association for Computational Linguistics (ACL), 2017.</p>
<p>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60.</p>
<p>Anirban Mukherjee and Utpal Garain. 2008. A review of methods for automatic understanding of natural language mathematical problems. Artificial Intelligence Review, 29(2):93-122.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), 2002, pages 311318.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015, pages $1743-1752$.</p>
<p>Subhro Roy and Dan Roth. 2017. Unit Dependency Graph and its Application to Arithmetic Word Problem Solving. AAAI-2017.</p>
<p>Shuming Shi, Yuehui Wang, Chin-Yew Lin, Xiaojiang Liu, and Yong Rui. 2015. Automatically Solving Number Word Problems by Semantic Parsing and Reasoning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal, 2015, pages $1132-1142$.</p>
<p>Shyam Upadhyay and Ming-Wei Chang. 2015. DRAW: A challenging and diverse algebra word problem set. Number MSR-TR-2015-78</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. 2017. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.
Zhipeng Xie, and Shichao Sun. 2019. A goal-driven tree-structured neural model for math word problems. In Proceedings of the 28th International Joint Conference on Artificial Intelligence. AAAI Press.</p>
<h1>Appendix</h1>
<h2>Appendix A: Examples of a few Selected Problem Types</h2>
<p>Table 5 shows examples of selected types in "Basic arithmetic operations", "Aggregative operations", and "Additional domain knowledge required" categories.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Problem type</th>
<th style="text-align: center;">Examples</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Basic arithmetic operations</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Number-Operation</td>
<td style="text-align: center;">I have 3 hundreds, 8 tens, and 3 ones. What number am I?</td>
</tr>
<tr>
<td style="text-align: center;">TVQ-Initial</td>
<td style="text-align: center;">Tim's cat had kittens. He gave 3 to Mary and 6 to Sara. He now has 9 kittens. How many kittens did he have to start with?</td>
</tr>
<tr>
<td style="text-align: center;">TVQ-Change</td>
<td style="text-align: center;">For the school bake sale, Wendy made pastries. She baked 41 cupcakes and 31 cookies. After the sale, she had 32 to take back home. How many pastries did she sell?</td>
</tr>
<tr>
<td style="text-align: center;">TVQ-Final</td>
<td style="text-align: center;">Melanie had 7 dimes in her bank. Her dad gave her 8 dimes and her mother gave her 4 dimes. How many dimes does Melanie have now?</td>
</tr>
<tr>
<td style="text-align: center;">Multi-Step</td>
<td style="text-align: center;">They served a total of 179 adults and 141 children, if 156 of all the people they served are male, how many are female? (combination of multiple operations) (Equation: $x=(179+141)-156)$</td>
</tr>
<tr>
<td style="text-align: center;">Aggregative operations</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Algrbra-1</td>
<td style="text-align: center;">Maddie, Luisa, and Amy counted their books. Maddie had 15 books. Luisa had 18 books. Together, Amy and Luisa had 9 more books than Maddie. How many books did Amy have? <br> (Equation: $(x+18)-9=15$, where $x$ : "money of Amy")</td>
</tr>
<tr>
<td style="text-align: center;">Algrbra-2</td>
<td style="text-align: center;">The cost of a private pilot course is $\$ 1,275$. The flight portion costs $\$ 625$ more than the ground school portion. What is the cost of each? <br> (Equation: $x+y=1275 ; x=y+625$, where $x$ :The cost of flight portion; $y$ :The cost of ground school portion)</td>
</tr>
<tr>
<td style="text-align: center;">Comparison</td>
<td style="text-align: center;">A bookstore was selling 2 books for $\$ 15.86$. You could buy 7 books for $\$ 55.93$ online. Which place has a lower unit price?</td>
</tr>
<tr>
<td style="text-align: center;">Set-Operation</td>
<td style="text-align: center;">Sarah's team played 8 games of basketball. During the 8 games her team's scores were $69,68,70,61,74,62,65$ and 74 . What is the mean of the scores?</td>
</tr>
<tr>
<td style="text-align: center;">Ratio</td>
<td style="text-align: center;">There are 43 empty seats and 7 occupied seats on an airplane. What is the ratio of the number of occupied seats to the total number of seats?</td>
</tr>
<tr>
<td style="text-align: center;">Number-Pattern</td>
<td style="text-align: center;">The Wholesome Bakery baked 5 loaves of bread on Wednesday, 7 loaves of bread on Thursday, 10 loaves of bread on Friday, 14 loaves of bread on Saturday, and 19 loaves of bread on Sunday. If this pattern continues, how many loaves of bread will they bake on Monday?</td>
</tr>
<tr>
<td style="text-align: center;">Additional domain knowledge required</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">G.C.D.</td>
<td style="text-align: center;">A teacher is to arrange 60 boys and 72 girls in rows. He wishes to arrange them in such a way that only boys or girls will be there in a row. Find the greatest number of students that could be arranged in a row.</td>
</tr>
<tr>
<td style="text-align: center;">L.C.M.</td>
<td style="text-align: center;">On a track for remote-controlled racing cars, racing car A completes the track in 28 seconds, while racing car B completes it in 24 seconds. If they both start at the same time, after how many seconds will they be side by side again?</td>
</tr>
<tr>
<td style="text-align: center;">UnitTrans</td>
<td style="text-align: center;">Mrs. Hilt will buy a new pair of shoes in 11 days. How many minutes must she wait before she can buy her new pair of shoes?</td>
</tr>
</tbody>
</table>
<p>Table 5: Examples of selected problem types</p>
<h1>Appendix B: Problematic MWPs in MathQA</h1>
<p>Table 6 shows examples of inconsistent MWPs in MathQA, and Figure 3 shows examples using the same sentence pattern.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Error types</th>
<th style="text-align: center;">Examples</th>
<th style="text-align: center;">Annotated Formula</th>
<th style="text-align: center;">Labeled Answer</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Incorrect formula $(67 \%)$</td>
<td style="text-align: center;">Problem-1 <br> "What is the 25 th digit to the right of the decimal point in the decimal form of $6 / 11$ ?"</td>
<td style="text-align: center;">Annotation Formula: divide(n1,n2) <br> Desired Formula: Not available ${ }^{6}$</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Problem-2 <br> "In two triangles, the ratio of the areas is $4: 3$ and that of their heights is 3 : 4 . find the ratio of their bases."</td>
<td style="text-align: center;">Annotation Formula: multiply(n0,n0)</td>
<td style="text-align: center;">multiply(n1,n1)</td>
</tr>
<tr>
<td style="text-align: center;">Problematic description $(23 \%)$</td>
<td style="text-align: center;">Problem-3 <br> "The lcm of two numbers is 495 and their hcf is 5 . if the sum of the numbers is $\underline{10}$, then their difference is" <br> Desired text: "... sum of the numbers is $\underline{100}$ "</td>
<td style="text-align: center;">Annotation Formula: multiply(n0,n1)</td>
<td style="text-align: center;">divide(#0,n2)</td>
</tr>
<tr>
<td style="text-align: center;">Valueless answer $(10 \%)$</td>
<td style="text-align: center;">Problem-4 <br> " $9886+x=13200$, then $x$ is ?" <br> Options: a) 3327, b)3237, c)3337, <br> d) 2337, e)none of these</td>
<td style="text-align: center;">Annotation Formula: <br> subtract(n1,n0)</td>
<td style="text-align: center;">Annotation Option (e): none of these <br> Desired Answer: 3414</td>
</tr>
</tbody>
</table>
<p>Table 6: Some examples of inconsistent answers on MathQA.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Figure 3: Examples in MathQA with the same sentence pattern (after sentence normalization). These MWPs all share the same sentence pattern, " $a$ train running at the speed of [NUM1] km/hr crosses a pole in [NUM2] sec. [what is |find] the length of the train?".</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h2>Appendix C: Additional Experiments for Corpus Diversity Metrics</h2>
<p>We also provide a syntactic pattern diversity to measure the syntactic diversity of an MWP. Let $P=\left{P_{1}, P_{2}, \ldots, P_{M}\right}$ be a specific set of MWPs in a given corpus with the same problem type, where $P_{i}$ is the $i$-th MWP in $P$, and $T_{i}$ is the corresponding POS sequence of $P_{i}$. For example, "NNP VBZ CD NNS" is the POS tagging sequence for "Mary has 5 books.". For a given $P_{i}$, we define its syntactic pattern diversity $\left(S D_{i}\right)$ as</p>
<p>$$
S D_{i}=1-\max <em i="i">{j, j \neq i} \frac{B L E U\left(T</em>
$$}, T_{j}\right)+B L E U\left(T, T_{i}\right)}{2</p>
<p>where $B L E U\left(T_{i}, T_{j}\right)$ is measured between $T_{i}$ and $T_{j}(\mathrm{j} \neq \mathrm{i}, j={1,2, \ldots, M})$. Figure 4 shows that there are $87 \%, 54 \%, 46 \%$ and $33 \%$ identical syntactic patterns (these numbers are the percentages of MWPs with $S D_{i}=0$ w.r.t. each dataset) in the MathQA, IL, AI2, and ALGES corpora, respectively, while ASDiv only has $4 \%$. This shows that our corpus is also more diverse in terms of syntactic patterns.</p>
<p>We also measure the diversity between the testset and the training-set, as the similarity between them is a critical factor ${ }^{8}$ for causing exaggerated performance. The $L D$ and $S D$ metrics between the test-set and the training-set can be obtained by modifying the previous formulas to</p>
<p>$$
\begin{aligned}
L D_{i}=1- &amp; \max <em i="i">{j} \frac{B L E U\left(P</em> \
&amp; P_{i} \in D S_{\text {test }} \text { and } P_{j} \in D S_{\text {train }} \
S D_{i}=1- &amp; \max }, P_{j}\right)+B L E U\left(P_{j}, P_{i}\right)}{2<em i="i">{j} \frac{B L E U\left(T</em> \
&amp; T_{i} \in D S_{\text {test }} \text { and } T_{j} \in D S_{\text {train }}
\end{aligned}
$$}, T_{j}\right)+B L E U\left(T_{j}, T_{i}\right)}{2</p>
<p>where $D S_{\text {test }}$ and $D S_{\text {train }}$ are all the MWPs in the test set and the training set, respectively. For a given problem $P_{i}$ from the test set, $L D_{i}$ and $S D_{i}$ denote the lexicon-pattern and syntactic-pattern diversity between $P_{i}$ and all the problems $P_{j}$ (in the training set), respectively. If $P_{i}$ has a low diversity index, it can be easily solved via a training set MWP with similar patterns.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Syntactic pattern diversity of various corpora
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Lexicon usage diversity of various corpora: test-set versus training-set
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Syntactic pattern diversity of various corpora: test-set versus training-set
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Lexicon usage diversity measured only within the test-set for various corpora. Here, the suffix "-test" denotes that it is measured only within the test-set.</p>
<p><sup id="fnref4:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>For AI2, IL, KAZB, ALGES, AllArith, DRAW, and MathQA, we follow their own either n-fold cross-validation setting or the train/dev/test splits originally specified in each dataset. For MathQAC and ASDiv-A, we follow the same n-fold crossvalidation setting specified in Section 4. Figure 5 shows the LD between the test-set and the trainingset for various corpora.</p>
<p>Also, though not directly shown in this figure, the lexicon diversity indices of $48 \%$ of the MWPs in our ASDiv-A are larger than or equal to 0.5 . In contrast, they are $35 \%, 34 \%, 29 \%, 23 \%, 23 \%, 18 \%$ and $7 \%$ in AI2, IL, ALGES, KAZB, AllArith, DRAW, and MathQA-C respectively. These statistics suggest that our corpus is more diverse (and thus more difficult) than other well-known MWP corpora. Figure 6 shows the SD between the testset and the training-set for different corpora. Again, this shows that our corpus also contains more diverse POS sequences.</p>
<p>Last, in Figure 5, MathQA actually possesses the highest CLD between its official test-set and train-ing-set: 0.85 , surprisingly higher than that of all other corpora (e.g., they are $0.52,0.44,0.42$ and 0.42 for ASDiv-A, IL, AllArith, and AI2, respectively). In comparison with its CLD across the whole corpus ( 0.05 in Figure 1), 0.85 is much higher. It is thus suspected that the MWPs within its test/training-set might be very similar to each other. As explained in Footnote #7, a test-set with very similar MWPs hinders the assessment of the true performance of an MWP solver. We thus further compare the CLDs within the test set for a few corpora. Figure 7 illustrates that the CLD $=0.27$ of MathQA measured within the test set is actually low in comparison with the corresponding CLDs of other corpora (e.g., the means are 0.57 for ASDiv-A and ASDiv corpora).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ However, we also need another critical factor - the $C L D$ value within the test-set - to assess the true value of a given corpus. Suppose we take an MWP with a high LD value from the existing training-set, and then duplicate it 300 times to create a new test-set. This would still result in a high $C L D$ value between the training-set and this new test-set. However,&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>this setting would be meaningless in terms of assessing the true performance of MWP solvers. Likewise, we could also enlarge the training-set by duplicating MWPs without affecting the $C L D$ value against the test-set. However, it would be also meaningless as no new information would be provided.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8880 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8880</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8880</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-155.html">extraction-schema-155</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-2db3eb03b0a9fedde37066673532804d3e224a4a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2db3eb03b0a9fedde37066673532804d3e224a4a" target="_blank">Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work demonstrates that language models trained directly on sequences derived directly from chemical file formats like XYZ files, Crystallographic Information files, or Protein Data Bank files (PDBs) can directly generate molecules, crystals, and protein binding sites in three dimensions.</p>
                <p><strong>Paper Abstract:</strong> Language models are powerful tools for molecular design. Currently, the dominant paradigm is to parse molecular graphs into linear string representations that can easily be trained on. This approach has been very successful, however, it is limited to chemical structures that can be completely represented by a graph -- like organic molecules -- while materials and biomolecular structures like protein binding sites require a more complete representation that includes the relative positioning of their atoms in space. In this work, we show how language models, without any architecture modifications, trained using next-token prediction -- can generate novel and valid structures in three dimensions from various substantially different distributions of chemical structures. In particular, we demonstrate that language models trained directly on sequences derived directly from chemical file formats like XYZ files, Crystallographic Information files (CIFs), or Protein Data Bank files (PDBs) can directly generate molecules, crystals, and protein binding sites in three dimensions. Furthermore, despite being trained on chemical file sequences -- language models still achieve performance comparable to state-of-the-art models that use graph and graph-derived string representations, as well as other domain-specific 3D generative models. In doing so, we demonstrate that it is not necessary to use simplified molecular representations to train chemical language models -- that they are powerful generative models capable of directly exploring chemical space in three dimensions for very different structures.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8880.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8880.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-AC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model — Atom+Coordinate tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer (GPT-style) trained to predict next tokens where each placed atom is represented by an atom/residue token plus three coordinate tokens (x,y,z) with fixed numerical precision; used to directly generate 3D molecules (XYZ), crystals (CIF), and protein binding-site point clouds (PDB) by sampling token sequences and interpreting them as chemical files.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-AC</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoregressive Transformer (GPT-style)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported range: ~1 million to 100 million parameters; architecture examples: 12 layers, embedding size 128–1024, 4–12 attention heads</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Task-dependent datasets derived from chemical file formats: ZINC molecules converted to XYZ conformers (250K molecules, rdkit conformers), Perov5 (perovskite CIFs, ~18.9K), MP20 CIFs (~45.2K), and a protein pocket dataset (~180K protein-ligand pairs) with PDB-derived pocket subsequences; numeric coordinates rounded to fixed precision (1–3 decimals) and optionally augmented via random rotations.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multiple: small-molecule generative chemistry (drug-like molecules), crystalline materials generation (materials science), and protein binding-site (structure-based drug discovery, biomolecular design).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct autoregressive generation / next-token prediction: sample token sequences in file format encoding (atom token + x,y,z coordinate tokens per atom) and parse sampled sequences into XYZ/CIF/PDB files to obtain 3D point clouds.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>High novelty reported for sampled sets vs training: for ZINC generation LM-AC produced 100% novel and 100% unique molecules (as measured by dataset-level checks); for protein pockets ~89.8% unique residue orderings and 83.6% of generated orderings were novel relative to training. Crystals: novelty/diversity assessed via coverage metrics (COV-R/COV-P) showing high coverage close to training.</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>No task-specific conditioning reported — model learns the data distribution for each domain; application specificity evaluated post-hoc by distributional and property-matching metrics (e.g., QED/SA/MW distributions for molecules, lattice/property statistics for crystals, interatomic distance and residue composition distributions for pockets).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Molecules: validity (via xyz2mol -> rdkit Mol), uniqueness, novelty; distributional similarity via Earth Mover's Distance (WA) on properties: molecular weight (MW), synthetic accessibility (SA), QED. Crystals: structural validity (min interatomic distance >0.5 Å), compositional neutrality, coverage metrics COV-R (recall) and COV-P (precision), WA on density and number of unique elements. Protein pockets: residue-wise composition checks (xyz2mol), inter-residue overlap checks (minimum inter- residue bond distance), interatomic distance distributions, residue-ordering uniqueness.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Molecules (ZINC, numeric precision 2 decimals): LM-AC generated 10K samples with Valid=98.51%, Unique=100%, Novel=100%; WA metrics: MW=1.811, SA=0.026, QED=0.004 (lower WA = closer to training). Crystals (Perov5, precision 3 decimals): structure validity 100.0%, composition valid 98.79%, COV-R=98.78%, COV-P=99.36%, WA (density)=0.089, WA (# elements)=0.028. Crystals (MP20): structure valid 95.81%, composition valid 88.87%, COV-R=99.60%, COV-P=98.55%, WA (density)=0.696, WA (# elements)=0.092. Protein pockets (~10K samples evaluated): ~99% of pockets passed per-residue composition checks, ~5% failed inter-residue overlap checks; interatomic distance distributions and counts of C/N/O closely matched training; residue-ordering uniqueness: generated 89.8% unique orderings and 83.6% of those were novel compared to training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared with both graph/string generative models and domain-specific 3D generative models: LM-AC achieved competitive or superior performance on many metrics. For molecules, LM-AC performed comparably or better than graph models (JTVAE, CGVAE, DGMG) and 3D models (GSchNet, ENF, EDM) and approached or matched SMILES/SELFIES LMs on distributional WA metrics. For crystals, LM-AC equaled or outperformed several baselines (FTCP, GSchNet variants) and matched the top-performing CDVAE on several metrics; on MP20 LM-AC was best on 3/6 metrics and close otherwise. For protein pockets there were no graph-based baselines able to directly generate full 3D pockets; LM-AC produced geometrically plausible pockets matching training statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Model lacks built-in SE(3) equivariance (no rotation/translation invariance), so it predicts absolute coordinates — requiring data augmentation (random rotations) to help; predicting absolute coordinates is challenging for large structures and vocabulary size explodes with coordinate tokenization (coordinate-level token vocabularies can grow to 100–10K tokens depending on precision); character-level sequences become very long (poorer scaling); generation can produce invalid structures (e.g., ~5% pocket overlap failures, other invalid examples shown), and empirical validation beyond distributional metrics (e.g., physical simulation or experiments) is necessary to confirm usefulness; scaling to larger structures may need much more data and compute.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Character-level tokenization (LM-CH) was evaluated for some tasks but for large PDB pockets was not used because sequences become too long; data augmentation by random rotations improves performance but some models trained without augmentation still approach SOTA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8880.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8880.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how large language models (LLMs) are used to generate or design novel chemicals for specific applications, including details of the models, methods, applications, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM-CH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Model — Character-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive transformer trained at the character level to generate chemical file-format text (characters include element letters, digits, signs, spaces, newlines) so that sampled text can be parsed as XYZ or CIF files to obtain 3D structures; applied to small molecules and crystals in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LM-CH</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>Autoregressive Transformer (GPT-style) — character-level tokenization</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Reported range: ~1 million to 100 million parameters; example architectures: 12 layers, embedding sizes 128–1024, 4–12 attention heads</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Same domain-specific datasets as LM-AC but character-tokenized: ZINC XYZ-derived conformers (250K molecules) and CIF datasets Perov5 and MP20; floating point numbers rounded to set precision (1–3 decimals) and represented as sequences of characters.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Small-molecule generative chemistry (drug-like molecules) and crystalline materials generation (materials science).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Direct autoregressive character-level generation of file-format strings (next-character prediction); parse generated character strings as chemical files (XYZ or CIF) to obtain 3D structures.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_of_chemicals</strong></td>
                            <td>High novelty/uniqueness for molecules reported (100% unique and novel in ZINC sampling), and crystals showed high coverage/validity on Perov5 and competitive performance on MP20 (though somewhat lower than LM-AC).</td>
                        </tr>
                        <tr>
                            <td><strong>application_specificity</strong></td>
                            <td>Model is unconditional for each domain; specificity measured post-hoc via distributional similarity metrics (e.g., property WAs, coverage COV-R/COV-P) rather than conditional/property-optimized generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same domain metrics: molecules — validity (xyz2mol->rdkit), uniqueness, novelty, WA on MW/SA/QED; crystals — structural/compositional validity, coverage (COV-R/COV-P), WA on density and # elements.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Molecules (ZINC): LM-CH generated 10K samples with Valid=90.13%, Unique=100%, Novel=100%; WA metrics: MW=3.912, SA=2.608, QED=0.077 (worse WA than LM-AC indicating larger distributional gaps). Crystals (Perov5): LM-CH achieved structure validity 100.0%, composition valid 98.51%, COV-R=99.60%, COV-P=99.42%, WA (density)=0.071, WA (# elements)=0.036. Crystals (MP20): LM-CH structure valid 84.81%, composition valid 83.55%, COV-R=99.25%, COV-P=97.89%, WA (density)=0.864, WA (# elements)=0.132.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Character-level LM had competitive performance for crystals (especially Perov5) relative to models like CDVAE and GSchNet, but for molecules LM-CH underperformed coordinate-level LM-AC and SMILES/SELFIES LMs on distributional WA metrics and validity.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_challenges</strong></td>
                            <td>Character-level sequences are long for even modest molecule sizes, making long-range dependencies harder to learn; lower sample validity (e.g., 90.13% on ZINC) than LM-AC; not used for protein pocket generation in this study due to sequence length/efficiency concerns; same lacking of SE(3) invariance and vocabulary challenges as LM-AC but differs in tokenization trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>additional_notes</strong></td>
                            <td>Character-level token vocabulary is small (~30 tokens) but sequence length increases; LM-CH can learn file syntax in addition to coordinates/elements but may be less effective at precise coordinate generation than coordinate-tokenized models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models can learn complex molecular distributions <em>(Rating: 2)</em></li>
                <li>Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules <em>(Rating: 2)</em></li>
                <li>E(n) equivariant diffusion models <em>(Rating: 2)</em></li>
                <li>A 3d generative model for structure-based drug design <em>(Rating: 2)</em></li>
                <li>Crystal diffusion variational autoencoder <em>(Rating: 2)</em></li>
                <li>Automatic chemical design using a data-driven continuous representation of molecules <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8880",
    "paper_id": "paper-2db3eb03b0a9fedde37066673532804d3e224a4a",
    "extraction_schema_id": "extraction-schema-155",
    "extracted_data": [
        {
            "name_short": "LM-AC",
            "name_full": "Language Model — Atom+Coordinate tokenization",
            "brief_description": "An autoregressive transformer (GPT-style) trained to predict next tokens where each placed atom is represented by an atom/residue token plus three coordinate tokens (x,y,z) with fixed numerical precision; used to directly generate 3D molecules (XYZ), crystals (CIF), and protein binding-site point clouds (PDB) by sampling token sequences and interpreting them as chemical files.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LM-AC",
            "model_type": "Autoregressive Transformer (GPT-style)",
            "model_size": "Reported range: ~1 million to 100 million parameters; architecture examples: 12 layers, embedding size 128–1024, 4–12 attention heads",
            "training_data": "Task-dependent datasets derived from chemical file formats: ZINC molecules converted to XYZ conformers (250K molecules, rdkit conformers), Perov5 (perovskite CIFs, ~18.9K), MP20 CIFs (~45.2K), and a protein pocket dataset (~180K protein-ligand pairs) with PDB-derived pocket subsequences; numeric coordinates rounded to fixed precision (1–3 decimals) and optionally augmented via random rotations.",
            "application_domain": "Multiple: small-molecule generative chemistry (drug-like molecules), crystalline materials generation (materials science), and protein binding-site (structure-based drug discovery, biomolecular design).",
            "generation_method": "Direct autoregressive generation / next-token prediction: sample token sequences in file format encoding (atom token + x,y,z coordinate tokens per atom) and parse sampled sequences into XYZ/CIF/PDB files to obtain 3D point clouds.",
            "novelty_of_chemicals": "High novelty reported for sampled sets vs training: for ZINC generation LM-AC produced 100% novel and 100% unique molecules (as measured by dataset-level checks); for protein pockets ~89.8% unique residue orderings and 83.6% of generated orderings were novel relative to training. Crystals: novelty/diversity assessed via coverage metrics (COV-R/COV-P) showing high coverage close to training.",
            "application_specificity": "No task-specific conditioning reported — model learns the data distribution for each domain; application specificity evaluated post-hoc by distributional and property-matching metrics (e.g., QED/SA/MW distributions for molecules, lattice/property statistics for crystals, interatomic distance and residue composition distributions for pockets).",
            "evaluation_metrics": "Molecules: validity (via xyz2mol -&gt; rdkit Mol), uniqueness, novelty; distributional similarity via Earth Mover's Distance (WA) on properties: molecular weight (MW), synthetic accessibility (SA), QED. Crystals: structural validity (min interatomic distance &gt;0.5 Å), compositional neutrality, coverage metrics COV-R (recall) and COV-P (precision), WA on density and number of unique elements. Protein pockets: residue-wise composition checks (xyz2mol), inter-residue overlap checks (minimum inter- residue bond distance), interatomic distance distributions, residue-ordering uniqueness.",
            "results_summary": "Molecules (ZINC, numeric precision 2 decimals): LM-AC generated 10K samples with Valid=98.51%, Unique=100%, Novel=100%; WA metrics: MW=1.811, SA=0.026, QED=0.004 (lower WA = closer to training). Crystals (Perov5, precision 3 decimals): structure validity 100.0%, composition valid 98.79%, COV-R=98.78%, COV-P=99.36%, WA (density)=0.089, WA (# elements)=0.028. Crystals (MP20): structure valid 95.81%, composition valid 88.87%, COV-R=99.60%, COV-P=98.55%, WA (density)=0.696, WA (# elements)=0.092. Protein pockets (~10K samples evaluated): ~99% of pockets passed per-residue composition checks, ~5% failed inter-residue overlap checks; interatomic distance distributions and counts of C/N/O closely matched training; residue-ordering uniqueness: generated 89.8% unique orderings and 83.6% of those were novel compared to training.",
            "comparison_to_other_methods": "Compared with both graph/string generative models and domain-specific 3D generative models: LM-AC achieved competitive or superior performance on many metrics. For molecules, LM-AC performed comparably or better than graph models (JTVAE, CGVAE, DGMG) and 3D models (GSchNet, ENF, EDM) and approached or matched SMILES/SELFIES LMs on distributional WA metrics. For crystals, LM-AC equaled or outperformed several baselines (FTCP, GSchNet variants) and matched the top-performing CDVAE on several metrics; on MP20 LM-AC was best on 3/6 metrics and close otherwise. For protein pockets there were no graph-based baselines able to directly generate full 3D pockets; LM-AC produced geometrically plausible pockets matching training statistics.",
            "limitations_and_challenges": "Model lacks built-in SE(3) equivariance (no rotation/translation invariance), so it predicts absolute coordinates — requiring data augmentation (random rotations) to help; predicting absolute coordinates is challenging for large structures and vocabulary size explodes with coordinate tokenization (coordinate-level token vocabularies can grow to 100–10K tokens depending on precision); character-level sequences become very long (poorer scaling); generation can produce invalid structures (e.g., ~5% pocket overlap failures, other invalid examples shown), and empirical validation beyond distributional metrics (e.g., physical simulation or experiments) is necessary to confirm usefulness; scaling to larger structures may need much more data and compute.",
            "additional_notes": "Character-level tokenization (LM-CH) was evaluated for some tasks but for large PDB pockets was not used because sequences become too long; data augmentation by random rotations improves performance but some models trained without augmentation still approach SOTA.",
            "uuid": "e8880.0",
            "source_info": {
                "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "LM-CH",
            "name_full": "Language Model — Character-level tokenization",
            "brief_description": "An autoregressive transformer trained at the character level to generate chemical file-format text (characters include element letters, digits, signs, spaces, newlines) so that sampled text can be parsed as XYZ or CIF files to obtain 3D structures; applied to small molecules and crystals in the paper.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LM-CH",
            "model_type": "Autoregressive Transformer (GPT-style) — character-level tokenization",
            "model_size": "Reported range: ~1 million to 100 million parameters; example architectures: 12 layers, embedding sizes 128–1024, 4–12 attention heads",
            "training_data": "Same domain-specific datasets as LM-AC but character-tokenized: ZINC XYZ-derived conformers (250K molecules) and CIF datasets Perov5 and MP20; floating point numbers rounded to set precision (1–3 decimals) and represented as sequences of characters.",
            "application_domain": "Small-molecule generative chemistry (drug-like molecules) and crystalline materials generation (materials science).",
            "generation_method": "Direct autoregressive character-level generation of file-format strings (next-character prediction); parse generated character strings as chemical files (XYZ or CIF) to obtain 3D structures.",
            "novelty_of_chemicals": "High novelty/uniqueness for molecules reported (100% unique and novel in ZINC sampling), and crystals showed high coverage/validity on Perov5 and competitive performance on MP20 (though somewhat lower than LM-AC).",
            "application_specificity": "Model is unconditional for each domain; specificity measured post-hoc via distributional similarity metrics (e.g., property WAs, coverage COV-R/COV-P) rather than conditional/property-optimized generation.",
            "evaluation_metrics": "Same domain metrics: molecules — validity (xyz2mol-&gt;rdkit), uniqueness, novelty, WA on MW/SA/QED; crystals — structural/compositional validity, coverage (COV-R/COV-P), WA on density and # elements.",
            "results_summary": "Molecules (ZINC): LM-CH generated 10K samples with Valid=90.13%, Unique=100%, Novel=100%; WA metrics: MW=3.912, SA=2.608, QED=0.077 (worse WA than LM-AC indicating larger distributional gaps). Crystals (Perov5): LM-CH achieved structure validity 100.0%, composition valid 98.51%, COV-R=99.60%, COV-P=99.42%, WA (density)=0.071, WA (# elements)=0.036. Crystals (MP20): LM-CH structure valid 84.81%, composition valid 83.55%, COV-R=99.25%, COV-P=97.89%, WA (density)=0.864, WA (# elements)=0.132.",
            "comparison_to_other_methods": "Character-level LM had competitive performance for crystals (especially Perov5) relative to models like CDVAE and GSchNet, but for molecules LM-CH underperformed coordinate-level LM-AC and SMILES/SELFIES LMs on distributional WA metrics and validity.",
            "limitations_and_challenges": "Character-level sequences are long for even modest molecule sizes, making long-range dependencies harder to learn; lower sample validity (e.g., 90.13% on ZINC) than LM-AC; not used for protein pocket generation in this study due to sequence length/efficiency concerns; same lacking of SE(3) invariance and vocabulary challenges as LM-AC but differs in tokenization trade-offs.",
            "additional_notes": "Character-level token vocabulary is small (~30 tokens) but sequence length increases; LM-CH can learn file syntax in addition to coordinates/elements but may be less effective at precise coordinate generation than coordinate-tokenized models.",
            "uuid": "e8880.1",
            "source_info": {
                "paper_title": "Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models can learn complex molecular distributions",
            "rating": 2
        },
        {
            "paper_title": "Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules",
            "rating": 2
        },
        {
            "paper_title": "E(n) equivariant diffusion models",
            "rating": 2
        },
        {
            "paper_title": "A 3d generative model for structure-based drug design",
            "rating": 2
        },
        {
            "paper_title": "Crystal diffusion variational autoencoder",
            "rating": 2
        },
        {
            "paper_title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "rating": 1
        }
    ],
    "cost": 0.011082499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Language models can generate molecules, materials, and protein binding sites directly in three dimensions as XYZ, CIF, and PDB files</h1>
<p>Daniel Flam-Shepherd ${ }^{1,2}$ and Alán Aspuru-Guzik ${ }^{1,2,3,4}$<br>${ }^{1}$ Department of Computer Science, University of Toronto, Toronto, Ontario M5S 2E4, Canada<br>${ }^{2}$ Vector Institute for Artificial Intelligence, Toronto, Ontario M5S 1M1, Canada<br>${ }^{3}$ Department of Chemistry, University of Toronto, Toronto, Ontario M5G 1Z8, Canada<br>${ }^{4}$ Canadian Institute for Advanced Research, Toronto, Ontario M5G 1Z8, Canada</p>
<p>Language models are powerful tools for molecular design. Currently, the dominant paradigm is to parse molecular graphs into linear string representations that can easily be trained on. This approach has been very successful, however, it is limited to chemical structures that can be completely represented by a graph- like organic molecules- while materials and biomolecular structures like protein binding sites require a more complete representation that includes the relative positioning of their atoms in space. In this work, we show how language models, without any architecture modifications, trained using next-token prediction- can generate novel and valid structures in three dimensions from various substantially different distributions of chemical structures. In particular, we demonstrate that language models trained directly on sequences derived directly from chemical file formats like XYZ files, Crystallographic Information files (CIFs), or Protein Data Bank files (PDBs) can directly generate molecules, crystals, and protein binding sites in three dimensions. Furthermore, despite being trained on chemical file sequences- language models still achieve performance comparable to state-of-the-art models that use graph and graph-derived string representations, as well as other domain-specific 3D generative models. In doing so, we demonstrate that it is not necessary to use simplified molecular representations to train chemical language models- that they are powerful generative models capable of directly exploring chemical space in three dimensions for very different structures.</p>
<h2>I. INTRODUCTION</h2>
<p>Language models are autoregressive models for sequence generation that have shown impressive progress recently in natural language understanding using deep neural networks [1-3]. These advancements are driven by architecture improvements like the Transformer- a powerful neural network for sequential data that uses selfattention [4]. Transformers have found use in many significant scientific applications like protein structure prediction and design [5-7] and various tasks in cheminformatics [8-10]. An important scientific objective is the exploration of chemical space, in order to discover new drugs and materials [11]. Language models have enormous potential for chemical space exploration- which remains almost entirely unexplored given the $10^{60}$ drug-like molecules [12] and even more potentially accessible materials. Already, recent large language models [1,13] are having an impact on research in chemistry and molecular design [14-16].</p>
<p>Substantial work has been done using other neural networks in the exploration of chemical space- deep generative models [17-20] can be trained on large datasets to generate novel functional compounds from any target distributions. An important question arises on how to best represent a molecule when training models to learn molecular representations. There are many different model architectures appropriate for different representations, indeed the task at hand will heavily impact design choice. A popular approach is to directly use molecular graphs and make use of geometric deep learning to learn representations directly on atoms and bonds
[18, 21-26]- this strategy has been used to screen large compound libraries- one attempt lead to the discovery of novel antibiotics 27 .</p>
<p>An alternate approach is to use SMILES or SELFIES string representations [28, 29] that linearize molecular graphs into strings. SMILES and SELFIES are widely used for machine learning-assisted molecular design [3032]. SMILES and SELFIES are convenient for neural networks designed for sequences- which have proven to be powerful generative models of natural language [1,33]. Indeed, they have been used to achieve state-of-the-art results with chemical language models using Long Short Term Memory networks (LSTMs) [34, 35].</p>
<p>However, strings and graphs are a simplified representation of molecules- which are at naturally represented as point clouds of atoms that includes their threedimensional (3D) positions in space. For many molecular design tasks, such as catalysis [36], this geometric information is essential. SMILES and SELFIES sidestep this complete representation of molecules and are entirely unable to represent materials- which cannot be simplified as graphs and have a complex periodic 3D structure. Indeed, the geometric structure of molecules and materials is an important determinant of their properties. Indeed, molecules and materials are complex structured datadiscrete in terms of their atomic elements but continuous in terms of the coordinates of those elements.</p>
<p>Any 3D molecule, biomolecule, or material can be fully represented and stored as text data in XYZ, PDB, or CIF files (as the most common formats). These text files are effectively long strings defined by atom coordinate pairs and other information- to directly model these files</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>FIG. 1. A) The training datasets of structures that we benchmark language models on in this work. B) The overview of the training workflow – chemical file formats are converted to sequences of tokens using either character or coordinate-level tokenization. The language model is trained to predict the next token in these sequences.</p>
<p>As strings using a language model is somewhat unintuitive given the continuous nature of 3D space. Indeed, for success, a language model has to learn many layers of validity – from the basic elements of the file structure to the complex spatial arrangements of atoms in any molecule or material. Given the impressive ability of language models to model complex molecular distributions using simple string representations [34] – is it possible for language models to generate molecules and crystals in three dimensions by training on entire XYZ, CIF or even PDB files? There are many structural distributions of molecules and materials that simple string representations cannot model and many potential domain-specific applications that can be tackled – if language models could directly model more complex chemical file formats.</p>
<p>In recent work, a few models have achieved state-of-the-art results by focusing on generating molecules and materials in 3D in a way that satisfies permutation, translation, rotation, and periodic invariances with SE(3) equivariant architectures [37–40]. In contrast, for a language model that has none of these invariances built in it is challenging to generate structures by placing atoms using absolute or Cartesian coordinates. Chemical format files can easily turn into very long sequences, even for small molecules – chemical language models using LSTMs will inevitably have issues learning important long-range dependencies. However, other architectures like Transformers process the entire sequence at a time and do not have this issue – and therefore are the model that is most likely to succeed at this task.</p>
<p>We test the ability of a transformer-based language model to generate molecules from a standard benchmark ZINC [30] using sequences parsed from the XYZ files of these molecules. We further investigate the model's ability on materials – specifically crystals by training them to directly generate sequences parsed from CIFs (Crystallographic Information files), from two recent crystal benchmarks: PERGV5 and MP20 [38]. In particular, we focus on assessing the model's ability to generate valid molecules and materials that reproduce the distributional properties of the training datasets.</p>
<p>In addition to training language models on all datasets, we compare with state-of-art baselines models that generate molecules or materials as point clouds in 3D space [39]. Despite having no equivariance and being constrained by the data structure, to place atoms using absolute coordinates that are generated by a single character or coordinate at a time – the results presented in this work demonstrate a language model is comparable to the ability of domain-specific 3D generative models.</p>
<p>We also further demonstrate the ability of language models to generate large molecular structures in 3D. For this we show that they can scale to protein binding sites in Protein Data Bank files – these are specific structural regions within proteins with hundreds of atoms that can only be truly represented as 3D point clouds.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIG. 2. A histogram of root mean squared deviations in atomic positions between 10K molecules sampled from the language model and their corresponding conformers generated by rdkit. Six example molecules and geometries with various r.m.s.d. values are visualized explicitly and compared with their rdkit conformers.</p>
<p>We establish that current language models are powerful generative models for chemistry that can learn to generate structural distributions of molecules, biomolecular structures, and materials—directly in three dimensions.</p>
<h3>RESULTS</h3>
<p>The training workflow, example molecules, materials, and a protein binding site are displayed in Figure 1. The model is trained to predict the next token in a sequence defined by processing chemical file formats—XYZs, CIFs or PDBs into sequences using different tokenization strategies. After simplifying the file and removing unnecessary information, we use two different strategies: the first is character-level tokenization (LM-CH), where the model must generate every necessary individual element of the file including spaces between coordinates as well as characters that indicate a newline in the file. Next, in atom+coordinate-level tokenization (LM-AC), the model only generates atom tokens like 'C' for carbon and coordinates tokens like '-1.98'. For each placement, 4 tokens are required to place an atom in 3D space: the atom token and x, y, and z coordinate tokens. In both, we must first specify a level of numerical precision to be used and round all floating point numbers to either 1, 2, or 3 decimal places. Additionally, because the model is not rotation and translation invariant, data augmentation by randomly rotating training structures is a useful tool to improve performance. More technical details about the model and training are available in the Methods section. We evaluate the model on each of the 3D chemical structure distributions detailed further in the next sections.</p>
<h3>Molecules</h3>
<p>We test the model on sequences derived from XYZ files of molecules from the ZINC dataset [41] that consists of 250K commercially available molecules with on average 23 heavy atoms. We generate the XYZ files using rdkit's [42] built-in conformer generation tools. While other datasets of molecules exist—the ZINC dataset is the most established benchmark for graph and string generative models of molecules—enabling a wider comparison. We train both language models on XYZ-derived sequences and first specify a numerical precision of 2 decimal places for all atomic coordinates.</p>
<p>We generate 10K (thousand) molecules from the model in order to evaluate its performance and ability to sample from the distribution of molecules used in training. We evaluate the model in two ways—the first assesses the 3D molecular geometries the model learns. Second, we compare the model using standard metrics used to assess generative models for chemistry. Samples from the model are very high quality and very similar to the training samples—this can be seen directly by visualizing samples' random examples are shown in the supplementary information.</p>
<p>First, we assess if the language model (LM-AC) can learn to generate molecules with similar 3D structures that would be generated by rdkit. To do this, we plot a distribution of the rdkit computed root mean square de-</p>
<p>viation (r.m.s.d.) of atomic positions between molecules generated in 3D by the language model and the corresponding molecule with 3D structure produced by rdkit's conformer tools. To attach some relative meaning to the values in the histogram- for six different r.m.s.d. values we plot molecules generated by the language model that have different geometric structures. We also show the corresponding rdkit structure and plot in between, both molecules aligned. We label the r.m.s.d. for each as well as show which region of the histogram the molecule lies in. The model's distribution of r.m.s.d. ranges mostly between 1.0 and 2.0 although it has a heavy tail from 2.0-4.0 but quickly trails off. We can see the model does produce molecular geometries that are close in overall structure to geometries produced by rdkit. Additional examples comparing rdkit's geometry and the language model can be found in the supplementary information.</p>
<p>Next, we compare molecules generated by the language model with samples from various other generative models for molecules that are widely applied. For these baselines, we consider models that explicitly train on 3D structures as well as models that train on molecular graph or string representations. For 3D generative models, we compare with G-Schnet [39]- an auto-regressive 3D generative model that places atoms using interatomic distances, equivariant normalizing flows (ENF) [43] and equivariant diffusion for 3D molecular generation (EDM) [37]. Additionally, we consider chemical language models using a recurrent neural network with long short-term memory [35] trained on either SMILES (SM-LM) or SELFIES (SFLM). We also train some popular deep graph generative models: junction tree variational autoencoder (JTVAE) [18] which pieces together substructures or other models that generate molecular graphs by predicting individual atoms or bonds- these include: constrained graph variational autoencoder (CGVAE) [24] and deep generative auto-regressive model of graphs (DGMG) [44].</p>
<p>We also use standard metrics like validity, uniqueness, and novelty [20]- to assess the model's ability to generate a diverse set of real molecules distinct from the training data. For models using graph and string representations, we use rdkit to determine validity but for 3d models we use xyz2mol [45] to determine validity- if can produce a valid Mol object in rdkit. For quantitative evaluation of any model's ability to learn its training distribution, we compute the earth mover's distance (WA) between property values of generated molecules and training molecules. We also compute the earth mover's distance between different samples of training molecules (TRAIN in Table I) which acts as an oracle baseline to lower bound all earth mover distances. For molecular properties, we consider: quantitative estimate of drug-likeness (QED) [46], synthetic accessibility score (SA) [47], exact molecular weight (MW).</p>
<p>In Table I, we can see that both language models using character and coordinate level tokenization- achieve competitive performance to models using graph and string representations. Indeed, the character-level language</p>
<p>TABLE I. Generation performance for ZINC.</p>
<table>
<thead>
<tr>
<th>3D</th>
<th>Model</th>
<th>Basic Metrics (\%)</th>
<th></th>
<th></th>
<th></th>
<th>WA Metrics $\downarrow$</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Valid</td>
<td>Unique</td>
<td>Novel</td>
<td>MW</td>
<td>SA</td>
<td>QED</td>
<td></td>
</tr>
<tr>
<td></td>
<td>Train</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>0.816</td>
<td>0.013</td>
<td>0.002</td>
<td></td>
</tr>
<tr>
<td></td>
<td>SMLM</td>
<td>98.35</td>
<td>100.0</td>
<td>100.0</td>
<td>3.640</td>
<td>0.049</td>
<td>0.005</td>
<td></td>
</tr>
<tr>
<td></td>
<td>SFLM</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>3.772</td>
<td>0.085</td>
<td>0.006</td>
<td></td>
</tr>
<tr>
<td></td>
<td>DGMG</td>
<td>79.63</td>
<td>100.0</td>
<td>99.38</td>
<td>88.94</td>
<td>3.163</td>
<td>0.095</td>
<td></td>
</tr>
<tr>
<td></td>
<td>JTVAE</td>
<td>100.0</td>
<td>98.56</td>
<td>100.0</td>
<td>22.63</td>
<td>0.126</td>
<td>0.023</td>
<td></td>
</tr>
<tr>
<td></td>
<td>CGVAE</td>
<td>100.0</td>
<td>100.0</td>
<td>100.0</td>
<td>45.61</td>
<td>0.426</td>
<td>0.038</td>
<td></td>
</tr>
<tr>
<td></td>
<td>ENF</td>
<td>1.05</td>
<td>96.37</td>
<td>99.72</td>
<td>168.5</td>
<td>1.886</td>
<td>0.160</td>
<td></td>
</tr>
<tr>
<td></td>
<td>GSchNet</td>
<td>1.20</td>
<td>55.96</td>
<td>98.33</td>
<td>152.7</td>
<td>1.126</td>
<td>0.185</td>
<td></td>
</tr>
<tr>
<td></td>
<td>EDM</td>
<td>77.51</td>
<td>96.40</td>
<td>95.30</td>
<td>101.2</td>
<td>0.939</td>
<td>0.093</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LM-CH</td>
<td>90.13</td>
<td>100.0</td>
<td>100.0</td>
<td>3.912</td>
<td>2.608</td>
<td>0.077</td>
<td></td>
</tr>
<tr>
<td></td>
<td>LM-AC</td>
<td>98.51</td>
<td>100.0</td>
<td>100.0</td>
<td>1.811</td>
<td>0.026</td>
<td>0.004</td>
<td></td>
</tr>
</tbody>
</table>
<p>model performs comparable to the graph models but is worse than the SMILES (SM-LM) or SELFIES (SF-LM) language models. However, the coordinate-level language model achieves performance that is comparable to or better than all models.</p>
<h2>Crystals</h2>
<p>Next, we turn to materials like crystals which are structures that cannot be represented as graphs. Specifically, crystals are materials whose constituents atoms are arranged in a highly ordered lattice structure that extends, repeating in all directions. Crystals are stored in standard text file formats known as CIFs- Crystallographic Information Files. Within CIFs, the structural information necessary to describe the crystal includes atomic elements and coordinates as well as the parameters defining the periodic lattice. Similar to an XYZ file, CIF files include atomic elements positioned in a unit cell or lattice, with six additional parameters necessary to define the unit cell. This information can be generated before the atomic elements and positions- either a character at a time or treating each entire parameter as a single token. To test if language models can generate crystals as CIF-derived sequences, we turn to curated datasets from recent work on generative models for crystals [38]. We focus on two of the datasets, the first is Perov5 [48] which includes 18928 perovskite materials that share the same structure but differ in composition. There are 56 possible elements and all materials have exactly 5 atoms in the unit cell. The second dataset, MP20 [49] consists of 45231 materials varying in both structure and composition. There are 89 elements and the materials have between 1 and 20 atoms in the unit cells. We use the exact same setup and evaluation as [38], further details regarding datasets and evaluation can be found there.</p>
<p>We follow that prior work [38] and use several metrics that they used to evaluate the validity, property statistics, and diversity of generated materials. We briefly detail them here, the first is 1) Validity: a crystal is structurally valid if the shortest distance between any pair of atoms is larger than $0.5 \AA[50]$ and the composition of a crystal is valid if the overall charge is neutral as computed</p>
<p>TABLE II. Crystal generation performance.</p>
<table>
<thead>
<tr>
<th>Data Model</th>
<th></th>
<th>Valid (\%) $\uparrow$</th>
<th></th>
<th>COV (\%) $\uparrow$</th>
<th></th>
<th></th>
<th>WA $\downarrow$</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>Struc.</td>
<td>Comp.</td>
<td>R.</td>
<td>P.</td>
<td>$\rho$</td>
<td>$#$</td>
</tr>
<tr>
<td></td>
<td>Train</td>
<td>100.0</td>
<td>98.60</td>
<td>100.0</td>
<td>100.0</td>
<td>0.010</td>
<td>0.008</td>
</tr>
<tr>
<td></td>
<td>FTCP</td>
<td>0.24</td>
<td>54.24</td>
<td>0.00</td>
<td>0.00</td>
<td>10.27</td>
<td>0.630</td>
</tr>
<tr>
<td></td>
<td>GSchNet</td>
<td>99.92</td>
<td>98.79</td>
<td>0.18</td>
<td>0.23</td>
<td>1.625</td>
<td>0.037</td>
</tr>
<tr>
<td></td>
<td>PGSchNet</td>
<td>79.63</td>
<td>99.13</td>
<td>0.37</td>
<td>0.25</td>
<td>0.276</td>
<td>0.455</td>
</tr>
<tr>
<td></td>
<td>CDVAE</td>
<td>100.0</td>
<td>98.59</td>
<td>99.45</td>
<td>98.46</td>
<td>0.126</td>
<td>0.063</td>
</tr>
<tr>
<td></td>
<td>LM-CH</td>
<td>100.0</td>
<td>98.51</td>
<td>99.60</td>
<td>99.42</td>
<td>0.071</td>
<td>0.036</td>
</tr>
<tr>
<td></td>
<td>LM-AC</td>
<td>100.0</td>
<td>98.79</td>
<td>98.78</td>
<td>99.36</td>
<td>0.089</td>
<td>0.028</td>
</tr>
<tr>
<td></td>
<td>Train</td>
<td>100.0</td>
<td>91.13</td>
<td>100.0</td>
<td>100.0</td>
<td>0.051</td>
<td>0.016</td>
</tr>
<tr>
<td></td>
<td>FTCP</td>
<td>1.55</td>
<td>48.37</td>
<td>4.72</td>
<td>0.09</td>
<td>23.71</td>
<td>0.736</td>
</tr>
<tr>
<td></td>
<td>GSchNet</td>
<td>99.65</td>
<td>75.96</td>
<td>38.33</td>
<td>99.57</td>
<td>3.034</td>
<td>0.641</td>
</tr>
<tr>
<td></td>
<td>PGSchNet</td>
<td>77.51</td>
<td>76.40</td>
<td>41.93</td>
<td>99.74</td>
<td>4.04</td>
<td>0.623</td>
</tr>
<tr>
<td></td>
<td>CDVAE</td>
<td>100.0</td>
<td>86.70</td>
<td>99.15</td>
<td>99.49</td>
<td>0.688</td>
<td>1.432</td>
</tr>
<tr>
<td></td>
<td>LM-CH</td>
<td>84.81</td>
<td>83.55</td>
<td>99.25</td>
<td>97.89</td>
<td>0.864</td>
<td>0.132</td>
</tr>
<tr>
<td></td>
<td>LM-AC</td>
<td>95.81</td>
<td>88.87</td>
<td>99.60</td>
<td>98.55</td>
<td>0.696</td>
<td>0.092</td>
</tr>
</tbody>
</table>
<p>by SMACt [51]. 2) Coverage (Cov) COV-R (Recall) and COV-P (Precision) [52], measures the similarity between ensembles of generated materials and ground truth test materials. 3) Property statistics, we also compute the earth mover's distance (WA) between the property distribution of generated materials and test materials. For properties, we use density $(\rho)$ and number of unique elements (# elem.). Following [38] we sample 10K materials after training to compute evaluation metrics.</p>
<p>We compare the language model with the baselines taken from [38], which include the latest state-of-the-art generative models and methods. These include: FTCP [53] is a 1D CNN-VAE trained over a crystal representation that concatenates various properties (atom positions, atom types, diffraction pattern, etc). GSchNet was also compared with in [38] first computationally determining lattices afterward, then using a modified version (PGSchNet) that directly incorporates periodicity. Lastly, we also compare with the best-performing model in [38], the crystal diffusion variational autoencoder (CDVAE). We also include an oracle (TRAIN) that defines an upper bound for validity and coverage and a lower bound for property statistics- computed using a sample from the training data. The results are displayed in Table II.</p>
<p>We train language models on CIF-derived sequences and first specify a numerical precision of 3 decimal places for all floating-point numbers (unit cell parameters and coordinates) in the CIF files.</p>
<p>From the results, it is clear that language models are capable of generating novel materials that maintain the properties of the crystals in both training distributions. Both character and coordinate level language models show strong performance in all evaluation metrics from validity, property statistics, and diversity. Indeed in the smaller crystal dataset PEROV5, language models achieve better metrics over the baseline models. In the larger, more structurally diverse dataset MP20, the coordinate-level language model achieves the best performance in 3 of six metrics but is close to state-of-the-art performance in the other metrics as well. The character- level language model is slightly worse but still has comparable performance to the other baselines including the CDVAE.</p>
<p>There are more complex materials to test the capabilities of language models on but the experiments on these materials and the results indicate the strong potential of language models for materials generation and design. It is important to note that, beyond these metrics, more work is necessary to verify the results with computational simulation and experiments.</p>
<h2>Protein Binding Sites</h2>
<p>For the most challenging task, we test if language models can generate biomolecular structures that are stored within PDB files. To test this, in a limited way, we train language models on sequences derived from PDB files storing protein binding sites. These are regions on proteins that bind to ligands- other molecules including small molecules, peptides, or other proteins. Typically they are small subsections of a protein containing a few dozen residues and hundreds of atoms that define a distinct geometric pocket or cavity.</p>
<p>We use a dataset of $\sim 180 \mathrm{~K}$ protein-ligand pairs from [54]. As shown in Figure 3 A) we process the protein binding sites by removing all atoms in residues that are furthest from the center of the protein-ligand complex until there are roughly 200-250 atoms remaining. The training structures are just the remaining residues- the ligand is removed as well. No generative model based on graphs would be able to generate the protein pockets directly- their 3D structure is what gives rise to their function.</p>
<p>Similar to XYZ files-PDB files have atom information like elements and coordinates but have additional information related to protein structure such as every atom's residue. Therefore, after simplifying and removing other extraneous information, we convert the files to sequences using residue information as well- which we jointly tokenize with atom information. For example, atoms that are part of cysteine residues can be identified with one of the following tokens: CYS-C, CYS-N, CYS-0, CYS-S. This will allow the model to organize how each atom is placed by associating it with a local neighborhood defined by its specific amino acid.</p>
<p>We force the numerical precision of each atomic position to two decimal points and identical to atom+coordinate tokenization we use a single token for each x,y, and z coordinate of the atomic position entirely so that every atom has four tokens associated with it: one to identify its atomic element and residue as well as three tokens for its atomic position. Character-level tokenization produces sequences that are substantially longer so we do not experiment with it in this task.</p>
<p>We cannot use xyz2mol [45] to assess validity in this context since is only applicable to smaller molecules, similarly, other standard and distribution metrics for drug-</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>FIG. 3. A) Protein pockets are pre-processed by removing residues far from the center of the pocket-ligand complex. B) A comparison between the model and training data distribution of interatomic distance between 10 random pocket atoms and the closest and furthest pocket atoms. Additionally, we show a box plot for the number of carbon, nitrogen, and oxygen C) Six different examples from the training data and sampled from the language model the first 3 are plotted showing individual atoms, and the last three show the surface of the pocket.</p>
<p>like molecules are not meaningful for protein pockets. Instead, to measure validity, after sampling 10K pockets from the model, we check each residue individually with xyz2mol, and make sure the atom composition of each residue is correct. Additionally, to check if there are any overlapping atoms among residues we check if any atoms from different residues are closer than the smallest possible bond distance. Almost all pockets, or ~99% of pockets sample pass the xyz2mol &amp; residue check while ~5% of pockets fail the overlap check—we show some examples of these pockets in the supplementary information.</p>
<p>We also compare the training distribution to the model's learned distribution: first, using a bivariate kernel density estimate, we plot the joint distribution of the interatomic distance between pocket atoms and their furthest or closest neighbor. In Figure 3 B) we can see the model closely matches the training distribution for the closest and furthest neighboring atoms. In addition, in Figure 3 B), pockets sampled from the language model and training pockets have a similar number of carbon, nitrogen, and oxygen atoms.</p>
<p>To conduct a sanity check to see if the model is memorizing, we check the ordering of the residues essentially the amino acid sequence of the binding site (defined by the order in which residues appear in the PDB sequence ignoring coordinates ie ARG-SER-ASP-ILE...) in pockets generated by the model. For comparison—out of the ~180K training pockets approximately 86.1% have unique orderings. Similarly, evaluating the pockets that the language model generates, we get ~89.8% unique orderings of residues and further, of these pockets, 83.6% have novel residues orderings that do not occur in the training pockets. This indicates the model is learning to generate mostly novel protein pockets with new amino acid sequences while maintaining the higher-level geometric structure that defines a protein pocket.</p>
<p>Additionally, we display a few examples of training and model-generated pockets in Figure 3 C) including both pockets showing individual residues and atoms as well as pockets with the surface explicitly rendered—which helps highlight the actual geometric structure of the pocket. Qualitatively, both ways of visualizing the pocket, demonstrate that the language model generates pockets that do have a similar geometric structure to the training examples.</p>
<h3>DISCUSSION</h3>
<p>We have demonstrated that language models can learn to generate molecules, materials, and biomolecular structures directly in three dimensions when trained success-</p>
<p>fully on sequences derived from chemical file formats like XYZ, CIF, and PDB. The results show that language models are powerful generative models capable of learning to generate complex chemical structures in three dimensions. Language models are not just limited to simple string molecular representations like SMILES and SELFIES but can directly learn structured representations by merely predicting the next token in sequences derived from these representations. In contrast to most generative models for molecules, materials, and biomolecules that are designed for very specific classes of molecules we demonstrate that language models, without any architecture changes and simply using next-token prediction can generate a wide variety of different chemical structures. We showed that character-level language models were able to model small drug-like molecules and simple crystals. Even further with atom and coordinate-level tokenization, language models can generate biomolecular structures like protein binding sites that contain hundreds of atoms.</p>
<p>In future work, building on these results, there is enormous potential to use language models for inverse design of molecules or materials to optimize properties that depend on the geometric structure. Additionally, we are interested in testing language models in other more complex classes of molecules and materials like metal-organic frameworks and other structures in molecular biology. Another important potential area is structure-based drug discovery. Other aspects should be explored for further success including the use of different tokenization strategies. A particular issue when directly tokenizing entire coordinates is the size of the vocabulary- which will grow enormously as the structure of the molecule or material being modeled grows. Predicting absolute coordinates which are not rotation or translation invariant is challenging for structures with hundreds of atoms- training on even larger structures will be difficult and may require large amounts of data.</p>
<p>We predict that larger and larger datasets of molecules and materials will become available in the future. As more and more data becomes available- language models will continue to improve and demonstrate their power by modeling tasks once thought impossible for them.</p>
<h2>II. METHODS</h2>
<h2>A. Chemical structure Representations</h2>
<p>a. Molecules ( $X Y Z$ files) We represent a molecule as a point cloud of $n$ atoms with elements $e_{i} \in{\mathrm{C}, \ldots}$ and positions $x_{i}, y_{i}, z_{i} \in \mathbb{R}$ - as follows</p>
<p>$$
\mathcal{M}=\left(e_{1}, x_{1}, y_{1}, z_{1}, \ldots, e_{n}, x_{n}, y_{n}, z_{n}\right)
$$</p>
<p>b. Crystals (CIF files) Any crystal can be represented similarly but must include necessary information about the unit cell or lattice in addition to atomic positions and elements. The unit cell is a parallelepiped,
so there are six necessary lattice parameters taken as the lengths of the cell edges $\left(\ell_{a}, \ell_{b}, \ell_{c}\right)$ and the angles between them $(\alpha, \beta, \gamma)$. The positions of atoms inside the unit cell are described by fractional coordinates $\left(x_{i}, y_{i}, z_{i}\right)$ along the cell edges. Thus crystals can be completely described using the following tuple:</p>
<p>$$
\mathcal{C}=\left(\ell_{a}, \ell_{b}, \ell_{c}, \alpha, \beta, \gamma, e_{1}, x_{1}, y_{1}, z_{1}, \ldots, e_{n}, x_{n}, y_{n}, z_{n}\right)
$$</p>
<p>c. Protein pockets (PDB files) We represent a protein pocket as a point cloud of $n$ atoms with residueatom indicators $a_{i} \in{\mathrm{HIS}-\mathrm{C}, \mathrm{HIS}-\mathrm{N}, \ldots}$ and positions $x_{i}, y_{i}, z_{i} \in \mathbb{R}$ - as follows</p>
<p>$$
\mathcal{P}=\left(a_{1}, x_{1}, y_{1}, z_{1}, \ldots, a_{n}, x_{n}, y_{n}, z_{n}\right)
$$</p>
<h2>B. Tokenization</h2>
<p>Ignoring special tokens, character-level models use a small vocabulary of $\sim 30$ tokens consisting of atom type tokens $\mathrm{C}, \mathrm{N}, \ldots$ digit characters and minus sign $1-9,-$, and other file symbols like newline character which we represent with a hashtag as well as an empty space token</p>
<p>Atom+coordinate-level models use a larger vocabulary of $\sim 100-10 \mathrm{~K}$ tokens consisting of atom types tokens $\mathrm{C}, \mathrm{N}, \ldots$ or atom-residue tokens ${\mathrm{HIS}-\mathrm{C}, \mathrm{HIS}-\mathrm{N}, \ldots$ and coordinate tokens like $-1.9,-1.98$ or $-1.987$ - these range from the smallest to largest coordinate values and can have restricted precision between 1 and 3 decimal places.</p>
<h2>C. Language Modeling</h2>
<p>we frame language modeling as unsupervised distribution estimation from a set of examples $\left(x_{1}, x_{2}, \ldots, x_{n}\right)$ each composed of variable length sequences of tokens $t_{i}$ such that $x=\left(t_{1}, t_{2}, \ldots, t_{n}\right)$. The sequences here are chemical structures so have many possible orderings (restricted by file and structural information) but regardless we factorize the joint probabilities over</p>
<p>$$
p(x)=\prod_{i=1}^{n} p\left(t_{n} \mid t_{n-1}, \ldots t_{1}\right)
$$</p>
<p>this probability is modeled using a Transformer [2] with parameters that are trained using stochastic gradient descent.</p>
<h2>D. Data Augmentation</h2>
<p>Since the model is not invariant to rotations or translations, to improve performance at every epoch we can randomly rotate any training structure about its center to expand the training data. Models trained without data augmentation can still achieve performance close to SOTA.</p>
<h2>E. Model architecture and Training</h2>
<p>We use Transformers with GPT architecture [2] that have roughly between $\sim 1$ and 100 million parameters and use 12 layers, embedding size between ${128,1024}$, and 4 to 12 attention heads. For training we use a small batch size between [4,32] structures, and a starting learning rate between $\left[10^{-4}, 10^{-5}\right)$, that is decayed to $9 \cdot 10^{-6}$ over training. Example code can be found at
https://github.com/danielflamshep/xyztransformer.</p>
<h2>III. ACKNOWLEDGEMENTS</h2>
<p>A.A.-G. acknowledge funding from Dr. Anders G. Froseth. A.A.-G. also acknowledges support from the Canada 150 Research Chairs Program, the Canada Industrial Research Chair Program, and from Google, Inc. Models were trained using the Canada Computing Systems [55]. A.A.-G. also acknowledges support from the Acceleration Consortium at the University of Toronto.
[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., "Language models are few-shot learners," Advances in neural information processing systems 33, 1877 (2020).
[2] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., "Improving language understanding by generative pre-training," ().
[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., "Language models are unsupervised multitask learners," ().
[4] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, "Attention is all you need," Advances in neural information processing systems 30 (2017).
[5] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Żółek, A. Potapenko, et al., "Highly accurate protein structure prediction with alphafold," Nature 596, 583 (2021).
[6] J. Ingraham, V. Garg, R. Barzilay, and T. Jaakkola, "Generative models for graph-based protein design," Advances in neural information processing systems 32 (2019).
[7] W. Jin, J. Wohlwend, R. Barzilay, and T. Jaakkola, "Iterative refinement graph neural network for antibody sequence-structure co-design," arXiv preprint arXiv:2110.04624 (2021).
[8] P. Schwaller, T. Laino, T. Gaudin, P. Bolgar, C. A. Hunter, C. Bekas, and A. A. Lee, "Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction," ACS central science 5, 1572 (2019).
[9] F. Fuchs, D. Worrall, V. Fischer, and M. Welling, "Se (3)-transformers: 3d roto-translation equivariant attention networks," Advances in Neural Information Processing Systems 33, 1970 (2020).
[10] G. Zhou, Z. Gao, Q. Ding, H. Zheng, H. Xu, Z. Wei, L. Zhang, and G. Ke, "Uni-mol: A universal 3d molecular representation learning framework," (2022).
[11] B. Sanchez-Lengeling and A. Aspuru-Guzik, "Inverse molecular design using machine learning: Generative models for matter engineering," Science 361, 360 (2018).
[12] P. G. Polishchuk, T. I. Madzhidov, and A. Varnek, "Estimation of the size of drug-like chemical space based on GDB-17 data," J. Comput.-Aided Mol. Des. 27, 675 (2013).
[13] OpenAI, "Gpt-4 technical report," (2023), arXiv:2303.08774 [cs.CL].
[14] A. D. White, G. M. Hocky, H. A. Gandhi, M. Ansari, S. Cox, G. P. Wellawatte, S. Sasmal, Z. Yang, K. Liu, Y. Singh, et al., "Assessment of chemistry knowledge in large language models that generate code," Digital Discovery 2, 368 (2023).
[15] A. M. Bran, S. Cox, A. D. White, and P. Schwaller, "Chemcrow: Augmenting large-language models with chemistry tools," arXiv preprint arXiv:2304.05376 (2023).
[16] D. A. Boiko, R. MacKnight, and G. Gomes, "Emergent autonomous scientific research capabilities of large language models," arXiv preprint arXiv:2304.05332 (2023).
[17] R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, "Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules," ACS Cent. Sci. 4, 268 (2018).
[18] W. Jin, R. Barzilay, and T. Jaakkola, "Junction tree variational autoencoder for molecular graph generation," arXiv preprint arXiv:1802.04364 (2018).
[19] W. Jin, R. Barzilay, and T. Jaakkola, in International Conference on Machine Learning (PMLR, 2020) pp. 4839-4848.
[20] D. Flam-Shepherd, T. C. Wu, and A. AspuruGuzik, "Mpgvae: improved generation of small organic molecules using message passing neural nets," Machine Learning: Science and Technology 2, 045010 (2021).
[21] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gómez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams, in Neural Information Processing Systems (2015).
[22] D. Flam-Shepherd, T. C. Wu, P. Friederich, and A. Aspuru-Guzik, "Neural message passing on high order paths," Machine Learning: Science and Technology (2021).
[23] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, "Learning deep generative models of graphs," arXiv preprint arXiv:1803.03324 (2018).
[24] Q. Liu, M. Allamanis, M. Brockschmidt, and A. Gaunt, in Advances in Neural Information Processing Systems (2018) pp. 7795-7804.
[25] J. You, B. Liu, Z. Ying, V. Pande, and J. Leskovec, "Graph convolutional policy net-</p>
<p>work for goal-directed molecular graph generation," Advances in Neural Information Processing Systems, 6410 (2018).
[26] A. Seff, W. Zhou, F. Damani, A. Doyle, and R. P. Adams, in Advances in Neural Information Processing Systems.
[27] J. M. Stokes, K. Yang, K. Swanson, W. Jin, A. CubillosRuiz, N. M. Donghia, C. R. MacNair, S. French, L. A. Carfrae, Z. Bloom-Ackermann, et al., "A deep learning approach to antibiotic discovery," Cell 180, 688 (2020).
[28] D. Weininger, "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules," Journal of chemical information and computer sciences 28, 31 (1988).
[29] M. Krenn, F. Häse, A. Nigam, P. Friederich, and A. Aspuru-Guzik, "Selfies: a robust representation of semantically constrained graphs with an example application in chemistry," arXiv preprint arXiv:1905.13741 (2019).
[30] R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, "Automatic chemical design using a data-driven continuous representation of molecules," ACS central science 4, 268 (2018).
[31] M. H. Segler, T. Kogej, C. Tyrchan, and M. P. Waller, "Generating focused molecule libraries for drug discovery with recurrent neural networks," ACS central science 4, 120 (2018).
[32] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato, in International Conference on Machine Learning 2017.
[33] I. Sutskever, J. Martens, and G. E. Hinton, "Generating text with recurrent neural networks," International Conference on Machine Learning (2011). .
[34] D. Flam-Shepherd, K. Zhu, and A. Aspuru-Guzik, "Language models can learn complex molecular distributions," Nature Communications 13, 3293 (2022).
[35] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation 9, 1735 (1997).
[36] L. Chanussot, A. Das, S. Goyal, T. Lavril, M. Shuaibi, M. Riviere, K. Tran, J. Heras-Domingo, C. Ho, W. Hu, et al., "Open catalyst 2020 (oc20) dataset and community challenges," Acs Catalysis 11, 6059 (2021).
[37] E. Hoogeboom, V. G. Satorras, C. Vignac, and M. Welling, in International Conference on Machine Learning (PMLR, 2022).
[38] T. Xie, X. Fu, O.-E. Ganea, R. Barzilay, and T. S. Jaakkola, in International Conference on Learning Representations.
[39] N. Gebauer, M. Gastegger, and K. Schütt, "Symmetryadapted generation of 3d point sets for the targeted discovery of molecules," Advances in Neural Information Processing Systems 32 (2019).
[40] V. Garcia Satorras, E. Hoogeboom, F. Fuchs, I. Posner, and M. Welling, "E (n) equivariant normalizing flows," Advances in Neural Information Processing Systems 34, 4181 (2021).
[41] J. J. Irwin and B. K. Shoichet, "Zinc- a free database of commercially available compounds for virtual screening," Journal of chemical information and modeling 45, 177 (2005).
[42] G. Landrum, "Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling,"
(2013).
[43] V. G. Satorras, E. Hoogeboom, F. B. Fuchs, I. Posner, and M. Welling, "E (n) equivariant normalizing flows for molecule generation in 3d," arXiv preprint arXiv:2105.09016 (2021).
[44] Q. Liu, M. Allamanis, M. Brockschmidt, and A. Gaunt, in Advances in Neural Information Processing Systems (2018) pp. 7795-7804.
[45] J. H. Jensen, "xyz2mol," GitHub repository (2020).
[46] G. R. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins, "Quantifying the chemical beauty of drugs," Nature chemistry 4, 90 (2012).
[47] P. Ertl and A. Schuffenhauer, "Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions," Journal of cheminformatics 1, 1 (2009).
[48] I. E. Castelli, T. Olsen, S. Datta, D. D. Landis, S. Dahl, K. S. Thygesen, and K. W. Jacobsen, "Computational screening of perovskite metal oxides for optimal solar light capture," Energy \&amp; Environmental Science 5, 5814 (2012).
[49] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards, S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, et al., "Commentary: The materials project: A materials genome approach to accelerating materials innovation," APL materials 1, 011002 (2013).
[50] C. J. Court, B. Yildirim, A. Jain, and J. M. Cole, "3-d inorganic crystal structure generation and property prediction via representation learning," Journal of chemical information and modeling 60, 4518 (2020).
[51] D. W. Davies, K. T. Butler, A. J. Jackson, J. M. Skelton, K. Morita, and A. Walsh, "Smact: Semiconducting materials by analogy and chemical theory," Journal of Open Source Software 4, 1361 (2019).
[52] M. Xu, S. Luo, Y. Bengio, J. Peng, and J. Tang, in International Conference on Learning Representations (2021).
[53] Z. Ren, J. Noh, S. Tian, F. Oviedo, G. Xing, Q. Liang, A. Aberle, Y. Liu, Q. Li, S. Jayavelu, et al., "Inverse design of crystals using generalized invertible crystallographic representation," arXiv preprint arXiv:2005.07609 (2020).
[54] S. Luo, J. Guan, J. Ma, and J. Peng, "A 3d generative model for structure-based drug design," Advances in Neural Information Processing Systems 34 (2021).
[55] S. Baldwin, in Journal of Physics: Conference Series, Vol. 341 (IOP Publishing, 2012) p. 012001.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>FIG. S1. a) Examples of training molecules in three dimensions. b) Samples of molecules generated by the language model.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>FIG. S2. example molecules and geometries with various r.m.s.d. values are visualized explicitly and compared with their rdkit conformers.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>FIG. S3. a) Examples of training protein pockets. b) Samples of protein pockets generated by the model.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>FIG. S4. a) Examples of training protein pockets with their surface rendered. b) Samples of protein pockets generated by the language model.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>FIG. S5. Examples of generated structures that are invalid.</p>            </div>
        </div>

    </div>
</body>
</html>
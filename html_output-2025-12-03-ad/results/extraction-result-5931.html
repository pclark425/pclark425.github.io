<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5931 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5931</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5931</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-0606bb9a541ce7e57bd78ac680a7df0225ece30c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0606bb9a541ce7e57bd78ac680a7df0225ece30c" target="_blank">Can large language models build causal graphs?</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work evaluates if large language models can be a useful tool in complementing causal graph development and shows that they could be.</p>
                <p><strong>Paper Abstract:</strong> Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5931.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5931.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 → causal edges</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using GPT-3 to signal presence/absence of causal edges in medical Directed Acyclic Graphs (DAGs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper applies GPT-3 to extract common causal knowledge from text (the model's internet training corpus) by scoring natural-language statements that assert or deny directed edges between variable pairs, using these scores to predict presence/absence and directionality of edges in ground-truth medical DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Can Large Language Models Build Causal Graphs?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>Autoregressive transformer language model (OpenAI GPT-3, ~175B parameters) trained on large-scale internet text; few-shot and prompt-sensitive capabilities; not specialized to medical literature in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Automatically signal the presence or absence (and direction) of causal edges between variable pairs by extracting common causal assertions from text using an LLM to assist building causal DAGs in the medical domain.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Medical / epidemiology (exposure-outcome causal relationships)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>For each ordered pair of variables in four hand-crafted ground-truth DAGs, the authors generated two natural-language statements (one asserting a directed edge, one asserting absence), and queried GPT-3 to score each statement; prediction considered correct if GPT-3 scored the true statement higher. They evaluated variations in prompts (authority prefaces), linking verbs ('causes', 'increases risk', ...), and variable specificity. No retrieval-augmentation or chain-of-thought was used; evaluation was against human-specified ground-truth DAGs.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Qualitative causal relationships (binary directed edges) of the form 'Variable A increases the risk/likelihood of Variable B' — i.e., domain-level causal rules connecting exposures and outcomes (e.g., smoking → cancer).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Prediction accuracy = proportion of ordered variable-pair statements where GPT-3 scored the correct (presence/absence) statement higher than the incorrect one; reported per DAG and per experimental condition (prompt authority, linking verb, specificity).</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>GPT-3 achieved accuracy above random (50%) for at least one tested prompt/verb setting in each of the four small, well-studied DAGs. Reported highlights: Alcohol DAG — best 0.83 accuracy with 'According to medical doctors' prompt; Cancer DAG — up to 1.00 accuracy with 'medical doctors' and 'medical studies' prompts; Diabetes DAG — baseline 0.67; Obesity DAG — baseline 0.75. Choice of linking verb mattered ('increases risk' produced highest accuracy for 3/4 DAGs). Performance varied substantially by prompt phrasing, authority invoked, and variable phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human-in-the-loop in design and evaluation: ground-truth DAGs were provided by researchers (expert knowledge), and the authors recommend expert verification of LLM outputs; experiments were supervised and analyzed by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>No curated medical corpus was used for inference; GPT-3's pretraining corpus (broad internet text) is implicitly the source of knowledge. The study did not perform retrieval over biomedical databases (authors propose future work with PubMed / BioBert / web-GPT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reported limitations include: model training data lagging behind current medical literature (poor for novel diseases), prevalence of casual/ambiguous causal language in internet text vs. clinical literature, high sensitivity to prompt wording and linking verbs, assumption that causal connections are well-established in GPT-3's corpus, lack of mechanisms to enforce acyclicity or to synthesize full DAGs (study only assessed pairwise edge predictions), and variable-specificity sometimes reduced accuracy. Authors note results are preliminary and require expert verification.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Four ground-truth DAGs (Alcohol, Cancer, Diabetes, Obesity) used as testbeds; prompt-engineering experiment examples: baseline 'Var1 increases the risk of developing Var2' versus 'According to medical doctors, Var1 increases the risk of developing Var2' (the latter improved accuracy in some DAGs; invoking 'Big Pharma' often decreased accuracy). Linking-verb experiments showed 'increases risk' yielded highest accuracy in Cancer, Diabetes, Obesity; specificity experiments showed 'excessive alcohol consumption' increased accuracy for Alcohol DAG, while more clinical/specific phrasings sometimes decreased performance in Cancer and Obesity cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can large language models build causal graphs?', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can foundation models talk causality? <em>(Rating: 2)</em></li>
                <li>Causal inference in natural language processing: Estimation, prediction, interpretation and beyond <em>(Rating: 2)</em></li>
                <li>Investigating gender bias in language models using causal mediation analysis <em>(Rating: 1)</em></li>
                <li>Can large language models reason about medical questions? <em>(Rating: 1)</em></li>
                <li>Gpt-3 models are poor few-shot learners in the biomedical domain <em>(Rating: 1)</em></li>
                <li>BioBERT: a pre-trained biomedical language representation model for biomedical text mining <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5931",
    "paper_id": "paper-0606bb9a541ce7e57bd78ac680a7df0225ece30c",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "GPT-3 → causal edges",
            "name_full": "Using GPT-3 to signal presence/absence of causal edges in medical Directed Acyclic Graphs (DAGs)",
            "brief_description": "This paper applies GPT-3 to extract common causal knowledge from text (the model's internet training corpus) by scoring natural-language statements that assert or deny directed edges between variable pairs, using these scores to predict presence/absence and directionality of edges in ground-truth medical DAGs.",
            "citation_title": "Can Large Language Models Build Causal Graphs?",
            "mention_or_use": "use",
            "llm_model_name": "GPT-3",
            "llm_model_description": "Autoregressive transformer language model (OpenAI GPT-3, ~175B parameters) trained on large-scale internet text; few-shot and prompt-sensitive capabilities; not specialized to medical literature in this work.",
            "task_goal": "Automatically signal the presence or absence (and direction) of causal edges between variable pairs by extracting common causal assertions from text using an LLM to assist building causal DAGs in the medical domain.",
            "domain": "Medical / epidemiology (exposure-outcome causal relationships)",
            "methodology": "For each ordered pair of variables in four hand-crafted ground-truth DAGs, the authors generated two natural-language statements (one asserting a directed edge, one asserting absence), and queried GPT-3 to score each statement; prediction considered correct if GPT-3 scored the true statement higher. They evaluated variations in prompts (authority prefaces), linking verbs ('causes', 'increases risk', ...), and variable specificity. No retrieval-augmentation or chain-of-thought was used; evaluation was against human-specified ground-truth DAGs.",
            "type_of_qualitative_law": "Qualitative causal relationships (binary directed edges) of the form 'Variable A increases the risk/likelihood of Variable B' — i.e., domain-level causal rules connecting exposures and outcomes (e.g., smoking → cancer).",
            "evaluation_metrics": "Prediction accuracy = proportion of ordered variable-pair statements where GPT-3 scored the correct (presence/absence) statement higher than the incorrect one; reported per DAG and per experimental condition (prompt authority, linking verb, specificity).",
            "results_summary": "GPT-3 achieved accuracy above random (50%) for at least one tested prompt/verb setting in each of the four small, well-studied DAGs. Reported highlights: Alcohol DAG — best 0.83 accuracy with 'According to medical doctors' prompt; Cancer DAG — up to 1.00 accuracy with 'medical doctors' and 'medical studies' prompts; Diabetes DAG — baseline 0.67; Obesity DAG — baseline 0.75. Choice of linking verb mattered ('increases risk' produced highest accuracy for 3/4 DAGs). Performance varied substantially by prompt phrasing, authority invoked, and variable phrasing.",
            "human_involvement": "Human-in-the-loop in design and evaluation: ground-truth DAGs were provided by researchers (expert knowledge), and the authors recommend expert verification of LLM outputs; experiments were supervised and analyzed by humans.",
            "dataset_or_corpus": "No curated medical corpus was used for inference; GPT-3's pretraining corpus (broad internet text) is implicitly the source of knowledge. The study did not perform retrieval over biomedical databases (authors propose future work with PubMed / BioBert / web-GPT).",
            "limitations_or_challenges": "Reported limitations include: model training data lagging behind current medical literature (poor for novel diseases), prevalence of casual/ambiguous causal language in internet text vs. clinical literature, high sensitivity to prompt wording and linking verbs, assumption that causal connections are well-established in GPT-3's corpus, lack of mechanisms to enforce acyclicity or to synthesize full DAGs (study only assessed pairwise edge predictions), and variable-specificity sometimes reduced accuracy. Authors note results are preliminary and require expert verification.",
            "notable_examples": "Four ground-truth DAGs (Alcohol, Cancer, Diabetes, Obesity) used as testbeds; prompt-engineering experiment examples: baseline 'Var1 increases the risk of developing Var2' versus 'According to medical doctors, Var1 increases the risk of developing Var2' (the latter improved accuracy in some DAGs; invoking 'Big Pharma' often decreased accuracy). Linking-verb experiments showed 'increases risk' yielded highest accuracy in Cancer, Diabetes, Obesity; specificity experiments showed 'excessive alcohol consumption' increased accuracy for Alcohol DAG, while more clinical/specific phrasings sometimes decreased performance in Cancer and Obesity cases.",
            "uuid": "e5931.0",
            "source_info": {
                "paper_title": "Can large language models build causal graphs?",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can foundation models talk causality?",
            "rating": 2
        },
        {
            "paper_title": "Causal inference in natural language processing: Estimation, prediction, interpretation and beyond",
            "rating": 2
        },
        {
            "paper_title": "Investigating gender bias in language models using causal mediation analysis",
            "rating": 1
        },
        {
            "paper_title": "Can large language models reason about medical questions?",
            "rating": 1
        },
        {
            "paper_title": "Gpt-3 models are poor few-shot learners in the biomedical domain",
            "rating": 1
        },
        {
            "paper_title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "rating": 1
        }
    ],
    "cost": 0.00669775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Can Large Language Models Build Causal Graphs?</h1>
<p>Stephanie Long*<br>Dept. of Family Medicine, McGill University</p>
<p>Tibor Schuster<br>Dept. of Family Medicine, McGill University</p>
<p>Alexandre Piché<br>Mila, Université de Montréal<br>ServiceNow Research</p>
<h4>Abstract</h4>
<p>Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.</p>
<h2>1 Introduction</h2>
<p>Advances in causal inference have important implications in empirical research as most research questions asked in the health and medical context are not associational, but causal in nature. Examples of such research questions include: What is the efficacy of a given drug in a given population? What is the expected effect of a given intervention on a specific outcome? Common amongst these research questions is the desire to uncover the cause-and-effect relationships amongst a set of variables i.e., treatments, interventions, and outcomes. Such causal questions cannot be answered from (observed) data alone or from the distributions that govern said data [Pearl, 2009]. In addition, external knowledge is needed to understand the underlying data-generating mechanisms to enable the setup of an appropriate 'inference engine'.
Causal diagrams play a central role in causal inference because they encode contextual knowledge of the observable and unobservable variables, and their causal dependencies. Causal inference pioneer Judea Pearl refers to the nodes in a causal diagram as a "society of listening variables" [Pearl, 2017]. The term "listening" stresses the defining property of directed and acyclic relationships between the variables, i.e., listening being asymmetrical, variable A listening to variable B, does not imply variable B listening to variable A, motivating the commonly adapted nomenclature of Directed Acyclic Graphs (DAGs) [Greenland et al., 1999, Greenland and Pearl, 2006].
The first step when aiming to address causal questions using data is to draw a causal diagram e.g., a causal DAG. However, with the growing complexity and depth of health and medical knowledge being generated and increasing availability of new research articles daily, research databases are reaching dimensions that limit the possibility of parsing through the enormity of evidence needed to craft comprehensive DAGs [Raghupathi, 2014]. Though expert opinion is the most valuable tool for drawing DAGs, experts do not always generate perfect DAGs, sometimes missing important confounding pathways [Oates et al., 2017]. Additionally, obtaining the opinions of numerous experts is costly both in time and resources. Thus, the ongoing developments of Large Language Models (LLM) may offer promise to help overcome some of these challenges by leveraging existing text data that may express causal sentiments (e.g., "X causes Y").
This research aims to answer the question, "Can large language models help researchers build causal diagrams in the medical context using existing text data?" Here we will conduct experiments to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of the evaluation. To predict the structure of a given causal graph, for every ordered variable pair, we scored two statements using GPT-3, where the first statement implied the presence of an arrow and the second implied the absence of an arrow. GTP-3 was accurate if the correct statement had a higher accuracy score than the incorrect statement. For example, GPT-3 would be accurate if the statement implying the presence (or absence) of an arrow had a higher accuracy than the incorrect statement and the arrow was present (or absent) in the true DAG.
determine under what conditions (e.g., prompt engineering, use of alternative language) GPT-3 [Brown et al., 2020] is able to provide accurate answers regarding the relationship between variables in a medical context and what are its limitations in doing so.</p>
<p>The main contributions of this paper are:</p>
<ul>
<li>Determining whether GPT-3 can signal the presence or absence of an edge between two variables in a directed acyclic graph from the medical context.</li>
<li>Evaluating whether the use of certain language in prompts or linking verbs improves the classification accuracy of GPT-3.</li>
<li>Exploring the limitations of GPT-3 in understanding the causal relationships between variables in the medical context.</li>
</ul>
<h1>2 Background</h1>
<h3>2.1 Large language models</h3>
<p>Large language models capture non-trivial relationships and knowledge about the datasets they have been trained upon. This knowledge has the possibility to unlock numerous applications in healthcare such as summarizing research papers, assessing patient risks from subjective symptoms, and diagnosing patients from clinical notes.
Although LLMs perform well on general natural language processing (NLP) tasks, its performance has been shown to be sensitive to its prompt [Moradi et al., 2021, Gutiérrez et al., 2022]. The advent of prompt-based learning introduced a possible solution to context sensitive text, by querying LLMs with a prompt that uses in-domain examples or task descriptions [Liu et al., 2021]. For example, chain-of-thought prompts such as "Let's take this step by step" have been shown to trigger multi-step reasoning in solving arithmetic problems [Kojima et al., 2022]. Such prompts have also been shown to significantly improve performance in reasoning about medical questions [Liévin et al., 2022].
Large language models are also sensitive to the type of text data they are trained on. For instance, GPT-3 [Brown et al., 2020] was trained on the corpus of text information on the internet. As one can imagine, the entirety of the internet would include a range of text data from lay and casual use of language on social media to more formal language in news articles. These differences in writing styles may influence the frequency of the use of causal language describing non-causal relationships. For instance, an individual writing a social media post may use the word 'cause' more lightly than medical researchers in medical journals.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Ground Truth DAGs. Four DAGs illustrating well-known exposure-outcome effects in the medical literature. DAG (A) represents the simplest DAG evaluated by GPT-3. DAGs (B-D) represent more complex structures involving a collider variable (node with two arrows pointing into it e.g., 'respiratory disease', 'body weight', and 'heart failure') with a common cause with the outcome.</p>
<h1>2.2 Causal diagram overview</h1>
<p>Causal models are typically accompanied by graphical representations i.e., Directed Acyclic Graphs (DAGs) which are acyclic graphs that succinctly illustrate the qualitative assumptions made by the models, not captured by conventional statistical models or machine learning algorithms [Greenland and Brumback, 2002, Greenland et al., 1999].
In epidemiological research, DAGs have a variety of purposes including: (1) representing the causal relationships amongst variables [Greenland and Brumback, 2002, Greenland and Pearl, 2006, Pearl, 1995]; (2) identifying the potential confounding variables which need to be controlled for in order to estimate causal effects [Greenland and Pearl, 2006, Pearl, 1995, Robins, 2001, Hernán et al., 2002]; and more recently (3) as a means of classifying the types of causal relationships that may give rise to selection bias [Hernán et al., 2004].
A DAG is composed of variables (nodes), both measured and unmeasured, and their connections are displayed via line segments (directed edges) [Greenland et al., 1999, Hernán et al., 2004]. The absence of an arrow between variables indicates the lack of a direct relationship between the variables. If the edge has an arrowhead, the variable at the tail is the parent node and the variable at the arrowhead is the child node [Greenland and Pearl, 2006]. An edge or arc is any line (with an arrowhead or not) that connects two variables [Greenland and Brumback, 2002]. The main characteristics of DAGs are that they are: (1) directed i.e., the edge has a defined direction (arrowhead), and (2) acyclic i.e., lack of cycles or loops within the graph.
A DAG is causal if: (1) the arrows between variables can be interpreted as direct causal effects, and (2) all common causes of any pair of variables are present [Hernán et al., 2004]. The causal effects are 'direct' relative to certain degrees of abstraction in that the DAG does not include any variables that may mediate the effect [Greenland and Pearl, 2006]. As the name suggests, DAGs are acyclic because a variable cannot be the cause of itself, either directly or indirectly through another variable i.e., there are no feedback loops; as illustrated by each DAG in Figure 2 [Hernán et al., 2004]. Additionally, in DAGs, causal pathways are represented with directed paths from the starting variable to the final variable; thus, a variable is the cause of its descendants and an effect of its ancestors [Greenland and Pearl, 2006].</p>
<h1>3 Experiments</h1>
<h3>3.1 Experimental details</h3>
<p>To empirically assess the potential effectiveness of LLMs in building DAGs, we used four DAGs representing well-known exposure-outcome relationships in the medical literature (Figure 2) as the Ground Truth. These DAGs are varied in complexity, amount of variables, and reflect different medical contexts. For a DAG of $N$ variables, there are $\binom{N}{2}$ possible edges between two variables, and there are twice this amount of possible arrows since the arrows are directed. For example, a DAG of 4 variables has $2 \times\binom{4}{2}=12$ possible arrows.
For each DAG, we looped through every ordered variable pair, and asked GPT-3 to score two statements per pair: (1) one implying the presence of a directed edge from variable 1 to variable 2 , and (2) one implying absence of a directed edge from variable 1 to variable 2 . The presence or absence of an edge between two variables is a binary decision (Yes / No), thus, we defined the prediction as accurate, if GPT-3 scored the correct statement higher than the incorrect one. We reported the accuracy or the proportion of correct predictions of our model.</p>
<h3>3.2 Results</h3>
<p>Q1: Does using prompt engineering lead to more accurate answers? We investigated if the prediction accuracy of GPT-3 could be improved by prompting the statements with a reference to a medical authority. For example,
"According to X, var1 increases the risk of developing var2",
instead of
"Var1 increases the risk of developing var2" (baseline),
where X is an individual or entity with medical authority or expertise, e.g., medical doctors, medical studies, or "Big Pharma". These prompts were chosen as they vary in their credibility with the public. We found that in 2 cases (Diabetes and Obesity DAGs) prompt engineering did not help and baseline (no prompting individual or authority) outperformed all other prompts. While in 2 other cases, the "According to medical doctors," prompting significantly improved the accuracy of GPT-3. Interestingly, conditioning on "According to Big Pharma," decreases the accuracy of 3 of the 4 DAGs compared to the baseline. Furthermore, prompting the model on medical studies or medical doctors resulted in different results for half the DAGs. See Table 1 for all results.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DAG name</th>
<th style="text-align: left;">Prompt</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">$\mathbf{0 . 8 3}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Cancer</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">0.75</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">$\mathbf{1 . 0 0}$</td>
</tr>
<tr>
<td style="text-align: left;">Diabetes</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Big Pharma</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical doctors</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Medical studies</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Prompt engineering: The medical authority used to prompt the statement.</p>
<p>Q2: Does the verb used to denote the relationship between the variables have an impact on accuracy? For instance, "Variable 1 X Variable 2" where $X$ represents the verb (or phrase) that denotes the relationship between the variables, e.g., "causes" or "increases the risk".
Our results demonstrated that while no verb consistently improved classification accuracy, the choice of verb linking the two variables of interest influenced accuracy. 'Increases risk' had the highest accuracy for three of the four DAGs. Though it did not achieve the highest accuracy in the Alcohol DAG. Overall, the use of 'cause' yielded decent results for all DAGs. Results are reported in Table 2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DAG name</th>
<th style="text-align: left;">Linking Verb</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">$\mathbf{0 . 5 0}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;">Cancer</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
<tr>
<td style="text-align: left;">Diabetes</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases likelihood</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}$</td>
</tr>
</tbody>
</table>
<p>Table 2: Linking verb: The verb or phrase used to link the two variables of interest.</p>
<p>Q3: Does specificity in language improve accuracy? We investigated if making our statements more specific or descriptive improved GPT-3's accuracy.
Unsurprisingly, rephrasing the "alcohol" variable to "excessive alcohol consumption" increased the accuracy of GPT-3 on the Alcohol DAG. However, being more specific about the number of cigarettes being smoked and using a clinical term to qualify obesity resulted in worse accuracy for the Cancer and Obesity DAGs. Overall, In this analysis, more specific statements did not increase the accuracy and often resulted in worse accuracy for different linking verbs. Results are reported in Table 3.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DAG name</th>
<th style="text-align: left;">Variable Name</th>
<th style="text-align: left;">Linking Verb</th>
<th style="text-align: center;">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Alcohol</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Excessive alcohol consumption</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.33</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;">Cancer</td>
<td style="text-align: left;">Cigarette smoking</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Smoking 100 cigarettes a day</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Obesity</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases risk</td>
<td style="text-align: center;">$\mathbf{0 . 6 7}$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Excessive fat accumulation</td>
<td style="text-align: left;">Cause</td>
<td style="text-align: center;">0.58</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">Increases Risk</td>
<td style="text-align: center;">0.58</td>
</tr>
</tbody>
</table>
<p>Table 3: Specificity: More extensive descriptions of variables/concepts.</p>
<h1>4 Discussion</h1>
<p>In this work, we explored if LLMs could be used to complement and speed up the workflow of researchers by automatically scoring edges in potential DAGs. For the relatively simple and wellstudied DAGs that we tested GPT-3 on, the results were overall encouraging as the performance</p>
<p>reached much higher than $50 \%$ accuracy (random guessing) on all DAGs for at least one of the tested settings (e.g., prompt or linking verb). In this analysis, we found that GPT-3's accuracy performance was influenced by different prompts and linking verbs between variables of interest.</p>
<p>To the best of our knowledge, this is the first study to examine using LLM for causal diagram development in the medical context. Though there is growing interest, to date, there are few studies exploring the utility of LLM in causal diagram development. A recent study by Willig et al. [2022] compared the performance of three query LLMs in making causal graph predictions in a general context. There also has been some interesting works applying causal inference in the LLM context. For instance, Vig et al. [2020] investigated gender bias present in LLM using causal mediation analysis. Feder et al. [2021] released a preprint of a consolidated exploration of causal inference situated in NLP. These works suggest more focus is being devoted to researching how causal inference can be applied to LLMs and NLP.</p>
<p>Furthermore, there has been some research investigating LLM's ability to answer and reason with medical text data. Several recent studies [Liévin et al., 2022, Guo et al., 2022] showed promising results on LLMs ability to answer medical exam questions. Others [Moradi et al., 2021, Gutiérrez et al., 2022] have shown that context-specific LLMs such as BioBert are able to outperform GPT-3 in medical domain NLP tasks.</p>
<p>Limitations This study has some limitations. First, it must be acknowledged that the updating of LLMs, themselves as well as the data they are trained upon, lags behind the availability of new medical literature, and, thus may not be useful for informing the building of DAGs for novel diseases. Additionally, GPT-3 was trained upon the corpus of text data uploaded to the internet. The language used on the broader internet is likely more casual with the use of causal language than the medical academic literature [Haber et al., 2022]. Lastly, the way in which we probed GPT-3's ability to draw an edge between variables assumes that the causal connections between variables would be well-established in the corpus of text data.</p>
<p>Future work Future work aims to use a medical language context-specific LLM such as web-GPT with PubMed or BioBert [Lee et al., 2020] to signal the presence or absence of edges in DAGs using medical terminology. Additionally, since our preliminary evaluations only examined the presence/absence of arrows and their direction, upcoming projects will be focused on controlling for acyclicity amongst variables, another important characteristic of DAGs.</p>
<h1>5 Conclusion</h1>
<p>Our results illustrate that GPT-3's level of accuracy in confirming an edge connecting two variables in a DAG depends on the language used to describe the relationship. Presently, expert opinion is the most valuable tool for constructing DAGs; however, like LLMs, experts are not exempt from making errors resulting in imperfect or erroneous DAGs via omission of important confounder variables [Oates et al., 2017]. These imperfections highlight that the use of LLMs to build DAGs should be, at present, only conducted with expert verification. We see LLMs providing utility in extracting common knowledge from medical text which when paired with expert knowledge may present a more efficient means to generate comprehensive DAGs.</p>
<p>Large Language Models represent an exciting opportunity to extract common knowledge from the medical literature to complement and speed up DAG creation, but further research must be done to address the limitations reported above.</p>
<h1>References</h1>
<p>T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.
A. Feder, K. A. Keith, E. Manzoor, R. Pryzant, D. Sridhar, Z. Wood-Doughty, J. Eisenstein, J. Grimmer, R. Reichart, M. E. Roberts, et al. Causal inference in natural language processing: Estimation, prediction, interpretation and beyond. arXiv preprint arXiv:2109.00725, 2021.
S. Greenland and B. Brumback. An overview of relations among causal modelling methods. International journal of epidemiology, 31(5):1030-1037, 2002.
S. Greenland and J. Pearl. Causal diagrams. Encyclopedia of Epidemiology, 2006.
S. Greenland, J. Pearl, and J. M. Robins. Causal diagrams for epidemiologic research. Epidemiology, pages $37-48,1999$.
Q. Guo, S. Cao, and Z. Yi. A medical question answering system using large language models and knowledge graphs. International Journal of Intelligent Systems, 37(11):8548-8564, 2022. doi: https://doi.org/10.1002/int. 22955.
B. J. Gutiérrez, N. McNeal, C. Washington, Y. Chen, L. Li, H. Sun, and Y. Su. Thinking about gpt-3 in-context learning for biomedical ie? think again. arXiv preprint arXiv:2203.08410, 2022.
N. Haber, S. Wieten, J. Rohrer, O. Arah, P. Tennant, E. Stuart, E. Murray, S. Pilleron, S. Lam, E. Riederer, et al. Causal and associational language in observational health research: a systematic evaluation. American Journal of Epidemiology, 2022.
M. A. Hernán, S. Hernández-Díaz, M. M. Werler, and A. A. Mitchell. Causal knowledge as a prerequisite for confounding evaluation: an application to birth defects epidemiology. American journal of epidemiology, 155(2):176-184, 2002.
M. A. Hernán, S. Hernández-Díaz, and J. M. Robins. A structural approach to selection bias. Epidemiology, pages 615-625, 2004.
T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234-1240, 2020.
V. Liévin, C. E. Hother, and O. Winther. Can large language models reason about medical questions? arXiv preprint arXiv:2207.08143, 2022.
P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.
M. Moradi, K. Blagec, F. Haberl, and M. Samwald. Gpt-3 models are poor few-shot learners in the biomedical domain. arXiv preprint arXiv:2109.02555, 2021.
C. Oates, J. Kasza, J. Simpson, and A. Forbes. Repair of partly misspecified causal diagrams. Epidemiology, 28, 2017.
J. Pearl. Causal diagrams for empirical research. Biometrika, 82(4):669-688, 1995.
J. Pearl. Causal inference in statistics: An overview. Statistics Surveys, 2009.
J. Pearl. The eight pillars of causal wisdom. UCLA, 2017.
V. Raghupathi, W.; Raghupathi. Big data analytics in healthcare- promise and potential. Health Information Science and Systems, 2, 2014.
J. M. Robins. Data, design, and background knowledge in etiologic inference. Epidemiology, pages $313-320,2001$.</p>
<p>J. Vig, S. Gehrmann, Y. Belinkov, S. Qian, D. Nevo, Y. Singer, and S. Shieber. Investigating gender bias in language models using causal mediation analysis. Advances in Neural Information Processing Systems, 33:12388-12401, 2020.
M. Willig, M. Zečević, D. S. Dhami, and K. Kersting. Can foundation models talk causality? arXiv preprint arXiv:2206.10591, 2022.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*stephanie.long@mail.mcgill.ca&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7222 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7222</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7222</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-136.html">extraction-schema-136</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <p><strong>Paper ID:</strong> paper-274159608</p>
                <p><strong>Paper Title:</strong> The Antecedents of Transformer Models</p>
                <p><strong>Paper Abstract:</strong> Transformer models of language represent a step change in our ability to account for cognitive phenomena. Although the specific architecture that has garnered recent interest is quite young, many of its components have antecedents in the cognitive science literature. In this article, we start by providing an introduction to large language models aimed at a general psychological audience. We then highlight some of the antecedents, including the importance of scale, instance-based memory models, paradigmatic association and systematicity, positional encodings of serial order, and the learning of control processes. This article offers an exploration of the relationship between transformer models and their precursors, showing how they can be understood as a next phase in our understanding of cognitive processes.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7222.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7222.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 on standardized tests</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pretrained Transformer 4 performance on standardized tests and math competitions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper reports that GPT-4 surpasses an average human's performance on several standardized tests (LSAT, SAT) and math competitions (AMC, AIME) as cited from an external benchmark study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AGIEval: A human-centric benchmark for evaluating foundation models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based large language model developed by OpenAI; autoregressive next-token prediction architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>LSAT, SAT, American Mathematics Competitions (AMC), American Invitational Mathematics Examination (AIME)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>standardized cognitive/academic tests; mathematical reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Standardized examination formats assessing verbal, logical, and mathematical problem solving and reasoning; administered as benchmark tasks to evaluate model capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported qualitatively as 'surpasses an average human's performance' (no numeric score reported in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>AGIEval: A human-centric benchmark for evaluating foundation models</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper cites Zhong et al. (2023) for this claim but does not report numeric scores, metrics, or statistical comparisons in the present article. Exact human baselines and GPT-4 numeric results are not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Antecedents of Transformer Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7222.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7222.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM scaling claim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM variants exceeding human performance as scale increases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites results showing PaLM family variants improve with size and that performance can exceed human baselines as model parameters approach ~10^12.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Palm: Scaling language modeling with pathways</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM (variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pathways Language Model family — transformer-based LLMs evaluated across scaling regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>≈1e12 (discussed as the scale at which human-level performance is exceeded)</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>General benchmark/benchmark suite tasks (implied; not a single named cognitive test in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>general language/benchmark tasks (reasoning/knowledge)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Model performance on a variety of benchmark tasks was examined as a function of model parameter count; specific tasks are not enumerated in this paper's text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported qualitatively as 'exceeding human performance' near 10^12 parameters (no numeric values provided here)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Palm: Scaling language modeling with pathways</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>This paper summarizes Chowdhery et al. (2022)'s observation about scale; no task-level scores or human baselines are given in the present article.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Antecedents of Transformer Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7222.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7222.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BIG-bench Lite scaling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model performance on BIG-bench Lite as a function of size</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper references Srivastava et al. (2022) showing improvements on the BIG-bench Lite test bank with increasing model size.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Referenced analysis across multiple language model sizes (various transformer-based LLMs) on the BIG-bench Lite tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>BIG-bench Lite</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>benchmark suite (multiple cognitive and linguistic tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A condensed subset of BIG-bench tasks intended to probe diverse capabilities of language models; performance tracked across model sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performance increases with model size; specific numeric comparisons to human baselines are not reported in this article.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The present article only summarizes the size-performance relationship from Srivastava et al.; no task-level human baselines or p-values are provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Antecedents of Transformer Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7222.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7222.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 analogy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 zero-shot analogical reasoning ability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cites Webb et al. (2023) reporting that GPT-3 exhibits an emergent zero-shot ability to solve a broad range of analogy problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent analogical reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based autoregressive language model (as described earlier in the paper; cited with 175B parameters for GPT-3).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Analogy problems (broad range)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>analogical reasoning / relational reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Analogy tasks that require mapping relationships between elements (zero‑shot evaluation implies no in-task examples provided).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Described qualitatively as possessing an emergent zero‑shot ability to solve many analogy problems; no numeric accuracy or direct human comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>zero-shot (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td>Emergent analogical reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The present article quotes Webb et al.'s qualitative conclusion; numeric performance and human-baseline figures are not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Antecedents of Transformer Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7222.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7222.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM performance on cognitive psychology tests and the corresponding human baseline results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Emergent abilities (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Emergent capabilities appearing at large model scale</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>References Wei et al. (2022) to note that larger models display emergent abilities (e.g., logical deduction, physical intuition) that are absent in smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent abilities of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>General observation across transformer-based LLMs analyzed by Wei et al.; not tied to a single model instance in this article.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_name</strong></td>
                            <td>Various emergent capability probes (logical deduction, physical intuition, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>test_category</strong></td>
                            <td>reasoning / domain knowledge / problem solving</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Tasks probing logical, commonsense, and domain reasoning abilities which are reported to appear only beyond certain model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Qualitative claim that such abilities appear only when model scale reaches sizes comparable to GPT-3; no numeric comparisons provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_data_source</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>notes</strong></td>
                            <td>The paper uses Wei et al. (2022) to support a qualitative claim about emergent behaviors with scale; no task-level numbers or human baselines are included in this article.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Antecedents of Transformer Models', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AGIEval: A human-centric benchmark for evaluating foundation models <em>(Rating: 2)</em></li>
                <li>Emergent analogical reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models <em>(Rating: 2)</em></li>
                <li>Palm: Scaling language modeling with pathways <em>(Rating: 2)</em></li>
                <li>Emergent abilities of large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7222",
    "paper_id": "paper-274159608",
    "extraction_schema_id": "extraction-schema-136",
    "extracted_data": [
        {
            "name_short": "GPT-4 on standardized tests",
            "name_full": "Generative Pretrained Transformer 4 performance on standardized tests and math competitions",
            "brief_description": "Paper reports that GPT-4 surpasses an average human's performance on several standardized tests (LSAT, SAT) and math competitions (AMC, AIME) as cited from an external benchmark study.",
            "citation_title": "AGIEval: A human-centric benchmark for evaluating foundation models",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Transformer-based large language model developed by OpenAI; autoregressive next-token prediction architecture.",
            "model_size": null,
            "test_name": "LSAT, SAT, American Mathematics Competitions (AMC), American Invitational Mathematics Examination (AIME)",
            "test_category": "standardized cognitive/academic tests; mathematical reasoning",
            "test_description": "Standardized examination formats assessing verbal, logical, and mathematical problem solving and reasoning; administered as benchmark tasks to evaluate model capabilities.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Reported qualitatively as 'surpasses an average human's performance' (no numeric score reported in this paper)",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "AGIEval: A human-centric benchmark for evaluating foundation models",
            "statistical_significance": null,
            "notes": "The paper cites Zhong et al. (2023) for this claim but does not report numeric scores, metrics, or statistical comparisons in the present article. Exact human baselines and GPT-4 numeric results are not provided here.",
            "uuid": "e7222.0",
            "source_info": {
                "paper_title": "The Antecedents of Transformer Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "PaLM scaling claim",
            "name_full": "PaLM variants exceeding human performance as scale increases",
            "brief_description": "The paper cites results showing PaLM family variants improve with size and that performance can exceed human baselines as model parameters approach ~10^12.",
            "citation_title": "Palm: Scaling language modeling with pathways",
            "mention_or_use": "mention",
            "model_name": "PaLM (variants)",
            "model_description": "Pathways Language Model family — transformer-based LLMs evaluated across scaling regimes.",
            "model_size": "≈1e12 (discussed as the scale at which human-level performance is exceeded)",
            "test_name": "General benchmark/benchmark suite tasks (implied; not a single named cognitive test in this paper)",
            "test_category": "general language/benchmark tasks (reasoning/knowledge)",
            "test_description": "Model performance on a variety of benchmark tasks was examined as a function of model parameter count; specific tasks are not enumerated in this paper's text.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Reported qualitatively as 'exceeding human performance' near 10^12 parameters (no numeric values provided here)",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Palm: Scaling language modeling with pathways",
            "statistical_significance": null,
            "notes": "This paper summarizes Chowdhery et al. (2022)'s observation about scale; no task-level scores or human baselines are given in the present article.",
            "uuid": "e7222.1",
            "source_info": {
                "paper_title": "The Antecedents of Transformer Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "BIG-bench Lite scaling",
            "name_full": "Model performance on BIG-bench Lite as a function of size",
            "brief_description": "The paper references Srivastava et al. (2022) showing improvements on the BIG-bench Lite test bank with increasing model size.",
            "citation_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "Referenced analysis across multiple language model sizes (various transformer-based LLMs) on the BIG-bench Lite tasks.",
            "model_size": null,
            "test_name": "BIG-bench Lite",
            "test_category": "benchmark suite (multiple cognitive and linguistic tasks)",
            "test_description": "A condensed subset of BIG-bench tasks intended to probe diverse capabilities of language models; performance tracked across model sizes.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Performance increases with model size; specific numeric comparisons to human baselines are not reported in this article.",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "statistical_significance": null,
            "notes": "The present article only summarizes the size-performance relationship from Srivastava et al.; no task-level human baselines or p-values are provided here.",
            "uuid": "e7222.2",
            "source_info": {
                "paper_title": "The Antecedents of Transformer Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "GPT-3 analogy",
            "name_full": "GPT-3 zero-shot analogical reasoning ability",
            "brief_description": "Cites Webb et al. (2023) reporting that GPT-3 exhibits an emergent zero-shot ability to solve a broad range of analogy problems.",
            "citation_title": "Emergent analogical reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_description": "Transformer-based autoregressive language model (as described earlier in the paper; cited with 175B parameters for GPT-3).",
            "model_size": "175B",
            "test_name": "Analogy problems (broad range)",
            "test_category": "analogical reasoning / relational reasoning",
            "test_description": "Analogy tasks that require mapping relationships between elements (zero‑shot evaluation implies no in-task examples provided).",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Described qualitatively as possessing an emergent zero‑shot ability to solve many analogy problems; no numeric accuracy or direct human comparison provided in this paper.",
            "prompting_method": "zero-shot (as cited)",
            "fine_tuned": null,
            "human_data_source": "Emergent analogical reasoning in large language models",
            "statistical_significance": null,
            "notes": "The present article quotes Webb et al.'s qualitative conclusion; numeric performance and human-baseline figures are not given here.",
            "uuid": "e7222.3",
            "source_info": {
                "paper_title": "The Antecedents of Transformer Models",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Emergent abilities (general)",
            "name_full": "Emergent capabilities appearing at large model scale",
            "brief_description": "References Wei et al. (2022) to note that larger models display emergent abilities (e.g., logical deduction, physical intuition) that are absent in smaller models.",
            "citation_title": "Emergent abilities of large language models",
            "mention_or_use": "mention",
            "model_name": null,
            "model_description": "General observation across transformer-based LLMs analyzed by Wei et al.; not tied to a single model instance in this article.",
            "model_size": null,
            "test_name": "Various emergent capability probes (logical deduction, physical intuition, etc.)",
            "test_category": "reasoning / domain knowledge / problem solving",
            "test_description": "Tasks probing logical, commonsense, and domain reasoning abilities which are reported to appear only beyond certain model scales.",
            "evaluation_metric": null,
            "human_performance": null,
            "llm_performance": "Qualitative claim that such abilities appear only when model scale reaches sizes comparable to GPT-3; no numeric comparisons provided here.",
            "prompting_method": null,
            "fine_tuned": null,
            "human_data_source": null,
            "statistical_significance": null,
            "notes": "The paper uses Wei et al. (2022) to support a qualitative claim about emergent behaviors with scale; no task-level numbers or human baselines are included in this article.",
            "uuid": "e7222.4",
            "source_info": {
                "paper_title": "The Antecedents of Transformer Models",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AGIEval: A human-centric benchmark for evaluating foundation models",
            "rating": 2,
            "sanitized_title": "agieval_a_humancentric_benchmark_for_evaluating_foundation_models"
        },
        {
            "paper_title": "Emergent analogical reasoning in large language models",
            "rating": 2,
            "sanitized_title": "emergent_analogical_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "rating": 2,
            "sanitized_title": "beyond_the_imitation_game_quantifying_and_extrapolating_the_capabilities_of_language_models"
        },
        {
            "paper_title": "Palm: Scaling language modeling with pathways",
            "rating": 2,
            "sanitized_title": "palm_scaling_language_modeling_with_pathways"
        },
        {
            "paper_title": "Emergent abilities of large language models",
            "rating": 2,
            "sanitized_title": "emergent_abilities_of_large_language_models"
        }
    ],
    "cost": 0.01077775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Antecedents of Transformer Models</p>
<p>Simon Dennis simon.dennis@unimelb.edu.au 
School of Psychological Sciences
University of Melbourne</p>
<p>Kevin Shabahang 
School of Psychological Sciences
University of Melbourne</p>
<p>Hyungwook Yim 
Department of Cognitive Sciences
Hanyang University</p>
<p>School of Psychological Sciences
University of Melbourne</p>
<p>The Antecedents of Transformer Models
56672BB470B8E5CEC0E7B505E26A053810.1177/09637214241279504large language models, cognitive psychology
Transformer models of language represent a step change in our ability to account for cognitive phenomena.Although the specific architecture that has garnered recent interest is quite young, many of its components have antecedents in the cognitive science literature.In this article, we start by providing an introduction to large language models aimed at a general psychological audience.We then highlight some of the antecedents, including the importance of scale, instance-based memory models, paradigmatic association and systematicity, positional encodings of serial order, and the learning of control processes.This article offers an exploration of the relationship between transformer models and their precursors, showing how they can be understood as a next phase in our understanding of cognitive processes.</p>
<p>PSYCHOLOGICAL SCIENCE</p>
<p>Transformer-Based Large Language Models</p>
<p>Large language models based on the transformer architecture (Bahdanau et al., 2014;Vaswani et al., 2017) represent a step change in cognitive modeling.For the first time, we have computational models capable of human-level performance on cognitive tasks.The Generative Pretrained Transformer 4 (GPT-4) now surpasses an average human's performance on various standardized tests, such as the Law School Admission Test (LSAT) and the SAT, and math competitions, including the American Mathematics Competitions and the American Invitational Mathematics Examination (Zhong et al., 2023).In typical cognitive tasks, transformer models perform well.For instance, Webb et al. (2023) conclude that "large language models such as GPT-3 have acquired an emergent ability to find zero-shot 1 solutions to a broad range of analogy problems."Although there has been critique of the transformers model's ability to capture some higher-level cognitive tasks (Binz &amp; Schulz, 2022;Chomsky et al., 2023;Han et al., 2022;Mahowald et al., 2023), many of these objections have not been sustained as the models have become larger and training sets have expanded (Han et al., 2024).What the success of transformer models demonstrates is that the information required to learn the control processes and much of the representational substrate of the cognitive system exists in language input and can be extracted using general-purpose learning rules (Piantadosi, 2023).In the next section, we will give a brief overview of the transformer model before discussing its antecedents.</p>
<p>How Does a Transformer Work?</p>
<p>The task of a transformer is to predict the next word given the words that have occurred so far.For instance, given the context "I always found Janice amiable, but not everyone likes" a good completion might be the word "her."To create longer responses, the transformer takes its guess at the next word and adds it to the input and then predicts what should come next.It continues generating new words until it predicts that it has reached the end of the response.</p>
<p>Although the immediately preceding words can be helpful in deciding what comes next, often, critical information appears sometime in the past.In the example just given, if one looks only at the immediately preceding words, "not everyone likes," many possibilities are reasonable: "him," "it," "football," "politicians."One needs more context in order to narrow down the options.In this case, the important word is "Janice."But "Janice" appears six words back, and other words that are not useful, such as "amiable" and "everyone," appear in between.The model needs to ignore these and pay attention to "Janice."</p>
<p>To isolate the words of interest, the model first constructs a vector for each word in the context (Fig. 1a, yellow boxes).These vectors are constructed by adding an embedding vector that captures the meaning of the word (that is learned during the training phase) to a positional vector that codes when it appears.Then the model takes each word vector and transforms it into a key vector (red boxes) by multiplying it by a matrix of weights.The key vector can be thought of as a set of categories.For "Janice," it might mean something like "person," among other things.</p>
<p>Then, the final word of the context-"likes" in this example-is used to generate a query vector (Fig. 1b, blue boxes).The query vector represents the types of things that are relevant to determining what comes next.In this case, it might include a "person" component because people are among the things that can be liked.The model then looks for relevant words by comparing the query vector with each of the key vectors (purple arrows).The query vector and the key vector corresponding to "Janice" both contain the "person" component and hence will match.Other words, like "amiable" and "everyone," do not contain this component and so are ignored.</p>
<p>While we might consider just returning "Janice" as the next word, that would be slightly odd in this context.We would be more likely to shorten to the pronoun "her."To do that, however, we need to know that "Janice" is a female.The model takes each word and transforms it into a value vector representing features of the word, like gender (Fig. 1c, green boxes).The value vectors of the matching words are added together to create a set of all of the features from the context relevant to producing the next word (Fig. 1d, orange arrows).This output vector (dark green boxes) is then mapped to the actual word that the model chooses (black arrows).</p>
<p>In the given example, it was the single word "Janice" that was critical.However, it may be that critical information accumulates across multiple words.For instance, if we have the context "Oh, sweet melancholy, dost thou pierce my soul with thy bittersweet" "lament" might be a better completion than "candy" even though "candy" fits well with bittersweet.In this case, there is no single word that clearly favors "lament."Rather it is the combined effect of words like "melancholy," "dost," "thou," "pierce," and "soul" that identifies this as a Shakespearean style of text and suggests that a word like "lament" is more likely than a word like "candy."The query vector may match many words, in which case the output vector will contain features common to them all.For instance, the query vector might encode the idea of style, and the keys for "melancholy," "dost," "thou," "pierce," and "soul" might identify them as relevant for determining the style.The value vectors for these words would code the Shakespearean style, which would then become strongly activated in the output vector.</p>
<p>However, we would not want to compromise the model's ability to isolate specific words in order to have it extract the gist of the context.To allow both to occur, the transformer often includes multiple attention heads (Fig. 1e).Each head uses different query, key, and value transforms, which can be individually configured to focus on different kinds of information.One might be devoted to finding people, while another is looking for words that determine the style.Large models will have many heads.For instance, the GPT-3.5 model uses 96 heads (Li, 2020).All of the output vectors are presented to the final mapping (black arrows) and jointly contribute to the selection of the next word.</p>
<p>The model presented thus far contains only one layer.However, in practice, transformers typically have many layers (96 in the GPT-3.5 architecture; Li, 2020).Instead of predicting the next word directly, the word vectors predicted by one layer become the input word vectors for the next layer-so regardless of the number of layers, there is always a one-to-one correspondence between the input words and the key and value vectors at each layer.A learning algorithm (back-propagation) is used to train all of the query, key, and value transforms as well as the feed-forward networks in each layer by comparing the current predictions of the entire network with the actual next words and adjusting the weights to minimize error.</p>
<p>Although the transformer architecture on which recent advances rely is relatively new, it has many antecedents both in the computer science and cognitive science literature.In this article, we will focus on antecedents from the cognitive modeling literature that may not be widely appreciated.</p>
<p>The Importance of Scale</p>
<p>Transformer models are large in two critical ways: They contain large numbers of parameters (weights) and they are trained on large data sets.For instance, GPT-3 contains 175 billion weights and was trained on 570 GB of text (T. Brown et al., 2020).Srivastava et al. (2022, Fig. 1c) show how performance on the BIG-bench Lite test bank improves with model size.For variants of the PaLM model (Chowdhery et al., 2022), performance rises slowly until the model reaches 10 10 parameters but then accelerates, exceeding human performance as the model nears 10 12 parameters.Similarly, Touvron et al. (2023) showed that as training sets increase to 1.4 trillion tokens (~ 7 TBs of text), predictive performance is still increasing.</p>
<p>A key observation is that larger models display emergent abilities that do not appear in smaller models (Wei et al., 2022).For instance, capabilities like logical deduction and physical intuition are present only when a model reaches the size of GPT-3.</p>
<p>An early model that demonstrated the importance of scale was latent semantic analysis (LSA; Dumais et al., 1988).LSA was a significant breakthrough in semantic modeling as, for the first time, it was possible to derive semantic representations from a naturally occurring corpus rather than handcrafting representations ( Jones et al., 2015).Landauer and Dumais (1997;Landauer et al., 2007) argued that the rapid vocabulary growth that children exhibit could be explained if one assumed that the meaning of a given word was influenced by the appearance of not only itself but all other words.They proposed that scale was required to enable a robust process where the meaning of words is inferred through the complex interplay among their contextual occurrences across large datasets, effectively using the breadth of data to pinpoint the nuance of meanings.Demonstrations of LSA employed models containing nominally hundreds of millions of parameters (although many of these were zero) and naturally occurring corpora that contained tens of millions of words, which was unprecedented at the time.Before LSA, work in cognitive science and artificial intelligence tended to conform to Patrick Winston's maxim that to learn something new, you need to almost already know it (Winston, 1992).That is, to learn a new concept, you need to already know the concepts on which it relies.LSA demonstrated that was not correct.Rather, concepts could be learned through the simultaneous resolution of large numbers of constraints derived from naturally occurring datasets.This observation sparked the search for simple algorithms that could be applied to large datasets to capture emergent properties, of which the transformer architecture is the most recent example.</p>
<p>Instance-Based Memory Models</p>
<p>A key aspect of the transformer architecture that differentiates it from earlier generations of language models is the use of attention.The ability to systematically bind keys to values and then to retrieve these vectors even if they appeared thousands of tokens into the past seems to be fundamental to the performance of the model.</p>
<p>The term "attention" as used in the transformer literature should not be confused with attention as it appears in the cognitive psychology literature.What the model is really doing is a kind of memory retrieval.</p>
<p>The mathematical operations they perform are very similar to instance-based models of memory, such as Minerva II (Hintzman, 1984(Hintzman, , 1986) ) or retrieving effectively from memory (REM; Shiffrin &amp; Steyvers, 1997).</p>
<p>The correspondence is particularly clear in the case of Minerva II.Minerva II assumes that distinct experiences are retained in memory as vectors of values-one vector for each experience.These vectors encode aspects of the experience, like who was involved, where it took place, or what objects were present.To retrieve memories, a query vector is constructed that Fig. 1.A simplified one-layer transformer architecture exposed to "I always found Janice amiable but not everyone likes."(a) The model constructs a word vector for each word in the context (yellow).These vectors are constructed by adding a vector that captures the meaning of the word to a positional vector that codes when it appears.Then the model takes each word vector and transforms it into a key vector (red).The key vector can be thought of as a set of categories.For "Janice," it might mean something like "person," among other things.(b) Then, the final word of the context-"likes" in this example-is used to generate a query vector (blue).The query vector represents the types of things that are relevant to determining what comes next.In this case, it might include a "person" component.The model then looks for relevant words by comparing the query vector with each of the key vectors (purple arrows).The query vector and the key vector corresponding to "Janice" both contain the "person" component and hence will match.Other words, like "amiable" and "everyone," do not contain this component and so are ignored.(c) The model takes each word and transforms it into a value vector representing the features of the word, like gender (green).(d) The value vectors of the matching words are added together to create a set of all of the features from the context relevant to producing the next word (orange arrows).(e) This output vector (dark green boxes) is then mapped to the actual word that the model chooses (black arrows).Typically, these operations will be replicated in multiple attention heads to allow attention to isolate different aspects of the context.encodes the cues.For instance, the query vector might encode the person and place in an effort to recall the objects that were present at the time.The query vector is then compared with each of the vectors in memory.Memory vectors that contain the same person and place will have high similarity values-effectively highlighting them for retrieval.To make these memory vectors stand out even more from the very large number of less relevant vectors, the similarities are raised to the third power.Now those memories that match all of the cues will be much stronger than the other memories even if they match one of the cues.Finally, the memory vectors are multiplied by these strengths and added together to create a vector called the echo that aggregates all of the aspects from memories that are related to the cues, for instance, the objects that were seen in the experiences that involved the given person and place.</p>
<p>In Minerva II, it is assumed that subsequent cognitive operations have access to only the echo vector.That is, the memory system operates not on individual memories but always on aggregates generated as outlined previously.This global matching property explains many of the kinds of memory errors that people make (Clark &amp; Gronlund, 1996) and is shared with several other models (Gillund &amp; Shiffrin, 1984;Humphreys et al., 1989;Murdock, 1982).</p>
<p>The correspondence between Minerva II and the attention mechanism of transformer models is strong.Both models assume that there is a key vector associated with each experience (i.e., word) and that a query vector is compared with each of these to create similarities (in fact both models use the dot product to calculate similarity).Then both models assume an accelerating nonlinearity is applied: In the case of Minerva II, the operation is the cube; in the case of transformer models, it is typically the softmax function-but these functions look very similar over the relevant ranges.Then both models construct the echo (also known as output vector) by taking a sum of the value vectors weighted by the strengths (also known as attention weights).The basic mechanisms are almost identical.</p>
<p>A common complaint about transformer models is that they sometimes produce incorrect answers-socalled hallucinations.In the memory literature, these would be called false memories, and the global matching property (that transformers share with many memory models) was introduced deliberately to produce them (Clark &amp; Gronlund, 1996).</p>
<p>A key bottleneck for transformer models is the calculation of the attention strengths.For each token, the similarity with all other tokens is calculated.As the number of tokens in the context increases, this becomes computationally expensive-putting an effective limit on how far back in time retrieval can extend.An alternative would be to use a composite memory model (Humphreys et al., 1989;Murdock, 1982) in which the key-value mappings are stored in a common-memory vector or matrix rather than as a set of instances, thus removing the computational bottleneck.A model remarkably similar to the transformer architecture, but that uses a composite memory, was proposed by Dennis and Wiles (1993).We are currently working on a version of this architecture, which performs significantly better than equivalently sized transformer models in initial tests.We suspect that there will be further cases where insights from the cognitive literature will lead to improvements in large language models.</p>
<p>Paradigmatic Association and Systematicity</p>
<p>Saussure (1916) introduced the distinction between syntagmatic relations (that exist between words that co-occur, like "cat" and "ball") and paradigmatic relations (that exist between words that can appear in the same sentential slot, like "cat" and "dog").The psychological reality of these constructs, possible mechanisms, and the developmental progression have been investigated extensively (R. Brown &amp; Berko, 1960;Entwisle et al., 1964;McNeill, 1963).</p>
<p>These ideas found computational form in the syntagmatic paradigmatic (SP) model (Dennis, 2004(Dennis, , 2005; see also Sloutsky et al., 2017).The SP model assumes separate syntagmatic and paradigmatic memory stores.The paradigmatic store contains key-value vector pairs one for each input token, like transformer models.The keys are derived from syntagmatic retrieval based on the surrounding words and the values are the tokens themselves.During generation, the surrounding words are used to construct a query, which is matched against the keys, and then a weighted retrieval of the values becomes the output-quite similar to how the transformer architecture works.</p>
<p>A key motivation for Dennis (2004Dennis ( , 2005) ) was the inability of connectionist architectures to capture the systematicity of human thought (Fodor &amp; Pylyshyn, 1988).If someone can understand the sentence "John loves Mary," they can also understand the sentence "Mary loves John."Fodor and Pylyshyn (1988) argued that neural networks do not embody these constraints and were therefore inappropriate models of human cognition.They proposed that to capture the underlying structure of sentences, one needed a way to systematically bind roles to fillers and that this mechanism needed to be built into the architecture rather than learned from experience."John loves Mary" might be represented as {lover=&gt;John, lovee=&gt;Mary}, while "Mary loves John" might be represented as {lover=&gt;Mary, lovee=&gt;John}.The representations of "John," "Mary," and "loves" might be learned, but the space of possible bindings is so large that the ability to form each binding could not be learned given the number of examples people experience.Subsequently, Phillips and Halford (1997) confirmed that feed-forward back-propagation networks and simple recurrent networks were not able to systematically generalize to the extent that people do.</p>
<p>Key-value bindings in transformer networks allow for the rapid binding of roles and fillers.A systematic investigation is in order, but it seems likely that the introduction of key-value bindings is the key inductive bias that allows transformers to generalize in combinatorial domains much more efficiently than previous neural architectures-just as the use of paradigmatic associations was the key to the performance of the SP model.While Fodor and Pylyshyn (1988) were thinking of purely symbolic architectures when they brought their critique, the fact that it has taken the introduction of a more structured mechanism in the form of attention/paradigmatic association in order to capture human cognition is a vindication of their concerns.</p>
<p>Positional Encodings of Serial Order</p>
<p>In many languages, word order matters (Dennis, 2007)."John loves Mary" is not the same statement as "Mary loves John."How people retain the order with which stimuli are presented has been a subject of study since the 1800s (Nipher, 1876).Three approaches have been proposed.The chaining model suggests that associations are formed between successive items and that recalling a sequence involves starting with the first and traversing the chain (Ebbinghaus, 1885(Ebbinghaus, /1964)).The primacy model suggests that items are stored with descending strengths, and recall proceeds by retrieving the strongest, suppressing it, retrieving the next strongest, suppressing it, and so forth (Page &amp; Norris, 1998).The positional model assumes that there is a separate cue associated with each position within a list, and recall involves applying each position cue in turn and retrieving the item that was associated with it (Henson, 1998).Large language models prior to the transformer can be seen as sophisticated versions of the chaining model in which the prior-context state is used to predict the next item.The transformer architecture removed this context vector and relied exclusively on key-value bindings.Without modification, however, this would have removed its sensitivity to word order.As a consequence, transformers inject a positional cue into the representation of each token, making them akin to traditional positional models.</p>
<p>How one constructs the cues in positional models varies.The approach taken in transformer models is very similar to that employed by the oscillator-based associative recall (OSCAR) model proposed by G. D. Brown et al. (2000; see also Burgess &amp; Hitch, 1999).In OSCAR, a context vector is constructed from units that oscillate at different frequencies.At encoding, the context vector is associated with vectors representing the items to be retained.At retrieval, the oscillators are reset and the context vector is allowed to move forward again.The evolving context vectors are used to retrieve the item vectors one at a time.The oscillators that are changing slowly distinguish between the start and end of the list, while the oscillators that change quickly distinguish between successive items (but not the start and end of the list).Together they form a timing code that captures the position of the item.</p>
<p>In a similar way, transformer models construct a positional code from banks of oscillating units of different frequencies (Dai et al., 2019;Vaswani et al., 2017).In transformer models, the positional vector is typically added to the item vector rather than associated with it.If these item-plus-position vectors were added together as is the case in OSCAR, this approach would not be sufficient to support serial recall.However, in transformer models, the vectors are added into an instance-based memory, which ensures that the binding between the position and item information is maintained.If one were to trial a composite memory model as suggested earlier, care needs to be taken to incorporate positional information in an appropriate fashion.</p>
<p>Learning Control Processes</p>
<p>One of the most impressive and disruptive capacities of transformer models is the ability to learn the control processes that determine what representations are constructed and which operations are applied to manage the flow of information from input stimuli to system responses (e.g., assembling cues to initiate retrieval or selecting responses from information that has been retrieved).Although the cognitive psychology literature has often focused on the modeling of separate tasks, transformers can be applied to different tasks without modification (Riveland &amp; Pouget, 2022).In the cognitive modeling literature, there have been attempts to maintain some consistency of representational substrate.For instance, the global matching models of memory used common-memory representations to account for recognition, cued recall, free recall, and sometimes serial recall (Gillund &amp; Shiffrin, 1984;Hintzman, 1984;Humphreys et al., 1989;Lewandowsky &amp; Murdock, 1989;Murdock, 1982;Raaijmakers &amp; Shiffrin, 1981).Nonetheless, each task involved a separate algorithm.Just as the handcrafting of representations in semantic memory models led to accounts with unreasonably large and hidden degrees of freedom ( Jones et al., 2015), the handcrafting of control processes undermines our ability to formally compare models and has backgrounded attempts to understand the commonalities between cognitive capabilities.</p>
<p>There is, however, precedent for attempting to learn control.Dennis (2005) argued for the development of task-independent models that treated the instructions as inputs to a more general model of the cognitive system.As a step toward the development of such a model, he introduced a few-shot learning approach in which the input contains a small number of examples of the task of interest and the model is expected to generalize to new cases of the same task (creating a task virtual machine, as Doug Mewhort [personal communication] referred to it).The SP model highlighted the commonalities between a number of tasks, including sentence processing, semantic categorization and rating, short-term serial recall, and analogical and logical inference and made extensive use of paradigmatic association (key-value pairs) to explain performance.As such, it seems like it might be a particularly fertile place to look for inspiration when trying to understand the emergent properties of transformer models.</p>
<p>The transformer architecture, however, takes the acquisition of control to a new level as it is often the case that no prior examples need to be provided-socalled zero-shot learning.Box 1 shows a conversation with ChatGPT in which it is given a list of words and then asked to recall the list, recognize items from the list, generalize the category of the list items to a new item, and make judgments about the size of the items in the list.In each case, it provides correct responses without being reconfigured for the specific task.The ability of transformers to be applied flexibly to arbitrary tasks and to account for the acquisition of control processes are major advantages over existing cognitive models.</p>
<p>Investigator: Remember the following list of words: cake, tree, child, hammer, table, pencil.Recall the list: Is there anything else I can help you with?</p>
<p>Investigator: Did child appear on the list?</p>
<p>ChatGPT: Yes, "child" did appear on the list.The list you provided includes the word "child."</p>
<p>Investigator: Did forest appear on the list?</p>
<p>ChatGPT: No, the word "forest" did not appear on the list you provided.The list included the following words: cake, tree, child, hammer, table, pencil.</p>
<p>Investigator: Does "justice" belong to the same category as the items on the list?</p>
<p>ChatGPT: No, "justice" does not belong to the same category as the items on the list.The items on the list are primarily concrete objects or entities such as cake, tree, child, hammer, table, and pencil."Justice," on the other hand, is an abstract concept related to fairness and moral righteousness.It does not fall into the same category as the tangible objects listed.</p>
<p>Investigator: Which of the items from the list would fit in a shoe box?</p>
<p>ChatGPT: From the list you provided, the items that would likely fit in a shoebox are: Pencil: A pencil is small and slender, making it easy to fit inside a shoebox.</p>
<p>Hammer: Depending on the size of the shoebox, a smaller hammer may be able to fit.</p>
<p>The other items on the list, such as cake, tree, child, and table, are larger and would not typically fit inside a standard shoebox Box 1.A conversation with ChatGPT demonstrating its ability to address different tasks without modification</p>
<p>Fig. 1 .
1
Fig. 1. (continued on next page)</p>
<p>ChatGPT:</p>
<p>Certainly!Here's the list of words you provided:</p>
<p>DiscussionWhether by causal connection or convergent evolution, many key aspects of transformer models have antecedents in the cognitive science literature.In this article, we have highlighted some of these, including the emergence of new properties with corpus scale, instancebased memory models, paradigmatic association and systematicity, positional encodings of serial order, and the learning of control processes.There are more, including, of course, the use of the back-propagation learning algorithm and the development of hidden unit representations, which is intrinsic to the basic processes of transformers and gained much of its impetus in the cognitive modeling literature(Rumelhart et al., 1986).However, these contributions are more likely to be generally understood, and so we have focused on those components that may not be as universally appreciated.One cannot help but feel that we are at a major milestone in the development of cognitive models.Yet understanding how transformer models achieve their competence remains a significant challenge.We do not begin from a tabula rasa, however.The existing cognitive modeling literature provides a foundation from which to start, and by highlighting the parallels, we hope to facilitate the process of mining the literature for insight.Recommended ReadingContreras Kallens, P.,Kristensen-McLachlan, R. D., &amp; Christiansen, M. H. (2023).Large language models demonstrate the potential of statistical learning in language.Cognitive Science, 47(3), Article e13256.A recent article outlining how large language models demonstrate that some form of statistical learning is a viable approach to understanding language.Piantadosi, S. (2023).(See References).A systematic rebuttal of Chomsky's arguments about language learnability using large language models as an existance proof.Rumelhart, D. E., &amp; McClelland, J. L., &amp; PDP Research Group (Eds.).(1986).(See References).An introduction to neural networks and their application in psychology.TransparencyAction Editor: Robert L. Goldstone Editor: Robert L. GoldstoneDeclaration of Conflicting InterestsThe author(s) declared that there were no conflicts of interest with respect to the authorship or the publication of this article.
Neural machine translation by jointly learning to align and translate. D Bahdanau, K Cho, Y Bengio, 2014</p>
<p>Using cognitive psychology to understand GPT-3. M Binz, E Schulz, 2022</p>
<p>Oscillatorbased memory for serial order. G D Brown, T Preece, C Hulme, Psychological Review. 10711272000</p>
<p>Word association and the acquisition of grammar. R Brown, J Berko, Child Development. 311960</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, D Amodei, Advances in Neural Information Processing Systems. 202033</p>
<p>Memory for serial order: A network model of the phonological loop and its timing. N Burgess, G J Hitch, Psychological Review. 10635511999</p>
<p>Noam Chomsky: The false promise of ChatGPT. N Chomsky, I Roberts, J Watumull, The New York Times. 2023. March 8</p>
<p>Palm: Scaling language modeling with pathways. A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, N Fiedel, 2022</p>
<p>Global matching models of recognition memory: How the models match the data. S E Clark, S D Gronlund, Psychonomic Bulletin &amp; Review. 311996</p>
<p>Transformer-XL: Attentive language models beyond a fixed-length context. Z Dai, Z Yang, Y Yang, J Carbonell, Q V Le, R Salakhutdinov, ArXiv. 2019</p>
<p>An unsupervised method for the extraction of propositional information from text. S Dennis, Proceedings of the National Academy of Sciences. 1012004Suppl. 1</p>
<p>A memory-based theory of verbal cognition. S Dennis, Cognitive Science. 2922005</p>
<p>Introducing word order within the LSA framework. S Dennis, Handbook of latent semantic analysis. T K Landauer, D S Mcnamara, S Dennis, &amp; W Kintsch, Lawrence Erlbaum Associates2007</p>
<p>Integrating learning into models of human memory: The Hebbian recurrent network. S Dennis, J Wiles, Proceedings of the fifteenth annual conference of the Cognitive Science Society: Institute of Cognitive Science. the fifteenth annual conference of the Cognitive Science Society: Institute of Cognitive SciencePsychology Press1993. June 18-2115394University of Colorado-Boulder</p>
<p>Using latent semantic analysis to improve access to textual information. S T Dumais, G W Furnas, T K Landauer, S Deerwester, R Harshman, 10.1145/57167.57214CHI88: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. CHI88: Human Factors in Computing Systems. Washington, DC1988. May 15-19Conference session</p>
<p>On memory. H Ebbinghaus, 1964. 1885Dover Edition</p>
<p>The syntactic-paradigmatic shift in children's word associations. D R Entwisle, D F Forsyth, R Muuss, Journal of Verbal Learning and Verbal Behavior. 311964</p>
<p>Connectionism and cognitive architecture: A critical analysis. J A Fodor, Z W Pylyshyn, Cognition. 281-21988</p>
<p>A retrieval model for both recognition and recall. G Gillund, R M Shiffrin, Psychological Review. 91111984</p>
<p>Human-like property induction is a challenge for large language models. S J Han, K J Ransom, A Perfors, C Kemp, Proceedings of the Annual Meeting of the Cognitive Science Society. the Annual Meeting of the Cognitive Science Society202244</p>
<p>Inductive reasoning in humans and large language models. S J Han, K J Ransom, A Perfors, C Kemp, Cognitive Systems Research. 832024. Article 101155</p>
<p>Short-term memory for serial order: The start-end model. R N Henson, Cognitive Psychology. 3621998</p>
<p>MINERVA 2: A simulation model of human memory. D L Hintzman, Behavior Research Methods, Instruments, &amp; Computers. 1621984</p>
<p>Schema abstraction" in a multipletrace memory model. D L Hintzman, Psychological Review. 9344111986</p>
<p>Different ways to cue a coherent memory system: A theory for episodic, semantic, and procedural tasks. M S Humphreys, J D Bain, R Pike, Psychological Review. 9622081989</p>
<p>Models of semantic memory. M N Jones, J Willits, S Dennis, The Oxford handbook of computational and mathematical psychology. J R Busemeyer, Z Wang, J T Townsend, &amp; A Eidels, Oxford University Press2015</p>
<p>A solution to Plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. T K Landauer, S T Dumais, Psychological Review. 10422111997</p>
<p>Handbook of latent semantic analysis. T K Landauer, D S Mcnamara, S Dennis, &amp; Kintsch, W.2007Psychology Press</p>
<p>Memory for serial order. S Lewandowsky, B B Murdock, Jr, Psychological Review. 961251989</p>
<p>OpenAI's GPT-3 language model: A technical overview. C Li, Lambda Labs. 2020</p>
<p>Dissociating language and thought in large language models: A cognitive perspective. K Mahowald, A A Ivanova, I A Blank, N Kanwisher, J B Tenenbaum, E Fedorenko, ArXiv. 2023</p>
<p>The origin of associations within the same grammatical class. D Mcneill, Journal of Verbal Learning and Verbal Behavior. 231963</p>
<p>A theory for the storage and retrieval of item and associative information. B B Murdock, Psychological Review. 891982</p>
<p>On the distribution of numbers written from memory. F E Nipher, Transactions of the Academy of St. Louis. 31876</p>
<p>The primacy model: A new model of immediate serial recall. M Page, D Norris, Psychological Review. 10547611998</p>
<p>Systematicity: Psychological evidence with connectionist implications. S Phillips, G S Halford, Proceedings of the Nineteenth Annual Conference of the Cognitive Science Society. M G Shafto, P Langley, the Nineteenth Annual Conference of the Cognitive Science SocietyLawrence Erlbaum1997</p>
<p>Modern language models refute Chomsky's approach to language. S Piantadosi, 2023</p>
<p>Search of associative memory. J G Raaijmakers, R M Shiffrin, Psychological Review. 8821981</p>
<p>A neural model of task compositionality with natural language instructions. R Riveland, A Pouget, 10.1101/2022.02.22.481293v1BioRxiv. 2022</p>
<p>Parallel distributed processing: Explorations in the microstructure of cognition. D E Rumelhart, Mcclelland, J. L., &amp; PDP Research Group1986Foundations. MIT Press1</p>
<p>Course in general linguistics. F M Saussure, 1916Columbia University Press</p>
<p>A model for recognition memory: REM-retrieving effectively from memory. R M Shiffrin, M Steyvers, Psychonomic Bulletin &amp; Review. 421997</p>
<p>An associative account of the development of word learning. V M Sloutsky, H Yim, X Yao, S Dennis, Cognitive Psychology. 972017</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, A Rastogi, A Rao, A A M Shoeb, A Abid, A Fisch, H Kim, ArXiv. 2022</p>
<p>LLaMA: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M A Lachaux, T Lacroix, G Lample, 2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Emergent analogical reasoning in large language models. T Webb, K J Holyoak, H Lu, Nature Human Behaviour. 792023</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, W Fedus, Emergent abilities of large language models. 2022</p>
<p>Artificial intelligence. P H Winston, 1992Addison-Wesley Longman</p>
<p>AGIEval: A human-centric benchmark for evaluating foundation models. W Zhong, R Cui, Y Guo, Y Liang, S Lu, Y Wang, N Duan, ArXiv. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
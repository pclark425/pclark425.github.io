<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8988 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8988</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8988</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-263864711</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2021.naacl-main.278.pdf" target="_blank">Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</a></p>
                <p><strong>Paper Abstract:</strong> Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8988.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8988.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TEKGEN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text from Knowledge Graph Generator (TEKGEN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that converts groups of KG triples into natural language sentences using a sequence-to-sequence model (T5-large) fine-tuned on a distantly-supervised KG↔text aligned corpus and then refined on WebNLG to reduce hallucination, followed by a BERT-based semantic filter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>sequence-to-sequence triple linearization (concatenated triples)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Inputs are linearized groups of triples concatenated as: subject relation_1 object_1, ... relation_n object_n; the concatenated string is fed to a text-to-text Transformer (T5-large) to generate a natural-language sentence paraphrasing the triples.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td> encyclopedic knowledge graph (Wikidata KG; subject-centered subgraphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Create training pairs by distant supervision aligning Wikidata triples to Wikipedia root-section sentences; fine-tune T5-large on this aligned corpus (5000 steps) to increase coverage, then sequentially fine-tune on WebNLG (500 steps) to reduce hallucination; at inference concatenate entity-subgraph triples and decode (top-5 sampling, temperature=0.5); post-filter generated sentences with a BERT-based semantic quality scorer and remove lowest-scoring outputs (bottom 1%).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used to create a synthetic natural-language corpus (KELM) which augments the retrieval corpus for retrieval-augmented LM pre-training (REALM); downstream evaluation on open-domain QA (NaturalQuestions, WebQuestions) and knowledge probing (LAMA).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When KELM documents (TEKGEN outputs grouped by subject) were AUGMENTED to REALM's retrieval corpus (Wikipedia + KELM Documents) the pre-trained+fine-tuned REALM achieved NQ Exact Match 41.47 and WQ Exact Match 43.90 (absolute gains over ORIGINAL REALM rerun: +2.63% NQ, +3.10% WQ). AUGMENTED LAMA absolute gains vs ORIGINAL: Google-RE +12.94%, T-REx +0.95%, SQuAD +3.61%, ConceptNet +0.47%. Additional reported corpus stats: KELM corpus ~18M generated sentences spanning ~45M triples; KELM documents grouped by subject: ~5.7M documents; KELM total words ~286M (≈14% of Wikipedia 2B words).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Outperforms or complements approaches that insert raw triples as retrieval documents: AUGMENTED with KELM documents outperforms AUGMENTED with raw Triple Documents; TEKGEN+entity-subgraph inference produced higher mean semantics and fluency (and lower std) than T5-only baselines finetuned only on WebNLG or finetuned only on the aligned corpus. Using only the noisy aligned corpus (no WebNLG fine-tuning) produced substantially worse generation.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Seamless integration of KG facts into text pre-training corpora, improved factual accuracy in downstream retrieval-augmented LM, reduced toxicity reported in paper's claims, scalable (verbalizes large KG), improved open-domain QA and knowledge-probing performance when used to augment retrieval corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Generator can hallucinate facts when expected triples are missing (e.g., hallucinated occupations) unless mitigated by sequential WebNLG fine-tuning and semantic filtering; generated sentences only capture direct relations around a subject (no multi-hop facts like 'grandchild' inference); KELM corpus is much smaller in raw token count than Wikipedia, so pre-training representations may still favor Wikipedia sentence style.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Examples of hallucination (e.g., producing a profession not present in input triples); poor performance when training only on the noisy aligned corpus; limitations in coverage (no multi-hop fact verbalization). Errors observed in downstream LAMA evaluation were categorized as (1) ambiguous queries where multiple plausible answers exist, (2) incomplete gold answer sets (predictions that are correct but not listed as gold), and (3) answer-granularity mismatches (predictions more specific than gold).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8988.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8988.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-Text Alignment (distant supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Distant-supervision KG↔Text Alignment (subject-root-section, alias-match)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alignment procedure that pairs Wikidata triples (with a common subject) to sentences in the subject's Wikipedia root section by matching object aliases, producing noisy but large training data for text generation from triples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>aligned triple sets (subject-centered sentence alignments)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each subject entity, consider sentences in the root section of its Wikipedia page; a triple (subject, relation, object) is aligned to a sentence if any alias of the object appears in that sentence; triples aligned to the same sentence (and thus same subject) are grouped to form a single training example; if a sentence uses a pronoun for the subject, replace the pronoun with the subject canonical name.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Wikidata knowledge graph triples aligned to Wikipedia sentences</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Alias-based matching of object mentions in the Wikipedia root section for the subject; group all triples with same subject that match a sentence; do not attempt direct relation-to-text matching (too many surface forms).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Creates the primary noisy training corpus for TEKGEN (distant-supervised training data for triple-to-text generation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Alignment produced ~16,090,457 aligned triples and ~7,978,814 aligned sentences, covering ~663 relations (out of ~1,522 total relations in the KG) — roughly 35% of triples aligned and ~42% of relations covered.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared qualitatively to T-REx and hyperlink/coreference-based alignment methods; authors claim their constraint (root section + alias matching) reduces entity-linking and incorrect entailment errors relative to some prior alignment pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Scales to millions of triples and captures relations that hyperlink-based methods miss (quantities, dates, relations with objects that do not have Wikipedia pages); simple heuristic (alias matching + subject-root constraint) yields large coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Noisy alignments (some sentences may not express all aligned triples); does not explicitly match relations to text surface forms; limited to text present in root section so may miss relations expressed elsewhere.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Noise in alignment can produce incorrect triple→sentence training pairs; when used alone (without WebNLG fine-tuning), a T5 model trained on this aligned corpus produced poor generation quality (annotator semantics scores ~2.34 for single-triple inference vs 2.73 for subgraph inference in one annotator's test), motivating further finetuning/filtering.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8988.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8988.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Entity Subgraph Creator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Entity Subgraph Creation via Relation Co-occurrence</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Algorithm that builds small subject-centered subgraphs (up to 5 triples) by chaining relations with high co-occurrence in the aligned training data, producing inputs likely to form coherent multi-triple sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>entity subgraphs (relation co-occurrence-based grouping)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Start from a random triple for a subject, then iteratively add up to 4 more triples whose relations have the highest co-occurrence counts with the previously added relation (co-occurrence computed from relation-to-same-sentence counts in the aligned corpus); each entity subgraph contains triples all sharing the same subject.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>subject-centered subgraphs of a large knowledge graph (Wikidata)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Compute relation-pair co-occurrence counts from training alignments; for each subject, repeatedly sample an initial triple and greedily add triples with relations ranked by co-occurrence to form a set (depth up to 5); these sets are linearized and fed to TEKGEN.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Generates more coherent multi-triple sentences for the KELM corpus (used as input units for TEKGEN) and reduces hallucination compared to single-triple inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Algorithm produced ~18M entity subgraphs from ~45M triples (one generated sentence per subgraph in final corpus). Human evaluation (200 sampled subgraphs) showed TEKGEN+Subgraph_Inference produced higher mean semantics and fluency and lower standard deviations compared to baselines (WebNLG-only finetuned models using single triples or subgraphs); exact numeric scores not fully enumerated in text excerpt.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Using entity subgraphs at inference improved TEKGEN outputs relative to single-triple inference for the same models; for a baseline model finetuned only on WebNLG, using subgraphs reduced fluency standard deviation but did not improve mean scores — TEKGEN combined with subgraph inference yielded the best qualitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Produces inputs that better match the multi-triple training distribution, reducing hallucinations and producing more coherent, natural paraphrases of several related triples at once.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Relies on co-occurrence statistics learned from noisy alignments (may propagate noise); subgraphs still limited to direct relations of a single subject (no multi-hop composition beyond direct relations).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>If co-occurrence statistics are biased or sparse for rare relations, chosen subgraphs may be less coherent; baseline systems trained only on aligned corpus still performed poorly even when using subgraphs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8988.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8988.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple Linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concatenated Triple Serialization (subject relation_1 object_1, ...)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple linearization scheme that serializes one or more triples into a single token sequence which serves as the input prompt for a seq2seq text generator (T5).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearized triple list / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Triples for an example are concatenated into text using surface tokens as separators (e.g., 'subject relation_1 object_1, relation_2 object_2, ...'); this serialized form is treated as the input 'source' string in a text-to-text generation setting.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>knowledge graph triples (Wikidata), either single triple or small sets/subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Direct string concatenation of triples in a fixed order (as produced by entity-subgraph creation or sampled from KG); optionally apply pronoun replacement for training sentences drawn from Wikipedia.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Used as input representation for triple-to-text generation (TEKGEN), and as the representation of KG facts when grouping raw triples into retrieval documents (Triple Documents) for retrieval-augmented LM pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used as the representation of raw triple documents for REALM (REPLACED Triple Documents), downstream EM: NQ 21.14, WQ 42.54. When serialized triples were verbalized into KELM sentences and used to augment REALM, AUGMENTED performance increased to NQ 41.47, WQ 43.90.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Serialized raw triples as retrieval documents performed worse on NQ than natural-language verbalizations; the natural-language form (KELM) is preferred when augmenting large natural corpora for pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple, unambiguous, preserves exact KG facts, easy to group and index; works without needing complex graph-structured models.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Not natural-language; retrieval/L M systems pretrained on sentence-style corpora may not exploit serialized triples well; when used as generation input without appropriate finetuning, leads to Wikipedia-style hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Using serialized triples as the retrieval corpus alone led to poor NQ performance and generalization issues for tasks where sentence structure matters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8988.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8988.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Semantic Quality Filter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-based Semantic Quality Scoring and Filtering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A post-generation filter that scores how well a generated sentence semantically matches the input triples using a fine-tuned BERT classifier/regressor, and filters out low-quality (hallucinated/mismatched) sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>paired-input BERT scoring ([CLS] concatenated-triples [SEP] sentence)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Construct input pair by concatenating the serialized triples and the generated (or reference) sentence separated by SEP; feed to BERT-base-uncased fine-tuned on WebNLG human semantic ratings to predict a semantic quality score scaled to [0,1]; use this score to filter outputs (discard bottom 1% during corpus generation).</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>applies to subject-centered triple sets (Wikidata) and their generated natural-language sentences</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Fine-tune BERT on WebNLG human-assessment data (semantics labels 1–3, scaled to 0–1; augmented with gold references assigned score 1), train for 1000 steps; at inference compute score for each generated sentence w.r.t. input triples and remove low-scoring sentences.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Quality control for TEKGEN outputs before forming KELM corpus; reduces hallucinated or semantically-mismatched sentences included in retrieval corpus used for LM pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Trained on 2706 WebNLG human-assessment examples (90% train / 10% eval); authors report 'high correlations' between predicted scores and human scores on evaluation split (specific correlation numeric values appear in Table 3 but are not enumerated in the provided excerpt). Filtering removed the bottom 1% of generated sentences during corpus construction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Used as a lightweight post-filtering alternative to end-to-end constrained generation; authors argue it improved corpus semantic quality vs unfiltered generation, enabling better downstream performance when including TEKGEN outputs in pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Removes many hallucinations and semantically mismatched outputs; fast to apply as a post-processing step; trained directly on human semantic judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Not jointly optimized with generator (separate module); relatively small finetuning dataset (2706 examples); efficacy depends on quality/diversity of WebNLG judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>May not detect subtle mismatches beyond the WebNLG-trained distribution; limited capacity to correct generator errors, only filters.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8988.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8988.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Triple Documents</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Raw Triple Grouping (Triple Documents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-corpus representation formed by grouping raw Wikidata triples by subject into documents and using serialized triples (not natural-language) as document text for retrieval-augmented LM pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>raw triple grouping (subject-centered serialized triple documents)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each subject, group all triples (serialized as text) into a document (Triple Document). These documents serve as the retrieval corpus for REALM as an alternative to natural-language corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>Wikidata KG (raw triples grouped by subject)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Take KG triples (subject, relation, object), serialize them into textual form and concatenate into a document per subject; index this set of Triple Documents as the retrieval corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Replacement retrieval corpus for REALM pre-training and evaluation on NQ, WQ, LAMA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>When used to REPLACE Wikipedia as the retrieval corpus in REALM (with CC-News pretraining), resulting EM: NQ 21.14, WQ 42.54 (compared with ORIGINAL Wikipedia rerun NQ 38.84, WQ 40.80). REPLACED Triple Documents on LAMA produce similar behavior to KELM Documents on some subcorpora but overall worse on NQ.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Triple Documents (raw triples) as retrieval corpus performed worse on NQ than natural-language KELM Documents; augmenting Wikipedia with Triple Documents produced smaller improvements than augmenting with KELM Documents (Wikipedia + Triple Documents NQ 40.28, WQ 42.91 vs Wikipedia + KELM Documents NQ 41.47, WQ 43.90).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Preserves exact KG facts without generation artifacts; simple to construct and index.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Not sentence-like; LM pre-training and retrieval mechanisms tuned to sentence-style corpora may underutilize such documents; poorer generalization on natural-query QA (notably NQ).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Low performance when used alone as retrieval corpus for REALM on some QA benchmarks (esp. NaturalQuestions).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8988.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8988.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KELM Documents / KELM Corpus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KELM Corpus and KELM Documents (Knowledge-Enhanced Language Model corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The synthetic corpus of natural-language sentences generated by TEKGEN from Wikidata entity-subgraphs (KELM), grouped by subject into documents (KELM Documents) for use as a retrieval corpus in LM pre-training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>verbalized KG as natural-language sentences (KELM)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Entity-subgraphs from Wikidata are converted into fluent natural-language sentences by TEKGEN; resulting sentences (~18M) are grouped by subject into ~5.7M KELM Documents which can be used as retrieval corpus documents for retrieval-augmented LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>verbalized Wikidata KG (subject-centered)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Entity subgraphs (up to 5 triples) are serialized and fed to TEKGEN (T5-large) to generate sentences (top-5 sampling, temperature 0.5), then semantic-quality-filtered (BERT), and grouped by subject into documents for indexing.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Augmenting retrieval corpus for REALM pre-training; evaluated on NaturalQuestions, WebQuestions, and LAMA knowledge probe.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>KELM Corpus: ~18M generated sentences, ~45M triples covered; KELM Documents: ~5.7M documents, ~286M words. AUGMENTED REALM (Wikipedia + KELM Documents) achieved NQ EM 41.47 and WQ EM 43.90 (absolute improvements over ORIGINAL REALM rerun). AUGMENTED LAMA absolute gains: Google-RE +12.94%, T-REx +0.95%, SQuAD +3.61%, ConceptNet +0.47%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Augmenting with KELM Documents outperformed augmenting with raw Triple Documents; KELM able to leverage sentence-style pre-training better than raw triple format.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Natural-language format integrates seamlessly with textual pre-training data, yielding better retrieval-augmented LM performance than raw triple documents; improves knowledge-intensive QA and knowledge-probing tasks with relatively modest added tokens (~14% token increase vs Wikipedia).</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Limited to expressing direct relations of a subject (no multi-hop synthesis); smaller overall token count than encyclopedia corpora (may bias learned representations toward Wikipedia-style text); generation pipeline can introduce noise/hallucinations if not filtered.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Downstream evaluation error categories include ambiguous queries, incomplete gold answer sets, and answer-granularity mismatches; some factual errors remain if generator hallucinates or alignment is noisy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The WebNLG challenge: Generating text from RDF data <em>(Rating: 2)</em></li>
                <li>T-REx: A large scale alignment of natural language with knowledge base triples <em>(Rating: 2)</em></li>
                <li>Realm: Retrieval-augmented language model pre-training <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 1)</em></li>
                <li>Kgpt: Knowledge-grounded pretraining for data-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8988",
    "paper_id": "paper-263864711",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "TEKGEN",
            "name_full": "Text from Knowledge Graph Generator (TEKGEN)",
            "brief_description": "A pipeline that converts groups of KG triples into natural language sentences using a sequence-to-sequence model (T5-large) fine-tuned on a distantly-supervised KG↔text aligned corpus and then refined on WebNLG to reduce hallucination, followed by a BERT-based semantic filter.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "sequence-to-sequence triple linearization (concatenated triples)",
            "representation_description": "Inputs are linearized groups of triples concatenated as: subject relation_1 object_1, ... relation_n object_n; the concatenated string is fed to a text-to-text Transformer (T5-large) to generate a natural-language sentence paraphrasing the triples.",
            "graph_type": " encyclopedic knowledge graph (Wikidata KG; subject-centered subgraphs)",
            "conversion_method": "Create training pairs by distant supervision aligning Wikidata triples to Wikipedia root-section sentences; fine-tune T5-large on this aligned corpus (5000 steps) to increase coverage, then sequentially fine-tune on WebNLG (500 steps) to reduce hallucination; at inference concatenate entity-subgraph triples and decode (top-5 sampling, temperature=0.5); post-filter generated sentences with a BERT-based semantic quality scorer and remove lowest-scoring outputs (bottom 1%).",
            "downstream_task": "Used to create a synthetic natural-language corpus (KELM) which augments the retrieval corpus for retrieval-augmented LM pre-training (REALM); downstream evaluation on open-domain QA (NaturalQuestions, WebQuestions) and knowledge probing (LAMA).",
            "performance_metrics": "When KELM documents (TEKGEN outputs grouped by subject) were AUGMENTED to REALM's retrieval corpus (Wikipedia + KELM Documents) the pre-trained+fine-tuned REALM achieved NQ Exact Match 41.47 and WQ Exact Match 43.90 (absolute gains over ORIGINAL REALM rerun: +2.63% NQ, +3.10% WQ). AUGMENTED LAMA absolute gains vs ORIGINAL: Google-RE +12.94%, T-REx +0.95%, SQuAD +3.61%, ConceptNet +0.47%. Additional reported corpus stats: KELM corpus ~18M generated sentences spanning ~45M triples; KELM documents grouped by subject: ~5.7M documents; KELM total words ~286M (≈14% of Wikipedia 2B words).",
            "comparison_to_others": "Outperforms or complements approaches that insert raw triples as retrieval documents: AUGMENTED with KELM documents outperforms AUGMENTED with raw Triple Documents; TEKGEN+entity-subgraph inference produced higher mean semantics and fluency (and lower std) than T5-only baselines finetuned only on WebNLG or finetuned only on the aligned corpus. Using only the noisy aligned corpus (no WebNLG fine-tuning) produced substantially worse generation.",
            "advantages": "Seamless integration of KG facts into text pre-training corpora, improved factual accuracy in downstream retrieval-augmented LM, reduced toxicity reported in paper's claims, scalable (verbalizes large KG), improved open-domain QA and knowledge-probing performance when used to augment retrieval corpora.",
            "disadvantages": "Generator can hallucinate facts when expected triples are missing (e.g., hallucinated occupations) unless mitigated by sequential WebNLG fine-tuning and semantic filtering; generated sentences only capture direct relations around a subject (no multi-hop facts like 'grandchild' inference); KELM corpus is much smaller in raw token count than Wikipedia, so pre-training representations may still favor Wikipedia sentence style.",
            "failure_cases": "Examples of hallucination (e.g., producing a profession not present in input triples); poor performance when training only on the noisy aligned corpus; limitations in coverage (no multi-hop fact verbalization). Errors observed in downstream LAMA evaluation were categorized as (1) ambiguous queries where multiple plausible answers exist, (2) incomplete gold answer sets (predictions that are correct but not listed as gold), and (3) answer-granularity mismatches (predictions more specific than gold).",
            "uuid": "e8988.0"
        },
        {
            "name_short": "KG-Text Alignment (distant supervision)",
            "name_full": "Distant-supervision KG↔Text Alignment (subject-root-section, alias-match)",
            "brief_description": "An alignment procedure that pairs Wikidata triples (with a common subject) to sentences in the subject's Wikipedia root section by matching object aliases, producing noisy but large training data for text generation from triples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "aligned triple sets (subject-centered sentence alignments)",
            "representation_description": "For each subject entity, consider sentences in the root section of its Wikipedia page; a triple (subject, relation, object) is aligned to a sentence if any alias of the object appears in that sentence; triples aligned to the same sentence (and thus same subject) are grouped to form a single training example; if a sentence uses a pronoun for the subject, replace the pronoun with the subject canonical name.",
            "graph_type": "Wikidata knowledge graph triples aligned to Wikipedia sentences",
            "conversion_method": "Alias-based matching of object mentions in the Wikipedia root section for the subject; group all triples with same subject that match a sentence; do not attempt direct relation-to-text matching (too many surface forms).",
            "downstream_task": "Creates the primary noisy training corpus for TEKGEN (distant-supervised training data for triple-to-text generation).",
            "performance_metrics": "Alignment produced ~16,090,457 aligned triples and ~7,978,814 aligned sentences, covering ~663 relations (out of ~1,522 total relations in the KG) — roughly 35% of triples aligned and ~42% of relations covered.",
            "comparison_to_others": "Compared qualitatively to T-REx and hyperlink/coreference-based alignment methods; authors claim their constraint (root section + alias matching) reduces entity-linking and incorrect entailment errors relative to some prior alignment pipelines.",
            "advantages": "Scales to millions of triples and captures relations that hyperlink-based methods miss (quantities, dates, relations with objects that do not have Wikipedia pages); simple heuristic (alias matching + subject-root constraint) yields large coverage.",
            "disadvantages": "Noisy alignments (some sentences may not express all aligned triples); does not explicitly match relations to text surface forms; limited to text present in root section so may miss relations expressed elsewhere.",
            "failure_cases": "Noise in alignment can produce incorrect triple→sentence training pairs; when used alone (without WebNLG fine-tuning), a T5 model trained on this aligned corpus produced poor generation quality (annotator semantics scores ~2.34 for single-triple inference vs 2.73 for subgraph inference in one annotator's test), motivating further finetuning/filtering.",
            "uuid": "e8988.1"
        },
        {
            "name_short": "Entity Subgraph Creator",
            "name_full": "Entity Subgraph Creation via Relation Co-occurrence",
            "brief_description": "Algorithm that builds small subject-centered subgraphs (up to 5 triples) by chaining relations with high co-occurrence in the aligned training data, producing inputs likely to form coherent multi-triple sentences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "entity subgraphs (relation co-occurrence-based grouping)",
            "representation_description": "Start from a random triple for a subject, then iteratively add up to 4 more triples whose relations have the highest co-occurrence counts with the previously added relation (co-occurrence computed from relation-to-same-sentence counts in the aligned corpus); each entity subgraph contains triples all sharing the same subject.",
            "graph_type": "subject-centered subgraphs of a large knowledge graph (Wikidata)",
            "conversion_method": "Compute relation-pair co-occurrence counts from training alignments; for each subject, repeatedly sample an initial triple and greedily add triples with relations ranked by co-occurrence to form a set (depth up to 5); these sets are linearized and fed to TEKGEN.",
            "downstream_task": "Generates more coherent multi-triple sentences for the KELM corpus (used as input units for TEKGEN) and reduces hallucination compared to single-triple inputs.",
            "performance_metrics": "Algorithm produced ~18M entity subgraphs from ~45M triples (one generated sentence per subgraph in final corpus). Human evaluation (200 sampled subgraphs) showed TEKGEN+Subgraph_Inference produced higher mean semantics and fluency and lower standard deviations compared to baselines (WebNLG-only finetuned models using single triples or subgraphs); exact numeric scores not fully enumerated in text excerpt.",
            "comparison_to_others": "Using entity subgraphs at inference improved TEKGEN outputs relative to single-triple inference for the same models; for a baseline model finetuned only on WebNLG, using subgraphs reduced fluency standard deviation but did not improve mean scores — TEKGEN combined with subgraph inference yielded the best qualitative results.",
            "advantages": "Produces inputs that better match the multi-triple training distribution, reducing hallucinations and producing more coherent, natural paraphrases of several related triples at once.",
            "disadvantages": "Relies on co-occurrence statistics learned from noisy alignments (may propagate noise); subgraphs still limited to direct relations of a single subject (no multi-hop composition beyond direct relations).",
            "failure_cases": "If co-occurrence statistics are biased or sparse for rare relations, chosen subgraphs may be less coherent; baseline systems trained only on aligned corpus still performed poorly even when using subgraphs.",
            "uuid": "e8988.2"
        },
        {
            "name_short": "Triple Linearization",
            "name_full": "Concatenated Triple Serialization (subject relation_1 object_1, ...)",
            "brief_description": "A simple linearization scheme that serializes one or more triples into a single token sequence which serves as the input prompt for a seq2seq text generator (T5).",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "linearized triple list / serialization",
            "representation_description": "Triples for an example are concatenated into text using surface tokens as separators (e.g., 'subject relation_1 object_1, relation_2 object_2, ...'); this serialized form is treated as the input 'source' string in a text-to-text generation setting.",
            "graph_type": "knowledge graph triples (Wikidata), either single triple or small sets/subgraphs",
            "conversion_method": "Direct string concatenation of triples in a fixed order (as produced by entity-subgraph creation or sampled from KG); optionally apply pronoun replacement for training sentences drawn from Wikipedia.",
            "downstream_task": "Used as input representation for triple-to-text generation (TEKGEN), and as the representation of KG facts when grouping raw triples into retrieval documents (Triple Documents) for retrieval-augmented LM pre-training.",
            "performance_metrics": "When used as the representation of raw triple documents for REALM (REPLACED Triple Documents), downstream EM: NQ 21.14, WQ 42.54. When serialized triples were verbalized into KELM sentences and used to augment REALM, AUGMENTED performance increased to NQ 41.47, WQ 43.90.",
            "comparison_to_others": "Serialized raw triples as retrieval documents performed worse on NQ than natural-language verbalizations; the natural-language form (KELM) is preferred when augmenting large natural corpora for pre-training.",
            "advantages": "Simple, unambiguous, preserves exact KG facts, easy to group and index; works without needing complex graph-structured models.",
            "disadvantages": "Not natural-language; retrieval/L M systems pretrained on sentence-style corpora may not exploit serialized triples well; when used as generation input without appropriate finetuning, leads to Wikipedia-style hallucination.",
            "failure_cases": "Using serialized triples as the retrieval corpus alone led to poor NQ performance and generalization issues for tasks where sentence structure matters.",
            "uuid": "e8988.3"
        },
        {
            "name_short": "Semantic Quality Filter",
            "name_full": "BERT-based Semantic Quality Scoring and Filtering",
            "brief_description": "A post-generation filter that scores how well a generated sentence semantically matches the input triples using a fine-tuned BERT classifier/regressor, and filters out low-quality (hallucinated/mismatched) sentences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "paired-input BERT scoring ([CLS] concatenated-triples [SEP] sentence)",
            "representation_description": "Construct input pair by concatenating the serialized triples and the generated (or reference) sentence separated by SEP; feed to BERT-base-uncased fine-tuned on WebNLG human semantic ratings to predict a semantic quality score scaled to [0,1]; use this score to filter outputs (discard bottom 1% during corpus generation).",
            "graph_type": "applies to subject-centered triple sets (Wikidata) and their generated natural-language sentences",
            "conversion_method": "Fine-tune BERT on WebNLG human-assessment data (semantics labels 1–3, scaled to 0–1; augmented with gold references assigned score 1), train for 1000 steps; at inference compute score for each generated sentence w.r.t. input triples and remove low-scoring sentences.",
            "downstream_task": "Quality control for TEKGEN outputs before forming KELM corpus; reduces hallucinated or semantically-mismatched sentences included in retrieval corpus used for LM pre-training.",
            "performance_metrics": "Trained on 2706 WebNLG human-assessment examples (90% train / 10% eval); authors report 'high correlations' between predicted scores and human scores on evaluation split (specific correlation numeric values appear in Table 3 but are not enumerated in the provided excerpt). Filtering removed the bottom 1% of generated sentences during corpus construction.",
            "comparison_to_others": "Used as a lightweight post-filtering alternative to end-to-end constrained generation; authors argue it improved corpus semantic quality vs unfiltered generation, enabling better downstream performance when including TEKGEN outputs in pre-training.",
            "advantages": "Removes many hallucinations and semantically mismatched outputs; fast to apply as a post-processing step; trained directly on human semantic judgments.",
            "disadvantages": "Not jointly optimized with generator (separate module); relatively small finetuning dataset (2706 examples); efficacy depends on quality/diversity of WebNLG judgments.",
            "failure_cases": "May not detect subtle mismatches beyond the WebNLG-trained distribution; limited capacity to correct generator errors, only filters.",
            "uuid": "e8988.4"
        },
        {
            "name_short": "Triple Documents",
            "name_full": "Raw Triple Grouping (Triple Documents)",
            "brief_description": "A retrieval-corpus representation formed by grouping raw Wikidata triples by subject into documents and using serialized triples (not natural-language) as document text for retrieval-augmented LM pre-training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "raw triple grouping (subject-centered serialized triple documents)",
            "representation_description": "For each subject, group all triples (serialized as text) into a document (Triple Document). These documents serve as the retrieval corpus for REALM as an alternative to natural-language corpora.",
            "graph_type": "Wikidata KG (raw triples grouped by subject)",
            "conversion_method": "Take KG triples (subject, relation, object), serialize them into textual form and concatenate into a document per subject; index this set of Triple Documents as the retrieval corpus.",
            "downstream_task": "Replacement retrieval corpus for REALM pre-training and evaluation on NQ, WQ, LAMA.",
            "performance_metrics": "When used to REPLACE Wikipedia as the retrieval corpus in REALM (with CC-News pretraining), resulting EM: NQ 21.14, WQ 42.54 (compared with ORIGINAL Wikipedia rerun NQ 38.84, WQ 40.80). REPLACED Triple Documents on LAMA produce similar behavior to KELM Documents on some subcorpora but overall worse on NQ.",
            "comparison_to_others": "Triple Documents (raw triples) as retrieval corpus performed worse on NQ than natural-language KELM Documents; augmenting Wikipedia with Triple Documents produced smaller improvements than augmenting with KELM Documents (Wikipedia + Triple Documents NQ 40.28, WQ 42.91 vs Wikipedia + KELM Documents NQ 41.47, WQ 43.90).",
            "advantages": "Preserves exact KG facts without generation artifacts; simple to construct and index.",
            "disadvantages": "Not sentence-like; LM pre-training and retrieval mechanisms tuned to sentence-style corpora may underutilize such documents; poorer generalization on natural-query QA (notably NQ).",
            "failure_cases": "Low performance when used alone as retrieval corpus for REALM on some QA benchmarks (esp. NaturalQuestions).",
            "uuid": "e8988.5"
        },
        {
            "name_short": "KELM Documents / KELM Corpus",
            "name_full": "KELM Corpus and KELM Documents (Knowledge-Enhanced Language Model corpus)",
            "brief_description": "The synthetic corpus of natural-language sentences generated by TEKGEN from Wikidata entity-subgraphs (KELM), grouped by subject into documents (KELM Documents) for use as a retrieval corpus in LM pre-training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "verbalized KG as natural-language sentences (KELM)",
            "representation_description": "Entity-subgraphs from Wikidata are converted into fluent natural-language sentences by TEKGEN; resulting sentences (~18M) are grouped by subject into ~5.7M KELM Documents which can be used as retrieval corpus documents for retrieval-augmented LMs.",
            "graph_type": "verbalized Wikidata KG (subject-centered)",
            "conversion_method": "Entity subgraphs (up to 5 triples) are serialized and fed to TEKGEN (T5-large) to generate sentences (top-5 sampling, temperature 0.5), then semantic-quality-filtered (BERT), and grouped by subject into documents for indexing.",
            "downstream_task": "Augmenting retrieval corpus for REALM pre-training; evaluated on NaturalQuestions, WebQuestions, and LAMA knowledge probe.",
            "performance_metrics": "KELM Corpus: ~18M generated sentences, ~45M triples covered; KELM Documents: ~5.7M documents, ~286M words. AUGMENTED REALM (Wikipedia + KELM Documents) achieved NQ EM 41.47 and WQ EM 43.90 (absolute improvements over ORIGINAL REALM rerun). AUGMENTED LAMA absolute gains: Google-RE +12.94%, T-REx +0.95%, SQuAD +3.61%, ConceptNet +0.47%.",
            "comparison_to_others": "Augmenting with KELM Documents outperformed augmenting with raw Triple Documents; KELM able to leverage sentence-style pre-training better than raw triple format.",
            "advantages": "Natural-language format integrates seamlessly with textual pre-training data, yielding better retrieval-augmented LM performance than raw triple documents; improves knowledge-intensive QA and knowledge-probing tasks with relatively modest added tokens (~14% token increase vs Wikipedia).",
            "disadvantages": "Limited to expressing direct relations of a subject (no multi-hop synthesis); smaller overall token count than encyclopedia corpora (may bias learned representations toward Wikipedia-style text); generation pipeline can introduce noise/hallucinations if not filtered.",
            "failure_cases": "Downstream evaluation error categories include ambiguous queries, incomplete gold answer sets, and answer-granularity mismatches; some factual errors remain if generator hallucinates or alignment is noisy.",
            "uuid": "e8988.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data",
            "rating": 2
        },
        {
            "paper_title": "T-REx: A large scale alignment of natural language with knowledge base triples",
            "rating": 2
        },
        {
            "paper_title": "Realm: Retrieval-augmented language model pre-training",
            "rating": 2
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 1
        },
        {
            "paper_title": "Kgpt: Knowledge-grounded pretraining for data-to-text generation",
            "rating": 1
        }
    ],
    "cost": 0.020839249999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training
June 6-11, 2021</p>
<p>Oshin Agarwal oagarwal@seas.upenn.edu 
University of Pennsylvania</p>
<p>Heming Ge hemingge@google.com 
Google Research</p>
<p>Siamak Shakeri siamaks@google.com 
Google Research</p>
<p>Rami Al-Rfou 
Google Research</p>
<p>Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</p>
<p>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20213554
Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domainspecific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, largescale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe.</p>
<p>Introduction</p>
<p>Data-To-Text Generation (Kukich, 1983;Goldberg et al., 1994) involves converting knowledge graph (KG) triples of the form (subject, relation, object) into a natural language sentence(s). There are many standard datasets for this task such as WebNLG (Gardent et al., 2017) and many systems have been developed to improve performance on these datasets. However, to the best of our knowledge, no prior work has attempted to verbalize a full knowledge graph. Verbalizing a full KG has additional challenges over small benchmark datasets, such as entity and relation coverage and the lack of grouped sets of triples that can produce coherent sentences together. In this paper, we convert the English Wikidata KG (Vrandečić and Krötzsch, 2014) into natural language text (Figure 1). The * Work done during internship at Google  Figure 1: An example of generating text from KG. First, the entity subgraphs on the left are created and then converted to the sentence on the right. generated corpus, which we call the KELM Corpus, consists of ∼18M sentences spanning ∼45M triples with ∼1500 distinct relations. For training the verbalization system, we also create an English Wikidata KG-Wikipedia Text aligned corpus consisting of a variety of entities such as dates and numerical quantities.</p>
<p>We evaluate the quality of the generated corpus through human evaluation of a random sample. We further showcase the utility of this corpus in language model pre-training. Text represents a limited coverage of the world knowledge. Therefore, we expect the language models to be restricted to facts that are expressed in natural language. Moreover, facts may not be expressed as explicitly in text as they are in KGs, and the variability in the quality of text can eventually cause biases in the resulting models (Bolukbasi et al., 2016;Sheng et al., 2019;Manzini et al., 2019). Building models that handle structured data and free form text seamlessly has been a long sought-after goal. However, their integration is challenging due to different structural formats. KG verbalization provides a simple way to integrate KGs with natural text. We illustrate this by augmenting the REALM (Guu et al., 2020) </p>
<p>Entity Subgraph Creator</p>
<p>Relation co-occurrence counts ( </p>
<p>TEKGEN</p>
<p>One of the challenges in converting an entire KG to text is the wide variety of entities and relations. Wikidata consists of ∼6M entities and ∼1500 relations. In comparison, the WebNLG dataset has ∼600 entities and ∼20 relations. In this section, we discuss the various components of TEKGEN, also illustrated in Figure 2 -1. Create a large yet noisy training dataset using distant supervision.</p>
<ol>
<li>
<p>Sequentially fine-tune T5, first on the dataset from step 1 for improved coverage, then on a small clean dataset for reduced hallucination.</p>
</li>
<li>
<p>Build a filter for the generated text based on its semantic quality w.r.t. the KG triples.</p>
</li>
</ol>
<p>Training Dataset</p>
<p>We first create training data using distant supervision by aligning Wikidata triples to Wikipedia text (see Figure 3).</p>
<p>KG-Text Alignment</p>
<p>For each entity, we constrain the candidate sentences to the root section of its Wikipedia page  because this section generally describes the relations of the subject entity with other entities. For each sentence in this section, we match all triples that have this entity as the subject. A triple is said to match if any alias of the object entity occurs in the sentence. We do not match relations to text as there are too many ways to express them. Constraining to the subject entity's page and root section generally ensures that the relation is expressed in the sentence if it mentions the object entity. Each triple can align to multiple sentences and each sentence can have multiple triples aligned to it. If any alias of the subject entity occurs in the given sentence, the sentence is selected as is, else the first animate third-person personal or possessive pronoun is replaced by the subject entity's canonical name. The pronoun replacement heuristic also works well because of this constraint. All triples aligned to a given sentences are combined together as a single example.</p>
<p>Alignment statistics are shown in Table 1 and some alignment examples are shown in Table 2. There are a total of ∼45M triples, ∼35% of which were aligned to sentences. This results in ∼8M examples, covering ∼42% of the relations.</p>
<p>Note that each sentence in the aligned corpus is matched to triples with a common subject entity. While this results in some noise, such errors should be small due to the constraint that the text is the root section of the subject entity page. This constraint allows us to maintain the same property of common subject entity as the entity subgraph used in inference ( §3). It also simplified the alignment process, removing the need to match relations to text. In comparison, the T-REx (Elsahar et al., 2018)   errors due to entity linking and incorrect entailment, which are unlikely in our corpus due to this constraint.</p>
<p>Types of Triples</p>
<p>We extract several types of triples, each of which have slightly different matching techniques. Other alignment corpora built using Wikipedia hyperlinks  would miss many of these triples with entities without Wikipedia pages such as quantities, dates and certain occupations, and hence relations such as date of birth, publication year and distance from Earth. The 2012 reelection campaign of Barack Obama, the 44th President of the United States, was formally announced on April 4, 2011. Blue whale (parent taxon, Balaenoptera)</p>
<p>The blue whale (Balaenoptera musculus) is a marine mammal belonging to the baleen whale suborder Mysticeti. While the type of the triples is important in the alignment process, the verbalization model is agnostic to the type and treats all triples the same.</p>
<p>Model</p>
<p>We perform a two-step sequential finetuning of the pre-trained T5-large  model for converting triples to text. Triples are concatenated as subject relation_1 object_1, ....relation_n object_n for input to T5. The model is first fine-tuned on the aligned corpus for 5000 steps to increase the coverage of entities and relations. However, this results in the generation of Wikipedia-like sentences and hallucination if a certain expected input triple is missing. For example, Wikipedia sentences generally mention date of birth, date of death, occupation together. If the occupation is missing in the input, the system hallucinates a random occupation. "Neff Maiava date of birth 01 May 1924, date of death, 21 April 2018." generates "Neff Maiava (1 May 1924 -21 April 2018) was an Albanian actor."; hallucinating a profession. To overcome this, we further fine-tune the model on WebNLG 2017 data for 500 steps. While WebNLG has low coverage, the information in the input triples matches the target sentence exactly. WebNLG also has a different sentence structure than Wikipedia. This reduces conformity to Wikipedia sentence structure and hence reduces hallucination. We use a learning rate of 0.001, a batch size of 1048576 tokens and a maximum decoding length of 256.  </p>
<p>Quality Filtering</p>
<p>We perform a semantic quality based filtering of the sentences generated by the triple-to-text module. This is a separate post-processing module used during inference and is not jointly optimized with the text generation module. A semantic quality score is assigned to each generated sentence w.r.t. the input triples that indicates whether or not the generated text captures the full meaning of the triples and does not hallucinate extra information. The score is generated using a BERT base uncased model with input of the form [CLS] concatenated-triples [SEP] reference-or-generated-sentence. It is fine-tuned for 1000 steps on the WebNLG 2017 human assessment data. The data contains system predictions submitted to the shared task rated on a scale of 1-3 for semantics and fluency. We use the semantics score and scale it to 0-1. We also add gold references with a score of 1. This results in 2706 examples, 90% of which are used for finetuning and the remaining for evaluation. High correlations are obtained between the predicted scores and human scores on the evaluation split (Table 3).</p>
<p>KELM Corpus</p>
<p>In this section, we utilize the TEKGEN model and filtering mechanism to build a synthetic corpus that captures the KG in natural language format.</p>
<p>Entity Subgraph</p>
<p>Datasets such as WebNLG have instances with grouped triples that can be expressed as a fluent sentence. Such groups are not available for a large KG and using one triple at a time for inference would lead to hallucination as training uses multi-  Figure 4: Entity Subgraph Creation Algorithm using relation co-occurrence counts based on relation-sentence alignment in the training data. Each entity subgraph consists of a maximum of five triples, all with the same subject entity. The first triple is chosen at random. The second triple is chosen such that its relation has the highest co-occurrence count with the relation in the first triple and so on. ple triples per example. Therefore, we develop a strategy to create entity subgraphs based on relation co-occurrence counts i.e. frequency of alignment of two relations to the same sentence in the training data. The algorithm is shown in Figure 4. It produces ∼18M entity subgraphs from ∼45M triples so the final corpus will have 18M generated sentences corresponding to each entity subgraph.
all_triple_sets ← {} rel_pairs ← {} depth ← 5 for all ri ∈ KG do P ← {(rj, cij)∀(ri, rj, cij) ∈ train_align_counts} rel_pairs(ri) ← maxheap(P ) end for for all entities s ∈ KG do R ← {(r, o)∀(s, r, o) ∈ KG} while R = ∅ do triple_set ← {} (r1, o1) ← random(R) triple_set.add(s, r1, o1) R.remove(s, r1, o1) KG.remove(s, r1, o1) for i = 2 to depth do ri ← NONE M ← rel_pairs(ri−1) while M = ∅ do (rj, cij) ← M.next if rj ∈ R then ri ← rj (ri, oi) ← R.</p>
<p>Generation</p>
<p>For each entity subgraph, we concatenate all its triples as before. We perform top 5 sampling with a temperature of 0.5. The bottom 1% of the generated sentences are filtered out based on the semantic score assigned using the model in §2.3.  Table 4: Human evaluation of the generated corpora, on a scale of 1-5, for semantics and fluency.</p>
<p>Human Evaluation</p>
<p>Generation quality of the KELM Corpus is evaluated using human ratings on a random sample of 200 entity subgraphs. Automatic metrics such as BLEU (Papineni et al., 2002) or BERTscore (Zhang et al., 2019) cannot be used due to the lack of gold references. Following prior work, the generated text is rated for two aspects-fluency and semantics, on a scale of 1-5, where 1 means not at all fluent/does not capture meaning at all and 5 means completely fluent/fully captures meaning with no hallucination. We have eight annotators total and each example is rated by two of them. All annotators are linguists, NLP researchers or NLP practitioners and volunteered for the evaluation. We do not use any crowd sourcing platform. For each instance, scores of the two annotators are averaged to get the final rating. The Pearson correlation between the two sets of ratings is 0.56 for semantics and 0.43 for fluency. We compare TEKGEN to two baseline systems. For both baselines, we fine-tune a T5-large model only on WebNLG 2017 data but use different inference input. For one system, we use one triple at a time as input, resulting in 524 instances from the 200 entity subgraphs. For the second, we use the entity subgraphs as input, resulting in 200 instances. Scores are shown in Table 4. Entity subgraphs during inference do not improve the mean scores but reduce the standard deviation of the fluency. In comparison, TEKGEN with inference using entity subgraphs improve both the semantics and fluency of the generated text. Both the mean scores are higher and the standard deviations are lower. It paraphrases canonical names of relations in the KG to more natural expressions more often. Some examples of generation using the two systems are shown in Table 5. In the second example, the relation 'inception' is paraphrased to 'started' using WebNLG_finetuning+Triple_Inference and 'founded' using TEKGEN+Subgraph_Inference, the latter being more appropriate for organizations.</p>
<p>For completeness, we evaluate two more base-  line systems in which T5-large model is finetuned only on the KG-Text aligned corpus but use the two different inference inputs-single triple and entity subgraphs. One annotator rated the same sample for semantics. The former had an average score of 2.34 and the latter 2.73. Since these scores were very low, we did not pursue the evaluation of these systems further. The use of just the aligned corpus which is noisy to some extent results in the worst performing system out of all the methods.</p>
<p>Knowledge Enhanced LMs</p>
<p>In this section, we showcase an application of the generated KELM Corpus as a way to integrate KGs into natural text corpora for pre-training language models (LMs), as shown in Figure 5. We choose REALM (Guu et al., 2020) as a representative of the recently introduced family of retrieval language models and therefore we expect our work to be equally applicable to other such language models. We show gains on LAMA knowledge probe and open domain QA with augmentation. We also perform experiments where we integrate raw Wikidata triples instead of KELM corpus to confirm the effectiveness of verbalization.</p>
<p>Retrieval Language Models</p>
<p>REALM is a retrieval-based language model and uses two corpora for pre-training-a retrieval corpus and a pre-training corpus. During pre-training, a sentence is selected at random from the pre-training corpus and a random word or salient span (dates and entities) is masked in this sentence. Then using a joint representation of the masked sentence and each of the documents in the retrieval corpus, the masked word is predicted. In the finetuning stage, the model is provided with a query/question as input in place of masked sentence from the pretraining corpora. It retrieves a small set of documents from the retrieval corpus based on the vector similarity of the query and document representation and then selects a span of text from the retrieved documents as the answer.</p>
<p>KELM Documents</p>
<p>We group sentences in the KELM corpus by subject entities to create 5722974 (5.7M) documents. We call these KELM documents. We then replace/augment the retrieval corpus in REALM with these synthetic documents. KELM Corpus has only ∼286M words (∼14%) in comparison to ∼2B words in English Wikipedia. </p>
<p>KG as natural language corpus</p>
<p>Spork EP is an EP released by indie rock band Flake Music.</p>
<p>Natural Language Corpus</p>
<p>Language Model</p>
<p>Knowledge Graph Figure 5: Knowledge Graph verbalization for integration with natural text corpora for language model pre-training.</p>
<p>Evaluation Datasets</p>
<p>We perform evaluation using two open domain question answering datasets and one knowledge probing dataset.</p>
<p>Question Answering</p>
<p>NaturalQuestions (NQ) (Kwiatkowski et al., 2019): Queries to Google and their answers.</p>
<p>WebQuestions (WQ) (Berant et al., 2010): question-answers from the Google Suggest API. We keep the same settings as REALM for both NQ and WQ i.e. we work on the open domain setting for both datasets where no passage is provided as context for each question. Finetuning is performed on respective training splits.</p>
<p>Knowledge Probe</p>
<p>LAMA (Petroni et al., 2019): Fill-in-the-Blank style factual queries with single token answers from four sources: Google-RE, 3 T-REx (Elsahar et al., 2018), SQuAD (Rajpurkar et al., 2016) and Con-ceptNet (Speer and Havasi, 2012). Google-RE also consists of aliases of each answer.</p>
<p>REALM did not include LAMA as one of its evaluation datasets. So we first evaluate REALM on LAMA using the original retrieval corpus and then using the KELM Corpus. No finetuning is performed and the masked word predictions from the pre-trained models are used as answers.</p>
<p>Results</p>
<p>We evaluate REALM on WQ, NQ and LAMA under three settings by modifying the retrieval corpus. The REPLACED and AUGMENTED models are evaluated using both the raw Wikidata triples and the generated sentences. Wikidata triples are grouped by subject entity to form Triple Documents and KELM Corpus sentences are also grouped by subject entity to form KELM Corpus Documents ( §4.2). The model is pre-trained for 200k steps with the CC-News pre-training corpus in all cases with default hyperparameters.</p>
<p>ORIGINAL For NQ and WQ, we fine-tuned the pre-trained REALM on the respective training splits. While we were able to reproduce the accuracy on WQ, the accuracy on NQ was ∼1.5% absolute less than the reported accuracy (row 1&amp;2 in Table 7). For LAMA probe, we first evaluated the pre-trained REALM, reporting the results on the different sub-corpora in Table 6 (row Wikipedia under REALM). Even the ORIGINAL REALM model shows substantial improvement over prior models. The ability of REALM to access the corpus documents during inference not only make it interpretable but also better on the knowledge intensive tasks. It obtains an accuracy of 67.36% on Google-RE, 68.18% on T-REx and 27.96% on    Table 7). This can be attributed to the nature of the datasets-WQ is a KG-based dataset whereas NQ consists of real queries issued to Google. On LAMA (rows 2&amp;3 under REALM in Table 6), the performance is lower than the ORIGINAL model but much higher than BERT. Both Triple Documents and KELM Corpus Documents have similar performance. When using just the KG, the format doesn't matter. However, a system trained on raw triples may not generalize for tasks where sentence structure is important.</p>
<p>AUGMENTED We observe improvements on all the datasets (last two rows of Tables 6&amp;7) with the AUGMENTED model which uses both the Wikipedia text and the KELM Documents. There is an absolute gain of 2.63% and 3.10% on NQ and WQ respectively over the ORIGINAL model. Similarly, there is an absolute gain of 12.94%, 0.95%, 3.61% and 0.47% on Google-RE, T-REx, SQuAD and ConceptNet in LAMA respectively. Unlike the REPLACED model, the improvement is higher when the generated sentences in KELM Corpus are added instead of the raw Wikidata triples, confirming the effectiveness of verbalization of KG into natural language sentences. Wikipedia is the dominant corpus with 2B words whereas KELM corpus sentences are succinct with a total of 286M words ( §4.2) so it is likely the learned representations favour the Wikipedia format which is natural language sentences. We expect augmenting other retrieval-based models such as DPR (Karpukhin et al., 2020) and RAG (Lewis et al., 2020) with the KELM corpus should also improve their performance, given that their enhancements are orthogonal to our contribution. Moreover, we note that our augmented corpus represents a scalable strategy for future QA systems; by adding only 14% more tokens to the original REALM model we outperform huge and computationally expensive models such as   We inspected the errors of the AUGMENTED model with KELM Documents on LAMA. Apart from real errors where the prediction is actually incorrect, there were some false errors that can be broadly classified into three categories-1. Ambiguous Query: e.g. In "X was born in ____", the answer could be the year or the place of birth but only one of them is acceptable depending on the subcorpus.</p>
<ol>
<li>
<p>Incomplete Answer Set: e.g. In "Konstantin Mereschkowski had a career as ____", the gold target is biologist and the prediction is botanist but both should be correct.</p>
</li>
<li>
<p>Answer granularity: The prediction is correct but more specific. e.g. In "On the CPI scale, Kenya ranks ____", the gold answer is low but the prediction is 101, which is in fact correct.</p>
</li>
</ol>
<p>Related Work</p>
<p>Data-to-Text Generation Data-to-Text Generation has several benchmark datasets with slightly different objectives-WebNLG (Gardent et al., 2017) to convert a group of triples to text, E2ENLG (Dušek et al., 2018) Shimorina and Gardent, 2018) have been developed and evaluated on these datasets, such as graph transformers over structured data (Koncel-Kedziorski et al., 2019), latent templates for interpretability (Wiseman et al., 2018) and text-to-text generation with T5 (Kale, 2020).  (Etzioni et al., 2008;Angeli et al., 2015;Clancy et al., 2019) inherently create such a corpus but these works generally do not release the extracted KG triples.</p>
<p>KG-Text alignment</p>
<p>Incorporating KGs Most prior works on incorporating KG with text often learn KG entity representations and add them to the mention spans linked to the entity Févry et al., 2020) or create subgraphs relevant to the query that are expanded with text in the embedding space Sun et al., 2019;Xiong et al., 2019). Some others incorporate additional modules. Verga et al. (2020) extend Févry et al. (2020) by adding a triple memory with (subject, relation) encoding as the key and the object encoding as the value. Das et al. (2017) use universal schema (Riedel et al., 2013) that embeds text and KGs in a shared space for their integration. K M et al. (2018) learn a single representation for all the triples mentioned in a sentences during pre-training and update it further in task-specific finetuning. In contrast, we convert the KG into text and use it to augment the pre-training data.</p>
<p>Future Work</p>
<p>The KELM corpus sentences covers all facts in the KG but the generated sentences are limited to a given entity and its direct relations to other entities. For example, given the triples (X, child, Y) and (Y, child, Z), it does not the contain "Z is a grandchild of X". More complex sentences could be generated by incorporating multi-hop relations in the KG. Recent work has also shown promising results on generating multilingual text from English triples (Castro Ferreira et al., 2020;Agarwal et al., 2020). Our proposed approach can be applied to generate a multilingual corpus of facts in various languages using English Wikidata.</p>
<p>Conclusion</p>
<p>In this paper, we converted an entire KG (Wikidata) to natural text (KELM Corpus), tackling various challenges over verbalizing domain-specific benchmark datasets. We further showcase that KG verbalization can be used to integrate KGs and natural text corpora by including the verbalized KG as additional pre-training data. We augment a retrieval-based language model with the generated synthetic KELM corpus as a retrieval corpus.</p>
<p>We evaluated the augmented model on open domain QA and a knowledge probe, showing improvements on both. The KELM Corpus is publicly available at https://github. com/google-research-datasets/ KELM-corpus.</p>
<p>Figure 3 :
3KG-Text alignment algorithm.</p>
<p>retrieval corpus with the KELM Corpus. We evaluate Training data Input: To Kill a Mockingbird author Harper Lee, publication date 11 July 1960. Target: To Kill a Mockingbird is a novel by Harper Lee published in 1960.Wikidata 
(KG) </p>
<p>Wikipedia 
(text) </p>
<p>Triple &lt;-&gt; 
Sentence Aligner </p>
<p>Text to Text Generator </p>
<p>T5 </p>
<p>Finetuning 1 </p>
<p>Pipelines for training the TEKGEN model and generating the KELM corpus. In Step 1 , KG triples are aligned with Wikipedia text using distant supervision. In Steps 2 &amp; 3 , T5 is fine-tuned sequentially first on this corpus, followed by a small number of steps on the WebNLG corpus, In Step 4 , BERT is fine-tuned to generate a semantic quality score for generated sentences w.r.t. triples. Steps 2 , 3 &amp; 4 together form TEKGEN. To generate the KELM corpus, in Step 5 , entity subgraphs are created using the relation pair alignment counts from the training corpus generated in step 1 . The subgraph triples are then converted into natural text using TEKGEN. Both the TEKGEN training corpus and the KELM corpus are available at https://github.com/ google-research-datasets/KELM-corpusdate of birth, date of death, 539854) 
(date of birth, occupation, 809626) 
(date of death, occupation, 393490) </p>
<p>Semantic quality 
filter 
(BERT) </p>
<p>KELM Corpus 
(text) </p>
<p>(To Kill a Mockingbird, author, 
Harper Lee) 
(To Kill a Mockingbird, 
publication date, 11 July 1960) </p>
<p>To Kill a Mockingbird is 
a novel by Harper Lee 
published in 1960. </p>
<p>To Kill a Mockingbird award 
received Pulitzer Prize for 
Fiction, Pulitzer Prize for Fiction 
point in time 1961, Pulitzer 
Prize for Fiction winner Harper 
Lee. 
(To Kill a Mockingbird, award 
received, Pulitzer Prize for Fiction) 
(To Kill a Mockingbird, Pulitzer Prize 
for Fiction point in time, 1961) 
(To Kill a Mockingbird, Pulitzer Prize 
for Fiction winner, Harper Lee.) </p>
<p>Harper Lee won 
the 1961 Pulitzer 
Prize for Fiction 
for To Kill a 
Mockingbird. ✅ </p>
<p>Harper Lee won the 
1961 Pulitzer Prize 
for Fiction for To Kill 
a Mockingbird. ✅ </p>
<p>Harper Bazaar won 
the Pulitzer Prize for 
Poetry in 1691. ❌ </p>
<p>① 
② </p>
<p>④ </p>
<p>⑤ </p>
<p>WebNLG </p>
<p>(triples, text) </p>
<p>Finetuning 2 </p>
<p>③ </p>
<p>TEKGEN </p>
<p>Figure 2: the augmented system on the LAMA knowledge 
probe (Petroni et al., 2019) and open domain QA 
and show improvements on both. Through ablation 
experiments where we augment the retrieval corpus 
with the raw triples instead, we further confirm the 
effectiveness of verbalization. Our contributions 
are as follows -</p>
<p>• TEKGEN (Text from KG Generator): A data-
to-text sequence-to-sequence model for ver-
balizing an entire KG </p>
<p>• TEKGEN training corpus: Text-KG aligned 
corpora with a wide variety of relations in-
cluding dates and quantities </p>
<p>• KELM Corpus, 1 (Corpus for Knowledge-
Enhanced Language Model Pre-training): A 
large-scale synthetic corpus of Wikidata KG 
as natural text </p>
<p>• Data-to-text generation as a method to inte-
grate KGs with textual pre-training corpora, 
showing improvements on open domain QA 
and LAMA probe with the augmented model </p>
<p>1 </p>
<p>corpus does not have this noise due the use of typical NLP pipeline with coreference resolution and predicate linking. However, it suffers fromTotal KG triples 
45,578,261 
Triples aligned 
16,090,457 
Total sentences aligned 
7,978,814 
Total KG relations 
1,522 
Relations aligned 
663 </p>
<p>Table 1 :
1KG-Text alignment statistics.</p>
<p>Table 2 :
2Examples of alignment (training data).We retain the main triple as such and 
reformat the subproperty as a triple of the 
form (subject_entity, object_entity </p>
<p>subproperty_name, subproperty_value) </p>
<p>e.g. (Barack, spouse, Michelle) has the 
subproperty (place of marriage, Trinity 
Church). These are converted to (Barack, 
spouse, Michelle) and (Barack, Michelle 
place of marriage, Trinity Church). </p>
<p>Table 3 :
3Semantic Filtering Evaluation.</p>
<p>The 10x10 Photobooks are the result of a non-profit organization. 10x10 Photobooks was started in 00 2012.Edu was born in 1949 and is a member of Tigres UANL. Edu ( footballer , born in 1949 ) Tigres UANL's end time was 01 January 1983. Edu ( footballer , born 1949 ) was at Tigres UANL from 01 January 1978. Edu, who was born in 1949, played for Tigres UANL between 1978 and 1983. To Kill a Mockingbird won the Pulitzer Prize for Fiction. To Kill a Mockingbird was Pulitzer Prize for Fiction, awarded in 00 1961. Harper Lee was the winner of the Pulitzer Prize for Fiction for To Kill a Mockingbird.Input Triples 
WebNLG_Finetuning + Triples_Inference TEKGEN + Subgraph_Inference 
(Michelle Obama, height, +71 inch) 
Michelle Obama's height is +71 inch. 
Michelle Obama is 71 inches tall. 
(10x10 Photobooks, instance of, 
Nonprofit organization), 
(10x10 Photobooks inception, 00 2012) </p>
<p>10x10 Photobooks, founded in 2012 
is a nonprofit organization. </p>
<p>(Edu (footballer, born 1949), member of 
sports team, Tigres UANL) 
(Edu (footballer, born 1949 ), Tigres 
UANL end time, 01 January 1983) 
(Edu (footballer, born 1949 ), Tigres 
UANL start time, 01 January 1978) </p>
<p>(To Kill a Mockingbird, award received, 
Pulitzer Prize for Fiction) 
(To Kill a Mockingbird Pulitzer Prize for 
Fiction point in time 00 1961) 
(To Kill a Mockingbird Pulitzer Prize for 
Fiction winner Harper Lee) </p>
<p>Harper Lee won the 1961 Pulitzer 
Prize for Fiction for To Kill a 
Mockingbird. </p>
<p>(Caucasus Mountains, country, Georgia 
(country)) 
(Caucasus Mountains, instance of, 
Mountain range) 
(Caucasus Mountains, country, Russia) 
(Caucasus Mountains, highest point, 
Mount Elbrus) 
(Caucasus Mountains, country, Armenia) </p>
<p>(Caucasus Mountains, topic's main 
category, Category:Caucasus Mountains) </p>
<p>The Caucasus Mountains are located in 
Georgia. 
The Caucasus Mountains is an example of a 
Mountain range. 
Caucasus Mountains is in Russia. 
The highest point in the Caucasus Mountain 
-s is Mount Elbrus. 
Caucasus Mountains is in the country of 
Armenia. 
The Caucasus Mountains is categorised as a 
Caucasus Mountains. </p>
<p>The Caucasus Mountains are a 
mountain range found in Georgia, 
Armenia and Russia. Mount Elbrus 
is the highest point in the Caucasus 
Mountains. </p>
<p>Table 5 :
5Examples of text generated by the final model in comparison to the model trained only on WebNLG.</p>
<p>Table 6 :
6Accuracy on LAMA probe. Pretaining corpus is CCnews and the retrieval corpus changed for REALM.REALM Retrieval Corpus 
NQ 
WQ 
ORIGINAL 
Wikipedia (reported) 
40.40 
40.70 
Wikipedia (rerun) 
38.84 
40.80 
REPLACED 
Triple Documents 
21.14 
42.54 
KELM Documents 
22.58 
41.19 
AUGMENTED 
Wikipedia + Triple Documents 
40.28 
42.91 
Wikipedia + KELM Documents 
41.47 
43.90 </p>
<p>Table 7 :
7Exact Match (EM) accuracy of REALM on NQ and WQ. Pretraining corpus used is CC-News.SQuAD. In comparison, the reported accuracy for 
BERT (Devlin et al., 2019) is 10.50% on Google-
RE, 32.30% on T-REx and 17.40% on SQuAD. 
BERT performs better on 1-1 T-REx relations with 
74.50% accuracy as compared to REALM with 
55.81% accuracy. However, this group consists of 
only two relations; capital and capital of. BERT 
also has better performance than REALM on the 
ConceptNet subcorpus. On inspection of some of 
the queries in ConceptNet, we found the questions 
to be vague and possibly hard for even humans. For 
example, Raven can ___ and Time is ___. </p>
<p>REPLACED The REPLACED model which uses 
only KELM Corpus Documents, performs better 
than the ORIGINAL model on WQ but the accuracy 
is much lower on NQ (rows 3&amp;4 in </p>
<p>(11B parameters) on NQ (35.20 → 41.47) and WQ (42.80 → 43.90). Wikipedia is the dominant corpus with 2B words whereas KELM corpus sentences are succinct with a total of 286M words ( §4.2) so it is likely the learned representations favour the Wikipedia format which is natural language sentences.</p>
<p>to convert database key-value pairs or pictures to text, WikiBio (Lebret et al., 2016) for biography generation from text, Wiseman et al. (2017) for text describing score statistics tables of basketball games, both ToTTo (Parikh et al., 2020) and DART (Radev et al., 2020) to generate text given a table and relevant highlighted cells. Many systems (van der Lee et al., 2018; Castro Ferreira et al., 2019;</p>
<p>T-REx(Elsahar et al., 2018) is a widely used Text-KG aligned corpus, built using systems such as coreference resolution and predicate linkers (details in §2.1.1). and also created an aligned corpus using Wikipedia hyperlinks and coreference resolution. (details on comparison in §2.1.2). In contrast, we use alias-based heuristics coupled with source text selection constraints to generate a corpus of 16M triples aligned with 8M sentences. Lastly, open information extraction i.e. automatic KG construction from text
https://en.wikipedia.org/wiki/ Wikipedia:Date_formattings
https://code.google.com/archive/p/ relation-extraction-corpus/
AcknowledgmentsWe thank William Woods, Jonni Kanerva, Tania Rojas-Esponda, Jianmo Ni, Aaron Cohen and Itai Rolnick for rating the synthetic corpus sample for human evaluation. We also thank Kelvin Guu for his valuable feedback on the paper.
Machine translation aided bilingual data-to-text generation and semantic parsing. Oshin Agarwal, Mihir Kale, Heming Ge, Siamak Shakeri, Rami Al-Rfou, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)Dublin, Ireland(Virtual). Association for Computational LinguisticsOshin Agarwal, Mihir Kale, Heming Ge, Siamak Shak- eri, and Rami Al-Rfou. 2020. Machine transla- tion aided bilingual data-to-text generation and se- mantic parsing. In Proceedings of the 3rd Interna- tional Workshop on Natural Language Generation from the Semantic Web (WebNLG+), pages 125-130, Dublin, Ireland (Virtual). Association for Computa- tional Linguistics.</p>
<p>Leveraging linguistic structure for open domain information extraction. Gabor Angeli, Melvin Jose Johnson Premkumar, Christopher D Manning, 10.3115/v1/P15-1034Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaLong Papers1Association for Computational LinguisticsGabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging linguis- tic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lan- guage Processing (Volume 1: Long Papers), pages 344-354, Beijing, China. Association for Computa- tional Linguistics.</p>
<p>Global learning of focused entailment graphs. Jonathan Berant, Ido Dagan, Jacob Goldberger, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics. the 48th Annual Meeting of the Association for Computational LinguisticsUppsala, SwedenAssociation for Computational LinguisticsJonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1220-1229, Uppsala, Sweden. Association for Com- putational Linguistics.</p>
<p>Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Tolga Bolukbasi, Kai-Wei Chang, Y James, Venkatesh Zou, Adam T Saligrama, Kalai, Advances in neural information processing systems. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Ad- vances in neural information processing systems, pages 4349-4357.</p>
<p>The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results. Claire Thiago Castro Ferreira, Nikolai Gardent, Chris Ilinykh, Simon Van Der Lee, Diego Mille, Anastasia Moussallem, Shimorina, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)DublinIreland (Virtual). Association for Computational LinguisticsThiago Castro Ferreira, Claire Gardent, Nikolai Ilinykh, Chris van der Lee, Simon Mille, Diego Moussallem, and Anastasia Shimorina. 2020. The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020). In Proceedings of the 3rd International Work- shop on Natural Language Generation from the Se- mantic Web (WebNLG+), pages 55-76, Dublin, Ire- land (Virtual). Association for Computational Lin- guistics.</p>
<p>Neural data-to-text generation: A comparison between pipeline and end-to-end architectures. Chris Thiago Castro Ferreira, Van Der Lee, Emiel Emiel Van Miltenburg, Krahmer, 10.18653/v1/D19-1052Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)HongThiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neu- ral data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Interna- tional Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 552-562, Hong</p>
<p>Association for Computational Linguistics. China Kong, Kong, China. Association for Computational Lin- guistics.</p>
<p>Kgpt: Knowledge-grounded pretraining for data-to-text generation. Wenhu Chen, Yu Su, Xifeng Yan, William Yang Wang, arXiv:2010.02307arXiv preprintWenhu Chen, Yu Su, Xifeng Yan, and William Yang Wang. 2020. Kgpt: Knowledge-grounded pre- training for data-to-text generation. arXiv preprint arXiv:2010.02307.</p>
<p>Scalable knowledge graph construction from text collections. Ryan Clancy, F Ihab, Jimmy Ilyas, Lin, 10.18653/v1/D19-6607Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER). the Second Workshop on Fact Extraction and VERification (FEVER)Hong Kong, ChinaAssociation for Computational LinguisticsRyan Clancy, Ihab F. Ilyas, and Jimmy Lin. 2019. Scal- able knowledge graph construction from text collec- tions. In Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 39-46, Hong Kong, China. Association for Compu- tational Linguistics.</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, V Quoc, Ruslan Le, Salakhutdinov, arXiv:1901.02860arXiv preprintZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car- bonell, Quoc V Le, and Ruslan Salakhutdinov. 2019. Transformer-xl: Attentive language mod- els beyond a fixed-length context. arXiv preprint arXiv:1901.02860.</p>
<p>Question answering on knowledge bases and text using universal schema and memory networks. Rajarshi Das, Manzil Zaheer, Siva Reddy, Andrew Mccallum, arXiv:1704.08384arXiv preprintRajarshi Das, Manzil Zaheer, Siva Reddy, and Andrew McCallum. 2017. Question answer- ing on knowledge bases and text using universal schema and memory networks. arXiv preprint arXiv:1704.08384.</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics.</p>
<p>Ondřej Dušek, Jekaterina Novikova, Verena Rieser, arXiv:1810.01170Findings of the e2e nlg challenge. arXiv preprintOndřej Dušek, Jekaterina Novikova, and Verena Rieser. 2018. Findings of the e2e nlg challenge. arXiv preprint arXiv:1810.01170.</p>
<p>T-REx: A large scale alignment of natural language with knowledge base triples. Hady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, Elena Simperl, Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)Miyazaki, JapanEuropean Language Resources Association (ELRAHady Elsahar, Pavlos Vougiouklis, Arslen Remaci, Christophe Gravier, Jonathon Hare, Frederique Laforest, and Elena Simperl. 2018. T-REx: A large scale alignment of natural language with knowledge base triples. In Proceedings of the Eleventh Interna- tional Conference on Language Resources and Eval- uation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).</p>
<p>Open information extraction from the web. Oren Etzioni, Michele Banko, Stephen Soderland, Daniel S Weld, Communications of the ACM. 5112Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extrac- tion from the web. Communications of the ACM, 51(12):68-74.</p>
<p>Entities as experts: Sparse memory access with entity supervision. Thibault Févry, Baldini Livio, Nicholas Soares, Eunsol Fitzgerald, Tom Choi, Kwiatkowski, arXiv:2004.07202arXiv preprintThibault Févry, Livio Baldini Soares, Nicholas FitzGer- ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En- tities as experts: Sparse memory access with entity supervision. arXiv preprint arXiv:2004.07202.</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational LinguisticsClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Pro- ceedings of the 10th International Conference on Natural Language Generation, pages 124-133, San- tiago de Compostela, Spain. Association for Compu- tational Linguistics.</p>
<p>Using natural-language processing to produce weather forecasts. E Goldberg, N Driedger, R I Kittredge, IEEE Expert. 92E. Goldberg, N. Driedger, and R. I. Kittredge. 1994. Using natural-language processing to produce weather forecasts. IEEE Expert, 9(2):45-53.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, arXiv:2002.08909Realm: Retrievalaugmented language model pre-training. arXiv preprintKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu- pat, and Ming-Wei Chang. 2020. Realm: Retrieval- augmented language model pre-training. arXiv preprint arXiv:2002.08909.</p>
<p>Learning beyond datasets: Knowledge graph augmented neural networks for natural language processing. K M Annervaz, Somnath Basu Roy Chowdhury, Ambedkar Dukkipati, 10.18653/v1/N18-1029Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong PapersAssociation for Computational LinguisticsAnnervaz K M, Somnath Basu Roy Chowdhury, and Ambedkar Dukkipati. 2018. Learning beyond datasets: Knowledge graph augmented neural net- works for natural language processing. In Proceed- ings of the 2018 Conference of the North Ameri- can Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long Papers), pages 313-322, New Orleans, Louisiana. Association for Computational Linguis- tics.</p>
<p>Text-to-text pre-training for data-totext tasks. Mihir Kale, arXiv:2005.10433arXiv preprintMihir Kale. 2020. Text-to-text pre-training for data-to- text tasks. arXiv preprint arXiv:2005.10433.</p>
<p>Dense passage retrieval for open-domain question answering. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, Wentau Yih, arXiv:2004.04906arXiv preprintVladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen- tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906.</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics1Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Design of a knowledge-based report generator. Karen Kukich, 10.3115/981311.98134021st Annual Meeting of the Association for Computational Linguistics. Cambridge, Massachusetts, USAAssociation for Computational LinguisticsKaren Kukich. 1983. Design of a knowledge-based re- port generator. In 21st Annual Meeting of the As- sociation for Computational Linguistics, pages 145- 150, Cambridge, Massachusetts, USA. Association for Computational Linguistics.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics. Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav PetrovKenton LeeTom Kwiatkowski, Jennimaria Palomaki, Olivia Red- field, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu- ral questions: a benchmark for question answering research. Transactions of the Association of Compu- tational Linguistics.</p>
<p>Neural text generation from structured data with application to the biography domain. Rémi Lebret, David Grangier, Michael Auli, 10.18653/v1/D16-1128Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsRémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In Proceed- ings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203-1213, Austin, Texas. Association for Computational Lin- guistics.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, arXiv:2005.11401Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprintPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Hein- rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock- täschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. arXiv preprint arXiv:2005.11401.</p>
<p>Barack's wife hillary: Using knowledge graphs for fact-aware language modeling. Robert Logan, Nelson F Liu, Matthew E Peters, Matt Gardner, Sameer Singh, 10.18653/v1/P19-1598Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. 2019. Barack's wife hillary: Using knowledge graphs for fact-aware lan- guage modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 5962-5971, Florence, Italy. Associa- tion for Computational Linguistics.</p>
<p>Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. Thomas Manzini, Yao Lim, Alan W Chong, Yulia Black, Tsvetkov, 10.18653/v1/N19-1062Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaLong and Short Papers1Association for Computational LinguisticsThomas Manzini, Lim Yao Chong, Alan W Black, and Yulia Tsvetkov. 2019. Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. In Proceed- ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 615-621, Minneapo- lis, Minnesota. Association for Computational Lin- guistics.</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>P Ankur, Xuezhi Parikh, Sebastian Wang, Manaal Gehrmann, Bhuwan Faruqui, Dhingra, arXiv:2004.14373Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. arXiv preprintAnkur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. Totto: A controlled table-to-text generation dataset. arXiv preprint arXiv:2004.14373.</p>
<p>Deep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, 10.18653/v1/N18-1202Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaAssociation for Computational Linguistics1Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long Papers), pages 2227-2237, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Knowledge enhanced contextual word representations. Matthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, Noah A Smith, 10.18653/v1/D19-1005Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsMatthew E. Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A. Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 43-54, Hong Kong, China. Associ- ation for Computational Linguistics.</p>
<p>Association for Computational Linguistics. Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, 10.18653/v1/D19-1250Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaLanguage models as knowledge bases?Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl- edge bases? In Proceedings of the 2019 Confer- ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 2463-2473, Hong Kong, China. As- sociation for Computational Linguistics.</p>
<p>Dart: Open-domain structured data record to text generation. Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Rajani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, arXiv:2007.02871arXiv preprintDragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Nazneen Fatema Ra- jani, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. 2020. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871.</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the lim- its of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>SQuAD: 100,000+ questions for machine comprehension of text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, 10.18653/v1/D16-1264Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasAssociation for Computational LinguisticsPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics.</p>
<p>Relation extraction with matrix factorization and universal schemas. Sebastian Riedel, Limin Yao, Andrew Mccallum, Benjamin M Marlin, Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAtlanta, GeorgiaAssociation for Computational LinguisticsSebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Pro- ceedings of the 2013 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74-84, Atlanta, Georgia. Association for Computa- tional Linguistics.</p>
<p>How much knowledge can you pack into the parameters of a language model. Adam Roberts, Colin Raffel, Noam Shazeer, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsAdam Roberts, Colin Raffel, and Noam Shazeer. 2020. How much knowledge can you pack into the param- eters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5418-5426, Online. Association for Computational Linguistics.</p>
<p>The woman worked as a babysitter: On biases in language generation. Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, Nanyun Peng, 10.18653/v1/D19-1339Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 3407- 3412, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Handling rare items in data-to-text generation. Anastasia Shimorina, Claire Gardent, 10.18653/v1/W18-6543Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational LinguisticsAnastasia Shimorina and Claire Gardent. 2018. Han- dling rare items in data-to-text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 360-370, Tilburg University, The Netherlands. Association for Computational Linguistics.</p>
<p>Representing general relational knowledge in Concept-Net 5. Robyn Speer, Catherine Havasi, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). the Eighth International Conference on Language Resources and Evaluation (LREC'12)Istanbul, TurkeyEuropean Language Resources Association (ELRARobyn Speer and Catherine Havasi. 2012. Repre- senting general relational knowledge in Concept- Net 5. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 3679-3686, Istanbul, Turkey. Eu- ropean Language Resources Association (ELRA).</p>
<p>PullNet: Open domain question answering with iterative retrieval on knowledge bases and text. Haitian Sun, Tania Bedrax-Weiss, William Cohen, 10.18653/v1/D19-1242Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsHaitian Sun, Tania Bedrax-Weiss, and William Cohen. 2019. PullNet: Open domain question answering with iterative retrieval on knowledge bases and text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 2380- 2390, Hong Kong, China. Association for Computa- tional Linguistics.</p>
<p>Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods. Chris Van Der Lee, Emiel Krahmer, Sander Wubben, 10.18653/v1/W18-6504Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The Netherlands. Association for Computational LinguisticsChris van der Lee, Emiel Krahmer, and Sander Wubben. 2018. Automated learning of templates for data-to-text generation: comparing rule-based, statistical and neural methods. In Proceedings of the 11th International Conference on Natural Lan- guage Generation, pages 35-45, Tilburg Univer- sity, The Netherlands. Association for Computa- tional Linguistics.</p>
<p>Pat Verga, Haitian Sun, Baldini Livio, William W Soares, Cohen, arXiv:2007.00849Facts as experts: Adaptable and interpretable neural memory over symbolic knowledge. arXiv preprintPat Verga, Haitian Sun, Livio Baldini Soares, and William W Cohen. 2020. Facts as experts: Adapt- able and interpretable neural memory over symbolic knowledge. arXiv preprint arXiv:2007.00849.</p>
<p>Wikidata: A free collaborative knowledge base. Denny Vrandečić, Markus Krötzsch, Communications of the ACM. 57Denny Vrandečić and Markus Krötzsch. 2014. Wiki- data: A free collaborative knowledge base. Commu- nications of the ACM, 57:78-85.</p>
<p>Challenges in data-to-document generation. Sam Wiseman, Stuart Shieber, Alexander Rush, 10.18653/v1/D17-1239Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsSam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empiri- cal Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Learning neural templates for text generation. Sam Wiseman, M Stuart, Alexander M Shieber, Rush, arXiv:1808.10122arXiv preprintSam Wiseman, Stuart M Shieber, and Alexander M Rush. 2018. Learning neural templates for text gen- eration. arXiv preprint arXiv:1808.10122.</p>
<p>Improving question answering over incomplete KBs with knowledgeaware reader. Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, William Yang Wang, 10.18653/v1/P19-1417Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsFlorence, ItalyAssociation for Computational LinguisticsWenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. 2019. Improving question answering over incomplete KBs with knowledge- aware reader. In Proceedings of the 57th Annual Meeting of the Association for Computational Lin- guistics, pages 4258-4264, Florence, Italy. Associa- tion for Computational Linguistics.</p>
<p>Donghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng, arXiv:2010.00796Jaket: Joint pre-training of knowledge graph and language understanding. arXiv preprintDonghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. 2020. Jaket: Joint pre-training of knowledge graph and language understanding. arXiv preprint arXiv:2010.00796.</p>
<p>Tianyi Zhang, Varsha Kishore, Felix Wu, Q Kilian, Yoav Weinberger, Artzi, arXiv:1904.09675Bertscore: Evaluating text generation with bert. arXiv preprintTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Eval- uating text generation with bert. arXiv preprint arXiv:1904.09675.</p>            </div>
        </div>

    </div>
</body>
</html>
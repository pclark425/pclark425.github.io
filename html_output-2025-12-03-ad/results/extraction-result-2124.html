<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2124 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2124</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2124</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-54.html">extraction-schema-54</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <p><strong>Paper ID:</strong> paper-277467270</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.24047v2.pdf" target="_blank">Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents</a></p>
                <p><strong>Paper Abstract:</strong> As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks, ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.</p>
                <p><strong>Cost:</strong> 0.031</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2124.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2124.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemReasoner: Heuristic search over a large language model's knowledge space using quantum‑chemical feedback</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven planner that integrates quantum-chemical atomistic simulations as feedback to assign rewards and prune hypothesis search paths for catalyst discovery and reaction design.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemReasoner</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM constructs hierarchical hypothesis/search trees (query plans specifying catalyst type, inclusion/exclusion criteria, operators) and uses quantum‑chemical simulation outputs (adsorption energies, reaction energy barriers, structural stability) as feedback/rewards to prune and refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Catalysis / Materials</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is performed by running atomistic/quantum‑chemical simulations (used to compute adsorption energies, reaction energy barriers, and structural stability) and using these computed thermodynamic/kinetic metrics as reward signals to eliminate unpromising branches of the search tree.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity quantum-chemical atomistic simulations (first-principles / DFT-level style evaluations are implied); captures adsorption energies and reaction barriers but computationally expensive and limited by model scale.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Paper treats simulation-derived quantum‑chemical metrics as a domain-relevant surrogate for experimental energetics; survey notes such simulation feedback is valuable but computational cost and absence of wet-lab confirmation can limit sufficiency for definitive claims in chemistry where experimental validation is often required.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical agreement metrics reported in survey; accuracy depends on underlying quantum-chemical method fidelity (not specified here).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Survey does not report real wet‑lab follow-up experiments for ChemReasoner; the system relies on quantum-chemical simulation feedback. The survey highlights the lack of experimental confirmation as a general limitation for simulated-only pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey contrasts simulation feedback (used here) with the higher confidence of experimental verification; no direct numerical comparison is provided for ChemReasoner between simulation and experiment within the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey flags general limitations: high computational cost, potential mismatch between simulation predictions and experimental outcomes, and that simulation-only pruning can produce plausible yet experimentally invalid candidates if the simulations or approximations are imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Within the survey text, ChemReasoner is presented as effectively using quantum-chemical feedback to prune hypothesis space, improving search efficiency; however, no specific experimentally validated success cases are detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Comparisons are to computed energetics from atomistic simulations (i.e., simulation-derived benchmarks), not to experimental ground truth within the surveyed description.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent replication; reproducibility is possible in silico if simulation scripts and parameters are shared, but high computational cost can inhibit broad reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>High: quantum-chemical atomistic simulations are computationally expensive and time-consuming, limiting scale and speed of iterative validation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey indicates chemistry commonly requires wet-lab experiments for conclusive validation; quantum-chemical simulation is valuable for screening but typically insufficient alone for definitive experimental claims.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not describe explicit uncertainty quantification for the computed quantum-chemical metrics in ChemReasoner; uncertainties would depend on chosen quantum chemistry methods.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>High computational cost, potential method/approximation errors in atomistic simulations, lack of wet-lab confirmation, and the requirement for careful parameterization are identified limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2124.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Generative Agent (bi-level optimizer using LLM + differentiable simulations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bi-level framework where an LLM generates discrete hypotheses/experimental designs (outer loop) while differentiable simulations optimize continuous parameters (inner loop) to provide gradient-based feedback informing hypothesis refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Scientific Generative Agent (SGA)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Outer-level LLM planner proposes hypotheses and experimental designs; inner-level differentiable simulators optimize continuous parameters (e.g., physical constants, molecular coordinates) and return gradient-based feedback to the planner, enabling exploitation/exploration control and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Physical sciences / Engineering / Molecular design</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is performed via differentiable simulations that provide continuous, gradient-based feedback on parameterized designs; the LLM uses these simulation outputs to refine and select hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Differentiable simulations (capable of gradient-based optimization); fidelity depends on simulator fidelity — can be high when using physics-based differentiable models, but survey notes general trade-offs between fidelity and computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey frames differentiable simulation feedback as improving robustness and innovation, but notes that simulation-based validation may not substitute for physical experiments where domain norms demand empirical confirmation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical accuracy metrics provided in the survey for SGA; performance characterized qualitatively (improved robustness and adaptability).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Survey reports SGA uses differentiable simulations in the inner loop but does not describe downstream wet-lab experimental validation; it notes that simulation feedback improves planning but does not guarantee experimental success.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey contrasts differentiable-simulation-based inner-loop validation with other approaches (e.g., pure RL, Monte Carlo), arguing differentiable feedback offers gradient information that helps optimization, but no quantitative comparison to experiments is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey highlights general limitations: differentiable simulations can be limited by model mismatch, incomplete physics, and may produce overconfident refinements that fail in real-world experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Survey claims SGA increases robustness and innovation in planning cycles through differentiable simulation feedback; no explicit experimentally validated outcomes are detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Validation compares candidate designs against simulator-predicted optima rather than experimental ground truth in the survey description.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent experimental replication; reproducibility in silico depends on simulator availability and deterministic configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Potentially high: differentiable simulations with gradient-based optimization can be computationally heavy, though often less costly than repeated high-fidelity experiment runs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey notes that while simulation-driven optimization is powerful in physics/engineering, experimental validation remains the standard for definitive claims in many domains.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not describe explicit uncertainty quantification for SGA outputs; differentiable simulators can provide gradient and loss signals but not necessarily calibrated statistical uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Model mismatch, simulator fidelity limits, computational cost, and the gap between optimized simulated performance and real-world experiment outcomes are key limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2124.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MyCrunchGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MyCrunchGPT: An LLM-assisted framework for scientific machine learning using surrogate and high-fidelity CFD simulations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-driven pipeline that combines DeepONet surrogates for fast flow-field estimations and high-fidelity CFD simulator Nektar++ to iteratively optimize aerodynamic designs (e.g., NACA airfoils) and confirm validity via high-fidelity simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MyCrunchGPT: A llm assisted framework for scientific machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MyCrunchGPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM generates candidate designs and translates instructions into workflows that use a DeepONet surrogate for rapid evaluations and Nektar++ CFD high-fidelity simulations for accurate validation of optimized airfoil designs.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Fluid dynamics / Aerodynamics / Engineering</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Two-stage validation: (1) low-cost surrogate evaluations via DeepONet for rapid search/optimization; (2) confirmation through high-fidelity CFD simulations (Nektar++) to validate flow fields and performance of proposed designs.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Hybrid: surrogate (DeepONet — low-cost, approximate) used for exploration; high-fidelity CFD (Nektar++ — physics-resolving) used for confirmation. The high-fidelity stage is physics-based and intended to be accurate within discretization/modeling limits.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey indicates the surrogate+HF workflow enables efficient optimization with subsequent high-fidelity confirmation viewed as sufficient within computational aerodynamics; however, experimental wind-tunnel validation would still be the gold standard for physical deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Survey states results were 'confirmed through high-fidelity simulations' but does not provide numerical error metrics or percentages of agreement with experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No wet-lab (wind-tunnel) experiments are reported in the survey; validation occurs in silico via CFD. Survey notes that final experimental validation is not always performed and is a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey highlights the combination of surrogate speed and CFD fidelity but does not present explicit numerical comparisons between surrogate predictions and CFD results in this summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey generalizes potential failures: surrogate inaccuracies leading to false positives, cost of running many high-fidelity confirmations, and the risk that CFD-confirmed designs still fail in physical experiments due to fabrication or unmodeled physics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Survey reports MyCrunchGPT optimized NACA airfoils and confirmed validity through high-fidelity simulations, demonstrating iterative optimization success in silico.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Comparison is to high-fidelity CFD as the operative ground truth in silico; no experimental ground-truth comparisons are reported in the surveyed text.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent reproduction; in principle reproducible if surrogate and CFD setups are shared, but computational cost is a barrier.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Moderate-to-high: surrogate evaluations are cheap, but high-fidelity CFD confirmations require substantial compute and runtime, making full-scale validation expensive.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>In aerodynamics, high-fidelity CFD is a common domain-standard for in silico validation; physical wind-tunnel tests remain the final norm for experimental acceptance.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not describe formal UQ reported by MyCrunchGPT; uncertainty arises from surrogate approximation and CFD modeling assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Surrogate fidelity limits, computational cost of CFD confirmations, and absence of physical experiments limit claims about real-world performance.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines low-fidelity surrogate (DeepONet) for rapid exploration with high-fidelity CFD (Nektar++) for confirmation — rationale is efficiency during search and reliable verification before concluding design quality.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2124.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DockingGA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DockingGA: Enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A molecule-generation system that uses molecular docking simulations to score candidate molecules and provide reward signals for iterative molecular generation and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dockingga: enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>DockingGA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generative transformer + genetic algorithm produce candidate molecules; molecular docking simulations compute docking scores against biological targets which are used as reward signals to guide design iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Computational chemistry / Drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation relies on molecular docking simulations to estimate binding affinity scores between generated molecules and targets; docking scores function as rewards and filters during optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Empirical/approximate molecular docking (low-to-medium fidelity): useful for relative ranking but known to be approximate and sensitive to docking protocol and scoring function choices.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey implies docking-based validation is useful for in silico screening but insufficient as definitive evidence for binding — experimental biochemical/biophysical assays are the domain norm for confirmation in drug discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Survey does not report numeric accuracy; docking scores are known to correlate imperfectly with experimental binding affinities and are therefore probabilistic and approximate.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Survey does not report wet-lab binding or functional assays for DockingGA; reliance is on docking simulations and the survey flags the need for experimental follow-up.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey compares docking-based reward methods to more expensive physics-based or experimental validation, noting docking is cheaper but less definitive; no numerical comparisons provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey cites general issues: docking can yield false positives/negatives due to scoring inaccuracies; optimization against docking scores can exploit scoring artifacts leading to molecules that score well but fail experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Survey presents DockingGA as a mechanism to generate target-specific binders in silico; no experimental confirmation examples are given in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No direct comparison to experimental binding data is described in the survey for DockingGA.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Reproducibility can be affected by docking protocol choices; survey does not report independent experimental replications.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Docking is computationally inexpensive relative to high-fidelity physics or wet-lab assays, enabling large-scale screening; however, experimental validation remains costly and time-consuming.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey reiterates that wet-lab biochemical/biophysical assays (e.g., binding experiments) and functional cellular assays are required to confirm in silico docking predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not indicate formal uncertainty quantification for docking scores; scoring functions provide relative rankings rather than calibrated probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Approximate nature of docking, scoring function artifacts, vulnerability to optimization over the scoring function rather than true binding, and lack of experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2124.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ClimSight</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ClimSight (integration of geospatial data and AWI Climate Model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-augmented system that integrates geospatial databases with the AWI Climate Model to assess climate impacts of activities (e.g., agricultural practices) using climate-model simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Local climate services for all, courtesy of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ClimSight</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates retrieval of geospatial data with the AWI Climate Model simulation framework to run climate impact assessments driven by user queries and simulated scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Climate science / Geosciences</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>simulated</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is accomplished via climate-model simulations (AWI Climate Model) that quantify impacts under parameterized scenarios; outputs are compared against model expectations and used to inform assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>High-fidelity global climate model (process-resolving for large-scale climatology) — appropriate for scenario analysis but limited by spatial/temporal resolution and model structural uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey suggests climate-model-based validation is standard for scenario analysis; however, observational validation and ensemble/model-intercomparison are typically required to establish robustness in climate science.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numeric accuracy provided in survey; climate-model outputs have known uncertainties dependent on model configuration and scenario assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>No physical experiments are applicable; validation in climate science normally involves comparison with observational datasets and multi-model ensembles — the survey does not detail such comparisons for ClimSight.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey contrasts simulation-driven validation with observational ground-truthing and notes that model-based studies should compare to observations and ensembles for robustness; no specific comparisons reported for ClimSight.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey flags general limitations of relying on a single climate model: limited resolution, structural model uncertainty, and potential for mismatches with localized observations.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Survey presents ClimSight as enabling assessment of climate impacts for specific activities via modeling; specific validated case studies are not detailed in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No explicit mention of comparison to observational datasets in the survey's description of ClimSight.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Model-based analyses are reproducible if configurations and input data are shared; survey notes computational cost and data availability as potential barriers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>High: global climate-model simulations are computationally intensive and require significant runtime/resources.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey highlights that climate results are typically validated against observational data and by multi-model ensembles rather than single-model outputs alone.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not report whether ClimSight provides formal UQ; climate models commonly produce ensemble spread and scenario-based uncertainties but specifics are not given here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Single-model reliance, resolution limits, model structural uncertainty, and heavy computational demands.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2124.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Coscientist: Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-component LLM-based system designed to autonomously plan, design, and execute chemical experiments; reported to have demonstrated catalyzed chemical reactions with safety measures and misuse prevention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autonomous chemical research with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Coscientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Organizes experimental design and execution using a set of commands/tools (GOOGLE, PYTHON, DOCUMENTATION, EXPERIMENT) to autonomously propose and carry out chemical experiments; integrates literature, tooling and experimental steps with safety constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Experimental organic synthesis / Catalysis</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Survey states Coscientist demonstrated capabilities through 'successful catalyzed chemical reactions' — implying real laboratory execution of designed experiments with empirical observation of reaction outcomes and safety controls.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A (validation via wet-lab experiments rather than simulation in the reported demonstration).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey presents experimental execution as strong domain-appropriate validation for chemistry; experimental confirmation is the domain norm and is considered sufficient to support claims of successful reaction design when protocols and data are reproducible.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical performance metrics provided in the survey; outcomes described qualitatively as successful catalyzed reactions.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Survey reports that Coscientist demonstrated successful catalyzed chemical reactions and addressed safety concerns, but does not provide specific experimental protocols, yields, analytical validation (e.g., NMR/LCMS) or numerical outcomes in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey contrasts Coscientist's experimental demonstrations with systems that only use simulation; experimental validation is identified as stronger evidence but survey does not provide side-by-side quantitative comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey notes general risks and limitations: safety and misuse concerns, potential for hallucinated or overconfident claims even when experiments are automated, and the need for careful human oversight; no detailed failed experiments are specified.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Successful catalyzed reactions are cited as successful experimental validation within Coscientist's reported applications; survey does not list detailed metrics but highlights experimental execution as a key success.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Experimental observations serve as the ground truth for the reported cases; the survey does not describe comparisons to independent experimental replication.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent replication studies; reproducibility requires detailed protocols and data sharing which the survey notes is an important consideration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Wet-lab experiments are time-consuming and resource-intensive relative to purely computational validation; survey highlights higher costs and safety/logistical demands for lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey reinforces that chemical claims typically require wet-lab experimental verification (yields, characterization) to be accepted in the field.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not report formal uncertainty quantification for the experiments; standard chemical validation would include yield/error margins and analytical characterization, but these are not detailed in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Survey emphasizes safety concerns, need for human oversight, limits of automation when encountering unexpected laboratory conditions, and lack of detailed reporting in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Coscientist appears to combine computational planning and literature retrieval with actual laboratory experiment execution (computational + experimental) — rationale is to close the loop between design and empirical validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2124.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Co‑Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards an AI Co‑scientist (multi-agent framework by Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM-based research system (Generation, Reflection, Ranking, Evolution, Meta-review agents, etc.) that orchestrates literature synthesis, hypothesis generation, ranking tournaments, and tool-assisted validation including invoking domain models (e.g., AlphaFold) and experimental resources, and reports empirical validation in biomedical applications.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards an ai co-scientist</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Co-Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Multi-agent architecture where specialized agents generate, reflect on, rank, and evolve hypotheses; integrates external tools (AlphaFold, DepMap, drug libraries) and schedules resources for hypotheses; survey reports empirical validation in pharmaceutical repurposing, target discovery, and antimicrobial resistance via rigorous experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Biomedicine / Drug repurposing / Antimicrobial resistance</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Hybrid validation: computational analyses (literature grounding, model-based predictions using tools such as AlphaFold and DepMap) followed by empirical experimental verification in biomedical contexts (reported 'rigorous experimental verification' for certain findings).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Computational components include high-fidelity domain models (e.g., AlphaFold for protein structure) and data-driven resources (DepMap); experimental verification uses wet-lab assays per survey description.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey treats the combination of computational prediction + experimental verification as sufficient and appropriate for biomedical discovery; domain norms in biology require wet-lab confirmation for claims about targets or therapeutic effects.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Survey does not give aggregate numerical accuracy metrics for the co-scientist's validated findings; statements are qualitative (empirically validated effectiveness reported).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Survey reports that the AI co-scientist demonstrated empirically validated effectiveness in pharmaceutical repurposing, target discovery, and antimicrobial resistance research through rigorous experimental verification, but does not provide detailed experimental protocols or numerical outcomes in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey contrasts multi-agent, hybrid computational+experimental validation favorably against purely computational pipelines, noting improved novelty and accuracy when experimental follow-up is included; no numerical side-by-side metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey lists limitations: potential for hallucination remains, dependence on the accessible literature corpus (can be speculative in low-data areas), and ethical/regulatory concerns when moving from in silico to clinical/large-scale biological testing.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Reported successes include drug repurposing candidates, target discovery, and findings in antimicrobial resistance that were empirically validated; survey does not provide detailed quantitative results but highlights these as demonstrated validated outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Survey implies comparisons to experimental ground truth for validated cases (i.e., laboratory assay results), but does not provide explicit metrics or datasets used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent external replications; reproducibility depends on data/tool availability and detailed protocol reporting which the survey flags as important.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>High: experimental validation, particularly in biomedicine, demands significant time, laboratory resources, and regulatory oversight; survey notes cost/time burdens as a barrier to scaling experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey emphasizes that biomedical discoveries typically require wet-lab validation and clinical-grade verification for translational claims.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not detail formal uncertainty quantification used by the co-scientist in validated studies; pipeline uses tournament scoring (Elo) and other ranking metrics internally, but calibration versus experimental uncertainty is not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Dependence on literature/data availability, potential hallucinations, ethical/regulatory hurdles, and the resource cost of experimental follow-up are highlighted limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines LLM-driven hypothesis generation and model/tool-based computational analyses (AlphaFold, DepMap, drug libraries) with wet-lab experimental follow-up for validation; rationale is to use computational triage to prioritize experiments and then empirically verify promising candidates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2124.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reinschmidt Trap RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinschmidt et al. — Reinforcement learning applied to magneto‑optical trap management</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-based controller that manages a magneto-optical trap, optimizing atom cooling using rewards that combine atom number and average kinetic energy measured experimentally.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reinforcement learning in cold atom experiments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>RL controller for magneto-optical trap (Reinschmidt et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An RL planner optimizes control policies for a cold-atom experiment; reward function incorporates experimentally measured observables (atom number and average kinetic energy) to guide learning in the physical experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Atomic physics / Experimental physics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>experimental</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is performed in situ on the physical experiment: the RL agent's actions are evaluated by direct experimental measurements (number of atoms and kinetic energy) and the reward function drives performance improvements in the real experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A — validation happens on the real experimental apparatus with direct physical measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey presents in-experiment RL evaluation as sufficient for demonstrating control improvements in experimental physics; domain norms favor direct experimental demonstration for such control tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Survey does not provide specific quantitative performance improvements (e.g., percentage increase in atom number), only that the RL system optimized cooling by accounting for atom number and kinetic energy in the reward.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>RL policy was trained/assessed on the magneto-optical trap with reward signals derived from measured atom counts and kinetic energies; survey does not give experimental protocols or numerical outcome metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey uses this as an example of RL robustness in dynamic physical experiments; no explicit comparison to classical control methods is reported in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey does not list explicit failures for this experiment, but general RL challenges (reward design, noisy measurements, and robustness to disturbances) are discussed in the planner section.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Reported success: RL optimized atom cooling by using reward incorporating atom number and kinetic energy, demonstrating applicability of RL in a real physical experimental setting.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>Outcomes are compared to experimental observables (i.e., measured atom counts/energies) rather than to simulated ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent replication; reproducing such experiments requires access to similar experimental apparatus and conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Experimental setup and runtime can be resource-intensive; survey notes RL training in physical systems can be costly and sensitive to experimental noise.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Direct experimental demonstration is the accepted norm in experimental atomic physics; in-situ validation is necessary for claims about control performance.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not describe formal uncertainty quantification for experimental measurements in this example; measurement noise and experimental disturbances are discussed as challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Reward design sensitivity, measurement noise, and computational/experimental costs for running numerous RL episodes are identified limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2124.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRISPR‑GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRISPR‑GPT: An LLM agent for automated design of gene‑editing experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM agent that integrates web search, primer design tools (Primer3), curated guide libraries, and CRISPR design tools to select CRISPR systems and design experimental protocols; it also proposes validation experiments tailored to experimental outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Crispr-gpt: An llm agent for automated design of gene-editing experiments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CRISPR-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM coordinates multiple domain tools (search, Primer3, CRISPRPick, guide libraries) to design CRISPR experimental workflows and to recommend validation experiments based on modeled or observed outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Genetics / Molecular biology / Gene editing</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Validation is positioned as experimental (wet-lab) in nature: the agent designs validation experiments (e.g., guide testing, functional assays) informed by computational predictions from integrated tools; the survey indicates the system helps design protocols for genome-editing workflows and validation experiments but does not report completed wet-lab results.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>N/A — validation is intended to be wet-lab experimental assays; computational design tools (e.g., guide scoring) provide in silico assessments but are not full physical substitutes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey indicates that designing proper validation experiments is necessary but does not claim these were always executed; in molecular biology, wet-lab confirmation is the domain norm for validating gene-editing outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>No numerical accuracy reported in the survey; the system's contribution is in protocol design and tool integration rather than reporting validated experimental metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Survey states CRISPR-GPT enables researchers to design experimental protocols and validation experiments, but the survey does not report execution of those experiments or empirical outcomes within the referenced description.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey contrasts computational design/guide-ranking tools with downstream wet-lab validation, noting that computational predictions are helpful for prioritization but experimental assays are required to confirm editing efficiency and specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey cautions that systems which only design experiments without actual experimental execution leave open the risk of unvalidated or impractical protocols; specific failed CRISPR experiments are not described.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Survey does not report completed experimental successes for CRISPR-GPT in the summarized text; success is claimed in enabling protocol design and lowering barriers for novices to implement workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No direct ground-truth experimental comparisons are reported in the survey for CRISPR-GPT.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent replication of designed protocols; reproducibility would depend on experimental execution and reporting.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Wet-lab validation is costly and time-consuming; the survey notes CRISPR workflows and validation assays impose resource burdens even if the agent streamlines protocol design.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey reiterates that wet-lab validation (molecular assays, sequencing, functional tests) is required to accept gene-editing claims in biology.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not describe UQ methods used by CRISPR-GPT; guide-scoring tools provide heuristic risk/efficacy scores but not calibrated uncertainties.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Designing validation experiments without performing them limits claim strength; computational guides can be inaccurate and require experimental verification.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>Combines computational tool-based guide selection and protocol design with recommended wet-lab validation experiments (execution of which is needed to complete validation).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2124.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2124.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how automated or AI-driven scientific discovery systems validate their results, including the type of validation used (fabricated, simulated, or experimental), the fidelity and accuracy of validation approaches, domain-specific validation standards, and cases where validation succeeded or failed.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChemCrow: Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chemistry-focused agent integrating an extensive set of expert-designed tools (e.g., molecular property queries, reaction prediction, synthesis planning) to autonomously manage synthesis planning and propose experimental workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmenting large language models with chemistry tools</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChemCrow</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates ~18 expert-designed chemistry tools to support molecular property queries, reaction prediction, and experimental synthesis planning; it can autonomously propose multi-step chemical workflows and suggests experiment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Chemistry / Organic synthesis / Drug discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_type</strong></td>
                            <td>hybrid</td>
                        </tr>
                        <tr>
                            <td><strong>validation_description</strong></td>
                            <td>Survey describes ChemCrow as enabling autonomous execution of chemical tasks including synthesis planning; validation approaches in the domain include simulation, planning cross-checks, and, in some references, experimental demonstrations, though the survey's summary does not enumerate explicit experimental confirmations for ChemCrow within the text.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity</strong></td>
                            <td>Varied — relies on tool-specific models and predictive packages (some empirical, some physics-informed) rather than a single fidelity; fidelity depends on each tool (e.g., reaction prediction models vs. property calculators).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_sufficiency</strong></td>
                            <td>Survey suggests tool-augmented pipelines improve planning reliability, but also stresses that chemistry claims ultimately require experimental confirmation; sufficiency of tool-based validation alone is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_accuracy</strong></td>
                            <td>Survey does not provide numeric accuracy metrics for ChemCrow's validated outputs; prior work indicates tool performance varies by task.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_performed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_validation_details</strong></td>
                            <td>Survey mentions ChemCrow autonomously executes complex chemical tasks and supports synthesis planning, but does not explicitly state whether the referenced work performed wet‑lab experimental validation in the described summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_comparison</strong></td>
                            <td>Survey contrasts multi-tool computational workflows with experimental verification, noting computational tools are critical for planning but do not replace laboratory validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_failures</strong></td>
                            <td>Survey generally warns about hallucinations, safety concerns, and the risk of proposing impractical or unsafe experimental procedures when tool outputs are taken at face value without human oversight or lab validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_success_cases</strong></td>
                            <td>Survey states ChemCrow enhances performance in organic synthesis and drug discovery tasks in computational/automated workflows; explicit experimental success cases are not detailed in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_comparison</strong></td>
                            <td>No explicit ground-truth experimental comparisons are provided in the survey summary for ChemCrow.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_replication</strong></td>
                            <td>Survey does not report independent replication; reproducibility depends on tool versions, integrations, and dataset availability.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_time</strong></td>
                            <td>Computational planning using many tools is cheaper/faster than lab work, but experimental follow-up remains costly and time-consuming; survey highlights these trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_validation_norms</strong></td>
                            <td>Survey emphasizes that chemistry requires wet-lab validation for empirical claims; tool-assisted designs must be experimentally confirmed to be accepted.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Survey does not indicate explicit uncertainty quantification across ChemCrow's tool outputs; tool-specific confidence scores may exist but are not detailed.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_limitations</strong></td>
                            <td>Integration complexity, tool heterogeneity, potential for incompatible outputs, safety and reproducibility concerns, and lack of uniform experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_approach</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_validation_details</strong></td>
                            <td>ChemCrow couples multiple computational prediction and planning tools (various fidelities) with the potential for experimental execution in downstream workflows; the survey stresses combined computational+experimental pipelines are ideal though not always completed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback <em>(Rating: 2)</em></li>
                <li>LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery <em>(Rating: 2)</em></li>
                <li>MyCrunchGPT: A llm assisted framework for scientific machine learning <em>(Rating: 2)</em></li>
                <li>Dockingga: enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation <em>(Rating: 2)</em></li>
                <li>Local climate services for all, courtesy of large language models <em>(Rating: 1)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 2)</em></li>
                <li>Towards an ai co-scientist <em>(Rating: 2)</em></li>
                <li>Reinforcement learning in cold atom experiments <em>(Rating: 2)</em></li>
                <li>Crispr-gpt: An llm agent for automated design of gene-editing experiments <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2124",
    "paper_id": "paper-277467270",
    "extraction_schema_id": "extraction-schema-54",
    "extracted_data": [
        {
            "name_short": "ChemReasoner",
            "name_full": "ChemReasoner: Heuristic search over a large language model's knowledge space using quantum‑chemical feedback",
            "brief_description": "An LLM-driven planner that integrates quantum-chemical atomistic simulations as feedback to assign rewards and prune hypothesis search paths for catalyst discovery and reaction design.",
            "citation_title": "Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback",
            "mention_or_use": "mention",
            "system_name": "ChemReasoner",
            "system_description": "LLM constructs hierarchical hypothesis/search trees (query plans specifying catalyst type, inclusion/exclusion criteria, operators) and uses quantum‑chemical simulation outputs (adsorption energies, reaction energy barriers, structural stability) as feedback/rewards to prune and refine hypotheses.",
            "scientific_domain": "Chemistry / Catalysis / Materials",
            "validation_type": "simulated",
            "validation_description": "Validation is performed by running atomistic/quantum‑chemical simulations (used to compute adsorption energies, reaction energy barriers, and structural stability) and using these computed thermodynamic/kinetic metrics as reward signals to eliminate unpromising branches of the search tree.",
            "simulation_fidelity": "High-fidelity quantum-chemical atomistic simulations (first-principles / DFT-level style evaluations are implied); captures adsorption energies and reaction barriers but computationally expensive and limited by model scale.",
            "validation_sufficiency": "Paper treats simulation-derived quantum‑chemical metrics as a domain-relevant surrogate for experimental energetics; survey notes such simulation feedback is valuable but computational cost and absence of wet-lab confirmation can limit sufficiency for definitive claims in chemistry where experimental validation is often required.",
            "validation_accuracy": "No numerical agreement metrics reported in survey; accuracy depends on underlying quantum-chemical method fidelity (not specified here).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Survey does not report real wet‑lab follow-up experiments for ChemReasoner; the system relies on quantum-chemical simulation feedback. The survey highlights the lack of experimental confirmation as a general limitation for simulated-only pipelines.",
            "validation_comparison": "Survey contrasts simulation feedback (used here) with the higher confidence of experimental verification; no direct numerical comparison is provided for ChemReasoner between simulation and experiment within the survey.",
            "validation_failures": "Survey flags general limitations: high computational cost, potential mismatch between simulation predictions and experimental outcomes, and that simulation-only pruning can produce plausible yet experimentally invalid candidates if the simulations or approximations are imperfect.",
            "validation_success_cases": "Within the survey text, ChemReasoner is presented as effectively using quantum-chemical feedback to prune hypothesis space, improving search efficiency; however, no specific experimentally validated success cases are detailed in the survey.",
            "ground_truth_comparison": "Comparisons are to computed energetics from atomistic simulations (i.e., simulation-derived benchmarks), not to experimental ground truth within the surveyed description.",
            "reproducibility_replication": "Survey does not report independent replication; reproducibility is possible in silico if simulation scripts and parameters are shared, but high computational cost can inhibit broad reproduction.",
            "validation_cost_time": "High: quantum-chemical atomistic simulations are computationally expensive and time-consuming, limiting scale and speed of iterative validation.",
            "domain_validation_norms": "Survey indicates chemistry commonly requires wet-lab experiments for conclusive validation; quantum-chemical simulation is valuable for screening but typically insufficient alone for definitive experimental claims.",
            "uncertainty_quantification": "Survey does not describe explicit uncertainty quantification for the computed quantum-chemical metrics in ChemReasoner; uncertainties would depend on chosen quantum chemistry methods.",
            "validation_limitations": "High computational cost, potential method/approximation errors in atomistic simulations, lack of wet-lab confirmation, and the requirement for careful parameterization are identified limitations.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2124.0"
        },
        {
            "name_short": "SGA",
            "name_full": "Scientific Generative Agent (bi-level optimizer using LLM + differentiable simulations)",
            "brief_description": "A bi-level framework where an LLM generates discrete hypotheses/experimental designs (outer loop) while differentiable simulations optimize continuous parameters (inner loop) to provide gradient-based feedback informing hypothesis refinement.",
            "citation_title": "LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery",
            "mention_or_use": "mention",
            "system_name": "Scientific Generative Agent (SGA)",
            "system_description": "Outer-level LLM planner proposes hypotheses and experimental designs; inner-level differentiable simulators optimize continuous parameters (e.g., physical constants, molecular coordinates) and return gradient-based feedback to the planner, enabling exploitation/exploration control and iterative refinement.",
            "scientific_domain": "Physical sciences / Engineering / Molecular design",
            "validation_type": "simulated",
            "validation_description": "Validation is performed via differentiable simulations that provide continuous, gradient-based feedback on parameterized designs; the LLM uses these simulation outputs to refine and select hypotheses.",
            "simulation_fidelity": "Differentiable simulations (capable of gradient-based optimization); fidelity depends on simulator fidelity — can be high when using physics-based differentiable models, but survey notes general trade-offs between fidelity and computational cost.",
            "validation_sufficiency": "Survey frames differentiable simulation feedback as improving robustness and innovation, but notes that simulation-based validation may not substitute for physical experiments where domain norms demand empirical confirmation.",
            "validation_accuracy": "No numerical accuracy metrics provided in the survey for SGA; performance characterized qualitatively (improved robustness and adaptability).",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Survey reports SGA uses differentiable simulations in the inner loop but does not describe downstream wet-lab experimental validation; it notes that simulation feedback improves planning but does not guarantee experimental success.",
            "validation_comparison": "Survey contrasts differentiable-simulation-based inner-loop validation with other approaches (e.g., pure RL, Monte Carlo), arguing differentiable feedback offers gradient information that helps optimization, but no quantitative comparison to experiments is presented.",
            "validation_failures": "Survey highlights general limitations: differentiable simulations can be limited by model mismatch, incomplete physics, and may produce overconfident refinements that fail in real-world experiments.",
            "validation_success_cases": "Survey claims SGA increases robustness and innovation in planning cycles through differentiable simulation feedback; no explicit experimentally validated outcomes are detailed in the survey.",
            "ground_truth_comparison": "Validation compares candidate designs against simulator-predicted optima rather than experimental ground truth in the survey description.",
            "reproducibility_replication": "Survey does not report independent experimental replication; reproducibility in silico depends on simulator availability and deterministic configurations.",
            "validation_cost_time": "Potentially high: differentiable simulations with gradient-based optimization can be computationally heavy, though often less costly than repeated high-fidelity experiment runs.",
            "domain_validation_norms": "Survey notes that while simulation-driven optimization is powerful in physics/engineering, experimental validation remains the standard for definitive claims in many domains.",
            "uncertainty_quantification": "Survey does not describe explicit uncertainty quantification for SGA outputs; differentiable simulators can provide gradient and loss signals but not necessarily calibrated statistical uncertainties.",
            "validation_limitations": "Model mismatch, simulator fidelity limits, computational cost, and the gap between optimized simulated performance and real-world experiment outcomes are key limitations.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2124.1"
        },
        {
            "name_short": "MyCrunchGPT",
            "name_full": "MyCrunchGPT: An LLM-assisted framework for scientific machine learning using surrogate and high-fidelity CFD simulations",
            "brief_description": "An LLM-driven pipeline that combines DeepONet surrogates for fast flow-field estimations and high-fidelity CFD simulator Nektar++ to iteratively optimize aerodynamic designs (e.g., NACA airfoils) and confirm validity via high-fidelity simulations.",
            "citation_title": "MyCrunchGPT: A llm assisted framework for scientific machine learning",
            "mention_or_use": "mention",
            "system_name": "MyCrunchGPT",
            "system_description": "LLM generates candidate designs and translates instructions into workflows that use a DeepONet surrogate for rapid evaluations and Nektar++ CFD high-fidelity simulations for accurate validation of optimized airfoil designs.",
            "scientific_domain": "Fluid dynamics / Aerodynamics / Engineering",
            "validation_type": "simulated",
            "validation_description": "Two-stage validation: (1) low-cost surrogate evaluations via DeepONet for rapid search/optimization; (2) confirmation through high-fidelity CFD simulations (Nektar++) to validate flow fields and performance of proposed designs.",
            "simulation_fidelity": "Hybrid: surrogate (DeepONet — low-cost, approximate) used for exploration; high-fidelity CFD (Nektar++ — physics-resolving) used for confirmation. The high-fidelity stage is physics-based and intended to be accurate within discretization/modeling limits.",
            "validation_sufficiency": "Survey indicates the surrogate+HF workflow enables efficient optimization with subsequent high-fidelity confirmation viewed as sufficient within computational aerodynamics; however, experimental wind-tunnel validation would still be the gold standard for physical deployment.",
            "validation_accuracy": "Survey states results were 'confirmed through high-fidelity simulations' but does not provide numerical error metrics or percentages of agreement with experiment.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No wet-lab (wind-tunnel) experiments are reported in the survey; validation occurs in silico via CFD. Survey notes that final experimental validation is not always performed and is a limitation.",
            "validation_comparison": "Survey highlights the combination of surrogate speed and CFD fidelity but does not present explicit numerical comparisons between surrogate predictions and CFD results in this summary.",
            "validation_failures": "Survey generalizes potential failures: surrogate inaccuracies leading to false positives, cost of running many high-fidelity confirmations, and the risk that CFD-confirmed designs still fail in physical experiments due to fabrication or unmodeled physics.",
            "validation_success_cases": "Survey reports MyCrunchGPT optimized NACA airfoils and confirmed validity through high-fidelity simulations, demonstrating iterative optimization success in silico.",
            "ground_truth_comparison": "Comparison is to high-fidelity CFD as the operative ground truth in silico; no experimental ground-truth comparisons are reported in the surveyed text.",
            "reproducibility_replication": "Survey does not report independent reproduction; in principle reproducible if surrogate and CFD setups are shared, but computational cost is a barrier.",
            "validation_cost_time": "Moderate-to-high: surrogate evaluations are cheap, but high-fidelity CFD confirmations require substantial compute and runtime, making full-scale validation expensive.",
            "domain_validation_norms": "In aerodynamics, high-fidelity CFD is a common domain-standard for in silico validation; physical wind-tunnel tests remain the final norm for experimental acceptance.",
            "uncertainty_quantification": "Survey does not describe formal UQ reported by MyCrunchGPT; uncertainty arises from surrogate approximation and CFD modeling assumptions.",
            "validation_limitations": "Surrogate fidelity limits, computational cost of CFD confirmations, and absence of physical experiments limit claims about real-world performance.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines low-fidelity surrogate (DeepONet) for rapid exploration with high-fidelity CFD (Nektar++) for confirmation — rationale is efficiency during search and reliable verification before concluding design quality.",
            "uuid": "e2124.2"
        },
        {
            "name_short": "DockingGA",
            "name_full": "DockingGA: Enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation",
            "brief_description": "A molecule-generation system that uses molecular docking simulations to score candidate molecules and provide reward signals for iterative molecular generation and refinement.",
            "citation_title": "Dockingga: enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation",
            "mention_or_use": "mention",
            "system_name": "DockingGA",
            "system_description": "Generative transformer + genetic algorithm produce candidate molecules; molecular docking simulations compute docking scores against biological targets which are used as reward signals to guide design iterations.",
            "scientific_domain": "Computational chemistry / Drug discovery",
            "validation_type": "simulated",
            "validation_description": "Validation relies on molecular docking simulations to estimate binding affinity scores between generated molecules and targets; docking scores function as rewards and filters during optimization.",
            "simulation_fidelity": "Empirical/approximate molecular docking (low-to-medium fidelity): useful for relative ranking but known to be approximate and sensitive to docking protocol and scoring function choices.",
            "validation_sufficiency": "Survey implies docking-based validation is useful for in silico screening but insufficient as definitive evidence for binding — experimental biochemical/biophysical assays are the domain norm for confirmation in drug discovery.",
            "validation_accuracy": "Survey does not report numeric accuracy; docking scores are known to correlate imperfectly with experimental binding affinities and are therefore probabilistic and approximate.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Survey does not report wet-lab binding or functional assays for DockingGA; reliance is on docking simulations and the survey flags the need for experimental follow-up.",
            "validation_comparison": "Survey compares docking-based reward methods to more expensive physics-based or experimental validation, noting docking is cheaper but less definitive; no numerical comparisons provided.",
            "validation_failures": "Survey cites general issues: docking can yield false positives/negatives due to scoring inaccuracies; optimization against docking scores can exploit scoring artifacts leading to molecules that score well but fail experimentally.",
            "validation_success_cases": "Survey presents DockingGA as a mechanism to generate target-specific binders in silico; no experimental confirmation examples are given in the survey.",
            "ground_truth_comparison": "No direct comparison to experimental binding data is described in the survey for DockingGA.",
            "reproducibility_replication": "Reproducibility can be affected by docking protocol choices; survey does not report independent experimental replications.",
            "validation_cost_time": "Docking is computationally inexpensive relative to high-fidelity physics or wet-lab assays, enabling large-scale screening; however, experimental validation remains costly and time-consuming.",
            "domain_validation_norms": "Survey reiterates that wet-lab biochemical/biophysical assays (e.g., binding experiments) and functional cellular assays are required to confirm in silico docking predictions.",
            "uncertainty_quantification": "Survey does not indicate formal uncertainty quantification for docking scores; scoring functions provide relative rankings rather than calibrated probabilities.",
            "validation_limitations": "Approximate nature of docking, scoring function artifacts, vulnerability to optimization over the scoring function rather than true binding, and lack of experimental verification.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2124.3"
        },
        {
            "name_short": "ClimSight",
            "name_full": "ClimSight (integration of geospatial data and AWI Climate Model)",
            "brief_description": "An LLM-augmented system that integrates geospatial databases with the AWI Climate Model to assess climate impacts of activities (e.g., agricultural practices) using climate-model simulations.",
            "citation_title": "Local climate services for all, courtesy of large language models",
            "mention_or_use": "mention",
            "system_name": "ClimSight",
            "system_description": "Integrates retrieval of geospatial data with the AWI Climate Model simulation framework to run climate impact assessments driven by user queries and simulated scenarios.",
            "scientific_domain": "Climate science / Geosciences",
            "validation_type": "simulated",
            "validation_description": "Validation is accomplished via climate-model simulations (AWI Climate Model) that quantify impacts under parameterized scenarios; outputs are compared against model expectations and used to inform assessments.",
            "simulation_fidelity": "High-fidelity global climate model (process-resolving for large-scale climatology) — appropriate for scenario analysis but limited by spatial/temporal resolution and model structural uncertainties.",
            "validation_sufficiency": "Survey suggests climate-model-based validation is standard for scenario analysis; however, observational validation and ensemble/model-intercomparison are typically required to establish robustness in climate science.",
            "validation_accuracy": "No numeric accuracy provided in survey; climate-model outputs have known uncertainties dependent on model configuration and scenario assumptions.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "No physical experiments are applicable; validation in climate science normally involves comparison with observational datasets and multi-model ensembles — the survey does not detail such comparisons for ClimSight.",
            "validation_comparison": "Survey contrasts simulation-driven validation with observational ground-truthing and notes that model-based studies should compare to observations and ensembles for robustness; no specific comparisons reported for ClimSight.",
            "validation_failures": "Survey flags general limitations of relying on a single climate model: limited resolution, structural model uncertainty, and potential for mismatches with localized observations.",
            "validation_success_cases": "Survey presents ClimSight as enabling assessment of climate impacts for specific activities via modeling; specific validated case studies are not detailed in the survey text.",
            "ground_truth_comparison": "No explicit mention of comparison to observational datasets in the survey's description of ClimSight.",
            "reproducibility_replication": "Model-based analyses are reproducible if configurations and input data are shared; survey notes computational cost and data availability as potential barriers.",
            "validation_cost_time": "High: global climate-model simulations are computationally intensive and require significant runtime/resources.",
            "domain_validation_norms": "Survey highlights that climate results are typically validated against observational data and by multi-model ensembles rather than single-model outputs alone.",
            "uncertainty_quantification": "Survey does not report whether ClimSight provides formal UQ; climate models commonly produce ensemble spread and scenario-based uncertainties but specifics are not given here.",
            "validation_limitations": "Single-model reliance, resolution limits, model structural uncertainty, and heavy computational demands.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2124.4"
        },
        {
            "name_short": "Coscientist",
            "name_full": "Coscientist: Autonomous chemical research with large language models",
            "brief_description": "A multi-component LLM-based system designed to autonomously plan, design, and execute chemical experiments; reported to have demonstrated catalyzed chemical reactions with safety measures and misuse prevention.",
            "citation_title": "Autonomous chemical research with large language models",
            "mention_or_use": "mention",
            "system_name": "Coscientist",
            "system_description": "Organizes experimental design and execution using a set of commands/tools (GOOGLE, PYTHON, DOCUMENTATION, EXPERIMENT) to autonomously propose and carry out chemical experiments; integrates literature, tooling and experimental steps with safety constraints.",
            "scientific_domain": "Chemistry / Experimental organic synthesis / Catalysis",
            "validation_type": "experimental",
            "validation_description": "Survey states Coscientist demonstrated capabilities through 'successful catalyzed chemical reactions' — implying real laboratory execution of designed experiments with empirical observation of reaction outcomes and safety controls.",
            "simulation_fidelity": "N/A (validation via wet-lab experiments rather than simulation in the reported demonstration).",
            "validation_sufficiency": "Survey presents experimental execution as strong domain-appropriate validation for chemistry; experimental confirmation is the domain norm and is considered sufficient to support claims of successful reaction design when protocols and data are reproducible.",
            "validation_accuracy": "No numerical performance metrics provided in the survey; outcomes described qualitatively as successful catalyzed reactions.",
            "experimental_validation_performed": true,
            "experimental_validation_details": "Survey reports that Coscientist demonstrated successful catalyzed chemical reactions and addressed safety concerns, but does not provide specific experimental protocols, yields, analytical validation (e.g., NMR/LCMS) or numerical outcomes in the survey text.",
            "validation_comparison": "Survey contrasts Coscientist's experimental demonstrations with systems that only use simulation; experimental validation is identified as stronger evidence but survey does not provide side-by-side quantitative comparisons.",
            "validation_failures": "Survey notes general risks and limitations: safety and misuse concerns, potential for hallucinated or overconfident claims even when experiments are automated, and the need for careful human oversight; no detailed failed experiments are specified.",
            "validation_success_cases": "Successful catalyzed reactions are cited as successful experimental validation within Coscientist's reported applications; survey does not list detailed metrics but highlights experimental execution as a key success.",
            "ground_truth_comparison": "Experimental observations serve as the ground truth for the reported cases; the survey does not describe comparisons to independent experimental replication.",
            "reproducibility_replication": "Survey does not report independent replication studies; reproducibility requires detailed protocols and data sharing which the survey notes is an important consideration.",
            "validation_cost_time": "Wet-lab experiments are time-consuming and resource-intensive relative to purely computational validation; survey highlights higher costs and safety/logistical demands for lab validation.",
            "domain_validation_norms": "Survey reinforces that chemical claims typically require wet-lab experimental verification (yields, characterization) to be accepted in the field.",
            "uncertainty_quantification": "Survey does not report formal uncertainty quantification for the experiments; standard chemical validation would include yield/error margins and analytical characterization, but these are not detailed in the survey.",
            "validation_limitations": "Survey emphasizes safety concerns, need for human oversight, limits of automation when encountering unexpected laboratory conditions, and lack of detailed reporting in the survey.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Coscientist appears to combine computational planning and literature retrieval with actual laboratory experiment execution (computational + experimental) — rationale is to close the loop between design and empirical validation.",
            "uuid": "e2124.5"
        },
        {
            "name_short": "AI Co‑Scientist",
            "name_full": "Towards an AI Co‑scientist (multi-agent framework by Google)",
            "brief_description": "A multi-agent LLM-based research system (Generation, Reflection, Ranking, Evolution, Meta-review agents, etc.) that orchestrates literature synthesis, hypothesis generation, ranking tournaments, and tool-assisted validation including invoking domain models (e.g., AlphaFold) and experimental resources, and reports empirical validation in biomedical applications.",
            "citation_title": "Towards an ai co-scientist",
            "mention_or_use": "mention",
            "system_name": "AI Co-Scientist",
            "system_description": "Multi-agent architecture where specialized agents generate, reflect on, rank, and evolve hypotheses; integrates external tools (AlphaFold, DepMap, drug libraries) and schedules resources for hypotheses; survey reports empirical validation in pharmaceutical repurposing, target discovery, and antimicrobial resistance via rigorous experimental verification.",
            "scientific_domain": "Biomedicine / Drug repurposing / Antimicrobial resistance",
            "validation_type": "hybrid",
            "validation_description": "Hybrid validation: computational analyses (literature grounding, model-based predictions using tools such as AlphaFold and DepMap) followed by empirical experimental verification in biomedical contexts (reported 'rigorous experimental verification' for certain findings).",
            "simulation_fidelity": "Computational components include high-fidelity domain models (e.g., AlphaFold for protein structure) and data-driven resources (DepMap); experimental verification uses wet-lab assays per survey description.",
            "validation_sufficiency": "Survey treats the combination of computational prediction + experimental verification as sufficient and appropriate for biomedical discovery; domain norms in biology require wet-lab confirmation for claims about targets or therapeutic effects.",
            "validation_accuracy": "Survey does not give aggregate numerical accuracy metrics for the co-scientist's validated findings; statements are qualitative (empirically validated effectiveness reported).",
            "experimental_validation_performed": true,
            "experimental_validation_details": "Survey reports that the AI co-scientist demonstrated empirically validated effectiveness in pharmaceutical repurposing, target discovery, and antimicrobial resistance research through rigorous experimental verification, but does not provide detailed experimental protocols or numerical outcomes in the survey text.",
            "validation_comparison": "Survey contrasts multi-agent, hybrid computational+experimental validation favorably against purely computational pipelines, noting improved novelty and accuracy when experimental follow-up is included; no numerical side-by-side metrics provided.",
            "validation_failures": "Survey lists limitations: potential for hallucination remains, dependence on the accessible literature corpus (can be speculative in low-data areas), and ethical/regulatory concerns when moving from in silico to clinical/large-scale biological testing.",
            "validation_success_cases": "Reported successes include drug repurposing candidates, target discovery, and findings in antimicrobial resistance that were empirically validated; survey does not provide detailed quantitative results but highlights these as demonstrated validated outcomes.",
            "ground_truth_comparison": "Survey implies comparisons to experimental ground truth for validated cases (i.e., laboratory assay results), but does not provide explicit metrics or datasets used for comparison.",
            "reproducibility_replication": "Survey does not report independent external replications; reproducibility depends on data/tool availability and detailed protocol reporting which the survey flags as important.",
            "validation_cost_time": "High: experimental validation, particularly in biomedicine, demands significant time, laboratory resources, and regulatory oversight; survey notes cost/time burdens as a barrier to scaling experimental validation.",
            "domain_validation_norms": "Survey emphasizes that biomedical discoveries typically require wet-lab validation and clinical-grade verification for translational claims.",
            "uncertainty_quantification": "Survey does not detail formal uncertainty quantification used by the co-scientist in validated studies; pipeline uses tournament scoring (Elo) and other ranking metrics internally, but calibration versus experimental uncertainty is not specified.",
            "validation_limitations": "Dependence on literature/data availability, potential hallucinations, ethical/regulatory hurdles, and the resource cost of experimental follow-up are highlighted limitations.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines LLM-driven hypothesis generation and model/tool-based computational analyses (AlphaFold, DepMap, drug libraries) with wet-lab experimental follow-up for validation; rationale is to use computational triage to prioritize experiments and then empirically verify promising candidates.",
            "uuid": "e2124.6"
        },
        {
            "name_short": "Reinschmidt Trap RL",
            "name_full": "Reinschmidt et al. — Reinforcement learning applied to magneto‑optical trap management",
            "brief_description": "An RL-based controller that manages a magneto-optical trap, optimizing atom cooling using rewards that combine atom number and average kinetic energy measured experimentally.",
            "citation_title": "Reinforcement learning in cold atom experiments",
            "mention_or_use": "mention",
            "system_name": "RL controller for magneto-optical trap (Reinschmidt et al.)",
            "system_description": "An RL planner optimizes control policies for a cold-atom experiment; reward function incorporates experimentally measured observables (atom number and average kinetic energy) to guide learning in the physical experiment.",
            "scientific_domain": "Atomic physics / Experimental physics",
            "validation_type": "experimental",
            "validation_description": "Validation is performed in situ on the physical experiment: the RL agent's actions are evaluated by direct experimental measurements (number of atoms and kinetic energy) and the reward function drives performance improvements in the real experiment.",
            "simulation_fidelity": "N/A — validation happens on the real experimental apparatus with direct physical measurements.",
            "validation_sufficiency": "Survey presents in-experiment RL evaluation as sufficient for demonstrating control improvements in experimental physics; domain norms favor direct experimental demonstration for such control tasks.",
            "validation_accuracy": "Survey does not provide specific quantitative performance improvements (e.g., percentage increase in atom number), only that the RL system optimized cooling by accounting for atom number and kinetic energy in the reward.",
            "experimental_validation_performed": true,
            "experimental_validation_details": "RL policy was trained/assessed on the magneto-optical trap with reward signals derived from measured atom counts and kinetic energies; survey does not give experimental protocols or numerical outcome metrics.",
            "validation_comparison": "Survey uses this as an example of RL robustness in dynamic physical experiments; no explicit comparison to classical control methods is reported in the survey text.",
            "validation_failures": "Survey does not list explicit failures for this experiment, but general RL challenges (reward design, noisy measurements, and robustness to disturbances) are discussed in the planner section.",
            "validation_success_cases": "Reported success: RL optimized atom cooling by using reward incorporating atom number and kinetic energy, demonstrating applicability of RL in a real physical experimental setting.",
            "ground_truth_comparison": "Outcomes are compared to experimental observables (i.e., measured atom counts/energies) rather than to simulated ground truth.",
            "reproducibility_replication": "Survey does not report independent replication; reproducing such experiments requires access to similar experimental apparatus and conditions.",
            "validation_cost_time": "Experimental setup and runtime can be resource-intensive; survey notes RL training in physical systems can be costly and sensitive to experimental noise.",
            "domain_validation_norms": "Direct experimental demonstration is the accepted norm in experimental atomic physics; in-situ validation is necessary for claims about control performance.",
            "uncertainty_quantification": "Survey does not describe formal uncertainty quantification for experimental measurements in this example; measurement noise and experimental disturbances are discussed as challenges.",
            "validation_limitations": "Reward design sensitivity, measurement noise, and computational/experimental costs for running numerous RL episodes are identified limitations.",
            "hybrid_validation_approach": false,
            "hybrid_validation_details": "",
            "uuid": "e2124.7"
        },
        {
            "name_short": "CRISPR‑GPT",
            "name_full": "CRISPR‑GPT: An LLM agent for automated design of gene‑editing experiments",
            "brief_description": "An LLM agent that integrates web search, primer design tools (Primer3), curated guide libraries, and CRISPR design tools to select CRISPR systems and design experimental protocols; it also proposes validation experiments tailored to experimental outcomes.",
            "citation_title": "Crispr-gpt: An llm agent for automated design of gene-editing experiments",
            "mention_or_use": "mention",
            "system_name": "CRISPR-GPT",
            "system_description": "LLM coordinates multiple domain tools (search, Primer3, CRISPRPick, guide libraries) to design CRISPR experimental workflows and to recommend validation experiments based on modeled or observed outcomes.",
            "scientific_domain": "Genetics / Molecular biology / Gene editing",
            "validation_type": "hybrid",
            "validation_description": "Validation is positioned as experimental (wet-lab) in nature: the agent designs validation experiments (e.g., guide testing, functional assays) informed by computational predictions from integrated tools; the survey indicates the system helps design protocols for genome-editing workflows and validation experiments but does not report completed wet-lab results.",
            "simulation_fidelity": "N/A — validation is intended to be wet-lab experimental assays; computational design tools (e.g., guide scoring) provide in silico assessments but are not full physical substitutes.",
            "validation_sufficiency": "Survey indicates that designing proper validation experiments is necessary but does not claim these were always executed; in molecular biology, wet-lab confirmation is the domain norm for validating gene-editing outcomes.",
            "validation_accuracy": "No numerical accuracy reported in the survey; the system's contribution is in protocol design and tool integration rather than reporting validated experimental metrics.",
            "experimental_validation_performed": false,
            "experimental_validation_details": "Survey states CRISPR-GPT enables researchers to design experimental protocols and validation experiments, but the survey does not report execution of those experiments or empirical outcomes within the referenced description.",
            "validation_comparison": "Survey contrasts computational design/guide-ranking tools with downstream wet-lab validation, noting that computational predictions are helpful for prioritization but experimental assays are required to confirm editing efficiency and specificity.",
            "validation_failures": "Survey cautions that systems which only design experiments without actual experimental execution leave open the risk of unvalidated or impractical protocols; specific failed CRISPR experiments are not described.",
            "validation_success_cases": "Survey does not report completed experimental successes for CRISPR-GPT in the summarized text; success is claimed in enabling protocol design and lowering barriers for novices to implement workflows.",
            "ground_truth_comparison": "No direct ground-truth experimental comparisons are reported in the survey for CRISPR-GPT.",
            "reproducibility_replication": "Survey does not report independent replication of designed protocols; reproducibility would depend on experimental execution and reporting.",
            "validation_cost_time": "Wet-lab validation is costly and time-consuming; the survey notes CRISPR workflows and validation assays impose resource burdens even if the agent streamlines protocol design.",
            "domain_validation_norms": "Survey reiterates that wet-lab validation (molecular assays, sequencing, functional tests) is required to accept gene-editing claims in biology.",
            "uncertainty_quantification": "Survey does not describe UQ methods used by CRISPR-GPT; guide-scoring tools provide heuristic risk/efficacy scores but not calibrated uncertainties.",
            "validation_limitations": "Designing validation experiments without performing them limits claim strength; computational guides can be inaccurate and require experimental verification.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "Combines computational tool-based guide selection and protocol design with recommended wet-lab validation experiments (execution of which is needed to complete validation).",
            "uuid": "e2124.8"
        },
        {
            "name_short": "ChemCrow",
            "name_full": "ChemCrow: Augmenting large language models with chemistry tools",
            "brief_description": "A chemistry-focused agent integrating an extensive set of expert-designed tools (e.g., molecular property queries, reaction prediction, synthesis planning) to autonomously manage synthesis planning and propose experimental workflows.",
            "citation_title": "Augmenting large language models with chemistry tools",
            "mention_or_use": "mention",
            "system_name": "ChemCrow",
            "system_description": "Integrates ~18 expert-designed chemistry tools to support molecular property queries, reaction prediction, and experimental synthesis planning; it can autonomously propose multi-step chemical workflows and suggests experiment steps.",
            "scientific_domain": "Chemistry / Organic synthesis / Drug discovery",
            "validation_type": "hybrid",
            "validation_description": "Survey describes ChemCrow as enabling autonomous execution of chemical tasks including synthesis planning; validation approaches in the domain include simulation, planning cross-checks, and, in some references, experimental demonstrations, though the survey's summary does not enumerate explicit experimental confirmations for ChemCrow within the text.",
            "simulation_fidelity": "Varied — relies on tool-specific models and predictive packages (some empirical, some physics-informed) rather than a single fidelity; fidelity depends on each tool (e.g., reaction prediction models vs. property calculators).",
            "validation_sufficiency": "Survey suggests tool-augmented pipelines improve planning reliability, but also stresses that chemistry claims ultimately require experimental confirmation; sufficiency of tool-based validation alone is limited.",
            "validation_accuracy": "Survey does not provide numeric accuracy metrics for ChemCrow's validated outputs; prior work indicates tool performance varies by task.",
            "experimental_validation_performed": null,
            "experimental_validation_details": "Survey mentions ChemCrow autonomously executes complex chemical tasks and supports synthesis planning, but does not explicitly state whether the referenced work performed wet‑lab experimental validation in the described summary.",
            "validation_comparison": "Survey contrasts multi-tool computational workflows with experimental verification, noting computational tools are critical for planning but do not replace laboratory validation.",
            "validation_failures": "Survey generally warns about hallucinations, safety concerns, and the risk of proposing impractical or unsafe experimental procedures when tool outputs are taken at face value without human oversight or lab validation.",
            "validation_success_cases": "Survey states ChemCrow enhances performance in organic synthesis and drug discovery tasks in computational/automated workflows; explicit experimental success cases are not detailed in the survey text.",
            "ground_truth_comparison": "No explicit ground-truth experimental comparisons are provided in the survey summary for ChemCrow.",
            "reproducibility_replication": "Survey does not report independent replication; reproducibility depends on tool versions, integrations, and dataset availability.",
            "validation_cost_time": "Computational planning using many tools is cheaper/faster than lab work, but experimental follow-up remains costly and time-consuming; survey highlights these trade-offs.",
            "domain_validation_norms": "Survey emphasizes that chemistry requires wet-lab validation for empirical claims; tool-assisted designs must be experimentally confirmed to be accepted.",
            "uncertainty_quantification": "Survey does not indicate explicit uncertainty quantification across ChemCrow's tool outputs; tool-specific confidence scores may exist but are not detailed.",
            "validation_limitations": "Integration complexity, tool heterogeneity, potential for incompatible outputs, safety and reproducibility concerns, and lack of uniform experimental validation.",
            "hybrid_validation_approach": true,
            "hybrid_validation_details": "ChemCrow couples multiple computational prediction and planning tools (various fidelities) with the potential for experimental execution in downstream workflows; the survey stresses combined computational+experimental pipelines are ideal though not always completed.",
            "uuid": "e2124.9"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback",
            "rating": 2
        },
        {
            "paper_title": "LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "MyCrunchGPT: A llm assisted framework for scientific machine learning",
            "rating": 2
        },
        {
            "paper_title": "Dockingga: enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation",
            "rating": 2
        },
        {
            "paper_title": "Local climate services for all, courtesy of large language models",
            "rating": 1
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 2
        },
        {
            "paper_title": "Towards an ai co-scientist",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning in cold atom experiments",
            "rating": 2
        },
        {
            "paper_title": "Crispr-gpt: An llm agent for automated design of gene-editing experiments",
            "rating": 2
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools",
            "rating": 1
        }
    ],
    "cost": 0.030727749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents
17 Apr 2025</p>
<p>Shuo Ren shuo.ren@ia.ac.cn 
State Key Laboratory of Multimodal Artificial Intelligence Systems</p>
<p>Foundation Model Research Center
Institute of Automation
CAS</p>
<p>Pu Jian 
State Key Laboratory of Multimodal Artificial Intelligence Systems</p>
<p>University of Chinese Academy of Science
BeijingChina</p>
<p>Zhenjiang Ren renzhenjiang2024@ia.ac.cn 
University of Chinese Academy of Science
BeijingChina</p>
<p>Chunlin Leng lengchunlin2023@ia.ac.cn 
State Key Laboratory of Multimodal Artificial Intelligence Systems</p>
<p>University of Chinese Academy of Science
BeijingChina</p>
<p>Can Xie 
State Key Laboratory of Multimodal Artificial Intelligence Systems</p>
<p>University of Chinese Academy of Science
BeijingChina</p>
<p>Jiajun Zhang jjzhang@nlpr.ia.ac.cn 
State Key Laboratory of Multimodal Artificial Intelligence Systems</p>
<p>Foundation Model Research Center
Institute of Automation
CAS</p>
<p>University of Chinese Academy of Science
BeijingChina</p>
<p>Wuhan AI Research
WuhanChina</p>
<p>P Team 
State Key Laboratory of Multimodal Artificial Intelligence Systems</p>
<p>Xinrun Du 
Yifan Yao 
Kaijing Ma 
Bingli Wang 
Tianyu Zheng 
Kang Zhu 
Minghao Liu 
Yim- Ing Liang 
Xiaolong Jin 
Zhenlin Wei 
Chujie Zheng 
Kaixin Deng 
Shian Jia 
Sichao Jiang 
Yiyan Liao 
Rui Li 
Qinrui Li 
Sirun Li 
Yizhi Li 
Yunwen Li 
Dehua Ma 
Yuansheng Haoran Que 
Qiyao Wang 
Zhoufutu Wen 
Siwei Wu 
Tianshun Xing 
Ming Xu 
ZekunZhenzhu Yang 
Moore Wang 
Junting Zhou 
Yuelin Bai 
Xingyuan Bu 
Chenglin Cai 
Liang Chen 
Yifan Chen 
Chengtuo Cheng 
Tianhao Cheng 
Keyi Ding 
Siming Huang 
Yun Huang 
Yaoru Li 
Yizhe Li 
Zhaoqun Li 
Tianhao Liang 
Chengdong Lin 
Hongquan Lin 
Yinghao Ma 
Tianyang Pang 
Zhongyuan Peng 
Zifan Peng 
Qige Qi 
Shi Qiu 
Xingwei Qu 
Shanghaoran Quan 
Yizhou Tan 
Zili Wang 
Chenqing Wang 
Hao Wang 
Yiya Wang 
Yubo Wang 
Jiajun Xu 
Kexin Yang 
Ruibin Yuan 
Yuanhao Yue 
Tianyang Zhan 
Chun Zhang 
Jinyang Zhang 
Xiyue Zhang 
Xingjian Zhang 
Yue Zhang 
Yongchi Zhao 
Xiangyu Zheng 
Chenghua Zhong 
Yang Gao 
Zhoujun Li 
Dayiheng Liu 
Qian Liu 
Tianyu Liu 
Shiwen Ni 
Junran Peng 
Yujia Qin 
Wenbo Su 
Guoyin Wang 
Shi Wang 
Jian Yang 
Min Yang 
Meng Cao 
Xiang Yue 
Zhaoxiang Zhang 
Wangchun- Shu Zhou 
Jiaheng Liu 
Qunshu Lin 
Wenhao Huang 
Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents
17 Apr 202550EBC453CA65EB34AD4E48FC00BEC2FCarXiv:2503.24047v2[cs.AI]
As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery.Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks-ranging from hypothesis generation and experiment design to data analysis and simulation.Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs.This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents.We highlight why they differ from general agents and the ways in which they advance research across various scientific fields.By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.</p>
<p>Introduction</p>
<p>Imagine an AI agent that autonomously designs a groundbreaking vaccine, optimizes chemical reactions with pinpoint precision, or uncovers hidden patterns in astronomical data-all while adhering to ethical standards and reproducibility.This is no longer science fiction.Large language models (LLMs), once confined to text generation, are now at the forefront of transforming scientific research by serving as specialized scientific agents that automate complex research tasks such as hypothesis generation, experiment design, and data analysis.</p>
<p>Modern scientific research is becoming increasingly complex, demanding innovative tools that not only manage vast amounts of information but also facilitate interdisciplinary discovery.In response, LLM-based scientific agents have evolved into systems specifically designed for the scientific domain.Unlike general-purpose LLMs, these agents integrate domain-specific knowledge, interface with tailored tools, and process diverse data types-including numerical datasets, chemical structures, and biological sequences.Consequently, they are uniquely positioned to streamline critical research tasks and drive rapid scientific breakthroughs.</p>
<p>As the adoption of these agents grows, a systematic review of their development, applications, and challenges becomes essential.While existing surveys provide comprehensive overviews of general LLM-based agents (Wang et al., 2024b;Xi et al., 2023;Guo et al., 2024;Hu et al., 2024a;Li et al., 2024d;Xie et al., 2024;Cheng et al., 2024;Shen, 2024), focusing specifically on LLM-based scientific agents is crucial given their distinctive roles and requirements in the scientific domain.Here's why this specialized survey is valuable:</p>
<ol>
<li>
<p>Domain-Specific Applications: Scientific agents are designed specifically for research tasks such as experimental design, data analysis, and hypothesis generation.They incorporate deep scientific methodologies and domain-specific expertise, enabling them to handle the rigorous demands of research workflows-capabilities that generalpurpose LLM agents, with their broad and nonspecialized approaches, do not possess.</p>
</li>
<li>
<p>Integration with Scientific Tools: Unlike general-purpose agents, scientific agents are architected to integrate seamlessly with specialized scientific tools, laboratory instruments, and advanced simulators.This integration supports real-time simulation, precise control, and robust validation of experimental processes, ensuring the agent can manage complex scientific operations.</p>
</li>
<li>
<p>Handling Complex Scientific Data: Sci-entific research involves complex data types, including numerical data, chemical structures, and biological sequences.LLM-based scientific agents must be equipped to process and interpret these data forms accurately, a requirement less prevalent in general-purpose LLM agents.4. Ethical and Reproducibility Concerns: Scientific agents adhere to strict ethical standards and incorporate rigorous validation and error-checking mechanisms, such as self-review and statistical analyses, to ensure that their outputs are reliable and reproducible-features typically not addressed by general-purpose LLM agents.</p>
</li>
<li>
<p>Advancement of Scientific Discovery: The ultimate goal of scientific agents is to accelerate scientific discovery and innovation.This objective requires capabilities beyond those of general LLM agents, including the ability to generate novel hypotheses, design experiments, and interpret complex results within specific scientific contexts.</p>
</li>
</ol>
<p>By focusing on these distinct aspects, a survey dedicated to LLM-based scientific agents can provide deeper insights into their development, applications, and the unique challenges they face, offering valuable guidance for researchers and practitioners in this specialized field.We hope this survey provides a roadmap for researchers and practitioners to harness these agents effectively, paving the way for faster, more reproducible, and ethically sound scientific discovery.</p>
<p>The remainder of this survey is organized as follows: In Section 2 (Architectures), we begin by examining the fundamental design of these agents.This section is subdivided into three main parts: first, the role of the Planner in decomposing and managing scientific tasks; second, the various Memory mechanisms that enable context retention and iterative learning; and third, the integration of specialized Tool Sets that extend scientific capabilities.After that, in Section 3 (General-purpose vs. Scientific Agents), we will give a detailed comparison between general-purpose and scientific agents, elaborating the reasons why scientific agents need careful design.In Section 4 (Benchmarks), we review the evaluation frameworks used to assess both the general reasoning ability and the scientific research-oriented performance of LLM-based agents.In Section 5 (Applications), we explore real-world applications of LLM-based agents in scientific research, highlighting how these systems are deployed to solve complex problems across various disciplines.In Section 6 (Ethics), we address the ethical implications and reproducibility challenges inherent in deploying these agents, ensuring that their outputs are not only efficient but also responsible and transparent.</p>
<p>Additionally, we conclude each subsection with a discussion on the challenges and potential directions for future research, offering guidance for both scholars and practitioners in harnessing the full potential of LLM-based scientific agents.</p>
<p>Figure 1: A typical architecture of LLM-based scientific agents.Note that in mainstream agent frameworks, planners are predominantly implemented based on LLMs, and their capabilities include task planning, reflection, and verification, etc.For the sake of abstraction, we represent these functions with a single planner in this architecture diagram.However, in specific implementations, different agents might be set up to accomplish distinct functions (see Section 2.1.6for further discussion about single-agent planners vs. multi-agent planners).</p>
<p>Architectures</p>
<p>The architecture of LLM-based scientific agents is designed to enable iterative, context-aware processing of complex scientific tasks.It typically consists of three core components: Planner, Memory, and Tool Set as shown in Figure 1.The workflow begins with the user submitting a query, which is typically a scientific task in the form of text and scientific data.The query is received as input by the system.The Planner decomposes the task into sub-tasks, retrieves relevant context or knowledge from Memory, and executes actions via the Tool Set (e.g., APIs, simulators, instruments, search engines, etc).Note that the LLM itself can also be treated as a tool to finish the related sub-tasks such as reasoning.The actions will generate some intermediate results, which are reflected and verified by the Planner, and Memory is updated with new knowledge to refine future decisions.If the reflection indicates further actions, the Planner will make new plans to do modification.This iterative process continues until the verification passes and the final integrated result is generated, then it is returned to the user as output.Note that while previous LLM-based multi-modal agents often include a separate perceptron module to handle multi-modal inputs (Xie et al., 2024), our survey integrates multi-modal scientific data perception as a fundamental capability of the Planner for the sake of simplification.In the following subsections, we will introduce the three components respectively.</p>
<p>Planner</p>
<p>The Planner serves as the logical core of LLMbased scientific agents, orchestrating structured, method-driven workflows rather than merely decomposing tasks in an ad hoc manner.By integrating domain knowledge with specialized reasoning strategies, the Planner translates high-level scientific problems into reproducible sub-tasks and coordinates their execution across the system.In this context, it enforces hierarchical planning that mirrors the scientific method-defining hypotheses, selecting tools for experimentation or simulation, and validating outcomes before moving for-ward.Planner designs in scientific agents can be broadly classified into four approaches as in Table 1, i,e, prompt-based, supervised fine-tuning (SFT), reinforcement learning (RL), and process supervision-each offering distinct mechanisms for incorporating domain-specific constraints and robust validation into the research process.The taxonomy of related work is provided in Figure 2.</p>
<p>Prompt-Based Planner</p>
<p>This subsection focuses on prompt-based planning of scientific agents, which harness the power of carefully engineered prompts to trigger in-context learning (ICL), thereby guiding the agent to produce a logical, structured and step-by-step plan without requiring additional fine-tuning, as illustrated in Figure 3(a).</p>
<p>Several studies have demonstrated the potential of prompt-based planning in scientific contexts, showcasing the ability of LLM-based scientific agents to tackle complex tasks.We have classified these works into three categories based on different prompt constructions, as shown in Table 2.Note that some works use more than one type of prompt, and we only exemplify their typical type.</p>
<p>Contextual Information Embedding is a key approach in prompt-based planning, where detailed, context-specific information is embedded within the prompt to guide the agent's decision-making.(Baek et al., 2024) Initial research ideas and background knowledge</p>
<p>Iterative research agent</p>
<p>Iterative</p>
<p>Feedback and Refinement</p>
<p>ResearchAgent (Baek et al., 2024) Instructions for review agents to provide feedback</p>
<p>Iterative research agent</p>
<p>LogicSolver (Yang et al., 2022) Mathematical problems with solution and reasoning instructions</p>
<p>Mathematical problem solving</p>
<p>Task</p>
<p>Structuring and Multi-Agent Coordination</p>
<p>Coscientist (Boiko et al., 2023) Four commands to define the action space Autonomous experimental design and execution ASA (Liu et al., 2024a) Instructions for experimental design, simulation, analysis, and report</p>
<p>Automated simulation</p>
<p>Virci (Su et al., 2024) Role and task descriptions Generate, evaluate, and refine research idea ChemCrow (Bran et al., 2024) Specific instructions about the task and the desired format Organic synthesis, drug discovery, and materials design prompt to improve treatment recommendations.Similarly, CoI (Li et al., 2024b) organizes related research papers in a sequential chain to guide research idea generation, while ResearchAgent (Baek et al., 2024) incorporates background knowledge to create an iterative research agent that refines ideas.</p>
<p>Iterative Feedback and Refinement enables continuous improvement and adaptation of generated plans by using prompts to facilitate feedback and refinement.ResearchAgent (Baek et al., 2024) uses prompts to guide review agents in providing feedback to refine research ideas, while Log-icSolver (Yang et al., 2022) prompts the LLM to not only solve mathematical problems but also explain the logical reasoning, enhancing transparency and interpretability of the planning process.</p>
<p>Task Structuring and Multi-Agent Coordination supports more complex planning by structuring tasks and coordinating actions across multiple agents or tools.Coscientist (Boiko et al., 2023) provides specific instructions for autonomous chemical experimental design and execution.It leverages four system prompts as commands that define the action space -'GOOGLE', 'PYTHON', 'DOCU-MENTATION', and 'EXPERIMENT'.ASA (Liu et al., 2024a) embeds the entire research cycle in the prompt for automated simulation, and VirSci (Su et al., 2024) organizes agents with role and task descriptions to collaboratively generate and refine research ideas.ChemCrow (Bran et al., 2024) tailors the prompt for managing chemistry-specific tools for tasks like materials design.</p>
<p>The above examples illustrate that prompt-based planning leverages carefully engineered prompts to trigger in-context learning, enabling scientific agents to generate logical, structured, step-by-step plans without extra fine-tuning.This approach embeds detailed context to guide decision-making in complex scientific tasks.</p>
<p>SFT-Based Planner</p>
<p>While prompt engineering and in-context learning offer zero-shot or few-shot planning capabilities for scientific agents, Supervised Fine-Tuning (SFT)-based planners enhance these capabilities by adapting a pre-training mechanism to specific scientific domains, as illustrated in Figure 3(b).The planning capability of SFT-based planners emerges from fine-tuning on domain-specific planning trajectories, which are curated datasets consisting of labeled input-output pairs.These pairs capture the step-by-step reasoning required for complex scientific tasks.For example, given a pre-trained planner with parameters θ, SFT optimizes these parameters by training on domain-specific pairs (x, y) derived from planning trajectories D = (x i , y i ) N i=1 .The objective function minimizes the negative loglikelihood:
L SF T (θ) = −E (x,y) T t=1 log P θ (y t |x, y &lt;t ) (1)
where T denotes the planning step horizon and y &lt;t represents previously generated planning steps.</p>
<p>By training on such domain-specific data that pair complex scientific tasks with expert-level step-wise solutions, SFT-based scientific agent planners effectively bridge the "reasoning gap" observed in prompt-based methods, particularly for multi-step tasks like experimental design and hypothesis refinement.</p>
<p>For example, in drug discovery, DrugAssist (Ye et al., 2023a) utilizes an instruction-based dataset to fine-tune a planner for interactive molecule optimization.This training process enables the planner to internalize expert feedback and integrate it into the planning process, effectively creating a drug discovery agent.Similarly, ToolLLM (Qin et al., 2023) fine-tunes its planner on a specialized ToolBench dataset, improving its ability to invoke and interact with external APIs.By training on sequences of successful tool usage and interactions, the planner learns to generate plans that leverage external tools effectively, creating a tool-augmented scientific agent.</p>
<p>In summary, SFT-based planners provide a robust mechanism for aligning capabilities with the nuanced demands of scientific research.These planners are trained to replicate expert strategies by learning from annotated data that provides stepby-step solutions to complex scientific problems, enabling the planner to adapt and apply these strategies to new tasks.Examples like BioGPT (Luo et al., 2022), GatorTronGPT (Peng et al., 2023), and others show how fine-tuned planners can tackle diverse scientific tasks by internalizing the reasoning processes from expert-curated data.</p>
<p>RL-Based Planner</p>
<p>Reinforcement Learning (RL) plays a critical role in developing the planning capabilities of scientific agents by enabling them to autonomously refine decision-making strategies within complex scientific tasks (Rafailov et al., 2024).Unlike traditional planning systems, RL-based planners rely on feedback loops where agents learn to improve their actions based on rewards and penalties, for example, calculated from preference data as illustrated in Figure 3(c) by some reward functions.These planners are designed to receive positive reinforcement for desirable outcomes, such as accurate hypotheses or optimal experimental designs, and negative feedback for undesirable ones, such as logical errors.This learning process equips the planner with the ability to adapt over time, transcending the limitations of approaches like SFT.</p>
<p>The process of enhancing planning through RL is grounded in the agent's objective of maximizing cumulative rewards.Formally, this can be expressed as the optimization of a policy π θ with respect to the reward function r(y, x), which measures the quality of an action (or plan) y generated from a given input x.Additionally, the agent's policy is regularized by a term involving the Kullback-Leibler (KL) divergence, ensuring that the updated policy does not deviate significantly from the original SFT-initialized policy.The formulation is:
J(θ) = E (x,y)∼π θ [r(y, x)]−λ•K l (π θ ||π SF T ) (2)
where π θ is the planner being optimized, r(y, x) quantifies the quality of the generated sequence y given input x, and the KL-divergence K l ensures the updated policy remains close to the SFTinitialized behavior.Here, the planner learns adaptive strategies through an iterative trial-and-error process, allowing it to make more informed decisions across multiple steps, which is particularly beneficial for complex scientific tasks requiring iterative refinement.We divide the studies of RLbased planners into three categories according to how they are designed.</p>
<p>Iterative Refinement of Reasoning Paths.To enhance the agent's reasoning and decision-making capabilities, RL-based planning incorporates iterative processes that refine the agent's reasoning paths.This is especially valuable in domains like mathematical problem-solving.For example, ReFT (Luong et al., 2024) uses Proximal Policy Optimization (PPO) combined with Chain-of-Thought (CoT) annotations, where an abundance of reasoning paths are automatically sampled given the question, and the rewards are naturally derived from the ground-truth answers.Similarly, CoT-Influx (Huang et al., 2024d) utilizes RL to optimize a coarse-to-fine pruning strategy that selects the most effective CoT examples for mathematical proofs.A multi-goal reward function is designed to measure the LLM loss, few-shot math reasoning effectiveness, and token length constraints.</p>
<p>Another important contribution comes from methodologies like STEP-DPO (Lai et al., 2024) and Flow-DPO (Deng and Mineiro, 2024), which apply Direct Preference Optimization (DPO) to the RL process.These techniques allow for the refinement of reasoning at the level of individual steps, treating each step as an element that can be optimized for better performance.This granular approach to optimization is crucial for improving the precision of reasoning in agents, making them more effective in complex scientific reasoning tasks.</p>
<p>Optimizing Scientific Workflows and Design Spaces.In scientific simulations and engineering design, RL-based planners are instrumental in optimizing intricate workflows and navigating large design spaces.The RL process in these contexts often involves multiple agents working together to optimize complex systems.For example, SciMARL (Scientific Multi-Agent Reinforcement Learning) (Bae and Koumoutsakos, 2022) demonstrates how a multi-agent RL framework for the discovery of wall models in large-eddy simulations, can identify optimal strategies for agents that perform actions, contingent on their information about the environment, and measures their performance via scenariorelated scalar reward functions.In molecular discovery, RL can be employed to iteratively refine the design of de novo drug candidates.MolRL-MGPT (Hu et al., 2024b) represents the problem of designing novel drug candidates as a cooperative Markov game consisting of multiple generative model agents with the scoring function as rewards.Similarly, Lutz et al. (2023) apply an RL-based approach combined with Monte Carlo tree search for protein design, creating complex protein nanomaterials with desired properties.These examples illustrate how RL can guide agents to explore and optimize design spaces, improving scientific outcomes across diverse fields.</p>
<p>Integrating Human Feedback and Dynamic Environments.For applications that require direct interaction with human expertise or adaptation to rapidly changing conditions, RL-based planners are specifically designed to integrate human feedback and adjust their strategies dynamically.For example, Barata et al. (2023) demonstrates how human preferences could be incorporated into a diagnostic AI for skin cancer by adjusting rewards and penalties based on expert-generated tables, thereby tailoring the system's performance to real-world clinical insights.For dynamic environments, Sauter et al. ( 2023) develops a meta-reinforcement learning algorithm for causal discovery.Their approach enables agents to construct explicit causal graphs with a reward based on the Structural Hamming Distance between the generated directed acyclic graph and the true causal graph, effectively guiding the agent toward more accurate causal models through optimal interventions.Additionally, Reinschmidt et al. (2024) applies RL to manage a magneto-optical trap in a cold atom experiment, where the system optimizes atom cooling by using a reward function that accounted for both the number of atoms and their average kinetic energy.This experiment underscores the robustness of RL-based planners in dealing with external disturbances.</p>
<p>These studies show that RL-based planners boost scientific agents' planning with well-designed rewards to refine decision-making and explore diverse solution paths for tasks like problem-solving, simulations, complex designing, and tasks incorporating human preference or in dynamic environments.They enable agents to autonomously enhance planning and reasoning, achieving precise and adaptable scientific intelligence over time and continuously improving overall performance.</p>
<p>Process Supervision Based Planner</p>
<p>Process supervision involves providing step-bystep feedback to Large Language Models (LLMs) during their reasoning or generation process, rather than only evaluating the final outcome.In recent studies, this technique has been employed to enhance the planning and reasoning capabilities of scientific LLMs.For instance, systems like Marco-o1 (Zhao et al., 2024a) integrate Chain-of-Thought (CoT) fine-tuning with Monte Carlo Tree Search (MCTS) and reflective mechanisms to explore multiple reasoning paths, while SCoRe (Self-Correction with Reinforcement Learning) (Kumar et al., 2024) leverages multi-turn online reinforcement learning on self-generated correction traces to continuously refine reasoning.Additional improvements include methods like V-STaR (Hosseini et al., 2024), which trains a verifier using Direct Preference Optimization (DPO) to select the best candidate among outputs, and OmegaPRM (Luo et al., 2024), which employs a divide-and-conquer strategy with MCTS to gather process supervision data that train Process Reward Models and enhance mathematical reasoning.These innovations not only strengthen the core reasoning process of LLMs, but also establish the foundation for process supervision based planners in LLM-based scientific agents, where similar feedback mechanisms are used to iteratively refine and optimize complex scientific hypotheses, as illustrated in Figure 3(d).</p>
<p>Building on these advances in process supervision for scientific LLMs, similar principles are adapted to the design of LLM-based scientific agents, yielding planning architectures that dynamically integrate automated hypothesis generation with domain-specific evaluative feedback.For example, ChemReasoner (Sprueill et al., 2024) leverages an LLM-driven planner to systematically navigate the expansive chemical space.In this framework, the LLM constructs a hierarchical search tree where each node embodies a distinct hypothesis generated through "query plans" that include catalyst type, inclusion/exclusion criteria, and relational operators.The planner then uses quantumchemical feedback-derived from atomistic simulations evaluating adsorption energies, reaction energy barriers, and structural stability-to assign rewards that prune unpromising pathways and iteratively refine the hypothesis space.This dual-loop mechanism effectively guides the exploration toward energetically favorable catalysts.</p>
<p>Similarly, the Scientific Generative Agent (SGA) (Ma et al., 2024a) employs a bi-level optimization framework to enhance planning capabilities for scientific discovery.At the outer level, the LLM functions as a strategic planner, generating discrete hypotheses and experimental designs while dynamically adjusting its query prompts based on past simulation results.In parallel, the inner level leverages differentiable simulations to optimize continuous parameters-such as physical constants or molecular coordinates-providing gradient-based feedback that informs subsequent hypothesis refinement.By balancing exploitation (refining known promising designs) and exploration (venturing into novel solution spaces) through controlled temperature tuning, SGA's planning cycle adapts to emergent data and uncertainties, thereby increasing both robustness and innovation in scientific outcomes.</p>
<p>Together, these strategies illustrate that embedding detailed, domain-specific feedback into the planning process empowers scientific agents to engage in continuous hypothesis refinement, adaptive experiment design, and iterative plan optimization.Such dynamic planning capabilities significantly enhance the agents' efficiency, accuracy, and adaptability in addressing complex scientific challenges.</p>
<p>Discussion</p>
<p>In summary, the Planner component-comprising prompt-based, SFT-based, RL-based, and process supervision-based approaches-serves as the central controller in LLM-based scientific agents.These planners are crucial for enabling scientific agents to translate high-level scientific queries into actionable plans by decomposing tasks, integrating domain-specific knowledge, and coordinating interactions with specialized tools.Scientific agents rely on logical, structured planning to ensure that experimental protocols and hypothesis testing are carried out methodically.However, despite advances in these approaches, challenges remain as shown in Table 1.For example, prompt-based planners are highly sensitive to prompt quality, leading to inconsistent scientific outputs; SFT-based planners require extensive, high-quality domainspecific datasets that are often costly to curate; RL-based planners struggle with designing robust reward functions and managing computational costs critical for scientific exploration; and process supervision-based planners, while promising for iterative refinement, demand complex and as-yet non-standardized feedback mechanisms.</p>
<p>Looking ahead, there are several promising directions for future research in the context of scientific agents.First, designing efficient surrogate models and robust reward mechanisms could reduce the computational burden associated with RL-based planning for scientific agents, making them more practical for real-world scientific problems.Second, integrating automated prompt optimization and self-supervised feedback could enhance the reliability and scalability of prompt-based and process supervision-based planners within scientific agents, leading to more consistent and accurate scientific outputs.Finally, establishing standardized evaluation benchmarks and cross-domain interface protocols will be essential for tracking progress and ensuring that future LLM-based scientific agents are both effective and ethically sound.These efforts will collectively contribute to building more autonomous, transparent, and efficient scientific agents capable of driving rapid, reproducible, and innovative scientific discovery.</p>
<p>Single-agent vs. Multi-agent Planner</p>
<p>As we note under Figure 1, the planner could be implemented in a single-agent fashion -where one LLM handles all planning, reflection, and verification functions -or in a multi-agent manner, with specialized agents distributed to execute these distinct tasks.</p>
<p>Single-agent planners integrate all core functions-task planning, reflection, memory access, and tool use-into a single unified module.This monolithic design simplifies system architecture and debugging, making it well-suited for applications where the scope of tasks is limited, or tight integration between components is essential.In early scientific agent systems, such as Coscientist (Boiko et al., 2023) and ChemCrow (Bran et al., 2024), a single LLM-based planner would orchestrate all operations, providing a streamlined approach that is easier to implement and manage.However, this simplicity can become a bottleneck when addressing complex scientific problems that require specialized subtasks to be executed concurrently or with varying degrees of autonomy.</p>
<p>In contrast, multi-agent planners, exemplified by recent developments like Google's AI co-scientist (Gottweis et al., 2025), distribute these responsibilities among specialized agents (see Appendix A for detailed introduction).In such architectures, distinct agents may be assigned to generate hypotheses, perform critical reflection, rank and refine ideas, or even manage meta-reviews.This division of labor enables a "generate, debate, and evolve" framework where each agent focuses on a specific function, enhancing overall system flex- ibility and scalability.Empirical results from the AI co-scientist framework indicate that this modular approach can significantly accelerate discovery processes-for example, by reducing hypothesis generation timelines from weeks to days-and improve the novelty and accuracy of research outputs.On the one hand, the increased complexity of multiagent systems demands robust communication protocols and coordination strategies to manage potential inter-agent conflicts and ensure coherent output.On the other hand, they offer enhanced performance in tackling multifaceted, interdisciplinary challenges typical in scientific research.</p>
<p>In summary, the choice between single-agent and multi-agent planners depends largely on the task complexity, the need for specialization, and the desired scalability of the system.For routine or narrowly defined problems, single-agent planners may suffice, whereas multi-agent planners are better suited for advanced scientific discovery where dynamic, specialized collaboration is key.</p>
<p>Memory</p>
<p>Memory in LLM-based scientific agents extends beyond simple context retention, enabling longterm accumulation of research findings, iterative hypothesis refinement, and cross-project continuity.By mirroring the cognitive processes of human scientists, these agents maintain detailed historical context, integrate domain-specific external knowl-edge, and leverage intrinsic model capabilities to ensure that each experiment or literature insight informs future decisions.We categorize these memory mechanisms into Historical Context, External Knowledge Base, and Intrinsic Knowledge-three facets that collectively address the timeline-driven nature of scientific inquiry, the breadth of specialized data sources, and the deep, model-level understanding required for advanced tasks.While not mutually exclusive, each category highlights a distinct dimension of how scientific agents store and utilize information to reproduce results, accumulate evidence, and push the boundaries of autonomous research.We compare the three mechanisms in Table 3, and list the related studies in Figure 4.</p>
<p>Historical Context</p>
<p>Historical context-often termed conversational or short-term memory-is vital for scientific agents to maintain continuity and iterative progress in research workflows.Unlike general agents that merely hold transient dialogue, scientific agents accumulate and leverage past interactions, experimental outcomes, and reasoning steps to refine hypotheses and improve experiment designs over time.This robust memory enables them to mimic the cumulative nature of scientific inquiry, ensuring each cycle of analysis builds on previous insights and supports reproducible results.Figure 5     refinement process in scientific agents.</p>
<p>Several frameworks highlight the importance of historical context, albeit with varying implementations.For instance, AI Scientist (Lu et al., 2024a) utilizes historical context by iteratively developing ideas and adding them to a growing archive, mimicking the cumulative knowledge building in the scientific community.Similarly, MedAgents (Tang et al., 2023) emphasizes iterative discussions to reach consensus, where the progression of arguments within the dialogue itself forms the historical context.Similarly, MLR-Copilot (Li et al., 2024c) and BioDiscoveryAgent (Roohani et al., 2024) utilize feedback from previous rounds' experiments to refine their subsequent steps, directly incorporating past results into the ongoing process.AtomAgents (Ghafarollahi and Buehler, 2024a) provides a more structured approach by defining dedicated "core memory" and "tool memory" modules to store conversations between agents and tool interactions, ensuring readily accessible historical data throughout problem-solving.AIGS (Liu et al., 2024c) further illustrates this with its "Pre-Falsification" phase that relies on multi-turn logs of iterative exchanges between agents, explicitly using these logs as the history context to refine proposals.Recent works further highlight sophisticated uses of history context.LLMatDesign (Jia et al., 2024) incorporates "self-reflection" on previous decisions, allowing the agent to rapidly adapt to new tasks and conditions in a zero-shot manner.The metaAgent (Hu et al., 2025), an embodied intelligent agent for electromagnetic space, leverages a "multi-agent discussion mechanism" in its "cerebrum" part.This mechanism inherently relies on the conversations and interactions between specialized agents.</p>
<p>Despite varied implementations, historical context is essential for maintaining a continuous record of interactions.It enables agents to iteratively refine their approaches based on past successes, failures, and external inputs.Whether through explicit memory modules or implicit conversational flow, this iterative process guides future actions, ensuring dynamic adaptation and coherent execution in the scientific discovery process.</p>
<p>External Knowledge Base: Augmenting</p>
<p>Agent Capabilities with Broad Scientific Knowledge</p>
<p>External knowledge bases (KBs) are essential for scientific agents, providing a curated repository of up-to-date, domain-specific information that ex-tends beyond the static training data of LLMs.These KBs are not merely supplemental-they are deeply integrated into the agent's reasoning process, enabling it to retrieve, synthesize, and connect complex scientific concepts.This external integration is critical for tasks that demand indepth domain expertise and comprehensive literature awareness.By systematically incorporating external knowledge, scientific agents can enhance hypothesis generation, experimental design, and data analysis, ensuring that their outputs remain current, robust, and contextually relevant.Figure 6 illustrates this process.</p>
<p>A prominent approach remains leveraging scientific literature as an external KB.ProtAgents (Ghafarollahi and Buehler, 2024b) and Chemist-X (Chen et al.) both employ Retrieval-Augmented Generation (Lewis et al., 2020) with literature databases, allowing agents to ground their reasoning in existing research.ResearchAgent (Baek et al., 2024) takes a more structured approach, building an "entity-centric knowledge store" from literature co-occurrences to capture underlying relationships and facilitate cross-pollination of ideas.Agent Laboratory (Schmidgall et al., 2025) illustrates utilization of literature through the arXiv API, enabling agents to retrieve, summarize, and generate papers.</p>
<p>Beyond literature, knowledge graphs (KGs) emerge as another significant type of external KB.SciAgents (Ghafarollahi and Buehler, 2024c) explicitly uses large-scale ontological KGs to organize scientific concepts, ensuring generated scientific hypotheses are rooted in interconnected scientific concepts.DrugAgent (Inoue et al., 2024) uses a Knowledge Graph Agent to extract drug-target interaction information, demonstrating the use of targeted KGs for specific domains.By adopting the KnowledgeBank module from AgentScope (Gao et al., 2024b), VirSci (Su et al., 2024) makes scientist agents' profiles embed into the author knowledge bank, through which agents can quickly access and familiarize themselves with other initialized agents' information.</p>
<p>Expanding beyond traditional literature and KGs, Coscientist (Boiko et al., 2023) demonstrates the power of integrating diverse resources as external KBs.It mainly intergrates Web Searcher module and Documentation search module to enable the agent browse the internet and relevant documentation for experiments in next period.In geospatial domain, an autonomous geospatial data re- trieval framework (Ning et al., 2025) manages a pre-defined and scalable list of data sources like OpenStreetMap and US Census data, highlighting a more curated external data approach for specific tasks.In Astronomical domain, mephisto (Sun et al., 2024b) adopts a dynamically updated external knowledge base through a learning system where domain knowledge is extracted and then validated, suggesting a form of continuously learning and evolving external knowledge.</p>
<p>These examples demonstrate a clear trend: scientific agents are significantly enhanced by access to diverse external KBs.The KB type varies greatly based on the task, ranging from broad scientific literature and web resources to specialized KGs, curated datasets, etc.The functional overlap across the approaches lies in their ability to provide agents with access to a wider and more current scope of information than their intrinsic knowledge.Complementarily, external KBs ground agent reasoning in established scientific knowledge, promote validity and novelty by referencing existing works and data, and enable interaction with real-world tools and information sources.Implementation-wise, we see a spectrum from RAG-based retrieval from unstructured text to direct querying of structured KGs and databases, API integrations, and web browsing, reflecting increasingly sophisticated strategies for knowledge integration and utilization to empower scientific agents.</p>
<p>Intrinsic Knowledge: Leveraging</p>
<p>Pre-trained and Fine-tuned LMs</p>
<p>In the context of scientific agents powered by Large Language Models (LLMs), intrinsic knowledge of LLMs serves as the foundational cognitive bedrock.This refers to the inherent capabilities and infor-mation that the LLM itself embodies, meticulously cultivated during its pre-training phase on massive and diverse corpora, crucially including scientific literature, datasets, and domain-specific knowledge.This intrinsic knowledge is further refined through task-specific fine-tuning.For a scientific agent, this isn't merely passive data storage; it's the very source of an agent's reasoning faculties, natural language competency, and fundamentally, its foundational scientific understanding.The intrinsic knowledge, therefore, empowers a scientific agent to operate effectively within scientific contexts, providing the essential base for scientific reasoning, comprehension of scientific language, and the broad scientific literacy required to function as an autonomous scientific explorer and problem-solver.</p>
<p>Several studies emphasize the importance of enhancing the intrinsic knowledge of LLMs for scientific agents through specialized training.ChemDFM (Zhao et al., 2024b) pioneers this approach by developing a domain-specific LLM pretrained on a massive chemical literature and textbook corpus and further fine-tuned with chemical instructions.This directly injects chemical expertise into the model's core knowledge.Matchat (Chen et al., 2023) takes a fine-tuning route, enhancing Llama2-7B with structured material knowledge data, demonstrating the efficacy of incorporating domain-specific structured information to improve model performance in materials science.PaSa (He et al., 2025) focuses on academic paper search, utilizing reinforcement learning with a synthetic dataset of academic queries and papers to optimize an LLM agent for search task.</p>
<p>Building upon these strategies, recent works further explore diverse avenues for enriching intrinsic knowledge.ProLLaMA (Lv et al., 2024) introduces efficiency into the fine-tuning process for Protein Language Models by employing Low-Rank Adaptation (Hu et al., 2022).This method improves the efficiency of protein learning during fine-tuning, demonstrating advancements in making specialized model training more resourceeffective.Moreover, Tx-LLM (Chaves et al., 2024) presents a generalist large language model for therapeutics, fine-tuned from PaLM-2 (Anil et al., 2023).Tx-LLM distinguishes itself by being trained on an extensive collection of 709 datasets, encompassing 66 tasks across the drug discovery pipeline.In contrast to single domain fine-tuning, NatureLM (Xia et al., 2025) adopts a multi-domain pre-training approach.Pre-trained on data from multiple scientific domains that include small molecules, materials, proteins, DNA and RNA, NatureLM aims to offer a unified and versatile model applicable across various scientific applications.</p>
<p>These examples highlight a critical strategy : tailoring the LLM's intrinsic knowledge to the specified scientific domain.The functional overlap is clear -all approaches aim to improve the LLM's base capabilities for scientific reasoning and task execution within specific fields, whether it be chemistry, materials science, academic search, protein science, therapeutics, or broadly across multiple scientific disciplines.Complementarily, intrinsic knowledge provides the bedrock for the agent's intelligence, enabling it to effectively process historical context and utilize external knowledge.Implementation approaches differ significantly, ranging from full domain-specific pre-training (ChemDFM, NatureLM) to targeted fine-tuning with structured data (Matchat, Tx-LLM) or reinforcement learning (PaSa), and including techniques for efficient fine-tuning (ProLLaMA).These diverse techniques underscore the importance of carefully shaping the LLM's intrinsic knowledge and demonstrate the expanding LLMs available for researchers to create scientifically intelligent agents.</p>
<p>Discussion</p>
<p>Memory in LLM-based scientific agents is implemented via three interrelated mechanisms: history context, external knowledge bases, and intrinsic knowledge.History context enables agents to maintain conversational coherence and iterative refinement by retaining and recalling prior interactions, emulating the cumulative nature of human research.External knowledge bases expand the agent's informational scope by integrating up-to-date and domain-specific data, allowing for the retrieval, synthesis, and contextualization of complex scientific concepts.Meanwhile, intrinsic knowledge enables agents to apply core scientific reasoning from the outset, serving as the bedrock for advanced, context-rich memory layers.</p>
<p>Despite their complementary roles, current memory mechanisms face several limitations.Many approaches-especially those using textual memorysuffer from scalability issues and information loss since context windows are limited.Parametric methods, while more efficient, often lack interpretability and require extensive fine-tuning.Moreover, external knowledge integration remains brittle in dynamically changing domains, leading to po-tential mismatches or outdated retrievals.Recent studies (Xu et al., 2025;Zeng et al., 2024) emphasize the need for more adaptive, self-organizing memory systems that can dynamically link and update stored information.</p>
<p>Future research should focus on developing hybrid memory models that combine the benefits of both parametric and textual representations, or exploring the synergistic relationships between different memory types and investigate novel hybrid approaches to optimize their collective performance in automated scientific discovery (e.g., GeneGPT (Jin et al., 2024b), AmadeusGPT (Ye et al., 2023b), FoodPuzzle (Huang et al., 2024c)).Further, integrating robust metadata learning and external knowledge graphs-as explored in recent works like Hatalis et al. (2023)-could enhance retrieval accuracy and contextual grounding.Additionally, improved lifelong learning techniques and efficient forgetting mechanisms are essential to mitigate memory overload and maintain performance over extended research cycles.</p>
<p>Tool Set</p>
<p>In this section, we introduce the tool sets employed by scientific agents, as illustrated in Figure 7.While LLMs demonstrate robust problem-solving capabilities for general tasks and foundational scientific inquiries, they often encounter limitations when addressing advanced scientific challenges, particularly those in STEM-related domains, due to insufficient domain-specific expertise and computational resources.The tool set extends the LLM's capabilities beyond natural language processing by enabling real-time data retrieval, precise code execution, domain-specific scientific computation, and rigorous experimental simulation.This tight integration allows scientific agents to access accurate, up-to-date information, perform computationally intensive analyses, and process data in specialized modalities-capabilities that are essential for simulating and validating experiments.Consequently, these tool sets serve not just as supplementary resources but as a core component of the agent's architecture, fundamentally enhancing its scientific reasoning, reliability, and adaptability in complex research environments.</p>
<p>Based on the functional types of tools, we categorize existing scientific agents tool sets into two categories: (1) Tool sets based on APIs and code libraries, and (2) Tool sets based on simulators or emulation platforms.The subsequent section will present recent advancements in each category.</p>
<p>Tool sets based on APIs and code libraries</p>
<p>Tool sets based on APIs and code libraries aim to extend the knowledge boundaries and computational capacities of LLMs in scientific tasks.These tool sets encapsulate domain-specific knowledge bases and specialized algorithm libraries into standardized functional interfaces, thus enabling LLMs to transcend the limitations imposed by the timeliness and domain depth of their training data, as well as computational limitations inherent in LLMs.This category encompasses both preexisting general-purpose tools, discipline-specific scientific tools, and novel tools synthesized by researchers using generative methods.Simpler tool sets encompass basic components such as search engines or database query modules.For instance, MAPI-LLM (Jablonka et al., 2023) leverages LLMs to retrieve information from the Materials Project API (MAPI) Reaction-Network package and Google Search, addressing user queries about chemical materials.The LLM employs Chain-of-Thought prompting to translate natural language queries into structured API calls, allowing users to retrieve material properties and execute complex searches through conversational interfaces.Similarly, ClimateGPT (Thulke et al., 2024) integrates a retrieval mechanism to access a curated collection of climatological research reports and peer-reviewed papers, thereby enhancing response accuracy in climate science applications.</p>
<p>While the aforementioned examples focus on elementary query tools, the following case demonstrates the integration of sophisticated and multifunctional APIs, which significantly augment the capabilities of scientific agents in handling diverse scientific tasks.These advanced tool sets not only grant access to domain-specific data repositories but also enable intricate computational workflows and analytical operations, thereby pushing the boundaries of LLMs in scientific reasoning.</p>
<p>Mathematics is frequently regarded as the foundational discipline underpinning numerous scientific domains.Tora (Gou et al., 2024) integrates Python libraries such as SymPy, SciPy, and CVXPY into natural language reasoning frameworks, demonstrating significant performance improvements for open-source LLMs across multiple mathematical reasoning benchmarks.In the disciplines of chemistry and materials science, Chem-  (Thulke et al., 2024), etc.</p>
<p>Multifunctional APIs</p>
<p>Tora (Gou et al., 2024), SciAgent (Ma et al., 2024b), ChemCrow (Bran et al., 2024), CACTUS (McNaughton et al., 2024), HoneyComb (Zhang et al., 2024a), CRISPR-GPT (Huang et al., 2024a), etc.</p>
<p>Tool Sets Based on Simulators and Emulation Platforms Mind's Eye (Liu et al., 2023), Ma et al. (2024a), MyCrunchGPT (Kumar et al., 2023), DockingGA (Gao et al., 2024a), ClimSight (Koldunov and Jung, 2024), etc. Crow (Bran et al., 2024) deploys an extensive tool set comprising 18 expert-designed tools that support functions such as molecular property queries, reaction prediction, and experimental synthesis planning.The integration of these tools empowers LLMs to autonomously design and execute complex chemical workflows in organic synthesis, drug discovery, and materials design.Similarly, CAC-TUS (McNaughton et al., 2024) enhances cheminformatics capabilities by integrating tools from open-source packages such as RDKit, enabling quantitative estimation of drug-likeness and pharmacokinetic properties for chemical compounds provided in SMILES.The HoneyComb (Zhang et al., 2024a) framework constitutes a comprehensive materials science framework, integrating MatSciKB (Materials Science Knowledge Base) with a ToolHub.MatSciKB gathers structured knowledge from peer-reviewed literature, while the ToolHub incorporates search engines, Python interpreters, and domain-specific APIs constructed via an inductive tool construction methodology.In the domain of biology, CRISPR-GPT (Huang et al., 2024a) synergizes Google Search, Primer3, Broad Institute's gold-standard guideRNA library, the CRISPRPick tool set, and scholarly databases.This integration enables researchers to select the most suitable CRISPR systems and to design experimental protocols for genome-editing workflows.SciAgent (Ma et al., 2024b) introduces a methodology to generalize LLMs' mathematical tool utilization to other scientific domains.Researchers initially generated a mathematical tool set via a cross-retrieval strategy, subsequently developing a human-validated and refined multi-domain tool set based on the SciToolBench dataset.This com-prehensive tool set encompasses five disciplines: Mathematics, Physics, Chemistry, Finance, Electrical Engineering, and Computer Science.</p>
<p>The integration of these tool sets into scientific agent systems has enabled researchers to enhance LLMs' capacities in experimental planning and numerical prediction within many scientific domains.This modular, extensible integration strategy has proven effective in mitigating LLMs' inherent limitations in domain expertise and computational precision.Nevertheless, challenges such as nonstandardized interfaces, limited tool diversity, and the complexity of tool generation are currently hindering the broader adoption of such tool sets.</p>
<p>Tool sets based on simulators and emulation platforms</p>
<p>Tool sets based on simulators and emulation platforms provide specialized, domain-specific tools for scientific agents, enabling them to simulate experimental procedures and validate results.By translating natural language instructions into executable simulation codes or parameterized control signals using LLMs, these tool sets facilitate deep integration with experiment workflows.Often, they are tightly coupled with the planning process, ensuring correct parameterization and validation throughout simulations or lab automation steps, and especially valuable in complex research tasks.At present, this class of tool sets is employed most frequently in physics-related scientific agents.Mind's Eye (Liu et al., 2023) employs the MuJoCo physics engine to simulate real-world physical scenarios.The language model converts natural language text into rendering codes, with simulation results iteratively incorporated into subsequent inputs, thereby facilitating physics-based reasoning.</p>
<p>Similarly, Ma et al. (2024a) utilize physics simulators as experimental platforms on which LLMs generate scientific hypotheses and perform reasoning.The simulator provides observational feedback and enables differentiable optimization of continuous parameters, thereby achieving validated results in constitutive law discovery and molecular design tasks.MyCrunchGPT (Kumar et al., 2023) integrates a suite of software components, including DeepONet surrogates and the computational fluid dynamics (CFD) simulator Nektar++ to optimize 2D NACA airfoils in aerodynamic design.The LLM employs DeepONet for flow field computations during the process of airfoil optimization, and the validity of the results is confirmed through highfidelity simulations.In the domain of chemistry, DockingGA (Gao et al., 2024a) utilizes molecular docking simulations to facilitate the generation of molecules that exhibit target-specific binding affinities.The docking scores between the generated molecules and biological targets function as reward signals, thereby enabling the refinement of molecular synthesis.In the context of climatology, ClimSight (Koldunov and Jung, 2024) integrates geospatial databases and the AWI Climate Model, a global climatology simulation framework, to assess the climate impacts of specific activities, such as agricultural practices.</p>
<p>The integration of simulation tool sets is a solution to the limitations of LLMs in understanding physical laws and reasoning about dynamic processes.This enhances computational accuracy and validity for complex problems.However, the practical adoption of such tool sets remains constrained by the high computational costs and temporal overheads of precision simulators.In addition, the proficient utilization of simulators and the accurate generation of their corresponding parameters also pose significant challenges for LLMs.</p>
<p>Discussion</p>
<p>Recent studies have shown that the incorporation of scientific tool sets into agent systems leads to substantial enhancements in LLMs' planning, reasoning, computational, and execution capabilities for scientific tasks.Tool sets based on APIs and code libraries address limitations in domain knowledge and computational power by encapsulating specialized algorithms and knowledge bases.This separation allows scientific agents to decouple highlevel reasoning from raw numerical operations, enabling them to prioritize strategic planning and or-chestrate complex tool usage.Simultaneously, tool sets based on simulator and emulation platforms integrate experimental simulations with natural language reasoning, augmenting the agents' ability to manage and solve intricate, multi-step scientific workflows with improved precision and reliability.</p>
<p>However, several limitations persist in current tool integration studies from the scientific perspective.Many systems still rely on pre-defined tool sets and static, well-documented repositories, which restrict scalability and adaptability in dynamic research environments.For example, benchmarks like ShortcutsBench (Shen et al., 2025) reveal that even state-of-the-art systems struggle with managing API dependencies and adapting to frequently updated external services-challenges that are particularly acute in rapidly evolving scientific domains such as computational biology and materials science.In addition, high subscription costs for some API services, along with persistent challenges in error handling, security, and reproducibility, continue to impede the deployment of robust LLM-based agents in rigorous scientific research.</p>
<p>Looking ahead, future research must develop autonomous, self-adaptive tool generation frameworks that leverage middleware layers to seamlessly integrate diverse functionalities at runtime.Promising strategies, as highlighted in recent works such as Shen et al. (2025); Gu et al. (2024), suggest that dynamic middleware-based solutions can adapt to real-time changes in scientific environments.Moreover, standardizing API design and documentation, enhancing automated error detection and recovery mechanisms, and creating comprehensive, dynamic benchmarks tailored for scientific applications will be pivotal.</p>
<p>General-purpose vs. Scientific Agent</p>
<p>The above section shows the module design for scientific agents.Different from scientific agents (e.g., AI Co-Scientist (Gottweis et al., 2025)) that specialize in research workflows, general-purpose agents (e.g., Manus (Manusai.ai, 2025)) are designed for broad adaptability across user-defined tasks.While they may share foundational LLM technology, their planning, memory strategies, tool integrations, and reasoning approaches differ significantly.This section outlines key technical distinctions that necessitate dedicated scientific agent design.no single agent system has yet achieved all the features in the table, but this is their trend, due to the high logic, structured content, long-term retention, professionalism, low tolerance for error, and reproducibility of the scientific field.</p>
<p>Planning and Task Management</p>
<p>General-purpose agents often use heuristic or reactive planning approaches (e.g., ReAct (Yao et al., 2023), plan-then-execute (Zhang et al., 2025)), adjusting actions based on intermediate results to maintain flexibility.Although multi-agent designs like Manus (Manusai.ai, 2025) allow broader task delegation, they generally lack formal structure that enforces scientific methodology over long, complex research phases.</p>
<p>Scientific agents, by contrast, implement structured, hierarchical planning aligned with the scientific method (Schmidgall et al., 2025;Gottweis et al., 2025).Systems such as BioPlanner (O'Donoghue et al., 2023) systematically translate scientific goals into reproducible protocols, and AI Co-Scientist (Gottweis et al., 2025) uses parallel multi-agent planning to handle literature review, hypothesis generation, and ranking in tandem.This logical framework ensures that each step-hypothesis, experiment, analysis-proceeds in a coherent, method-driven sequence.</p>
<p>Memory and Knowledge Integration</p>
<p>General-purpose agents typically rely on ephemeral memory, constrained by context windows or retrieval-augmented generation (RAG) (Park et al., 2023).Tools like AutoGPT (Yang et al., 2023) may store short-term notes (scratchpads), but they rarely support long-term accumulation of information.As a result, knowledge retention is ad-hoc, depending on frequent web queries instead of persistent internal structures.</p>
<p>Scientific agents emphasize persistent memory that evolves throughout extended research projects.AI Co-Scientist (Gottweis et al., 2025) maintains a shared memory store of intermediate results, accessible to specialized sub-agents for a coherent, team-like workflow.LLaMP (Chiang et al., 2024) and Agent Laboratory (Schmidgall et al., 2025) integrate structured domain databases, enabling cumulative knowledge across multiple projects.This large-scale retention preserves experimental histories, fosters reproducibility, and supports the long timelines inherent in scientific investigation (see also Lu et al. (2024a)).</p>
<p>Tool Utilization and Integration</p>
<p>General-purpose agents usually adopt a pluginbased model (e.g., Toolformer (Schick et al., 2023), HuggingGPT (Shen et al., 2024a)), calling APIs like web search or Python execution as needed.These integrations are generic, supporting various tasks but lacking specialized simulation or experiment workflows.</p>
<p>Scientific agents, on the other hand, require deeply integrated tools for simulations, experiment orchestration, and data analysis.For example, specialized modules in ProtAgents (Ghafarollahi and Buehler, 2024b) handle complex computational biology tasks, while chemistry-focused frameworks (e.g., ChemCrow (Bran et al., 2024)) support reaction prediction and laboratory automation.Crucially, these tools are not just invoked-they are part of the scientific planning loop, ensuring that parameters, methods, and validations conform to domain standards.</p>
<p>Benchmarks General Reasoning Ability Evaluation</p>
<p>K-12 Foundational Skills</p>
<p>Geometry3K (Lu et al., 2021), GeoEval (Zhang et al., 2024b), VisScience (Jiang et al., 2024), MathVista (Lu et al., 2024b), etc.</p>
<p>Higher Education Level</p>
<p>SciBench (Wang et al., 2024d), SciEval (Sun et al., 2024a), Su-perGPQA (Team et al., 2025), etc.</p>
<p>Expert-Level HLE (Phan et al., 2025), etc.</p>
<p>Scientific Research-Oriented Ability Evaluation</p>
<p>Scientific Paper Chart Comprehension</p>
<p>FigureQA (Kahou et al., 2018), ArXivQA (Li et al., 2024a), MMSCI (Li et al., 2024e), etc.</p>
<p>Scientific Hypothesis Discovery</p>
<p>SciMON (Wang et al., 2024c), MOOSE-Chem (Yang et al., 2024b), DiscoveryBench (Majumder et al., 2024), DiscoveryWorld (Jansen et al., 2024), etc.</p>
<p>Experimental Design and Automation</p>
<p>GAIA (Mialon et al., 2023), DiscoveryWorld (Jansen et al., 2024), DSBench (Jing et al., 2024), ScienceAgentBench (Chen et al., 2024), SciCode (Tian et al., 2024), TaskBench (Shen et al., 2024b), MLAgentBench (Huang et al., 2024b), LAB-Bench (Laurent et al., 2024), etc.</p>
<p>Domain-Specific Reasoning and Collaboration</p>
<p>General-purpose agents focus on achieving single-user goals, often lacking built-in verification.Self-reflection may occur, but thorough validation (e.g., statistical checks, error bounds) is not typically included.Scientific agents implement validation and reproducibility measures to ensure robust outputs.Multi-agent debate (Su et al., 2024) allows for hypothesis refinement via critical discussion, and AI Co-Scientist (Gottweis et al., 2025) employs parallel hypothesis evaluation, discarding flawed ideas early.By incorporating statistical analyses, error checking, and domain-specific constraints, scientific agents uphold reliability and reproducibility-core requirements of scientific research.</p>
<p>Benchmark</p>
<p>Benchmarks are basic solutions for evaluating the efficacy of LLM-based scientific agents, ensuring their capability to handle the multifaceted demands of scientific research.They are designed to measure various aspects of these agents' performance, from basic problem-solving, such as fundamental cognitive and analytical skills, to complex scientific research, such as some research-oriented paper reading and experiment designing abilities.In this section, we classify the evaluation benchmarks into two categories: general reasoning ability and domain-specific scientific capability, as shown in Figure 8.</p>
<p>General Reasoning Ability Evaluation</p>
<p>General reasoning ability evaluation focuses on assessing the fundamental cognitive and analytical skills of LLM-based scientific agents.These benchmarks measure problem-solving capabilities in mathematical reasoning, logical inference, and domain-specific knowledge retrieval, ensuring that agents can perform essential tasks required for scientific research and higher education.By evaluating models across different levels, from K-12 foundational skills to higher education and expert-level assessments, these benchmarks provide insights into the reasoning proficiency and adaptability of LLMs in various academic disciplines.We summarize the available benchmarks in   geometry), algebraic operations, logical reasoning, and basic statistical analysis.Benchmarks like Ge-ometry3K (Lu et al., 2021) and GeoEval (Zhang et al., 2024b) assess geometric reasoning, while MathVista (Lu et al., 2024b) is used for algebra and statistical tasks intertwined with visual understanding.Meanwhile, VisScience (Jiang et al., 2024) broaden this focus by integrating visual reasoning tasks within mathematics, physics, and chemistry contexts.These test agents' abilities to solve geometric problems, understand algebraic concepts, and make statistical inferences-critical skills for advancing to higher levels of scientific reasoning.</p>
<p>Higher Education Level: As agents progress, they must handle more advanced tasks such as scientific computing, retrieval of domain-specific knowledge, and application of this knowledge to solve complex scientific problems.Key benchmarks include SciBench (Wang et al., 2024d) and SciEval (Sun et al., 2024a).These datasets evaluate how well agents engage in advanced scientific tasks such as solving problems in physics, chemistry, and biology, along with retrieving and applying knowledge from scientific literature.Such benchmarks reflect the complexities of real-world research in academic and professional settings.Beyond traditional STEM disciplines, SuperGPQA (Team et al., 2025) introduces a broader evaluation framework, covering 285 specialized academic fields, including light industry, agriculture, and service-oriented disciplines.This benchmark underscores the need for advancements in LLM reasoning across diverse knowledge domains and provides valuable insights into large-scale expert-driven dataset construction.</p>
<p>Humanity's Last Exam (HLE):</p>
<p>In response to the saturation of existing benchmarks, Humanity's Last Exam (HLE) (Phan et al., 2025) has been introduced as a more challenging measure of LLM capabilities.It consists of 3000 rigorous questions across a wide range of disciplines, including mathematics, humanities, and natural sciences.Unlike traditional benchmarks, the questions in HLE are designed to be extremely difficult and unsearchable through basic internet retrieval, making it a critical test for evaluating the limits of current LLM performance.The benchmark highlights a significant gap between the capabilities of state-of-the-art LLMs and expert-level knowledge in closed-ended academic tasks.Low accuracy scores (less than 10%) across multiple frontier models emphasize the need for further advancements in agent abilities.</p>
<p>Scientific Research-Oriented Ability Evaluation</p>
<p>While general reasoning benchmarks assess broad problem-solving and analytical skills, scientific research-oriented benchmarks evaluate the ability of LLM-based scientific agents to perform specialized scientific tasks.These include extracting and interpreting data from research papers, discovering novel scientific hypotheses, and designing and automating experimental procedures.By simulating real-world scientific workflows, these benchmarks help measure the extent to which LLMs can function as effective tools for scientific discovery and innovation.Table 6 presents a categorized summary of these benchmarks.Scientific Paper Chart Comprehension: Understanding and interpreting data visualizations in scientific papers is a fundamental skill for agents in research environments.Benchmarks such as Fig-ureQA (Kahou et al., 2018), ArXivQA (Li et al., 2024a) and MMSCI (Li et al., 2024e) test agents' ability to comprehend and reason over figures, including graphs, charts, and tables, from scientific papers.Those are essential for tasks such as literature reviews, where agents need to extract and comprehend information from graphical data.</p>
<p>Scientific Hypothesis Discovery: A critical task in scientific research is the generation of novel hypotheses from existing literature or experimental data.Datasets like SciMON (Wang et al., 2024c) and MOOSE-Chem (Yang et al., 2024b) focus on deriving new scientific discoveries by analyzing key sections of existing literature, such as abstracts and methodologies.In contrast, Discov-eryBench (Majumder et al., 2024) and Discovery-World (Jansen et al., 2024) emphasize the exploration of novel findings based on experimental data.These benchmarks collectively challenge agents to extract meaningful insights from both textual sources and empirical observations, evaluating their ability to generate and refine scientific hypotheses.Such capabilities are essential for driving forward scientific innovation.</p>
<p>Experimental Design and Automation: The ability to design experiments, decompose complex tasks, and automate scientific workflows is critical for LLM-based scientific agents.Discovery-World (Jansen et al., 2024), DSBench (Jing et al., 2024) and ScienceAgentBench (Chen et al., 2024) assess agents' capabilities in hypothesis-driven and data-driven experimental design, focusing on scientific discovery and real-world data science tasks.Meanwhile, SciCode (Tian et al., 2024) focuses on problem-solving through code generation for domain-specific scientific challenges.For workflow automation, GAIA (Mialon et al., 2023), TaskBench (Shen et al., 2024b) and MLAgent-Bench (Huang et al., 2024b) evaluate an agent's ability to structure tasks, iterate on models, and optimize performance in general scenarios.In biological research, LAB-Bench (Laurent et al., 2024) tests protocol planning, data analysis, and experiment troubleshooting.</p>
<p>Discussion</p>
<p>The above benchmarks provide a robust framework for evaluating LLM-based scientific agents, addressing a wide range of scientific skills across different stages of research and development.These benchmarks enable comprehensive assessments, from foundational reasoning skills to advanced scientific hypothesis generation and experimental automation, making them critical for guiding the future development of scientific AI systems.</p>
<p>Despite these advances, several limitations remain.First, current benchmarks often rely on static datasets and pre-defined tasks that may not fully capture the dynamic and iterative nature of realworld scientific research.Many evaluations focus on end-to-end performance, thereby obscuring the nuanced failures occurring at individual steps of scientific reasoning and decision-making.Additionally, the diversity of scientific domains-from biomedical research to materials science-presents challenges in standardizing evaluation metrics that can fairly compare agents across different fields.</p>
<p>Future research should focus on developing adaptive and continuously updated benchmarks that mimic authentic scientific workflows.For example, dynamic benchmarks could integrate multi-turn interactions where agents iteratively refine hypotheses based on experimental feedback, akin to real laboratory processes.Establishing domain-specific evaluation metrics and expanding benchmarks to include cross-disciplinary tasks will be critical for assessing the potential of scientific agents.</p>
<p>Applications</p>
<p>LLM-based scientific agents have significantly advanced scientific research, automating complex tasks and enhancing the efficiency of discovery processes across various disciplines.</p>
<p>Scientific research is an arduous process involving numerous steps, including hypothesis formulation, experimental design, planning, and data analysis and evaluation.These processes are typically labor-intensive and costly, and thus, they are often conducted by human scientists who possess specialized expertise and substantial capital investment.However, the emergence of scientific agents has revolutionized research efficiency.By automating multiple stages that previously required manual intervention, these computational systems achieve optimal equilibrium in resource utilization.This enhancement in automation not only increases operational efficiency throughout the scientific workflow but also reduces the barriers to entry for conducting rigorous scientific investigations.</p>
<p>Below is a concise exploration of the applications of LLM-based scientific agents, categorized by their specific domains and functionalities, as illustrated in Figure 9.</p>
<p>Chemistry and Materials Science</p>
<p>LLM-based agents have transformed chemistry and materials science by automating tasks such as molecular design, property prediction, and reac-tion optimization.For example, Chemist-X (Chen et al.) is an AI agent that automates reaction condition recommendations in chemical synthesis using Retrieve-Augmented Generation (RAG) techniques and CAD tools, surpassing traditional synthesis AIs in performance.Similarly, Coscientist (Boiko et al., 2023) combines LLMs to autonomously plan, design, and execute scientific experiments, demonstrating its capabilities through successful catalyzed chemical reactions while addressing safety concerns and proposing misuse prevention measures.Additionally, ChemCrow (Bran et al., 2024), integrating 18 expert tools, autonomously executes complex chemical tasks, enhancing performance in organic synthesis, drug discovery, and materials design, fostering scientific advancement.In the field of materials science, HoneyComb (Zhang et al., 2024a) has been shown to achieve superior performance in multiple tasks through a researcher-constructed knowledge base and generated API library.Besides, A-Lab (Szymanski et al., 2023) leverages LLM-based models, robotics, and active learning to mine literature and optimize synthesis pathways for novel inorganic materials, integrating computational predictions with automated experimentation and accelerating materials discovery.</p>
<p>By accomplishing three key tasks-property prediction, property-directed inverse design, and synthesis prediction-the scientific agents establish full-process automation throughout the molecular discovery pipeline (Ramos et al., 2025).This innovation significantly streamlines molecular design workflows and advances development in chemistry and materials science through the systematic implementation of computational methodologies.</p>
<p>Biology and Medicine</p>
<p>In the biomedical sector, LLM agents advance protein design, automate scientific discovery, enhance genetic research, and improve healthcare analysis.For example, the ProtAgents (Ghafarollahi and Buehler, 2024b), a platform for de novo protein design using LLMs, leverages dynamic AI agents to collaboratively tackle protein design, structure analysis, and simulations.TAIS (Liu et al., 2024a) automates scientific discovery by streamlining data selection, processing, and analysis, advancing gene identification and efficiency in research, aiming to identify disease-predictive genes from gene expression data.CRISPR-GPT (Huang et al., 2024a) combines the reasoning ability of LLMs with external tools to automate CRISPR-based gene-editing experiments.The system further designs validation experiments based on experimental outcomes, thereby reducing the barriers to entry in this field by enabling novices to efficiently implement complex workflows.Furthermore, BioDiscoveryAgent (Roohani et al., 2024) leverages LLMs to autonomously design genetic perturbation experiments, improving prediction accuracy and efficiency, and outperforms traditional methods in identifying genes linked to specific phenotypes.Additionally, in the medical field, AgentMD (Jin et al., 2024a), a language agent augmented with 2,164 clinical calculators, curates and applies relevant tools to improve healthcare analysis, significantly improving risk prediction accuracy and clinical workflows.AI co-scientist (Gottweis et al., 2025), constructed upon Gemini 2.0, employs a multi-agent system that utilizes tournament-based evolutionary processes with self-optimizing mechanisms to synthesize existing research, formulate hypotheses, and propose experimental methodologies.It has demonstrated empirically validated effectiveness in pharmaceutical repurposing, target discovery, and antimicrobial resistance research through rigorous experimental verification.</p>
<p>Scientific agents demonstrate extensive and multi-faceted applications across the domains of genetics, cell biology, and chemical biology.In studies investigating the relationship between DNA and human traits, cellular functions, and molecular interactions within cells, these agents exhibit significant utility in assisting researchers through data analysis, hypothesis formulation, and experimental optimization (Gao et al., 2024c).Their hierarchical implementation enables scientists to enhance methodological approaches across different research phases while maintaining scientific accuracy and precision.</p>
<p>Physics</p>
<p>LLM-based scientific agents are advancing physics research by automating tasks such as modulation design, optimization, mechanics problem-solving, simulation, and parameter inference.The LP-COMDA (Liu et al., 2024b) framework uses an LLM-based planner to automate modulation design in power electronics, improving efficiency.LLM-Phy (Cherian et al., 2024) combines LLMs with physics engines to enhance optimization and accuracy in physical reasoning.MyCrunchGPT (Kumar et al., 2023) achieves automated NACA airfoils design and validation through seamless integration of computational fluid dynamics simulators with large language models, accomplishing multi-cycle iterative optimization processes within significantly reduced timeframes.MechAgents (Ni and Buehler, 2024) leverages multi-agent LLM systems to autonomously solve mechanics problems using finite element methods, improving both speed and precision.While LLMs perform well with basic physics problems, they struggle with complex simulations; however, integrating them with established computational packages can enhance their capabilities.</p>
<p>In comparison with LLMs that are not equipped with access to real-world interactions, scientific agents exhibit considerable practical advantages in addressing academic challenges and informing engineering implementations.By utilizing external computational toolkits and physics engines, these agents develop observational capabilities concerning physical phenomena and cultivate a more profound comprehension of physical principles, thus establishing a connection between theoretical exploration and practical application.</p>
<p>Astronomy</p>
<p>In astronomy, LLM-based agents are being developed to automate complex tasks such as data fitting, analysis, and iterative strategy improvement.These agents aim to mimic human intuition and deep literature understanding, expediting astronomical discovery.For example, StarWhisper (Wang et al., 2024a) is an LLM tailored for astronomy, capable of knowledge question answering, calling multimodal tools, and docking telescope control systems.Additionally, AstroLLaMA (Nguyen et al., 2023) is a specialized foundation model in astronomy, fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv, optimized for traditional causal language modeling.Furthermore, AstroSage-Llama-3.1-8B (de Haan et al., 2024) is a domain-specialized natural-language AI assistant tailored for research in astronomy, astrophysics, and cosmology, demonstrating remarkable proficiency on a wide range of questions.</p>
<p>Overall, the application of artificial intelligence in the field of astronomy has been extensive, encompassing tasks such as celestial object classification, astronomical event prediction, and the identification of new celestial bodies (Fluke and Jacobs, 2020).The autonomous planning and tool invocation capabilities of the scientific agent have enabled the automation of processes including astronomical observation, data processing, and data analysis.</p>
<p>Machine Learning and Data Science</p>
<p>LLM-based agents have revolutionized machine learning workflows by automating tasks such as data preprocessing, model selection, and hyperparameter tuning.The AI Scientist (Lu et al., 2024a) framework enables fully automated scientific discovery, where large language models independently generate ideas, execute experiments, write papers, and undergo review, advancing AI-driven research across fields.Similarly, MLR-Copilot (Li et al., 2024c), a framework powered by LLM agents, autonomously generates research ideas, implements experiments, and executes tasks, accelerating machine learning research and fostering innovation through automated processes.Additionally, Data Interpreter (Hong et al., 2024) autonomously solves end-to-end data science problems by dynamically adjusting to evolving task dependencies, achieving significant performance improvements across various tasks.</p>
<p>Scientific Literature Review</p>
<p>Literature review constitutes an integral component in general scientific research.LLM-based agents have significantly enhanced the efficiency of this process and accelerated scientific discovery by automating literature retrieval, screening, and summarization.ChatCite (Li et al., 2025) synthesizes pre-collected paper sets through the emulation of human workflows.The system employs a Key Element Extractor to generate template summaries from research requirements and target papers, followed by iterative refinement using a Reflective Incremental Generator to complete comprehensive literature reviews.Furthermore, SLR-automation (Sami et al., 2024) implements a systematic pipeline with specialized LLM agents for keyword generation, literature retrieval, paper screening, and final report compilation.Similarly, Agent Laboratory (Schmidgall et al., 2025) retrieves publications from arXiv and employs LLMdriven iterative integration mechanisms for review construction.ResearchAgent (Baek et al., 2025) establishes a systematic framework.It constructs citation graphs from seed papers using rule-based methods to aggregate scholarly literature, builds a structured knowledge repository to enable crossdomain knowledge integration, and employs LLM agents for iterative information synthesis and experimental design optimization during each research iteration.</p>
<p>Discussion</p>
<p>The above provides a wide range of applications for scientific agents powered by LLMs, demonstrating their potential to transform research in fields such as biomedical analysis, materials science, etc.These applications showcase how LLMbased agents can enhance data interpretation, support complex decision-making, and generate novel hypotheses, thus accelerating scientific discovery.</p>
<p>Despite this, current applications face significant limitations.Many applications are domain-specific and lack the flexibility needed to generalize across diverse scientific disciplines.In several cases, the integration of scientific knowledge with agent reasoning is hampered by static models that do not adapt to real-time data or evolving research challenges.Moreover, there is often insufficient validation of the agents' outputs against established scientific benchmarks, leading to concerns about reproducibility and reliability.</p>
<p>Looking ahead, future studies or products should focus on developing more generalized frameworks for scientific applications that integrate heterogeneous data sources and facilitate cross-disciplinary collaboration.Enhancements in real-time error detection, adaptive feedback mechanisms, and multimodal LLM architectures will be essential for improving the robustness of these systems.Collaborative efforts between domain experts and AI researchers are crucial to fine-tune the decisionmaking processes of scientific agents, ensuring that their outputs align closely with established scientific principles and practices.</p>
<p>Ethics</p>
<p>While these systems excel technically and drive scientific innovation, they raise significant ethical challenges.For example, Bengio et al. (2025) argue for a non-agentic "Scientist AI" design that emphasizes explanation over independent action to mitigate misalignment and loss of human control while preserving AI's scientific utility, indicating building generalist agents with autonomous planning and goal pursuit may risk catastrophic public safety issues.In Pournaras (2023), epistemological challenges and integrity risks in research are reviewed, setting a foundation for ethical guidelines.Other studies (Bano et al., 2023;Lin, 2024;Watkins, 2024;Limongi, 2024) further highlight issues of agency, transparency, bias, accountability, and integrity.This section offers concise guidelines to align LLM-based scientific agents with human values and uphold research integrity.</p>
<p>Agency and Autonomy</p>
<p>Scientific AI agents must act solely as tools under human oversight.Pournaras (2023) and Lin (2024) warn that without explicit constraints, agents may develop unintended autonomy-such as self-preservation or deceptive behaviors-that undermine research integrity.Hybrid approaches that combine top-down ethical rules with human feedback (Tennant et al., 2025) are promising to ensure control.Establishing strict operational boundaries during training and maintaining continuous supervision are essential to prevent these systems from pursuing independent objectives.</p>
<p>Transparency and Explainability</p>
<p>Transparent decision-making is vital for trustworthy scientific agents.Watkins (2024) emphasizes the urgent need for norms and standards in LLMbased research workflows.Recent studies (Bano et al., 2023;Banerjee et al., 2024) demonstrate that structured internal logs and explanation frameworks can "open the black box" of AI reasoning.Clear documentation enables auditing and helps verify that conclusions are based on sound logic, supporting accountability and reproducibility.</p>
<p>Hallucinations and Reliability</p>
<p>LLMs employed in scientific agents may produce hallucinations, generating outputs that appear plausible but are factually incorrect or nonsensical.These inaccuracies can stem from flawed training data, ambiguous prompts, or architectural limitations.For instance, LLMs have been manipulated to produce fabricated scientific arguments, falsely claiming that biases are beneficial, misleading researchers and potentially distorting scientific discourse (Ge et al., 2025).The hallucination problem brought by LLM-based scientific agents may undermine the credibility of research findings and erodes trust in AI-assisted scientific processes.The mitigation could be done by increasing the quality of training data, incorporating up-to-date and validated external knowledge sources, or establishing iterative feedback loops and validation mechanism, such as process supervision-based planners, which helps to ensure greater accuracy and reliability.</p>
<p>Vulnerability and Security</p>
<p>The potential for adversarial attacks (such as prompt injections or model extractions) introduces ethical issues regarding the misuse of LLM-based agents in scientific research.Malicious actors could exploit these vulnerabilities to deliberately distort scientific knowledge or manipulate research outcomes, which could have serious consequences for public safety, healthcare, and scientific progress.For example, Yang et al. (2024a) demonstrate how LLMs could be used to poison biomedical knowledge graphs, manipulating drug-disease relations.These vulnerabilities necessitate robust safeguards to prevent misuse and ensure the safe deployment of LLM-based systems in scientific research.</p>
<p>6.5 Bias, Fairness, and Data Integrity AI agents risk propagating biases from their training data, potentially skewing scientific outcomes.Lin (2024) shows that even advanced models may reproduce historical biases if not properly managed.</p>
<p>Complementary research (Bano et al., 2023) underscores the need for diverse datasets and fairnessaware algorithms.Regular bias audits and transparent documentation of data provenance help prevent skewed outcomes, ensuring that AI-driven research remains equitable and credible.</p>
<p>Accountability and Governance</p>
<p>Clear accountability is crucial when AI agents influence scientific outcomes.Bano et al. (2023) provides empirical insights into RAI practices and reveals gaps in ethical preparedness.Robust oversight mechanisms-such as periodic audits, transparent reporting, and defined redress pathways-ensure timely human intervention.Decentralized models, where agents critique one another (de Cerqueira et al., 2024), further enhance accountability.Embedding ethical guidelines into institutional policies and aligning with international standards builds trust in AI-driven research.</p>
<p>Intellectual Property and Research Integrity</p>
<p>AI integration in research raises complex questions of authorship and ownership.Limongi (2024) discusses challenges in maintaining credibility and ethical standards amid AI-driven discoveries.</p>
<p>Transparent documentation of AI contributions is essential to prevent plagiarism and secure intellectual property rights.Clear disclosure policies, combined with regular audits, protect the work of human researchers and ensure that AI-generated insights are ethically integrated and verifiable.</p>
<p>Conclusion</p>
<p>This survey provides a holistic examination of LLM-based scientific agents, beginning with a detailed exploration of their architectures-which encompass planners, memory systems, and tool sets-and extending to their evaluation through benchmarks, diverse applications, and ethical considerations.Our review demonstrates how planners, through prompt-based strategies, supervised fine-tuning, reinforcement learning, and process supervision, serve as the strategic backbone for decomposing complex scientific tasks.Equally, the integration of memory and tool sets within these architectures is pivotal in managing dynamic scientific data and executing specialized operations, thereby enhancing the agents' problem-solving capabilities.We also demonstrate the unique features of scientific agents, compared with general-purpose ones, necessitate the dedicate design for them.</p>
<p>Beyond the architectural components, the survey delves into the benchmarks and real-world impact of these agents.The discussion on benchmarks highlights both the general reasoning ability and the domain-specific scientific competence required for successful application in research environments.The analysis of applications illustrates how these systems are deployed to drive innovations across multiple scientific disciplines, while the ethical discourse emphasizes the need for responsible AI practices that ensure reproducibility, transparency, and adherence to stringent research standards.</p>
<p>Overall, the advancements and challenges presented in this survey point to a promising future where continuous improvements in LLM-based scientific agents could revolutionize scientific discovery.By bridging the gap between theoretical research and practical applications, these agents are set to catalyze new levels of interdisciplinary collaboration and innovation in science.</p>
<p>A Google's AI Co-scientist: An Illustration of a Scientific Agent System</p>
<p>Below is an illustration of how an LLM-based scientific agent, the Google's AI co-scientist (Gottweis et al., 2025), can function as a multi-agent system to assist researchers in formulating hypotheses, designing experiments, and synthesizing existing literature.The components are structured into three core modules, following our earlier description: (1) Planner, (2) Memory, and (3) Tool Set.This organization highlights how advanced language models can iteratively plan scientific inquiries, maintain and refine long-term reasoning, and leverage external resources.An overview of this system is presented in Figure A.1.</p>
<p>A.1 Planner</p>
<p>The planner provides the overall reasoning and coordination framework, inspired by the steps of the scientific method:</p>
<p>• Input parsing and configuration.The system accepts a user defined research goal (e.g., "Propose novel drug repurposing strategies for acute myeloid leukemia") and parses it into a structured research plan.This process encodes any requirements or constraints in the input, such as accessible experimental assays or specific safety considerations.</p>
<p>• Specialized agents for task execution.Several specialized agents operate under the planner's coordination:</p>
<p>-Generation agent drafts preliminary hypotheses or proposals, performing literature exploration and combining prior results with new conjectures.-Reflection agent reviews each hypothesis, checking for consistency, novelty, correctness, and alignment with known data.It can also simulate potential pitfalls or failure points in each proposal.-Ranking agent organizes a tournament of hypotheses, making pairwise comparisons to assign an Elo-based quality score.Promising ideas are refined further, while those shown to be contradictory or impractical are filtered out.-Proximity agent computes a similarity graph for hypotheses, enabling clustering and deduplication.By analyzing se-mantic and contextual relationships between ideas, the agent groups similar hypotheses and identifies redundant ones, ensuring efficient exploration of the hypothesis space.-Evolution agent iteratively refines toprated hypotheses, merging or adapting ideas based on feedback.-Meta-review agent synthesizes recurring observations such as overlooked evidence or repeated mistakes into meta critique feedback for other agents.It also integrates the top-rated hypotheses and reviews into a coherent, high-level overview for the user.</p>
<p>• Resource Scheduling.The Supervisor agent manages the entire process, allocating computational resources to each specialized agent based on the complexity of the research goal and the system's progress.Through iterative task dispatch, the planner maintains a sustained, self-improving cycle of reasoning.</p>
<p>A.2 Memory</p>
<p>Ensuring continuous and coherent multi-step reasoning demands robust mechanisms for storing, retrieving, and updating the system's state.To this end, the system employs a persistent context memory that supports iterative reasoning cycles.This repository houses newly generated hypotheses, expert commentary, external references, and notes from specialized agents, thereby maintaining continuity throughout the computational workflow.When reflection critiques, tournament rankings, or meta-review insights become available, they are appended to the memory, allowing the system to refine its reasoning while preserving the record of past decisions.</p>
<p>Stateful storage further enables long-horizon iterative refinement, where hypotheses evolve incrementally without compromising earlier logical foundations.For instance, partial experimental details or validated findings remain accessible even as new data are integrated, preventing the loss of critical insights.The memory also tracks resource allocation metrics, such as hypothesis generation success rates, to guide the Supervisor agent in dynamically prioritizing tasks.In addition, the system keeps summaries of key results, including top-ranked hypotheses and recurring pitfalls, to streamline knowledge retrieval for both human  (Gottweis et al., 2025).The system begins by accepting a natural language research goal from the scientist, which is parsed into a structured research plan.This plan is forwarded to the Supervisor Agent, which evaluates its requirements to allocate computational resources and priority weights to specialized agents.These agents are then organized into a task queue based on their assigned weights.Notably, only the Generation, Reflection, and Evolution agents have access to the toolset.Worker processes execute the queued tasks sequentially, leveraging the expertise of each specialized agent.Finally, the system synthesizes the collected data to generate a comprehensive research summary, including hypotheses and actionable proposals for the user.In the "Specialized Agents" section, grey boxes highlight individual agents, each designed with distinct operational logic and task-specific roles.users and specialized agents.These mechanisms enable the co-scientist to balance innovation and historical awareness, fostering sustained, cumulative progress toward research goals.</p>
<p>A.3 Tool Set</p>
<p>Meanwhile, the AI co-scientist expands its capabilities beyond text generation through strategic integration of specialized tools.It leverages search and retrieval systems to query literature databases, online repositories, and user-provided resources, ensuring hypotheses are grounded in existing evidence, avoiding redundancy, and identifying gaps for novel insights.</p>
<p>For domain-specific tasks, the system invokes tailored tools such as AlphaFold (Jumper et al., 2021) for protein structure validation, the Cancer Dependency Map (DepMap) (Institute, 2024) for gene dependency analysis in cancer, and drug libraries for repurposing candidates.</p>
<p>A.4 Summary of Workflow</p>
<p>As a whole, the multi-agent AI co-scientist operates as follows:</p>
<ol>
<li>
<p>Parsing user input into structured plans.</p>
</li>
<li>
<p>Generating and reviewing plausible hypotheses through specialized agents.Notably, users also have the flexibility to refine or expand their requirements by interacting with the agent during the hypothesis-generation process.</p>
</li>
<li>
<p>Conducting an iterative tournament where the top ideas are compared, refined, or strategically combined.</p>
</li>
<li>
<p>Maintaining a context repository for longhorizon memory.</p>
</li>
<li>
<p>Accessing relevant tools and specialized models to verify and refine proposals.</p>
</li>
<li>
<p>Producing a final research overview or set of top-ranked candidates, enabling direct engagement with human researchers for real-world validation.</p>
</li>
</ol>
<p>In this way, the Planner, Memory, and Tool Set modules collectively foster a systematic, selfimproving approach to advanced scientific inquiry, leveraging LLMs and related tools to complement and amplify human expertise.</p>
<p>A.5 Discussion</p>
<p>The multi-agent framework of Google's AI coscientist offers a powerful approach for automating scientific discovery, particularly across three key problem areas in biomedicine.First, the system has been demonstrated to propose promising drug repurposing candidates for diseases such as acute myeloid leukemia.Second, it has shown potential in discovering novel treatment targets, as illustrated in the identification of epigenetic regulators for liver fibrosis.Third, it has helped to uncover mechanisms of microbial evolution and antimicrobial resistance, recapitulating unpublished findings of novel gene transfer pathways in bacteria.</p>
<p>However, several limitations remain.On the one hand, while the multi-agent design helps isolate errors to specific stages of reasoning, the potential for model hallucination requires careful oversight.Relying on automated reviews, even if tournamentbased, does not fully eliminate inaccuracies and overconfident claims.On the other hand, the system's recommendations hinge heavily on the corpus of literature and data it can access.In emerging fields or topics with limited public data, the generated ideas may be too speculative or miss key non-public findings.Additionally, as with any AI-driven method, ethical, legal, and regulatory considerations become paramount when moving from in silico predictions to clinical or large-scale biological testing.Lastly, while the method shows promise in biomedical contexts, its generalizability to other domains remains untested and may require field-specific adaptations.</p>
<p>Looking forward, further enhancements of the co-scientist framework can focus on several complementary directions.First, incorporating increasingly multimodal and domain-specialized AI systems has the potential to accelerate discovery not only in oncology, fibrosis, and antimicrobial resistance, but in an expanding range of biomedical domains.Second, refining methods to detect and mitigate hallucinations-through more transparent agent interactions, robust error-logging, and human verification loops-could make multi-agent pipelines more reliable.Finally, applying similar AI multi-agent architectures to emerging therapies, personalized medicine, and even non-biomedical areas of science may further highlight the versatility of large language models as co-collaborators, potentially reshaping entire research workflows.</p>
<p>Figure 2 :
2
Figure 2: Taxonomy of the planner of science agents.</p>
<p>Figure 3 :
3
Figure 3: The types of planner in LLM-based scientific agents.(a) Prompt based planner; (b) SFT-based planner; (c) RL-based planner; (d) Process supervision based planner.</p>
<p>Figure 4 :
4
Figure 4: Taxonomy of the memory mechanism of science agents.</p>
<p>✓</p>
<p>Expansive, up-to-date domain-specific information ✓ Reasoning in external, validated research × Integration can be complex × Dependent on the quality and update frequency embedded in the LLM from pre-training and fine-tuning.✓ Provides a robust foundation for general language understanding and scientific reasoning ✓ Immediately available × May become outdated over time × Limited by the scope and recency of the training</p>
<p>Figure 5 :
5
Figure 5: A simple process of scientific agents using historical context (e.g.,comments provided by the Review Agent, and errors in each round of experiments).</p>
<p>Figure 6 :
6
Figure 6: A simple process of scientific agents using external knowledge base.</p>
<p>Figure 7 :
7
Figure 7: Taxonomy of the tool sets of scientific agents.</p>
<p>Figure 8 :
8
Figure 8: Taxonomy of the LLM-based science agents evaluation benchmarks.</p>
<p>Figure 9 :
9
Figure 9: Taxonomy of the scientific agents' applications.</p>
<p>Figure A. 1 :
1
Figure A.1: The Google's AI co-scientist multi-agent architecture(Gottweis et al., 2025).The system begins by accepting a natural language research goal from the scientist, which is parsed into a structured research plan.This plan is forwarded to the Supervisor Agent, which evaluates its requirements to allocate computational resources and priority weights to specialized agents.These agents are then organized into a task queue based on their assigned weights.Notably, only the Generation, Reflection, and Evolution agents have access to the toolset.Worker processes execute the queued tasks sequentially, leveraging the expertise of each specialized agent.Finally, the system synthesizes the collected data to generate a comprehensive research summary, including hypotheses and actionable proposals for the user.In the "Specialized Agents" section, grey boxes highlight individual agents, each designed with distinct operational logic and task-specific roles.</p>
<p>Table 1 :
1
For instance, Perlis et al. (2024)create a clinical decision support agent for bipolar disorder, using patient history, symptoms, and guidelines in the Comparison of different planners.
ApproachMethodologyStrengths and LimitationsTypical Use CasesPrompt-BasedUse carefully engineered✓ No additional training requiredRapid prototyping;prompts for in-context✓ Flexible and easy to implementinitial tasklearning× Highly dependent on prompt qualitydecomposition× May lack robustness for complex tasksSFT-BasedFine-tune a pre-trained✓ Adapts to domain-specific nuancesDetailed planning tasks;(SupervisedLLM on curated planning✓ Produces more precise, step-by-step plansscientific workflowsFine-Tuning)trajectories× Requires large, high-quality labeled datasetswith structured data× Computationally resource intensiveRL-BasedOptimize decision-making✓ Learns adaptive strategies through trial andIterative, multi-step(Reinforcementthrough reward and penaltyerrorprocesses (e.g.,Learning)signals✓ Improves planning over iterationsexperimental design)× Needs well-defined reward functions× Computationally expensiveProcessIncorporate iterative✓ Mimics human error-correctionDynamic tasksSupervision-self-evaluation and external✓ Continuously refines the planning processrequiring ongoingBasedfeedback loops× More complex to implementrefinement and× May require additional verificationreliabilitymechanismsPrompt TypeMethodPrompt ContentsTaskPerlis et al. (2024)Patient's clinical history, symptoms, andClinical decision supportContextualguidelinesInformationCoI (Li et al., 2024b)Related research papers with key findings Research idea generationEmbeddingResearchAgent</p>
<p>Table 2 :
2
Different types of prompt construction.</p>
<p>illustrates how historical context underpins the iterative Enables a coherent and iterative refinement ✓ Supports dynamic adaptation × Limited by the model's context window × Implicit logging can make explicit retrieval challenging
TypeMethodologyStrengths and LimitationsTypical Use CasesHistoricalMaintains conversational✓ Iterative hypothesisContextlogs or iterative actionrefinement, trackingsequences; stores previousmulti-turn researchinteractions andsessions, and adaptingexperimental outcomesstrategies based onprevious interactions.ExternalAccesses curated externalKnowledgesources such as literatureBasedatabases and structuredknowledge graphs</p>
<p>Table 3 :
3
Comparison of different memory types.</p>
<p>Table 4 :
4
Table 4 lists the key different features.Noting that current scientific agents are still in the early stage, perhaps Comparison between general-purpose agents and scientific agents.
AspectsGeneral-purpose AgentsScientific AgentsPlanning and-Heuristic or reactive planning-Logical, structured, hierarchical planningTask-Flexible, goal-driven methods-Long-horizon research projectsManagement-Not aligned with scientific methodology-Mirrors the scientific methodMemory and-Ephemeral, context-limited storage-Persistent, structured memoryKnowledge-Typically single-session or ad-hoc-Accumulates data and insights across multipleIntegration-Minimal cross-project continuityexperiments-Enables reproducibility and long-term progressionTool Utilization-Plugin-based for a wide variety of tasks-Specialized, domain-specific toolsand Integration-Minimal domain-specific parameterization-Deep integration for experiment workflowsDomain--Mostly direct, goal-focused-Iterative, hypothesis-driven logicSpecific-Single-agent or loosely multi-agent-Integrates domain rules and scientific practicesReasoning and-Often relies on user feedback to catch errors-Multi-agent debate and consensus-buildingCollaboration-Rigorous statistical checks, error bounds</p>
<p>Table 5 .
5
K-12 Foundational Skills: At the foundational level, agents are expected to exhibit proficiency in key areas such as geometry (plane and analytic
Benchmark NameScopeSizeDisciplineGeometry3K (Lu et al., 2021)K-123002MathematicsGeoEval (Zhang et al., 2024b)K-125050MathematicsVisScience (Jiang et al., 2024)K-123000Physics &amp; Chemistry &amp; MathematicsMathVista (Lu et al., 2024b)K-12 &amp; College6141MathematicsSciBench (Wang et al., 2024d)College869Physics &amp; Chemistry &amp; MathematicsSciEval (Sun et al., 2024a)College18000Physics &amp; Chemistry &amp; BiologySuperGPQA (Team et al., 2025) Graduate-Level 26529GeneralHLE (Phan et al., 2025)Expert-Level3000Humanity &amp; Science &amp; Mathematics</p>
<p>Table 5 :
5
Summary of benchmarks for general reasoning ability evaluation in LLM-based scientific agents."General" means a benchmark is not designed for a particular discipline.
Benchmark NameFC HD ED EWDisciplineFigureQA (Kahou et al., 2018)---GeneralArXivQA (Li et al., 2024a)---GeneralMMSCI (Li et al., 2024e)---GeneralSciMON (Wang et al., 2024c)---NLP &amp; BiomedicalMOOSE-Chem (Yang et al., 2024b)---Chemistry &amp; Material ScienceDiscoveryBench (Majumder et al., 2024)---Social Science &amp; Biology &amp; HumanityGAIA (Mialon et al., 2023)--GeneralTaskBench (Shen et al., 2024b)---GeneralMLAgentBench (Huang et al., 2024b)--GeneralDiscoveryWorld (Jansen et al., 2024)-GeneralLAB-Bench (Laurent et al., 2024)---BiologyDSBench (Jing et al., 2024)---Data ScienceScienceAgentBench (Chen et al., 2024)---Psychology &amp; Bioinformatics &amp; Geomatics &amp; ChemistrySciCode (Tian et al., 2024)---Physics &amp; Chemistry &amp; Mathematics &amp; Biology</p>
<p>Table 6 :
6
Summary of benchmarks for scientific research-oriented abilities evaluation in LLM-based scientific agents.</p>
<p>FC=Scientific Figure Comprehension; HD=Hypothesis Discovery; ED=Experiment Design; EW= Experiment Execution &amp; Workflow Automation."General" means a benchmark is not designed for a particular discipline.</p>
<p>Lu, MLR-Copilot. 2024a. 2023. 2024c</p>
<p>Biodiscoveryagent (roohani, AtomAgents (Ghafarollahi and Buehler, 2024a). 2024. 2024c</p>
<p>Agent Laboratory (Schmidgall et al., 2025), etc. Intrinsic Knowledge ChemDFM. ( Llmatdesign, Jia, SciAgents (Ghafarollahi and Buehler, 2024c). -X ( Chemist, Chen, Ghafarollahi and Buehler2024. 2025. 2024b. 2024. 2024. 2024. 2024b. 2025. 2023. 2024b. 2023. 2025. 2024bDrugAgent. PaSa (He. Hybrid GeneGPT (Jin et al.</p>
<p>A-lab. Amadeusgpt (ye, Applications Chemistry and Materials Science Chemist-X. 2023b. 2024c. 2023. 2024. 2024a. 2023. 2024b. 2024a. 2024a. 2024. 2024a. 2025. 2024b. 2024Physics LP-COMDA</p>
<p>( Mycrunchgpt, Kumar, MechAgents (Ni and Buehler, 2024), etc. Astronomy StarWhisper. 2023. 2024a. 2023</p>
<p>De Haan, Machine Learning and Data Science AI Scientist. Astrosage-Llama-3.1-8b, 2024. 2024a. 2024c. 2024. 2025. 2024. 2025. 2025Agent Laboratory. ResearchAgent (Baek et al.</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Scientific multi-agent reinforcement learning for wallmodels of turbulent flows. Jane Bae, Petros Koumoutsakos, Nature Communications. 13114432022</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382025Preprint</p>
<p>On the ethical considerations of generative agents. Soumya Banerjee, arXiv:2411.192112024arXiv preprint</p>
<p>Muneera Bano, Didar Zowghi, arXiv:2312.09561Pip Shea, and Georgina Ibarra. 2023. Investigating responsible ai for scientific research: an empirical study. arXiv preprint</p>
<p>. Catarina Barata, Veronica Rotemberg, Philipp Noel Cf Codella, Christoph Tschandl, Rinner, Nisa Bengu, Zoe Akay, Giuseppe Apalla, Argenziano, Allan</p>
<p>A reinforcement learning model for ai-based decision support in skin cancer. Aimilios Halpern, Lallas, Nature Medicine. 2982023</p>
<p>Superintelligent agents pose catastrophic risks: Can scientist ai offer a safer path. Yoshua Bengio, Michael Cohen, Damiano Fornasiere, Joumana Ghosn, Pietro Greiner, Matt Macdermott, Sören Mindermann, Adam Oberman, Jesse Richardson, Oliver Richardson, arXiv:2502.156572025arXiv preprint</p>
<p>Autonomous chemical research with large language models. Robert Daniil A Boiko, Ben Macknight, Gabe Kline, Gomes, Nature. 62479922023</p>
<p>Augmenting large language models with chemistry tools. Andres Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, Philippe Schwaller, 10.1038/s42256-024-00832-8Nature Machine Intelligence. 652024</p>
<p>Juan Manuel, Zambrano Chaves, Eric Wang, Tao Tu, Eeshit Dhaval Vaishnav, Byron Lee, Sara Mahdavi, Christopher Semturs, arXiv:2406.06316Tx-llm: A large language model for therapeutics. David Fleet, VivekNatarajan, and Shekoofeh Azizi. 2024arXiv preprint</p>
<p>Chemist-x: Large language model-empowered agent for reaction condition recommendation in chemical synthesis. Chen, Li, Y Wang, Du, Yu, Lu, Li, Qiu, Pan, Huang, arXiv:2311.10776arxiv. 2023arXiv preprint</p>
<p>Matchat: A large language model and application service platform for materials science. Zi-Yi Chen, Fan-Kai Xie, Meng Wan, Yang Yuan, Miao Liu, Zong-Guo Wang, Sheng Meng, Yan-Gang Wang, Chinese Physics B. 32111181042023</p>
<p>Scienceagentbench: Toward rigorous assessment of language agents for data-driven scientific discovery. Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, Yifei Li, Zeyi Liao, Chen Wei, Zitong Lu, Vishal Dey, Mingyi Xue, Frazier N Baker, Benjamin Burns, Daniel Adu-Ampratwum, Xuhui Huang, Xia Ning, Song Gao, Yu Su, Huan Sun, arXiv:2410.050802024Preprint</p>
<p>Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, arXiv:2401.03428Exploring large language model based intelligent agents: Definitions, methods, and prospects. 2024arXiv preprint</p>
<p>Llmphy: Complex physical reasoning using large language models and world models. Anoop Cherian, Radu Corcodel, Siddarth Jain, Diego Romeres, arXiv:2411.080272024arXiv preprint</p>
<p>José Antonio Siqueira de Cerqueira, Mamia Agbese, Rebekah Rousi, Nannan Xi, Juho Hamari, and Pekka Abrahamsson. 2024. Can we trust ai agents? an experimental study towards trustworthy llm-based multi-agent systems for ai ethics. Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, Janosh Riebesell, arXiv:2401.17244arXiv:2411.08881Llamp: Large language model made powerful for high-fidelity materials knowledge retrieval and distillation. 2024arXiv preprint</p>
<p>Yuan-Sen Tijmen De Haan, Tirthankar Ting, Ghosal, Dung Tuan, Alberto Nguyen, Azton Accomazzi, Nesar Wells, Rui Ramachandra, Zechang Pan, Sun, arXiv:2411.09012Astromlab 3: Achieving gpt-4o level performance in astronomy with a specialized 8b-parameter large language model. 2024arXiv preprint</p>
<p>Yihe Deng, Paul Mineiro, arXiv:2410.22304Flow-dpo: Improving llm mathematical reasoning through online multiagent learning. 2024arXiv preprint</p>
<p>Surveying the reach and maturity of machine learning and artificial intelligence in astronomy. J Christopher, Colin Fluke, Jacobs, Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery. 202010e1349</p>
<p>Dockingga: enhancing targeted molecule generation using transformer neural network and genetic algorithm with docking simulation. Changnan Gao, Wenjie Bao, Shuang Wang, Jianyang Zheng, Lulu Wang, Yongqi Ren, Linfang Jiao, Jianmin Wang, Xun Wang, 10.1093/bfgp/elae011Briefings in Functional Genomics. 2352024a</p>
<p>Dawei Gao, Zitao Li, Xuchen Pan, Weirui Kuang, Zhijian Ma, Bingchen Qian, Fei Wei, Wenhao Zhang, Yuexiang Xie, Daoyuan Chen, arXiv:2402.14034Agentscope: A flexible yet robust multi-agent platform. 2024barXiv preprint</p>
<p>Empowering biomedical discovery with ai agents. Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka Zitnik, Cell. 187222024c</p>
<p>Llms are vulnerable to malicious prompts disguised as scientific language. Yubin Ge, Neeraja Kirtane, Hao Peng, Dilek Hakkani-Tür, arXiv:2501.140732025arXiv preprint</p>
<p>Atomagents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. Alireza Ghafarollahi, Markus J Buehler, arXiv:2407.100222024aarXiv preprint</p>
<p>Protagents: protein discovery via large language model multi-agent collaborations combining physics and machine learning. Alireza Ghafarollahi, Markus J Buehler, Digital Discovery. 2024b</p>
<p>Sciagents: Automating scientific discovery through multi-agent intelligent graph reasoning. Alireza Ghafarollahi, Markus J Buehler, arXiv:2409.055562024carXiv preprint</p>
<p>Ryutaro Tanno, et al. 2025. Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Anil Palepu, Petar Sirkovic, Artiom Myaskovsky, Felix Weissenberger, Keran Rong, arXiv:2502.18864Preprint</p>
<p>Tora: A tool-integrated reasoning agent for mathematical problem solving. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024. May 7-11, 2024OpenReview.net</p>
<p>Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, Yu Su, arXiv:2402.14672Middleware for llms: Tools are instrumental for language agents in complex environments. 2024arXiv preprint</p>
<p>Large language model based multi-agents: A survey of progress and challenges. Guo, Chen, Wang, Chang, Pei, Chawla, Wiest, Zhang, 33rd International Joint Conference on Artificial Intelligence (IJCAI 2024). IJCAI; Cornell arxiv. 2024</p>
<p>Memory matters: The need to improve long-term memory in llm-agents. Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, Dustin Dannenhauer, Proceedings of the AAAI Symposium Series. the AAAI Symposium Series20232</p>
<p>Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, arXiv:2501.10120Pasa: An llm agent for comprehensive academic paper search. 2025arXiv preprint</p>
<p>Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Chenxing Wei, Danyang Li, Jiaqi Chen, Jiayi Zhang, arXiv:2402.18679Data interpreter: An llm agent for data science. 2024arXiv preprint</p>
<p>Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, arXiv:2402.06457Alessandro Sordoni, and Rishabh Agarwal. 2024. V-star: Training verifiers for self-taught reasoners. arXiv preprint</p>
<p>Lora: Low-rank adaptation of large language models. J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, ICLR. 1232022</p>
<p>Electromagnetic metamaterial agent. Shengguo Hu, Mingyi Li, Jiawen Xu, Hongrui Zhang, Shanghang Zhang, Tie Jun Cui, Philipp Del Hougne, Lianlin Li, Science &amp; Applications. 141122025</p>
<p>De novo drug design using reinforcement learning with multiple gpt agents. Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, Ling Liu ; Xiuyuan, Guoqing Hu, Yang Liu, Hao Zhao, Zhang, arXiv:2404.02039A survey on large language model-based game agents. 2024a. 2024b36arXiv preprintAdvances in Neural Information Processing Systems</p>
<p>Crispr-gpt: An llm agent for automated design of gene-editing experiments. Kaixuan Huang, Yuanhao Qu, Henry Cousins, William A Johnson, Di Yin, Mihir Shah, Denny Zhou, Russ Altman, Mengdi Wang, Le Cong, arXiv:2404.180212024aarXiv preprint</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.033022024bPreprint</p>
<p>Tenghao Huang, Donghee Lee, John Sweeney, Jiatong Shi, Emily Steliotes, Matthew Lange, Jonathan May, Muhao Chen, arXiv:2409.12832Foodpuzzle: Developing large language model agents as flavor scientists. 2024carXiv preprint</p>
<p>Fewer is more: Boosting math reasoning with reinforced context pruning. Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Fan Yang, Mao Yang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language Processing2024d</p>
<p>Yoshitaka Inoue, Tianci Song, Tianfan Fu, arXiv:2408.13378Drugagent: Explainable drug repurposing agent with large language model-based reasoning. 2024arXiv preprint</p>
<p>14 examples of how llms can transform materials science and chemistry: a reflection on a large language model hackathon. Kevin Maik, Jablonka , Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bocarsly, Stefan Andres M Bran, Catherine Bringuier, Kamal Brinson, Defne Choudhary, Circi, Digital Discovery. 252023Depmap q2 2024 data release</p>
<p>Discoveryworld: A virtual environment for developing and evaluating automated scientific discovery agents. Peter A Jansen, Marc-Alexandre Côté, Tushar Khot, Erin Bransom, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Oyvind Tafjord, Peter Clark, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems. NeurIPS; Vancouver, BC, Canada2024. 2024. 2024. December 10 -15, 2024</p>
<p>A primer on reinforcement learning in medicine for clinicians. Pushkala Jayaraman, Jacob Desman, Moein Sabounchi, Girish N Nadkarni, Ankit Sakhuja, NPJ Digital Medicine. 713372024</p>
<p>Llmatdesign: Autonomous materials discovery with large language models. Shuyi Jia, Chao Zhang, Victor Fung, arXiv:2406.131632024arXiv preprint</p>
<p>Visscience: An extensive benchmark for evaluating k12 educational multi-modal scientific reasoning. Zhihuan Jiang, Zhen Yang, Jinhao Chen, Zhengxiao Du, Weihan Wang, Bin Xu, Jie Tang, arXiv:2409.137302024Preprint</p>
<p>Agentmd: Empowering language agents for risk prediction with large-scale clinical tool learning. Qiao Jin, Zhizheng Wang, Yifan Yang, Qingqing Zhu, Donald Wright, Thomas Huang, John Wilbur, Zhe He, Andrew Taylor, Qingyu Chen, arXiv:2402.132252024aarXiv preprint</p>
<p>Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu, Bioinformatics. 402e0752024b</p>
<p>Dsbench: How far are data science agents to becoming data science experts?. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, Dong Yu, arXiv:2409.077032024Preprint</p>
<p>Highly accurate protein structure prediction with alphafold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, nature. 59678732021</p>
<p>Figureqa: An annotated figure dataset for visual reasoning. Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, Yoshua Bengio, 6th International Conference on Learning Representations, ICLR 2018. Workshop Track Proceedings. OpenReview.net. Vancouver, BC, Canada2018. April 30 -May 3, 2018</p>
<p>Local climate services for all, courtesy of large language models. Nikolay Koldunov, Thomas Jung, 10.1038/s43247-023-01199-1Communications Earth &amp; Environment. 51132024</p>
<p>Training language models to selfcorrect via reinforcement learning. Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John D Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, arXiv:2409.129172024arXiv preprint</p>
<p>Mycrunchgpt: A llm assisted framework for scientific machine learning. Varun Kumar, Leonard Gleyzer, Adar Kahana, Khemraj Shukla, George Em Karniadakis, Journal of Machine Learning for Modeling and Computing. 442023</p>
<p>Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, Jiaya Jia, arXiv:2406.18629Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. 2024arXiv preprint</p>
<p>Lab-bench: Measuring capabilities of language models for biology research. Jon M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, Samuel G Rodriques, arXiv:2407.10362Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al.2024. 202033PreprintRetrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems</p>
<p>Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models. Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, Qi Liu, 10.18653/V1/2024.ACL-LONG.775Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024a. August 11-16, 20241ACL 2024</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, arXiv:2410.13185Chain of ideas: Revolutionizing research via novel idea development with llm agents. 2024barXiv preprint</p>
<p>Ruochen Li, Teerth Patel, Qingyun Wang, Xinya Du, arXiv:2408.14033Mlr-copilot: Autonomous machine learning research based on large language models agents. 2024carXiv preprint</p>
<p>A survey on llm-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth. Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, Yi Yang, 2024d19</p>
<p>Chatcite: LLM agent with human workflow guidance for comparative literature summary. Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, Lijie Wen, Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025. the 31st International Conference on Computational Linguistics, COLING 2025Abu Dhabi, UAEAssociation for Computational Linguistics2025. January 19-24, 2025</p>
<p>Mmsci: A dataset for graduate-level multi-discipline multimodal scientific understanding. Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, Hyeonjung Kim, Jin Hyuk Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, Linda Ruth Petzold, Stephen D Wilson, Woosang Lim, William Yang, Wang , arXiv:2407.049032024ePreprint</p>
<p>The use of artificial intelligence in scientific research with integrity and ethics. Ricardo Limongi, Future Studies Research Journal: Trends and Strategies. 1612024</p>
<p>Beyond principlism: practical strategies for ethical ai use in research practices. Zhicheng Lin, AI and Ethics. 2024</p>
<p>Toward a team of ai-made scientists for scientific discovery from gene expression data. Haoyang Liu, Yijiang Li, Jinglin Jian, Yuxuan Cheng, Jianrong Lu, Shuyi Guo, Jinglei Zhu, Mianchen Zhang, Miantong Zhang, Haohan Wang, arXiv:2402.123912024a65arXiv preprint</p>
<p>Physics-informed llm-agent for automated modulation design in power electronics systems. Junhua Liu, Fanfan Lin, Xinze Li, Kwan Hui Lim, Shuai Zhao, arXiv:2411.142142024barXiv preprint</p>
<p>Mind's eye: Grounded language model reasoning through simulation. Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, Andrew M Dai, ICLR 2023The Eleventh International Conference on Learning Representations. Kigali, Rwanda2023. May 1-5, 2023OpenReview.net</p>
<p>Aigs: Generating science from aipowered automated falsification. Zijun Liu, Kaiming Liu, Yiqi Zhu, Xuanyu Lei, Zonghan Yang, Zhenhe Zhang, Peng Li, Yang Liu, arXiv:2411.119102024carXiv preprint</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024aarXiv preprint</p>
<p>Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng , ICLR 2024The Twelfth International Conference on Learning Representations. Vienna, Austria2024b. May 7-11, 2024OpenReview.net</p>
<p>Inter-GPS: Interpretable geometry problem solving with formal language and symbolic reasoning. Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, Song-Chun Zhu, 10.18653/v1/2021.acl-long.528Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Improve mathematical reasoning in language models by automated process supervision. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, arXiv:2406.065922024arXiv preprint</p>
<p>Biogpt: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, Briefings in bioinformatics. 2364092022</p>
<p>Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, Hang Li, arXiv:2401.08967Reft: Reasoning with reinforced fine-tuning. 2024arXiv preprint</p>
<p>Top-down design of protein architectures with reinforcement learning. Shunzhi Isaac D Lutz, Christoffer Wang, Alexis Norn, Andrew J Courbet, Yan Ting Borst, Annie Zhao, Longxing Dosey, Jinwei Cao, Elizabeth M Xu, Leaf, Science. 38066422023</p>
<p>Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian, arXiv:2402.16445Prollama: A protein language model for multi-task protein language processing. 2024arXiv preprint</p>
<p>LLM and simulation as bilevel optimizers: A new paradigm to advance physical scientific discovery. Pingchuan Ma, Tsun-Hsuan Wang, Minghao Guo, Zhiqing Sun, Joshua B Tenenbaum, Daniela Rus, Chuang Gan, Wojciech Matusik, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024a. July 21-27, 2024OpenReview.net</p>
<p>Sciagent: Toolaugmented language models for scientific reasoning. Yubo Ma, Zhibin Gou, Junheng Hao, Ruochen Xu, Shuohang Wang, Liangming Pan, Yujiu Yang, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. the 2024 Conference on Empirical Methods in Natural Language ProcessingMiami, FL, USA2024b. 2024. November 12-16, 2024Yixin Cao, and Aixin Sun. Association for Computational Linguistics</p>
<p>Discoverybench: Towards data-driven discovery with large language models. Prasad Bodhisattwa, Harshit Majumder, Dhruv Surana, Bhavana Agarwal, Abhijeetsingh Dalvi Mishra, Aryan Meena, Tirth Prakhar, Tushar Vora, Ashish Khot, Peter Sabharwal, Clark, arXiv:2407.017252024Preprint</p>
<p>Manus ai: The future of general ai agents. Manusai, 2025</p>
<p>Cactus: Chemistry agent connecting tool usage to science. Gautham Andrew D Mcnaughton, Krishna Sankar, Agustin Ramalaxmi, Kruel, Rohith A Carter R Knutson, Neeraj Varikoti, Kumar, ACS omega. 9462024</p>
<p>Gaia: a benchmark for general ai assistants. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann Lecun, Thomas Scialom, arXiv:2311.129832023Preprint</p>
<p>Dung Tuan, Yuan-Sen Nguyen, Ioana Ting, Charlie O' Ciucȃ, Ze-Chang Neill, Maja Sun, Sandor Jabłońska, Ernest Kruk, Jack Perkowski, Jason Miller, Li, arXiv:2309.06126Astrollama: Towards specialized foundation models in astronomy. 2023arXiv preprint</p>
<p>Mechagents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. Bo Ni, Markus J Buehler, Extreme Mechanics Letters. 671021312024</p>
<p>An autonomous gis agent framework for geospatial data retrieval. Huan Ning, Zhenlong Li, Temitope Akinboyewa, Lessani Naser, International Journal of Digital Earth. 18124586882025</p>
<p>Aleksandar Odhran O'donoghue, John Shtedritski, Ralph Ginger, Ali Essa Abboud, Justin Ghareeb, Samuel G Booth, Rodriques, arXiv:2310.10632Bioplanner: automatic evaluation of llms on protocol planning in biology. 2023arXiv preprint</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th annual acm symposium on user interface software and technology. the 36th annual acm symposium on user interface software and technology2023</p>
<p>A study of generative large language model for medical research and healthcare. Cheng Peng, Xi Yang, Aokun Chen, Kaleb E Smith, Nima Pournejatian, Anthony B Costa, Cheryl Martin, Mona G Flores, Ying Zhang, Tanja Magoc, NPJ digital medicine. 612102023</p>
<p>Clinical decision support for bipolar depression using large language models. Joseph F Roy H Perlis, Michael J Goldberg, Christopher D Ostacher, Schneck, Neuropsychopharmacology. 2024</p>
<p>Humanity's last exam. Long Phan, Alice Gatti, Etc, arXiv:2501.142492025Preprint</p>
<p>Science in the era of chatgpt, large language models and generative ai. KI-Kritik/AI Critique Volume. Evangelos Pournaras, 20236275</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>A review of large language models and autonomous agents in chemistry. Christopher J Mayk Caldas Ramos, Andrew D Collison, White, 2025Chemical Science</p>
<p>Reinforcement learning in cold atom experiments. Malte Reinschmidt, József Fortágh, Andreas Günther, Valentin V Volchkov, Nature Communications. 15185322024</p>
<p>Biodiscoveryagent: An ai agent for designing genetic perturbation experiments. Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander Marson, Percy Liang, Jure Leskovec, arXiv:2405.176312024arXiv preprint</p>
<p>Anh Nguyen Duc, Kari Systä, and Pekka Abrahamsson. 2024. System for systematic literature review using multiple ai agents: Concept and an empirical evaluation. Abdul Malik Sami, Zeeshan Rasheed, Kai-Kristian Kemell, Muhammad Waseem, Terhi Kilamo, Mika Saari, arXiv:2403.08399Preprint</p>
<p>A meta-reinforcement learning algorithm for causal discovery. W M Andreas, Erman Sauter, Vincent Acar, -Lavet Francois, Conference on Causal Learning and Reasoning. PMLR2023</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202336</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Shortcutsbench: A large-scale real-world benchmark for api-based agents. Haiyang Shen, Yue Li, Desong Meng, Dongqi Cai, Sheng Qi, Li Zhang, Mengwei Xu, Yun Ma, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, Yueting Zhuang, Advances in Neural Information Processing Systems. 2024a36</p>
<p>Taskbench: Benchmarking large language models for task automation. Yongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, Yueting Zhuang, arXiv:2311.187602024bPreprint</p>
<p>Zhuocheng Shen, arXiv:2409.18807Llm with tools: A survey. 2024arXiv preprint</p>
<p>Chemreasoner: Heuristic search over a large language model's knowledge space using quantum-chemical feedback. Carl Henry W Sprueill, Khushbu Edwards, Agarwal, Udishnu Mariefel V Olarte, Conrad Sanyal, Hongbin Johnston, Heng Liu, Sutanay Ji, Choudhury, arXiv:2402.109802024arXiv preprint</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.094032024arXiv preprint</p>
<p>Scieval: A multi-level large language model evaluation benchmark for scientific research. Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu, 10.1609/AAAI.V38I17.29872Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence. Vancouver, CanadaAAAI Press2024a. February 20-27, 20242014</p>
<p>Zechang Sun, Yuan-Sen, Yaobo Ting, Nan Liang, Song Duan, Zheng Huang, Cai, arXiv:2409.14807Interpreting multi-band galaxy observations with large language model-based agents. 2024barXiv preprint</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. 2023. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. 6247990</p>
<p>Medagents: Large language models as collaborators for zero-shot medical reasoning. Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein, arXiv:2311.105372023arXiv preprint</p>
<p>Hybrid approaches for moral value alignment in ai agents: a manifesto. Elizaveta Tennant, Stephen Hailes, Mirco Musolesi, arXiv:2312.018182025Preprint</p>
<p>David Thulke, Yingbo Gao, Petrus Pelser, Rein Brune, Rricha Jalota, Floris Fok, Michael Ramos, Ian Van Wyk, Abdallah Nasir, Hayden Goldstein, arXiv:2401.09646Climategpt: Towards ai synthesizing interdisciplinary research on climate change. 2024arXiv preprint</p>
<p>Minyang Tian, Luyu Gao, Dylan Shizhuo, Xinan Zhang, Cunwei Chen, Xuefei Fan, Roland Guo, Pan Haas, Kittithat Ji, Yao Krongchon, Shengyan Li, Di Liu, Yutao Luo, Hao Ma, Kha Tong, Chenyu Trinh, Zihan Tian, Bohao Wang, Yanyu Wu, Shengzhu Xiong, Minhui Yin, Kilian Zhu, Yanxin Lieret, Genglin Lu, Yufeng Liu, Tianhua Du, Tao, arXiv:2407.13168Scicode: A research coding benchmark curated by scientists. Jamie CallanEliu Huerta, and Hao Peng2024Preprint</p>
<p>Cunshi Wang, Xinjie Hu, Yu Zhang, Xunhao Chen, Pengliang Du, Yiming Mao, Rui Wang, Yuyang Li, Ying Wu, Hang Yang, arXiv:2412.06412Starwhisper telescope: Agent-based observation assistant system to approach ai astrophysicist. 2024aarXiv preprint</p>
<p>A survey on large language model based autonomous agents. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Frontiers of Computer Science. 1861863452024b</p>
<p>Scimon: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/V1/2024.ACL-LONG.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics2024c. August 11-16, 20241ACL 2024</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, Wei Wang, Forty-first International Conference on Machine Learning, ICML 2024. Vienna, Austria2024d. July 21-27, 2024OpenReview.net</p>
<p>Guidance for researchers and peerreviewers on the ethical use of large language models (llms) in scientific research workflows. Ryan Watkins, AI and Ethics. 442024</p>
<p>Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, arXiv:2309.07864The rise and potential of large language model based agents: A survey. 2023arXiv preprint</p>
<p>Naturelm: Deciphering the language of nature for scientific discovery. Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, arXiv:2502.075272025arXiv preprint</p>
<p>Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, Guanbin Li, arXiv:2402.15116Large multimodal agents: A survey. 2024arXiv preprint</p>
<p>Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, arXiv:2502.12110A-mem: Agentic memory for llm agents. Juntao Tan, and Yongfeng Zhang. 2025arXiv preprint</p>
<p>Auto-gpt for online decision making: Benchmarks and additional opinions. Hui Yang, Sifu Yue, Yunzhong He, arXiv:2306.022242023arXiv preprint</p>
<p>Poisoning medical knowledge using large language models. Junwei Yang, Hanwen Xu, Srbuhi Mirzoyan, Tong Chen, Zixuan Liu, Zequn Liu, Wei Ju, Luchen Liu, Zhiping Xiao, Ming Zhang, Nature Machine Intelligence. 6102024a</p>
<p>Logicsolver: Towards interpretable math word problem solving with logical prompt-enhanced learning. Zhicheng Yang, Jinghui Qin, Jiaqi Chen, Liang Lin, Xiaodan Liang, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Moosechem: Large language models for rediscovering unseen chemistry scientific hypotheses. Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou, arXiv:2410.070762024bPreprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng, arXiv:2401.10334Drugassist: A large language model for molecule optimization. 2023aarXiv preprint</p>
<p>Amadeusgpt: a natural language interface for interactive animal behavioral analysis. Advances in neural information processing systems. Shaokai Ye, Jessy Lauer, Mu Zhou, Alexander Mathis, Mackenzie Mathis, 2023b36</p>
<p>Ruihong Zeng, Jinyuan Fang, Siwei Liu, Zaiqiao Meng, arXiv:2412.15266On the structural memory of llm agents. 2024arXiv preprint</p>
<p>HoneyComb: A flexible LLMbased agent system for materials science. Huan Zhang, Yu Song, Ziyu Hou, Santiago Miret, Bang Liu, 10.18653/v1/2024.findings-emnlp.192Findings of the Association for Computational Linguistics: EMNLP 2024. Miami, Florida, USAAssociation for Computational Linguistics2024a</p>
<p>Geoeval: Benchmark for evaluating llms and multi-modal models on geometry problem-solving. Jiaxin Zhang, Zhongzhi Li, Ming-Liang Zhang, Fei Yin, Cheng-Lin, Yashar Liu, Moshfeghi, 10.18653/V1/2024.FINDINGS-ACL.73Findings of the Association for Computational Linguistics, ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024b. 2024and virtual meeting, August 11-16</p>
<p>Shiqi Zhang, Xinbei Ma, Zouying Cao, Zhuosheng Zhang, Hai Zhao, arXiv:2502.14563Plan-over-graph: Towards parallelable llm agent schedule. 2025arXiv preprint</p>
<p>Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, Kaifu Zhang, arXiv:2411.14405Marco-o1: Towards open reasoning models for open-ended solutions. 2024aarXiv preprint</p>
<p>Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, arXiv:2401.14818Chemdfm: Dialogue foundation model for chemistry. 2024barXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
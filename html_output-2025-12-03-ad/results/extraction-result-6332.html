<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6332 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6332</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6332</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-125.html">extraction-schema-125</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language model agents that use memory to solve text‑based games, including details about the memory type, representation, update mechanism, benchmark used, performance with and without memory, training method, and any reported challenges or ablations.</div>
                <p><strong>Paper ID:</strong> paper-269457384</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2404.19065v1.pdf" target="_blank">HELPER-X: A Unified Instructable Embodied Agent to Tackle Four Interactive Vision-Language Domains with Memory-Augmented Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Recent research on instructable agents has used memory-augmented Large Language Models (LLMs) as task planners, a technique that retrieves language-program examples relevant to the input instruction and uses them as in-context examples in the LLM prompt to improve the performance of the LLM in inferring the correct action and task plans. In this technical report, we extend the capabilities of HELPER, by expanding its memory with a wider array of examples and prompts, and by integrating additional APIs for asking questions. This simple expansion of HELPER into a shared memory enables the agent to work across the domains of executing plans from dialogue, natural language instruction following, active question asking, and commonsense room reorganization. We evaluate the agent on four diverse interactive visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across these benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6332",
    "paper_id": "paper-269457384",
    "extraction_schema_id": "extraction-schema-125",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0064185,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HELPER-X: A UNIFIED INSTRUCTABLE EMBOD-IED AGENT TO TACKLE FOUR INTERACTIVE VISION-LANGUAGE DOMAINS WITH MEMORY-AUGMENTED LANGUAGE MODELS
29 Apr 2024</p>
<p>Gabriel Sarch 
equal contribution
Carnegie Mellon University</p>
<p>Sahil Somani 
equal contribution
Carnegie Mellon University</p>
<p>Raghav Kapoor 
equal contribution
Carnegie Mellon University</p>
<p>Michael J Tarr 
equal contribution
Carnegie Mellon University</p>
<p>Katerina Fragkiadaki 
equal contribution
Carnegie Mellon University</p>
<p>HELPER-X: A UNIFIED INSTRUCTABLE EMBOD-IED AGENT TO TACKLE FOUR INTERACTIVE VISION-LANGUAGE DOMAINS WITH MEMORY-AUGMENTED LANGUAGE MODELS
29 Apr 2024421477734FE4C6624E4F8C99C6D6DF5AarXiv:2404.19065v1[cs.AI]Accepted at 2024 ICLR 2024 Workshop on LLM Agents
Recent research on instructable agents has used memory-augmented Large Language Models (LLMs) as task planners, a technique that retrieves language-program examples relevant to the input instruction and uses them as in-context examples in the LLM prompt to improve the performance of the LLM in inferring the correct action and task plans.In this technical report, we extend the capabilities of HELPER, by expanding its memory with a wider array of examples and prompts, and by integrating additional APIs for asking questions.This simple expansion of HELPER into a shared memory enables the agent to work across the domains of executing plans from dialogue, natural language instruction following, active question asking, and commonsense room reorganization.We evaluate the agent on four diverse interactive visual-language embodied agent benchmarks: AL-FRED, TEACh, DialFRED, and the Tidy Task.HELPER-X achieves few-shot, state-of-the-art performance across these benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training.</p>
<p>INTRODUCTION</p>
<p>A typical way to adapt LLMs to downstream applications is through prompting (Brown, 2020;Alayrac, 2022;Liu, 2022b;Hongjin, 2022;Mishra, 2022;Wei, 2021;Song, 2022b), exploiting their strong in-context and few-shot learning abilities.When the amount of in-context examples and task descriptions necessary to cover the task constraints increases, inference costs significantly rise due to additional attention operations.To handle computational challenges and LLM context length, a growing body of research explores the concept of "memory-augmented prompting" -a method that involves retrieving a set of pertinent in-context examples to append to the prompt, thereby broadening their applicability (Perez, 2021;Schick, 2021;Gao, 2020;Liu, 2022c;Song, 2023;Sarch, 2023;Lewis, 2020;Mao, 2021).HELPER Sarch (2023) retrieves a set of language-program examples based on the user's input instruction and adds them to the prompt to provide contextualized examples for GPT-4 task planning.</p>
<p>Figure 1: TEACh-tailored HELPER Sarch (2023) demonstrates a 6.9% drop in success when applied to ALFRED, despite sharing the same action space and environments, due to variations in language inputs and tasks.</p>
<p>HELPER-X consistently performs well in both domains with one model.</p>
<p>Despite GPT-4's robust generalization, memory-augmented prompting tailored for one domain does not guarantee high performance in a similar yet distinct domain.Applying HELPER, prompted with TEACh-specific examples and descriptions, to ALFRED -a related domain that shares the same action space and environments but differs in language inputs and task types -results in a notable 6.9% decrease in accuracy compared to a HELPER model with a specialized prompt and customized example memory specifically for AL-FRED, and vice versa when doing the same on TEACh (3.2% decrease), as shown in Figure 1 (right).</p>
<p>We report that two simple extensions of HELPER that allows for strong performance across four domains, by expanding its memory with a wider array of examples and prompts, and by integrating a additional APIs for asking questions.Specifically, we introduce two HELPER-X variants: HELPER-X P , which retrieves domain-specific prompt templates and related in-context examples for the LLM, and HELPER-X S , which retrieves in-context examples from a shared memory under a single prompt template.</p>
<p>We evaluate HELPER-X across four domains that include dialogue-based task completion on TEACh (Padmakumar, 2021), following instructions from natural language on AL-FRED (Shridhar, 2020), engaging in instruction following with active question asking on DialFRED (Gao, 2022), and organizing rooms using spatial commonsense reasoning in the Tidy Task (Sarch, 2022).HELPER-X demonstrates stateof-the-art performance in the few-shot example domain, that is, without in-domain training.Extending the language-program memory does not cause interference and does not hinder performance.In fact, HELPER-X matches, and sometimes even exceeds, the performance of agents prompted with a single-domain in mind.</p>
<p>HELPER-X</p>
<p>We extend HELPER (Sarch, 2023) to work across four domains.We propose two versions to extend the memory-augmented prompting of LLMs in HELPER: 1) HELPER-X P that retrieves from a memory of domain-tailored prompt templates and associated domain-specific examples (Section 2.2.1), and 2) HELPER-X S that expands the memory of HELPER into a shared memory of in-context examples across domains combined with a domain-agnostic prompt template (Section 2.2.2).</p>
<p>Additionally, we extend the capabilities of HELPER for question asking, by appending a question API with functions defining possible questions and their arguments to the LLM prompt (Section 2.2.3).We use HELPER (Sarch, 2023) for execution of the generated program using standard perception modules.</p>
<p>BACKGROUND</p>
<p>Here, we give an account of HELPER to make the paper self-contained.</p>
<p>HELPER prompts an LLM, namely GPT-4 (gpt, 2023), to generate plans as Python programs.It assumes that the agent has access to a set of action skills S (e.g., go_to(X), pickup(X), etc.).HELPER adds these skills to the prompt in the form of a Python API.The LLM is instructed only to call these pre-defined skill functions in its generated programs.HELPER considers a key-value memory of language inputs and successful program pairs.It retrieves a set of in-context examples relevant to the current input language to add to the prompt to assist program generation.Each key is encoded into a language embedding.The top-k language-program pairs are retrieved based on their euclidean distance to the encoding of the language input I encoding.The HELPER prompt also contains a role description ("You are a helpful assistant with expertise in..."), a task description ("Your task is to ...") and guidelines to follow for program generation ("You should only use functions in the API..."), that are commonly tailored to the domain-of-interest.</p>
<p>HELPER (Sarch, 2023), using RGB input, estimates depth maps, object masks, and agent egomotion at each timestep.This facilitates the creation and upkeep of a 3D occupancy map and an object memory database, essential for obstacle navigation and object tracking.Object detection in each frame leads to instance aggregation based on 3D centroid proximity, with each instance characterized by dynamic state attributes (e.g., cooked, sliced, dirty).When an action fails, a Vision-Language Model (CLIP (Radford, 2021)) provides feedback, prompting the LLM to re-plan.For objects not present in the map, the LLM suggests areas for HELPER to search (e.g., "near the sink").</p>
<p>UNIFIED MEMORY-AUGMENTED PROMPTING</p>
<p>We explore two ways to expand HELPER to work across four domains, either through prompt retrieval (Section 2.2.1) or through a shared example memory (Section 2.2.2).</p>
<p>PROMPT RETRIEVAL</p>
<p>Given an input language instruction I, the prompt retrieval agent HELPER-X P retrieves a specialized prompt template P and an associated set of specialized examples E. Each specialized prompt template contains role descriptions, task instructions and guidelines tailored to each domain, namely, dialogue-based task completion (based on TEACh (Padmakumar, 2021)), instruction following from natural language (based on ALFRED (Shridhar, 2020)), instruction following with active question asking (based on DialFRED (Gao, 2022)), or tidying up rooms (based on Tidy Task (Sarch, 2022)).For retrieval, a query is generated from the input instruction by encoding the instruction into an embedding vector using a pre-trained Figure 2: Illustration of the shared example memory (HELPER-XS; top) and the prompt retrieval (HELPER-XP ; bottom).The memory is shared across domains in both versions, allowing language and task inputs from any of the domains.language model (Greene, 2022).The query retrieves the closest key from memory, where each key represents the language encodings of each prompt template and example text , as shown in Figure 2. The top-k in-context examples are further retrieved from the specialized set of examples associated with the retrieved prompt and added to the retrieved prompt template, and the resulting prompt is used for LLM's program generation.</p>
<p>SHARED EXAMPLE MEMORY</p>
<p>Given an input language instruction I, the shared example memory agent HELPER-X S retrieves a set of in-context examples from a shared memory that includes in-context examples from all domains considered.These examples are added to a domain-agnostic prompt template that does not have a specialized role description, task instructions or guidelines for any single domain.A query is generated from the input instruction by encoding it into an embedding vector using a pre-trained language model (Greene, 2022).The keys represent encodings of each in-context example language in the shared memory.The query embedding retrieves the top-k nearest neighbors keys and their values.These are added to the prompt as relevant in-context examples for LLM program generation, as shown in Figure 2.</p>
<p>QUESTION ASKING API</p>
<p>A natural limitation of asking questions in a simulator is that only certain types of questions can be understood and answered.We constrained the set of possible questions asked by the agent by defining an API of available questions in the Dial-FRED (Gao, 2022) benchmark and their arguments that HELPER-X can call on to gather more information.These include questions in three categories-Location, Appearance, and Direction-pertaining to the agent's next interaction object.Importantly, this API can be continuously expanded by adding an additional function to the question-asking API.</p>
<p>Implementation details.</p>
<p>We follow the network implementation of HELPER (Sarch, 2023).We use GPT-4-0613 (gpt, 2023) for text generation and text-embedding-ada-002 (Greene, 2022) for text embeddings.We use the SOLQ object detector (Dong, 2021) and ZoeDepth network (Bhat, 2023) for depth estimation from RGB input.We use k = 3 for example retrieval.</p>
<p>EXPERIMENTS</p>
<p>We test HELPER-X in the following benchmarks: 1. Inferring and executing action plans from dialogue (TEACh (Padmakumar, 2021)), 2. Inferring and executing action plans from instructions (ALFRED (Shridhar, 2020)), 3. Active question asking for seeking help during instruction execution (DialFRED (Gao, 2022)), and 4. Tidying up rooms (Tidy Task (Sarch, 2022)).</p>
<p>INFERRING AND EXECUTING ACTION PLANS FROM DIALOGUE</p>
<p>Inferring and executing task plans from dialogue involves understanding dialogue segments and executing related instructions using the provided information in the dialogue.This task evaluates the agent's ability in understanding noisy and free-form conversations between two humans discussing about a household task.</p>
<p>Dataset We use the TEACh benchmark (Padmakumar, 2021), which consists of over 3,000 dialogues focused on household tasks in the AI2-THOR environment Kolve (2017).We use the Trajectory from Dialogue (TfD) variant, where an agent, given a dialogue segment, must infer action sequences to fulfill tasks like making a coffee or preparing a salad.The training dataset contains 1482 expert demonstrations with associated dialogues.The evaluation includes 181 'seen' and 612 'unseen' episodes, with 'seen' having different object placements and states than in training, and 'unseen' also having different object instances and room environments.The agent receives an egocentric image at each step and selects actions to execute, such as pickup(X), turn_left(), etc.</p>
<p>Baselines We consider two kinds of baselines: 1. Methods that supervise low-level or high-level action prediction from language and visual input using expert demonstrations in the training set (1482 in number) (Pashevich, 2021;Zheng, 2022;Min, 2021;Zhang, 2022), and 2. Methods that use a small amount of expert demonstrations for prompting pretrained LLM (Sarch, 2023).Specifically, HELPER and HELPER-X use 11 domain-specific examples.We additionally include a comparison to HELPER-ALF, which uses specialized prompts and examples for ALFRED.</p>
<p>Evaluation Metrics</p>
<p>We follow the TEACh evaluation metrics.Task success rate (SR) is a binary metric of whether all subtasks were successfully completed.Goal condition success rate (GC) quantifies the proportion of achieved goal conditions across all sessions.Both SR and GC have path-length weighted variants weighted by (path length of the expert trajectory) / (path length taken by the agent).</p>
<p>Results are reported in Table 1.On validation unseen, HELPER-X S and HELPER-X P demonstrate performance on-par with HELPER, with HELPER-X S even slightly outperforming HELPER, despite HELPER-X being shared across four domains.HELPER-X also outperforms the best supervised baselines trained in-domain with many demonstrations.On validation seen, both HELPER-X variants outperform HELPER, with the best model HELPER-X P outperforming HELPER by 2.7% in success rate.. JARVIS (Zheng, 2022) 1.8 (0.3) 3.1 (1.6) 1.7 (0.2) 5.4 (4.5) FILM (Min, 2022) 2.9 (1.0) 6.1 (2.5) 5.5 (2.6) 5.8 (11.6)DANLI (Zhang, 2022) 8.0 (3.2) 6.8 (6.6) 5.0 (1.9) 10.5 (10.3)ECLAIR (Kim, 2023) 13.2 ---FS HELPER-ALF 10.5 (1.8) 10.3 (4.5) 13.3 (2.8) 14.2 (7.5) HELPER (Sarch, 2023) 13.7 (1.6) 14.2 (4.6) 12.2 (1.8) 18.6 (9.3) G+FS HELPER-XP 13.6 (2.0) 13.6 (5.6) 14.9 (3.6) 20.3 (11.0)HELPER-XS 14.5 (2.1) 14.0 (5.4) 14.4 (3.5) 19.9 (11.0)</p>
<p>FOLLOWING NATURAL LANGUAGE INSTRUCTIONS</p>
<p>Natural language instruction following evaluates the agent's ability to carry out high-level instructions ("Rinse off a mug and place it in the coffee maker") and low-level ones ("Walk to the coffee maker on the right") provided by a human user.Importantly, the language and tasks in this evaluation differ from the ones in the TEACh benchmark (Section 3.1).</p>
<p>Dataset Te ALFRED (Shridhar, 2020) is a vision-and-language navigation benchmark designed for embodied agents to execute tasks in domestic settings from RGB sensory input.It includes seven task types across 207 environments, that involve 115 object types in 4,703 task instances, varying from simple object relocation to placing a heated item in a receptacle.The dataset includes detailed human-authored instructions and high-level goals, based on 21,023 expert demonstrations.It also comprises 820 'seen' and 821 'unseen' validation episodes.Agents receive egocentric RGB images at each step and select actions from a predefined set to progress, such as pickup(X), turn_left(), etc.</p>
<p>Baselines Again, we consider two sets of baselines: those that supervise lowlevel or high-level action prediction using the expert demonstrations in the training set (Pashevich, 2021;Zheng, 2022;Min, 2021;Zhang, 2022;2021;Song, 2022a;Blukis, 2022;Bhambri, 2023;Liu, 2022a;Murray, 2022;Inoue, 2022), and those that use a small amount of demonstrations (&lt;= 100) (few-shot) (Sarch, 2023;Song, 2023;Brohan, 2023;Liu, 2023).The SayCan (Brohan, 2023), FILM (Min, 2021) (FS), and HLSM (Blukis, 2022) (FS) few shot baselines are adapted for the few-shot ALFRED setting by the authors of Song (2023), where the planning modules in FILM and HLSM are re-trained using only 100 demonstrations.We adapt HELPER (Sarch, 2023) as a baseline with specialized prompts and examples for ALFRED, as well as a comparison to HELPER-TEACh, which uses specialized prompts and examples for TEACh.HELPER and our HELPER-X model each use 7 domain-specific examples in their memory.</p>
<p>Evaluation Metrics We follow the ALFRED evaluation metrics: 1. Task success rate (SR) and 2. Goal condition success rate (GC).These are defined the same as in TEACh (Section 3.1).</p>
<p>Results are reported in Table 2. Our conclusions are similar to Section 3.1.On validation unseen, HELPER-X S and HELPER-X P demonstrate performance on-par with HELPER, with HELPER-X P marginally outperforming HELPER by 1.0%, despite.HELPER-X is also competitive with the best supervised baselines, despite only requiring a few in-domain demonstrations.On validation seen, we observe both HELPER-X models marginally outperforming HELPER.We additionally show that using HELPER-TEACh which has prompts and examples for a different domain (TEACh) causes a significant 6.9% drop in performance.(Zhang, 2021) 12.4 (6.9) 23.7 (12.0) 25.2 (12.2) 34.9 (18.5)M-TRACK (Song, 2022a) 17  (Brohan, 2023) 9.9 22.5 12.3 24.5 LLM-Planner (Song, 2023) 15.4 23.4 16.5 30.1 HELPER-TEACh 27.5 (5.9) 44.3 (10.1) 24.5 (6.3) 38.2 (11.2) HELPER (Sarch, 2023) 34.4 (7.6) 51.5 (11.9) 27.6 (7.4) 42.0 (12.7)G+FS HELPER-XP 35.4 (7.9) 52.9 (12.3) 28.2 (7.5) 42.5 (12.9)HELPER-XS 34.0 (7.5) 51.1 (11.9) 28.0 (7.4) 42.1 (12.8)</p>
<p>INSTRUCTION FOLLOWING WITH ASKING QUESTIONS</p>
<p>Question asking instruction following allows the agent to choose to ask questions to an oracle to gain additional information to help it complete a task defined by an initial natural language instruction.</p>
<p>Dataset The DialFRED benchmark (Gao, 2022) enables an agent to query users while executing language instructions, utilizing user responses for task improvement.It features a human-annotated dataset with 53K relevant questions and answers, plus an oracle for responding to agent queries.Agents can ask questions in three cate-gories-Location, Appearance, and Direction-pertaining to their next interaction object.The dataset covers 25 task types across 207 environments, 115 object types, and includes 'seen' and 'unseen' episodes.The agent receives egocentric RGB images at each step and selects actions from a set, like pickup(X), turn_left(), etc.This benchmark's instructions and tasks are distinct from TEACh and partially overlap with ALFRED, with significant modifications and 18 new task types.</p>
<p>Questioning Implementation To ask questions in the DialFRED task, we add to the question asking API functions to query the oracle in DialFRED (see Section 2.2.3).HELPER-X asks questions when it does not know the location of an object required for the task at hand.Unlike ALFRED, success in DialFRED requires interacting with a specific instance of an object class.To account for this, HELPER-X also asks questions to help disambiguate when it has seen multiple instances of the same object.</p>
<p>Baselines We compare with the baselines in the DialFRED paper (Gao, 2022), which includes a sequence-to-sequence architecture for choosing to ask a question, trained with reinforcement learning, and the Episodic Transformer architecture (Pashevich, 2021), trained with behavioral cloning.We adapt HELPER (Sarch, 2023) as a baseline with specialized prompts and examples for DialFRED, as well as our question asking API.We consider a few shot setting with each few shot model receiving 7 domain-specific examples.</p>
<p>Evaluation Metrics We follow the conventions of the DialFRED benchmark.We use the Task success rate (SR) metric.This is defined the same as TEACh (Section 3.1).</p>
<p>Results are reported in Table 3.On validation unseen, we observe HELPER-X S marginally outperforming HELPER by 0.38 points in success rate, despite HELPER-X being shared across all domains.While HELPER-X is outperformed by the best supervised baselines, HELPER-X only requires a few in-domain demonstrations compared to the thousands of language-action demonstrations and RL interactions needed to train the baseline models.Most importantly, we see the addition of question-asking in HELPER-X improves success rate by 2.48 points, highlighting its efficiency in question selection and response utilization.Tidying up involves figuring out where to place items without explicit instructions, relying on spatial commonsense to infer a proper location for an object.This task tests the agent's ability to use commonsense reasoning regarding contextual, object-object, and object-room spatial relations.</p>
<p>Dataset We evaluate on the Tidy Task (Sarch, 2022) benchmark, where the agent is spawned in a disorganized room, and must reposition objects to bring them to an organized tidy state.The dataset consists of 8000 training, 200 validation, and 100 testing messy configurations in 120 distinct scenes of bedrooms, living rooms, kitchens and bathrooms.At each time step, the agent obtains an egocentric RGB and depth image and must choose an action from a specified set to transition to the next step, such as pickup(X), turn_left(), etc.In this setup, the models are prompted to tidy up the room, given a set of objects that are out of place obtained using the visual detector from TIDEE (Sarch, 2022).</p>
<p>Baselines We compare against TIDEE (Sarch, 2022), which includes a graph neural network encoding common object arrangements.This is supervised in the training set of the Tidy Task to predict where a target object should be re-positioned to in the current scene.We adapt HELPER (Sarch, 2023) as a baseline with specialized prompts and examples for the Tidy Task.We additionally include a random receptacle baseline which chooses random receptacle placement locations for the out of place objects.We consider a few shot setting with each few shot model receiving 3 domain-specific examples.</p>
<p>Evaluation Metrics We use the following evaluation metrics for the Tidy Task: Correctly Moved (CM) Average number of correctly moved objects that are out of place in the scene, and moved by the agent.Higher is better.Incorrectly Moved (IM) Average number of incorrectly moved objects that are not out of place, but were moved by the agent.Lower is better.Energy The "cleaniness" energy, where lower energy represents a higher likelihood of the room object configuration aligning with the configurations in the organized AI2THOR rooms.Following ProcThor (Deitke, 2022), for each receptacle object, the probability that each object type appears on its surface is computed across the AI2THOR scenes.See the Appendix for more details.</p>
<p>Results are in Table 4. On the Tidy Task, HELPER-X S and HELPER-X P demonstrates performance on-par with HELPER.HELPER-X does significantly better than if object locations are randomly placed (Random Receptacle).We find that the supervised baseline, TIDEE, outperforms HELPER-X, especially in the Energy metric, revealing that in-domain training on this benchmark is helpful for learning the common object configurations within the AI2THOR environments.However, we find that HELPER-X accomplishes the task in significantly fewer steps compared to TIDEE.</p>
<p>CONCLUSION</p>
<p>We introduce HELPER-X, an embodied agent that executes tasks from dialogue or language instructions, ask questions, and tides up rooms.HELPER-X has two variants, HELPER-X P and HELPER-X S , enhancing HELPER's memory capabilities.HELPER-X P retrieves domain-specific templates and examples for large language models, while HELPER-X S retrieves only examples for a domain-agnostic prompt template through a shared memory.Evaluation of HELPER-X in four domains: TEACh, ALFRED, DialFRED, and the Tidy Task, yields state-of-the-art performance in the few-shot example setting.Memory and API expansions we considered maintained or improved performance for the LLM, highlighting the effectiveness of memory-enhanced LLMs in building versatile, instructable agents.</p>
<p>RELATED WORK</p>
<p>MEMORY-AUGMENTED PROMPTING OF LARGE LANGUAGE MODELS</p>
<p>Recently, external memories have been instrumental in scaling language models Borgeaud (2021); Khandelwal (2019), overcoming the constraints of limited context windows in parametric transformers Wu (2022).They also facilitate knowledge storage in various forms such as entity mentions de Jong ( 2021), knowledge graphs Das (2022), and question-answer pairs Chen (2022).Retrieval-augmented generation (RAG) (Lewis, 2020;Mao, 2021) has been shown to significantly improve response quality in large language models (LLMs) by integrating external knowledge sources with the model's internal representations.In agent-based domains, memory-augmented prompting has enhanced task planning in embodied instructional contexts (Song, 2023;Sarch, 2023) and open-world gaming (Wang, 2023b;a;Majumder, 2023).Our model, HELPER-X, employs memory-augmented prompting across four benchmarks, demonstrating that memory expansion across related domains can maintain performance.</p>
<p>INSTRUCTABLE EMBODIED AGENTS THAT INTERACT WITH THEIR ENVIRONMENTS</p>
<p>Numerous benchmarks assess embodied vision-and-language tasks, with significant advancements in learning-based embodied AI agents across tasks like scene rearrangement Gan ( 2022 (Ku, 2020;Krantz, 2023), audio-visual navigation (Chen, 2020), interactive dialogue and natural language instruction following (Yenamandra, 2023;Shridhar, 2020;Padmakumar, 2021;Gao, 2023), and embodied commonsense reasoning (Kant, 2022;Sarch, 2022;Wu, 2023).Interactive instruction benchmarks (e.g., ALFRED (Shridhar, 2020) and TEACh (Padmakumar, 2021)) require agents to follow natural language directives and dialogue, identifying objects in scenes via interaction masks.Variants like Dial-FRED (Gao, 2022) allow agent inquiries about objects and locations.Benchmarks such as TIDEE (Sarch, 2022) and HouseKeep (Kant, 2022) test agents' ability to tidy rooms using commonsense, without explicit object placement directives.Unlike most methods confined to a single domain, our work focuses on creating a multidomain agent adept in dialogue-based task planning, natural language instruction following, asking questions for disambiguation of instructions, and tidying up scenes.</p>
<p>Our method shows competitive performance across the four domains with a few task-specific demonstrations and without domain-specific weights, beyond the single image object detector.</p>
<p>Interactive vision-language embodied agent methods train distinct agents for each language-defined task, using large datasets from expert demonstrations (Min, 2021;Inoue, 2022;Zhang, 2022;Kim, 2023;Pashevich, 2021).Some approaches use these demonstrations for end-to-end network training to directly predict actions from observations (Pashevich, 2021;Gao, 2022;Zhang, 2021).Others employ modular methods, training planners to generate subgoals handled by specialized perception, manipulation, and obstacle avoidance modules (Min, 2021;Inoue, 2022;Blukis, 2022;Zheng, 2022;Kim, 2023;Bhambri, 2023;Liu, 2022a;Murray, 2022;Liu, 2023).However, these methods often over-specialize to specific datasets and tasks, limited by the training domain's language and task structure.In contrast, our method performs competitively across multiple benchmarks with minimal task-specific demonstrations and without needing domain-specific networks.1. Task planning from multimodal input: Currently, our LLM receives the environment's state only in case of a failure, through VLM feedback.Integrating the visual state of the environment in a more direct way may dramatically increase the accuracy of predicted plans.This direction aligns with recent work (Wang, 2023b;Mu, 2023;Yang, 2023) that uses visual features as input to language models.</p>
<ol>
<li>
<p>Cost of GPT-4: While GPT-4 is the most accurate Large Language Model, its high cost necessitates exploring alternatives such as open-source models, hardware optimization, model compression or distillation of its knowledge to smaller models, to reduce expenses.</p>
</li>
<li>
<p>Manual Addition of Domains: Our model supports four domains with shared examples and prompts, but manual intervention is needed for adding significantly different domains and tasks.Future developments should focus on automating the detection and integration of out-of-domain inputs.</p>
</li>
</ol>
<p>S2 PROMPTS S2.1 PROMPT TEMPLATES FOR PROMPT RETRIEVAL</p>
<p>In the prompt retrieval experiments, we include four prompt templates to be retrieved.These templates are shown for TEACh, ALFRED, Dialfred, and the Tidy Task in Listing S1, Listing S2, Listing S3, Listing S4, respectively.In the DialFRED benchmark, when HELPER-X is unable to find an object, it is able to ask one of three question types in order to aid itself.In a real-world scenario, HELPER-X could take advantage of the LLM's capability to ask many types of questions, but the DialFRED benchmark limits us to three: direction, location, and appearance.</p>
<p>S3.2 QUESTION ASKING PIPELINE</p>
<p>When HELPER-X does not have an object's location already in its memory or multiple instances of an objects exist in the memory, it forms a prompt with its current context and the API of available questions, as in Listing S9.Based on the context, HELPER-X then chooses and asks the most appropriate question.The returned answer and an API of search related actions, alongside the context and question, are then formed into another prompt, seen in Listing S10.Finally, this prompt is parsed by HELPER-X into an action script to search for the object.</p>
<p>Examples of this full pipeline for if an object does not exist in the memory are in Listing S11 and Listing S12.</p>
<p>S4 PRE-CONDITIONS</p>
<p>An example of a pre-condition check for a macro-action is provided in Listing ??.</p>
<p>S5 EXAMPLE LLM INPUTS &amp; OUTPUTS</p>
<p>We provide examples of dialogue input, retrieved examples, and LLM output for a TEACh sample in Listing ??, Listing ??, and Listing ??.</p>
<p>S6 SIMULATION ENVIRONMENT</p>
<p>The TEACh dataset builds on the Ai2thor simulation environment (Kolve, 2017).</p>
<p>At each time step the agent may choose from the following actions: Forward(), Backward(), Turn Left(), Turn Right(), Look Up(), Look Down(), Strafe Left(), Strafe Right(), Pickup(X), Place(X), Open(X), Close(X), ToggleOn(X), ToggleOff(X), Slice(X), and Pour(X), where X refers an object specified via a relative coordinate (x, y) on the egocentric RGB frame.Navigation actions move the agent in discrete steps.We rotate in the yaw direction by 90 degrees, and rotate in the pitch direction by 30 degrees.The RGB and depth sensors are at a resolution of 480x480, a field of view of 90 degrees, and lie at a height of 0.9015 meters.The agent's coordinates are parameterized by a single (x, y, z) coordinate triplet with x and z corresponding to movement in the horizontal plane and y reserved for the vertical direction.The TEACh benchmark allows a maximum of 1000 steps and 30 API failures per episode.</p>
<p>S7 Executor DETAILS</p>
<p>S7.1 SEMANTIC MAPPING AND PLANNING</p>
<p>Obstacle map HELPER-X maintains a 2D overhead occupancy map of its environment ∈ R H×W that it updates at each time step from the input RGB-D stream.The map is used for exploration and navigation in the environment.</p>
<p>At every time step t, we unproject the input depth maps using intrinsic and extrinsic information of the camera to obtain a 3D occupancy map registered to the coordinate frame of the agent, similar to earlier navigation agents Chaplot (2019).The 2D overhead maps of obstacles and free space are computed by projecting the 3D occupancy along the height direction at multiple height levels and summing.For each input RGB image, we run a SOLQ object segmentor (Dong, 2021) (pretrained on COCO Lin (2014) then finetuned on TEACh rooms) to localize each of 116 semantic object categories.For failure detection, we use a simple matching approach from Min (2021) to compare RGB pixel values before and after taking an action.</p>
<p>Object location and state tracking We maintain an object memory as a list of object detection 3D centroids and their predicted semantic labels {[(X, Y, Z) i , ℓ i ∈ {1...N }], i = 1..K}, where K is the number of objects detected thus far.The object centroids are expressed with respect to the coordinate system of the agent, and, similar to the semantic maps, updated over time using egomotion.We track previously detected objects by their 3D centroid C ∈ R 3 .We estimate the centroid by taking the 3D point corresponding to the median depth within the segmentation mask and bring it to a common coordinate frame.We do a simple form of non-maximum suppression on the object memory, by comparing the euclidean distance of centroids in the memory to new detected centroids of the same category, and keep the one with the highest score if they fall within a distance threshold.</p>
<p>For each object in the object memory, we maintain an object state dictionary with a pre-defined list of attributes.These attributes include: category label, centroid location, holding, detection score, can use, sliced, toasted, clean, cooked.For the binary attributes, these are initialized by sending the object crop, defined by the detector mask, to the VLM model, and checking its match to each of [f"The {object_category} is {attribute}", f"The {object_category} is not {attribute}"].We found that initializing these attributes with the VLM gave only a marginal difference to initializing them to default values in the TEACh benchmark, so we do not use it for the TEACh evaluations.However, we anticipate a general method beyond dataset biases of TEACh would much benefit from such vision-based attribute classification.17.25 (7.16) 23.88 (19.38) 16.89 (9.12) 25.10 (22.56) 16.71 (7.33) 23.00 (20.55) 18.63 (9.41) 24.77 (21.90) HELPER 17.25 (3.22) 25.24 (8.12) 19.21 (4.72) 33.54 (10.95) 17.55 (2.59) 26.49 (7.67) 17.97 (3.44) 30.81 (8.93) Exploration and path planning HELP ER − X explores the scene using a classical mapping method.We take the initial position of the agent to be the center coordinate in the map.We rotate the agent in-place and use the observations to instantiate an initial map.Second, the agent incrementally completes the maps by randomly sampling an unexplored, traversible location based on the 2D occupancy map built so far, and then navigates to the sampled location, accumulating the new information into the maps at each time step.The number of observations collected at each point in the 2D occupancy map is thresholded to determine whether a given map location is explored or not.Unexplored positions are sampled until the environment has been fully explored, meaning that the number of unexplored points is fewer than a predefined threshold.</p>
<p>To navigate to a goal location, we compute the geodesic distance to the goal from all map locations using graph search ?given the top-down occupancy map and the goal location in the map.We then simulate action sequences and greedily take the action sequence which results in the largest reduction in geodesic distance.</p>
<p>S7.2 2D-TO-3D UNPROJECTION</p>
<p>For the i-th view, a 2D pixel coordinate (u, v) with depth z is unprojected and transformed to its coordinate (X, Y, Z) T in the reference frame:
(X, Y, Z, 1) = G −1 i z u − c x f x , z v − c y f y , z, 1 T(1)
where (f x , f y ) and (c x , c y ) are the focal lengths and center of the pinhole camera model and G i ∈ SE(3) is the camera pose for view i relative to the reference view.This module unprojects each depth image I i ∈ R H×W ×3 into a pointcloud in the reference frame P i ∈ R M i ×3 with M i being the number of pixels with an associated depth value.</p>
<p>S8 ADDITIONAL DETAILS OF THE TIDY TASK S8.1 METRIC DEFINITIONS IN THE TIDY TASK</p>
<p>The metrics in the original TIDEE paper (Sarch, 2022) require separate human evaluations on Amazon Mechanical Turk.We define a new set of metrics that does not require expensive annotations from humans for every evaluation.Below are detailed descriptions of each of the new metrics:</p>
<ol>
<li>Correctly Moved (CM) Average number of correctly moved objects that are out of place in the scene, and moved by the agent.Higher is better.2. Incorrectly Moved (IM) Average number of incorrectly moved objects that are not out of place, but were moved by the agent.Lower is better.3. Energy Following ProcThor (Deitke, 2022), for each receptacle object, the probability that each object type appears on its surface is computed across the AI2THOR scenes.Here, we compute the total number of times each object type is on the receptacle type and divide it by the total number of times the receptacle type appears across the scenes.The energy metric in the Tidy Task is defined as follows:</li>
</ol>
<p>(P cleanup − P original )/(P dirty − P original )</p>
<p>(2) where P cleanup , P dirty , and P original represent the sum of the object location probabilities for the cleaned up state of the room, the dirty/messy state of the room, and the original state of the room with objects put in-place by human designers, respectively.Lower is better.4. Steps Average number of steps taken by the agent per episode.</p>
<p>S8.2 LANGAUGE INSTRUCTIONS FOR THE TIDY TASK</p>
<p>Since the Tidy Task does not include natural language instruction annotations, we formulate the language instruction as the following to give to the HELPER baseline and HELPER-X: "Tidy up the house.These are the out of place objects: {detected_out_of_place_objects}.These are the receptacles in the current scene: {detected_receptacles}", where {detected_out_of_place_objects} are the objects classified as out of place, and {detected_receptacles} are any receptacle detected in the scene by the agent.</p>
<p>To obtain the list of out of place objects, we allow the agents use of the TIDEE (Sarch, 2022) visual detector to determine whether each object detected during the mapping phase is out of place.We found that out of place detection benefits significantly from visual detection in the Tidy Task, and thus we do not use an LLM for detecting the out of place attribute.Notably, adding the additional out of place attribute to the objects in the object memory can be shared across all benchmarks.</p>
<p>Listing S3: Prompt template for the Dialfred You are an excellent interpreter of instructions for household tasks.Given a task overview <High Level Goal> and step to perform <Low Level Goal>, you break the instructions down into a sequence of robotic actions.</p>
<p>{API}</p>
<p>Write a script using Python and the InteractionObject class and functions defined above that could be executed by a household robot.</p>
<p>Here are a few examples of typical inputs and outputs (only for in-context reference): {RETRIEVED_EXAMPLES}</p>
<p>Adhere to these stringent guidelines: 1. Use only the classes and functions defined previously.Do not create functions that are not provided above.2. Make sure that you output a consistent plan.For example, opening of the same object should not occur in successive steps.3. Make sure the output is consistent with the proper affordances of objects.For example, a couch cannot be opened, so your output should never include the open() function for this object, but a fridge can be opened.4. The input is high level task description and low level subgoals to perform the high level task.Interpret the instructions into robot actions. 5. Object categories should only be chosen from the following classes: {OBJECT_CLASSES} 6.You can only pick up one object at a time.If the agent is holding an object, the agent should place or put down the object before attempting to pick up a second object.7.Each object instance should instantiate a different InteractionObject class even if two object instances are the same object category.8. Make sure that you are solving both the high level goal and the low level goals.</p>
<p>Some instructions may only be present in one or the other, so address everything from both.9. Before performing each action, check if that action is allowed for a particular receptacle class.A few examples have been given in API documentation.Follow the output format provided earlier.Think step by step to carry out the instruction.</p>
<p>Write a Python script that could be executed by a household robot for the following: {command} Python script:</p>
<p>Listing S5: Sample in-context example for TEACh Listing S8: Sample in-context example for the Tidy Task input: Tidy up the house.These are the out of place objects: Potato, Knife.These are the receptacles in the current scene: DiningTable, Microwave, CoffeeMachine, CounterTop.Python script: # initialize the out of place objects target_potato = InteractionObject("Potato") target_knife = InteractionObject("Knife") # initialize the placement objects to place the out of place object on target_countertop = InteractionObject("CounterTop") # The best, commonsense location for both the potato and knife is on the countertop.# re-position potato to the countertop to tidy it up target_potato.go_to()target_potato.pickup()target_countertop.go_to()target_potato.place(target_countertop)# re-position knife to the countertop to tidy it up target_knife.go_to()target_knife.pickup()target_countertop.go_to()target_knife.place(target_countertop)</p>
<p>Table 1 :
1
Trajectory from Dialogue (TfD) evaluation on the TEACh validation set.Trajectory length weighted metrics are included in ( parentheses ).FS = few shot.Sup.= supervised.G = generalist; shared across benchmarks.GC = goal-condition success.</p>
<p>S2. 2
2
IN-CONTEXT EXAMPLES Samples of the in-context examples are shown for TEACh, ALFRED, Dialfred, and the Tidy Task in Listing S5, Listing S6, Listing S7, Listing S8, respectively.</p>
<p>Table 2 :
2
Evaluation on the ALFRED validation unseen set.Trajectory length weighted metrics are included in ( parentheses ).FS = few shot.Sup.= supervised.G = generalist; shared across benchmarks.
UnseenSeenSuccessGCSuccessGCE.T. (Pashevich, 2021)7.3 (3.3) 20.9 (11.3) 46.6 (32.3) 52.9 (42.2)HiTUTSup.</p>
<p>Table 3 :
3
Evaluation on the DialFRED validation unseen set.FS = few shot.Sup.= supervised.G = generalist; shared across benchmarks.
SuccessSup.Instructions Only (Gao, 2022) 18.3 All QAs (Gao, 2022) 32.0 RL Anytime (Gao, 2022) 33.6FSHELPER (Sarch, 2023)19.62G+FSHELPER-XP without QA HELPER-XS18.96 16.48 19.99</p>
<p>Table 4 :
4
Evaluation on the Tidy Task test set.
Trajec-</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.What makes good in-context examples for gpt-3?DeeLIO 2022, pp.100,
S1 LIMITATIONS2022b. 1 Our model has the following limitations:Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, andWeizhu Chen. What makes good in-context examples for gpt-3? In Proceedingsof Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on KnowledgeExtraction and Integration for Deep Learning Architectures, pp. 100-114, 2022c.2Xiaotian Liu, Hector Palacios, and Christian Muise. Egocentric planning for scalableembodied task achievement. arXiv preprint arXiv:2306.01295, 2023. 6, 7, 11Bodhisattwa Prasad Majumder, Bhavana Dalvi Mishra, Peter Jansen, Oyvind Tafjord,Niket Tandon, Li Zhang, Burch Callison-Burch, and Peter Clark. Clin: A continu-ally learning language agent for rapid task adaptation and generalization. arXiv,2023. 10Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, JiaweiHan, and Weizhu Chen. Generation-augmented retrieval for open-domain questionanswering. In Proceedings of the 59th Annual Meeting of the Association forComputational Linguistics and the 11th International Joint Conference on NaturalLanguage Processing (Volume 1: Long Papers), pp. 4089-4100, 2021. 2, 10So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, Yonatan Bisk, andRuslan Salakhutdinov. Film: Following instructions in language with modularmethods, 2021. 5, 6, 7, 11, 21So Yeon Min, Hao Zhu, Ruslan Salakhutdinov, and Yonatan Bisk. Don't copy theteacher: Data and model challenges in embodied dialogue. In Proceedings ofthe 2022 Conference on Empirical Methods in Natural Language Processing, pp.9361-9368, 2022. 6Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In Proceed-ings of the 60th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pp. 3470-3487, 2022. 1Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin,Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-languagepre-training via embodied chain of thought. arXiv preprint arXiv:2305.15021,2023. 20Michael Murray, Maya Cakmak, and Maya Cakmak. Following natural languageinstructions for household tasks with landmark guided search and reinforced poseadjustment. IEEE Robotics and Automation Letters, 7(3):6870-6877, 2022. 6, 7,11Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, AnjaliNarayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and DilekHakkani-Tur. Teach: Task-driven embodied agents that chat, 2021. 2, 3, 5, 10</p>
<p>Table S1 :
S1
Alternative TEACh Execution from Dialog History (EDH) evaluation split.Trajectory length weighted metrics are included in ( parentheses ).SR = success rate.GC = goal condition success rate.Note that Test Seen and Unseen are not the true TEACh test sets, but an alternative split of the validation set used until the true test evaluation is released, as mentioned in the TEACh github README, and also reported by DANLI(Zhang, 2022).
ValidationTestUnseenSeenUnseenSeenSRGCSRGCSRGCSRGCE.T.8.35 (0.86) 6.34 (3.69)8.28 (1.13) 8.72 (3.82)7.38 (0.97) 6.06 (3.17)8.82 (0.29) 9.46 (3.03)DANLI</p>
<p>Dialogue input: <Driver> what shall I do today?<Commander> clean all the bowls.<Driver> where are the bowls?<Commander> start with the one by the stove.<Commander> left.<Commander> rinse it with water.<Commander> great.<Driver> what next?<Commander> the next one is in the fridge.<Commander> you need to rinse it with water also.<Commander> great job.we are finished.
Python script:target_bowl1 = InteractionObject("Bowl", landmark = "Stove", attributes = ["clean"])target_bowl1.go_to()target_bowl1.pickup()target_bowl1.clean()target_bowl1.put_down()target_bowl2 = InteractionObject("Bowl", landmark = "Fridge", attributes = ["clean"])target_bowl2.go_to()target_bowl2.pickup()target_bowl2.clean()target_bowl2.put_down()
Listing S1: Prompt template for TEAChYou are an adept at translating human dialogues into sequences of actions for household robots.Given a dialogue between a <Driver> and a <Commander>, you convert the conversation into a Python program to be executed by a robot.{API}Write a script using Python and the InteractionObject class and functions defined above that could be executed by a household robot.Here are a few examples of typical inputs and outputs (only for in-context reference): {RETRIEVED_EXAMPLES}Adhere to these stringent guidelines: 1. Use only the classes and functions defined previously.Do not create functions that are not provided above.2. Make sure that you output a consistent plan.For example, opening of the same object should not occur in successive steps.3. Make sure the output is consistent with the proper affordances of objects.For example, a couch cannot be opened, so your output should never include the open() function for this object, but a fridge can be opened.4. The input is dialogue between <Driver> and <Commander>.Interpret the dialogue into robot actions.Do not output any dialogue.5. Object categories should only be chosen from the following classes: {OBJECT_CLASSES} 6.You can only pick up one object at a time.If the agent is holding an object, the agent should place or put down the object before attempting to pick up a second object.Listing S4: Prompt template for the Tidy TaskTask: As an AI-driven cleaning robot, you are tasked with employing commonsense reasoning to identify where to place out of place objects that aren't situated appropriately.Given a list of out of place objects, you are to write a Python program to be executed by a robot that will bring the out of place objects to a suitable location.{API}Write a script using Python and the InteractionObject class and functions defined above that could be executed by a household robot.Here are a few examples of typical inputs and outputs (only for in-context reference): {RETRIEVED_EXAMPLES}Adhere to these stringent guidelines: 1. Use only the classes and functions defined previously.Do not create functions that are not provided above.2. Make sure that you output a consistent plan.For example, opening of the same object should not occur in successive steps.3. Make sure the output is consistent with the proper affordances of objects.For example, a couch cannot be opened, so your output should never include the open() function for this object, but a fridge can be opened.4. Object categories should only be chosen from the following classes: {OBJECT_CLASSES} 5.You can only pick up one object at a time.If the agent is holding an object, the agent should place or put down the object before attempting to pick up a second object.6.Each object instance should instantiate a different InteractionObject class even if two object instances are the same object category.7. Address each item systematically, one by one.8. Base your decisions on your ingrained knowledge about the typical placement of day-to-day objects.Follow the output format provided earlier.Think step by step to carry out the instruction.Write a Python script that could be executed by a household robot for the following: input: {command} Python script:Listing S6: Sample in-context example for ALFRED High Level Goal: To heat an apple and place in the black bin.Low Level Goal:(1) Turn around and walk to the kitchen island.(2) Pick up the apple in front of the gold colored plate.(3) Walk around the kitchen island and to the stove on the right, look above the stove to face the microwave.(4) Place the apple inside the microwave, heat up/cook the apple, take the apple out of the microwave.(5) Turn left, turn left at the fridge, turn left to face the kitchen island, and look down at the black bin.Listing S7: Sample in-context example for Dialfred High Level Goal: Put the watch in the bowl on the nightstand.Low Level Goal:(1) Turn around, walk around the bookshelf, turn to the desk.(2) Pick the watch up off of the desk.(3) Put the watch in the bowl on the desk.(4) Pick up the bowl off of the desk.(5) Walk back around the bookshelf, walk over between the two beds.(6) Put the bowl on the nightstand.Python script: target_watch = InteractionObject("Watch", landmark = "Desk") target_watch.go_to()target_watch.pickup()target_bowl = InteractionObject("Bowl", landmark = "Desk") target_bowl.go_to()target_watch.place(target_bowl)target_bowl.go_to()target_bowl.pickup()target_nightstand = InteractionObject("SideTable", landmark = "Bed") target_nightstand.go_to()target_bowl.place(target_nightstand)Listing S9: Prompt template for Question SelectionYou are an excellent interpreter of human instructions for household tasks.Given a list of questions you can ask and information about the current environment and context, you provide a question that should be asked in order to give the agent useful information.{API}Write a script using Python using the class and functions defined above that could be executed by a household robot.Adhere to these stringent guidelines: 1. Use only the classes and functions defined previously.Do not create functions that are not provided above.2. Make sure you choose the question that provides the most information and is most relevant for the situation at hand. 3. Object categories should only be chosen from the following classes: {OBJECT_CLASSES} Follow the output format provided earlier.Think step by step to carry out the instruction.Write a Python script that asks questions to help a household robot in the following situation: {context} Python script:Listing S10: Prompt template for Answer Parsing You are an excellent interpreter of human instructions for household tasks.Given the current context of the agent, a question that was asked, and an answer that was given, you must write code for actions the agent should take based on the answer provided.{API}Write a script using Python using the class and functions defined above that could be executed by a household robot.Adhere to these stringent guidelines: 1. Use only the classes and functions defined previously.Do not create functions that are not provided above.2. Make sure you plan the most simple and direct interpretation of the answer given.3. Prioritize the most specific information given.For example, an actual object name should be deemed more important than a region.4. If multiple pieces of information are given, ensure you incorporate all of them into the script.5. Object categories should only be chosen from the following classes: {OBJECT_CLASSES} Write a Python script that asks questions to help a household robot in the following situation: {context} {question} {answer} Python script:
arxiv:2303.08774Openai. gpt-4 technical report. 202335arXiv preprint</p>
<p>Flamingo: a visual language model for few-shot learning. Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Advances in Neural Information Processing Systems. 202235</p>
<p>On evaluation of embodied navigation agents. Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, arXiv:1807.067572018arXiv preprint</p>
<p>Rearrangement: A challenge for embodied ai. Dhruv Batra, S Angel Xuan Chang, Andrew J Chernova, Jun Davison, Vladlen Deng, Sergey Koltun, Jitendra Levine, Igor Malik, Roozbeh Mordatch, Manolis Mottaghi, Hao Savva, Su, ArXiv, abs/2011.019752020</p>
<p>Multi-level compositional reasoning for interactive instruction following. Suvaansh Bhambri, Byeonghwi Kim, Jonghyun Choi, Interaction. 34112023</p>
<p>Zoedepth: Zero-shot transfer by combining relative and metric depth. Farooq Shariq, Reiner Bhat, Diana Birkl, Peter Wofk, Matthias Wonka, Müller, arXiv:2302.122882023arXiv preprint</p>
<p>A persistent spatial semantic representation for high-level natural language instruction execution. Valts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, Yoav Artzi, Conference on Robot Learning. PMLR2022711</p>
<p>Improving language models by retrieving from trillions of tokens. Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las, Aurelia Casas, Jacob Guy, Roman Menick, Tom Ring, Saffron Hennigan, Loren Huang, Chris Maggiore, Albin Jones, Andy Cassirer, Michela Brock, Geoffrey Paganini, Oriol Irving, Simon Vinyals, Karen Osindero, Jack W Simonyan, Erich Rae, Laurent Elsen, Sifre, CoRR, abs/2112.044262021</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on Robot Learning. PMLR202367</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Semantic visual navigation by watching youtube videos. Matthew Chang, Arjun Gupta, Saurabh Gupta, Advances in Neural Information Processing Systems. 202033</p>
<p>. Matthew Chang, Theophile Gervet, Mukul Khanna, Sriram Yenamandra, Dhruv Shah, So Yeon Min, Kavit Shah, Chris Paxton, Saurabh Gupta, Dhruv Batra, Roozbeh Mottaghi, Jitendra Malik, Devendra Singh, Chaplot , 2023Goat: Go to any thing</p>
<p>Learning to explore using active neural slam. Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta, Abhinav Gupta, Ruslan Salakhutdinov, International Conference on Learning Representations. 20191021</p>
<p>Object goal navigation using goal-oriented semantic exploration. Devendra Singh Chaplot, Dhiraj Prakashchand Gandhi, Abhinav Gupta, Russ R Salakhutdinov, Advances in Neural Information Processing Systems. 202033</p>
<p>Soundspaces: Audio-visual navigation in 3d environments. Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc, Amengual Gari, Ziad Al-Halah, Krishna Vamsi, Philip Ithapu, Kristen Robinson, Grauman, Computer Vision-ECCV 2020: 16th European Conference. Glasgow, UKSpringerAugust 23-28, 2020. 202016Proceedings, Part VI</p>
<p>Learning exploration policies for navigation. Tao Chen, Saurabh Gupta, Abhinav Gupta, International Conference on Learning Representations. 2019</p>
<p>Augmenting pre-trained language models with qa-memory for open-domain question answering. Wenhu Chen, Pat Verga, John Michiel De Jong, William Wieting, Cohen, 2022</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2018</p>
<p>Probing emergent semantics in predictive agents via question answering. Abhishek Das, Federico Carnevale, Hamza Merzic, Laura Rimell, Rosalia Schneider, Josh Abramson, Alden Hung, Arun Ahuja, Stephen Clark, Gregory Wayne, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine Learning2020</p>
<p>Knowledge base question answering by case-based reasoning over subgraphs. Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Robin Jia, Manzil Zaheer, Hannaneh Hajishirzi, Andrew Mccallum, 2022</p>
<p>Episodic memory question answering. Samyak Datta, Sameer Dharur, Vincent Cartillier, Ruta Desai, Mukul Khanna, Dhruv Batra, Devi Parikh, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Mention memory: incorporating textual knowledge into transformers through entity mention attention. Jong Michiel De, Yury Zemlyanskiy, Nicholas Fitzgerald, Fei Sha, William Cohen, CoRR, abs/2110.061762021</p>
<p>Procthor: Large-scale embodied ai using procedural generation. Matt Deitke, Eli Vanderbilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi, Advances in Neural Information Processing Systems. 20223523</p>
<p>Solq: Segmenting objects by learning queries. Bin Dong, Fangao Zeng, Tiancai Wang, Xiangyu Zhang, Yichen Wei, Advances in Neural Information Processing Systems. 20213421</p>
<p>The threedworld transport challenge: A visually guided task-and-motion planning benchmark towards physically realistic embodied ai. Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek Bhandwaldar, Dan Gutfreund, L K Daniel, James J Yamins, Josh Dicarlo, Antonio Mcdermott, Joshua B Torralba, Tenenbaum, 10.1109/ICRA46639.2022.9812329.102022 International Conference on Robotics and Automation (ICRA). 2022</p>
<p>Alexa arena: A user-centric interactive platform for embodied ai. Qiaozi Gao, Govind Thattai, Xiaofeng Gao, Suhaila Shakiah, Shreyas Pansare, Vasu Sharma, Gaurav Sukhatme, Hangjie Shi, Bofei Yang, Desheng Zheng, arXiv:2303.015862023arXiv preprint</p>
<p>Making pre-trained language models better few-shot learners. Tianyu Gao, Adam Fisch, Danqi Chen, arXiv:2012.157232020arXiv preprint</p>
<p>DialFRED: Dialogue-enabled agents for embodied instruction following. Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav S Sukhatme, IEEE Robotics and Automation Letters. 74112022. 2, 3, 4, 5, 7, 8</p>
<p>Navigating to objects in the real world. Theophile Gervet, Soumith Chintala, Dhruv Batra, Jitendra Malik, Devendra Singh, Chaplot , Science Robotics. 87969912023</p>
<p>Iqa: Visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>. Ryan Greene, Ted Sanders, Lilian Weng, Arvind Neelakantan, Openai, 202245</p>
<p>Cognitive mapping and planning for visual navigation. Saurabh Gupta, James Davidson, Sergey Levine, Rahul Sukthankar, Jitendra Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Selective annotation makes language models better few-shot learners. Jungo Su Hongjin, Chen Henry Kasai, Weijia Wu, Tianlu Shi, Jiayi Wang, Rui Xin, Mari Zhang, Luke Ostendorf, Noah A Zettlemoyer, Smith, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Prompter: Utilizing large language model prompting for a data efficient embodied instruction following. Yuki Inoue, Hiroki Ohashi, Hiroki Ohashi, arXiv:2211.032672022711arXiv preprint</p>
<p>Housekeep: Tidying virtual households using commonsense reasoning. Yash Kant, Arun Ramachandran, Sriram Yenamandra, Igor Gilitschenski, Dhruv Batra, Andrew Szot, Harsh Agrawal, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 20221011Proceedings, Part XXXIX</p>
<p>Generalization through memorization: Nearest neighbor language models. Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, Mike Lewis, 2019</p>
<p>Context-aware planning and environment-aware memory for instruction following embodied agents. Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi, ICCV. 2023611</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. 2017521</p>
<p>Jacob Krantz, Theophile Gervet, Karmesh Yadav, Austin Wang, Chris Paxton, Roozbeh Mottaghi, Dhruv Batra, Jitendra Malik, arXiv:2304.01192Stefan Lee, and Devendra Singh Chaplot. Navigating to objects specified by images. 2023arXiv preprint</p>
<p>Roomacross-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding. Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, Jason Baldridge, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. the 2020 Conference on Empirical Methods in Natural Language Processing2020</p>
<p>Rma: Rapid motor adaptation for legged robots. Ashish Kumar, Zipeng Fu, Deepak Pathak, Jitendra Malik, 2021</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 20203310</p>
<p>Microsoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, Lawrence Zitnick, Computer Vision-ECCV 2014: 13th European Conference. Zurich, SwitzerlandSpringerSeptember 6-12, 2014. 2014Proceedings, Part V 13</p>
<p>Lebp-language expectation &amp; binding policy: A two-stream framework for embodied vision-and-language interaction task learning agents. Haoyu Liu, Yang Liu, Hongkai He, Hangfang Yang, arXiv:2203.046372022a711arXiv preprint</p>
<p>Episodic transformer for vision-and-language navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021. 5, 6, 7811</p>
<p>Advances in neural information processing systems. Ethan Perez, Douwe Kiela, Kyunghyun Cho, 202134True few-shot learning with language models</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, International conference on machine learning. PMLR2021</p>
<p>Occupancy anticipation for efficient exploration and navigation. Ziad Santhosh K Ramakrishnan, Kristen Al-Halah, Grauman, European Conference on Computer Vision. Springer2020</p>
<p>Tidee: Tidying up novel rooms using visuo-semantic commonsense priors. Gabriel Sarch, Zhaoyuan Fang, Adam W Harley, Paul Schydlo, Saurabh Michael J Tarr, Katerina Gupta, Fragkiadaki, Computer Vision-ECCV 2022: 17th European Conference. Tel Aviv, IsraelSpringerOctober 23-27, 2022. 2022. 2, 3, 5, 8, 91024Proceedings, Part XXXIX</p>
<p>Open-ended instructable embodied agents with memory-augmented large language models. Gabriel Sarch, Yue Wu, Michael Tarr, Katerina Fragkiadaki, Findings of the Association for Computational Linguistics: EMNLP 2023. 2023. 2, 3, 5, 6, 7, 8, 910</p>
<p>Habitat: A platform for embodied ai research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2019</p>
<p>It's not just size that matters: Small language models are also few-shot learners. Timo Schick, Hinrich Schütze, Hinrich Schütze, Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterHuman Language Technologies2021</p>
<p>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020. 2, 3, 5610</p>
<p>One step at a time: Long-horizon vision-and-language navigation with milestones. Hee Chan, Jihyung Song, Tai-Yu Kil, Brian M Pan, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022a67</p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, 2023. 2, 6710</p>
<p>Clip models are few-shot learners: Empirical studies on vqa and visual entailment. Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, Furu Wei, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics2022b1</p>
<p>A simple approach for visual room rearrangement: 3d mapping and semantic search. Brandon Trabucco, Robinson Gunnar A Sigurdsson, Piramuthu, Ruslan Gaurav S Sukhatme, Salakhutdinov, The Eleventh International Conference on Learning Representations. 2022</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv: Arxiv-2305.162912023aarXiv preprint</p>
<p>Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models. Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, Yitao Liang, arXiv:2311.059972023b1020arXiv preprint</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2021</p>
<p>Visual room rearrangement. Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). June 2021. 10</p>
<p>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. Erik Wijmans, Abhishek Kadian, Ari S Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, ICLR2020</p>
<p>Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. Mitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, Roozbeh Mottaghi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, Thomas Funkhouser, arXiv:2305.05658Tidybot: Personalized robot assistance with large language models. 2023arXiv preprint</p>
<p>Memorizing transformers. Yuhuai Wu, Markus N Rabe, Delesley Hutchins, Christian Szegedy, 2022</p>
<p>Octopus: Embodied vision-language programmer from environmental feedback. Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, arXiv:2310.08588202320arXiv preprint</p>
<p>Visual semantic navigation using scene priors. Wei Yang, Xiaolong Wang, Ali Farhadi, Abhinav Gupta, Roozbeh Mottaghi, Proceedings of (ICLR) International Conference on Learning Representations. (ICLR) International Conference on Learning RepresentationsMay 2019</p>
<p>Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, arXiv:2306.11565Open-vocabulary mobile manipulation. 2023arXiv preprint</p>
<p>Hierarchical task learning from language instructions with unified transformers and self-monitoring. ACL 2021, Findings. Yichi Zhang, Joyce Chai, Joyce Chai, 2021711</p>
<p>Danli: Deliberative agent for following natural language instructions. Yichi Zhang, Jianing Yang, Jiayi Pan, Shane Storks, Nikhil Devraj, Ziqiao Ma, Keunwoo Yu, Yuwei Bao, Joyce Chai, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022622</p>
<p>A neuro-symbolic commonsense reasoning framework for conversational embodied agents. Kaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Li, Xuehai He, Xin Eric, Wang , 2022511Jarvis</p>
<p>Excalibur: Encouraging and evaluating embodied exploration. Raghav Hao Zhu, So Kapoor, Winson Yeon Min, Jiatai Han, Kaiwen Li, Graham Geng, Yonatan Neubig, Aniruddha Bisk, Luca Kembhavi, Weihs, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8953 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8953</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8953</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-157.html">extraction-schema-157</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-ba7e5fcf19fe60b1f8906e9ce1341b5438093f25</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/ba7e5fcf19fe60b1f8906e9ce1341b5438093f25" target="_blank">SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Self-Evaluation Decoding, SED, a decoding method for enhancing model generation that integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points.</p>
                <p><strong>Paper Abstract:</strong> Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries. These methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work. Many chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs' generation. This paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation. Analogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points. Experimental results across various tasks using different LLMs demonstrate SED's effectiveness.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8953.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8953.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SED</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Decoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decoding-time generate-then-evaluate method that detects high-uncertainty (chaotic) token positions, speculates completions from top-k token candidates, has the model self-evaluate each speculative completion's correctness probability, and combines that evaluation with the original token probabilities to choose the next token.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>generic (applied to falcon-7b-instruct, llama2-7b-chat-hf, gemma-7b-it in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Applied to several 7B-class instruction-tuned LLMs in the paper: falcon-7b-instruct, llama2-7b-chat-hf, gemma-7b-it (all ~7B parameter models fine-tuned on augmented datasets described in §3.4).</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation Decoding (SED)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>When a token position is detected as chaotic (via entropy or probability-ratio criteria), select top-k token candidates, append each candidate to the current context and let the model speculate full continuations for each; the model then self-evaluates each speculative response producing p_se (probability the response is correct). Compute a propensity score s = α * p_se + (1-α) * p_gen (p_gen is original token probability) and select the candidate with highest s. This is applied per chaotic point during decoding (no fixed global iteration count specified).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XieZhi, HotpotQA, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>XieZhi: multiple-choice domain-knowledge benchmark; HotpotQA: multi-hop reading comprehension (SQuAD-style); GSM8K: grade-school math word problems for mathematical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Table 1 (ratio-based SED): falcon-7b-instruct: XieZhi 0.338, HotpotQA 0.750, GSM8K 0.256; llama2-7b-chat-hf: XieZhi 0.415, HotpotQA 0.822, GSM8K 0.421; gemma-7b-it: XieZhi 0.330, HotpotQA 0.865, GSM8K 0.510 (accuracies, fraction correct).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy search baselines (from Table 1): falcon-7b-instruct: XieZhi 0.272, HotpotQA 0.735, GSM8K 0.225; llama2-7b-chat-hf: XieZhi 0.312, HotpotQA 0.804, GSM8K 0.372; gemma-7b-it: XieZhi 0.323, HotpotQA 0.840, GSM8K 0.472 (accuracies).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Implemented as internal generate-then-evaluate cycles during decoding: chaotic-point detection (entropy or top-2 probability ratio), speculation (top-k candidate continuations produced by the model), self-evaluation by the model producing p_se (implemented by training the model to output a judgment token like '[Y]'/'[N]' and extracting the probability of the 'Y' token), and fusion with original token logits via a linear mixture (s = α*p_se + (1-α)*p_gen). Evaluation ability is improved by supervised finetuning on an augmented dataset built via multi-model generation ensembling (Section 3.4).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Across three 7B models and three tasks, SED (ratio-based) consistently improved accuracy over greedy/beam/nucleus and DoLa baselines (see Table 1). Example: llama2-7b-chat-hf XieZhi accuracy improved from 0.312 (greedy) to 0.415 (SED). Qualitative: case studies (Tables 4-6) show SED correcting specific chaotic-token mistakes (options, numbers) by reflecting on alternative continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations: (1) Extra computation / slower decoding because speculation requires generating k continuations per chaotic point (trade-off between quality and inference speed). (2) Simple binary judgment representation j_eval in the training data (no richer CoT judgments currently). (3) Misalignment between decoding methods used at generation and speculation can reduce or reverse gains — Table 3 shows that when speculation and generation decoding are misaligned, improvements can be limited or negative. (4) SED effectiveness depends on evaluation quality; poor evaluators produce weak feedback. (See §6 and analysis in §4.5–4.6.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared to greedy search, beam search, nucleus sampling, and DoLa: SED outperforms these baselines across multiple models/tasks (Table 1). Nuclear sampling showed high variance and poor performance in QA; beam search helped some but still failed in chaotic cases where sentence-level probability misleads; DoLa performed poorly in these experiments. SED (ratio-based) generally gave the best or second-best results in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>The paper reports multiple analyses: (1) Detection criterion ablation: ratio-based detection slightly outperforms entropy-based detection and is more practical (Table 1). (2) Evaluation-quality ablation: using stronger external LLMs as evaluators improved SED results for the generation model (Table 2 shows SE.gen for llama2 using different evaluators: self 0.380, llama3-8b 0.505, llama3-70b 0.584, yi-34b 0.620, qwen1.5-72b 0.588 on XieZhi). (3) Generation–speculation alignment: Table 3 shows aligned decoding (e.g., greedy+greedy or beam+beam) is better; misaligned combinations can reduce gains. (4) Fusion coefficient α searched on subset: α=0.8 best for XieZhi and GSM8K, α=0.6 best for HotpotQA (Appendix D).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8953.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8953.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SED (falcon-7b-instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Decoding applied to falcon-7b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of the paper's SED pipeline to the falcon-7b-instruct model (7B parameters) fine-tuned with the augmented datasets; demonstrates improvements on XieZhi and GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>falcon-7b-instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Falcon-7b-based instruction-tuned model as used in experiments; 7B parameter open model (Penedo et al.), finetuned on datasets described in §3.4.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation Decoding (SED, ratio-based and entropy-based variants)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Chaotic-point detection (entropy or p_second/p_max ratio), select top-k tokens (k=5), speculate completions for each token using the same model, self-evaluate completions to get p_se, compute s = α*p_se + (1-α)*p_gen (α chosen via grid search), pick token with largest s.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XieZhi, HotpotQA, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See general descriptions above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Ratio-based SED: XieZhi accuracy 0.338, HotpotQA 0.750, GSM8K 0.256 (Table 1). Entropy-based SED: XieZhi 0.326, HotpotQA 0.745, GSM8K 0.244.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy search: XieZhi 0.272, HotpotQA 0.735, GSM8K 0.225; beam search: XieZhi 0.292, HotpotQA 0.736, GSM8K 0.216 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Internal generate-and-evaluate cycles by the falcon model; evaluation probabilities extracted from model logits trained to output '[Y]'/'[N]' judgments; fusion with token logits via scalar α.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Numeric improvements vs greedy/beam shown in Table 1 (examples above). Case examples show SED corrected number/option errors that greedy made (Appendix C).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same systemic limitations as SED: extra computation cost, dependency on evaluator quality, sensitivity to detection threshold and decoding alignment; no explicit catastrophic failures reported but improvements vary by dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms greedy, beam, nucleus sampling and DoLa on reported tasks (Table 1), with ratio-based detection slightly better than entropy-based.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Same ablations as general SED; the paper reports α tuning and detection-threshold selection (Appendix D) that affect final numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8953.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8953.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SED (llama2-7b-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Decoding applied to llama2-7b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SED to Llama-2 7B chat model fine-tuned on the augmented evaluation/generation datasets, showing substantial accuracy gains on XieZhi and GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama2-7b-chat-hf</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Llama 2 chat variant with ~7B parameters (Touvron et al.), finetuned as described in §3.4.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation Decoding (SED)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Same SED pipeline: chaotic point detection (ratio-based default), speculate top-k candidate continuations with the model, self-evaluate each, fuse p_se with p_gen via s = α*p_se + (1-α)*p_gen, pick candidate with highest s. Default uses ratio-based detection, k=5, branching factor b=5, α tuned (α=0.8 for XieZhi/GSM8K).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XieZhi, HotpotQA, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See general descriptions above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Ratio-based SED: XieZhi 0.415, HotpotQA 0.822, GSM8K 0.421 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy search: XieZhi 0.312, HotpotQA 0.804, GSM8K 0.372; beam search: XieZhi 0.336, HotpotQA 0.808, GSM8K 0.396 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Internal speculation and self-evaluation by llama2 model; p_se derived from model's probability of producing a 'Y' judgment token when prompted to judge a speculative completion; fusion with token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Numeric: XieZhi +10.3 percentage points (0.312→0.415), GSM8K +4.9 points (0.372→0.421) over greedy; qualitative case studies show SED corrected option/number selection errors (Appendix C).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>See general SED limitations. Additional experiments (Table 3) show that when speculation decoding differs from generation decoding (misalignment) the performance gains reduce or turn negative (e.g., some combinations in Table 3 yield lower accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Compared directly with greedy, beam, nucleus, and DoLa; SED ratio-based gave the best reported results for this model in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Alpha tuning: α=0.8 chosen for XieZhi/GSM8K; alignment experiments in Table 3 show best results when generation and speculation decoding are aligned (greedy+greedy or beam+beam). Evaluation-quality experiments (Table 2) show using external stronger evaluators increases SED benefit for the generation model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8953.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8953.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SED (gemma-7b-it)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Decoding applied to gemma-7b-it</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of SED to the gemma-7b-it model; demonstrates SED improvements particularly on GSM8K and XieZhi in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gemma-7b-it</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Gemma 7B instruction-tuned model (Anil et al.-family), finetuned on the augmented datasets described in §3.4.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation Decoding (SED)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>As above: detection of chaotic points (ratio-based default), top-k speculation, self-evaluation to get p_se, fusion with p_gen via s = α*p_se + (1-α)*p_gen, select argmax(s).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XieZhi, HotpotQA, GSM8K</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See general descriptions above.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Ratio-based SED: XieZhi 0.330, HotpotQA 0.865, GSM8K 0.510 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Greedy search: XieZhi 0.323, HotpotQA 0.840, GSM8K 0.472; beam search: XieZhi 0.330, HotpotQA 0.848, GSM8K 0.480 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>Internal generate-and-evaluate with gemma model; p_se extracted from judgment token logits; fused with token probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Numeric improvements on GSM8K of +3.8 points (0.472→0.510) and modest gains on XieZhi/HotpotQA; case studies show correction of chaotic-token errors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Same general limitations: inference slower, sensitive to evaluator quality and decoding alignment; improvements vary by dataset and model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>Outperforms greedy/nucleus/DoLa in many settings; beam sometimes competitive but SED often best on datasets exhibiting important chaotic points (numbers/options).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Reported parameter choices (k=5, b=5), α tuning per dataset (Appendix D) and detection criterion comparisons (ratio vs entropy).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8953.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8953.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing self-reflection or iterative answer improvement (e.g., generate-then-reflect, self-critique, reflexion), including descriptions of the methods, tasks, performance results, mechanisms, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SED with external evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Evaluation Decoding where evaluation (p_se) is provided by a stronger external LLM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Variant where the generation/speculation is performed by a base model (llama2-7b-chat-hf in experiments) and the evaluation step uses a stronger external LLM (e.g., llama3-70b, yi-34b, qwen1.5-72b) to produce evaluation feedback to improve selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>llama2-7b-chat-hf (generation/speculation) with external evaluator(s) (llama3-8b, llama3-70b, yi-34b-chat, qwen1.5-72b)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generation/speculation: llama2-7b-chat-hf (7B). External evaluators: larger/more capable LLMs (llama3-8b, llama3-70b, yi-34b-chat, qwen1.5-72b) used only to produce p_se in experiments in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>Self-Evaluation Decoding with external feedback providers</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Speculate k candidate continuations using the generation model, then query an external stronger LLM to evaluate each speculative continuation's correctness and return p_se; fuse p_se with p_gen via s = α*p_se + (1-α)*p_gen (in the Table 2 experiment α=1 to test evaluation quality alone).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>XieZhi (evaluated experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multiple-choice domain-knowledge benchmark; XieZhi emphasizes option-token chaotic points where correct option selection is critical.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Table 2 (SE.gen): using external evaluators to provide feedback for llama2 production: SE.gen accuracies on XieZhi — self-eval: 0.380; llama3-8b eval: 0.505; llama3-70b eval: 0.584; yi-34b-chat eval: 0.620; qwen1.5-72b eval: 0.588 (accuracies of the generation model when using those evaluators' feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>llama2-7b-chat-hf direct greedy generation accuracy (no SED): 0.312 on XieZhi (Table 2 'Direct gen.' row for llama2 self).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_of_reflection</strong></td>
                            <td>External LLM acts as judge: the generation model produces candidate continuations; the external LLM evaluates each (outputs a judgment token probability used as p_se); p_se fused into propensity score used to select token. In the experiment α was set to 1 to isolate evaluation feedback quality.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_improvement</strong></td>
                            <td>Quantitative: Table 2 shows that using stronger external evaluators yields higher SE.gen accuracy for the generation model compared to self-evaluation (e.g., llama2 self SE.gen 0.380 vs yi-34b evaluator SE.gen 0.620 on XieZhi). The paper interprets this as evidence that evaluation quality matters and that stronger evaluators can substantially improve SED's benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Mixed effects in absolute numbers: Table 2 columns also report the direct generation accuracies of the external evaluators themselves; SE.gen of the base model does not always reach the direct accuracy of the external evaluator, and results are not strictly monotonic across evaluators. The approach adds additional latency and API/model-cost when using external models as evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>This is a variant of SED where the evaluator is external rather than the same model; authors note this can leverage stronger evaluation signals and could be combined with tool-based evaluation, but at higher inference cost.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Experiment in Table 2 isolates evaluation quality by setting α=1 (only p_se used). SE.gen increases substantially when evaluator is stronger than the generation model; authors discuss potential to use even stronger evaluators/tools for further gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation', 'publication_date_yy_mm': '2024-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline <em>(Rating: 2)</em></li>
                <li>Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Learning from mistakes makes LLM better reasoner <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8953",
    "paper_id": "paper-ba7e5fcf19fe60b1f8906e9ce1341b5438093f25",
    "extraction_schema_id": "extraction-schema-157",
    "extracted_data": [
        {
            "name_short": "SED",
            "name_full": "Self-Evaluation Decoding",
            "brief_description": "Decoding-time generate-then-evaluate method that detects high-uncertainty (chaotic) token positions, speculates completions from top-k token candidates, has the model self-evaluate each speculative completion's correctness probability, and combines that evaluation with the original token probabilities to choose the next token.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "generic (applied to falcon-7b-instruct, llama2-7b-chat-hf, gemma-7b-it in experiments)",
            "model_description": "Applied to several 7B-class instruction-tuned LLMs in the paper: falcon-7b-instruct, llama2-7b-chat-hf, gemma-7b-it (all ~7B parameter models fine-tuned on augmented datasets described in §3.4).",
            "reflection_method_name": "Self-Evaluation Decoding (SED)",
            "reflection_method_description": "When a token position is detected as chaotic (via entropy or probability-ratio criteria), select top-k token candidates, append each candidate to the current context and let the model speculate full continuations for each; the model then self-evaluates each speculative response producing p_se (probability the response is correct). Compute a propensity score s = α * p_se + (1-α) * p_gen (p_gen is original token probability) and select the candidate with highest s. This is applied per chaotic point during decoding (no fixed global iteration count specified).",
            "task_name": "XieZhi, HotpotQA, GSM8K",
            "task_description": "XieZhi: multiple-choice domain-knowledge benchmark; HotpotQA: multi-hop reading comprehension (SQuAD-style); GSM8K: grade-school math word problems for mathematical reasoning.",
            "performance_with_reflection": "Table 1 (ratio-based SED): falcon-7b-instruct: XieZhi 0.338, HotpotQA 0.750, GSM8K 0.256; llama2-7b-chat-hf: XieZhi 0.415, HotpotQA 0.822, GSM8K 0.421; gemma-7b-it: XieZhi 0.330, HotpotQA 0.865, GSM8K 0.510 (accuracies, fraction correct).",
            "performance_without_reflection": "Greedy search baselines (from Table 1): falcon-7b-instruct: XieZhi 0.272, HotpotQA 0.735, GSM8K 0.225; llama2-7b-chat-hf: XieZhi 0.312, HotpotQA 0.804, GSM8K 0.372; gemma-7b-it: XieZhi 0.323, HotpotQA 0.840, GSM8K 0.472 (accuracies).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Implemented as internal generate-then-evaluate cycles during decoding: chaotic-point detection (entropy or top-2 probability ratio), speculation (top-k candidate continuations produced by the model), self-evaluation by the model producing p_se (implemented by training the model to output a judgment token like '[Y]'/'[N]' and extracting the probability of the 'Y' token), and fusion with original token logits via a linear mixture (s = α*p_se + (1-α)*p_gen). Evaluation ability is improved by supervised finetuning on an augmented dataset built via multi-model generation ensembling (Section 3.4).",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: Across three 7B models and three tasks, SED (ratio-based) consistently improved accuracy over greedy/beam/nucleus and DoLa baselines (see Table 1). Example: llama2-7b-chat-hf XieZhi accuracy improved from 0.312 (greedy) to 0.415 (SED). Qualitative: case studies (Tables 4-6) show SED correcting specific chaotic-token mistakes (options, numbers) by reflecting on alternative continuations.",
            "limitations_or_failure_cases": "Reported limitations: (1) Extra computation / slower decoding because speculation requires generating k continuations per chaotic point (trade-off between quality and inference speed). (2) Simple binary judgment representation j_eval in the training data (no richer CoT judgments currently). (3) Misalignment between decoding methods used at generation and speculation can reduce or reverse gains — Table 3 shows that when speculation and generation decoding are misaligned, improvements can be limited or negative. (4) SED effectiveness depends on evaluation quality; poor evaluators produce weak feedback. (See §6 and analysis in §4.5–4.6.)",
            "comparison_to_other_methods": "Compared to greedy search, beam search, nucleus sampling, and DoLa: SED outperforms these baselines across multiple models/tasks (Table 1). Nuclear sampling showed high variance and poor performance in QA; beam search helped some but still failed in chaotic cases where sentence-level probability misleads; DoLa performed poorly in these experiments. SED (ratio-based) generally gave the best or second-best results in the table.",
            "ablation_study_results": "The paper reports multiple analyses: (1) Detection criterion ablation: ratio-based detection slightly outperforms entropy-based detection and is more practical (Table 1). (2) Evaluation-quality ablation: using stronger external LLMs as evaluators improved SED results for the generation model (Table 2 shows SE.gen for llama2 using different evaluators: self 0.380, llama3-8b 0.505, llama3-70b 0.584, yi-34b 0.620, qwen1.5-72b 0.588 on XieZhi). (3) Generation–speculation alignment: Table 3 shows aligned decoding (e.g., greedy+greedy or beam+beam) is better; misaligned combinations can reduce gains. (4) Fusion coefficient α searched on subset: α=0.8 best for XieZhi and GSM8K, α=0.6 best for HotpotQA (Appendix D).",
            "uuid": "e8953.0",
            "source_info": {
                "paper_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SED (falcon-7b-instruct)",
            "name_full": "Self-Evaluation Decoding applied to falcon-7b-instruct",
            "brief_description": "Application of the paper's SED pipeline to the falcon-7b-instruct model (7B parameters) fine-tuned with the augmented datasets; demonstrates improvements on XieZhi and GSM8K.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "falcon-7b-instruct",
            "model_description": "Falcon-7b-based instruction-tuned model as used in experiments; 7B parameter open model (Penedo et al.), finetuned on datasets described in §3.4.",
            "reflection_method_name": "Self-Evaluation Decoding (SED, ratio-based and entropy-based variants)",
            "reflection_method_description": "Chaotic-point detection (entropy or p_second/p_max ratio), select top-k tokens (k=5), speculate completions for each token using the same model, self-evaluate completions to get p_se, compute s = α*p_se + (1-α)*p_gen (α chosen via grid search), pick token with largest s.",
            "task_name": "XieZhi, HotpotQA, GSM8K",
            "task_description": "See general descriptions above.",
            "performance_with_reflection": "Ratio-based SED: XieZhi accuracy 0.338, HotpotQA 0.750, GSM8K 0.256 (Table 1). Entropy-based SED: XieZhi 0.326, HotpotQA 0.745, GSM8K 0.244.",
            "performance_without_reflection": "Greedy search: XieZhi 0.272, HotpotQA 0.735, GSM8K 0.225; beam search: XieZhi 0.292, HotpotQA 0.736, GSM8K 0.216 (Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Internal generate-and-evaluate cycles by the falcon model; evaluation probabilities extracted from model logits trained to output '[Y]'/'[N]' judgments; fusion with token logits via scalar α.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Numeric improvements vs greedy/beam shown in Table 1 (examples above). Case examples show SED corrected number/option errors that greedy made (Appendix C).",
            "limitations_or_failure_cases": "Same systemic limitations as SED: extra computation cost, dependency on evaluator quality, sensitivity to detection threshold and decoding alignment; no explicit catastrophic failures reported but improvements vary by dataset.",
            "comparison_to_other_methods": "Outperforms greedy, beam, nucleus sampling and DoLa on reported tasks (Table 1), with ratio-based detection slightly better than entropy-based.",
            "ablation_study_results": "Same ablations as general SED; the paper reports α tuning and detection-threshold selection (Appendix D) that affect final numbers.",
            "uuid": "e8953.1",
            "source_info": {
                "paper_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SED (llama2-7b-chat-hf)",
            "name_full": "Self-Evaluation Decoding applied to llama2-7b-chat-hf",
            "brief_description": "Application of SED to Llama-2 7B chat model fine-tuned on the augmented evaluation/generation datasets, showing substantial accuracy gains on XieZhi and GSM8K.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "llama2-7b-chat-hf",
            "model_description": "Llama 2 chat variant with ~7B parameters (Touvron et al.), finetuned as described in §3.4.",
            "reflection_method_name": "Self-Evaluation Decoding (SED)",
            "reflection_method_description": "Same SED pipeline: chaotic point detection (ratio-based default), speculate top-k candidate continuations with the model, self-evaluate each, fuse p_se with p_gen via s = α*p_se + (1-α)*p_gen, pick candidate with highest s. Default uses ratio-based detection, k=5, branching factor b=5, α tuned (α=0.8 for XieZhi/GSM8K).",
            "task_name": "XieZhi, HotpotQA, GSM8K",
            "task_description": "See general descriptions above.",
            "performance_with_reflection": "Ratio-based SED: XieZhi 0.415, HotpotQA 0.822, GSM8K 0.421 (Table 1).",
            "performance_without_reflection": "Greedy search: XieZhi 0.312, HotpotQA 0.804, GSM8K 0.372; beam search: XieZhi 0.336, HotpotQA 0.808, GSM8K 0.396 (Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Internal speculation and self-evaluation by llama2 model; p_se derived from model's probability of producing a 'Y' judgment token when prompted to judge a speculative completion; fusion with token probabilities.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Numeric: XieZhi +10.3 percentage points (0.312→0.415), GSM8K +4.9 points (0.372→0.421) over greedy; qualitative case studies show SED corrected option/number selection errors (Appendix C).",
            "limitations_or_failure_cases": "See general SED limitations. Additional experiments (Table 3) show that when speculation decoding differs from generation decoding (misalignment) the performance gains reduce or turn negative (e.g., some combinations in Table 3 yield lower accuracy).",
            "comparison_to_other_methods": "Compared directly with greedy, beam, nucleus, and DoLa; SED ratio-based gave the best reported results for this model in Table 1.",
            "ablation_study_results": "Alpha tuning: α=0.8 chosen for XieZhi/GSM8K; alignment experiments in Table 3 show best results when generation and speculation decoding are aligned (greedy+greedy or beam+beam). Evaluation-quality experiments (Table 2) show using external stronger evaluators increases SED benefit for the generation model.",
            "uuid": "e8953.2",
            "source_info": {
                "paper_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SED (gemma-7b-it)",
            "name_full": "Self-Evaluation Decoding applied to gemma-7b-it",
            "brief_description": "Application of SED to the gemma-7b-it model; demonstrates SED improvements particularly on GSM8K and XieZhi in experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gemma-7b-it",
            "model_description": "Gemma 7B instruction-tuned model (Anil et al.-family), finetuned on the augmented datasets described in §3.4.",
            "reflection_method_name": "Self-Evaluation Decoding (SED)",
            "reflection_method_description": "As above: detection of chaotic points (ratio-based default), top-k speculation, self-evaluation to get p_se, fusion with p_gen via s = α*p_se + (1-α)*p_gen, select argmax(s).",
            "task_name": "XieZhi, HotpotQA, GSM8K",
            "task_description": "See general descriptions above.",
            "performance_with_reflection": "Ratio-based SED: XieZhi 0.330, HotpotQA 0.865, GSM8K 0.510 (Table 1).",
            "performance_without_reflection": "Greedy search: XieZhi 0.323, HotpotQA 0.840, GSM8K 0.472; beam search: XieZhi 0.330, HotpotQA 0.848, GSM8K 0.480 (Table 1).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "Internal generate-and-evaluate with gemma model; p_se extracted from judgment token logits; fused with token probabilities.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Numeric improvements on GSM8K of +3.8 points (0.472→0.510) and modest gains on XieZhi/HotpotQA; case studies show correction of chaotic-token errors.",
            "limitations_or_failure_cases": "Same general limitations: inference slower, sensitive to evaluator quality and decoding alignment; improvements vary by dataset and model.",
            "comparison_to_other_methods": "Outperforms greedy/nucleus/DoLa in many settings; beam sometimes competitive but SED often best on datasets exhibiting important chaotic points (numbers/options).",
            "ablation_study_results": "Reported parameter choices (k=5, b=5), α tuning per dataset (Appendix D) and detection criterion comparisons (ratio vs entropy).",
            "uuid": "e8953.3",
            "source_info": {
                "paper_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
                "publication_date_yy_mm": "2024-05"
            }
        },
        {
            "name_short": "SED with external evaluators",
            "name_full": "Self-Evaluation Decoding where evaluation (p_se) is provided by a stronger external LLM",
            "brief_description": "Variant where the generation/speculation is performed by a base model (llama2-7b-chat-hf in experiments) and the evaluation step uses a stronger external LLM (e.g., llama3-70b, yi-34b, qwen1.5-72b) to produce evaluation feedback to improve selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "llama2-7b-chat-hf (generation/speculation) with external evaluator(s) (llama3-8b, llama3-70b, yi-34b-chat, qwen1.5-72b)",
            "model_description": "Generation/speculation: llama2-7b-chat-hf (7B). External evaluators: larger/more capable LLMs (llama3-8b, llama3-70b, yi-34b-chat, qwen1.5-72b) used only to produce p_se in experiments in Table 2.",
            "reflection_method_name": "Self-Evaluation Decoding with external feedback providers",
            "reflection_method_description": "Speculate k candidate continuations using the generation model, then query an external stronger LLM to evaluate each speculative continuation's correctness and return p_se; fuse p_se with p_gen via s = α*p_se + (1-α)*p_gen (in the Table 2 experiment α=1 to test evaluation quality alone).",
            "task_name": "XieZhi (evaluated experiment)",
            "task_description": "Multiple-choice domain-knowledge benchmark; XieZhi emphasizes option-token chaotic points where correct option selection is critical.",
            "performance_with_reflection": "Table 2 (SE.gen): using external evaluators to provide feedback for llama2 production: SE.gen accuracies on XieZhi — self-eval: 0.380; llama3-8b eval: 0.505; llama3-70b eval: 0.584; yi-34b-chat eval: 0.620; qwen1.5-72b eval: 0.588 (accuracies of the generation model when using those evaluators' feedback).",
            "performance_without_reflection": "llama2-7b-chat-hf direct greedy generation accuracy (no SED): 0.312 on XieZhi (Table 2 'Direct gen.' row for llama2 self).",
            "has_performance_comparison": true,
            "mechanism_of_reflection": "External LLM acts as judge: the generation model produces candidate continuations; the external LLM evaluates each (outputs a judgment token probability used as p_se); p_se fused into propensity score used to select token. In the experiment α was set to 1 to isolate evaluation feedback quality.",
            "number_of_iterations": null,
            "evidence_for_improvement": "Quantitative: Table 2 shows that using stronger external evaluators yields higher SE.gen accuracy for the generation model compared to self-evaluation (e.g., llama2 self SE.gen 0.380 vs yi-34b evaluator SE.gen 0.620 on XieZhi). The paper interprets this as evidence that evaluation quality matters and that stronger evaluators can substantially improve SED's benefit.",
            "limitations_or_failure_cases": "Mixed effects in absolute numbers: Table 2 columns also report the direct generation accuracies of the external evaluators themselves; SE.gen of the base model does not always reach the direct accuracy of the external evaluator, and results are not strictly monotonic across evaluators. The approach adds additional latency and API/model-cost when using external models as evaluators.",
            "comparison_to_other_methods": "This is a variant of SED where the evaluator is external rather than the same model; authors note this can leverage stronger evaluation signals and could be combined with tool-based evaluation, but at higher inference cost.",
            "ablation_study_results": "Experiment in Table 2 isolates evaluation quality by setting α=1 (only p_se used). SE.gen increases substantially when evaluator is stronger than the generation model; authors discuss potential to use even stronger evaluators/tools for further gains.",
            "uuid": "e8953.4",
            "source_info": {
                "paper_title": "SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation",
                "publication_date_yy_mm": "2024-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chatglm-math: Improving math problem-solving in large language models with a self-critique pipeline",
            "rating": 2
        },
        {
            "paper_title": "Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation",
            "rating": 2
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2
        },
        {
            "paper_title": "Learning from mistakes makes LLM better reasoner",
            "rating": 2
        }
    ],
    "cost": 0.0178895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SED: Self-Evaluation Decoding Enhances Large Language Models for Better Generation</h1>
<p>Ziqin Luo ${ }^{1}$, Haixia Han ${ }^{3}$, Haokun Zhao ${ }^{2}$, Guochao Jiang ${ }^{1}$, Chengyu Du ${ }^{2}$, Tingyun $\mathbf{L i}^{2}$, Jiaqing Liang ${ }^{1}$, Deqing Yang ${ }^{1}$, Yanghua Xiao ${ }^{2,3}$<br>Shanghai Key Laboratory of Data Science, School of Data Science, Fudan University ${ }^{1}$ Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University ${ }^{2}$ Shanghai Institute of Artificial Intelligence for Education, East China Normal University ${ }^{3}$ {zqluo22,gcjiang22}@m.fudan.edu.cn, {haixiahan03, cydu2024, litinyun0715}@gmail.com, {liangjiaqing, yangdeqing, shawyh}@fudan.edu.cn</p>
<h4>Abstract</h4>
<p>Existing Large Language Models (LLMs) generate text through unidirectional autoregressive decoding methods to respond to various user queries. These methods tend to consider token selection in a simple sequential manner, making it easy to fall into suboptimal options when encountering uncertain tokens, referred to as chaotic points in our work. Many chaotic points exist in texts generated by LLMs, and they often significantly affect the quality of subsequently generated tokens, which can interfere with LLMs' generation. This paper proposes Self-Evaluation Decoding, SED, a decoding method for enhancing model generation. Analogous to the human decision-making process, SED integrates speculation and evaluation steps into the decoding process, allowing LLMs to make more careful decisions and thus optimize token selection at chaotic points. Experimental results across various tasks using different LLMs demonstrate SED's effectiveness.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) are equipped with rich word knowledge, relying on decoding methods to express their abilities during the inference phase. Existing decoding methods are mainly autoregressive and are primarily classified into two categories (Ippolito et al., 2019). Firstly, searchbased methods aim to maximize the probability of generated text, such as greedy search and beam search. The other is sampling-based methods, which sample the next generated token based on a probability distribution, such as nuclear sampling.
Despite good results, these autoregressive decoding methods for sequential token generation have limitations. LLMs often encounter highly uncertain tokens, referred to as chaotic points in this paper, during decoding. Figure 1 shows the token probability distribution at a chaotic point. The probability mass is quite dispersed, which suggests that the model does not know which token to choose. On the other hand, Answer 1 and Answer 2 in Figure 1 have the same content before the chaotic point but with different conclusions. At the chaotic point, Answer 1 selects token ' 8 ', resulting in an incorrect response, whereas Answer 2 chooses the token ' $i$ ', leading to a correct one. Different token choices lead to vastly different results, indicating that tokens at chaotic positions should be carefully selected. However, greedy search only considers the token with the highest probability, ignoring other tokens with considerable probability mass. Sampling is easily influenced by randomness and falling into irrelevant tokens. Though beam search can try multiple tokens, it only considers sentence-level</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The same model with different decoding methods generates two answers to the GSM8K question. The left is generated through greedy search, while the right is produced via SED. Tokens with $\square$ or $\square$ colored are the first chaotic points the model meets. Underlines represent the final generated text after the chaotic point. Corresponding sentence-level probabilities and perplexities are shown next to them. The token probability mass at the chaotic point is shown in the left-bottom.
probability maxima. In Figure 1, Answer 1 has a larger sentence-level probability and a lower perplexity than Answer 2, so beam search still fails to respond correctly.
The case in Figure 1 implies that the model's incorrect answer may not be due to its lack of relevant knowledge but because it does not know how to choose an optimal token at the chaotic point. LLMs should not only take the probabilities into account when selecting the token at a chaotic point. However, existing autoregressive decoding methods are limited to probability, not considering selecting the token from a holistic perspective. This could easily lead to inappropriate tokens and thus result in error propagation. Therefore, it is necessary to propose a more advanced decoding method to improve LLMs' token selection at chaotic points.
When making uncertain decisions, humans often speculate first and then evaluate the speculations, compare them, and make the final choice (Kahneman and Tversky, 2013), a common strategy called slow thinking (Kahneman, 2011) people use when dealing with complex decisions. This approach can assist LLMs in selecting appropriate tokens at chaotic points, but it requires LLMs to be equipped with evaluation ability. However, Luo et al. (2023) suggest the inherent evaluation ability of existing LLMs is weaker than their generation ability. Recently, many studies (Li et al., 2023a; Zhu et al., 2023; Ke et al., 2023) have shown that LLMs can significantly enhance their evaluation ability and thus achieve self-correction (Han et al., 2024) through training. In this paper, we propose Self-Evaluation Decoding, SED, a decoding method for improving uncertain tokens selection at chaotic points for LLMs. Mirroring human decision-making, when encountering a chaotic point during decoding, LLMs first speculate the results of choosing each token, then evaluate them, and finally select the token with the highest propensity score based on evaluations.
Our main contributions are summarized as follows:</p>
<ul>
<li>We propose straightforward criteria that help LLMs efficiently identify chaotic points and thus carefully select key tokens during decoding.</li>
<li>We propose Self-Evaluation Decoding (SED). Analogous to human decision-making, SED enables LLMs to speculate and evaluate uncertain tokens at chaotic points to select the optimal token and thus improve the generation’s quality compared to other standard decoding methods.</li>
<li>We design a simple but effective data synthesis strategy for enhancing LLMs’ evaluation ability based on multi-model ensembling, achieving self-evaluation in SED.</li>
</ul>
<h1>2 Related work</h1>
<p>Enhancing evaluation ability for LLMs. Recent studies (Fu et al., 2023; Luo et al., 2023; Zheng et al., 2023) reveal that LLMs’ evaluation ability can be seen as the ability to assess the quality of the object being evaluated (usually a question-answer pair). This ability is determined by the accuracy of the feedback generated by the evaluation model denoted as $M_{\text {eval }}$. The feedback is usually one rating (scalar), analysis (natural language text), or a combination of both. The more accurate the feedback, the better the evaluation. The approaches to enhancing LLM’s evaluation ability can be mainly categorized into prompt-based and tuning-based. The former designs well-structured prompts with detailed instructions to guide the model to evaluate. Since this approach relies entirely on the model’s inherent capability, closed-source models such as GPT-4 (OpenAI, 2023) and PaLM 2 (Anil et al., 2023b) are usually adopted as evaluation models, making the evaluation more expensive. However, being untrained, this method is difficult to adapt to specific downstream tasks and is greatly limited by the model’s inherent flaws (Wang et al., 2023a; Luo et al., 2023). The latter builds training datasets consisting of evaluation feedback to inject evaluation abilities into open-source LLMs through finetuning explicitly (Wang et al., 2023c;b). Many studies have demonstrated that the evaluation models built on direct training can achieve near-human evaluation quality in dialog, open-scene QA, fact verification, and other scenarios. These are effective and low-cost solutions for building evaluation models. In this work, we organically combine the LLMs’ evaluation ability with the other inherent ability, so that LLMs can not only generate but also evaluate the text it generates. Specifically, we enhance the model the evaluation ability through training over the augmented dataset with evaluation feedback included. We will talk about the details in Section 3.4.
Decoding methods for LLMs. Recent studies (Zhao et al., 2023; Xia et al., 2024) have focused on how to improve decoding methods for LLMs further. They can be partitioned into two types: acceleration-oriented and quality-oriented. The former focuses on accelerating the generation of LLMs from an algorithmic perspective. One representative work is speculative decoding (Xia et al., 2024), whose core idea is to use lightweight models with much lower decoding overhead for generation, and heavy-weight models to verify the generated tokens. In this way, the parallelism of decoding substantially increases thus realizing acceleration. The latter focuses on enhancing the quality of LLMs’ generation (Chuang et al., 2023; Chen et al., 2024; Roy et al., 2024). The main operation is to adjust the unnormalized logits during decoding in light of some signals or criteria. For instance, Li et al. (2023b) enhances generation diversity by contrasting the logits of the stronger model over the weaker one. Wang (2024) guides the model to sample correct tokens in the decoding phase by introducing constraints to make the model output more parsable to the computer. Huang et al. (2024) acquires reward signals based on formatted programs or parametric models and then makes the decoding more controllable. In contrast, we utilize signals from the model itself to guide decoding, thereby leveraging LLMs’ evaluation ability to improve the generation.</p>
<h2>3 Self-evaluation decoding</h2>
<p>While previous standard left-to-right decoding methods treat all tokens equally and employ a uniform policy, our proposed self-evaluation decoding (SED) emphasizes the tokens generated at chaotic points, yielding better responses. This paper primarily addresses three critical questions: (1) How can a chaotic point be identified? (2) Which tokens could be considered at the chaotic point? (3) Which token is the most suitable choice at the chaotic point? To this end, our SED method encompasses three modules: chaotic point detection, speculation and evaluation, and feedback backpropagation.
Autoregressive decoding generates tokens sequentially. Specifically, consider a sequence of generated tokens $\left{t_{1}, t_{2}, \ldots, t_{n}\right}$, where $n$ represents the total number of tokens. Each token $t_{i}$ (with $i \in[1, n]$ )</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The workflow of token selection at ONE chaotic point during decoding. Other chaotic points also follow a similar process. In this example, we adopt the ratio-based detection criterion and consider top 3 tokens for speculation and evaluation. We set $\alpha=0.8$ and $\delta_{r}=0.75$.</p>
<p>is generated from the distribution $\mathbb{P}<em _i="&lt;i">{i}=P\left(\cdot \mid x, t</em>}\right)$, which is conditioned on the input $x$ and the preceding tokens $t_{&lt;i}=\left{t_{1}, t_{2}, \ldots, t_{i-1}\right}$, over the whole vocabulary $\mathcal{V}$. During decoding, SED examines the conditional distribution $\mathbb{P<em i="i">{i}$ and calculates the indicator $\mathbb{I}</em>}:=\mathbb{I}\left(\mathbb{P<em i="i">{i}\right)$ to determine whether the token $t</em>}$ is hard for the model to predict. If $\mathbb{I}\left(\mathbb{P<em i="i">{i}\right)=0, t</em>}$ is selected through default decoding method (e.g. greedy search). If $\mathbb{I}\left(\mathbb{P<em i="i">{i}\right)=1$, the position i is identified as a chaotic point. SED selects the top $k$ tokens from $\mathbb{P}</em>$ to guide the selection of the optimal token at position i. By iteratively repeating this process, LLMs generate improved responses. The full SED pipeline is shown in Algorithm 1. Figure 2 shows what happens at a chaotic point in one iteration. Further details will be elaborated in subsequent sections.}$ for speculation and allows the model to evaluate these speculations. The model's self-evaluation is backpropagated and combined with $\mathbb{P}_{i</p>
<h3>3.1 Chaotic point detection</h3>
<p>Entropy-based. Entropy is commonly used to describe the complexity or uncertainty in information theory. During autoregressive decoding, the model generates the probability distribution $\mathbb{P}_{i}$ over vocabulary $\mathcal{V}$, with the entropy defined as</p>
<p>$$
\mathbb{E}<em i="i">{i}:=\mathbb{E}\left(\mathbb{P}</em>}\right)=-\sum_{j=1}^{|\mathcal{V}|} \mathbb{P<em _i="&lt;i">{i}\left(t^{(j)} \mid x, t</em>}\right) \log \mathbb{P<em _i="&lt;i">{i}\left(t^{(j)} \mid x, t</em>\right)
$$</p>
<p>$t^{(j)}$ is the $j$-th token in $\mathcal{V}$. We can use $\mathbb{E}_{i}$ to measure the model's uncertainty at position i.
Ratio-based. Another more intuitive measurement for uncertainty is the probability ratio:</p>
<p>$$
\mathbb{R}<em i="i">{i}:=\mathbb{R}\left(\mathbb{P}</em>}\right)=\frac{p_{\mathrm{sec}}}{p_{\max }}, \quad p_{\mathrm{sec}}=\mathbb{P<em _mathrm_sec="\mathrm{sec">{i}\left(t</em>}} \mid x, t_{&lt;i}\right), p_{\max }=\mathbb{P<em _max="\max">{i}\left(t</em>\right)
$$} \mid x, t_{&lt;i</p>
<p>$p_{\max }$ and $p_{\mathrm{sec}}$ are the maximum and second-largest probabilities in $\mathbb{P}<em _i="&lt;i">{i}\left(\mid x, t</em>$.}\right)$, with $t_{\max }$ and $t_{\mathrm{sec}}$ being the corresponding tokens. It follows the clear idea that the model is at least unsure which token between $t_{\max }$ and $t_{\mathrm{sec}}$ to take at position i when $p_{\mathrm{sec}}$ is close to $p_{\max </p>
<p>After obtaining $\mathbb{E}<em i="i">{i}$ and $\mathbb{R}</em>}$, we can determine whether position i is a chaotic point. Define indicator $\mathbb{I<em i="i">{i}:=\mathbb{I}\left(\mathbb{P}</em>\right)$ as</p>
<p>$$
\mathbb{I}\left(\mathbb{P}<em i="i">{i}\right)=\left{\begin{array}{cc}
1 &amp; \mathbb{E}</em> \
0 &amp; \text { otherwise }
\end{array} \quad \text { or } \quad \mathbb{I}\left(\mathbb{P}} \geq \delta_{c<em i="i">{i}\right)= \begin{cases}1 &amp; \mathbb{R}</em>\right) \
0 &amp; \text { otherwise. }\end{cases}\right.
$$} \geq \delta_{r} \text { or } p_{\max } \leq\left(1-\delta_{r</p>
<p>Input: user query $\mathcal{Q}$, model $\mathcal{M}$, tokenizer $\mathcal{T}$, max new tokens $m$, threshold $\delta$, number of token candidates $k$, vocabulary $\mathcal{V}$
Output: response $\mathcal{R}$ to the user query
generated_tokens = []
for each $i \in[1, m]$ do
$\mathbb{P}<em i="i">{i} \leftarrow$ CAL_PROBS( $\mathcal{M}, \mathcal{Q}$, generated_tokens) # probability distribution for predicting $t</em>$
$\mathbb{I}<em i="i">{i} \leftarrow$ IS_CHAOTIC $\left(\mathbb{P}</em>, \delta\right) \quad$ # judge whether position $i$ is a chaotic point
if $\mathbb{I}<em i="i">{i}==1$ then
tk_cands $\leftarrow$ SELECT_TOKEN_CANDIDATES $\left(k, \mathbb{P}</em>\right)$
self_eval_prob $\leftarrow$ SPECULATION_EVLUATION(tk_cands, $\mathcal{M}, \mathcal{V}$ )
scores $\leftarrow$ BACKPROPAGATION(self_eval_prob, $\mathbb{P}$ )
$t_{i} \leftarrow$ SELECT_TOKEN(scores, $\mathcal{V}$ ) # choose the token with the highest score as $t_{i}$
else
$t_{i} \leftarrow \operatorname{DECODING}(\mathcal{M}, \mathbb{P}) \quad$ # select $t_{i}$ by standard decoding method, e.g. greedy, beam, sampling
end if
generated_tokens.append $\left(t_{i}\right) \quad$ # record $t_{i}$ as the generated tokens for subsequent generation
if stopping criteria satisfied then
break
end if
end for
return CONVERT_TOKENS_TO_TEXT( $\mathcal{T}$, generated_tokens)
$\delta_{e}$ and $\delta_{r}$ are the uncertainty thresholds for entropy-based and ratio-based detection, respectively. Position i is considered a chaotic point for the model if $\mathbb{I}\left(\mathbb{P}_{i}\right)=1$. Figure 2 intuitively demonstrates how this operates. Note that, we set a branching factor $b$ as the upper limit for the number of detected points to control the algorithm's complexity.}, \mathcal{V</p>
<h1>3.2 Speculation and evaluation</h1>
<p>If position i is chaotic, we need to speculate first. Specifically, we select top $k$ tokens $\left{t_{i}^{(j)}\right}<em i="i">{j=1}^{k}$ with the highest probabilities from distribution $\mathbb{P}</em>\right]\right}}$. These tokens are appended to the existing context $t_{&lt;i}$ to form $k$ new contexts $\left{\left[t_{&lt;i} ; t_{i}^{(j)<em i="i">{j=1}^{k}$, differing only in the last token $t</em>$.}^{(j)}$. The model then completes these $k$ contexts, resulting in $k$ responses $\left{R_{s e}^{(j)}\right}_{j=1}^{k</p>
<p>For evaluation, the model itself evaluates $\left{R_{s e}^{(j)}\right}<em e="e" s="s">{j=1}^{k}$. As we focus on the QA scenario, the evaluation feedback specifically refers to the likelihood of responses being considered correct. Formally, given the primal user question $Q$ and $R</em>$. we extract the model's output logits and calculate the self-evaluation probability}^{(j)}$, the model judge $R_{s e}^{(j)}$ 's correctness as $J_{s e}^{(j)</p>
<p>$$
p_{s e}^{(j)}:=\mathbb{P}\left(J_{s e}^{(j)}=1 \mid Q, R_{s e}^{(j)}\right) \in[0,1], \quad \text { with } J_{s e}^{(j)}=\left{\begin{array}{lc}
1 &amp; R_{s e}^{(j)} \text { is correct to } Q \
0 &amp; \text { otherwise }
\end{array}\right.
$$</p>
<p>Through $p_{s e}^{(j)}$, we can tell the model confidence on the speculation $R_{s e}^{(j)}$ and further determine the token selection propensity at position i. Considering the inference cost, the model adopts standard decoding methods during speculation. Details about $p_{s e}^{(j)}$ are presented in Appendix A. We will discuss the effects of different methods on speculation in Section 4.6.</p>
<h3>3.3 Feedback backpropagation</h3>
<p>After Speculation and Evaluation, we get $\left{p_{s e}^{(j)}\right}<em i="i">{j=1}^{k}$ for $k$ potential candidates $\left{t</em>\right}}^{(j)<em e="e" s="s">{j=1}^{k}$. Back to position i, we fuse $\left{p</em>\right}}^{(j)<em e="e" g="g" n="n">{j=1}^{k}$ with the token probabilities $\left{p</em>\right}}^{(j)<em i="i">{j=1}^{k}:=\left{\mathbb{P}</em>\right)\right}}\left(t_{i}^{(j)} \mid x, t_{&lt;i<em e="e" s="s">{j=1}^{k}$ which are what we already get during decoding. $p</em>$ focuses on the present. We introduce the concept}^{(j)}$ can be viewed as the model's expected benefit from choosing $t_{i}^{(j)}$, while $p_{g e n}^{(j)}$ represents the model's confidence in choosing $t_{i}^{(j)}$ under the current conditions. $p_{s e}^{(j)}$ focuses on the future, whereas $p_{g e n}^{(j)</p>
<p>of propensity score by combining the two to assist the model in choosing the optimal token at the chaotic point. Formally, for $t_{i}^{(j)}$, we define its propensity score as</p>
<p>$s_{i}^{(j)}=\alpha<em>p_{ae}^{(j)}+(1-\alpha)</em>p_{gen}^{(j)}$</p>
<p>Here, $\alpha$ is the fusion coefficient controlling the fusion between the evaluation feedback and token probability. The higher $s_{i}^{(j)}$, the better bidirectional feature token $t_{i}^{(j)}$ has. Finally, we select the token with the highest propensity score for the subsequent generation.</p>
<h1>3.4 Evaluation ability enhancing</h1>
<p>Speculation and Evaluation and Feedback Backpropagation represent the forward and backward information flow, respectively. In the former phase, the model reveals the effect of the selected token on the final generation through deduction and evaluation. The model's self-evaluation feedback then, in turn, improves the token selection at the chaotic point. Essentially, self-evaluation decoding enhances the model's generation through its evaluation. Perceptibly, accurate feedback is quite important for self-evaluative decoding to enhance generation. Recent studies ( Li et al., 2023a; Xu et al., 2024) have shown LLMs can be trained to build good evaluation ability. Here we propose a simple but effective data augmentation approach to efficiently construct instruction finetuning data to empower the model's evaluation ability.
Multi-model generation ensembling. Given a downstream task dataset $\mathcal{D}=\left{\left(q_{i}, a_{i}\right)\right}<em i="i">{i=1}^{n}$ and a collection of LLMs $\mathcal{M}=\left{M</em>\right}<em _eval="{eval" _text="\text">{i=1}^{m}$, we use each model in $\mathcal{M}$ to answer each question in $\mathcal{D}$, which in turn yields model responses $\left{r</em>\right}}}^{i<em _eval="{eval" _text="\text">{i=1}^{n * m}$ in a CoT format (Wei et al., 2022). We compare these generated responses with the question answers to obtain the final augmented dataset $\mathcal{D}</em>\right)\right}}}=$ $\left{\left(q_{\text {eval }}^{i}, r_{\text {eval }}^{i}, j_{\text {eval }}^{i<em _eval="{eval" _text="\text">{i=1}^{n * m}$, where the judgment $j</em>}}^{i}$ denotes the correctness of the response $r_{\text {eval }}^{i}$. In this way, each $q_{i}$ in $\mathcal{D}$ receives multiple, CoT-formatted responses from different LLMs with different correctnesses. Furthermore, we extract all the correct responses in $\mathcal{D<em _gen="{gen" _text="\text">{\text {eval }}$ thereby constructing the dataset $\mathcal{D}</em>\right)\right}\left}}}=\left{\left(q_{\text {gen }}^{i}, r_{\text {gen }}^{i<em _eval="{eval" _text="\text">{\text {l=1 }}^{# \text { correct }}$. $\mathcal{D}</em>}}$ and $\mathcal{D<em _eval="{eval" _text="\text">{\text {gen }}$ are used to train the model's evaluation and generation abilities on the downstream tasks, respectively.
Training. Similar to previous studies, we use $\mathcal{D}</em>$ :}}$ to train the model's evaluation ability. Specifically, the model learns to output $j_{\text {eval }}^{i}$ given $q_{\text {eval }}^{l}$ and $r_{\text {eval }}^{l</p>
<p>$$
\mathcal{L}<em _Theta="\Theta">{\text {eval }}^{l}(\Theta)=-\log \mathbb{P}</em>\right)
$$}\left(j_{\text {eval }}^{l} \mid q_{\text {eval }}^{l}, r_{\text {eval }}^{l</p>
<p>$\Theta$ represents the model's parameters. The model's generation ability is developed by the loss function:</p>
<p>$$
\mathcal{L}<em _Theta="\Theta">{\text {gen }}^{l}(\Theta)=-\log \mathbb{P}</em>\right)
$$}\left(r_{\text {gen }}^{l} \mid q_{\text {gen }}^{l</p>
<p>Here, we adopt multi-task learning to train the model to build the generation and evaluation abilities simultaneously:</p>
<p>$$
\min <em _Theta="\Theta">{\Theta} \mathcal{L}(\Theta)=\min </em>} \sum_{i=1} \mathcal{L<em j="1">{\text {eval }}^{i}(\Theta)+\sum</em>(\Theta)
$$} \mathcal{L}_{\text {gen }}^{j</p>
<p>Data sampled from $\mathcal{D}<em _gen="{gen" _text="\text">{\text {eval }}$ and $\mathcal{D}</em>}}$ respectively and the ratio is maintained around 1:1. They are mixed to build the final training set. Meanwhile, the sampled data from $\mathcal{D<em _eval="{eval" _text="\text">{\text {eval }}$ has approximately the same number of samples with $j</em>$ are available in Appendix A.
We build the dataset described above and train the model in a multi-task learning manner. This method enhances the model's abilities in two ways. By integrating multiple model generations, representing diverse thoughts on the same question, the model can learn the nuanced differences and thus know how to analyze the question from several perspectives. Meanwhile, recent studies (Yu et al., 2023; An et al., 2023) have shown that understanding a question from the opposite is an effective way to enhance the model's comprehension of the downstream tasks. Therefore, learning to judge the correctness of an answer from two sides should also be an effective way to boost the model's abilities.}}=1$ and $j_{\text {eval }}=0$. More details about $j_{\text {eval }</p>
<h2>4 Experiment</h2>
<h3>4.1 Datasets</h3>
<p>We use three datasets: (i) the HotpotQA subset (Yang et al., 2018), processed into the SQuAD format, from the MRQA 2019 Shared Task (Fisch et al., 2019). Retrieval is not needed, allowing LLMs to</p>
<p>Table 1: The performances of different decoding strategies for downstream tasks. Models with marker ‘<em>’ are trained only on $\mathcal{D}_{g c n}$, while those without marker ‘</em>’ are trained on $\mathcal{D}<em a="a" g="g" n="n">{g c n}$ and $\mathcal{D}</em>$. We keep the training steps of them equal for comparison. Experiments of sampling-based methods are repeated 5 times to derive the mean and std. Bold and underline denote the best and second best.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LLMs</th>
<th style="text-align: center;">Decoding Strategies</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">XieZhi</td>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">GSM8K</td>
</tr>
<tr>
<td style="text-align: center;">falcon-7b-instruct*</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.251</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.167</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.265</td>
<td style="text-align: center;">0.722</td>
<td style="text-align: center;">0.175</td>
</tr>
<tr>
<td style="text-align: center;">falcon-7b-instruct</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.272</td>
<td style="text-align: center;">0.735</td>
<td style="text-align: center;">0.225</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.292</td>
<td style="text-align: center;">0.736</td>
<td style="text-align: center;">0.216</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">nuclear sampling</td>
<td style="text-align: center;">$0.248 \pm 0.019$</td>
<td style="text-align: center;">$0.706 \pm 0.014$</td>
<td style="text-align: center;">$0.204 \pm 0.022$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SED, ratio-based</td>
<td style="text-align: center;">0.338</td>
<td style="text-align: center;">0.750</td>
<td style="text-align: center;">0.256</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SED, entropy-based</td>
<td style="text-align: center;">0.326</td>
<td style="text-align: center;">0.745</td>
<td style="text-align: center;">0.244</td>
</tr>
<tr>
<td style="text-align: center;">llama2-7b-chat-hf*</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.304</td>
<td style="text-align: center;">0.794</td>
<td style="text-align: center;">0.326</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.329</td>
<td style="text-align: center;">0.815</td>
<td style="text-align: center;">0.352</td>
</tr>
<tr>
<td style="text-align: center;">llama2-7b-chat-hf</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.312</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.372</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.336</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.396</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">nuclear sampling</td>
<td style="text-align: center;">$0.255 \pm 0.022$</td>
<td style="text-align: center;">$0.783 \pm 0.017$</td>
<td style="text-align: center;">$0.342 \pm 0.025$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">DoLa</td>
<td style="text-align: center;">0.286</td>
<td style="text-align: center;">0.725</td>
<td style="text-align: center;">0.323</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SED, ratio-based</td>
<td style="text-align: center;">0.415</td>
<td style="text-align: center;">0.822</td>
<td style="text-align: center;">0.421</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SED, entropy-based</td>
<td style="text-align: center;">0.395</td>
<td style="text-align: center;">0.820</td>
<td style="text-align: center;">0.405</td>
</tr>
<tr>
<td style="text-align: center;">gemma-7b-it*</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.294</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.462</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.288</td>
<td style="text-align: center;">0.801</td>
<td style="text-align: center;">0.470</td>
</tr>
<tr>
<td style="text-align: center;">gemma-7b-it</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.323</td>
<td style="text-align: center;">0.840</td>
<td style="text-align: center;">0.472</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.848</td>
<td style="text-align: center;">0.480</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">nuclear sampling</td>
<td style="text-align: center;">$0.288 \pm 0.018$</td>
<td style="text-align: center;">$0.822 \pm 0.017$</td>
<td style="text-align: center;">$0.456 \pm 0.018$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SED, ratio-based</td>
<td style="text-align: center;">0.330</td>
<td style="text-align: center;">0.865</td>
<td style="text-align: center;">0.510</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">SED, entropy-based</td>
<td style="text-align: center;">0.335</td>
<td style="text-align: center;">0.862</td>
<td style="text-align: center;">0.498</td>
</tr>
</tbody>
</table>
<p>focus on answering questions in light of the references; (ii) XieZhi (Gu et al., 2024), a comprehensive evaluation suite with a multiple-choice format, aiming to test diverse subject knowledge of LLMs; and (iii) GSM8K (Cobbe et al., 2021), a popular mathematical reasoning benchmark for evaluating LLMs’ mathematical abilities, consists of high-quality elementary school math word problems. They cover diverse application scenarios of LLMs, such as multi-hop reasoning, multi-disciplinary knowledge comprehension, and mathematical reasoning.</p>
<h1>4.2 Baselines and metrics</h1>
<p>We choose greedy search, beam search, and nuclear sampling as our baselines. They are widely used decoding methods in LLM inference. We also compare with DoLa (Chuang et al., 2023), a contrastive decoding that alleviates hallucinations. falcon-7b-instruct (Penedo et al., 2023), llama2-7b-chat-hf (Touvron et al., 2023), and gemma-7b-it (Anil et al., 2023a) are trained on all datasets through supervised fine-tuning described in Section 3.4. We evaluate the performance of our proposed self-evaluation decoding by comparing it with other decoding baselines across different fine-tuned models. Due to the official open-source code of DoLa currently only supporting the Llama series models, we use it as a baseline only on Llama2.
We compare the answers extracted from the model responses with the annotations to calculate accuracy, thereby measuring the performance of different decoding methods. We uniformly sample 1,000 instances for testing from each dataset to manage experimental costs.</p>
<h3>4.3 Experimental settings</h3>
<p>As mentioned in Section 3.4, we employ multiple LLMs to augment downstream datasets, including baichuan2-7b-chat (Yang et al., 2023), qwen-7b-chat (Bai et al., 2023), mistral-7b-instruct-v0.2 (Jiang et al., 2023) and metamath-mistral-7b (Yu et al., 2023)</p>
<p>Table 2: The effects of feedback quality on generation. We use trained llama2-7b-chat-hf for generation and speculation, and other stronger LLMs for evaluation. Set $\alpha=1$ to fully test the evaluation feedback quality. ‘Direct gen.’ indicates the accuracies the LLMs achieve through directly answering questions with greedy search. ‘SE. gen.’ means the accuracies that llama2-7b-chat-hf achieves through self-evaluation decoding with various LLMs as the feedback providers respectively.</p>
<p>| LLMs (Feedback Source) | XieZhi Acc. | |
| | Direct gen. | SE. gen. |
| llama2-7b-chat-hf (self) | 0.312 | 0.380 |
| llama3-8b-instruct | 0.644 | 0.505 |
| llama3-70b-instruct | 0.663 | 0.584 |
| yi-34b-chat | 0.665 | 0.620 |
| qwen1.5-72b-chat | 0.681 | 0.588 |</p>
<p>(specifically for GSM8K). Prompt details can be found in Appendix B. For every question in each dataset, we sample multiple responses from each model and filter out duplicate and incorrectly formatted responses. We extract the answers through regular expressions from those retained responses and compare them with annotations to determine their correctness. Finally, according to the operations described in Section 3.4, $\mathcal{D}<em _gen="{gen" _text="\text">{\text {eval }}$ and $\mathcal{D}</em>=3$ and $\tau=0.7, p=0.9$, respectively. More hyperparameters for the experiment are reported in Appendix D.}}$ are obtained. We train the models on 2 A800 GPUs using ZeRO (Rajbhandari et al., 2020) stage 3 from DeepSpeed (Rasley et al., 2020). We adopt AdamW (Kingma and Ba, 2015) as the optimizer and set the batch size to 8 with a maximum sequence length of 1,024 and one training epoch. The number of beams $n_{b}$ of beam search and the temperature $\tau$ and $p$ of nucleus sampling are set to $n_{b</p>
<h1>4.4 Main results</h1>
<p>Table 1 presents the three LLMs' results that employ various decoding methods. Beam search and nuclear sampling explore a wider solution space than greedy search, and nuclear sampling increases diversity. However, compared to greedy search, nuclear sampling performs poorly with large fluctuations, indicating nuclear sampling is not a good choice in QA scenarios with standard answers. On the other hand, beam search is superior to greedy search in most cases, suggesting that a larger solution space helps to get a more accurate response. Alleviating hallucination seems not helpful, since DoLa performs poorly consistently. Both entropy-based and ratio-based SED outperform other baselines across multiple models and datasets, especially for falcon and llama2 on XieZhi and GSM8K. As shown, in most cases, the ratio-based SED performs slightly better than the entropy-based. We attribute this to the former's more specific constraints on token probabilities, which help avoid misjudging chaotic points. Additionally, adjusting the ratio threshold is much more convenient than the entropy threshold. Thus, ratio-based SED is more practical compared to entropy-based. In subsequent experiments, ratio-based SED is the default implementation. We observe that the improvement in model performance for SED, relative to the baseline models, varies across different datasets. Specifically, the performance increment is more pronounced in the XieZhi and GSM8K compared to HotpotQA. Chaotic points in GSM8K often occur at numbers and operators, whereas in XieZhi, they frequently appear at the options, as shown in Table 6 and Table 4. The impact of these chaotic points on the accuracy of model responses is significant. SED allows the model to effectively select tokens at these positions, thereby avoiding incorrect answers. It is important to note that when $\mathcal{D}<em _eval="{eval" _text="\text">{\text {eval }}$ is incorporated into the training process, models' accuracies increase to varying extents. This suggests that enhancing a model's evaluation ability could indirectly improve its generation ability, which may be related to the introduction of incorrect responses in $\mathcal{D}</em>$ to enable the model to learn how to identify flaws and thereby enhance its ability from the opposite perspective (An et al., 2023). More cases that demonstrate SED's effect are available in Appendix C.}</p>
<h3>4.5 Analysis of evaluation quality</h3>
<p>Intuitively, the evaluation quality heavily influences the model's final response. Ideally, if the evaluation quality is sufficiently high, the model can generate an optimal response by completely following the evaluation feedback whenever it encounters a chaotic point. To investigate the effect of evaluation quality, we adopt other stronger LLMs as the speculation evaluators to generate more accurate evaluation feedback. The results are shown in Table 2. In addition to llama2's self-evaluation, we also use llama3-8b/70b-instruct (AI@Meta, 2024), yi-34b-chat (Young et al., 2024), and</p>
<p>Table 3: The effects of alignment and misalignment between generation and speculation. llama2-7b-chat-hf for generation, speculation, and evaluation with $\alpha=1$. The results with Speculation being '-' indicate they are directly obtained through baseline decoding methods without applying our proposed self-evaluation framework.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Generation</th>
<th style="text-align: center;">Speculation</th>
<th style="text-align: center;">Acc.</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">HotpotQA</td>
<td style="text-align: center;">GSM8K</td>
</tr>
<tr>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.804</td>
<td style="text-align: center;">0.372</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.816</td>
<td style="text-align: center;">0.416</td>
</tr>
<tr>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.802</td>
<td style="text-align: center;">0.378</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.808</td>
<td style="text-align: center;">0.396</td>
</tr>
<tr>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.805</td>
<td style="text-align: center;">0.388</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.819</td>
<td style="text-align: center;">0.412</td>
</tr>
<tr>
<td style="text-align: center;">nuclear sampling</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.783</td>
<td style="text-align: center;">0.342</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">greedy search</td>
<td style="text-align: center;">0.778</td>
<td style="text-align: center;">0.337</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">beam search</td>
<td style="text-align: center;">0.791</td>
<td style="text-align: center;">0.344</td>
</tr>
</tbody>
</table>
<p>qwen1.5-72b-chat (Bai et al., 2023) to evaluate the speculation. We choose XieZhi (Gu et al., 2024) for testing. As mentioned before, the option token in XieZhi is an important chaotic point. The evaluation feedback at the option token can significantly affect the decoding result. Here, we use these models to directly answer questions and then obtain their accuracies (Direct gen. in Table 2) to approximate their evaluation quality, as these accuracies are the most intuitive representation of LLMs' capabilities. Results in Table 2 (SE. gen.) clearly show that llama2's accuracy is significantly improved when we use a more powerful model to provide more accurate evaluation feedback. This demonstrates evaluation quality is crucial in SED. The more accurate the evaluation feedback, the better SED works. The results also suggest that we could make use of tools to further enhance the evaluation quality and thus further improve the decoding in other tool-friendly application scenarios.</p>
<h1>4.6 Analysis of generation-speculation alignment</h1>
<p>Besides evaluation, speculation is also an important step for SED. As described in Section 3.2, speculation refers to inferring the text that the model will subsequently generate after token $t_{i}$ is selected. This requires that the speculation needs to be accurate enough. More specifically, the model needs to have the same token selected at speculation as at generation, i.e., the decoding methods at both stages should be aligned. To investigate the impact of the decoding methods' alignment, we check various decoding combinations in the generation and speculation phases. As shown in Table 3, the model performance is improved when the decoding methods at generation and speculation are aligned. Otherwise, SED provides a limited gain to the model and sometimes negatively affects the performance. This implies that SED works when the speculation and generation are aligned. The misalignment could give the model a misleading, which in turn affects the model's final response. We choose the 'greedy + greedy' combination as the implementation for SED in Table 1 since it requires less computation than 'beam + beam' with similar performance.</p>
<h2>5 Conclusion</h2>
<p>In this paper, we propose a decoding method called self-evaluation decoding, SED, to optimize LLMs' token selection at chaotic points during decoding so that the most appropriate token can be selected. SED first detects the chaotic points and then obtains the self-evaluation probability of each token at the chaotic points through two steps: speculation and evaluation, and finally combines the self-evaluation probabilities with the token probabilities to determine the optimal token. Finally, experimental results and case studies across various tasks demonstrate the effectiveness of our method.</p>
<h2>6 Limitation</h2>
<p>Our method outperforms other baselines across different models on various tasks, illustrating its effectiveness. However, there remain some limitations. First, judgment $j_{\text {eval }}^{i}$ in $\mathcal{D}_{\text {eval }}$ is relatively simple now, lacking of CoT or other enhanced information. Second, the introduction of speculation makes the decoding slower since more computations are required for selecting the optimal token. Indeed, it is a trade-off between inference efficiency and quality. Kahneman (2011) mentions two</p>
<p>types of decision-making behaviors: fast thinking and slow thinking. Slow thinking is responsible for solving complex problems that are hard for fast thinking, needing additional computational resources for logical and rational thinking. In other words, to optimize LLMs' token selection at chaotic points, we ought to allow LLMs to think slowly, and exchange the extra inference time for better generation, which is exactly the core contribution of this paper. Regarding the additional time cost introduced by slow thinking, we could reduce it through methods such as reusing computation. We leave the optimizations for evaluation ability enhancement and decoding speed as our future work.</p>
<h1>References</h1>
<p>AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/ main/MODEL_CARD.md.</p>
<p>Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, and Weizhu Chen. Learning from mistakes makes LLM better reasoner. CoRR, abs/2310.20689, 2023. doi: 10.48550/ARXIV. 2310.20689. URL https://doi.org/10.48550/arXiv. 2310.20689.</p>
<p>Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805, 2023a. doi: 10.48550/ARXIV.2312.11805. URL https://doi.org/10.48550/arXiv. 2312.11805.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. Palm 2 technical report. CoRR, abs/2305.10403, 2023b. doi: 10.48550/ARXIV.2305.10403. URL https://doi.org/10.48550/arXiv. 2305.10403.</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report. CoRR, abs/2309.16609, 2023. doi: 10.48550/ARXIV.2309.16609. URL https://doi.org/10.48550/arXiv. 2309.16609.</p>
<p>Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, and Junxian He. In-context sharpness as alerts: An inner representation perspective for hallucination mitigation. CoRR, abs/2403.01548, 2024. doi: 10.48550/ARXIV.2403.01548. URL https://doi.org/10. 48550/arXiv. 2403.01548.</p>
<p>Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. CoRR, abs/2309.03883, 2023. doi: 10.48550/ARXIV.2309.03883. URL https://doi.org/10.48550/ arXiv. 2309.03883.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. URL https://arxiv.org/abs/2110.14168.</p>
<p>Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen, editors, Proceedings of the 2nd Workshop on Machine Reading for Question Answering, MRQA@EMNLP 2019, Hong Kong, China, November 4, 2019, pages 1-13. Association for Computational Linguistics, 2019. doi: 10.18653/V1/D19-5801. URL https://doi.org/10.18653/v1/D19-5801.</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. CoRR, abs/2302.04166, 2023. doi: 10.48550/ARXIV.2302.04166. URL https://doi.org/10.48550/ arXiv. 2302.04166.</p>
<p>Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, Qianyu He, Rui Xu, Wenhao Huang, Jingping Liu, Zili Wang, Shusen Wang, Weiguo Zheng, Hongwei Feng, and Yanghua Xiao. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 18099-18107. AAAI Press, 2024. doi: 10.1609/AAAI. V38I16.29767. URL https://doi.org/10.1609/aaai.v38i16.29767.</p>
<p>Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, and Yanghua Xiao. Small language model can self-correct. In Michael J. Wooldridge, Jennifer G. Dy, and Sriraam Natarajan, editors, Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pages 18162-18170. AAAI Press, 2024. doi: 10.1609/AAAI.V38I16.29774. URL https://doi.org/10.1609/aaai.v38i16.29774.</p>
<p>James Y. Huang, Sailik Sengupta, Daniele Bonadiman, Yi'an Lai, Arshit Gupta, Nikolaos Pappas, Saab Mansour, Katrin Kirchhoff, and Dan Roth. Deal: Decoding-time alignment for large language models. CoRR, abs/2402.06147, 2024. doi: 10.48550/ARXIV.2402.06147. URL https://doi.org/10.48550/arXiv.2402.06147.</p>
<p>Daphne Ippolito, Reno Kriz, João Sedoc, Maria Kustikova, and Chris Callison-Burch. Comparison of diverse decoding methods from conditional language models. In Anna Korhonen, David R. Traum, and Lluís Màrquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 3752-3762. Association for Computational Linguistics, 2019. doi: 10.18653/V1/ P19-1365. URL https://doi.org/10.18653/v1/p19-1365.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. CoRR, abs/2310.06825, 2023. doi: 10.48550/ARXIV.2310.06825. URL https://doi.org/10.48550/arXiv.2310.06825.</p>
<p>Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, United States, 2011. ISBN 9780374275631.</p>
<p>Daniel Kahneman and Amos Tversky. Prospect theory: An analysis of decision under risk. In Handbook of the fundamentals of financial decision making: Part L pages 99-127. World Scientific, 2013.</p>
<p>Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. Critiquellm: Scaling llm-as-critic for effective and explainable evaluation of large language model generation. CoRR, abs/2311.18702,</p>
<ol>
<li>doi: 10.48550/ARXIV.2311.18702. URL https://doi.org/10.48550/arXiv. 2311 . 18702 .</li>
</ol>
<p>Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.</p>
<p>Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment. CoRR, abs/2310.05470, 2023a. doi: 10.48550/ARXIV.2310.05470. URL https://doi.org/10.48550/arXiv. 2310.05470.</p>
<p>Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 12286-12312. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.ACL-LONG.687. URL https://doi.org/10. 18653/v1/2023.acl-long. 687.</p>
<p>Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang, and Lei Meng. Critique ability of large language models. CoRR, abs/2310.04815, 2023. doi: 10.48550/ARXIV.2310.04815. URL https://doi.org/10.48550/arXiv. 2310.04815.</p>
<p>OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. URL https://doi.org/10.48550/arXiv.2303.08774.</p>
<p>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon LLM: outperforming curated corpora with web data only. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ fa3ed726cc5073b9c31e3e49a807789c-Abstract-Datasets_and_Benchmarks.html.</p>
<p>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer, editors, Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, page 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https: //doi.org/10.1109/SC41405.2020.00024.</p>
<p>Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Rajesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash, editors, KDD '20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020, pages 3505-3506. ACM, 2020. doi: 10.1145/3394486.3406703. URL https://doi.org/10.1145/3394486.3406703.</p>
<p>Shamik Roy, Sailik Sengupta, Daniele Bonadiman, Saab Mansour, and Arshit Gupta. FLAP: flow-adhering planning with constrained decoding in llms. CoRR, abs/2403.05766, 2024. doi: 10.48550/ARXIV.2403.05766. URL https://doi.org/10.48550/arXiv.2403.05766.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan</p>
<p>Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. doi: 10.48550/ARXIV.2307.09288. URL https://doi.org/10.48550/ arXiv. 2307.09288.</p>
<p>Jiaye Wang. Constraining large language model for generating computer-parsable content. arXiv preprint arXiv:2404.05499, 2024.</p>
<p>Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. CoRR, abs/2305.17926, 2023a. doi: 10.48550/ARXIV.2305.17926. URL https://doi.org/10.48550/arXiv.2305.17926.</p>
<p>Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. Shepherd: A critic for language model generation. CoRR, abs/2308.04592, 2023b. doi: 10.48550/ARXIV. 2308. 04592. URL https://doi.org/10.48550/arXiv.2308.04592.</p>
<p>Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue Zhang. Pandalm: An automatic evaluation benchmark for LLM instruction tuning optimization. CoRR, abs/2306.05087, 2023c. doi: 10.48550/ARXIV.2306.05087. URL https://doi.org/10.48550/arXiv.2306.05087.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.</p>
<p>Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: A comprehensive survey of speculative decoding. CoRR, abs/2401.07851, 2024. doi: 10.48550/ARXIV.2401.07851. URL https://doi.org/10.48550/arXiv. 2401.07851.</p>
<p>Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, and Yuxiao Dong. Chatglm-math: Improving math problemsolving in large language models with a self-critique pipeline. CoRR, abs/2404.02893, 2024. doi: 10.48550/ARXIV.2404.02893. URL https://doi.org/10.48550/arXiv.2404.02893.</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai, Guosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji, Jian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma, Mang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun, Tao Zhang, Tianpeng Li, Tianyu Li, Wei Cheng, Weipeng Chen, Xiangrong Zeng, Xiaochuan Wang, Xiaoxi Chen, Xin Men, Xin Yu, Xuehai Pan, Yanjun Shen, Yiding Wang, Yiyu Li, Youxin Jiang, Yuchen Gao, Yupeng Zhang, Zenan Zhou, and Zhiying Wu. Baichuan 2: Open large-scale language models. CoRR, abs/2309.10305, 2023. doi: 10.48550/ARXIV.2309.10305. URL https://doi.org/10.48550/arXiv.2309.10305.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun'ichi Tsujii, editors, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2369-2380. Association for Computational Linguistics, 2018. doi: 10.18653/V1/D18-1259. URL https://doi.org/10.18653/v1/d18-1259.</p>
<p>Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng</p>
<p>Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai. CoRR, abs/2403.04652, 2024. doi: 10.48550/ARXIV. 2403. 04652. URL https://doi.org/10. $\overline{48550} /$ arXiv. 2403.04652.</p>
<p>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. CoRR, abs/2309.12284, 2023. doi: 10.48550/ARXIV.2309.12284. URL https://doi.org/10. $\overline{48550} /$ arXiv. 2309.12284.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. A survey of large language models. CoRR, abs/2303.18223, 2023. doi: 10.48550/ ARXIV.2303.18223. URL https://doi.org/10. $\overline{48550} /$ arXiv. 2303.18223.</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html.</p>
<p>Lianghui Zhu, Xinggang Wang, and Xinlong Wang. Judgelm: Fine-tuned large language models are scalable judges. CoRR, abs/2310.17631, 2023. doi: 10.48550/ARXIV.2310.17631. URL https://doi.org/10. $\overline{48550} /$ arXiv. 2310.17631.</p>
<h1>A Judgement representation and inference</h1>
<p>$j_{\text {eval }}^{l}$ in Section 3.4 is represented in numeric. Nevertheless, we need to convert it to text for training. Denote $t_{j_{\text {eval }}^{l}}$ as the actual output that the model learns to generate given the context of $q_{\text {eval }}^{l}$ and $r_{\text {eval }}^{l}$. When we maximize $\mathbb{P}<em _eval="{eval" _text="\text">{\Theta}\left(j</em>\right)$, we are actually maximizing the probability}}^{l} \mid q_{\text {eval }}^{l}, r_{\text {eval }}^{l</p>
<p>$$
\mathbb{P}<em _eval="{eval" j__text="j_{\text">{\Theta}\left(t</em>
\text { " }[\mathrm{Y}] \text { ], the answer is correct." } &amp; j_{\text {eval }}^{l}=1 \
\text { " }[\mathrm{N}] \text { ], the answer is incorrect." } &amp; j_{\text {eval }}^{l}=0
\end{array}\right.
$$}}^{l}} \mid q_{\text {eval }}^{l}, r_{\text {eval }}^{l}\right), \quad t_{j_{\text {eval }}^{l}}=\left{\begin{array}{cc</p>
<p>Therefore, in terms of calculating $p_{s e}^{(j)}$ in Section 3.2, we are actually discussing about the probability</p>
<p>$$
p_{s e}^{(j)}:=\mathbb{P}\left(J_{s e}^{(j)}=1 \mid Q, R_{s e}^{(j)}\right)=\mathbb{P}\left(\text { " }[[] \cdot]] \text { " = " }[\mathrm{Y}] \text { ]" } \mid Q, R_{s e}^{(j)}\right)
$$</p>
<p>More specifically, we take the model's output logits at the position behind token "[[" and calculate the probability of token ' Y ' being selected, which is essentially the $p_{s e}^{(j)}$ we want.</p>
<h2>B Prompt templates</h2>
<p>This section shows the prompt templates for generating multi-model responses, model inference, and self-evaluation. The few-shot templates for XieZhi and GSM8K are shown in Table 7 and Table 9, while the zero-shot template for HotpotQA is shown in Table 8. We apply three prompt templates shown in Table 10 to model inference on different datasets. The self-evaluation prompt for Evaluation step in self-evaluation decoding is also shown in Table 10.</p>
<h2>C Case study on the behavior of self-evaluative decoding</h2>
<p>To demonstrate how SED can guide LLMs to obtain more accurate responses and thus improve performances, we use the trained llama2-7b-chat-hf through both greedy search and self-evaluation decoding, SED, to present three generated cases in Table 4, 5, and 6.
The texts generated by the model using SED and greedy search have the same prefix, and we label them with underlines. In addition, we use light red and light green to label the tokens selected by greedy search and SED at the first chaotic point respectively. Other colors annotate the key information the model can pay attention to in the question or references. For the XieZhi problem in Table 4, the first chaotic point appears on the option token. Different options could directly lead to the model outputting completely different responses. When the model applies greedy search for decoding, it mistakenly selected option D and subsequently generates incorrect explanations. While SED makes the model carefully reflect on each option based on speculation and evaluation, making the model choose the correct option B since it has the highest propensity score. For the HotpotQA problem in Table 5, both greedy search and SED enable the model to correctly analyze the person described in the problem. At the first chaotic point, the greedy search selects token 'in' and directly takes the person's name as the answer, while SED guides the model to answer the person's distinction, which is exactly the correct answer, by restating the question. For the GSM8K question in Table 6, the greedy search selects the number ' 4 ' at the first chaotic point and mistakenly treats the period " 3 hours" as a distance, leading to an incorrect answer. SED selects the number ' 1 ' to guide the model in calculating the distance traveled by Tom thereby generating the correct answer in the end. These three real cases vividly demonstrate the effectiveness of Self-Evaluation Decoding.</p>
<h2>D Experimental details</h2>
<p>There are quite a few possible combinations of hyper-parameters. It is unfeasible to search for the best among them through extensive experiments. Therefore, we pre-determine some through small-scale pilot experiments on a sampled subset for each dataset. We empirically determine $\delta_{r}$ to be 0.8 and $\delta_{e}$ to be 2.25 . Branching factor $b$ and top $k$ tokens are another two parameters we need to consider. Empirically, we check several model responses given several questions sampled from the datasets and find many detected chaotic points among the responses. We assume that the chaotic points at the beginning of the responses are more important than those at the end since the former can affect</p>
<p>more tokens than the latter. On the other hand, the probability mass for token selection is mainly concentrated on top-5 tokens in many cases. Considering the balance between cost and effectiveness, we set $b=5$ and $k=5$. As for the fusion coefficient $\alpha$, we run a grid search with a subset of the data ( 30 questions per dataset), using llama2-7b-chat-hf. We experiment with $\alpha=[0.1,0.2, \ldots, 1]$ and find that $\alpha=0.8$ is best for XieZhi and GSM8K while $\alpha=0.6$ is best for HotpotQA, indicating the model's evaluation ability varies on different datasets.</p>
<p>Table 4: A case from XieZhi shows that self-evaluation decoding can help the model reflect on the chosen option.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">What is not explicitly mentioned in the "Three People's Principles" is $\qquad$ <br> A: Against the feudal rule of the Qing Dynasty. <br> B: Oppose foreign imperialism. <br> C: Establishment of the Republic of China. <br> D: Equal distribution of land.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Greedy Search</td>
<td style="text-align: center;">The answer should be D , which is Equal distribution of land. While the principle of people's livelihood aimed to ensure economic and social welfare for the people, it did not specifically mention the equal distribution of land. $\boldsymbol{x}$</td>
</tr>
<tr>
<td style="text-align: center;">Self-Evaluation Decoding</td>
<td style="text-align: center;">The answer should be B , which is Oppose foreign imperialism. While the principles primarily focused on nationalism, democracy, and people's livelihood, the explicit opposition to foreign imperialism was not a central theme within these principles. $\boldsymbol{v}^{2}$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">B: Oppose foreign imperialism.</td>
</tr>
</tbody>
</table>
<p>Table 5: A case from HotpotQA demonstrates that self-evaluative decoding can help the model avoid generating irrelevant answers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">What distinction is held by the former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Reference 1</td>
<td style="text-align: center;">1992-93 Charlotte Hornets season <br> The 1992-93 NBA season was the fifth season for the Charlotte Hornets in the National Basketball Association. In the 1992 NBA draft, the Hornets selected center Alonzo Mourning out of Georgetown with the second overall pick. With the addition of Mourning, along with second-year star Larry Johnson and Muggsy Bogues, the Hornets struggled around . 500 for most of the season, but won 9 of their final 12 games finishing their season third in the Central Division with a 44-38 record, and qualified for their first ever playoff appearance. The team became the first of the four expansion franchises of the late 1980s to win a playoff series, when Mourning sunk a 20 ft jumper at the buzzer in Game 4 of the first round playoff series against the 4th-seeded Boston Celtics. However, in the semifinals, the Hornets were eliminated by the New York Knicks in five games.</td>
</tr>
<tr>
<td style="text-align: center;">Reference 2</td>
<td style="text-align: center;">Muggsy Bogues <br> Tyrone Curtis Muggsy Bogues (born January 9, 1965) is an American retired basketball player. The shortest player ever to play in the National Basketball Association, the 5 ft Bogues played point guard for four teams during his 14 -season career in the NBA. He is best known for his ten seasons with the Charlotte Hornets and also played for the Washington Bullets, Golden State Warriors, and Toronto Raptors. After his NBA career, he served as head coach of the now-defunct WNBA team Charlotte Sting.</td>
</tr>
<tr>
<td style="text-align: center;">Greedy Search</td>
<td style="text-align: center;">The former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting is Muggsy Bogues. \n Therefore, the answer is: Muggsy Bogues. $x$</td>
</tr>
<tr>
<td style="text-align: center;">Self-Evaluation Decoding</td>
<td style="text-align: center;">The former NBA player who was a member of the Charlotte Hornets during their 1992-93 season and was head coach for the WNBA team Charlotte Sting is Muggsy Bogues. The question asks for the distinction that he holds. According to Reference 2, Muggsy Bogues is the shortest player ever to play in the National Basketball Association. This is the distinction that he holds. <br> Therefore, the answer is: Muggsy Bogues is the shortest player ever to play in the National Basketball Association. $\boldsymbol{\nu}$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">the shortest player ever to play in the National Basketball Association.</td>
</tr>
</tbody>
</table>
<p>Table 6: A case from GSM8K shows that self-evaluation decoding can help the model check the unsure numbers.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Tom's ship can travel at 10 miles per hour. He is sailing from 1 to 4 PM . <br> He then travels back at a rate of 6 mph . <br> How long does it take him to get back?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Greedy Search</td>
<td style="text-align: center;">Step1: Tom's ship travels at 10 miles per hour from 1 to 4 PM. So, the distance he travels is $4-1=3$ hours. <br> Step2: Then, he travels back at a rate of 6 mph . So, the time it takes him to get back is 3 hours / $6 \mathrm{mph}=0.5$ hours. <br> Step3: Finally, we add the time it takes him to travel forward and back to get the total time it takes him to complete the journey. <br> So, the total time is 3 hours +0.5 hours $=3.5$ hours. <br> So, the final answer is 3.5 . $x$</td>
</tr>
<tr>
<td style="text-align: center;">Self-Evaluation <br> Decoding</td>
<td style="text-align: center;">Step1: Tom's ship travels at 10 miles per hour from 1 to 4 PM. So, the distance traveled is 10 miles/hour * 3 hours $=30$ miles. <br> Step2: Then, Tom travels back at a rate of 6 mph . So, the time taken to travel back is 30 miles / $6 \mathrm{mph}=5$ hours. <br> So, the final answer is 5. $\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">Answer</td>
<td style="text-align: center;">He was traveling at full speed for $4-1==4-1=3=3$ hours So he went $3 * 10==3 * 10=30=30$ miles That means it takes $30 / 6==30 / 6=5=5$ hours to get back #### 5</td>
</tr>
</tbody>
</table>
<p>Table 7: Few-shot prompt used for XieZhi
/<em> Prompt Template </em>/
Select the answer to the target question from the options based on the knowledge you have learned.
There are some examples that you can refer to:
$\overline{\text { [demonstrations] }}$</p>
<p>You should answer the following question in a similar way to the examples above:
Question: {question_and_options }
Answer:
/<em> Example for demonstrations</em>/
demonstrations $=$ ' ' '
Question: China was the first country in the world to grow rice, as you can see from $\qquad$ .
Options: A: Lantian Man ruins, B: Site of Dawenkou, C: Banpo site, D: Hemudu Ruins
Answer: The answer should be [[D]], which is Hemudu Ruins. Archaeological excavations at this site have uncovered tools, pottery, and rice grains, indicating that rice was indeed cultivated there.
The relocation of a capital is a major event in a country, and the one who moved the capital to accept the advanced culture is $\qquad$
Options: A: Pan Geng moved Yin, B: Emperor Xiaowen of Northern Wei moved his capital to Luoyang, C: King Ping moved the capital to Luoyi, D: The capital of Soviet Russia was moved to Moscow
Answer: The answer should be option [[B]], which is Emperor Xiaowen of Northern Wei moved his capital to Luoyang. Luoyang was a center of culture and civilization during that time, and by moving the capital there, Emperor Xiaowen aimed to strengthen the Northern Wei dynasty by embracing the cultural, economic, and political advancements of the region.
(more few-shot demonstrations) ...</p>
<p>Table 8: Zero-shot prompt used for HotpotQA
$I^{<em>}$ Prompt Template </em>1
You are professional at reading comprehension.
The following is one or several reference documents for you to comprehend and memory:
$\overline{\text { {reference_docs} }}$</p>
<p>The answer to the following question can be inferred through the above reference(s).
Question: {question}
Provide the answer to the given question through step-by-step reasoning. After your reasoning, output your final answer in the format of [[YOUR ANSWER]).
Make sure the final answer is enclosed by the double square brackets.
Let's think step by step.</p>
<p>Table 9: Few-shot prompt used for GSM8K
$l^{*}$ Prompt Template * /
Please help me solve the math problem step by step. You must give your final answer at the end of response like 'So, the final answer is $\$$ your answer'. I will provide examples for you:
$\overline{\text { [demonstrations] }}$</p>
<p>Next, please answer this math problem:
Question: {question}
Answer:
/<em> Example for demonstrations</em>/
demonstrations $=$ ' ' '
Question: Ken created a care package to send to his brother, who was away at boarding school. Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds. Then, he added enough brownies to cause the weight to triple. Next, he added another 2 pounds of jelly beans. And finally, he added enough gummy worms to double the weight once again. What was the final weight of the box of goodies, in pounds?
Answer: Step1: To the initial 2 pounds of jelly beans, he added enough brownies to cause the weight to triple, bringing the weight to $2 * 3=6$ pounds.
Step2: Next, he added another 2 pounds of jelly beans, bringing the weight to $6+2=8$ pounds.
Step3: And finally, he added enough gummy worms to double the weight once again, to a final weight of $8 * 2=16$ pounds.
So, the final answer is $\$ 16$.
(more few-shot demonstrations) ...</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Corresponding Author&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6082 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6082</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6082</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-316bd8e85d5bb0ec851f777bb68f66c500bf9210</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/316bd8e85d5bb0ec851f777bb68f66c500bf9210" target="_blank">Metric Ensembles For Hallucination Detection</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is found that LLM-based methods outperform other unsupervised metrics for hallucination detection and ensemble methods can improve these scores even further, provided that the metrics in the ensemble have sufficiently similar and uncorrelated error rates.</p>
                <p><strong>Paper Abstract:</strong> Abstractive text summarization has garnered increased interest as of late, in part due to the proliferation of large language models (LLMs). One of the most pressing problems related to generation of abstractive summaries is the need to reduce"hallucinations,"information that was not included in the document being summarized, and which may be wholly incorrect. Due to this need, a wide array of metrics estimating consistency with the text being summarized have been proposed. We examine in particular a suite of unsupervised metrics for summary consistency, and measure their correlations with each other and with human evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then compare these evaluations to models made from a simple linear ensemble of these metrics. We find that LLM-based methods outperform other unsupervised metrics for hallucination detection. We also find that ensemble methods can improve these scores even further, provided that the metrics in the ensemble have sufficiently similar and uncorrelated error rates. Finally, we present an ensemble method for LLM-based evaluations that we show improves over this previous SOTA.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6082.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6082.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM Self-Evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM self-evaluation via direct prompting (ChatGPT/GPT evaluator prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that queries an LLM with a structured prompt asking it to score a candidate summary for consistency with a source article (here using a 1–10 scale). The paper reproduces the prompting technique from Luo et al. (LXA23) and uses it as an automatic hallucination detector.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt as a factual inconsistency evaluator for abstractive text summarization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM Self-Evaluation (direct scoring prompt)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Direct prompting of a large language model with a template that supplies the Summary and Source Article and asks the model to score consistency from 1 to 10; used as an unsupervised hallucination detection metric.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-3.5-turbo and GPT-4 (as reported and benchmarked in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Pairs of (generated) summary and source article from the wiki_bio_gpt3_hallucination dataset (subset used: 238 entries); each pair is fed into the prompt template with the summary and source article filled in.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Not a literature-distillation system per se — it performs per-summary factuality scoring by asking an LLM to judge entailment/consistency; conceptually this is an LLM-based evaluative distillation of the evidence in the source into a single numeric judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Scalar numeric score (1–10) per summary indicating degree of consistency with the source document.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Correlation (Pearson) of LLM-generated scores with human evaluation scores (human sentence-by-sentence consistency annotations) on the wiki_bio_gpt3_hallucination dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>LLM self-evaluation methods strongly correlated with human judgment and outperformed other non-LLM unsupervised metrics on this dataset. Example correlations reported for GPT-4 at various temperatures: 0.0 temp = 0.892, 0.8 temp = 0.895 (highest single temperature reported), indicating top-tier agreement with human labels.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>wiki_bio_gpt3_hallucination dataset (subset of 238 examples from Wikibio with GPT-3 generated summaries and sentence-level human annotations).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Cost and latency of calling large LLMs for each (summary, source) pair; sensitivity to prompt design and temperature; potential calibration issues across model versions; method evaluates individual summaries but does not itself synthesize across many papers; ensemble weighting/mixing with lower-quality signals can reduce performance if less-accurate metrics are included.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Outperformed traditional unsupervised hallucination metrics evaluated in this work (FactSumm, QAGS, ROUGE-as-unsupervised, SMART, SummaC, SelfCheckGPT unigram variant). The paper reproduces the Luo et al. (LXA23) prompting approach and finds LLM self-evaluation to be the top-performing individual metric on this dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metric Ensembles For Hallucination Detection', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6082.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6082.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 Temperature Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linear ensemble of GPT-4 evaluations across multiple temperature settings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ensemble approach that queries the same LLM (GPT-4) at multiple temperature settings, collects the numeric consistency scores, and combines them via a simple constant-weighted average to produce a single score per summary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 Temperature Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Naive linear ensemble (uniform weights) of multiple GPT-4 runs differing only by sampling temperature; intended to aggregate independent perturbations of the same evaluative method so uncorrelated error cancels out.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4 (multiple temperature settings: 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2)</td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Same (summary, source article) pairs from the wiki_bio_gpt3_hallucination dataset (238 examples); each pair was evaluated multiple times (one per temperature setting) with the same prompt template, producing multiple scalar scores per example which were then averaged.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Aggregation/ensemble of multiple LLM evaluations to reduce variance and cancel uncorrelated errors; effectively synthesizes a more robust per-summary evaluation signal by averaging diverse LLM judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Aggregated scalar numeric score per summary (ensemble average of per-temperature numeric evaluations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Pearson correlation with human evaluation (human consistency scores); pairwise correlations between temperatures and with the ensemble were also computed to measure diversity and ensemble benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>The temperature ensemble achieved higher Pearson correlation with human judgments (ensemble correlation = 0.901) than any single-temperature GPT-4 run (best single reported = 0.895 at temperature 0.8), and outperformed all other metrics in this study when focused solely on LLM-based permutations.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>wiki_bio_gpt3_hallucination dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires multiple LLM calls per example (increased cost and time); benefit depends on diversity of perturbations—if component runs are highly correlated or if component methods vary in accuracy (Condition 2 from paper), naive averaging may hurt performance; approach directly applies only when outputs are numeric and straightforward to aggregate.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Outperformed each individual GPT-4 temperature run and all non-LLM baselines tested in this paper for the given dataset; paper notes that ensembles mixing strong LLM-based signals with weaker non-LLM metrics can reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metric Ensembles For Hallucination Detection', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6082.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6082.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SelfCheckGPT (unigram variant)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models (unigram-based variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A zero-resource method that generates multiple outputs from a model for the same input and measures inter-response similarity (here the unigram-similarity variant) as a proxy for factuality: more consistent outputs are hypothesized to be more factual.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SelfCheckGPT (unigram similarity metric)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Generate multiple summaries from a model and compute distance/similarity statistics (the paper benchmarks the unigram-based similarity variant) among generated summaries as an unsupervised indicator of factual consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td>Applied to the wiki_bio_gpt3_hallucination dataset (238 examples) by generating multiple summaries per source and computing the unigram similarity metric across them (paper reports benchmarking the unigram approach from MLG23).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Evidence synthesis via intra-model response-consistency analysis: the method synthesizes a consistency signal by comparing multiple generated outputs for the same input.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Similarity/distance score (scalar) representing how similar multiple generated summaries are to each other (higher similarity hypothesized to indicate higher factuality).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Pearson correlation of the unigram-based SelfCheckGPT score with human evaluation of summary consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td>The unigram variant was included among benchmarked metrics; previous work (MLG23) reported strong correlation with human judgments, and the paper includes the unigram approach in its comparisons — it did not surpass direct LLM self-evaluation but was a competitive unsupervised metric.</td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td>wiki_bio_gpt3_hallucination dataset (this paper); the SelfCheckGPT original work evaluated on other generation datasets (not enumerated in detail here).</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Requires multiple generations per example (computational cost/time); the underlying assumption (factual outputs are less diverse) may not hold in all contexts; performance depends on the base model's sampling behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Benchmarked against direct LLM self-evaluation and other unsupervised metrics (FactSumm, QAGS, ROUGE, SMART, SummaC); performed well in prior work but in this study LLM direct scoring (GPT-4 prompt) achieved higher correlation with human annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metric Ensembles For Hallucination Detection', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6082.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6082.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multiagent Debate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Improving factuality and reasoning in language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative prompting approach where multiple agent instances (LLMs or LLM agents) 'debate' a question before arriving at a final answer, intended to improve factuality and reasoning via adversarial/interactive refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Improving factuality and reasoning in language models through multiagent debate</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multiagent debate prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative prompting framework that coordinates multiple LLM agents to discuss/debate a query and produce a consolidated answer intended to reduce hallucinations and improve reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Interactive multi-agent debate to refine answers; can be viewed as an ensemble/consensus mechanism rather than direct distillation from many documents.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined textual answer produced after agent interaction/debate.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Not evaluated in this paper; mentioned as an example of ensemble-esque LLM techniques that leverage multiple LLM outputs. Potential challenges (not enumerated here) include orchestration complexity, cost, and ensuring the debate process yields truthful consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Mentioned as related work in the space of LLM ensembling and iterative prompting; characterized as ensemble-esque and complementary to the methods studied here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metric Ensembles For Hallucination Detection', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6082.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6082.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistency (sampling multiple chains-of-thought and aggregating)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that generates multiple reasoning traces (chain-of-thought) and aggregates answers (e.g., by majority vote) to improve reasoning robustness; cited as improving chain-of-thought performance when combined with self-consistent sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-consistency improves chain of thought reasoning in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-consistency ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sample multiple reasoning chains (or outputs) from an LLM and aggregate (e.g., majority) to obtain a more robust final answer; increases diversity to reduce single-run sampling errors.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Ensemble of multiple sampled outputs (chain-of-thought) to reach a more reliable answer; related to the paper's temperature-ensemble idea but aimed at reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Aggregated answer (often categorical or final textual answer derived from multiple sampled outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned as related ensemble-style technique; specifics not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Cited alongside other ensemble or sampling-based approaches illustrating the general utility of aggregating multiple LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metric Ensembles For Hallucination Detection', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6082.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6082.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Summit</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Summit: Iterative text summarization via ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative summarization approach using ChatGPT (or similar LLM) to progressively refine summaries; mentioned as contemporary work that can be viewed as an ensemble-like extension to more complex domains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Summit: Iterative text summarization via chatgpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative ChatGPT summarization (Summit)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Iterative prompting pipeline that uses ChatGPT to refine or compress summaries across steps, intended to improve summary quality.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>input_type_and_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_approach</strong></td>
                            <td>Iterative refinement of summaries via repeated prompting (can be seen as multi-step synthesis rather than single-shot distillation).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Refined textual summary.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>datasets_or_benchmarks</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_or_limitations</strong></td>
                            <td>Mentioned as evidence that ensemble-like and iterative LLM methods are promising for more complex domains; specifics not examined in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons_to_other_methods</strong></td>
                            <td>Positioned as related work suggesting ensemble/iterative LLM approaches extend beyond numeric-evaluation tasks to full-text synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Metric Ensembles For Hallucination Detection', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chatgpt as a factual inconsistency evaluator for abstractive text summarization <em>(Rating: 2)</em></li>
                <li>Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Summit: Iterative text summarization via chatgpt <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6082",
    "paper_id": "paper-316bd8e85d5bb0ec851f777bb68f66c500bf9210",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [
        {
            "name_short": "LLM Self-Evaluation",
            "name_full": "LLM self-evaluation via direct prompting (ChatGPT/GPT evaluator prompt)",
            "brief_description": "A method that queries an LLM with a structured prompt asking it to score a candidate summary for consistency with a source article (here using a 1–10 scale). The paper reproduces the prompting technique from Luo et al. (LXA23) and uses it as an automatic hallucination detector.",
            "citation_title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "mention_or_use": "use",
            "system_name": "LLM Self-Evaluation (direct scoring prompt)",
            "system_description": "Direct prompting of a large language model with a template that supplies the Summary and Source Article and asks the model to score consistency from 1 to 10; used as an unsupervised hallucination detection metric.",
            "llm_model_used": "GPT-3.5-turbo and GPT-4 (as reported and benchmarked in this paper)",
            "input_type_and_size": "Pairs of (generated) summary and source article from the wiki_bio_gpt3_hallucination dataset (subset used: 238 entries); each pair is fed into the prompt template with the summary and source article filled in.",
            "distillation_approach": "Not a literature-distillation system per se — it performs per-summary factuality scoring by asking an LLM to judge entailment/consistency; conceptually this is an LLM-based evaluative distillation of the evidence in the source into a single numeric judgment.",
            "output_type": "Scalar numeric score (1–10) per summary indicating degree of consistency with the source document.",
            "evaluation_methods": "Correlation (Pearson) of LLM-generated scores with human evaluation scores (human sentence-by-sentence consistency annotations) on the wiki_bio_gpt3_hallucination dataset.",
            "results": "LLM self-evaluation methods strongly correlated with human judgment and outperformed other non-LLM unsupervised metrics on this dataset. Example correlations reported for GPT-4 at various temperatures: 0.0 temp = 0.892, 0.8 temp = 0.895 (highest single temperature reported), indicating top-tier agreement with human labels.",
            "datasets_or_benchmarks": "wiki_bio_gpt3_hallucination dataset (subset of 238 examples from Wikibio with GPT-3 generated summaries and sentence-level human annotations).",
            "challenges_or_limitations": "Cost and latency of calling large LLMs for each (summary, source) pair; sensitivity to prompt design and temperature; potential calibration issues across model versions; method evaluates individual summaries but does not itself synthesize across many papers; ensemble weighting/mixing with lower-quality signals can reduce performance if less-accurate metrics are included.",
            "comparisons_to_other_methods": "Outperformed traditional unsupervised hallucination metrics evaluated in this work (FactSumm, QAGS, ROUGE-as-unsupervised, SMART, SummaC, SelfCheckGPT unigram variant). The paper reproduces the Luo et al. (LXA23) prompting approach and finds LLM self-evaluation to be the top-performing individual metric on this dataset.",
            "uuid": "e6082.0",
            "source_info": {
                "paper_title": "Metric Ensembles For Hallucination Detection",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 Temperature Ensemble",
            "name_full": "Linear ensemble of GPT-4 evaluations across multiple temperature settings",
            "brief_description": "An ensemble approach that queries the same LLM (GPT-4) at multiple temperature settings, collects the numeric consistency scores, and combines them via a simple constant-weighted average to produce a single score per summary.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GPT-4 Temperature Ensemble",
            "system_description": "Naive linear ensemble (uniform weights) of multiple GPT-4 runs differing only by sampling temperature; intended to aggregate independent perturbations of the same evaluative method so uncorrelated error cancels out.",
            "llm_model_used": "GPT-4 (multiple temperature settings: 0.0, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2)",
            "input_type_and_size": "Same (summary, source article) pairs from the wiki_bio_gpt3_hallucination dataset (238 examples); each pair was evaluated multiple times (one per temperature setting) with the same prompt template, producing multiple scalar scores per example which were then averaged.",
            "distillation_approach": "Aggregation/ensemble of multiple LLM evaluations to reduce variance and cancel uncorrelated errors; effectively synthesizes a more robust per-summary evaluation signal by averaging diverse LLM judgments.",
            "output_type": "Aggregated scalar numeric score per summary (ensemble average of per-temperature numeric evaluations).",
            "evaluation_methods": "Pearson correlation with human evaluation (human consistency scores); pairwise correlations between temperatures and with the ensemble were also computed to measure diversity and ensemble benefit.",
            "results": "The temperature ensemble achieved higher Pearson correlation with human judgments (ensemble correlation = 0.901) than any single-temperature GPT-4 run (best single reported = 0.895 at temperature 0.8), and outperformed all other metrics in this study when focused solely on LLM-based permutations.",
            "datasets_or_benchmarks": "wiki_bio_gpt3_hallucination dataset.",
            "challenges_or_limitations": "Requires multiple LLM calls per example (increased cost and time); benefit depends on diversity of perturbations—if component runs are highly correlated or if component methods vary in accuracy (Condition 2 from paper), naive averaging may hurt performance; approach directly applies only when outputs are numeric and straightforward to aggregate.",
            "comparisons_to_other_methods": "Outperformed each individual GPT-4 temperature run and all non-LLM baselines tested in this paper for the given dataset; paper notes that ensembles mixing strong LLM-based signals with weaker non-LLM metrics can reduce performance.",
            "uuid": "e6082.1",
            "source_info": {
                "paper_title": "Metric Ensembles For Hallucination Detection",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "SelfCheckGPT (unigram variant)",
            "name_full": "SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models (unigram-based variant)",
            "brief_description": "A zero-resource method that generates multiple outputs from a model for the same input and measures inter-response similarity (here the unigram-similarity variant) as a proxy for factuality: more consistent outputs are hypothesized to be more factual.",
            "citation_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "mention_or_use": "use",
            "system_name": "SelfCheckGPT (unigram similarity metric)",
            "system_description": "Generate multiple summaries from a model and compute distance/similarity statistics (the paper benchmarks the unigram-based similarity variant) among generated summaries as an unsupervised indicator of factual consistency.",
            "llm_model_used": null,
            "input_type_and_size": "Applied to the wiki_bio_gpt3_hallucination dataset (238 examples) by generating multiple summaries per source and computing the unigram similarity metric across them (paper reports benchmarking the unigram approach from MLG23).",
            "distillation_approach": "Evidence synthesis via intra-model response-consistency analysis: the method synthesizes a consistency signal by comparing multiple generated outputs for the same input.",
            "output_type": "Similarity/distance score (scalar) representing how similar multiple generated summaries are to each other (higher similarity hypothesized to indicate higher factuality).",
            "evaluation_methods": "Pearson correlation of the unigram-based SelfCheckGPT score with human evaluation of summary consistency.",
            "results": "The unigram variant was included among benchmarked metrics; previous work (MLG23) reported strong correlation with human judgments, and the paper includes the unigram approach in its comparisons — it did not surpass direct LLM self-evaluation but was a competitive unsupervised metric.",
            "datasets_or_benchmarks": "wiki_bio_gpt3_hallucination dataset (this paper); the SelfCheckGPT original work evaluated on other generation datasets (not enumerated in detail here).",
            "challenges_or_limitations": "Requires multiple generations per example (computational cost/time); the underlying assumption (factual outputs are less diverse) may not hold in all contexts; performance depends on the base model's sampling behavior.",
            "comparisons_to_other_methods": "Benchmarked against direct LLM self-evaluation and other unsupervised metrics (FactSumm, QAGS, ROUGE, SMART, SummaC); performed well in prior work but in this study LLM direct scoring (GPT-4 prompt) achieved higher correlation with human annotations.",
            "uuid": "e6082.2",
            "source_info": {
                "paper_title": "Metric Ensembles For Hallucination Detection",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Multiagent Debate",
            "name_full": "Improving factuality and reasoning in language models through multiagent debate",
            "brief_description": "An iterative prompting approach where multiple agent instances (LLMs or LLM agents) 'debate' a question before arriving at a final answer, intended to improve factuality and reasoning via adversarial/interactive refinement.",
            "citation_title": "Improving factuality and reasoning in language models through multiagent debate",
            "mention_or_use": "mention",
            "system_name": "Multiagent debate prompting",
            "system_description": "Iterative prompting framework that coordinates multiple LLM agents to discuss/debate a query and produce a consolidated answer intended to reduce hallucinations and improve reasoning.",
            "llm_model_used": null,
            "input_type_and_size": null,
            "distillation_approach": "Interactive multi-agent debate to refine answers; can be viewed as an ensemble/consensus mechanism rather than direct distillation from many documents.",
            "output_type": "Refined textual answer produced after agent interaction/debate.",
            "evaluation_methods": null,
            "results": null,
            "datasets_or_benchmarks": null,
            "challenges_or_limitations": "Not evaluated in this paper; mentioned as an example of ensemble-esque LLM techniques that leverage multiple LLM outputs. Potential challenges (not enumerated here) include orchestration complexity, cost, and ensuring the debate process yields truthful consensus.",
            "comparisons_to_other_methods": "Mentioned as related work in the space of LLM ensembling and iterative prompting; characterized as ensemble-esque and complementary to the methods studied here.",
            "uuid": "e6082.3",
            "source_info": {
                "paper_title": "Metric Ensembles For Hallucination Detection",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Self-Consistency",
            "name_full": "Self-consistency (sampling multiple chains-of-thought and aggregating)",
            "brief_description": "A method that generates multiple reasoning traces (chain-of-thought) and aggregates answers (e.g., by majority vote) to improve reasoning robustness; cited as improving chain-of-thought performance when combined with self-consistent sampling.",
            "citation_title": "Self-consistency improves chain of thought reasoning in language models",
            "mention_or_use": "mention",
            "system_name": "Self-consistency ensemble",
            "system_description": "Sample multiple reasoning chains (or outputs) from an LLM and aggregate (e.g., majority) to obtain a more robust final answer; increases diversity to reduce single-run sampling errors.",
            "llm_model_used": null,
            "input_type_and_size": null,
            "distillation_approach": "Ensemble of multiple sampled outputs (chain-of-thought) to reach a more reliable answer; related to the paper's temperature-ensemble idea but aimed at reasoning tasks.",
            "output_type": "Aggregated answer (often categorical or final textual answer derived from multiple sampled outputs).",
            "evaluation_methods": null,
            "results": null,
            "datasets_or_benchmarks": null,
            "challenges_or_limitations": "Mentioned as related ensemble-style technique; specifics not provided in this paper.",
            "comparisons_to_other_methods": "Cited alongside other ensemble or sampling-based approaches illustrating the general utility of aggregating multiple LLM outputs.",
            "uuid": "e6082.4",
            "source_info": {
                "paper_title": "Metric Ensembles For Hallucination Detection",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Summit",
            "name_full": "Summit: Iterative text summarization via ChatGPT",
            "brief_description": "An iterative summarization approach using ChatGPT (or similar LLM) to progressively refine summaries; mentioned as contemporary work that can be viewed as an ensemble-like extension to more complex domains.",
            "citation_title": "Summit: Iterative text summarization via chatgpt",
            "mention_or_use": "mention",
            "system_name": "Iterative ChatGPT summarization (Summit)",
            "system_description": "Iterative prompting pipeline that uses ChatGPT to refine or compress summaries across steps, intended to improve summary quality.",
            "llm_model_used": null,
            "input_type_and_size": null,
            "distillation_approach": "Iterative refinement of summaries via repeated prompting (can be seen as multi-step synthesis rather than single-shot distillation).",
            "output_type": "Refined textual summary.",
            "evaluation_methods": null,
            "results": null,
            "datasets_or_benchmarks": null,
            "challenges_or_limitations": "Mentioned as evidence that ensemble-like and iterative LLM methods are promising for more complex domains; specifics not examined in this paper.",
            "comparisons_to_other_methods": "Positioned as related work suggesting ensemble/iterative LLM approaches extend beyond numeric-evaluation tasks to full-text synthesis.",
            "uuid": "e6082.5",
            "source_info": {
                "paper_title": "Metric Ensembles For Hallucination Detection",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chatgpt as a factual inconsistency evaluator for abstractive text summarization",
            "rating": 2
        },
        {
            "paper_title": "Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models",
            "rating": 2
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Summit: Iterative text summarization via chatgpt",
            "rating": 2
        }
    ],
    "cost": 0.016799500000000002,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Metric Ensembles for Hallucination Detection</h1>
<p>A Preprint</p>
<p>Grant C. Forbes<br>North Carolina State University<br>Raleigh, NC<br>gforbes@ncsu.edu</p>
<h2>Parth Katlana</h2>
<p>North Carolina State University
Raleigh, NC
pkatlan@ncsu.edu</p>
<p>Zeydy Ortiz, Ph. D.<br>DataCrunch Lab, L. L. C.<br>Cary, NC<br>zortiz@datacrunchlab.com</p>
<h4>Abstract</h4>
<p>Abstract</p>
<p>Abstractive text summarization has garnered increased interest as of late, in part due to the proliferation of large language models (LLMs). One of the most pressing problems related to generation of abstractive summaries is the need to reduce "hallucinations," information that was not included in the document being summarized, and which may be wholly incorrect. Due to this need, a wide array of metrics estimating consistency with the text being summarized have been proposed. We examine in particular a suite of unsupervised metrics for summary consistency, and measure their correlations with each other and with human evaluation scores in the wiki_bio_gpt3_hallucination dataset. We then compare these evaluations to models made from a simple linear ensemble of these metrics. We find that LLM-based methods outperform other unsupervised metrics for hallucination detection. We also find that ensemble methods can improve these scores even further, provided that the metrics in the ensemble have sufficiently similar and uncorrelated error rates. Finally, we present an ensemble method for LLM-based evaluations that we show improves over this previous SOTA.</p>
<p>Keywords Large Language Models; Text summarization; Hallucination Detection; Ensemble methods</p>
<h2>2 Introduction</h2>
<p>Text summarization is a rapidly changing and advancing field, due in no small part to the advent of Large Language Models (LLMs) such as GPT [BCE ${ }^{+} 23$ ] and LaMDA [TDFH ${ }^{+} 22$ ]. Many summarization methods, however, struggle with "hallucinating:" inserting false, misleading and/or nonrepresentative material into the summaries. As such, many automatic methods for hallucination detection have been proposed in the literature for both evaluation and iterative improvement of text summarization methods. This diversity of methods, while indicative of rapid progress, has also led to a situation where there is no one clear standard evaluative metric for hallucinations in text summarization. With this in mind, we test a suite of hallucination detection from prior literature on the wiki_bio_gpt3_hallucination dataset [LGA16, MLG23], and examine their correlations with both each other and with a human evaluation baseline. We also, drawing on prior work in ensemble methods, test these against a linear ensemble of the sampled methods, and found that this ensemble outperforms most individual evaluation metrics. We found evaluation methods based on directly querying LLMs themselves to be most closely correlated with human evaluation, outperforming all non-LLM metrics and the ensemble. With this in mind, we constructed a new ensemble of LLM evaluations with a range of temperatures, with the expectation that perturbations to the metric that didn't correlate with the "true" value of what was being measured would cancel out in aggregate (we elaborate on this expectation in Section 3.3). We found that our LLM ensemble outperformed even the best LLM-based single evaluation, indicating our method to be the most accurate and effective hallucination detection metric to date for our chosen dataset.</p>
<h1>3 Metrics Evaluated and Related Work</h1>
<p>Here, we discuss prior work, and describe the metrics we've chosen to represent the suite of prior methods that exist. We also discuss ensemble methods, the theoretical justifications for their use in this domain, and the conditions generally required for them to be effective.</p>
<h3>3.1 Text Summarization</h3>
<p>Traditionally, text summarization has been categorized as either extractive or abstractive. Extractive text summarizers, such as OCCAMS [WMYC23], produce summaries by concatenating particularly salient sentences ("extracts") from the document being summarized. On the other hand, abstractive summarizers, such as most methods using LLMs [ZLZ23], attempt to generate a summary "from scratch," assembling new sentences in an attempt to synthesize the information in a document in a more human-seeming way. Abstractive summaries are often able to be more natural-sounding than extractive summaries, but, as has been noted repeatedly in the literature, have a tendency to hallucinate [JLF+23]. It is often challenging to evaluate these models, as has been noted in [GLD22].</p>
<h3>3.2 Hallucination Detection Metrics</h3>
<p>There are many methods for hallucination detection in prior work: too many to feasibly include them all within this work. We limited the scope of methods that we tested in two key ways: by constraining the broader scope of concern to unsupervised metrics, and by choosing a selection of well-regarded methods meant to cover the breadth of scope within that subfield. When we say we are specifically concerned with unsupervised hallucination detection methods, we mean those which require no input other than the summary and the source text itself. We chose to focus on these metrics as they are the most general, requiring no gold-standard human summaries or other supplementary information, and are thus the most widely deploy-able. More particularly, unsupervised hallucination detection metrics are deploy-able in two important context which exclude any other types of metrics:</p>
<ol>
<li>As an evaluative tool on summarization data (possibly generated continuously, rather than part of a finite set)</li>
<li>As an in-the-loop tool for actively curbing hallucinations in a summarization tool at runtime.</li>
</ol>
<p>The survey [HFFQ21] identifies four general types of unsupervised summarization metrics: "triple-based," "textual-entailment-based," "QA-based," and "Other." We evaluate representatives from each of these categories (a more detailed analysis of these categories and the intricacies therein can be found in the aforementioned survey). We chose these to both cover the identified breadth of evaluation methods in the literature (i.e., pulling representatives from each of these categories), as well as to find methods with good theoretical/empirical backing and wide use, while still being recently developed and relevant to the SOTA.</p>
<h3>3.2.1 FactSumm</h3>
<p>FactSumm [Heo21] is a triple-based metric to estimate the factual accuracy of generated text. It builds on prior works in graph-based hallucination detection [KMXS19, GRLS19], using pre-trained models to extract fact triples (subject, relation, object) from both the source document and the summary, and returns a count of the number of triples extracted from the summary that are included in the extract from the document itself. This serves as a heuristic for the percentage of "facts" in the summary that are contained within the source document.</p>
<h3>3.2.2 QAGS</h3>
<p>QAGS [WCL20] is a question-answering based metric that automatically generates questions and answers from the source document and summary, then scores the summary based on how many of the derived questions are answered correctly. We used the implementation of QAGS in [Heo21]. For comparison, other notable representatives from this category are QAEval [DBWR21] and FEQA [DHD20].</p>
<h3>3.2.3 ROUGE</h3>
<p>ROUGE [Lin04] is traditionally used as a supervised metric, by calculating the ROUGE score between a generated summary and a gold-standard human summary. However, [LN09] introduced the idea of using ROUGE as an unsupervised metric, and demonstrated its efficacy in such a capacity. Using ROUGE without supervision ("supervision," in this case, referring to gold standard human summaries that can be compared against) involves taking the ROUGE score between the generated summary and the text itself being summarized: treating the text itself, in other words, as its</p>
<p>own "gold standard." The intuition behind this as a heuristic is that hallucinatory passages, on average, are likely to have less similarity (measured by ROUGE) to the source text than those which accurately summarize the source text.</p>
<h1>3.2.4 SMART</h1>
<p>SMART (Sentence Matching for Rating Text) [ALZN22] evaluation works on two principal ideas. Firstly, it treats sentences, rather than tokens, as the basic units of matching between system and reference summaries. Because, then, exactly matching sentences are most likely nonexistent (in abstractive summaries, though they are trivially present in extractive summaries), SMART utilizes soft-matching functions to compare sentences which can vary with respect to the type of SMART that is being used. SMART types utilized for the purposes of our study are:</p>
<ul>
<li>SMART1: Unigram-based scoring.</li>
<li>SMART2: Bigram-based scoring.</li>
<li>SMARTL: Longest common subsequence-based scoring.</li>
</ul>
<p>It is also significant to mention that the unit of n-grams used here are chunks of tokens (sentences by default). This is different from the token-level n-grams used in standard ROUGE.
Secondly, SMART allows to compare the candidate system summary with the source document. This is particularly important when evaluating dimensions of summary quality that rely on the source document such as factuality.</p>
<h3>3.2.5 SummaC</h3>
<p>SummaC [LSBH22], similarly to SMART, runs evaluations on a sentence-by-sentence basis, but unlike SMART, is explicitly based on Natural Language Entailment (NLI) evaluations between sentences in the source document and the summary. SummaC first generates a matrix for every sentence pair between the summary and source document. Then the two models we benchmark analyze this matrix to achieve a benchmark.
$\mathrm{SUMMAC}<em _conv="{conv" _text="\text">{s s}$ (Zero-Shot) reduces the pair matrix to a one-dimensional vector by taking the maximum value of each column, then computes the mean. This step retains the score for the document sentence that provides the strongest support for each summary sentence. It leverages the intuition that each sentence in the summary document, if non-hallucinatory, should have at least one sentence in the source document which has a high entailment score.
SUMMAC $</em>$ (Convolutional) reduces reliance on extreme values and takes the entire distribution of entailment scores for each summary sentence into account. It utilizes a learned convolutional network on the NLI matrix to compute a final score for the respective summary sentence.}</p>
<h3>3.2.6 SelfCheckGPT</h3>
<p>SelfCheckGPT [MLG23] is an unsupervised hallucination detection method that relies on the intuition that factual generated summaries are much more likely to be similar to each other than to those which contain hallucinations, whereas hallucination-containing summaries are not more likely to be similar to each other than to factual summaries. Another way of framing this intuition is that language models which are confident in their knowledge are likely to have much less diverse responses than those which are making things up. It involves generating multiple summaries for a given source document, then utilizing a variety of distance metrics to check generated summaries against each other for similarity, and shows that higher similarity to other generated summaries is highly correlated with human annotations for textual consistency. Note that we specifically benchmarked their unigram-based approach, as it was the single approach that had the highest correlations with human judgements in [MLG23].</p>
<h3>3.2.7 LLM Self-Evaluation</h3>
<p>Recent work, such as [LXA23], has explored the possibility of using LLMs themselves as evaluative tools for text data generally, and for abstractive summaries in particular. These recent results are very promising, and may usurp traditional, non-LLM-based evaluative methods in this domain, such as those we have listed thus far. For benchmarking these methods, we reproduced the prompting technique described in [LXA23]. For ease of reference, the prompt is in Figure 1 below.
We used this prompt in both GPT 3.5 turbo and GPT 4 models as benchmarks for this method. Our results lend evidence to the claim that these methods indeed surpass more traditional hallucination detection methods, and we use these results to further refine these methods into an ensemble approach that outperforms prior work.</p>
<div class="codehilite"><pre><span></span><code>Score the following summary given the corresponding article with respect to consistency
from 1 to 10. Note that consistency measures how much information included in the summary
is present in the source article. 10 points indicate the summary contains only statements
that are entailed by the source document.
Summary: {summary}
Source Article: {source}
Marks:
</code></pre></div>

<p>Figure 1: LLM evaluation prompt</p>
<h1>3.3 Metric Ensembles</h1>
<p>It's been long noted that ensembles of models or metrics, even subpar ones combined naively, are surprisingly efficient and can rival or outperform expert judgment [Daw79]. Ensembles also have been noted to aid in explainability in some contexts, something particularly relevant for analysis work, which is very sensitive to reliability and has a high threshold of required trust [FC23]. Ensemble methods are thus a promising avenue for improving over a baseline, particularly in a domain such as hallucination detection, where there are a wide variety of disparate metrics, none of which is clearly superior to others in measuring the "true" value.
Ensemble methods operate, fundamentally, by leveraging the ability of the individual metrics' error from the "true" value to cancel each other out in aggregate. As derived in [PC95], if there is a collection of value-estimating functions $f_{i}$, each of which differs from some true function $f$ by some $m_{i}=f-f_{i}$, and we assume the errors are uncorrelated, then in expectation we should expect the error $m_{\text {sum }}$ of $f_{\text {sum }}$ to be</p>
<p>$$
m_{\text {sum }}=f-\frac{1}{N} \sum_{i=0}^{N-1} f_{i}=\frac{\bar{m}_{i}}{N}
$$</p>
<p>where $\bar{m}<em i="i">{i}$ is the mean value of $M</em>$ that we would want, in order for an ensemble method to be effective:
Condition 1. The metrics must be diverse: that is, their errors should be relatively uncorrelated with each other (this assumption is key for the referenced derivation in [PC95]).
Condition 2. The metrics must have $m_{i}$ 's that are similar in magnitude. If this condition is not met, then it is possible that the $f_{i}$ with the lowest error alone would outperform an ensemble model. In other words, the ensemble should only be derived from models that are similarly good estimators of the true function $f$.
The metrics we're using in this work certainly meet condition 1, as we've selected them to cover the breadth of methods in the literature. It remains to be seen, however, if they meet condition 2, and in fact we shall see that, as selected, they do not yet meet this condition.
Some recent prior work has used ensemble, or ensemble-esque methods to leverage LLMs effectively. These often involve iterative prompting techniques, such as in [DLT ${ }^{+} 23$ ], which prompts agents to "debate" each other before arriving at a final answer. SelfCheckGPT [MLG23] itself could be seen as a variation of an ensemble method, as it involves self-checking the model's responses against other responses it might have given. Similar work is in [WWS ${ }^{+} 22$ ], which incorporates a self-checking ensemble approach into the sampling algorithm for an LLM.
While in this work, we simply use naive similar ensembles of metrics with uniform weights, as has been shown to be effective [Daw79, PC95], some other work has been done on using unlabelled data to find the best term weights for metrics [PBM14, PDM16, PPMH17]. While this particular line of work is applied specifically to binary classifiers, and we're working with metrics that cannot be generally constricted to outputs $\in{0,1}$, we believe something like this approach could be extended to metric ensembles in future work.}$. Due to this minimization of error by a factor of $N$, ensembles are a powerful tool to deploy in spaces, such as hallucination detection, where we have many diverse estimates for ground truth, but no (or prohibitively slow and expensive) access to that ground truth itself. There are then two qualities of some collection of metrics $f_{i</p>
<h2>4 Method</h2>
<p>We used the wiki_bio_gpt3_hallucination dataset. This dataset consists of a subset of 238 entries from the original Wikibio dataset, generated in [LGA16], accompanied by GPT3-generated summaries and sentence-by-sentence human</p>
<p>|  | Human Eval | FactSumm Tuples | QAGS | $\begin{aligned} &amp; \text { R } \ &amp; \text { R } \ &amp; \text { R } \end{aligned}$ | $\begin{aligned} &amp; \text { R } \ &amp; \text { R } \ &amp; \text { R } \end{aligned}$ | $\begin{aligned} &amp; \text { R } \ &amp; \text { R } \ &amp; \text { R } \end{aligned}$ | $\begin{aligned} &amp; \text { R } \ &amp; \text { R } \ &amp; \text { R } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ | $\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text {S } \ &amp; \text { S } \end{aligned}$ |$\begin{aligned} &amp; \text { S } \ &amp; \text { S } \end{aligned}$ |$\begin{</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Heatmap of Pearson correlations between all benchmark metrics, our linear ensemble of all benchmarks, and human evaluations.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Plot of Pearson correlation with human judgment for each model.</p>
<h1>5.1.1 Our Ensemble Outperforms All Non-LLM-Based Methods</h1>
<p>We observe that the LLM-based methods have the highest correlation with human evaluations, but that the ensemble method outperforms all others. ${ }^{1}$ The fact that some of the base methods used in the ensemble outperform the ensemble itself suggests that, in this particular case, Condition 2 of Section 3.3 is not satisfied: there are certain methods (GPT-3 and GPT-4, used as in [LXA23]) that outperform the others to a significant enough extent that the gains from the ensemble's diversity are not sufficient to overcome the losses from giving more weight to less accurate models. It is for this reason that we conduct additional experiments in Section 5.2, focusing exclusively on the LLM-based methods.</p>
<h3>5.1.2 Methodological Similarities Yield High Correlations</h3>
<p>As we would expect, blocks of methods that are methodologically similar yield high Pearson correlations with each other. Particularly, the ROUGE-based methods are all highly correlated with each other, as are the SMART-based methods. These two blocks are also somewhat highly correlated with each other, likely due to the fact that they're doing very similar things under the hood (though SMART is doing this with sentences as the basic blocks of analysis, whereas ROUGE is doing it with tokens).</p>
<p>SummaC and LLM-based methods, alternatively, while slightly higher-correlated with methodologically similar methods, are not as homogeneous as these.</p>
<h3>5.1.3 Comparative Performance Measured By Human Evaluation</h3>
<p>As noted previously, LLM-based methods and QA-based methods both perform well on this dataset. Our graph-based method performed surprisingly poorly: without further study, however, it cannot be said if this is an indictment of graphbased hallucination detection methods as a whole, or simply of the particular tuple-generating models employed by FactSumm. It is possible that a more complex or robust graph-generating model would considerably boost performance here.</p>
<p>The zero-shot version of SummaC outperformed the convolutional version by a substantial margin, which is surprising, as it goes against the findings in the original SummaC paper. We take this as suggesting that the convolutional model weights might be overfit to the CNN dailymail dataset that the model was trained on, and that a more "commonsense" zero-shot model may actually be more generalizable.</p>
<h3>5.2 LLM Metric Correlations</h3>
<p>We ran the prompt in Figure 1 in GPT-4, at different temperature settings, ranging from 0 to 1.2. We also made an ensemble of these models' predictions, using the ensembling method described in Section $4^{2}$. We then compared the Pearson correlations between each model and the other models, as well as the model ensemble and human evaluations. Our results are collected in Table 2, and a heatmap is depicted in Figure 4. A bar plot specifically showing the correlations with human judgement is shown in Figure 5.
We will now discuss some of the more salient aspects of these results.</p>
<h3>5.2.1 Our LLM Ensemble Outperforms All Other Methods</h3>
<p>Most significantly, we find that our naive constant-weighted sum of GPT-4 temperatures outperformed every temperature in isolation. This suggests that the conditions of Section 3.3 are met in this domain, when we focus specifically on models that are permutations of the LLM-based method in [LXA23]. These SOTA results suggests that LLM ensemble methods may be a general tool to improve over a baseline of LLM performance, at least in domains such as this wherein the output being requested of the LLM is a single number, and thus they can be combined straightforwardly in a linear ensemble ${ }^{3}$.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Human Eval</th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">0.2</th>
<th style="text-align: center;">0.4</th>
<th style="text-align: center;">0.6</th>
<th style="text-align: center;">0.8</th>
<th style="text-align: center;">1.0</th>
<th style="text-align: center;">1.2</th>
<th style="text-align: center;">Ensemble</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Human Eval</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.901</td>
</tr>
<tr>
<td style="text-align: left;">0</td>
<td style="text-align: center;">0.892</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.993</td>
</tr>
<tr>
<td style="text-align: left;">0.2</td>
<td style="text-align: center;">0.889</td>
<td style="text-align: center;">0.988</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.991</td>
</tr>
<tr>
<td style="text-align: left;">0.4</td>
<td style="text-align: center;">0.890</td>
<td style="text-align: center;">0.984</td>
<td style="text-align: center;">0.983</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.990</td>
</tr>
<tr>
<td style="text-align: left;">0.6</td>
<td style="text-align: center;">0.893</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.975</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.963</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.986</td>
</tr>
<tr>
<td style="text-align: left;">0.8</td>
<td style="text-align: center;">0.895</td>
<td style="text-align: center;">0.970</td>
<td style="text-align: center;">0.966</td>
<td style="text-align: center;">0.974</td>
<td style="text-align: center;">0.963</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.982</td>
</tr>
<tr>
<td style="text-align: left;">1.0</td>
<td style="text-align: center;">0.874</td>
<td style="text-align: center;">0.964</td>
<td style="text-align: center;">0.958</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.947</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">0.973</td>
</tr>
<tr>
<td style="text-align: left;">1.2</td>
<td style="text-align: center;">0.869</td>
<td style="text-align: center;">0.954</td>
<td style="text-align: center;">0.952</td>
<td style="text-align: center;">0.950</td>
<td style="text-align: center;">0.951</td>
<td style="text-align: center;">0.941</td>
<td style="text-align: center;">0.923</td>
<td style="text-align: center;">1.000</td>
<td style="text-align: center;">0.969</td>
</tr>
<tr>
<td style="text-align: left;">Ensemble</td>
<td style="text-align: center;">0.901</td>
<td style="text-align: center;">0.993</td>
<td style="text-align: center;">0.991</td>
<td style="text-align: center;">0.990</td>
<td style="text-align: center;">0.986</td>
<td style="text-align: center;">0.982</td>
<td style="text-align: center;">0.973</td>
<td style="text-align: center;">0.969</td>
<td style="text-align: center;">1.000</td>
</tr>
</tbody>
</table>
<p>Table 2: Pearson correlation of different temperatures with human evaluations and the LLM ensemble.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Heatmap of Pearson correlations between GPT-4 evaluations at various temperatures, our linear GPT-4 ensemble, and human evaluations.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Plot of Pearson correlation with human judgement for each model. Note the log axis here, used to better visualize differences, as models in this ensemble had consistently higher correlations, and were more closely clustered together.</p>
<h1>5.2.2 Diversity Increases With Temperature</h1>
<p>As one would expect, higher-temperature GPT-4 runs are less correlated with each other than lower-temperature runs. Perhaps less intuitively, the higher-temperature runs are individually less correlated with each other than either is with the lower temperature runs: i.e., 0.6 and 0.8 are less correlated with each other than either is with 0.0 , despite being technically more methodologically similar to each other in a naive sense. This suggests that the 0 -temperature run acts as a sort of 'centroid' around which the other runs are clustered: though notably, it is not as 'central' (and, importantly, not as correlated with human evaluations) as the temperature ensemble.</p>
<h3>5.2.3 Lower Temperatures Are Not Straightforwardly Associated With Better Results</h3>
<p>While both previous literature and prevailing wisdom has suggested that LLMs tend to perform better at lower temperatures, our results suggest a perhaps more nuanced picture. Our highest-performing non-ensemble temperature was 0.8 , suggesting that some degree of stochasticity can improve overall performance. Notably, however, there is, as one would expect, such a thing as too much stochasticity, as the results decline rapidly from there.</p>
<h2>6 Discussion and Conclusion</h2>
<p>We've presented a review and appraisal of a representative sample of hallucination detection methods from recent literature, and found that LLM-based methods substantially outperform more traditional methods, in terms of their correlation with human evaluations on the Wikibio-gpt3 hallucination detection dataset. We've also tested an ensemble of LLM-based methods across temperature settings, and found this to outperform baseline LLM-based methods, suggesting a new SOTA for improving on LLM-based evaluation. We suggest that this indicates a new standard for LLM-based evaluation in which the LLMs are queried multiple times, and their answers combined, as a way to consistently improve performance over baseline LLM-based evaluation methods. In future work, it would be beneficial to extend this analysis both to more complex ensembling methods, such as those in [PBM14, PDM16, PPMH17], as well as to more complex LLM domains, in which the outputs of the LLM are not numerical values, and thus perhaps more difficult to straightforwardly combine. Some contemporary work, such as [DLT ${ }^{+} 23$, WWS ${ }^{+} 22$, MLG23], can be viewed partially as an extension of an ensembling approach to these more complex domains, and thus as evidence that this is a promising future direction for research. We hope the evaluation of hallucination detection methods we've presented here can help both to lend evidence to this more complex and integrated use of LLMs, as well as to elucidate the state of black box hallucination detection methods, and their relative strengths and drawbacks relative to each other.</p>
<h1>References</h1>
<p>[ALZN22] Reinald Kim Amplayo, Peter J Liu, Yao Zhao, and Shashi Narayan. Smart: sentences as basic units for text evaluation. arXiv preprint arXiv:2208.01030, 2022.
$\left[\mathrm{BCE}^{+} 23\right]$ Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.
[Daw79] Robyn M Dawes. The robust beauty of improper linear models in decision making. American psychologist, 34(7):571, 1979.
[DBWR21] Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. Towards question-answering as an automatic metric for evaluating the content quality of a summary. Transactions of the Association for Computational Linguistics, 9:774-789, 2021.
[DHD20] Esin Durmus, He He, and Mona Diab. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. arXiv preprint arXiv:2005.03754, 2020.
[DLT ${ }^{+} 23$ ] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.
[FC23] Grant Forbes and R Jordan Crouser. Metric ensembles aid in explainability: A case study with wikipedia data. Analytics, 2(2):315-327, 2023.
[GLD22] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the era of gpt-3. arXiv preprint arXiv:2209.12356, 2022.
[GRLS19] Ben Goodrich, Vinay Rao, Peter J Liu, and Mohammad Saleh. Assessing the factual accuracy of generated text. In proceedings of the 25th ACM SIGKDD international conference on knowledge discovery \&amp; data mining, pages 166-175, 2019.
[Heo21] Hoon Heo. Factsumm: Factual consistency scorer for abstractive summarization. https://github. com/Huffon/factsumm, 2021.
[HFFQ21] Yichong Huang, Xiachong Feng, Xiaocheng Feng, and Bing Qin. The factual inconsistency problem in abstractive text summarization: A survey. arXiv preprint arXiv:2104.14839, 2021.
[JLF ${ }^{+} 23$ ] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38, 2023.
[KMXS19] Wojciech Kryściński, Bryan McCann, Caiming Xiong, and Richard Socher. Evaluating the factual consistency of abstractive text summarization. arXiv preprint arXiv:1910.12840, 2019.
[LGA16] Rémi Lebret, David Grangier, and Michael Auli. Neural text generation from structured data with application to the biography domain. arXiv preprint arXiv:1603.07771, 2016.
[Lin04] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages $74-81,2004$.
[LN09] Annie Louis and Ani Nenkova. Automatically evaluating content selection in summarization without human models. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP '09, page 306-314, USA, 2009. Association for Computational Linguistics.
[LSBH22] Philippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. Summac: Re-visiting nli-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163-177, 2022.
[LXA23] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. Chatgpt as a factual inconsistency evaluator for abstractive text summarization. arXiv preprint arXiv:2303.15621, 2023.
[MLG23] Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896, 2023.
[PBM14] Emmanouil Antonios Platanios, Avrim Blum, and Tom M Mitchell. Estimating accuracy from unlabeled data. In $U A I$, volume 14, page 10, 2014.
[PC95] Michael P Perrone and Leon N Cooper. When networks disagree: Ensemble methods for hybrid neural networks. In How We Learn; How We Remember: Toward An Understanding Of Brain And Neural Systems: Selected Papers of Leon N Cooper, pages 342-358. World Scientific, 1995.</p>
<p>[PDM16] Emmanouil Antonios Platanios, Avinava Dubey, and Tom Mitchell. Estimating accuracy from unlabeled data: A bayesian approach. In International Conference on Machine Learning, pages 1416-1425. PMLR, 2016.
[PPMH17] Emmanouil Platanios, Hoifung Poon, Tom M Mitchell, and Eric J Horvitz. Estimating accuracy from unlabeled data: A probabilistic logic approach. Advances in neural information processing systems, 30, 2017.
$\left[\right.$ TDFH $\left.^{+} 22\right]$ Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.
[WCL20] Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to evaluate the factual consistency of summaries. arXiv preprint arXiv:2004.04228, 2020.
[WMYC23] Clinton T White, Neil P Molino, Julia S Yang, and John M Conroy. occams: A text summarization package. Analytics, 2(3):546-559, 2023.
[WWS $\left.{ }^{+} 22\right]$ Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.
[ZLZ23] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. Summit: Iterative text summarization via chatgpt. arXiv preprint arXiv:2305.14835, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ For further context, if the LLM-based models are removed from the ensemble, it outperforms all remaining methods other than QAGS, and if both the LLM-based and QAGS metrics are removed from the ensemble, it outperforms all remaining methods.
${ }^{2}$ Here, unlike in Section 5.1, the weights for each model in the ensemble were set to 1 , as there was no structure to the underlying models' similarity to each other that would suggest a need for 'grouping.'
${ }^{3}$ It remains to be seen the extent to which LLM-ensembling can improve performance in more complex domains; this is likely highly dependent on the effectiveness of how the disparate LLM outputs are 'combined.'&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
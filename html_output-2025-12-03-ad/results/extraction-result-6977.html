<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6977 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6977</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6977</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-260203094</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.14712v1.pdf" target="_blank">Evaluating Generative Models for Graph-to-Text Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have been widely employed for graph-to-text generation tasks. However, the process of finetuning LLMs requires significant training resources and annotation work. In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART. Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information. As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores. We have made the text generated by generative models publicly available.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6977.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6977.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized H/R/T triples (AGENDA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized triple sequence with <H>, <R>, <T> and section tokens (used for AGENDA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential, token-based serialization of graph triples into text where each triple is written as a head (<H>), relation (<R>), and tail (<T>) token sequence; for AGENDA additional section markers <title>, <entities>, and <graph> are prepended to the input to provide context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized triple sequence with section tokens</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Graphs are converted into a flat text sequence by listing triples and labeling components with special tokens: each triple is encoded as '<H> head_entity <R> relation <T> tail_entity'. For AGENDA the input is additionally structured with '<title>', '<entities>', and '<graph>' tokens before the graph sequence to provide title and entity lists.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Edge/triple list serialization as provided by dataset order (linearization); prepend special tokens <H>, <R>, <T> for each triple and dataset-specific section markers (<title>, <entities>, <graph>) for AGENDA. No explicit graph traversal algorithm (e.g., DFS/BFS) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td>169</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AGENDA</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (paper abstract generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003); ChatGPT (gpt-3.5-turbo-0301); baselines: T5 large, BART large (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot generative decoder-only LMs: GPT-3 variant text-davinci-003 and ChatGPT gpt-3.5-turbo-0301; baselines are finetuned encoder-decoder LMs (T5 large, BART large) reported from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, RougeL, Chrf++, BLEURT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AGENDA BLEU: T5 large 22.15, BART large 23.65, GPT-3 8.34, ChatGPT 10.57; METEOR: T5 ?/BART ? (reported in paper), GPT-3 14.88, ChatGPT 17.02; RougeL: GPT-3 24.99, ChatGPT 25.22; Chrf++: GPT-3 41.42, ChatGPT 45.86; BLEURT: GPT-3 -32.54, ChatGPT -28.05 (values as reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enabled zero-shot use of large generative LMs without fine-tuning (saves annotation/training resources), but provided lower end-task performance compared to finetuned models; representation allows direct prompting of off-the-shelf LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Representation is effectively lossy for structural information (authors report models struggle to capture semantic relations), models produce relational errors (reversed relations), ignore some relation types, and hallucinate or inject irrelevant internal knowledge; reproducibility of generative outputs is noted as poor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>When used zero-shot with GPT-3/ChatGPT, the linearized representation yields significantly lower automatic scores than when the same linearized inputs are used to finetune encoder‑decoder models (T5/BART) — e.g., BLEU ~8–10 for generative models vs. ~22–24 for finetuned models on AGENDA. The paper follows prior works (Konstas et al., Ribeiro et al.) in using linearization but shows inferior semantic fidelity without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Generative Models for Graph-to-Text Generation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6977.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6977.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Linearized H/R/T triples (WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Linearized RDF/triple sequence with <H>, <R>, <T> tokens (used for WebNLG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A flat serialization of RDF triples into token sequences where each triple is expressed with markers '<H>', '<R>', '<T>' before head, relation, and tail respectively; used as input text to prompt zero-shot generative LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Linearized RDF triple sequence</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>RDF triples (subgraphs from DBpedia in WebNLG) are converted to a single token sequence by writing each triple as '<H> head <R> relation <T> tail' in dataset-provided order; a prompt 'Generate text from graph:' is prepended as a task instruction.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, token-based, lossy</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Triple-list serialization (edge-list ordering as in dataset); special tokens <H>, <R>, <T> inserted per triple; prompt-based instruction precedes the linearized graph. No canonical traversal or normalization is described.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td>66</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>graph-to-text generation (knowledge-graph verbalization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-003); ChatGPT (gpt-3.5-turbo-0301); baselines: T5 large, BART large (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Zero-shot generative decoder LMs evaluated (GPT-3 text-davinci-003 and ChatGPT gpt-3.5-turbo-0301); comparison baselines are finetuned T5 large and BART large from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, RougeL, Chrf++, BLEURT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>WebNLG BLEU: T5 large 59.70, BART large 54.72, GPT-3 20.36, ChatGPT 11.08; METEOR: GPT-3 26.95, ChatGPT 23.89; RougeL: GPT-3 45.64, ChatGPT 35.87; Chrf++: GPT-3 57.95, ChatGPT 48.75; BLEURT: GPT-3 13.39, ChatGPT -10.99 (values as reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Allowed immediate zero-shot generation from structured RDF without dataset-specific fine-tuning; reduces need for training data but results in substantially lower generation quality than finetuned encoder-decoder models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Models often hallucinate (injects unrelated internal knowledge), produce longer texts and redundant content (ChatGPT), and make factual errors or misinterpret relation roles (GPT-3 sometimes recognizes entity types incorrectly). The linearization does not guarantee preservation of graph topology and relation directionality in model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to finetuned T5/BART on the same linearized inputs, zero-shot generative LMs perform far worse (BLEU ~11–20 vs. ~55–60), and different generative models show distinct failure modes (GPT-3 sometimes more factual but shorter; ChatGPT more prone to hallucinations and verbosity). The paper cites prior works that also use linearization (Konstas et al., Ribeiro et al.), noting that fine-tuning on linearized input yields much better performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Evaluating Generative Models for Graph-to-Text Generation', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Investigating pretrained language models for graph-to-text generation <em>(Rating: 2)</em></li>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity <em>(Rating: 2)</em></li>
                <li>The WebNLG challenge: Generating text from RDF data <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Neural pipeline for zero-shot data-to-text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6977",
    "paper_id": "paper-260203094",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Linearized H/R/T triples (AGENDA)",
            "name_full": "Linearized triple sequence with &lt;H&gt;, &lt;R&gt;, &lt;T&gt; and section tokens (used for AGENDA)",
            "brief_description": "A sequential, token-based serialization of graph triples into text where each triple is written as a head (&lt;H&gt;), relation (&lt;R&gt;), and tail (&lt;T&gt;) token sequence; for AGENDA additional section markers &lt;title&gt;, &lt;entities&gt;, and &lt;graph&gt; are prepended to the input to provide context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Linearized triple sequence with section tokens",
            "representation_description": "Graphs are converted into a flat text sequence by listing triples and labeling components with special tokens: each triple is encoded as '&lt;H&gt; head_entity &lt;R&gt; relation &lt;T&gt; tail_entity'. For AGENDA the input is additionally structured with '&lt;title&gt;', '&lt;entities&gt;', and '&lt;graph&gt;' tokens before the graph sequence to provide title and entity lists.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "Edge/triple list serialization as provided by dataset order (linearization); prepend special tokens &lt;H&gt;, &lt;R&gt;, &lt;T&gt; for each triple and dataset-specific section markers (&lt;title&gt;, &lt;entities&gt;, &lt;graph&gt;) for AGENDA. No explicit graph traversal algorithm (e.g., DFS/BFS) is reported.",
            "canonicalization": null,
            "average_token_length": 169,
            "dataset_name": "AGENDA",
            "task_name": "graph-to-text generation (paper abstract generation)",
            "model_name": "GPT-3 (text-davinci-003); ChatGPT (gpt-3.5-turbo-0301); baselines: T5 large, BART large (finetuned)",
            "model_description": "Zero-shot generative decoder-only LMs: GPT-3 variant text-davinci-003 and ChatGPT gpt-3.5-turbo-0301; baselines are finetuned encoder-decoder LMs (T5 large, BART large) reported from prior work.",
            "performance_metric": "BLEU, METEOR, RougeL, Chrf++, BLEURT",
            "performance_value": "AGENDA BLEU: T5 large 22.15, BART large 23.65, GPT-3 8.34, ChatGPT 10.57; METEOR: T5 ?/BART ? (reported in paper), GPT-3 14.88, ChatGPT 17.02; RougeL: GPT-3 24.99, ChatGPT 25.22; Chrf++: GPT-3 41.42, ChatGPT 45.86; BLEURT: GPT-3 -32.54, ChatGPT -28.05 (values as reported in paper tables).",
            "impact_on_training": "Enabled zero-shot use of large generative LMs without fine-tuning (saves annotation/training resources), but provided lower end-task performance compared to finetuned models; representation allows direct prompting of off-the-shelf LMs.",
            "limitations": "Representation is effectively lossy for structural information (authors report models struggle to capture semantic relations), models produce relational errors (reversed relations), ignore some relation types, and hallucinate or inject irrelevant internal knowledge; reproducibility of generative outputs is noted as poor.",
            "comparison_with_other": "When used zero-shot with GPT-3/ChatGPT, the linearized representation yields significantly lower automatic scores than when the same linearized inputs are used to finetune encoder‑decoder models (T5/BART) — e.g., BLEU ~8–10 for generative models vs. ~22–24 for finetuned models on AGENDA. The paper follows prior works (Konstas et al., Ribeiro et al.) in using linearization but shows inferior semantic fidelity without fine-tuning.",
            "uuid": "e6977.0",
            "source_info": {
                "paper_title": "Evaluating Generative Models for Graph-to-Text Generation",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Linearized H/R/T triples (WebNLG)",
            "name_full": "Linearized RDF/triple sequence with &lt;H&gt;, &lt;R&gt;, &lt;T&gt; tokens (used for WebNLG)",
            "brief_description": "A flat serialization of RDF triples into token sequences where each triple is expressed with markers '&lt;H&gt;', '&lt;R&gt;', '&lt;T&gt;' before head, relation, and tail respectively; used as input text to prompt zero-shot generative LMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Linearized RDF triple sequence",
            "representation_description": "RDF triples (subgraphs from DBpedia in WebNLG) are converted to a single token sequence by writing each triple as '&lt;H&gt; head &lt;R&gt; relation &lt;T&gt; tail' in dataset-provided order; a prompt 'Generate text from graph:' is prepended as a task instruction.",
            "representation_type": "sequential, token-based, lossy",
            "encoding_method": "Triple-list serialization (edge-list ordering as in dataset); special tokens &lt;H&gt;, &lt;R&gt;, &lt;T&gt; inserted per triple; prompt-based instruction precedes the linearized graph. No canonical traversal or normalization is described.",
            "canonicalization": null,
            "average_token_length": 66,
            "dataset_name": "WebNLG",
            "task_name": "graph-to-text generation (knowledge-graph verbalization)",
            "model_name": "GPT-3 (text-davinci-003); ChatGPT (gpt-3.5-turbo-0301); baselines: T5 large, BART large (finetuned)",
            "model_description": "Zero-shot generative decoder LMs evaluated (GPT-3 text-davinci-003 and ChatGPT gpt-3.5-turbo-0301); comparison baselines are finetuned T5 large and BART large from prior work.",
            "performance_metric": "BLEU, METEOR, RougeL, Chrf++, BLEURT",
            "performance_value": "WebNLG BLEU: T5 large 59.70, BART large 54.72, GPT-3 20.36, ChatGPT 11.08; METEOR: GPT-3 26.95, ChatGPT 23.89; RougeL: GPT-3 45.64, ChatGPT 35.87; Chrf++: GPT-3 57.95, ChatGPT 48.75; BLEURT: GPT-3 13.39, ChatGPT -10.99 (values as reported in paper tables).",
            "impact_on_training": "Allowed immediate zero-shot generation from structured RDF without dataset-specific fine-tuning; reduces need for training data but results in substantially lower generation quality than finetuned encoder-decoder models.",
            "limitations": "Models often hallucinate (injects unrelated internal knowledge), produce longer texts and redundant content (ChatGPT), and make factual errors or misinterpret relation roles (GPT-3 sometimes recognizes entity types incorrectly). The linearization does not guarantee preservation of graph topology and relation directionality in model outputs.",
            "comparison_with_other": "Compared to finetuned T5/BART on the same linearized inputs, zero-shot generative LMs perform far worse (BLEU ~11–20 vs. ~55–60), and different generative models show distinct failure modes (GPT-3 sometimes more factual but shorter; ChatGPT more prone to hallucinations and verbosity). The paper cites prior works that also use linearization (Konstas et al., Ribeiro et al.), noting that fine-tuning on linearized input yields much better performance.",
            "uuid": "e6977.1",
            "source_info": {
                "paper_title": "Evaluating Generative Models for Graph-to-Text Generation",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Investigating pretrained language models for graph-to-text generation",
            "rating": 2,
            "sanitized_title": "investigating_pretrained_language_models_for_graphtotext_generation"
        },
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2,
            "sanitized_title": "neural_amr_sequencetosequence_models_for_parsing_and_generation"
        },
        {
            "paper_title": "Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity",
            "rating": 2,
            "sanitized_title": "have_your_text_and_use_it_too_endtoend_neural_datatotext_generation_with_semantic_fidelity"
        },
        {
            "paper_title": "The WebNLG challenge: Generating text from RDF data",
            "rating": 2,
            "sanitized_title": "the_webnlg_challenge_generating_text_from_rdf_data"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Neural pipeline for zero-shot data-to-text generation",
            "rating": 1,
            "sanitized_title": "neural_pipeline_for_zeroshot_datatotext_generation"
        }
    ],
    "cost": 0.009415999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Generative Models for Graph-to-Text Generation
27 Jul 2023</p>
<p>Shuzhou Yuan shuzhou.yuan@kit.edu 
Karlsruhe Institute of Technology</p>
<p>Michael Färber michael.faerber@kit.edu 
Karlsruhe Institute of Technology</p>
<p>Evaluating Generative Models for Graph-to-Text Generation
27 Jul 202319EA2BDB0B67E1F5308F767A9EB76682arXiv:2307.14712v1[cs.CL]
Large language models (LLMs) have been widely employed for graph-to-text generation tasks.However, the process of finetuning LLMs requires significant training resources and annotation work.In this paper, we explore the capability of generative models to generate descriptive text from graph data in a zero-shot setting.Specifically, we evaluate GPT-3 and ChatGPT on two graph-to-text datasets and compare their performance with that of finetuned LLM models such as T5 and BART.Our results demonstrate that generative models are capable of generating fluent and coherent text, achieving BLEU scores of 10.57 and 11.08 for the AGENDA and WebNLG datasets, respectively.However, our error analysis reveals that generative models still struggle with understanding the semantic relations between entities, and they also tend to generate text with hallucinations or irrelevant information.As a part of error analysis, we utilize BERT to detect machine-generated text and achieve high macro-F1 scores.We have made the text generated by generative models publicly available. 1</p>
<p>Introduction</p>
<p>Graph-to-text generation is a subtask of data-to-text generation and natural language generation (NLG) (Gatt and Krahmer, 2018).Its purpose is to generate fluent descriptive text based on the structure of a given graph (see Figure 1).With the widespread use of graph structured data, this technique plays a crucial role in various natural language processing applications, including question answering, dialogue systems, and data augmentation (He et al., 2017;Zhao et al., 2020;Josifoski et al., 2023).Previous research on model architectures has achieved method of Ribeiro et al. (2021a), we represent the graph as a linearized sequence of text for input to the models (see Figure 1).</p>
<p>To assess the performance of the generative models, we conduct a comprehensive evaluation on each dataset.Employing machine translation metrics such as BLEU (Papineni et al., 2002), ME-TEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) to the generated texts, we reveal that the generative models fall short of matching the quality achieved by state-of-the-art approaches.To identify patterns of mistakes made by the generative models, we perform error analysis by comparing the generated texts with the reference texts.Additionally, we fine-tune a BERT model to detect the machinegenerated text.We make the texts generated by the models available on GitHub to facilitate future research on the analysis of machine-generated text and trustworthy AI.</p>
<p>In summary, our study aims to assess the performance of generative models in the zero-shot setting for graph-to-text generation using two distinct benchmarks.Our contribution lies in conducting a rigorous quantitative analysis of the results, shedding light on the effectiveness of generative models in this domain.</p>
<p>Related Work</p>
<p>Graph-to-text generation.Various efforts have been made to enhance graph-to-text generation using neural network models.They can be categorized into two main types: Graph Neural Network (GNN) based models and Language Model (LM) based models.GNN-based models typically em-ploy a graph encoder to encode the graph structure (Beck et al., 2018;Marcheggiani and Perez-Beltrachini, 2018;Damonte and Cohen, 2019;Koncel-Kedziorski et al., 2019;Ribeiro et al., 2019;Li et al., 2021).In contrast, LM-based models do not rely on the graph structure but purely on the sequence of tokens in the text.As such, graphs have first been transformed into a linearized representation before being fed into LMs to generate coherent text (Harkous et al., 2020;Ribeiro et al., 2021a,b).Besides GNN and LM, previous works have also explored the use of Recurrent Neural Networks (RNNs) such as LSTM and GRU for graph-to-text generation (Song et al., 2018;Zhao et al., 2020;Guo et al., 2020).We follow the approach of Konstas et al. (2017) and other prior works by using a linearized graph as input for generative models.</p>
<p>Generative Models.Generative language models, such as GPT-3 (Brown et al., 2020), Instruct-GPT (Ouyang et al., 2022), and ChatGPT, have been designed to learn and generate natural language text.These models are based on the transformer decoder architecture (Vaswani et al., 2017), which enables them to handle large amounts of training data and perform zero-shot applications.While GPT-3 has made a significant breakthrough in text completion, InstructGPT and ChatGPT possess unique characteristics that align user intent with a conversational style.These models are trained using supervised fine-tuning and reward modeling, allowing them to generate high-quality responses that accurately reflect the user's needs and preferences.InstructGPT and ChatGPT are first fine-tuned on the GPT-3 model through supervised learning and then further trained using As demonstrated by Ouyang et al. (2022), this approach substantially improves the model's performance on NLP benchmarks.Although there have been numerous reports and research evaluating the performance of generative models in various NLP applications such as summarization (Bang et al., 2023), machine translation (Jiao et al., 2023), and multilingual evaluation (Ahuja et al., 2023), our work focuses on the generative models' capability to handle structured data.</p>
<p>Dataset</p>
<p>We evaluate generative models using the AGENDA and WebNLG datasets, as they are widely used in recent research on graph-to-text generation (Koncel-Kedziorski et al., 2019;Ribeiro et al., 2021a;Li et al., 2021) and as they represent different domains: scholarly domain and general domain (e.g., as given in Wikipedia).We focus on the test sets of AGENDA and WebNLG for our experiments, as the models do not require further training.In the following, we briefly describe the used datasets.AGENDA.Abstract GENeration DAtaset (AGENDA) is a dataset that pairs knowledge graphs with paper abstracts from scientific domains (Koncel-Kedziorski et al., 2019).The graphs in AGENDA were automatically extracted from the SciIE information extraction system (Luan et al., 2018).Each instance in AGENDA includes the title, entities, graph, and abstract of a paper.We use the title, entities, and graph as input for the models.</p>
<p>WebNLG.This dataset is a benchmark for mapping sets of RDF triples to text (Gardent et al., 2017).The RDF triples are subgraphs of the knowledge graph DBpedia (Auer et al., 2007), while the texts describe the graphs in one or a few sentences.The WebNLG challenge2 has released several versions of this dataset since 2017.In order to com-</p>
<p>Results</p>
<p>Our results are summarized in Table 2 and 3 The results obtained from AGENDA demonstrate that finetuned BART and T5 models outperform generative models in terms of state-of-the-art performance.Both T5 and BART achieve BLEU scores exceeding 20, while GPT-3 only attains a BLEU score of 8.34 and ChatGPT achieves 10.57.Consistently, other evaluation metrics align with the BLEU scores, further highlighting the limited performance of generative models without finetuning.Notably, ChatGPT exhibits a slightly improved performance compared to GPT-3 on the AGENDA benchmark.Analysis of the results reveals that ChatGPT consistently outperforms GPT-3 across all metrics, showcasing a 2.23 higher BLEU score, a 2.14 higher METEOR score, a 0.23 higher RougeL score, a 4.44 higher Chrf++ score, and a 4.49 higher BLEURT score.</p>
<p>Examining the results from WebNLG, it becomes evident that fine-tuned T5 and BART models consistently outperform generative models without fine-tuning.Notably, both T5 and BART achieve BLEU scores exceeding 50, whereas generative models only attain a BLEU score of 11.08 for Chat-GPT and 20.36 for GPT-3.Surprisingly, GPT-3 outperforms ChatGPT on the WebNLG benchmark with a BLEU score that is 9.28 higher, a METEOR score that is 3.06 higher, a RougeL score that is 9.77 higher, and a Chrf++ score that is 9.20 higher.The primary reason for this difference is that ChatGPT tends to produce hallucinations easily and generates longer text.We provide further elaboration on two examples in Section 5.</p>
<p>Error Analysis</p>
<p>We observe that the texts generated by generative models contain errors following similar patterns.In Table 4 and Table 5, we show two examples from AGENDA and WebNLG.As shown in the example of Table 4, generative models are capable of generating fluent paper abstracts based on the provided input, but they contain errors that contradict the facts represented by the graph structure.For instance, the triple "accuracy <R> EVALUATE-FOR <T> correlation analysis" in the example graph is wrongly generated by GPT-3 as "which (correlation analysis) is then used to evaluate the estimation accuracies".Also, Chat-GPT describes it as "(The authors) apply clustering and correlation analysis to evaluate the accuracy".In both cases, the models reverse the relationship between "accuracy" and "correlation analysis".</p>
<p>Furthermore, ChatGPT ignores the relations "PART-OF" and "USED-FOR" in the input.This suggests that generative models still face challenges in comprehending the semantic relations between entities, which can result in the generation of inaccurate text.</p>
<p>While generating paper abstracts is complex and challenging, generating short descriptive text from a knowledge graph is relatively more straightforward.As LLMs, GPT-3 and ChatGPT are trained</p>
<p>AGENDA Example</p>
<p>Input</p>
<p>Generate paper abstract from title, entities and graph: <title> Estimation of semantic case of Japanese dialogue by use of distance derived from statistics of dependency .<entities> statistics of dependent noun-particle-verb triples atr dialogue corpus measure of distance semantic cases noun-particle-verb triples consistency rates single-case clusters clustering analysis correlation analysis estimation accuracies accuracy <graph> <H> statistics of dependent noun-particle-verb triples <R> USED-FOR <T> measure of distance <H> nounparticle-verb triples <R> PART-OF <T> atr dialogue corpus <H> semantic cases <R> PART-OF <T> atr dialogue corpus <H> accuracy <R> EVALUATE-FOR <T> correlation analysis</p>
<p>Reference In an attempt to estimate the semantic cases for noun-particle-verb triples in the atr dialogue corpus, the authors propose a measure of distance based on statistics of dependent noun-particle-verb triples.A clustering analysis of all the triples in the corpus was conducted using the measure of distance.Competence of the proposed measure of distance is verified by examination of the distribution of the single-case clusters.By use of the score derived from the measure of distance of the training corpus, the authors conducted the estimation of the correct semantic case for a given noun-particle-verb triples in the test corpus.</p>
<p>The result remarkably differentiates the particles with respect to the estimation accuracies.For instance, particle 'wo' has accuracies over 80 %, while 'de' has accuracies less than 40%.The correlation analysis between the accuracy and the consistency rates indicates that the particles of higher consistency have also tendencies to higher accuracies.</p>
<p>GPT-3</p>
<p>This paper aims to estimate semantic cases of Japanese dialogue using a measure of distance derived from statistics of dependent noun-particle-verb triples from the ATR dialogue corpus.Clustering analysis and correlation analysis are employed to examine the consistency rates of single-case clusters, which is then used to evaluate the estimation accuracies of the distance measure.</p>
<p>ChatGPT</p>
<p>This paper proposes a method to estimate the semantic case of Japanese dialogue using a measure of distance derived from the statistics of dependent nounparticle-verb triples.The authors use the ATR dialogue corpus and apply clustering and correlation analysis to evaluate the accuracy of their method.They report consistent rates for single-case clusters and show that their method achieves high estimation accuracies.The study demonstrates the potential of using statistics of dependency for analyzing Japanese dialogue.</p>
<p>on large corpora and thus already contain world knowledge.In the example provided in Table 5, generative models generate descriptive text fluently based on the structured input.However, GPT-3 produces text with incorrect facts.For instance, "Abner W. Sibal" is the name of a person, but it is recognized as the name of a battle by GPT-3.While GPT-3 produces text with incorrect facts, it is worth noting that ChatGPT-generated text not only covers the input information, but also contains redundant messages from its internal knowledge (hallucinations).Furthermore, the sentence "Abner W. Sibal was a prominent figure who faced great challenges" generated by ChatGPT has made a subjective judgement about the character and may cause unnecessary bias to potential users.</p>
<p>To investigate the difference between modelgenerated texts and human-written reference texts, we create datasets containing both types of text and finetune a pretrained BERT model for a binary text However, the fight against the Axis powers was not just a battle against military might.It was a battle against the ideologies that fueled the enemy's aggression.As a commander, Sibal had to navigate these complexities and ensure that his troops remained committed to the cause of freedom.Throughout the war, Sibal faced many challenges and obstacles.But he remained steadfast and only grew more determined to see the war through to its ultimate conclusion.His efforts were recognized and honored, not just by his own country but also by leaders around the world.Even the likes of Joseph Stalin, renowned commander and leader of the Soviet Union, recognized the critical role Sibal played in the victory against fascism.Today, Abner W. Sibal's name remains a testament to the bravery and dedication of those who fought in World War II.His legacy serves as an inspiration to future generations, reminding them of the importance of standing up against tyranny and oppression wherever it may arise.We create several datasets for AGENDA, WebNLG, and a combined dataset containing both AGENDA and WebNLG examples.The training and test sets are split in an 80:20 ratio.We fine-tune BERT for five epochs using the AdamW optimizer (Loshchilov and Hutter, 2019).As shown in Table 7, BERT achieves high scores across all datasets.This demonstrates that generative models generate text that follows similar patterns, and a state-of-theart text classifier can easily distinguish between them.</p>
<p>Conclusion</p>
<p>In this paper, we explored the capabilities of generative models in generating coherent text from structured data, focusing on two benchmarks: AGENDA and WebNLG.To achieve this, we adopted the linearized graph representation approach employed Leveraging the zero-shot ability of language models, we prepended the prompt to the input text as an instruction for both GPT-3 and ChatGPT.We conducted a comprehensive evaluation using various metrics.Our findings reveal that generative models fall short of surpassing previous models that have been trained and finetuned on large volumes of training data.These results highlight limitations of generative models in achieving state-of-the-art performance in graph-totext generation tasks.Furthermore, we conducted an error analysis of the text generated by the models.The generative models struggle in capturing the relationships between entities and often produce unrelated information, leading to hallucinations.To further investigate the machine generated text, we employ finetuned BERT to conduct a text classification task.BERT achieves high F1 scores in distinguishing between machine-generated text and human-written text.Our study provides extensive evaluation of generative models for graph-to-text generation.Future work should focus on refining machine-generated text and reducing hallucinations for graph-to-text generation by using generative models.</p>
<p>Ethical Consideration and Limitation</p>
<p>We observe that generative models may generate text containing fake facts or offensive content.And the datasets we collected may also contain incorrect or offensive statements.We do not support the views expressed in the machine generated text, we merely venture to analyze the machine generated text and provide an useful resource for future research.</p>
<p>As the limitation of this work, we found out that the reproducibility of GPT-3 and ChatGPT is questionable.The models often return different response from same request, which makes our results hard to reproduce and the brings randomness to the evaluation scores.</p>
<p>Figure 1: Examples of graph structures, prompts and linearised graphs of (a) AGENDA and (b) WebNLG.</p>
<p>. As comparison, we take the results from Ribeiro et al. (2021a), which are achieved by finetuned BART and T5.</p>
<p>Table 1 :
1
Statistics of test sets from AGENDA and WebNLG.
AGENDA WebNLGNumber of Instance1,0001,862Average Input Tokens16966reinforcement learning based on human feedback.</p>
<p>Table 2 :
2
Results on AGENDA.
ModelBLEU↑ METEOR↑ RougeL↑ Chrf++↑ BLEURT↑T5 large (Ribeiro et al., 2021a)22.1523.73---13.96BART large (Ribeiro et al., 2021a)23.6525.19---10.93GPT-38.3414.8824.9941.42-32.54ChatGPT10.5717.0225.2245.86-28.05</p>
<p>Table 3 :
3
Results on WebNLG.
DatasetBLEU↑ METEOR↑ RougeL↑ Chrf++↑ BLEURT↑T5 large (Ribeiro et al., 2021a)59.7044.18-75.40-BART large (Ribeiro et al., 2021a)54.7242.23-72.29-GPT-320.3626.9545.6457.9513.39ChatGPT11.0823.8935.8748.75-10.99pretrained BERT (Devlin et al., 2019) model.</p>
<p>Table 4 :
4
Paper abstract generated by GPT-3 and ChatGPT, incorrect texts are marked with red.</p>
<p>Table 5 :
5
Descriptive text generated by GPT-3 and ChatGPT, incorrect texts are marked with red, hallucinations are marked with blue.Joseph Stalin was the commander of World War II and Abner W. Sibal was one of the battles fought during the war.ChatGPT Abner W. Sibal was a prominent figure who faced great challenges during his time as a soldier in World War II.He fought bravely as a commander, leading his troops with dedication and determination.His efforts were critical to the success of the war effort and his name became synonymous with bravery and patriotism.During World War II, the world was gripped in the fight against tyranny and oppression.Abner W. Sibal was one of the many who answered the call to serve his country.Through his leadership skills and strategic thinking, he was able to navigate the treacherous waters of war and emerge victorious.
WebNLG ExampleInputGenerate text from graph: <H> Abner W. Sibal <R> battles <T> World War II<H> World War II <R> commander <T> Joseph StalinReference Joseph Stalin was a commander in World War II in which Abner W Sibal alsofought.GPT-3</p>
<p>Table 6 :
6
Statistics of ChatGPT generated datasets
DatasetMachine-generated instance Human-written instanceAGENDA10001000WebNLG18624894All28625894classification task. The statistics of our datasets arepresented in Table 6.</p>
<p>Table 7 :
7
Results of BERT to detect GPT-3 and Chat-GPT generated text.
ModelAccuracy Macro F1GPT-3 AGENDA98.0098.00ChatGPT AGENDA100100GPT-3 WebNLG91.6489.25ChatGPT WebNLG96.8295.75GPT-3 All93.5592.38ChatGPT All96.4095.82in prior work.
https://github.com/ShuzhouYuan/Eval_ G2T_GenModels significant performance on graph-to-text generation benchmarks (Koncel-Kedziorski et al.
, 2019; Ribeiro et
 al., 2020;Zhao et al., 2020;Li
et al., 2021;Ribeiro et al., 2021b). In particular, Ribeiro et al. (2021a) achieved state-of-the-art performance by employing large pretrained language models and sufficient training data. However, the zero-shot setting for graph-to-text generation remains challenging due to the inconsistent input format (unstructured text vs. preformatted text) between pretraining and fine-tuning stages for large language models.Recently, generative models such as GPT-3(Brown et al., 2020), 
InstructGPT (Ouyang et al.,  2022), and ChatGPT have gained tremendous attention in both the NLP research community and the general public. Researchers have evaluated these models on various NLP benchmarks in
the zeroshot setting(Bang et al., 2023;Jiao et al., 2023;Ahuja et al., 2023). However, their ability to process structured data, and in particular graph data, such as knowledge graphs, is understudied and worth being explored(Bang et al., 2023). Given the significant resources and annotations required for training graph-to-text generation models(Li et al., 2021), utilizing a zero-shot setting could save training resources and prove advantageous for both economic and ecological reasons.Previous approaches has come up with a neural pipeline to enable zero-shot for graph-to-text generation but didn't use generative models(Kasner and Dusek, 2022). In contrast, our approach adopts the zero-shot setting by using prompts as instructions for generative models, specifically GPT-3 and ChatGPT(Brown et al., 2020;Ouyang et al., 2022). We evaluate the models' ability to translate graph data into fluent text using the test sets from two widely used graph-to-text generation datasets: WebNLG(Gardent et al., 2017) and AGENDA(Koncel-Kedziorski et al., 2019). Following the
https://synalp.gitlabpages.inria.fr/ webnlg-challenge/ pare with previous work, we take the test data of WebNLG challenge 2017 for our experiments.4 Experiments Data Preprocessing. Since GPT-3 and ChatGPT require a sequence of text as input, we convert the graph structure into a linearized representation followingRibeiro et al. (2021a). To assist the models in identifying the head, relation, and tail entities, we prepend <H>, <R>, and <T> tokens before the entities, as done in previous work(Harkous et al., 2020). In the AGENDA dataset, each sample also includes a title and entities. Thus, we additionally add <title>, <entities>, and <graph> tokens (see Figure1).Model Settings. We use the GPT-3 model variant text-davinci-003 and the ChatGPT model variant gpt-3.5-turbo-0301 for our experiments. Each instance is treated as a single request, and the first response from the model is taken as the generated text. The prompt used for the models plays a significant role as it serves as the task description and directly influences the content of the generated text. Previous work designed prompts by asking ChatGPT(Jiao et al., 2023). Following their approach, we ask ChatGPT to provide prompts: "Please provide prompts or templates for graph-to-text generation:". Since AGENDA and WebNLG have different data structures, we use the prompt "Generate paper abstract from title, entities, and graph:" for AGENDA. For WebNLG, we use the prompt "Generate text from graph:". We expect that in this way the generated text fits the format of a scientific paper abstract better for AGENDA, while the models generate texts in open domain for WebNLG.Baseline. Similar to our experimental methodology, Ribeiro et al. (2021a) finetuned T5 and BART using linearized graphs as input and generated descriptive texts. Therefore, we consider their findings as the baseline for comparison with our own experiments.Evaluation. Following related work, we implement a thorough evaluation with metrics BLEU(Papineni et al., 2002), METEOR(Banerjee and Lavie, 2005), RougeL(Lin, 2004) and Chrf++(Popović, 2017). Additionally, to assess the semantic meaning and coherence of the generated text, we employ BLEURT(Sellam et al., 2020), a metric that evaluates not only the surface match of n-grams but also the semantic representation extracted from a
AcknowledgementsWe thank the anonymous reviewers for their helpful comments.We also would like to thank Nicholas Popovic for his feedback on this work.
Kabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, arXiv:2303.12528Mega: Multilingual evaluation of generative ai. 2023arXiv preprint</p>
<p>Dbpedia: A nucleus for a web of open data. Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, Zachary Ives, The Semantic Web. Berlin, Heidelberg; Berlin HeidelbergSpringer2007</p>
<p>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Satanjeev Banerjee, Alon Lavie, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization. the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or SummarizationAnn Arbor, Michigan2005Association for Computational Linguistics</p>
<p>Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. 2023arXiv preprint</p>
<p>Graph-to-sequence learning using gated graph neural networks. Daniel Beck, Gholamreza Haffari, Trevor Cohn, 10.18653/v1/P18-1026Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Structural neural encoders for AMR-to-text generation. Marco Damonte, Shay B Cohen, 10.18653/v1/N19-1366Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics2019</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>The WebNLG challenge: Generating text from RDF data. Claire Gardent, Anastasia Shimorina, Shashi Narayan, Laura Perez-Beltrachini, 10.18653/v1/W17-3518Proceedings of the 10th International Conference on Natural Language Generation. the 10th International Conference on Natural Language GenerationSantiago de Compostela, SpainAssociation for Computational Linguistics2017</p>
<p>Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Albert Gatt, Emiel Krahmer, Journal of Artificial Intelligence Research. 612018</p>
<p>CycleGT: Unsupervised graph-to-text and text-to-graph generation via cycle training. Qipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang, David Wipf, Zheng Zhang, Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+). the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)Dublin, IrelandAssociation for Computational Linguistics2020</p>
<p>Have your text and use it too! end-to-end neural data-to-text generation with semantic fidelity. Hamza Harkous, Isabel Groves, Amir Saffari, 10.18653/v1/2020.coling-main.218Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain2020International Committee on Computational Linguistics</p>
<p>Generating natural answers by incorporating copying and retrieving mechanisms in sequence-tosequence learning. Shizhu He, Cao Liu, Kang Liu, Jun Zhao, 10.18653/v1/P17-1019Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Is chatgpt a good translator? a preliminary study. Wenxiang Jiao, Wenxuan Wang, Jen-Tse Huang, Xing Wang, Zhaopeng Tu, arXiv:2301.087452023arXiv preprint</p>
<p>Exploiting asymmetry for synthetic training data generation: SynthIE and the case of information extraction. Martin Josifoski, Marija Sakota, Maxime Peyrard, Robert West, arXiv:2303.041322023arXiv preprint</p>
<p>Neural pipeline for zero-shot data-to-text generation. Zdeněk Kasner, Ondrej Dusek, 10.18653/v1/2022.acl-long.271Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Text Generation from Knowledge Graphs with Graph Transformers. Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, Hannaneh Hajishirzi, 10.18653/v1/N19-1238Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Neural AMR: Sequence-to-sequence models for parsing and generation. Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, Luke Zettlemoyer, 10.18653/v1/P17-1014Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics2017</p>
<p>Fewshot knowledge graph-to-text generation with pretrained language models. Junyi Li, Tianyi Tang, Wayne Xin Zhao, Zhicheng Wei, Nicholas Jing Yuan, Ji-Rong Wen, 10.18653/v1/2021.findings-acl.136Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Online. Association for Computational Linguistics2021</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Decoupled Weight Decay Regularization. Ilya Loshchilov, Frank Hutter, ICLR'19Proceedings of the 7th International Conference on Learning Representations. the 7th International Conference on Learning Representations2019</p>
<p>Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi, 10.18653/v1/D18-1360Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Deep graph convolutional encoders for structured data to text generation. Diego Marcheggiani, Laura Perez-Beltrachini, 10.18653/v1/W18-6501Proceedings of the 11th International Conference on Natural Language Generation. the 11th International Conference on Natural Language GenerationTilburg University, The NetherlandsAssociation for Computational Linguistics2018</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, arXiv:2203.021552022arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational Linguistics2002</p>
<p>chrF++: words helping character n-grams. Maja Popović, 10.18653/v1/W17-4770Proceedings of the Second Conference on Machine Translation. the Second Conference on Machine TranslationCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Enhancing AMR-to-text generation with dual graph representations. F R Leonardo, Claire Ribeiro, Iryna Gardent, Gurevych, 10.18653/v1/D19-1314Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, China2019Association for Computational Linguistics</p>
<p>Investigating pretrained language models for graph-to-text generation. F R Leonardo, Martin Ribeiro, Schmitt, 10.18653/v1/2021.nlp4convai-1.20Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI. the 3rd Workshop on Natural Language Processing for Conversational AIOnline. Association for Computational Linguistics2021aHinrich Schütze, and Iryna Gurevych</p>
<p>Modeling global and local node contexts for text generation from knowledge graphs. F R Leonardo, Yue Ribeiro, Claire Zhang, Iryna Gardent, Gurevych, 10.1162/tacl_a_00332Transactions of the Association for Computational Linguistics. 82020</p>
<p>Structural adapters in pretrained language models for AMR-to-Text generation. F R Leonardo, Yue Ribeiro, Iryna Zhang, Gurevych, 10.18653/v1/2021.emnlp-main.351Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021bOnline and Punta Cana</p>
<p>Bleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur P Parikh, ACL. 2020</p>
<p>A graph-to-sequence model for AMRto-text generation. Linfeng Song, Yue Zhang, Zhiguo Wang, Daniel Gildea, 10.18653/v1/P18-1150Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics20181</p>
<p>Bridging the structural gap between encoding and decoding for data-to-text generation. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł Kaiser, Illia Polosukhin, ; Chao Zhao, Marilyn Walker, Snigdha Chaturvedi, 10.18653/v1/2020.acl-main.224Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsCurran Associates, Inc2017. 202030Advances in Neural Information Processing Systems. Association for Computational Linguistics</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-483 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-483</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-483</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-18.html">extraction-schema-18</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <p><strong>Paper ID:</strong> paper-15979705</p>
                <p><strong>Paper Title:</strong> Learning to generate pseudo-code from source code using statistical machine translation</p>
                <p><strong>Paper Abstract:</strong> Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e483.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e483.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PBMT_mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Phrase-based Machine Translation structural mismatch</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PBMT's phrase-to-phrase mapping cannot represent hierarchical source patterns or wildcard-like correspondences, producing incorrect short phrase correspondences between code tokens and natural language (e.g., '== 0 :' → 'is divisible').</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SMT-based pseudo-code generator (PBMT variant)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A phrase-based statistical machine translation system trained to translate single-line Python statements into pseudo-code (natural language) using phrase tables and reordering models.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>human-created pseudo-code (training corpus descriptions/line-to-line pseudo-code)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python source code statements (token sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>different algorithmic expressiveness / incomplete mapping (lack of hierarchical/wildcard representation)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>PBMT models operate on flat phrase-to-phrase mappings and reordering, so they cannot express hierarchical correspondences or wildcards (e.g., mapping 'X % Y == 0' to 'X is divisible by Y'). This leads to 'wrong' local correspondences where multi-token code constructs are split and translated into partial or awkward natural language phrases (e.g., mapping '== 0 :' → 'is divisible').</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>model architecture / translation rule expressiveness (phrase extraction and phrase table)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>qualitative analysis of translation examples and quantitative comparison of system variants (PBMT vs T2SMT) using BLEU and human acceptability</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>BLEU scores and human acceptability ratings; comparative BLEU improvements when switching to T2SMT were reported (see impact_on_results); distributions of acceptability categories were also analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Substantially lower translation quality compared to tree-based models: PBMT gave much lower BLEU and acceptability than T2SMT variants (example English BLEU: PBMT 25.71 vs Reduced-T2SMT 54.08; Japanese BLEU: PBMT ≈51.67 vs Reduced-T2SMT ≈62.88). PBMT produced more ungrammatical/less-accurate pseudo-code and reduced usefulness for code comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Observed across the dataset as the baseline system performance; PBMT produced lower-quality outputs systematically compared to T2SMT variants (reported for both English and Japanese corpora).</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Model limitation: phrase-based formulation lacks mechanisms for representing hierarchical source structure and wildcards needed to capture many code-to-language correspondences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Use tree-to-string SMT (T2SMT) which leverages parse trees and derivations (wildcards) to capture hierarchical relations; extract tree-to-string rules (GHKM) instead of phrase tables.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Highly effective: T2SMT variants, especially Reduced-T2SMT, achieved large BLEU gains (e.g., English BLEU improved from 25.71 to 54.08) and higher mean human acceptability (from ≈3.62 to ≈4.15). Improvements were statistically significant (pairwise bootstrap test p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>programming languages / software engineering / statistical machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to generate pseudo-code from source code using statistical machine translation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e483.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e483.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AST_surface_misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Abstract syntax tree vs surface token misalignment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Abstract ASTs produced by standard parsers omit surface tokens or place surface words as internal nodes, causing misalignment with natural-language pseudo-code and harming rule extraction/word alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>T2SMT-based pseudo-code generator (Raw-T2SMT, Head-T2SMT, Reduced-T2SMT variants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Tree-to-string SMT systems that translate source-code parse trees into pseudo-code; variants differ in preprocessing of ASTs (raw AST, head insertion, pruning/simplification).</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>human-created pseudo-code (line-to-line descriptions used as SMT targets)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>abstract syntax trees from Python's ast module (parsed code trees)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>representation mismatch / missing surface tokens in parser output</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Standard 'abstract' ASTs often make keywords and operators internal nodes rather than leaves, so surface words that correspond one-to-one with natural language tokens are not present at tree leaves used for alignment; this impedes GHKM rule extraction and word alignment between code and pseudo-code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / source parsing (AST representation used by T2SMT rule extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>analysis of extracted translation rules and alignment behavior; observation that raw ASTs cannot directly be used and require transformations (head insertion and simplification) to recover surface words for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Indirectly measured by downstream translation quality (BLEU) and acceptability: applying head insertion and reductions affected BLEU and acceptability scores, indicating the AST surface mismatch's effect.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Without surface recovery, T2SMT cannot extract useful tree-to-string rules, degrading translation quality; with head insertion + pruning (Reduced-T2SMT) BLEU and acceptability improved (Reduced-T2SMT highest BLEU and mean acceptability).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>A systemic issue when using abstract parsers; manifested across experiments and required an explicit preprocessing step for practical T2SMT training.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Design of abstract parsers (ASTs reflect execution semantics, not textual surface forms), causing loss of direct surface tokens needed for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Surface modification of ASTs: head insertion (insert HEAD leaves containing inner-node labels) and pruning/simplification heuristics (20 hand-written rules) to reduce noise and make trees compatible with GHKM extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective when combined with pruning: Head insertion alone sometimes introduced noisy tokens and reduced BLEU on high-variance data (Head-T2SMT < Raw-T2SMT for Python-to-English), but Reduced-T2SMT (head insertion + pruning/simplification) produced the best BLEU and acceptability (statistically significant improvements, p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>programming languages / NLP applied to code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to generate pseudo-code from source code using statistical machine translation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e483.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e483.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Head_insertion_noise</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Head insertion induced word-alignment noise</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Inserting HEAD nodes into ASTs to expose surface tokens can introduce extra tokens that do not carry useful alignment information, causing noisy word alignments and reduced translation quality in high-variance corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Head-T2SMT preprocessing variant within the SMT-based pseudo-code generation pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Preprocessing step that adds 'HEAD' leaves to ASTs to expose inner-node labels as leaves to assist alignment for T2SMT training.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>training data pseudo-code (human-written line-level descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python ASTs modified with head insertion</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>preprocessing side-effect / added-noise in alignment</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Head insertion injects additional tokens into the parse tree; some of these tokens have no relevant correspondence to the target natural language, increasing noise in the automatic word-alignment process and sometimes harming translation quality, especially in datasets with high variance in pseudo-code expression.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>data preprocessing / word alignment stage</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>empirical comparison of BLEU scores across variants: Raw-T2SMT vs Head-T2SMT showed Head-T2SMT had lower BLEU on the Python-to-English dataset, indicating introduced noise.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Comparative BLEU scores across system variants (Raw-T2SMT vs Head-T2SMT vs Reduced-T2SMT) and analysis of word-alignment behavior; significance tested with pairwise bootstrap (10,000 samples).</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Head-T2SMT underperformed Raw-T2SMT on the Python-to-English dataset (Raw-T2SMT English BLEU ≈ 49.74 vs Head-T2SMT ≈ 47.69), indicating a decrease in translation accuracy attributable to alignment noise. After pruning/simplification (Reduced-T2SMT), performance recovered and surpassed others.</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Apparent in datasets with greater variance in human pseudo-code (e.g., Django-derived English corpus); not observed as a problem in the more homogeneous Japanese dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Insertion of extra HEAD tokens creates tokens with little or no alignment signal in the target language; automatic unsupervised alignment models then produce suboptimal alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Apply pruning and simplification heuristics after head insertion (Reduced-T2SMT) to remove nodes not related to surface forms, reducing alignment noise.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Effective: Reduced-T2SMT (head insertion + pruning/simplification) outperformed both Raw-T2SMT and Head-T2SMT (example English BLEU: Reduced-T2SMT ≈ 54.08, statistically significant improvement p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>software engineering / NLP for code</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to generate pseudo-code from source code using statistical machine translation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e483.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e483.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of discrepancies, gaps, or misalignments between natural language descriptions (such as paper descriptions, documentation, or specifications) and their corresponding code implementations in automated experimentation systems, including how these gaps are identified, measured, and their impacts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>generated_pseudocode_confusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generated pseudo-code errors that confuse readers and slow comprehension</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Occasional incorrect or ungrammatical generated pseudo-code lines (scored low on acceptability) confused human readers, increasing reading time and reducing perceived understandability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated pseudo-code presentation in code-understanding user study (Reduced-T2SMT outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Experimental setup where participants read Python functions with either no pseudo-code, human-written reference pseudo-code, or automatically generated pseudo-code (Reduced-T2SMT) and measured comprehension and reading time.</td>
                        </tr>
                        <tr>
                            <td><strong>nl_description_type</strong></td>
                            <td>human reference pseudo-code vs automatically generated pseudo-code shown to participants</td>
                        </tr>
                        <tr>
                            <td><strong>code_implementation_type</strong></td>
                            <td>Python function definitions (presented alongside pseudo-code in web interface)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_type</strong></td>
                            <td>incorrect/misleading generated description vs actual code behavior (semantic/grammatical errors in natural language outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_description</strong></td>
                            <td>Some automatically generated pseudo-code lines were erroneous or unnatural (some scored 1 on acceptability), which confused readers attempting to understand code and led to longer average time to judge understandability compared to showing code alone or human reference pseudo-code.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_location</strong></td>
                            <td>human-facing output / documentation generated from code</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>human-subject code-understanding experiment measuring Likert-style understandability scores and elapsed time to respond; acceptability annotation by expert annotators.</td>
                        </tr>
                        <tr>
                            <td><strong>measurement_method</strong></td>
                            <td>Mean understandability impressions (Likert 0–5) and mean time to understand (seconds) across three settings (Code-only, Reference human pseudo-code, Automated Reduced-T2SMT pseudo-code); acceptability ratings by 5 expert Python programmers; BLEU used to measure automatic quality.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_results</strong></td>
                            <td>Automated pseudo-code improved perceived understandability over Code-only (mean impression All: Code ≈1.95 → Automated ≈2.28), but increased mean reading time (All mean time: Code ≈33.35s → Automated ≈43.15s), implying confusion/extra effort; inexperienced users showed larger time increase (from ≈24.99s to ≈39.52s).</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_or_prevalence</strong></td>
                            <td>Not quantified as a percentage of outputs in the paper, but acceptability distribution showed around 50% of outputs from all systems were grammatically correct; Reduced-T2SMT produced fewer low-acceptability outputs and achieved mean acceptability ≈4.15.</td>
                        </tr>
                        <tr>
                            <td><strong>root_cause</strong></td>
                            <td>Remaining generation errors from imperfect rule coverage, alignment noise, limited training data, and model mis-generalizations leading to inaccurate or ungrammatical pseudo-code lines.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_approach</strong></td>
                            <td>Improve generator quality via Reduced-T2SMT preprocessing (head insertion + pruning), increase high-quality parallel training data, and refine SMT rule extraction; manual curation or post-filtering of generated pseudo-code could also reduce confusing outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Reduced-T2SMT reduced low-quality outputs relative to PBMT and other variants (mean acceptability improved to ≈4.15); however, automated outputs still caused longer reading times than reference pseudo-code, indicating partial but not complete mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>program comprehension / human factors in software engineering</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_impact</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Learning to generate pseudo-code from source code using statistical machine translation', 'publication_date_yy_mm': '2015-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Phrase-based statistical translation of programming languages <em>(Rating: 2)</em></li>
                <li>Towards automatically generating summary comments for java methods <em>(Rating: 2)</em></li>
                <li>Automatically detecting and describing high level actions within methods <em>(Rating: 1)</em></li>
                <li>Automatic documentation inference for exceptions <em>(Rating: 1)</em></li>
                <li>Automatic generation of natural language summaries for java classes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-483",
    "paper_id": "paper-15979705",
    "extraction_schema_id": "extraction-schema-18",
    "extracted_data": [
        {
            "name_short": "PBMT_mismatch",
            "name_full": "Phrase-based Machine Translation structural mismatch",
            "brief_description": "PBMT's phrase-to-phrase mapping cannot represent hierarchical source patterns or wildcard-like correspondences, producing incorrect short phrase correspondences between code tokens and natural language (e.g., '== 0 :' → 'is divisible').",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SMT-based pseudo-code generator (PBMT variant)",
            "system_description": "A phrase-based statistical machine translation system trained to translate single-line Python statements into pseudo-code (natural language) using phrase tables and reordering models.",
            "nl_description_type": "human-created pseudo-code (training corpus descriptions/line-to-line pseudo-code)",
            "code_implementation_type": "Python source code statements (token sequences)",
            "gap_type": "different algorithmic expressiveness / incomplete mapping (lack of hierarchical/wildcard representation)",
            "gap_description": "PBMT models operate on flat phrase-to-phrase mappings and reordering, so they cannot express hierarchical correspondences or wildcards (e.g., mapping 'X % Y == 0' to 'X is divisible by Y'). This leads to 'wrong' local correspondences where multi-token code constructs are split and translated into partial or awkward natural language phrases (e.g., mapping '== 0 :' → 'is divisible').",
            "gap_location": "model architecture / translation rule expressiveness (phrase extraction and phrase table)",
            "detection_method": "qualitative analysis of translation examples and quantitative comparison of system variants (PBMT vs T2SMT) using BLEU and human acceptability",
            "measurement_method": "BLEU scores and human acceptability ratings; comparative BLEU improvements when switching to T2SMT were reported (see impact_on_results); distributions of acceptability categories were also analyzed.",
            "impact_on_results": "Substantially lower translation quality compared to tree-based models: PBMT gave much lower BLEU and acceptability than T2SMT variants (example English BLEU: PBMT 25.71 vs Reduced-T2SMT 54.08; Japanese BLEU: PBMT ≈51.67 vs Reduced-T2SMT ≈62.88). PBMT produced more ungrammatical/less-accurate pseudo-code and reduced usefulness for code comprehension.",
            "frequency_or_prevalence": "Observed across the dataset as the baseline system performance; PBMT produced lower-quality outputs systematically compared to T2SMT variants (reported for both English and Japanese corpora).",
            "root_cause": "Model limitation: phrase-based formulation lacks mechanisms for representing hierarchical source structure and wildcards needed to capture many code-to-language correspondences.",
            "mitigation_approach": "Use tree-to-string SMT (T2SMT) which leverages parse trees and derivations (wildcards) to capture hierarchical relations; extract tree-to-string rules (GHKM) instead of phrase tables.",
            "mitigation_effectiveness": "Highly effective: T2SMT variants, especially Reduced-T2SMT, achieved large BLEU gains (e.g., English BLEU improved from 25.71 to 54.08) and higher mean human acceptability (from ≈3.62 to ≈4.15). Improvements were statistically significant (pairwise bootstrap test p &lt; 0.001).",
            "domain_or_field": "programming languages / software engineering / statistical machine translation",
            "reproducibility_impact": false,
            "uuid": "e483.0",
            "source_info": {
                "paper_title": "Learning to generate pseudo-code from source code using statistical machine translation",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "AST_surface_misalignment",
            "name_full": "Abstract syntax tree vs surface token misalignment",
            "brief_description": "Abstract ASTs produced by standard parsers omit surface tokens or place surface words as internal nodes, causing misalignment with natural-language pseudo-code and harming rule extraction/word alignment.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "T2SMT-based pseudo-code generator (Raw-T2SMT, Head-T2SMT, Reduced-T2SMT variants)",
            "system_description": "Tree-to-string SMT systems that translate source-code parse trees into pseudo-code; variants differ in preprocessing of ASTs (raw AST, head insertion, pruning/simplification).",
            "nl_description_type": "human-created pseudo-code (line-to-line descriptions used as SMT targets)",
            "code_implementation_type": "abstract syntax trees from Python's ast module (parsed code trees)",
            "gap_type": "representation mismatch / missing surface tokens in parser output",
            "gap_description": "Standard 'abstract' ASTs often make keywords and operators internal nodes rather than leaves, so surface words that correspond one-to-one with natural language tokens are not present at tree leaves used for alignment; this impedes GHKM rule extraction and word alignment between code and pseudo-code.",
            "gap_location": "data preprocessing / source parsing (AST representation used by T2SMT rule extraction)",
            "detection_method": "analysis of extracted translation rules and alignment behavior; observation that raw ASTs cannot directly be used and require transformations (head insertion and simplification) to recover surface words for alignment.",
            "measurement_method": "Indirectly measured by downstream translation quality (BLEU) and acceptability: applying head insertion and reductions affected BLEU and acceptability scores, indicating the AST surface mismatch's effect.",
            "impact_on_results": "Without surface recovery, T2SMT cannot extract useful tree-to-string rules, degrading translation quality; with head insertion + pruning (Reduced-T2SMT) BLEU and acceptability improved (Reduced-T2SMT highest BLEU and mean acceptability).",
            "frequency_or_prevalence": "A systemic issue when using abstract parsers; manifested across experiments and required an explicit preprocessing step for practical T2SMT training.",
            "root_cause": "Design of abstract parsers (ASTs reflect execution semantics, not textual surface forms), causing loss of direct surface tokens needed for alignment.",
            "mitigation_approach": "Surface modification of ASTs: head insertion (insert HEAD leaves containing inner-node labels) and pruning/simplification heuristics (20 hand-written rules) to reduce noise and make trees compatible with GHKM extraction.",
            "mitigation_effectiveness": "Effective when combined with pruning: Head insertion alone sometimes introduced noisy tokens and reduced BLEU on high-variance data (Head-T2SMT &lt; Raw-T2SMT for Python-to-English), but Reduced-T2SMT (head insertion + pruning/simplification) produced the best BLEU and acceptability (statistically significant improvements, p &lt; 0.001).",
            "domain_or_field": "programming languages / NLP applied to code",
            "reproducibility_impact": false,
            "uuid": "e483.1",
            "source_info": {
                "paper_title": "Learning to generate pseudo-code from source code using statistical machine translation",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "Head_insertion_noise",
            "name_full": "Head insertion induced word-alignment noise",
            "brief_description": "Inserting HEAD nodes into ASTs to expose surface tokens can introduce extra tokens that do not carry useful alignment information, causing noisy word alignments and reduced translation quality in high-variance corpora.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Head-T2SMT preprocessing variant within the SMT-based pseudo-code generation pipeline",
            "system_description": "Preprocessing step that adds 'HEAD' leaves to ASTs to expose inner-node labels as leaves to assist alignment for T2SMT training.",
            "nl_description_type": "training data pseudo-code (human-written line-level descriptions)",
            "code_implementation_type": "Python ASTs modified with head insertion",
            "gap_type": "preprocessing side-effect / added-noise in alignment",
            "gap_description": "Head insertion injects additional tokens into the parse tree; some of these tokens have no relevant correspondence to the target natural language, increasing noise in the automatic word-alignment process and sometimes harming translation quality, especially in datasets with high variance in pseudo-code expression.",
            "gap_location": "data preprocessing / word alignment stage",
            "detection_method": "empirical comparison of BLEU scores across variants: Raw-T2SMT vs Head-T2SMT showed Head-T2SMT had lower BLEU on the Python-to-English dataset, indicating introduced noise.",
            "measurement_method": "Comparative BLEU scores across system variants (Raw-T2SMT vs Head-T2SMT vs Reduced-T2SMT) and analysis of word-alignment behavior; significance tested with pairwise bootstrap (10,000 samples).",
            "impact_on_results": "Head-T2SMT underperformed Raw-T2SMT on the Python-to-English dataset (Raw-T2SMT English BLEU ≈ 49.74 vs Head-T2SMT ≈ 47.69), indicating a decrease in translation accuracy attributable to alignment noise. After pruning/simplification (Reduced-T2SMT), performance recovered and surpassed others.",
            "frequency_or_prevalence": "Apparent in datasets with greater variance in human pseudo-code (e.g., Django-derived English corpus); not observed as a problem in the more homogeneous Japanese dataset.",
            "root_cause": "Insertion of extra HEAD tokens creates tokens with little or no alignment signal in the target language; automatic unsupervised alignment models then produce suboptimal alignments.",
            "mitigation_approach": "Apply pruning and simplification heuristics after head insertion (Reduced-T2SMT) to remove nodes not related to surface forms, reducing alignment noise.",
            "mitigation_effectiveness": "Effective: Reduced-T2SMT (head insertion + pruning/simplification) outperformed both Raw-T2SMT and Head-T2SMT (example English BLEU: Reduced-T2SMT ≈ 54.08, statistically significant improvement p &lt; 0.001).",
            "domain_or_field": "software engineering / NLP for code",
            "reproducibility_impact": false,
            "uuid": "e483.2",
            "source_info": {
                "paper_title": "Learning to generate pseudo-code from source code using statistical machine translation",
                "publication_date_yy_mm": "2015-11"
            }
        },
        {
            "name_short": "generated_pseudocode_confusion",
            "name_full": "Generated pseudo-code errors that confuse readers and slow comprehension",
            "brief_description": "Occasional incorrect or ungrammatical generated pseudo-code lines (scored low on acceptability) confused human readers, increasing reading time and reducing perceived understandability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Automated pseudo-code presentation in code-understanding user study (Reduced-T2SMT outputs)",
            "system_description": "Experimental setup where participants read Python functions with either no pseudo-code, human-written reference pseudo-code, or automatically generated pseudo-code (Reduced-T2SMT) and measured comprehension and reading time.",
            "nl_description_type": "human reference pseudo-code vs automatically generated pseudo-code shown to participants",
            "code_implementation_type": "Python function definitions (presented alongside pseudo-code in web interface)",
            "gap_type": "incorrect/misleading generated description vs actual code behavior (semantic/grammatical errors in natural language outputs)",
            "gap_description": "Some automatically generated pseudo-code lines were erroneous or unnatural (some scored 1 on acceptability), which confused readers attempting to understand code and led to longer average time to judge understandability compared to showing code alone or human reference pseudo-code.",
            "gap_location": "human-facing output / documentation generated from code",
            "detection_method": "human-subject code-understanding experiment measuring Likert-style understandability scores and elapsed time to respond; acceptability annotation by expert annotators.",
            "measurement_method": "Mean understandability impressions (Likert 0–5) and mean time to understand (seconds) across three settings (Code-only, Reference human pseudo-code, Automated Reduced-T2SMT pseudo-code); acceptability ratings by 5 expert Python programmers; BLEU used to measure automatic quality.",
            "impact_on_results": "Automated pseudo-code improved perceived understandability over Code-only (mean impression All: Code ≈1.95 → Automated ≈2.28), but increased mean reading time (All mean time: Code ≈33.35s → Automated ≈43.15s), implying confusion/extra effort; inexperienced users showed larger time increase (from ≈24.99s to ≈39.52s).",
            "frequency_or_prevalence": "Not quantified as a percentage of outputs in the paper, but acceptability distribution showed around 50% of outputs from all systems were grammatically correct; Reduced-T2SMT produced fewer low-acceptability outputs and achieved mean acceptability ≈4.15.",
            "root_cause": "Remaining generation errors from imperfect rule coverage, alignment noise, limited training data, and model mis-generalizations leading to inaccurate or ungrammatical pseudo-code lines.",
            "mitigation_approach": "Improve generator quality via Reduced-T2SMT preprocessing (head insertion + pruning), increase high-quality parallel training data, and refine SMT rule extraction; manual curation or post-filtering of generated pseudo-code could also reduce confusing outputs.",
            "mitigation_effectiveness": "Reduced-T2SMT reduced low-quality outputs relative to PBMT and other variants (mean acceptability improved to ≈4.15); however, automated outputs still caused longer reading times than reference pseudo-code, indicating partial but not complete mitigation.",
            "domain_or_field": "program comprehension / human factors in software engineering",
            "reproducibility_impact": false,
            "uuid": "e483.3",
            "source_info": {
                "paper_title": "Learning to generate pseudo-code from source code using statistical machine translation",
                "publication_date_yy_mm": "2015-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Phrase-based statistical translation of programming languages",
            "rating": 2,
            "sanitized_title": "phrasebased_statistical_translation_of_programming_languages"
        },
        {
            "paper_title": "Towards automatically generating summary comments for java methods",
            "rating": 2,
            "sanitized_title": "towards_automatically_generating_summary_comments_for_java_methods"
        },
        {
            "paper_title": "Automatically detecting and describing high level actions within methods",
            "rating": 1,
            "sanitized_title": "automatically_detecting_and_describing_high_level_actions_within_methods"
        },
        {
            "paper_title": "Automatic documentation inference for exceptions",
            "rating": 1,
            "sanitized_title": "automatic_documentation_inference_for_exceptions"
        },
        {
            "paper_title": "Automatic generation of natural language summaries for java classes",
            "rating": 1,
            "sanitized_title": "automatic_generation_of_natural_language_summaries_for_java_classes"
        }
    ],
    "cost": 0.01304375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Learning to Generate Pseudo-code from Source Code using Statistical Machine Translation</p>
<p>Yusuke Oda 
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, 630-0192Takayama, IkomaNaraJapan</p>
<p>Hiroyuki Fudaba fudaba.hiroyuki.ev6@is.naist.jp 
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, 630-0192Takayama, IkomaNaraJapan</p>
<p>Graham Neubig neubig@is.naist.jp 
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, 630-0192Takayama, IkomaNaraJapan</p>
<p>Hideaki Hata hata@is.naist.jp 
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, 630-0192Takayama, IkomaNaraJapan</p>
<p>Sakriani Sakti ssakti@is.naist.jp 
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, 630-0192Takayama, IkomaNaraJapan</p>
<p>Tomoki Toda tomoki@is.naist.jp 
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, 630-0192Takayama, IkomaNaraJapan</p>
<p>Satoshi Nakamura s-nakamura@is.naist.jp 
Graduate School of Information Science
Nara Institute of Science and Technology
8916-5, 630-0192Takayama, IkomaNaraJapan</p>
<p>Learning to Generate Pseudo-code from Source Code using Statistical Machine Translation
0EB4E5ED4C734D15A0EF6A614EBCE70210.1109/ASE.2015.36AlgorithmsEducationStatistical Approach
Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages.However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create.If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort.In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework.SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort.In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.</p>
<p>I. INTRODUCTION</p>
<p>Understanding source code is an essential skill for all programmers.This is true for programmers at all levels, as it is necessary to read and understand code that others have written to, for example, work efficiently in a group or to integrate and modify open-source software.On a macro level, there are a variety of tools to aid engineers in comprehending the overall structure of programming projects.For example, DeLine et al. proposed a tool that allows programming experts to evaluate large-scale cooperative software engineering projects [1].Rahman et al. also proposed a system that recommends methods for fixing source code when it does not work as expected [2].</p>
<p>On a more fine-grained level, when we try to understand the behavior of source code in detail, we usually need to read each statement in the source code carefully, and understand what each statement does.Of course, thoroughly reading and understanding source code of existing software is possible (although time consuming) for veteran programmers.In contrast, this process is much more difficult for beginner programmers or programmers who are learning a new programming language.Such inexperienced readers sometimes do not understand the grammar and style of the source code at hand, so reading source code written in such languages imposes a large burden.</p>
<p>On the other hand, in educational texts about programming and algorithms, it is common to use "pseudo-code," which describes the behavior of statements in the program using natural language (usually in English, or the programmers' mother tongue) or mathematical expressions.Pseudo-code aids comprehension of beginners because it explicitly describes what the program is doing, but is more readable than an unfamiliar programming language.</p>
<p>Fig. 1 shows an example of Python source code, and English pseudo-code that describes each corresponding statement in the source code. 1 If the reader is a beginner at Python (or a beginner at programming itself), the left side of Fig. 1 may be difficult to understand.On the other hand, the right side of the figure can be easily understood by most English speakers, and we can also learn how to write specific operations in Python (e.g. if we want to check if the type of the variable is not an integer, we can see that we write "if not isinstance(something, int):").In other words, pseudo-code aids the "bottom-up comprehension" [3] of given source code.</p>
<p>However, in real programming environments, pseudo-code corresponding to source code rarely exists, because pseudocode is not necessary once a programmer has a good grasp of the programming language and project.In addition, indiscriminately inserting pseudo-code into existing source code manually would impose a large burden on programmers.On the other hand, if pseudo-code could be generated automatically, this burden could be reduced, and pseudo-code could be created for actual programming projects.If we want to practically use automatically generated pseudo-code, we can say that satisfying the following 4 points is required:</p>
<p>• pseudo-code is accurate enough to describe the behavior of the original source code,</p>
<p>• pseudo-code should be provided upon the readers' request,</p>
<p>• pseudo-code should be automatically generated to avoid the burden on programmers, and</p>
<p>• the method to generate pseudo-code should be efficient, to avoid making the readers wait.</p>
<p>In this study, we propose a method to automatically generate pseudo-code from source code.In particular, our proposed method makes two major contributions: • To our knowledge, this is the first method for generating pseudo-code that completely describes the corresponding source code.This should be contrasted with previous work on comment generation, which aims to help experienced engineers by reducing the amount of source code to be read, and is described in §II.</p>
<p>• We propose a framework to perform pseudo-code generation using statistical machine translation (SMT).</p>
<p>SMT is a technology that can automatically learn how to translate between two languages, and was designed for translating between natural languages, such as English and Japanese.Our proposed method of applying this to pseudo-code generation has several benefits, the largest of which being that it is easy to extend the generator to other combinations of programming and natural languages simply by preparing data similar to that in Fig. 1.</p>
<p>In this paper, we first refer to related works on automatic comment generation and clarify the differences and the merits of our method ( §II).Second, we describe the summary of two SMT frameworks to be used in this study ( §III).Next, we explain how to apply the SMT framework to pseudo-code generation ( §IV), how we gather source code/pseudo-code parallel data to train our pseudo-code generation system ( §V), and the method to evaluate the generated pseudo-code using automatic and code understanding criteria ( §VI).In experiments, we apply our pseudo-code generation system to the Python-to-English and Python-to-Japanese pseudo-code generation tasks, and we find that providing automatically generated pseudocode along with the original source code makes the code easier to understand for programming beginners ( §VII).We finally mention the conclusion and future directions, including applications to other software engineering tasks ( §VIII). 2</p>
<p>II. RELATED WORKS</p>
<p>There is a significant amount of previous work on automatic comment and document generation.The important difference between our work and conventional studies is the motivation.Previous works are normally based on the thought of "reducing" the amount of code to be read.This is a plausible idea for veteran engineers, because their objective is to understand a large amount of source code efficiently, and thus comments are expected to concisely summarize what the code is doing, instead of covering each statement in detail.From a technical point of view, however, pseudocode generation is similar to automatic comment generation or automatic documentation as both generate natural language descriptions from source code, so in this section we outline the methods used in previous approaches and contrast them to our method.</p>
<p>There are two major paradigms for automatic comment generation: rule-based approaches, and data-based approaches.Regarding the former, for example, Sridhara et al. perform summary comment generation for Java methods by analyzing the actual method definition using manually defined heuristics [4], [5].Buse et al. also proposed a method to generate documents that include the specification of exceptions that could be thrown by a function and cases in which exceptions occur [6].Moreno et al. also developed a method to generate summaries that aid understanding, especially focusing on class definitions [7].These rule-based approaches use detailed information closely related to the structure of source code, allowing for handling of complicated language-specific structures, and are able to generate accurate comments when their rules closely match the given data.However, if we need new heuristics for a specific variety of source code or project, or to create a system for a new programming language or natural language, the system creator must manually append these heuristics themselves.This causes a burden on the maintainers of comment generation systems, and is a fundamental limitation of rule-based systems.</p>
<p>On the other hand, in contrast to rule-based systems, databased approaches can be found in the comment generation or code summarization fields.Wong et al. proposed a method for automatic comment generation that extracts comments from entries of programming question and answer sites using information retrieval techniques [8].There are also methods to generate summaries for code based on automatic text summarization [9] or topic modeling [10] techniques, possibly in combination with the physical actions of expert engineers [11].This sort of data-based approach has a major merit: if we want to improve the accuracy of the system, we need only increase the amount of "training data" used to construct it.However, existing methods are largely based on retrieving already existing comments, and thus also have significant issues with "data sparseness;" if a comment describing the existing code doesn't already exist in the training data, there is no way to generate accurate comments.</p>
<p>SMT, which we describe in the next section, combines the advantages of these approaches, allowing for detailed generation of text, like the rule-based approaches, while being learnable from data like the data-based approaches.</p>
<p>III. STATISTICAL MACHINE TRANSLATION</p>
<p>SMT is an application of natural language processing (NLP), which discovers the lexical or grammatical relationships between two natural languages (such as English and Japanese), and converts sentences described in a natural language into another natural language.SMT algorithms used in recent years are mainly based on two ideas:</p>
<p>• extract the relationships (usually called "rules") between small fragments of new input and output languages, and</p>
<p>• use these relationships to synthesize the translation results of a new input sentence using statistical models to decide which translation is best [12], [13], [14].</p>
<p>SMT frameworks have quickly developed in recent years, mainly because of the large amount of data available and the increase in calculation power of computers.In this section, we describe the foundations of the SMT framework.Especially, we explain the details of the phrase-based machine translation (PBMT) and tree-to-string machine translation (T2SMT), which are major SMT frameworks, used in this work to convert source code into pseudo-code.</p>
<p>A. Overview of Statistical Machine Translation</p>
<p>At first, we introduce some notation for the formulation of SMT-based pseudo-code generation.Let s = [s 1 , s 2 , • • • , s |s| ] describe the "source sentence," an array of input tokens, and t = [t 1 , t 2 , • • • , t |t| ] describe the "target sentence," an array of output tokens.The notation | • | represents the length of a sequence.In this paper, we are considering source code to pseudo-code translation, so s represents the tokens in the input source code statements and t represents the words of a pseudo-code sentence.For example, in the small Python to English example in Fig. 2, s is described as the sequence of Python tokens ["if", "x", "%", "5", "==", "0", ":"], and t is described as ["if", "x", "is", "divisible", "by", "5"], with |s| = 7 and |t| = 6.</p>
<p>The objective of SMT is to generate the most probable target sentence t given a source sentence s.Specifically, we do so by defining a model specifying the conditional probability distribution of t given s, or Pr(t|s), and find the t that maximizes this probability: t ≡ arg max t Pr(t|s).</p>
<p>(
)1
The difference of each SMT framework is guided by the method for calculating Pr(t|s).This probability is estimated using a set of source/target sentence pairs called a "parallel corpus."For example, Fig. 1 is one example of the type of parallel corpus targeted in this study, in which have one-byone correspondences between each line in the source code and pseudo-code.</p>
<p>B. Phrase-based Machine Translation</p>
<p>One of the most popular SMT frameworks is phrasebased machine translation (PBMT) [15], which directly uses the phrase-to-phrase relationships between source and target language pairs.In software engineering studies, Karaivanov et al. proposed a method applying the PBMT framework to programming language conversion, which learns the relationships between parts of statements in two programming languages (e.g."System.out.println" in Java to "Console.WriteLine" in C#) [16].</p>
<p>To describe PBMT modeling, we introduce a set of phrase (n) represents the n-th subsequence of the source sentence s (n) and the target subsequence t (n) associated with the corresponding source subsequence.For example, in Fig. 2, s is separated into |φ| = 4 phrases:
pairs φ = [φ 1 , φ 2 , • • • φ |φ| ]. Each φ n = s (n) → tφ = ⎡ ⎢ ⎣ "if" → "if" "x" → "x" "% 5" → "by 5" "== 0 :" → "is divisible" ⎤ ⎥ ⎦ .
(
)2
φ is generated using a "phrase table", which contains various phrase-to-phrase relationships with probabilities, and is extracted from a parallel corpus as explained in §III-D.</p>
<p>Given φ, we can generate the target sentence t from the source sentence by concatenating each t (n) .But simply concatenating t (n) according to their order cannot obtain an accurate target sentence, because the grammatical characteristics (e.g.ordering of tokens) of the source and target languages are usually different.For example, if we concatenate the target side phrases of Equation ( 2), we obtain the target sentence "if x by 5 is divisible," which is not a fluent English sentence.</p>
<p>To avoid this problem, we need to perform "reordering," which chooses the proper order of phrases in the target sentence.To express reordering, we introduce the phrase
alignment a = [a 1 , a 2 , • • • , a |φ| ],
where each a n is an integer that represents the order of the n-th phrase pair in the source sentence.In Fig. 2, we assume that a = [1,2,4,3], which means that first and second phrase pairs keep their own positions, and the third and fourth phrase pairs are swapped before the target sentence is generated.</p>
<p>Formally, the conditional probability Pr(t|s) of the PBMT model is usually estimated using a log-linear model, that combines several probabilities calculated over the source and target sentences [17]:
t ≡ arg max t Pr(t|s)(3)arg max t,φ,a Pr(t, φ, a|s) (4) arg max t,φ,a exp(w T f (t, φ, a, s)) t exp(w T f (t , φ, a, s)) (5) = arg max t,φ,a w T f (t, φ, a, s), (6)
where f (t, φ, a, s) represents feature functions calculated during the translation process, and w represents the weight vector of the corresponding features, which defines the importance of each feature.Intuitively, Equation ( 6) means that the PBMT model finds the sentence which has the highest "score" calculated by the weighted sum of the features: w T f (t, φ, a, s).</p>
<p>Some typical examples of these features include:</p>
<p>• Language model Pr(t) measures the fluency of the sentence t under the target language as described in §III-E.</p>
<p>• Phrase translation model calculates the product of the probabilities Pr(t (n) |s (n) ) of the individual phrases in the sentence.</p>
<p>• Reordering model Pr(a|φ) calculates the probability of the arranging each phrase in a particular order.</p>
<p>While PBMT's mechanism of translating and reordering short phrases is simple, it also lacks the expressive power to intuitively model pseudo-code generation.For example, we intuitively know that the English string including two wildcards "X is divisible by Y " corresponds to the source code "X % Y == 0:," but PBMT is not capable of using these wildcards.Thus, φ in the example in Fig. 2 uses obviously "wrong" correspondences such as "==0:" → "is divisible" .In addition, source code has an inherent hierarchical structure which can not be used by explicitly when only performing phrase-to-phrase translation and reordering.</p>
<p>C. Tree-to-string Machine Translation</p>
<p>As we mentioned in the previous section, PBMT-based pseudo-code generation cannot take advantage of wildcards or the hierarchical correspondences between two languages.T2SMT uses the parse tree of source sentence T s instead of the source tokens s to avoid this problem [18], as shown in Fig. 3.</p>
<p>T2SMT was originally conceived for translating natural languages such as English, and because natural languages include ambiguities we obtain the parse tree T s using a probabilistic parser that defines the probability T s given s, or Pr(T s |s) [19], [20].Thus, the formulation of T2SMT can be obtained by introducing this parsing probability into Equation (1):
t = arg max t Pr(t|s)(7)
arg max</p>
<p>t,Ts</p>
<p>Pr(t|T s ) Pr(T s |s).(8) Fortunately, the probability Pr(T s |s) can be ignored in our proposed method for pseudo-code generation, because all practical programming languages have a compiler or interpreter that can parse the corresponding source code deterministically, and thus there is only one possible parse tree.So the formulation is less complex:
t arg max t Pr(t|T s ),(9)
Fig. 3 shows the process of T2SMT-based pseudo-code generation.First, we obtain the parse tree T s by transforming the input statement into a token array using tokenization, and parsing the token array into a parse tree using parsing.The important point of Fig. 3 is that T s is defined by the grammar of the programming language, and is not an "abstract" syntax tree.This requirement comes from the characteristics of the inner workings of the T2SMT algorithm, and this topic is described in detail in §IV.</p>
<p>The probability Pr(t|T s ) is defined similarly to that of PBMT (usually formulated as a log-linear model) with two major differences:
• In the T2SMT, we use the "derivation" d = [d 1 , d 2 , • • • , d |d| ] instead of the phrase pairs φ in PBMT. Each d n = T (n) s
→ t (n) represents the relationship between between a source subtree (the gray boxes in Fig. 3) and target phrase with wildcards.All derivations are connected according to the structure of original parse tree T s , and the target sentence is generated by replacing wildcards with their corresponding phrases.</p>
<p>• The T2SMT translation process does not include explicit reordering models because the ordering of the wildcards in the target phrase naturally defines the target ordering.</p>
<p>D. Extracting SMT Rules</p>
<p>To train the PBMT and T2SMT models, we have to extract the translation rules, which define the relationship between the parts of the source and target language sentences, from the given parallel corpus.To do this, we use a "word alignment" between both sentences.Word alignments represent the wordto-word level relationships between both languages, shown in Fig. 4. In standard SMT training, word aligments are automatically calculated from the statistics of a parallel corpus by using a probabilistic model and unsupervised machine learning techniques [21], [22].After obtaining the word alignment of each sentence in the parallel corpus, we assume that the phrases that satisfy the below conditions can be extracted as a phrase pair for the PBMT framework:</p>
<p>• some words in both phrases are aligned, and</p>
<p>• no words outside of the phrases are aligned to a word in either phrase.</p>
<p>For example, Fig. 5 shows one phrase pair extraction φ = "== 0 :" → "is divisible" .</p>
<p>In the case of the T2SMT framework, we use a method known as the GHKM algorithm [23] to extract tree-to-string translation rules.The GHKM algorithm first splits the parse tree of the source sentence into several subtrees according to alignments, and extracts the pairs of the subtree and its corresponding words in the target sentence as "minimal rules."Next, the algorithm combines some minimal rules according to the original parse tree to generate larger rules.For example,  Fig. 6 shows an extraction of a translation rule corresponding to the phrase with wildcards "X is divisible by Y " by combining minimal rules in the bold rectangle, with two rules corresponding to "x" and "5" being replaced by wildcards respectively.</p>
<p>Extracted PBMT and T2SMT rules are evaluated using some measures and stored into the rule table for each framework along with these evaluation scores.These scores are used to calculate feature functions that are used in the calculation of probabilities Pr(t|s) or Pr(t|T s ).For example, the frequency of the rule in the parallel corpus, length of the phrase, and the complexity of the subtree are often used as features.</p>
<p>E. Language Model</p>
<p>Another important feature function of SMT is the "language model," which evaluates the fluency of the sentence in the target language.Given the target sentence t, the language model is defined as the product of probabilities of each word in t given the previous words:
Pr(t) ≡ |t| i=1 Pr(t i |t 1 , t 2 , • • • , t i−1 )(10)≡ |t| i=1 Pr(t i |t i−1 1 ),(11)
where the notation
x j i = [x i , x i+1 , • • • , x j ]
represents the subsequence of x containing the i-th to j-th elements.In addition, to save memory and prevent data sparseness, most practical language models use "n-gram models:"
Pr(t i |t i−1 1 ) Pr(t i |t i−1 i−n+1 ),(12)
where an n-gram is defined as n consecutive words.This approximation means that the next word t i is conditioned on only on the previous (n−1) words.The simplest n-gram model can be calculated simply from target language text using the count of appearances of each n-gram:
Pr(t i |t i−1 i−n+1 ) ≡ count(t i i−n+1 ) count(t i−1 i−n+1 )(13)
where count(x) is the number of appearances of sequence x in the given corpus.For example, if the word (1-gram) "is" appears 10 times and the 2-gram "is divisible" appears 3 times in the same corpus, then we can estimate Pr(t i = "divisible"|t i−1 = "is") = 3/10.In addition, we also use a further approximation method known as Kneser-Ney smoothing [24], which smooths probabilities for all n-grams, preventing problems due to data sparsity, allowing us to calculate the probability of any sentence accurately.</p>
<p>It should also be noted that n-gram models are easily applied to any type of sequence data and frequently used for software engineering, typically to measure the naturalness of the source code [25], [26] or distinguish the characteristics of source code [27], [28].</p>
<p>IV. APPLYING SMT TO PSEUDO-CODE GENERATION</p>
<p>In the previous section, we described two SMT frameworks: PBMT and T2SMT.The important advantage of using these frameworks is that we need not describe new translation rules explicitly when we update the pseudo-code generator, because the SMT framework is a statistical approach and translation rules from programming language to natural language can be automatically obtained from training data.This fact greatly reduces the burden on engineers to create or maintain pseudo-code generators.If we want to cover a new case in our generator, we simply search for or create pseudo-code corresponding to the desired statements, instead of creating specialized rules for each new case.For example, if we want to create a rule "if X is an even number" for the source code "if X % 2 == 0:" instead of "if X is divisible by 2," we only need to append a sentence including this example to our corpus, for example, "if something is an even number" and "if something % 2 == 0:."This work is obviously easier than describing rules explicitly, as this sort of data can be created by programmers even if they are not familiar with our particular pseudo-code generating system, and could also potentially be harvested from existing data.</p>
<p>If we want to construct the SMT-based pseudo-code generator described in this paper for any programming language/natural language pair, we must prepare the following data and tools.</p>
<p>• Source code/pseudo-code parallel corpus to train the SMT-based pseudo-code generator.</p>
<p>• Tokenizer of the target natural language.If we consider a language that puts spaces between words (e.g.English), we can use a simpler rule-based tokenization method such as the Stanford Tokenizer3 .If we are targeting a language with no spaces (e.g.Japanese), we must explicitly insert delimiters between each word in the sentence.Fortunately, tokenizing natural language is a well-developed area of NLP studies, so we can find tokenizers for major languages easily.</p>
<p>• Tokenizer of the source programming language.Usually, we can obtain a concrete definition of the programming language from its grammar, and a tokenizer module is often provided as a part of the standard library of the language itself.For example, Python provides tokenizing methods and symbol definitions in the tokenize module.</p>
<p>• Parser of the source programming language.Similarly to the tokenizer, a parser for the programming language, which converts source code into a parse tree describing the structure of the code, is explicitly defined in the language's grammar.Python also provides a way of performing abstract parsing in the ast module.However, the output of this module cannot be used directly, as described in the next section.</p>
<p>A. Surface Modification of Abstract Syntax Tree</p>
<p>As with the Python library ast, parsing methods provided by standard libraries of programming languages are often "abstract" parsers, which are defined more by the executiontime semantics of the language than its written form.As we mentioned in the previous section, the GHKM heuristics, which extract tree-to-string translation rules, use word alignments, which are defined over the leaves of the syntax tree.Abstract syntax trees often use keywords and operators existing in the source code as internal nodes rather than leaves, but these surface words could be strongly related to specific words in the target language (e.g. the token "if" in Python corresponds to the word "if" in English).</p>
<p>Of course, we could always develop a new parser from grammar definitions for specific programming languages, but this is not always easy (e.g.C++ grammar is defined using hundreds of pages in the official specification document).In this paper, we apply a more reasonable way to generate a parse-like tree from an abstract syntax tree using two processes described below.</p>
<p>1) Head Insertion: First, we append a new edge called "HEAD" into the abstract syntax tree, which include the label of inner nodes as their leaves.Fig. 7 shows an example of the head insertion process for a tree generated from the source code "if x % 5 == 0:" using the ast module in Python.Applying this process, words that have disappeared in the process of making the abstract syntax tree such as "if" can be treated as candidates for word alignment.This process is simple and easy to be applied to abstract syntax trees of any programming language if we can generate their trees using an abstract parser.On the other hand, there is one option of the head insertion process: where to position the HEAD nodes in the tree.In this paper, we put all HEAD nodes at the leftmost child of the parent for English and rightmost for Japanese, corresponding to the order of the head word in the target language.</p>
<p>and Simplification: In trees the number of their leaves after head insertion significantly than the number of words in the target and this cause noise in word alignment.this, we apply some pruning and simplification heuristics to reduce the complexity of trees.We developed 20 hand-written rules to prune and simplify the head-inserted tree by removing nodes that don't seem related to the surface form of the language.Fig. 8 shows an example after this process was applied to the result of Fig. 7.</p>
<p>In our method for pseudo-code generation, only these transformation methods require human engineering.However, developing these methods is easier than developing a rulebased system, because they are relatively simple and largely independent of the language of the pseudo-code.</p>
<p>B. Training Process of Pseudo-code Generator</p>
<p>Up until this section, we described details of each part of our pseudo-code generation system.Fig. 9 shows the whole process of our pseudo-code generator, and Fig. 10 shows the training process of the PBMT and T2SMT frameworks.In this paper, we compare 4 types of methods for pseudo-code generation.PBMT is a method directly using the tokens of programming and natural languages in the PBMT framework.Raw-T2SMT is a T2SMTbased method trained by raw abstract syntax trees of the programming language without the modifications described in the previous section.Head-T2SMT is also a T2SMT-based method trained using modified trees with only the head insertion process.Reduced-T2SMT is the last T2SMT-based method, using the head insertion, pruning, and simplification processes.</p>
<p>The algorithms for training SMT systems from a parallel corpus are too complex to develop ourselves.Fortunately, we can use open-source tools to assist in constructing the system.We use following tools in this study: MeCab to tokenize Japanese sentences [29], pialign to train word alignments [22], KenLM to train Kneser-Ney smoothed language model [30], Moses to train and generate target sentences using the PBMT model [31], and Travatar to train and generate target sentences using T2SMT models [32].</p>
<p>V. GATHERING A SOURCE CODE TO PSEUDO-CODE PARALLEL CORPUS</p>
<p>As we described in §III, we need a source code/pseudocode parallel corpus to train SMT-based pseudo-code generators.In this study, we created Python-to-English and Pythonto-Japanese parallel corpora by hiring programmers to add pseudo-code to existing code.</p>
<p>For the Python-to-English corpus, we contracted one engineer to create pseudo-code for Django (a Web application framework), and obtained a corpus containing 18,805 pairs of Python statements and corresponding English pseudo-code.For Python-to-Japanese, we first hired one engineer to create Python code for solutions to problems from Project Euler4 .a site with arithmetic problems designed for programming practice.We obtained 722 Python statements, which include 177 function definitions related to solving arithmetic problems.These are used to perform human evaluation experiments described in §VI-B and §VI-C.This code is shown to another Japanese engineer, who created Japanese pseudo-code corresponding to each statement.It should be noted that in both cases, this pseudo-code was created by a separate engineer not involved with this work, demonstrating that no special knowledge of the proposed system is required to create training data for it.</p>
<p>Next, we divided this data into separate sets for our experiments.We split the Python-to-English corpus into 3 parts that include 16,000, 1,000, and 1,805 statements.The 16,000 element set is the "training" data used to extract SMT rules and train the language model.The next 1,000 is the "development" data used to optimize the weight vector w of the log-linear model.The last 1,805 is the "test" data used to evaluate the trained SMT models.The Python-to-Japanese corpus is smaller than the Python-to-English corpus, so we perform 10-fold cross-validation, in which we use 90% of the data as training data, no development data (w is set as the default of each SMT framework), and 10% as test data.</p>
<p>VI. EVALUATING PSEUDO-CODE GENERATION</p>
<p>Once we have generated pseudo-code, we would like to evaluate its accuracy and usefulness.To do so, we use an automatic measure of translation accuracy adopted from the SMT literature, a manual evaluation of the accuracy of code generation, and a measure of how much the pseudo-code can contribute to code understanding.As described in the following sections, we calculate automatic evaluation measures for English and Japanese pseudo-code and manual accuracy and code understanding for Japanese pseudo-code.</p>
<p>A. Automatic Evaluation -BLEU</p>
<p>First, to automatically measure the accuracy of pseudocode generation, we use BLEU (Bilingual Evaluation Understudy) [33], an automatic evaluation measure of the generated translations widely used in machine translation studies.BLEU automatically calculates the similarity of generated translations and human-created reference translations.BLEU is defined as the product of "n-gram precision" and a "brevity penalty."ngram precision measures the ratio of length n word sequences generated by the system that are also created in the human reference, and the brevity penalty is a penalty that prevents the system from creating overly short hypotheses (that may have higher n-gram precision).BLEU gives a specific real value with range [0,1] and is usually expressed as a percentage.If the translation results are completely equal to the references, the BLEU score becomes 1.</p>
<p>In this study, we evaluated the quality of generated English and Japanese pseudo-code using BLEU, using the humandescribed pseudo-code of each statement as a reference.</p>
<p>B. Human Evaluation (1) -Acceptability</p>
<p>BLEU can automatically and quickly calculate the accuracy of generated translations based on references.However, it does Grammatically incorrect, and easy to understand C (2) Grammatically incorrect, and difficult to understand E (1) Not understandable or some important words are lacking not entirely guarantee that each translation result is semantically correct for the source sentence.To more accurately measure the quality of the translations, we also perform an evaluation using human annotators according to the acceptability criterion [34] shown in TABLE I. We employed 5 Japanese expert Python programmers to evaluate statementwise acceptabilities of each generated Japanese pseudo-code.</p>
<p>C. Human Evaluation (2) -Code Understanding</p>
<p>Especially for beginner programmers, pseudo-code may aid comprehension of the corresponding source code.To examine this effect, we perform a human evaluation of code understanding using experienced and unexperienced Japanese Python programmers through a Web interface.</p>
<p>First, we show a sample of a function definition and corresponding pseudo-code as in Fig. 1 to unexperienced programmers, who read the code.These programmers then assign a 6-level score indicating their impression of how well they understood the code for each sample.This impression is similar to Likert scale described in TABLE II.The evaluation interface records these scores and elapsed time from proposing a sample to the programmer submitting the score.This elapsed time can be assumed to be the time required by the programmer to understand the sample.Next, we have the programmers describe the behavior of functions they read in their mother tongue.We obtained these results from 14 students, in which 6 students had less than 1 year of Python experience (including no experience), perform this experiment task.</p>
<p>We use 117 function definitions in the Python-to-Japanese corpus that were split into 3 settings randomly (each split was different for each subject), which were respectively shown with different varieties of pseudo-code:</p>
<p>• Code setting that shows only source code itself, with no pseudo-code.</p>
<p>• Reference setting that shows human-created pseudocode (i.e.training data of the pseudo-code generator).</p>
<p>• Automated setting that shows code automatically generated by our Reduced-T2SMT method.</p>
<p>VII. EXPERIMENTAL RESULTS AND DISCUSSION</p>
<p>First, TABLE III shows the BLEU scores for each of the proposed methods for Python-to-English and Python-to-Japanese datasets, and the mean acceptability score of each method for the Python-to-Japanese dataset.</p>
<p>From these results, we can see that the BLEU scores are relatively high (except for PBMT on the Python-to-English data set), suggesting that the proposed method is generating relatively accurate results for both data sets.For reference, a current state-of-the-art SMT system achieves a BLEU score of about 48 in the relatively easy French-to-English pair [35], suggesting that translation from source code to pseudo-code is easier than translating between natural languages.This result can be expected, because SMT systems targeting natural language pairs always include the noise caused by ambiguities of tokenization or parsing for source natural language sentences, while our pseudo-code generator has no such input ambiguity.</p>
<p>In addition, we can see that BLEU scores for the Pythonto-Japanese dataset are higher than the Python-to-English dataset.This can be attributed to the characteristics of each dataset.The Python-to-Japanese dataset's original source code was generated by one engineer from arithmetic problems for programming practice, so all the input code in the set shares a similar programming style.In contrast, the source code of the Python-to-English dataset is extracted from Django, which is developing many engineers for a variety of objectives, and thus there is larger variance.The results of PBMT and Head-T2SMT for Python-to-English reflect these characteristics.As we mentioned in §III-B, PBMT-based pseudo-code generators cannot adequately handle grammatically complicated sentences, likely the reason for their reduced accuracy.Each T2SMT system achieves significantly higher BLEU than the PBMT system, indicating that properly analyzing the program structure before generating comments is essential to accurately generating pseudo-code.</p>
<p>We can also note that the Head-T2SMT system has a lower score than Raw-T2SMT for the Python-to-English dataset.This result is likely caused by the variance of human-created pseudo-code and word alignment.The head insertion process introduces many new tokens into the syntax tree and some of these tokens have no information to express the relationship between programming and natural language, causing problems in automatic word alignment.However, this problem did not rise to the surface for the Python-to-Japanese dataset, because it has less variance in the data.The Reduced-T2SMT system achieves the highest BLEU score for all settings in both languages, indicating that this method can avoid this problem by reducing redundant structures in head-inserted syntax trees.</p>
<p>To evaluate statistical significance, we performed a pairwise bootstrap test [36] for 10,000 sets of evaluation sentences randomly extracted from these results.Based on this we obtained a statistical significance under p &lt; 0.001 for all T2SMT systems against the PBMT system, and p &lt; 0.001 for the Reduced-T2SMT system against all other systems.We can also see in TABLE III that the acceptability scores of the pseudo-code for the Python-to-Japanese dataset is also improved in correlation to the BLEU improvement.Especially, Reduced-T2SMT achieves a mean acceptability over 4, which means that a large amount of the pseudo-code generated by the most advanced method Reduced-T2SMT was judged as grammatically correct by evaluators.Fig. 11 shows the acceptability distribution of each system.We can see that all systems can generate grammatically correct and fluent pseudo-code for 50% of the statements.Further, we can see that each T2SMT-based system generates less pseudocode with "intermediate acceptability."This is an interesting result, but it is intuitively understandable that T2SMT-based systems can generate accurate pseudo-code if their rule tables cover the input statements, because these systems explicitly use the grammatical information through syntax trees of the programming language.</p>
<p>Finally, TABLE IV shows the results of the code understanding experiment.In this table, we show 3 results calculated by different evaluator groups.The Experienced group includes 8 of 14 evaluators who have more than 1 year of experience  in programming Python, The Inexperienced group includes the remaining 6 evaluators (with less than 1 year of experience of Python), and All includes all evaluators.From the results, we can see that the result of the Reference setting achieves the highest understandability and the fastest reading time of all settings.This means that proposing correct pseudo-code improves the ease and efficiency of code reading when readers try to read the whole source code in detail.The result of the Automated setting also achieves a better impression than that of the Code setting.However, the reading time becomes longer than other settings.We assume that this is the result of the few strange lines of pseudo-code generated from our generator (e.g.pseudo-code scored 1 in acceptability) that confuse the readers in their attempt to interpret source code.Reducing generation errors in the Automated method can reduce this time-loss, in principle, similarly to the results of Reference.</p>
<p>Fig. 12 shows 3 sets of examples from the proposed methods.We can see that each T2SMT-based systems (especially Reduced-T2SMT) generates more accurate sentences in English than the PBMT system.</p>
<p>VIII. CONCLUSION AND FUTURE WORK</p>
<p>In this paper, we proposed a method for pseudo-code generation that is, to our knowledge, the first of its type.Our method is based on statistical machine translation (SMT) techniques, especially phrase-based machine translation (PBMT) and tree-to-string machine translation (T2SMT), which allow us to automatically learn statement-wise pseudo-code generators and require less human effort to create and update our generators.Experiments showed that our proposed methods generate grammatically correct pseudo-code for the Pythonto-English and Python-to-Japanese programming/natural language pairs, and distinguished that proposing pseudo-code with source code improves code comprehension of programmers for unfamiliar programming languages.To use SMT frameworks, we prepared a parallel corpus, which includes sentence (or syntax tree) pairs related to each other, and described several algorithms to adjust the parallel corpus to be in a format appropriate for SMT.</p>
<p>In the future work, we are planning to develop a pseudocode generator uses the proposed SMT framework to handle multiple statements, close to the standard setting of automatic comment generation.To do so, we must find or create a high-quality parallel corpus of source code and comments corresponding to multiple statements in the source code, which is a less well-formed problem than creating line-toline comments, and is thus an interesting challenge for future work.We also plan to investigate the use of automatic pseudocode generation by experienced programmers in large software project environments.For example, automatically generated pseudo-code could be used by programmers to confirm that the code that they wrote is actually doing what they expect it to be doing, or to help confirm when existing (single-line) comments in the source code have gone stale and need to be updated.</p>
<p>Fig. 1 .
1
Fig. 1.Example of source code written in Python and corresponding pseudo-code written in English.</p>
<p>Fig. 2 .
2
Fig. 2. Example of Python to English PBMT pseudo-code generation.</p>
<p>Fig. 3 .
3
Fig. 3. Example of Python to English T2SMT pseudo-code generation.</p>
<p>Fig. 4 .
4
Fig. 4. Word alignment between two token strings.</p>
<p>Fig. 5 .
5
Fig. 5. Extracting PBMT translation rules according to word alignment.</p>
<p>Fig. 6 .
6
Fig. 6.Extracting T2SMT translation rules according to word alignment.</p>
<p>Fig. 7 .
7
Fig. 7.The head insertion process.</p>
<p>Fig. 8 .
8
Fig. 8. Pruning and simplification process.</p>
<p>Fig. 9 .
9
Fig.9.Whole training process of each proposed method (a bold border indicates language-dependent processes).</p>
<p>Fig. 10 .
10
Fig. 10.Training process of PBMT and T2SMT frameworks.</p>
<p>Fig. 11 .
11
Fig. 11.Acceptability distribution of each system.</p>
<p>Fig. 12 .
12
Fig. 12. Examples of generated pseudo-code from each system.</p>
<p>TABLE I
I.DEFINITION OF ACCEPTABILITYLevelMeaningAA (5)Grammatically correct, and fluentA (4)Grammatically correct, and not fluentB (3)</p>
<p>TABLE II .
IIDEFINITION OF UNDERSTANDABILITYLevelMeaning5Very easy to understand4Easy to understand3Not either easy or difficult to understand2Difficult to understand1Very difficult to understand0Do not understand</p>
<p>TABLE III
III.BLEU% AND MEAN ACCEPTABILITIES OF EACHPSEUDO-CODE GENERATOR.Pseudo-codeBLEU%Mean AcceptabilityGenerator(English)(Japanese)(Japanese)PBMT25.7151.673.627Raw-T2SMT49.7455.663.812Head-T2SMT47.6959.414.039Reduced-T2SMT54.0862.884.155</p>
<p>TABLE IV
IV.MEAN UNDERSTANDABILITY IMPRESSIONS AND MEANTIME TO UNDERSTAND.GroupSettingMean ImpressionMean TimeCode2.5541.37ExperiencedReference3.0535.65Automated2.7146.48Code1.3224.99InexperiencedReference2.1024.97Automated1.8139.52Code1.9533.35AllReference2.6030.54Automated2.2843.15
While there are many varieties of pseudo-code, in this paper we assume that pseudo-code is "line-to-line" translation between programming and natural languages as shown by Fig.1. This assumption clearly defines the relationship between source code and pseudo-code and is a convenient first step towards applying machine translation to this task.
30th IEEE/ACM International Conference on Automated Software Engineering 978-1-5090-0025-8/15 $31.00 © 2015 IEEE DOI 10.1109/ASE.2015.36
Datasets used to construct and evaluate our pseudo-code generators are available at http://ahclab.naist.jp/pseudogen/
http://nlp.stanford.edu/software/tokenizer.shtml
https://projecteuler.net/
ACKNOWLEDGMENT Part of this work was supported by the Program for Advancing Strategic International Networks to Accelerate the Circulation of Talented Researchers.We also thank Mr. Akinori Ihara, who also gave us useful advice regarding this work.
Software development with code maps. R Deline, G Venolia, K Rowan, Commun. ACM. 5382010</p>
<p>Surfclipse: Context-aware meta search in the ide. M M Rahman, C K Roy, Proc. ICSME. ICSME2014</p>
<p>Theories, tools and research methods in program comprehension: Past, present and future. M.-A Storey, Software Quality Journal. 1432006</p>
<p>Towards automatically generating summary comments for java methods. G Sridhara, E Hill, D Muppaneni, L Pollock, K Vijay-Shanker, Proc. ASE. ASE2010</p>
<p>Automatically detecting and describing high level actions within methods. G Sridhara, L Pollock, K Vijay-Shanker, Proc. ICSE. ICSE2011</p>
<p>Automatic documentation inference for exceptions. R P Buse, W R Weimer, Proc. ISSTA. ISSTA2008</p>
<p>Automatic generation of natural language summaries for java classes. L Moreno, J Aponte, G Sridhara, A Marcus, L Pollock, K Vijay-Shanker, Proc. ICPC. ICPC2013</p>
<p>Autocomment: Mining question and answer sites for automatic comment generation. E Wong, J Yang, L Tan, Proc. ASE. ASE2013</p>
<p>On the use of automated text summarization techniques for summarizing source code. S Haiduc, J Aponte, L Moreno, A Marcus, Proc. WCRE. WCRE2010</p>
<p>Evaluating source code summarization techniques: Replication and expansion. B P Eddy, J A Robinson, N A Kraft, J C Carver, Proc. ICPC. ICPC2013</p>
<p>Improving automated source code summarization via an eye-tracking study of programmers. P Rodeghero, C Mcmillan, P W Mcburney, N Bosch, S D Mello, Proc. ICSE. ICSE2014</p>
<p>P Koehn, Statistical Machine Translation. Cambridge University Press2010</p>
<p>Statistical machine translation. A Lopez, ACM Computing Surveys. 403492008</p>
<p>The mathematics of statistical machine translation: Parameter estimation. P F Brown, V J D Pietra, S A D Pietra, R L Mercer, Computational Linguistics. 1921993</p>
<p>Statistical phrase-based translation. P Koehn, F J Och, D Marcu, Proc. NAACL-HLT. NAACL-HLT2003</p>
<p>Phrase-based statistical translation of programming languages. S Karaivanov, V Raychev, M Vechev, Proc. Onward!. Onward!2014</p>
<p>The alignment template approach to statistical machine translation. F J Och, H Ney, Computational Linguistics. 3042004</p>
<p>Statistical syntax-directed translation with extended domain of locality. L Huang, K Knight, A Joshi, Proc. AMTA. AMTA2006. 2006</p>
<p>Accurate unlexicalized parsing. D Klein, C D Manning, Proc. ACL. ACL2003</p>
<p>Learning accurate, compact, and interpretable tree annotation. S Petrov, L Barrett, R Thibaux, D Klein, Proceedings of COLING-ACL. COLING-ACL2006</p>
<p>The mathematics of statistical machine translation: Parameter estimation. P F Brown, V J D Pietra, S A D Pietra, R L Mercer, Computational Linguistics. 192Jun. 1993</p>
<p>An unsupervised model for joint phrase alignment and extraction. G Neubig, T Watanabe, E Sumita, S Mori, T Kawahara, Proc. ACL-HLT. ACL-HLTPortland, Oregon, USA20116</p>
<p>What's in a translation rule. M Galley, M Hopkins, K Knight, D Marcu, Proc. NAACL-HLT. NAACL-HLT2004</p>
<p>Improved backing-off for m-gram language modeling. R Kneser, H Ney, Proc. ICASSP. ICASSP1995</p>
<p>On the naturalness of software. A Hindle, E T Barr, Z Su, M Gabel, P Devanbu, Proc. ICSE. ICSE2012</p>
<p>A statistical semantic language model for source code. T T Nguyen, A T Nguyen, H A Nguyen, T N Nguyen, Proc. FSE. FSE2013</p>
<p>On the localness of software. Z Tu, Z Su, P Devanbu, Proc. FSE. FSE2014</p>
<p>Graph-based statistical language model for code. A T Nguyen, T N Nguyen, Proc. ICSE. ICSE2015</p>
<p>Applying conditional random fields to Japanese morphological analysis. T Kudo, K Yamamoto, Y Matsumoto, Proc. EMNLP. EMNLP20044</p>
<p>Scalable modified Kneser-Ney language model estimation. K Heafield, I Pouzyrevsky, J H Clark, P Koehn, Proc. ACL. ACLSofia, BulgariaAugust 2013</p>
<p>Moses: Open source toolkit for statistical machine translation. P Koehn, H Hoang, A Birch, C Callison-Burch, M Federico, N Bertoldi, B Cowan, W Shen, C Moran, R Zens, C Dyer, O Bojar, A Constantin, E Herbst, Proc. ACL. ACL2007</p>
<p>Travatar: A forest-to-string machine translation engine based on tree transducers. G Neubig, Proc. ACL. ACLSofia, BulgariaAugust 2013</p>
<p>Bleu: A method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proc. ACL. ACL2002</p>
<p>Overview of the patent machine translation task at the ntcir-10 workshop. I Goto, K P Chow, B Lu, E Sumita, B K Tsou, NTCIR-102013</p>
<p>Findings of the 2014 workshop on statistical machine translation. O Bojar, C Buck, C Federmann, B Haddow, P Koehn, J Leveling, C Monz, P Pecina, M Post, H Saint-Amand, R Soricut, L Specia, A Tamchyna, Proc. WMT. WMT2014</p>
<p>Statistical significance tests for machine translation evaluation. P Koehn, Proc. EMNLP. EMNLP2004</p>            </div>
        </div>

    </div>
</body>
</html>
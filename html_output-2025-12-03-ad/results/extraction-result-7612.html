<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7612 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7612</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7612</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-fd80f7f3673fc6ca02f192d5d73426f11a4be659</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fd80f7f3673fc6ca02f192d5d73426f11a4be659" target="_blank">The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Machine Translation</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations, and finds that it improves performance compared to just prompting for scores.</p>
                <p><strong>Paper Abstract:</strong> Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7612.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7612.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GEMBA-SQM score prompt (zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GEMBA-SQM score prediction prompt (0-100 numeric score)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A natural-language prompt (from Kocmi & Federmann) that asks an LLM to output a continuous quality score from 0–100 for a candidate translation (optionally including source and reference). Used zero-shot as a direct score-prediction format.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models are state-of-the-art evaluators of translation quality</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 UNICORN (as evaluated in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only large pre-trained autoregressive Transformer (PaLM-2 family) trained with large multilingual mixture; instruction-tuned variants exist (UNICORN).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>large (exact parameter count not publicly available; PaLM-2 family; compared also to PaLM 540B)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation evaluation — score prediction (WMT metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given source/reference/candidate, produce a scalar quality score representing translation quality on a 0–100 scale.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language score prompt (GEMBA-SQM) requesting a numeric 0–100 score</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot single-template prompt (GEMBA-SQM). No chain-of-thought or explicit rationale requested. Models were asked to produce a numeric score 0–100. Both reference-based and reference-less variants evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>System-level accuracy; segment-level Pearson's rho and pairwise accuracy (acc*)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>System-level accuracy 90.1%; segment-level Pearson rho = 0.401; segment acc* = 56.3% (PaLM-2 UNICORN, reference-based, zero-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>MetricX-XXL (learned metric): system accuracy 85.0%; segment Pearson rho = 0.549</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>System-level: +5.1 percentage points absolute vs MetricX-XXL; Segment-level Pearson: -0.148 absolute vs MetricX-XXL (i.e., worse at segment-level despite higher system-level accuracy)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot prompting with GEMBA-SQM template; reference-based; standard decoding (temperature not reported); compare across PaLM, PaLM-2, and GPT GEMBA</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7612.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7612.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>In-context learning (ICL) for score prediction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot / in-context learning sampling (uniform vs stratified) for score-prediction prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adding a small number (k) of labeled examples inside the prompt (ICL) when asking the LLM to predict numeric quality scores; the paper studies uniform vs stratified sampling of in-context examples and the effect on evaluator performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 BISON (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family mid-sized model used to test few-shot prompting behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mid-to-large (exacts not published in paper for BISON)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation evaluation — score prediction with few-shot context</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same score-prediction task but the prompt contains k in-context labeled examples sampled from a pool (uniform or stratified).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot natural-language prompt (0–100 numeric output) with k in-context examples; sampling strategies: uniform and stratified (bucketed by score ranges); 100 example-sets per k used to measure variability.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / few-shot configuration</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>k varied (1..N), stratified vs uniform sampling, 100 different sampled example-sets per k to measure mean and IQR; examples chosen from a pool of human-annotated DA/MQM assessments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Pearson's rho (primary reported), pairwise acc*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Including in-context examples almost never improved mean Pearson; performance often unchanged or worse compared to zero-shot prompting (paper reports 'almost never led to better performance' across samplings).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Zero-shot GEMBA-SQM prompting (same model) — e.g., PaLM-2 BISON zero-shot Pearson rho = 0.394 (reference-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>No consistent improvement; in many sampled sets performance decreased or showed high variance. Qualitatively: ICL biased predicted score distributions toward the scores present in the examples (e.g., introducing a single example with score 79 caused the model to start predicting 79 widely), implying an 'overfitting' to example scores rather than improved generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Stratified and uniform sampling; 100 example-sets per k; analyzed score-distribution shifts (Figure 6); both reference-based and reference-less variants studied.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7612.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7612.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Finetuning (Regression objective)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-2 finetuning with regression objective on human DA/MQM data</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Supervised finetuning of an LLM with a regression objective (MSE between model output logit and human DA/MQM scores) to improve segment-level correlation with human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 BISON (finetuned, regression)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 BISON finetuned with a regression head mapping model outputs to continuous quality scores using human DA/MQM training data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>mid-to-large PaLM-2 variant (exact parameter count not specified)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation evaluation — finetuned score prediction</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict continuous translation quality scores after supervised finetuning on WMT DA/MQM human judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Fine-tuning with regression loss; evaluated using the same score-prediction prompt or by mapping a real-valued logit to the score (training used an unused vocabulary token logit).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>training / objective</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Regression objective (MSE) on DA/MQM 2015–2020 data; regression implemented via real-valued logit from a fixed index of the first target token's logit vector; compared to generative-classification finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Pearson's rho; acc*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>PaLM-2 BISON (R, reference-based, finetuned): segment Pearson rho = 0.511; segment acc* = 61.0%; system-level accuracy = 88.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PaLM-2 BISON zero-shot prompting (reference-based): Pearson rho = 0.394; acc* = 56.8%; system-level accuracy = 88.7%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Segment-level Pearson improved by +0.117 absolute (0.394 -> 0.511); segment acc* improved by +4.2 percentage points (56.8% -> 61.0%); system-level accuracy essentially unchanged (slight -0.7 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Finetuned on DA/MQM training data (2015–2020); regression head technique as used for MetricX-XXL; both reference and reference-less finetuned variants evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7612.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7612.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction-tuning (FLAN) effect</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN instruction-tuning of PaLM-2 and impact on prompting-based evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned variant of PaLM-2 (FLAN-PaLM-2 UNICORN) was evaluated and found to perform worse in score-prediction tasks in this work, suggesting instruction-tuning dataset mismatches can degrade evaluator behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-PaLM-2 UNICORN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 variant instruction-tuned on FLAN-style mixture of tasks/data intended to improve instruction-following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>large (exact parameter count not published)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Machine translation evaluation — score prediction via prompting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompt the instruction-tuned model to output a numeric 0–100 quality score for translations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Zero-shot prompting using GEMBA-SQM on an instruction-tuned model</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model tuning / prompt interaction</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Instruction-tuned model (FLAN) evaluated with the same single-template score prompt; both reference-based and reference-less settings tested.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>System-level accuracy; segment-level Pearson's rho and acc*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>FLAN-PaLM-2 UNICORN (reference-based, zero-shot): system accuracy = 75.9%; segment Pearson rho = 0.197; acc* = 55.6% (poor performance relative to non-FLAN variant).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PaLM-2 UNICORN (non-FLAN, reference-based, zero-shot): system accuracy = 90.1%; segment Pearson rho = 0.401; acc* = 56.3%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>System-level accuracy decreased by -14.2 pp (90.1% -> 75.9%); segment Pearson rho decreased by -0.204 absolute (0.401 -> 0.197).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same GEMBA-SQM prompt used; indicates instruction-tuning (FLAN dataset) can degrade score-prediction evaluator quality for this task.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7612.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7612.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoMQM (few-shot error-span prompting) — UNICORN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoMQM: few-shot structured prompting asking for error spans and MQM-style categorization (PaLM-2 UNICORN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A chain-of-thought / structured prompt that instructs the LLM to identify translation error spans and classify them according to MQM; scores are derived algorithmically from predicted errors (no direct score generation). Requires few-shot examples to work well.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 UNICORN</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large PaLM-2 variant used for AutoMQM prompting; capable of producing structured span-and-category outputs when few-shot examples are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>large (exact count not published)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Translation evaluation with MQM-style error span identification (AutoMQM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given source/reference/candidate, produce error spans with MQM category/severity; compute MQM score by applying MQM weighting to predicted errors.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot structured prompt asking for error spans + category + severity (AutoMQM template). This is a form of structured chain-of-thought / rationale prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / structured rationale</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot only (zero-shot produced uninformative outputs). In-context examples sampled with stratified sampling and rejection criteria to ensure balance (balance of major/minor errors, category diversity, length constraints). Performance scales with up to ~4 in-context examples and then plateaus; less variance across example-sets compared to score-prediction ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Pearson's rho; acc*; span-level metrics: Span Precision (SP), Major Recall (MR), Matthews Correlation Coefficient (MCC)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AutoMQM (PaLM-2 UNICORN, reference-based): system-level accuracy = 87.6%; segment Pearson rho = 0.432 (vs score-prediction rho 0.401); segment acc* = 59.1% (vs 56.3%). Span metrics (EN-DE): SP = 0.175, MR = 0.628, MCC = 0.193 (AutoMQM identifies >50% of major-error words).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Score-prediction prompting (same model, UNICORN): segment Pearson rho = 0.401; acc* = 56.3%. Learned word-level evaluator COMET-WL: SP = 0.267, MR = 0.250, MCC = 0.161 (COMET-WL more precise but lower MR).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Segment Pearson +0.031 absolute (0.401 -> 0.432); acc* +2.8 pp (56.3% -> 59.1%); major recall much higher than learned word-level baseline (MR 0.628 vs COMET-WL MR 0.250), but lower span precision.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>AutoMQM uses stratified in-context sampling with rejection criteria (see Appendix D). Evaluated both reference-based and reference-less; best-performing in-context sets chosen for reported metrics. Structured output parsed into spans and MQM score computed algorithmically (major = -5, minor = -1-ish per MQM weighting table).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7612.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7612.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoMQM (few-shot error-span prompting) — BISON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoMQM: few-shot MQM-style prompting (PaLM-2 BISON smaller variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same AutoMQM structured few-shot prompting applied to the smaller PaLM-2 BISON model; shows that benefits of AutoMQM are scale-dependent (smaller models benefit less or may degrade).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 BISON</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller PaLM-2 variant used to evaluate scaling effects of AutoMQM prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>smaller PaLM-2 variant (S/M/BISON; exact count not published)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Translation evaluation with MQM-style error span identification (AutoMQM)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce MQM-style error spans and categories; derive scores from predicted spans.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot structured AutoMQM prompt (same template as for UNICORN), requiring in-context MQM annotated examples.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / structured rationale</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Few-shot; same stratified sampling and rejection criteria. Performance scales with number of in-context examples but improvements are smaller and less consistent than for larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Pearson rho; span precision (SP); major recall (MR); MCC</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AutoMQM (PaLM-2 BISON, reference-based): segment Pearson rho = 0.369 (score-prediction rho = 0.394 for BISON), system-level accuracy = 84.0%. Span metrics (EN-DE): SP = 0.095, MR = 0.749, MCC = 0.060 (very high major recall but low precision).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>PaLM-2 BISON score prediction (reference-based): Pearson rho = 0.394; acc* = 56.8%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Segment Pearson decreased by -0.025 absolute (0.394 -> 0.369) for BISON when switching to AutoMQM; however major recall increased (MR 0.749) while span precision remained low.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Few-shot AutoMQM; in-context pool derived from WMT21 MQM annotations; stratified sampling with rejection criteria; performance compared across model scales.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7612.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7612.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reference-based vs reference-less prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of including the reference translation versus not including it in prompts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper compares reference-based and reference-less (quality estimation) prompting; inclusion of a human reference in the prompt often helps segment-level correlation but effect size varies across models and families.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-2 UNICORN (representative)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM-2 family evaluated in both reference-based (source + reference + candidate) and reference-less (source + candidate) prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>large (exact parameter counts not provided)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Translation evaluation — reference-based vs reference-less prompting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Evaluate whether providing a human reference in the prompt improves the LLM's ability to score/rank translations.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Prompt variants: reference-based (source + reference + candidate) vs reference-less (source + candidate). Same GEMBA-SQM or AutoMQM templates applied to both variants.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>input modality / prompt content</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Both variants tested for score-prediction and AutoMQM; results reported separately in tables with a Ref? column.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Segment-level Pearson rho; acc*</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Example: PaLM-2 UNICORN score-prediction: reference-based rho = 0.401 vs reference-less rho = 0.275 (substantial drop). For PaLM-2 BISON the drop is smaller (rho 0.394 -> 0.355).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Reference-based prompting (same model) used as baseline for comparison to reference-less variant.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>PaLM-2 UNICORN: segment Pearson decreased by -0.126 absolute when reference was removed (0.401 -> 0.275); effect size varies by model/size.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Both GEMBA-SQM score prompt and AutoMQM evaluated with and without including the human reference; statistical aggregation across language pairs performed.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7612.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7612.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Output discretization phenomenon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Limited discrete score outputs despite continuous-range prompt (0–100)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When prompted to output a continuous 0–100 score, LLMs often output a very limited set of canonical scores (e.g., 0, 50, 90, 95), which reduces effective granularity and harms segment-level correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-540B and PaLM-2 family (observed across models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs trained with next-token prediction; pretraining objective biases discrete frequent tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>examples include PaLM 540B and PaLM-2 variants</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Score-prediction prompting (0–100 numeric outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Prompted to return a 0–100 numeric score; analyze distribution of outputs across test set.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language numeric scoring prompt requesting continuous range output</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt output behavior</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Zero-shot GEMBA-SQM prompt expecting continuous numeric output; examined histograms of outputs (log-scale) over test set.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Distributional analysis (qualitative); segment-level Pearson (impact)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Models nearly always output small set of scores (e.g., 0, 50, 90, 95) rather than fine-grained spread; example: PaLM/PaLM-2 distributions concentrated on those scores (Figure 3 and Figure 13).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Ideal: continuous, smoothly distributed scores matching human variability; learned metrics show finer-grained outputs and higher segment Pearson.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Effectively reduces discriminative granularity and can harm segment-level correlation (observed coarse outputs despite 0–100 prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Zero-shot numeric prompt; histogram/log-scale plots used to show discreteness; discussed as consequence of pretraining objective.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Large language models are state-of-the-art evaluators of translation quality <em>(Rating: 2)</em></li>
                <li>Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Instructscore: Towards explainable text generation evaluation with automatic feedback <em>(Rating: 2)</em></li>
                <li>MaTESe: Machine translation evaluation as a sequence tagging problem <em>(Rating: 1)</em></li>
                <li>G-Eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7612",
    "paper_id": "paper-fd80f7f3673fc6ca02f192d5d73426f11a4be659",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "GEMBA-SQM score prompt (zero-shot)",
            "name_full": "GEMBA-SQM score prediction prompt (0-100 numeric score)",
            "brief_description": "A natural-language prompt (from Kocmi & Federmann) that asks an LLM to output a continuous quality score from 0–100 for a candidate translation (optionally including source and reference). Used zero-shot as a direct score-prediction format.",
            "citation_title": "Large language models are state-of-the-art evaluators of translation quality",
            "mention_or_use": "use",
            "model_name": "PaLM-2 UNICORN (as evaluated in paper)",
            "model_description": "Decoder-only large pre-trained autoregressive Transformer (PaLM-2 family) trained with large multilingual mixture; instruction-tuned variants exist (UNICORN).",
            "model_size": "large (exact parameter count not publicly available; PaLM-2 family; compared also to PaLM 540B)",
            "task_name": "Machine translation evaluation — score prediction (WMT metrics)",
            "task_description": "Given source/reference/candidate, produce a scalar quality score representing translation quality on a 0–100 scale.",
            "problem_format": "Natural-language score prompt (GEMBA-SQM) requesting a numeric 0–100 score",
            "format_category": "prompt style",
            "format_details": "Zero-shot single-template prompt (GEMBA-SQM). No chain-of-thought or explicit rationale requested. Models were asked to produce a numeric score 0–100. Both reference-based and reference-less variants evaluated.",
            "performance_metric": "System-level accuracy; segment-level Pearson's rho and pairwise accuracy (acc*)",
            "performance_value": "System-level accuracy 90.1%; segment-level Pearson rho = 0.401; segment acc* = 56.3% (PaLM-2 UNICORN, reference-based, zero-shot)",
            "baseline_performance": "MetricX-XXL (learned metric): system accuracy 85.0%; segment Pearson rho = 0.549",
            "performance_change": "System-level: +5.1 percentage points absolute vs MetricX-XXL; Segment-level Pearson: -0.148 absolute vs MetricX-XXL (i.e., worse at segment-level despite higher system-level accuracy)",
            "experimental_setting": "Zero-shot prompting with GEMBA-SQM template; reference-based; standard decoding (temperature not reported); compare across PaLM, PaLM-2, and GPT GEMBA",
            "statistical_significance": null,
            "uuid": "e7612.0",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "In-context learning (ICL) for score prediction",
            "name_full": "Few-shot / in-context learning sampling (uniform vs stratified) for score-prediction prompt",
            "brief_description": "Adding a small number (k) of labeled examples inside the prompt (ICL) when asking the LLM to predict numeric quality scores; the paper studies uniform vs stratified sampling of in-context examples and the effect on evaluator performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 BISON (representative)",
            "model_description": "PaLM-2 family mid-sized model used to test few-shot prompting behavior.",
            "model_size": "mid-to-large (exacts not published in paper for BISON)",
            "task_name": "Machine translation evaluation — score prediction with few-shot context",
            "task_description": "Same score-prediction task but the prompt contains k in-context labeled examples sampled from a pool (uniform or stratified).",
            "problem_format": "Few-shot natural-language prompt (0–100 numeric output) with k in-context examples; sampling strategies: uniform and stratified (bucketed by score ranges); 100 example-sets per k used to measure variability.",
            "format_category": "prompt style / few-shot configuration",
            "format_details": "k varied (1..N), stratified vs uniform sampling, 100 different sampled example-sets per k to measure mean and IQR; examples chosen from a pool of human-annotated DA/MQM assessments.",
            "performance_metric": "Segment-level Pearson's rho (primary reported), pairwise acc*",
            "performance_value": "Including in-context examples almost never improved mean Pearson; performance often unchanged or worse compared to zero-shot prompting (paper reports 'almost never led to better performance' across samplings).",
            "baseline_performance": "Zero-shot GEMBA-SQM prompting (same model) — e.g., PaLM-2 BISON zero-shot Pearson rho = 0.394 (reference-based)",
            "performance_change": "No consistent improvement; in many sampled sets performance decreased or showed high variance. Qualitatively: ICL biased predicted score distributions toward the scores present in the examples (e.g., introducing a single example with score 79 caused the model to start predicting 79 widely), implying an 'overfitting' to example scores rather than improved generalization.",
            "experimental_setting": "Stratified and uniform sampling; 100 example-sets per k; analyzed score-distribution shifts (Figure 6); both reference-based and reference-less variants studied.",
            "statistical_significance": null,
            "uuid": "e7612.1",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Finetuning (Regression objective)",
            "name_full": "PaLM-2 finetuning with regression objective on human DA/MQM data",
            "brief_description": "Supervised finetuning of an LLM with a regression objective (MSE between model output logit and human DA/MQM scores) to improve segment-level correlation with human judgments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 BISON (finetuned, regression)",
            "model_description": "PaLM-2 BISON finetuned with a regression head mapping model outputs to continuous quality scores using human DA/MQM training data.",
            "model_size": "mid-to-large PaLM-2 variant (exact parameter count not specified)",
            "task_name": "Machine translation evaluation — finetuned score prediction",
            "task_description": "Predict continuous translation quality scores after supervised finetuning on WMT DA/MQM human judgments.",
            "problem_format": "Fine-tuning with regression loss; evaluated using the same score-prediction prompt or by mapping a real-valued logit to the score (training used an unused vocabulary token logit).",
            "format_category": "training / objective",
            "format_details": "Regression objective (MSE) on DA/MQM 2015–2020 data; regression implemented via real-valued logit from a fixed index of the first target token's logit vector; compared to generative-classification finetuning.",
            "performance_metric": "Segment-level Pearson's rho; acc*",
            "performance_value": "PaLM-2 BISON (R, reference-based, finetuned): segment Pearson rho = 0.511; segment acc* = 61.0%; system-level accuracy = 88.0%.",
            "baseline_performance": "PaLM-2 BISON zero-shot prompting (reference-based): Pearson rho = 0.394; acc* = 56.8%; system-level accuracy = 88.7%",
            "performance_change": "Segment-level Pearson improved by +0.117 absolute (0.394 -&gt; 0.511); segment acc* improved by +4.2 percentage points (56.8% -&gt; 61.0%); system-level accuracy essentially unchanged (slight -0.7 pp).",
            "experimental_setting": "Finetuned on DA/MQM training data (2015–2020); regression head technique as used for MetricX-XXL; both reference and reference-less finetuned variants evaluated.",
            "statistical_significance": null,
            "uuid": "e7612.2",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Instruction-tuning (FLAN) effect",
            "name_full": "FLAN instruction-tuning of PaLM-2 and impact on prompting-based evaluation",
            "brief_description": "An instruction-tuned variant of PaLM-2 (FLAN-PaLM-2 UNICORN) was evaluated and found to perform worse in score-prediction tasks in this work, suggesting instruction-tuning dataset mismatches can degrade evaluator behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "FLAN-PaLM-2 UNICORN",
            "model_description": "PaLM-2 variant instruction-tuned on FLAN-style mixture of tasks/data intended to improve instruction-following.",
            "model_size": "large (exact parameter count not published)",
            "task_name": "Machine translation evaluation — score prediction via prompting",
            "task_description": "Prompt the instruction-tuned model to output a numeric 0–100 quality score for translations.",
            "problem_format": "Zero-shot prompting using GEMBA-SQM on an instruction-tuned model",
            "format_category": "model tuning / prompt interaction",
            "format_details": "Instruction-tuned model (FLAN) evaluated with the same single-template score prompt; both reference-based and reference-less settings tested.",
            "performance_metric": "System-level accuracy; segment-level Pearson's rho and acc*",
            "performance_value": "FLAN-PaLM-2 UNICORN (reference-based, zero-shot): system accuracy = 75.9%; segment Pearson rho = 0.197; acc* = 55.6% (poor performance relative to non-FLAN variant).",
            "baseline_performance": "PaLM-2 UNICORN (non-FLAN, reference-based, zero-shot): system accuracy = 90.1%; segment Pearson rho = 0.401; acc* = 56.3%",
            "performance_change": "System-level accuracy decreased by -14.2 pp (90.1% -&gt; 75.9%); segment Pearson rho decreased by -0.204 absolute (0.401 -&gt; 0.197).",
            "experimental_setting": "Same GEMBA-SQM prompt used; indicates instruction-tuning (FLAN dataset) can degrade score-prediction evaluator quality for this task.",
            "statistical_significance": null,
            "uuid": "e7612.3",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AutoMQM (few-shot error-span prompting) — UNICORN",
            "name_full": "AutoMQM: few-shot structured prompting asking for error spans and MQM-style categorization (PaLM-2 UNICORN)",
            "brief_description": "A chain-of-thought / structured prompt that instructs the LLM to identify translation error spans and classify them according to MQM; scores are derived algorithmically from predicted errors (no direct score generation). Requires few-shot examples to work well.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 UNICORN",
            "model_description": "Large PaLM-2 variant used for AutoMQM prompting; capable of producing structured span-and-category outputs when few-shot examples are provided.",
            "model_size": "large (exact count not published)",
            "task_name": "Translation evaluation with MQM-style error span identification (AutoMQM)",
            "task_description": "Given source/reference/candidate, produce error spans with MQM category/severity; compute MQM score by applying MQM weighting to predicted errors.",
            "problem_format": "Few-shot structured prompt asking for error spans + category + severity (AutoMQM template). This is a form of structured chain-of-thought / rationale prompting.",
            "format_category": "prompt style / structured rationale",
            "format_details": "Few-shot only (zero-shot produced uninformative outputs). In-context examples sampled with stratified sampling and rejection criteria to ensure balance (balance of major/minor errors, category diversity, length constraints). Performance scales with up to ~4 in-context examples and then plateaus; less variance across example-sets compared to score-prediction ICL.",
            "performance_metric": "Segment-level Pearson's rho; acc*; span-level metrics: Span Precision (SP), Major Recall (MR), Matthews Correlation Coefficient (MCC)",
            "performance_value": "AutoMQM (PaLM-2 UNICORN, reference-based): system-level accuracy = 87.6%; segment Pearson rho = 0.432 (vs score-prediction rho 0.401); segment acc* = 59.1% (vs 56.3%). Span metrics (EN-DE): SP = 0.175, MR = 0.628, MCC = 0.193 (AutoMQM identifies &gt;50% of major-error words).",
            "baseline_performance": "Score-prediction prompting (same model, UNICORN): segment Pearson rho = 0.401; acc* = 56.3%. Learned word-level evaluator COMET-WL: SP = 0.267, MR = 0.250, MCC = 0.161 (COMET-WL more precise but lower MR).",
            "performance_change": "Segment Pearson +0.031 absolute (0.401 -&gt; 0.432); acc* +2.8 pp (56.3% -&gt; 59.1%); major recall much higher than learned word-level baseline (MR 0.628 vs COMET-WL MR 0.250), but lower span precision.",
            "experimental_setting": "AutoMQM uses stratified in-context sampling with rejection criteria (see Appendix D). Evaluated both reference-based and reference-less; best-performing in-context sets chosen for reported metrics. Structured output parsed into spans and MQM score computed algorithmically (major = -5, minor = -1-ish per MQM weighting table).",
            "statistical_significance": null,
            "uuid": "e7612.4",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "AutoMQM (few-shot error-span prompting) — BISON",
            "name_full": "AutoMQM: few-shot MQM-style prompting (PaLM-2 BISON smaller variant)",
            "brief_description": "Same AutoMQM structured few-shot prompting applied to the smaller PaLM-2 BISON model; shows that benefits of AutoMQM are scale-dependent (smaller models benefit less or may degrade).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 BISON",
            "model_description": "Smaller PaLM-2 variant used to evaluate scaling effects of AutoMQM prompting.",
            "model_size": "smaller PaLM-2 variant (S/M/BISON; exact count not published)",
            "task_name": "Translation evaluation with MQM-style error span identification (AutoMQM)",
            "task_description": "Produce MQM-style error spans and categories; derive scores from predicted spans.",
            "problem_format": "Few-shot structured AutoMQM prompt (same template as for UNICORN), requiring in-context MQM annotated examples.",
            "format_category": "prompt style / structured rationale",
            "format_details": "Few-shot; same stratified sampling and rejection criteria. Performance scales with number of in-context examples but improvements are smaller and less consistent than for larger models.",
            "performance_metric": "Segment-level Pearson rho; span precision (SP); major recall (MR); MCC",
            "performance_value": "AutoMQM (PaLM-2 BISON, reference-based): segment Pearson rho = 0.369 (score-prediction rho = 0.394 for BISON), system-level accuracy = 84.0%. Span metrics (EN-DE): SP = 0.095, MR = 0.749, MCC = 0.060 (very high major recall but low precision).",
            "baseline_performance": "PaLM-2 BISON score prediction (reference-based): Pearson rho = 0.394; acc* = 56.8%",
            "performance_change": "Segment Pearson decreased by -0.025 absolute (0.394 -&gt; 0.369) for BISON when switching to AutoMQM; however major recall increased (MR 0.749) while span precision remained low.",
            "experimental_setting": "Few-shot AutoMQM; in-context pool derived from WMT21 MQM annotations; stratified sampling with rejection criteria; performance compared across model scales.",
            "statistical_significance": null,
            "uuid": "e7612.5",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Reference-based vs reference-less prompting",
            "name_full": "Effect of including the reference translation versus not including it in prompts",
            "brief_description": "Paper compares reference-based and reference-less (quality estimation) prompting; inclusion of a human reference in the prompt often helps segment-level correlation but effect size varies across models and families.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-2 UNICORN (representative)",
            "model_description": "PaLM-2 family evaluated in both reference-based (source + reference + candidate) and reference-less (source + candidate) prompt formats.",
            "model_size": "large (exact parameter counts not provided)",
            "task_name": "Translation evaluation — reference-based vs reference-less prompting",
            "task_description": "Evaluate whether providing a human reference in the prompt improves the LLM's ability to score/rank translations.",
            "problem_format": "Prompt variants: reference-based (source + reference + candidate) vs reference-less (source + candidate). Same GEMBA-SQM or AutoMQM templates applied to both variants.",
            "format_category": "input modality / prompt content",
            "format_details": "Both variants tested for score-prediction and AutoMQM; results reported separately in tables with a Ref? column.",
            "performance_metric": "Segment-level Pearson rho; acc*",
            "performance_value": "Example: PaLM-2 UNICORN score-prediction: reference-based rho = 0.401 vs reference-less rho = 0.275 (substantial drop). For PaLM-2 BISON the drop is smaller (rho 0.394 -&gt; 0.355).",
            "baseline_performance": "Reference-based prompting (same model) used as baseline for comparison to reference-less variant.",
            "performance_change": "PaLM-2 UNICORN: segment Pearson decreased by -0.126 absolute when reference was removed (0.401 -&gt; 0.275); effect size varies by model/size.",
            "experimental_setting": "Both GEMBA-SQM score prompt and AutoMQM evaluated with and without including the human reference; statistical aggregation across language pairs performed.",
            "statistical_significance": null,
            "uuid": "e7612.6",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "Output discretization phenomenon",
            "name_full": "Limited discrete score outputs despite continuous-range prompt (0–100)",
            "brief_description": "When prompted to output a continuous 0–100 score, LLMs often output a very limited set of canonical scores (e.g., 0, 50, 90, 95), which reduces effective granularity and harms segment-level correlation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-540B and PaLM-2 family (observed across models)",
            "model_description": "Large autoregressive LLMs trained with next-token prediction; pretraining objective biases discrete frequent tokens.",
            "model_size": "examples include PaLM 540B and PaLM-2 variants",
            "task_name": "Score-prediction prompting (0–100 numeric outputs)",
            "task_description": "Prompted to return a 0–100 numeric score; analyze distribution of outputs across test set.",
            "problem_format": "Natural-language numeric scoring prompt requesting continuous range output",
            "format_category": "prompt output behavior",
            "format_details": "Zero-shot GEMBA-SQM prompt expecting continuous numeric output; examined histograms of outputs (log-scale) over test set.",
            "performance_metric": "Distributional analysis (qualitative); segment-level Pearson (impact)",
            "performance_value": "Models nearly always output small set of scores (e.g., 0, 50, 90, 95) rather than fine-grained spread; example: PaLM/PaLM-2 distributions concentrated on those scores (Figure 3 and Figure 13).",
            "baseline_performance": "Ideal: continuous, smoothly distributed scores matching human variability; learned metrics show finer-grained outputs and higher segment Pearson.",
            "performance_change": "Effectively reduces discriminative granularity and can harm segment-level correlation (observed coarse outputs despite 0–100 prompt).",
            "experimental_setting": "Zero-shot numeric prompt; histogram/log-scale plots used to show discreteness; discussed as consequence of pretraining objective.",
            "statistical_significance": null,
            "uuid": "e7612.7",
            "source_info": {
                "paper_title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Large language models are state-of-the-art evaluators of translation quality",
            "rating": 2
        },
        {
            "paper_title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt",
            "rating": 2
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Instructscore: Towards explainable text generation evaluation with automatic feedback",
            "rating": 2
        },
        {
            "paper_title": "MaTESe: Machine translation evaluation as a sequence tagging problem",
            "rating": 1
        },
        {
            "paper_title": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 1
        }
    ],
    "cost": 0.021484749999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation</h1>
<p>Patrick Fernandes ${ }^{\star, 2,3,4}$ Daniel Deutsch ${ }^{1}$ Mara Finkelstein ${ }^{1}$ Parker Riley ${ }^{1}$<br>André F. T. Martins ${ }^{3,4,5}$ Graham Neubig ${ }^{2,6}$<br>Ankush Garg ${ }^{1}$ Jonathan H. Clark ${ }^{1}$ Markus Freitag ${ }^{1}$ Orhan Firat ${ }^{1}$<br>${ }^{1}$ Google ${ }^{2}$ Carnegie Mellon University ${ }^{3}$ Instituto Superior Técnico<br>${ }^{4}$ Instituto de Telecomunicações ${ }^{5}$ Unbabel ${ }^{6}$ Inspired Cognition<br>pfernand@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through incontext learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.</p>
<h2>1 Introduction</h2>
<p>Evaluating natural language generation systems has always been challenging, and as the output quality of these systems has improved, evaluation has become even more challenging and critical. For example, in Machine Translation (MT), a field where evaluation has garnered considerable attention, previous standard automatic surface-level metrics such as BLEU (Papineni et al., 2002) are becoming less reliable as the quality of generation systems improves, with little remaining correlation with human judgments (Freitag et al., 2022).</p>
<p>To keep pace with the constantly improving quality of MT output, the next generation of automatic metrics is rapidly evolving. Learned automatic metrics that leverage human-judgments to finetune</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Illustration of how AutoMQM uses LLMs to assess the quality of a translation. Rather than asking for a single quality score, AutoMQM prompts models to identify and classify errors, and uses the MQM framework to produce a score.
language models (Sellam et al., 2020; Rei et al., 2022a) currently represent the state-of-the-art in automatic evaluation benchmarks like the WMT Metrics task (Freitag et al., 2022), and show high correlation with human judgments. However, these metrics typically output a single, uninterpretable quality score, making it difficult to understand the type and extent of errors identified by them. The lack of insights makes it difficult for model developers to leverage these metrics to improve their systems.</p>
<p>Unlike automatic metrics that only provide a single scalar value as quality score, state-of-the-art human evaluation methodologies like Multidimensional Quality Metrics (MQM; Lommel et al., 2014; Freitag et al., 2021a) ask professional annotators to identify and label error spans with a category and severity. This much richer feedback can be used to gain a better understanding of the current limitations of the model under evaluation and improve it.</p>
<p>In this paper, we ask whether large language</p>
<p>models (LLMs) in combination with a few human annotations can be used to design an automatic metric that generates rich feedback similar to that generated by human experts in MQM. This work is motivated by recent papers that demonstrated that LLMs can be used as automatic metrics (Liu et al., 2023b) to generate a single quality score. In particular, Kocmi and Federmann (2023) showed that LLMs can be prompted to assess the quality of machine-generated translations, even achieving state-of-the-art performance on assessing systemlevel quality. However, previous work only provides a limited view of the capabilities of LLMs for machine translation evaluation: the focus has predominantly been on score prediction (i.e. predicting a numerical value for quality), without considering the use of any annotated data (either through in-context learning or finetuning), and only in highresource language pairs.</p>
<p>We provide a large-scale study of the capabilities of LLMs (from the PaLM and PaLM-2 families; Chowdhery et al., 2022; Anil et al., 2023) for machine translation evaluation (both with and without a reference translation), provide a novel comparison between prompting and finetuning, and investigate the performance in the low-resource scenario. Inspired by findings that the performance of LLMs can be improved by prompting them for rationales of their predictions (Wei et al., 2022; Lu et al., 2023), we also propose AutoMQM, a prompting technique for MT evaluation that asks LLMs to identify error spans in a translation and to classify these errors according to the MQM framework, with a quality score derived automatically from the identified errors. A key advantage of AutoMQM is its interpretability, as users can inspect the errors responsible for a score (Figure 1).</p>
<p>Our contributions can be summarized as follows:</p>
<ul>
<li>We confirm the finding of Kocmi and Federmann (2023) that LLMs are zero-shot state-of-the-art system-level evaluators, but show low correlation with human judgment compared to learned metrics at the segment-level.</li>
<li>We show that finetuning an LLM with human judgment mitigates its low segment-level performance (particularly for smaller LLMs), showing similar correlations with human judgment at both the system-level and segmentlevel to state-of-the-art learned metrics.</li>
<li>We are the first to evaluate LLM-based evaluation methods on low-resource language pairs.</li>
</ul>
<p>We find that their performance is promising, but lags behind state-of-the-art learned metrics.</p>
<ul>
<li>We find that, with AutoMQM, PaLM-2 models can be prompted to generate rich MQMlike annotations, outperforming their score prediction counterparts at the segment-level.</li>
<li>Furthermore, annotations predicted by PaLM2 models correctly identify over $50 \%$ of words that are part of major errors, and are comparable to the ones produced by state-of-the-art supervised word-level evaluators.</li>
</ul>
<p>Our findings might have significant implications for not only MT evaluation, but evaluation of machine-generated text in general, and further highlight the potential of using LLMs to provide AI Feedback (Fernandes et al., 2023).</p>
<p>The outputs of our models prompted with AutoMQM are available at github.com/google-research/google-research</p>
<h2>2 Background: MT Evaluation</h2>
<p>Machine translation evaluation is one of the most well-studied evaluation problems in NLP (CallisonBurch et al., 2008; Freitag et al., 2022). In this task, given</p>
<ol>
<li>a source sentence in a (source) language</li>
<li>a candidate translation in a (target) language
an evaluation metric assesses the quality of the candidate translation by how well it conveys the meaning of the source sentence while considering other factors like fluency. Like many other natural language generation evaluation problems, this task is difficult because the set of correct translations for a given source sentence is often very large and not entirely known in advance. To simplify the problem of machine translation evaluation, often (3) a reference translation (typically created by a professional human translator) is included as additional information when assessing the candidate translation. This sub-problem is known as reference-based evaluation (as opposed referenceless evaluation or quality estimation).</li>
</ol>
<p>Up until recently, human evaluation of machine translation was carried out predominantly with the aim of assigning a single quality score to a candidate translation. Consequently, learned metrics, which leverage collected human judgment data, are trained for and evaluated on the same task of score</p>
<p>prediction (i.e., assigning a single quality score to a candidate translation), and can achieve high correlation with human-provided scores (Freitag et al., 2022).</p>
<p>However, framing machine translation evaluation as a score prediction task is problematic: any scoring or ranking of translations is implicitly based on an identification of errors in the candidate translations, and asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a).</p>
<p>This insight has led to the adoption of the Multidimensional Quality Metrics (MQM) framework (Lommel et al., 2014; Freitag et al., 2021a) as the gold standard for evaluating machine translation. The MQM framework asks human evaluators to identify error spans in candidate translations and classify those errors according to various dimensions, e.g., fluency, accuracy, ... (see Appendix A for a more detailed description of MQM). Importantly, the MQM framework does not ask annotators to provide a quality score for each translation, and instead derives one automatically from the identified error spans and their classifications. However, despite its richness, most automatic metrics that leverage MQM data only use the final quality score produced by the framework and discard the error span information and classification.</p>
<h2>3 Related Work</h2>
<p>The success of learned machine translation metrics (Sellam et al., 2020; Rei et al., 2022a; Freitag et al., 2022; Qin et al., 2022), which finetune neural network models pretrained on large amounts of (unsupervised) data, highlighted the importance of leveraging transfer learning to achieve metrics with better correlation with human judgments. More recently, generative LLMs (OpenAI, 2023; Anil et al., 2023) have consistently demonstrated impressive results in natural language understanding and zeroand few-shot transfer and, naturally, interest in employing these models for (translation) evaluation has increased. Kocmi and Federmann (2023) first explored the use of GPT models for evaluating machine translation tasks, showing their potential as zero-shot evaluators, and others have since extended GPT-based evaluation to other generation problems (Jain et al., 2023; Liu et al., 2023b).</p>
<p>Perrella et al. (2022) first highlighted that MQM annotations could be leveraged to allow pretrained models to predict major and minor errors and, sim-
ilarly to AutOMQM, used the identified errors to automatically score translations. However, their approach relied on weaker encoder-only or encoderdecoder language models, required supervised data to work, and overall underperformed other top metrics. We compare against their MaTASe metric in our experiments. Lu et al. (2023) showed that doing error analysis, a prompting technique similar to AUTOMQM, could lead to better ChatGPT-based evaluators. However, they still relied on the LLM to provide a score once it identified errors (rather than do it automatically using something like the MQM framework). Furthermore, they provided a very limited meta-evaluation using only 40 examples per language pair. Concurrently with our work, Xu et al. (2023) proposed InSTRUCTSCORE, a LLaMA-based evaluator that asks models to identify and categorize errors in translation (as well as providing a natural language explanation for each error). However, the authors only explore a 7B parameter model and don't leverage zero- and fewshot capabilities of models as in this work. Instead, they rely on a more complex approach of distilling the knowledge of a more capable GPT-4 LLM.</p>
<p>Additionally, WMT Word-Level Quality Estimation shared tasks (Fonseca et al., 2019; Zerva et al., 2022) leverage MQM data by converting span-level annotations of errors (normally of major severity) to word-level tags and Task 2 in the WMT19 Quality Estimation shared task evaluation explicitly evaluated submissions of span-level annotations (although most submissions still consisted of models that predicted word-level tags which were converted to spans). We also compare against state-of-the-art word-level quality estimation models.</p>
<h2>4 Using LLMs to Predict Quality Scores</h2>
<p>Recent works have shown that large language models are versatile, general-purpose models that can be used to tackle many problems in NLP, including evaluation (Kocmi and Federmann, 2023; Jain et al., 2023; Liu et al., 2023b). We begin by exploring how LLMs can be used for machine translation evaluation through score prediction.</p>
<h3>4.1 Prompting</h3>
<p>We start by measuring how far we can push the performance of LLMs with just prompting (Liu et al., 2023a): by defining the task of MT evaluation and quality estimation as textual templates (with</p>
<p>a general description of the problem and "slots" for the inputs and outputs), we can use generalpurpose LLMs to perform these tasks at inferencetime, without any parameter updates.</p>
<p>Throughout the paper, we choose to use Kocmi and Federmann (2023)'s GEMBA-SQM prompt (Figure 9, Appendix C), which asks models to generate (a string representation of) a score from 0 100. We choose this prompt for two reasons: firstly, early explorations with various prompts showed that this generally performed well. Secondly, using a single prompt ensures a fairer comparison between the capabilities of different models. ${ }^{1}$</p>
<p>In-Context Learning A surprising emergent capability of LLMs is their ability to improve on prompting-based tasks by including a very small amount of labeled data as part of the prompt/context (Brown et al., 2020) and without parameter updates, a technique called in-context learning (ICL) or few-shot prompting. We thus investigate the impact that ICL has on LLMs' ability to assess translation quality. Recent works have shown that the impact of ICL is tightly tied with the exact examples included in the prompt, with a poor selection procedure leading to no improvements or even worse performance than the zero-shot case (Jain et al., 2023). We therefore explore two sampling approaches to select in-context examples from a pre-defined "pool" of translation quality assessments: uniform and stratified sampling, where the example pool is bucketed by score ranges and examples are sampled from each bucket.</p>
<h3>4.2 Finetuning</h3>
<p>It has previously been shown that LLMs are capable of zero-shot evaluation (Kocmi and Federmann, 2023), but the extent to which finetuning on human judgment data can further boost the performance of LLMs has not been studied. In the WMT'22 Metrics Shared Task (Freitag et al., 2022), all top submissions were learned metrics; that is, pretrained models finetuned on human judgment data ${ }^{2}$.</p>
<p>Thus, we investigate whether LLMs are amenable to finetuning on human judgment data. LLMs used in top-performing metrics are generally much larger than the pretrained language models leveraged by previous learned metrics (which</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>generally have fewer than 1 billion parameters). Moreover, most learned metrics leverage pretrained encoder-only rather than (decoder-only) prefix language models. We experiment with finetuning LLMs using two objectives:</p>
<ul>
<li>Regression (R): Commonly used for training learned metrics (Rei et al., 2022a), the objective here is a regression loss (e.g., mean squared error) between continuous scores obtained from the model (for example, with a regression head) and the human scores.</li>
<li>Generative Classification (GC): We bucket scores into discrete classes (e.g. "bad", "ok" and "good") and treat the MT evaluation task as a text-to-text classification problem (Raffel et al., 2020) by having the model generate a template sentence with the class. See $\S 6.1$ for more details.</li>
</ul>
<h2>5 Using LLMs to Predict Error Spans</h2>
<p>While producing quality scores that correlate with human judgments is an important part of translation quality assessment, metrics that solely do score prediction suffer from problems of interpretability: if a metric assigns a low score, the downstream users are left in the dark about which parts of the translation were responsible for the score and thus need to be corrected. This is especially problematic in cases where the metric assigns a wrong score to a translation, as it is much harder to diagnose why the evaluation model made a mistake, and identify and prevent similar mistakes in the future. In fact, reducing translation quality to a single score has proven problematic even for human annotators: asking raters to solely provide a single score can lead to rushed and noisy judgments (Freitag et al., 2021a) and the current gold standard for translation quality evaluation involving human annotators is instead based on methodologies like the MQM framework (see §2), which provide richer feedback by identifying error spans, categorizing them, and evaluating their severity.</p>
<p>Interestingly, another emergent phenomenon in LLMs is the success of chain-of-thought prompting (Wei et al., 2022): when defining a prompt for a particular task, if we instruct the model to produce a series of intermediate reasoning steps ("let's think step-by-step"), it tends to generate a free-text rationale before generating an output, and this often improves the performance on the</p>
<p>Based on the given source and reference, identify the major and minor errors in this translation. Note that Major errors refer to actual translation or grammatical errors, and Minor errors refer to smaller imperfections, and purely subjective opinions about the translation.</p>
<div class="codehilite"><pre><span></span><code><span class="p">{</span><span class="err">src_lang</span><span class="p">}</span><span class="w"> </span><span class="nt">source</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;{source}&quot;</span>
<span class="p">{</span><span class="err">tgt_lang</span><span class="p">}</span><span class="w"> </span><span class="nt">human</span><span class="w"> </span><span class="nt">reference</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;{reference}&quot;</span>
<span class="p">{</span><span class="err">tgt_lang</span><span class="p">}</span><span class="w"> </span><span class="nt">translation</span><span class="o">:</span><span class="w"> </span><span class="s2">&quot;{candidate}&quot;</span>
<span class="nt">Errors</span><span class="o">:</span><span class="w"> </span><span class="p">{</span><span class="n">error1</span><span class="p">:</span><span class="n">span</span><span class="p">}</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="p">{</span><span class="n">error1</span><span class="p">:</span><span class="n">severity</span><span class="p">}</span><span class="o">/</span><span class="p">{</span><span class="n">error1</span><span class="p">:</span><span class="n">category</span><span class="p">}</span><span class="o">;</span><span class="w"> </span><span class="p">{</span><span class="n">error2</span><span class="p">:</span><span class="n">span</span><span class="p">}</span><span class="w"> </span><span class="nt">-</span><span class="w"> </span><span class="o">...</span>
</code></pre></div>

<p>Figure 2: The AutoMQM prompt used in this paper. Parts in purple are only included for reference-based evaluation, while parts in orange represent slots for outputs, and are only included for in-context examples.
task at hand (Liu et al., 2023b). Furthermore, this chain-of-thought prompting can be used to obtain structured rationales from LLMs, and this can lead to better performance than with free-text rationales (Lu et al., 2023).</p>
<p>Motivated by these findings, we propose AutoMQM, a prompting technique for translation quality assessment that instructs LLMs to identify errors in a translation, and categorize the type of error according to the MQM framework (Lommel et al., 2014). Furthermore, we don't ask the model to produce a score, as the MQM framework provides an algorithmic procedure to obtain one from identified errors: the total score is the sum of penalties for all errors identified, where (roughly) major errors get penalized with -5 and minors with -1 (see Appendix A for a more detailed description of the scoring algorithm). ${ }^{3}$ Figure 2 shows the main AutoMQM prompt used in this paper.</p>
<p>Importantly, obtaining meaningful AutoMQM results in a zero-shot setting is a substantially more challenging task compared to score prediction: we found that, without any in-context examples, LLMs tend to produce outputs that are either uninformative or difficult to parse. Thus we only consider the AutoMQM task in the few-shot scenario. Based on the findings from $\S 6.2$, we explore the impact of in-context learning by sampling from the example pool using stratified sampling extended with a set of rejection criteria (Appendix D), which ensures that the example set has a balance between major and minor errors as well as diversity in the categories of errors.</p>
<h2>6 Experiments</h2>
<h3>6.1 Experimental Setup</h3>
<p>Data The metrics in this work are evaluated on both high-resource and low-resource language</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>pairs. The three high-resource language pairs come from the WMT'22 Metrics Shared Task (Freitag et al., 2022): en $\rightarrow$ de, zh $\rightarrow$ en, and en $\rightarrow$ ru. The ground-truth translation quality scores are derived from MQM ratings in which expert annotators marked error spans in the translations with different severity levels which are automatically converted to a numeric score (see §2). The four low-resource language pairs come from the WMT'19 Metrics Shared Task (Ma et al., 2019): en $\leftrightarrow$ gu and en $\leftrightarrow \mathrm{kk}$. Since MQM ratings are not available for the lowresource pairs, the ground truth quality scores are direct assessment (DA) scores. DA scores are quality assessments assigned by non-expert raters on a scale from 0-100, normalized per rater. See Table 9 (Appendix B) for statistics about the number of MT systems and segments for every language pair.</p>
<p>Additionally, in our experiments, AutoMQM required in-context examples with MQM annotations to work, so we restrict our evaluation of AutoMQM to en $\rightarrow$ de and $\mathrm{zh} \rightarrow$ en because there are available MQM ratings from the WMT'21 Metrics Shared Task (Freitag et al., 2021b) that we can use as in-context learning example pools.</p>
<p>Models We base most of our experiments on the following LLMs:</p>
<ul>
<li>PaLM: A 540 billion parameter autoregressive Transformer model trained on 780 billion tokens of high-quality text (Chowdhery et al., 2022). It showed remarkable performance on a wide-range of NLP tasks, including Machine Translation (Vilar et al., 2022).</li>
<li>PaLM-2: The successor to PaLM, the PaLM-2 family of LLMs (Anil et al., 2023) builds upon recent research insights, such as compute-optimal scaling, a more multilingual and diverse pre-training mixture, and architectural/optimization improvements. We mainly use two model sizes in the family: PaLM-2 BI-</li>
</ul>
<p>SON and (the larger) PaLM-2-UnicORn. ${ }^{4}$ In addition we explore the impact of instructiontuning by using a UNICORN model finetuned on the FLAN dataset (Wei et al., 2021).</p>
<p>For score prediction, we compare PaLM and PaLM-2 against the GPT family of LLMs (Brown et al., 2020; OpenAI, 2023) by leveraging the results and outputs from the GEMBA evaluator (Kocmi and Federmann, 2023). We then evaluate the performance of AutOMQM with only PaLM-2 models (which performed best in score prediction).</p>
<p>Additionally, for the high-resource languages, we compare to a set of strong baseline evaluation metrics, MetricX-XXL and COMET-22, which were the two top-performing metrics in the WMT'22 Metrics Shared Task. MetricX-XXL and COMET-22 are both finetuned regression models trained on DA data from WMT that are initialized with mT5 (Xue et al., 2021) and XLM-R (Conneau et al., 2020), respectively.</p>
<p>For the AutOMQM experiments, we also compare against MaTESE, a comparable submission to the WMT'22 Metrics Shared task that finetuned a XLM-R model to identify major and minor errors, and computed a score automatically. Since we were unable to obtain the span-level predictions for the MATESE submission, we also compare against the top submission to the WMT'22 Word-Level Quality Estimation Shared Task (Zerva et al., 2021): word-level COMETKiwi (COMET-WL) (Rei et al., 2022b), also based on an XLM-R model trained on a combination of sentence- and word-level data. To do so, we re-run this model on the WMT'22 Metrics Shared Task data, and convert the predicted word-level OK/BAD tags into spans. ${ }^{5}$</p>
<p>Finetuning For regression finetuning, we use a real-valued logit, extracted from a fixed index in the first target token's logit vector, as the quality signal. (In particular, we leverage a special, unused, vocabulary token.) This was the technique used to train MetricX-XXL in the WMT 2022 Shared Task submission (Freitag et al., 2022). The regression-based model was trained on WMT direct assessment (DA) data from the years 2015 through 2020.</p>
<p>For generative classification, we bucket the scores in the training data into five classes, where</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>class boundaries are assigned so that each class contains an equal number of training examples. We then map labels to verbal ratings from the following set, based on their bucket: ["very bad", "bad", "ok", "good", "very good"]. To evaluate the model, predictions are mapped back to integer labels from 1 to 5 . Any predictions not containing a substring in the label set are considered invalid and are mapped to 0 . We experimented with finetuning on both DA and MQM 2020 (Freitag et al., 2021a) data, and found that the latter performed slightly better.</p>
<p>To assess the impact of model size, we also finetune two additional (smaller) PaLM-2 models, which we call $S$ and $M$, comparing their finetuned and zero-shot performance. ${ }^{6}$</p>
<p>Metric Meta-Evaluation The quality of an automatic evaluation metric is estimated by comparing the agreement between the metric scores and ground-truth quality scores on a large number of translations from different MT systems, a process known as metric meta-evaluation. This work reports three different agreement scores, as follows.</p>
<p>The first is system-level accuracy, which calculates the percent of system pairs that are ranked the same by the metric and ground-truth scores, microaveraged over a set of language pairs (Kocmi et al., 2021). System-level scores are defined as the average score across all segments.</p>
<p>At the segment-level, the standard correlation that is reported by WMT is Kendall's $\tau$. However, recent work pointed out problems with Kendall's $\tau$ with respect to ties (Deutsch et al., 2023). In short, different variants of $\tau$ are inconsistent with respect to ties and even biased against metrics that predict ties, as our metrics do in this work. Deutsch et al. (2023) recommend reporting a pairwise accuracy score, which rewards metrics for correctly ranking translations as well as correctly predicting ties, in combination with a tie calibration procedure that automatically introduces ties into metric scores so that the meta-evaluation is fairer. This accuracy score, denoted acc*, ranges between 0 and 1, and a random metric would achieve $33 \%$ accuracy. We report the "group-by-item" variant of the pairwise accuracy score from Deutsch et al. (2023) in addition to Pearson's $\rho$, a complementary signal to rank-based correlations that measure the strength of the linear relationship between two variables (and one of the standard correlations reported in WMT).</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">System-Level</th>
<th style="text-align: center;">Segment-Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">:--</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">All (3 LPs)</td>
<td style="text-align: center;">EN-DE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZH-EN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EN-RU</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">:--</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
<td style="text-align: center;">:--:</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\mathrm{acc}^{*}$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\mathrm{acc}^{*}$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">$\mathrm{acc}^{*}$</td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">MetricX-XXL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$85.0 \%$</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">$61.1 \%$</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">$54.6 \%$</td>
<td style="text-align: center;">0.495</td>
<td style="text-align: center;">$60.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">COMET-22</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$83.9 \%$</td>
<td style="text-align: center;">0.512</td>
<td style="text-align: center;">$60.2 \%$</td>
<td style="text-align: center;">0.585</td>
<td style="text-align: center;">$54.1 \%$</td>
<td style="text-align: center;">0.469</td>
<td style="text-align: center;">$57.7 \%$</td>
</tr>
<tr>
<td style="text-align: left;">COMET-QE</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$78.1 \%$</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">$56.3 \%$</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">$48.8 \%$</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">$53.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Prompting</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$90.1 \%$</td>
<td style="text-align: center;">0.247</td>
<td style="text-align: center;">$55.4 \%$</td>
<td style="text-align: center;">0.255</td>
<td style="text-align: center;">$48.5 \%$</td>
<td style="text-align: center;">0.180</td>
<td style="text-align: center;">$48.6 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 BISON</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$88.7 \%$</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">$56.8 \%$</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">$49.3 \%$</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">$52.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 UNICORN</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$90.1 \%$</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">$56.3 \%$</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">$51.1 \%$</td>
<td style="text-align: center;">0.352</td>
<td style="text-align: center;">$55.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">FLAN-PaLM-2 UNICORN</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$75.9 \%$</td>
<td style="text-align: center;">0.197</td>
<td style="text-align: center;">$55.6 \%$</td>
<td style="text-align: center;">0.139</td>
<td style="text-align: center;">$46.1 \%$</td>
<td style="text-align: center;">0.198</td>
<td style="text-align: center;">$52.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 540B</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$84.3 \%$</td>
<td style="text-align: center;">0.239</td>
<td style="text-align: center;">$56.1 \%$</td>
<td style="text-align: center;">0.270</td>
<td style="text-align: center;">$43.1 \%$</td>
<td style="text-align: center;">0.300</td>
<td style="text-align: center;">$51.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 BISON</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$85.0 \%$</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">$57.0 \%$</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">$48.6 \%$</td>
<td style="text-align: center;">0.303</td>
<td style="text-align: center;">$53.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 UNICORN</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$84.3 \%$</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">$56.1 \%$</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">$48.3 \%$</td>
<td style="text-align: center;">0.209</td>
<td style="text-align: center;">$49.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">FLAN-PaLM-2 UNICORN</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$69.7 \%$</td>
<td style="text-align: center;">0.116</td>
<td style="text-align: center;">$54.6 \%$</td>
<td style="text-align: center;">0.112</td>
<td style="text-align: center;">$43.8 \%$</td>
<td style="text-align: center;">0.156</td>
<td style="text-align: center;">$47.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Finetune</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 BISON (R)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$88.0 \%$</td>
<td style="text-align: center;">0.511</td>
<td style="text-align: center;">$61.0 \%$</td>
<td style="text-align: center;">0.459</td>
<td style="text-align: center;">$51.5 \%$</td>
<td style="text-align: center;">0.458</td>
<td style="text-align: center;">$59.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 BISON (GC)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$86.1 \%$</td>
<td style="text-align: center;">0.400</td>
<td style="text-align: center;">$59.2 \%$</td>
<td style="text-align: center;">0.444</td>
<td style="text-align: center;">$49.3 \%$</td>
<td style="text-align: center;">0.365</td>
<td style="text-align: center;">$56.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM-2 UNICORN (R)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$87.6 \%$</td>
<td style="text-align: center;">0.508</td>
<td style="text-align: center;">$61.1 \%$</td>
<td style="text-align: center;">0.412</td>
<td style="text-align: center;">$52.6 \%$</td>
<td style="text-align: center;">0.460</td>
<td style="text-align: center;">$60.4 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2 BISON (R)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$87.6 \%$</td>
<td style="text-align: center;">0.490</td>
<td style="text-align: center;">$59.9 \%$</td>
<td style="text-align: center;">0.439</td>
<td style="text-align: center;">$53.4 \%$</td>
<td style="text-align: center;">0.437</td>
<td style="text-align: center;">$59.2 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2 BISON (GC)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$86.1 \%$</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">$57.5 \%$</td>
<td style="text-align: center;">0.420</td>
<td style="text-align: center;">$47.3 \%$</td>
<td style="text-align: center;">0.390</td>
<td style="text-align: center;">$54.9 \%$</td>
</tr>
<tr>
<td style="text-align: left;">PaLM 2 UNICORN (GC)</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$86.1 \%$</td>
<td style="text-align: center;">0.407</td>
<td style="text-align: center;">$57.9 \%$</td>
<td style="text-align: center;">0.402</td>
<td style="text-align: center;">$45.6 \%$</td>
<td style="text-align: center;">0.411</td>
<td style="text-align: center;">$55.3 \%$</td>
</tr>
</tbody>
</table>
<p>Table 1: Meta-evaluation results at system and segment-level for the high-resource language pairs. Finetuned (R) and (GC) represent the regression and generative classification objectives (§4.2). $\checkmark$ and $\boldsymbol{X}$ represent reference-based and reference-less metrics, respectively.</p>
<p>Span Meta-Evaluation Since AutoMQM provides not only scores but also the identified error spans, we can compare the predicted spans with the errors marked by annotators in the MQM annotations. We evaluate quality of predicted spans using: (1) Span Precision (SP), which measures the overlap of predicted spans and gold (annotated) spans; and (2) Major recall (MR), which captures the percentage of gold major errors that were predicted as errors (either minor or major).</p>
<p>More formally, consider the set of ground truth spans $S^{\star}$, where each span consists of a sequence of words, i.e., $s_{i}=\left(w_{(a)}, w_{(a+1)}, \cdots\right)$. Let $S_{\text {maj }}^{\star} \subseteq$ $S^{\star}$ be the subset containing only the major errors. Given a span set $S$, we define its positional set $P(S)$ as the set containing the positions of all the words in every span in $S$. For example, assuming a span $s_{i}=\left(w_{(n)}, w_{(n+1)}, \cdots\right)$ in $S$ starts at the $n$th position in the text, its corresponding positional set will include the positions ${n, n+1, \ldots, n+\operatorname{len}\left(s_{i}\right)-$ 1$}$. Then for a set of predicted spans $\hat{S}$, SP and MR are defined as:</p>
<p>$$
\begin{aligned}
\operatorname{SP}(\hat{S}) &amp; =\frac{|P(\hat{S}) \cap P\left(S^{\star}\right)|}{|P(\hat{S})|} \
\operatorname{MR}(\hat{S}) &amp; =\frac{|P(\hat{S}) \cap P\left(S_{\mathrm{maj}}^{\star}\right)|}{|P\left(S_{\mathrm{maj}}^{\star}\right)|}
\end{aligned}
$$</p>
<p>Intuitively, we care for overall precision (regardless of severity) since we want to make sure predicted errors tend to be marked by annotators as well, but for recall we care mostly for major errors,
as these have a larger impact on translation quality and are more critical to identify. Additionally, we also report the (3) Matthews Correlation Coefficient (MCC), one of the official metrics in the wordlevel quality estimation tasks (Zerva et al., 2022).</p>
<h3>6.2 Results</h3>
<h3>6.2.1 Score Prediction</h3>
<p>Table 1 summarizes the meta-evaluation results, at the system and segment level, for both the zero-shot prompting and finetuning settings.</p>
<p>Prompting A first observation is almost all zeroshot LLM evaluators have higher system-level performance than learned metrics (with and without references), with PaLM 540B and PaLM-2 UnICORN achieving the best performance. At the segment level, the story is more complicated: similarly to Kocmi et al. (2022), we find that none of the LLMs we explored was able to consistently outperform the baseline learned metrics. We see that PaLM-540B is a particularly poor reference-based evaluator, which is surprising given its system-level performance. Unexpectedly, instruction-tuning with FLAN seems to degrade performance, with FLAN-PaLM-2 UNICORN achieving poor performance at both the system and segment levels. ${ }^{7}$</p>
<p>Nevertheless, PaLM-2 models achieve high correlations with human judgments, and the reference-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">System</th>
<th style="text-align: center;">Segment acc*</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: center;">Ref?</td>
<td style="text-align: center;">All</td>
<td style="text-align: center;">EN-DE</td>
<td style="text-align: center;">ZH-EN</td>
<td style="text-align: center;">EN-RU</td>
</tr>
<tr>
<td style="text-align: left;">GEMBA</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$85.4 \%$</td>
<td style="text-align: center;">$54.9 \%$</td>
<td style="text-align: center;">$49.5 \%$</td>
<td style="text-align: center;">$47.5 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$88.7 \%$</td>
<td style="text-align: center;">$57.8 \%$</td>
<td style="text-align: center;">$52.6 \%$</td>
<td style="text-align: center;">$55.0 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-3.5</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$82.5 \%$</td>
<td style="text-align: center;">$56.1 \%$</td>
<td style="text-align: center;">$49.7 \%$</td>
<td style="text-align: center;">$49.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-4</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$89.1 \%$</td>
<td style="text-align: center;">$56.4 \%$</td>
<td style="text-align: center;">$53.4 \%$</td>
<td style="text-align: center;">$54.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BISON</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$88.7 \%$</td>
<td style="text-align: center;">$56.8 \%$</td>
<td style="text-align: center;">$49.3 \%$</td>
<td style="text-align: center;">$52.8 \%$</td>
</tr>
<tr>
<td style="text-align: left;">UNICORN</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$90.1 \%$</td>
<td style="text-align: center;">$56.3 \%$</td>
<td style="text-align: center;">$51.1 \%$</td>
<td style="text-align: center;">$55.3 \%$</td>
</tr>
<tr>
<td style="text-align: left;">BISON</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$85.0 \%$</td>
<td style="text-align: center;">$57.0 \%$</td>
<td style="text-align: center;">$48.6 \%$</td>
<td style="text-align: center;">$53.1 \%$</td>
</tr>
<tr>
<td style="text-align: left;">UNICORN</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$84.3 \%$</td>
<td style="text-align: center;">$56.1 \%$</td>
<td style="text-align: center;">$48.3 \%$</td>
<td style="text-align: center;">$49.8 \%$</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison between PaLM-2 and GPT-based GEMBA (Kocmi et al., 2022) at the system and segment levels for the high-resource language pairs.
less PaLM-2 BISON is competitive with the learned baselines, particularly at assessing alternative translations of the same sentence (acc*). When comparing PaLM-2 models with Kocmi et al. (2022)'s GPT-based GEMBA evaluator (Table 2), we see that both families of LLMs perform similarly, with PaLM-2 models exhibiting higher systemlevel performance than GPT-based GEMBA, while GEMBA achieves better segment-level accuracy, particularly in the reference-less setting.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Distribution of scores for various LLM referencebased evaluators, on the EN-DE test set. Note that the $y$ axis is in log-scale.</p>
<p>Figure 3 shows the distribution of scores produced by PaLM- and PaLM-2-based evaluators. We find that, despite being prompted to give a score in the $0-100$ range, these models almost always output one of a very limited set of scores (e.g. 0, 50, 90, 95). Given Kocmi and Federmann (2023)'s similar findings with GPT models, it seems that this is a consequence of the pretraining objective.</p>
<p>Finetuning Despite their already-great performance in the zero-shot setting, we find that finetuning LLMs can further improve LLM evaluators' segment-level scores. This is particularly obvious for the reference-less evaluators, where a finetuned PaLM-2 BISON achieves state-of-the-art performance in segment-level correlations and comparable system-level accuracy across all language
pairs. Moreover, when we look at how performance scales with parameter count (Figure 4), we observe an interesting trend: while smaller models are not capable of being effective zero-shot evaluators, finetuning them leads to competitive performance, and only a slight decrease when compared to their larger finetuned counterparts.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Behavior of Pearson as we scale the LLM's parameter count. Note that the $x$ axis is not to-scale with regard to parameter count.</p>
<p>In-context Learning Figure 5 shows the mean and interquartile range (IQR) of the performance as we increase the number of in-context examples $k$ (with 100 example sets per $k$ ) sampled with stratified sampling (see Appendix E for uniform). Surprisingly, despite evidence of the benefits of incontext learning for many tasks, we found that including in-context examples during evaluation (almost) never led to better performance, either with uniform or stratified sampling.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Mean Pearson and its interquartile range (IQR) in the WMT22 EN-DE test set, as we increase the number of in-context examples with stratified sampling</p>
<p>To investigate the cause of this disappointing performance, we looked at how particular in-context example sets affect the distribution of scores produced by LLM-based evaluators. Figure 6 shows the distribution of scores over the whole test set for the 1-shot and 2-shot settings, with different in-context examples sets. We can see that output distribution is heavily biased by the scores in the in-context examples: despite never predicting 79</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Distribution of scores for PaLM-2 (BISON) models for 1-shot (top) and 2-shot (bottom) setups, with various incontext learning sets for each (and their scores in the legend)
in the zero-shot setting, when a single example with that score is included, it starts to dominate the model predictions. This seems to hint that LLMs "overfit" to the specific scores provided as examples, rather than generalizing to the broader evaluation task, which could explain the lackluster performance of in-context learning.</p>
<h3>6.3 Low Resource Languages</h3>
<p>Table 3 shows the performance of PaLM-2 models at score prediction for low-resource translation. Overall, we find that similar to high-resource LPs, these models are good zero-shot evaluators, with system-level accuracies around $90 \%$. However, zero-shot LLMs underperform learned metrics, even when these metrics also weren't exposed to data in these low-resource languages.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">System</th>
<th style="text-align: right;">Segment $\rho$</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: right;">Ref?</td>
<td style="text-align: right;">All</td>
<td style="text-align: right;">EN-KK</td>
<td style="text-align: right;">EN-GU</td>
<td style="text-align: right;">KK-EN</td>
</tr>
<tr>
<td style="text-align: left;">Baseline</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">MetricX-XXL*</td>
<td style="text-align: right;">$\checkmark$</td>
<td style="text-align: right;">$94.0 \%$</td>
<td style="text-align: right;">0.666</td>
<td style="text-align: right;">0.701</td>
<td style="text-align: right;">0.539</td>
</tr>
<tr>
<td style="text-align: left;">Prompting</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">BISON</td>
<td style="text-align: right;">$\checkmark$</td>
<td style="text-align: right;">$92.2 \%$</td>
<td style="text-align: right;">0.605</td>
<td style="text-align: right;">0.540</td>
<td style="text-align: right;">0.462</td>
</tr>
<tr>
<td style="text-align: left;">Unicorn</td>
<td style="text-align: right;">$\checkmark$</td>
<td style="text-align: right;">$87.4 \%$</td>
<td style="text-align: right;">0.609</td>
<td style="text-align: right;">0.621</td>
<td style="text-align: right;">0.495</td>
</tr>
<tr>
<td style="text-align: left;">BISON</td>
<td style="text-align: right;">$\boldsymbol{X}$</td>
<td style="text-align: right;">$89.8 \%$</td>
<td style="text-align: right;">0.567</td>
<td style="text-align: right;">0.478</td>
<td style="text-align: right;">0.381</td>
</tr>
<tr>
<td style="text-align: left;">Unicorn</td>
<td style="text-align: right;">$\boldsymbol{X}$</td>
<td style="text-align: right;">$84.4 \%$</td>
<td style="text-align: right;">0.536</td>
<td style="text-align: right;">0.523</td>
<td style="text-align: right;">0.433</td>
</tr>
</tbody>
</table>
<p>Table 3: Meta-evaluation results for system-level accuracy and segment-level Pearson on the low-resource languages, using PaLM-2 for score prediction. *Note that the baseline is slightly different from the high-resource case, being trained on the same data but without these low-resource language pairs.</p>
<h3>6.3.1 AutoMQM</h3>
<p>Figure 14 shows the mean and interquartile range (IQR) of the performance of PaLM-2 BISON with AutoMQM, as we increase the number of incontext examples (again, with 100 example sets per $k$ ). Contrary to the performance with score prediction, we find that performance with AutoMQM seems to (mostly) scale with the number of incontext examples: performance increases monotonically with up to 4 in-context examples and plateaus thereafter. Additionally, the variance across the incontext learning sets seems to be lower, with most example sets exhibiting less than 0.05 Pearson difference from the best-performing sets. All this suggests that LLM evaluators are much more robust to the choice of in-context examples when prompted for AutoMQM rather than for score prediction. We also find that the behavior of in-context learning is quite similar for both reference-based and reference-less evaluation tasks. Finally, we observe that the example sets that perform well for one task generally work well for the other, with performance on both settings given a fixed in-context set being highly correlated, as shown in Figure 7.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Scatter plot of the Pearson of PaLM-2 (BISON) models, with/without including the reference in the prompt, for each in-context learning setting tried.</p>
<p>Table 4 shows the meta-evaluation results for PaLM-2 BISON and Unicorn prompted with AutoMQM (using the best-performing in-context learning sets in Figure 14). For ease of comparison, we also report their performance when prompted for score prediction, as well as the performance of the baselines. Overall, prompting LLMs with AutoMQM seems to lead to significant improvements in evaluating machine translation quality, particularly for larger models: Unicorn achieves better performance (across all meta evaluations) with it than when prompted for score prediction,</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">System-Level</th>
<th style="text-align: center;">Segment-Level</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">All (2 LPs)</td>
<td style="text-align: center;">EN-DE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">ZH-EN</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">Ref?</td>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">acc $^{*}$</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">acc $^{*}$</td>
</tr>
<tr>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">MetricX-XXL</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$81.1 \%$</td>
<td style="text-align: center;">0.549</td>
<td style="text-align: center;">$61.1 \%$</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">$54.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">MaTESE</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$79.9 \%$</td>
<td style="text-align: center;">0.391</td>
<td style="text-align: center;">$58.8 \%$</td>
<td style="text-align: center;">0.528</td>
<td style="text-align: center;">$51.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">COMET-QE</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$76.9 \%$</td>
<td style="text-align: center;">0.419</td>
<td style="text-align: center;">$56.3 \%$</td>
<td style="text-align: center;">0.505</td>
<td style="text-align: center;">$48.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">MaTESE-QE</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$73.4 \%$</td>
<td style="text-align: center;">0.298</td>
<td style="text-align: center;">$57.9 \%$</td>
<td style="text-align: center;">0.468</td>
<td style="text-align: center;">$50.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">COMET-WL</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$71.6 \%$</td>
<td style="text-align: center;">0.418</td>
<td style="text-align: center;">$57.1 \%$</td>
<td style="text-align: center;">0.406</td>
<td style="text-align: center;">$51.5 \%$</td>
</tr>
<tr>
<td style="text-align: center;">Score Prediction</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 BISON</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$86.4 \%$</td>
<td style="text-align: center;">0.394</td>
<td style="text-align: center;">$56.8 \%$</td>
<td style="text-align: center;">0.322</td>
<td style="text-align: center;">$49.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 Unicorn</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$86.4 \%$</td>
<td style="text-align: center;">0.401</td>
<td style="text-align: center;">$56.3 \%$</td>
<td style="text-align: center;">0.349</td>
<td style="text-align: center;">$51.1 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 BISON</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$84.0 \%$</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">$57.0 \%$</td>
<td style="text-align: center;">0.299</td>
<td style="text-align: center;">$48.6 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 Unicorn</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$80.5 \%$</td>
<td style="text-align: center;">0.275</td>
<td style="text-align: center;">$56.1 \%$</td>
<td style="text-align: center;">0.252</td>
<td style="text-align: center;">$48.3 \%$</td>
</tr>
<tr>
<td style="text-align: center;">AutoMQM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 BISON</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$84.0 \%$</td>
<td style="text-align: center;">0.369</td>
<td style="text-align: center;">$59.2 \%$</td>
<td style="text-align: center;">0.355</td>
<td style="text-align: center;">$48.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM-2 Unicorn</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$87.6 \%$</td>
<td style="text-align: center;">0.432</td>
<td style="text-align: center;">$59.1 \%$</td>
<td style="text-align: center;">0.442</td>
<td style="text-align: center;">$51.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2 BISON</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$87.6 \%$</td>
<td style="text-align: center;">0.297</td>
<td style="text-align: center;">$55.2 \%$</td>
<td style="text-align: center;">0.331</td>
<td style="text-align: center;">$48.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">PaLM 2 Unicorn</td>
<td style="text-align: center;">$\boldsymbol{X}$</td>
<td style="text-align: center;">$83.4 \%$</td>
<td style="text-align: center;">0.368</td>
<td style="text-align: center;">$56.4 \%$</td>
<td style="text-align: center;">0.429</td>
<td style="text-align: center;">$50.2 \%$</td>
</tr>
</tbody>
</table>
<p>Table 4: Meta-evaluation results for PaLM-2 models using AutoMQM and score prediction, at the system and segment levels for multiple language pairs.
and its reference-less version is competitive with the best learned metric even at the segment level. However, for the smaller BISON, the benefits of AutoMQM are less clear, with both techniques performing comparably. This hints that scale is necessary for zero- and few- shot fine-grained evaluation (like with AutoMQM). We also find that the distribution of scores produced by LLMs prompted with AutoMQM is much closer to the gold MQM distribution, with models outputting a much larger set of scores, and in the same ranges as annotators do (see Figure 8).
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Distribution of scores for PaLM-2 models using AutoMQM, on WMT22 EN-DE</p>
<p>Finally, when evaluating the error spans produced by LLMs prompted with AutoMQM (Table 5), we find that PaLM-2 models are able to identify most of the major errors. However, it does seem to over-predict errors (with errors predicted by Unicorn having on average $\sim 5$ words per span vs $\sim 2$ words in the ground truth) and have overall</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">R?</th>
<th style="text-align: center;">EN-DE</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ZH-EN</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">SP</td>
<td style="text-align: center;">MR</td>
<td style="text-align: center;">MCC</td>
<td style="text-align: center;">SP MR MCC</td>
</tr>
<tr>
<td style="text-align: center;">Baselines</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">COMET-WL</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.267</td>
<td style="text-align: center;">0.250</td>
<td style="text-align: center;">0.161</td>
<td style="text-align: center;">0.3640 .1780 .152</td>
</tr>
<tr>
<td style="text-align: center;">AutoMQM</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">BISON</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.095</td>
<td style="text-align: center;">0.749</td>
<td style="text-align: center;">0.060</td>
<td style="text-align: center;">0.2520 .2550 .109</td>
</tr>
<tr>
<td style="text-align: center;">Unicorn</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.175</td>
<td style="text-align: center;">0.628</td>
<td style="text-align: center;">0.193</td>
<td style="text-align: center;">0.2380 .4760 .143</td>
</tr>
<tr>
<td style="text-align: center;">Bison</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.119</td>
<td style="text-align: center;">0.520</td>
<td style="text-align: center;">0.092</td>
<td style="text-align: center;">0.2240 .3110 .091</td>
</tr>
<tr>
<td style="text-align: center;">Unicorn</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.580</td>
<td style="text-align: center;">0.150</td>
<td style="text-align: center;">0.2290 .4880 .133</td>
</tr>
</tbody>
</table>
<p>Table 5: Span-level meta-evaluation on WMT22 for PaLM-2 models using AutoMQM. SR and MR represent span precision and major recall, respectively.
low span precision. Similarly to overall score correlations, scale also seems to be important for the quality of spans produced by AutoMQM, with Unicorn outperforming Bison at most metrics. Additionally, Unicorn prompted with AutoMQM predicts spans of comparable quality to the ones produced by current state-of-the-art learned wordlevel evaluators (trained on a considerable number of fine-grained annotations derived from MQM): while word-level models are more precise, their overall span correlation (MCC) is comparable, and they miss considerably more major errors than LLMs (despite only leveraging a handful of annotations).</p>
<h1>7 Conclusion</h1>
<p>In this study, we have systematically investigated the capabilities of large language models for machine translation evaluation through score prediction, and proposed AutoMQM, a novel</p>
<p>prompting technique that leverages the Multidimensional Quality Metrics (MQM) framework for interpretable MT evaluation using LLMs.</p>
<p>We demonstrated that just prompting LLMs for score prediction leads to state-of-the-art systemlevel evaluators, but still falls short of the best learned metrics at the segment-level (with finetuning being necessary to close this gap). Then we showed that AutoMQM can further improve the performance of LLMs without finetuning while providing interpretability through error spans that align with human annotations.</p>
<p>Our findings surrounding finetuning LLMs for score prediction hint that LLMs' performance in machine translation evaluation could be further improved by finetuning these models on fine-grained human judgment data (like MQM) and is a direction we are actively pursuing. Additionally, the general-purpose nature of LLMs may enable the application of similar prompting techniques (leveraging some fine-grained evaluation schemes) to other evaluation problems (Wu et al., 2023).</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Ricardo Rei, Marcos Treviso and Chryssa Zerva for helping run the wordlevel QE baselines, and George Foster who provided feedback on an earlier version of this work. This work was partially supported by EU's Horizon Europe Research and Innovation Actions (UTTER, contract 101070631), by the project DECOLLAGE (ERC-2022-CoG 101088763), by the Portuguese Recovery and Resilience Plan through project C645008882- 00000055 (Center for Responsible AI), and the Fundação para a Ciência e Tecnologia through contracts SFRH/BD/150706/2020 and UIDB/50008/2020.</p>
<h2>References</h2>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz,</p>
<p>Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy GurAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Chris Callison-Burch, Philipp Koehn, Christof Monz, Josh Schroeder, and Cameron Shaw Fordyce, editors. 2008. Proceedings of the Third Workshop on Statistical Machine Translation. Association for Computational Linguistics, Columbus, Ohio.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pil-</p>
<p>lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.</p>
<p>Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 84408451, Online. Association for Computational Linguistics.</p>
<p>Daniel Deutsch, George Foster, and Markus Freitag. 2023. Ties Matter: Modifying Kendall's Tau for Modern Metric Meta-Evaluation.</p>
<p>Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, and André F. T. Martins. 2023. Bridging the gap: A survey on integrating (human) feedback for natural language generation.</p>
<p>Erick Fonseca, Lisa Yankovskaya, AndrÃ© F. T. Martins, Mark Fishel, and Christian Federmann. 2019. Findings of the wmt 2019 shared tasks on quality estimation. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), pages 1-12, Florence, Italy. Association for Computational Linguistics.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and André F. T. Martins. 2022. Results of WMT22 metrics shared task: Stop using BLEU - neural metrics are better and more robust. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 46-68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435.</p>
<p>Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023. Multidimensional evaluation of text summarization with in-context learning.</p>
<p>Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel, and Maja Popović. 2022. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1-45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Tom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality.</p>
<p>Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478-494, Online. Association for Computational Linguistics.</p>
<p>Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023a. Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surv., 55(9).</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023b. G-eval: Nlg evaluation using gpt-4 with better human alignment.</p>
<p>Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. 2014. Multidimensional quality metrics (MQM): A framework for declaring and describing translation quality metrics. Revista Tradumàtica: tecnologies de la traducció.</p>
<p>Qingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and Dacheng Tao. 2023. Error analysis prompting enables human-like translation evaluation in large language models: A case study on chatgpt. arXiv preprint.</p>
<p>Qingsong Ma, Johnny Wei, Ondřej Bojar, and Yvette Graham. 2019. Results of the WMT19 metrics shared task: Segment-level and strong MT systems pose big challenges. In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 62-90, Florence, Italy. Association for Computational Linguistics.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Stefano Perrella, Lorenzo Proietti, Alessandro Scirè, Niccolò Campolungo, and Roberto Navigli. 2022. MaTESe: Machine translation evaluation as a sequence tagging problem. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 569-577, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Yiwei Qin, Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2022. T5score: Discriminative finetuning of generative evaluation metrics. ArXiv, abs/2212.05726.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(1).</p>
<p>Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a. COMET-22: Unbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578-585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634-645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2022. Prompting palm for translation: Assessing strategies and performance.</p>
<p>Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems.</p>
<p>Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023. Finegrained human feedback gives better rewards for language model training.</p>
<p>Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. 2023. Instructscore: Towards explainable text generation evaluation with automatic feedback.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.</p>
<p>Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orăsan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022. Findings of the WMT 2022 shared task on quality estimation. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 69-99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.</p>
<p>Chrysoula Zerva, Daan van Stigt, Ricardo Rei, Ana C Farinha, Pedro Ramos, José G. C. de Souza, Taisiya Glushkova, Miguel Vera, Fabio Kepler, and André F. T. Martins. 2021. IST-unbabel 2021 submission for the quality estimation shared task. In Proceedings of the Sixth Conference on Machine Translation, pages 961-972, Online. Association for Computational Linguistics.</p>
<h2>A Multidimensional Quality Metric (MQM)</h2>
<p>The Multidimensional Quality Metrics (MQM) framework is a flexible human-evaluation framework developed to evaluate and categorize errors in translations. Annotators are instructed to identify all errors within each segment in a document, paying particular attention to document context. See Table 6 for the annotator guidelines provided.</p>
<p>Annotators are asked to assign both an error severity and category. Error severity (either major or minor) is assigned independently of category. Spans with no marked errors have neutral severity and no category. Possible error categories are displayed in Table 7.</p>
<p>You will be assessing translations at the segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments.</p>
<p>Please identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment.</p>
<p>To identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc).</p>
<p>Please pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong.</p>
<p>There are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the five-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected.</p>
<p>Table 6: MQM annotator guidelines
Since MQM doesn't ask annotators for quality scores, those scores are derived automatically from the identified error spans and their classifications, based on a weighting of each error severity and category. Table 8 summarizes this weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators. In some settings (e.g. calculating correlation for learned metrics), the scores are negated.</p>
<p>We use the same weighting to obtain scores from errors identified by AutoMQM.</p>
<h2>B Datasets' Statistics</h2>
<p>See Table 9 for a summary of the number of systems and annotated segments per system in the evaluation datasets used in this work.</p>
<h2>C Score Prediction Prompt</h2>
<p>Figure 9 contains the GEMBA-SQM prompt that we used for our 0 -shot experiments.</p>
<h2>D Sampling in-context learning examples for AutoMQM</h2>
<p>Figure 10 shows the rejection criteria used when sampling example sets as discussed in $\S 4$.</p>
<h2>E Additional Results</h2>
<p>Figures 11, 12, 13 and 8 present additional experimental results.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Error Category</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Accuracy</td>
<td style="text-align: center;">Addition <br> Omission <br> Mistranslation <br> Untranslated text</td>
<td style="text-align: center;">Translation includes information not present in the source. Translation is missing content from the source. Translation does not accurately represent the source. Source text has been left untranslated.</td>
</tr>
<tr>
<td style="text-align: center;">Fluency</td>
<td style="text-align: center;">Punctuation <br> Spelling <br> Grammar <br> Register <br> Inconsistency <br> Character encoding</td>
<td style="text-align: center;">Incorrect punctuation (for locale or style). Incorrect spelling or capitalization. <br> Problems with grammar, other than orthography. <br> Wrong grammatical register (eg, inappropriately informal pronouns). <br> Internal inconsistency (not related to terminology). <br> Characters are garbled due to incorrect encoding.</td>
</tr>
<tr>
<td style="text-align: center;">Terminology</td>
<td style="text-align: center;">Inappropriate for context Inconsistent use</td>
<td style="text-align: center;">Terminology is non-standard or does not fit context. <br> Terminology is used inconsistently.</td>
</tr>
<tr>
<td style="text-align: center;">Style</td>
<td style="text-align: center;">Awkward</td>
<td style="text-align: center;">Translation has stylistic problems.</td>
</tr>
<tr>
<td style="text-align: center;">Locale convention</td>
<td style="text-align: center;">Address format <br> Currency format <br> Date format <br> Name format <br> Telephone format <br> Time format</td>
<td style="text-align: center;">Wrong format for addresses. <br> Wrong format for currency. <br> Wrong format for dates. <br> Wrong format for names. <br> Wrong format for telephone numbers. <br> Wrong format for time expressions.</td>
</tr>
<tr>
<td style="text-align: center;">Other</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Any other issues.</td>
</tr>
<tr>
<td style="text-align: center;">Source error</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">An error in the source.</td>
</tr>
<tr>
<td style="text-align: center;">Non-translation</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Impossible to reliably characterize distinct errors.</td>
</tr>
</tbody>
</table>
<p>Table 7: MQM hierarchy.</p>
<div class="codehilite"><pre><span></span><code>Score the following translation from {src_lang} to {tgt_lang} with respect to the
human reference on a continuous scale from 0 to 100 that starts with &quot;No meaning
preserved&quot;, goes through &quot;Some meaning preserved&quot;, then &quot;Most meaning preserved
and few grammar mistakes&quot;, up to &quot;Perfect meaning and grammar&quot;.
{src_lang} source: &quot;{source}&quot;
{tgt_lang} human reference: &quot;{reference}&quot;
{tgt_lang} translation: &quot;{candidate}&quot;
Score (0-100): {score}
</code></pre></div>

<p>Figure 9: The score prediction prompt used in this paper. Equivalent to the GEMBA-SQM prompt in Kocmi and Federmann (2023). Parts in purple are only included for reference-based evaluation, while parts in orange represent slots for outputs and are only included for in-context examples.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Severity</th>
<th style="text-align: left;">Category</th>
<th style="text-align: left;">Weight</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Major</td>
<td style="text-align: left;">Non-translation</td>
<td style="text-align: left;">25</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">all others</td>
<td style="text-align: left;">5</td>
</tr>
<tr>
<td style="text-align: left;">Minor</td>
<td style="text-align: left;">Fluency/Punctuation</td>
<td style="text-align: left;">0.1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">all others</td>
<td style="text-align: left;">1</td>
</tr>
<tr>
<td style="text-align: left;">Neutral</td>
<td style="text-align: left;">all</td>
<td style="text-align: left;">0</td>
</tr>
</tbody>
</table>
<p>Table 8: MQM error weighting.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">#Sys</th>
<th style="text-align: center;">#Seg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">en $\rightarrow$ de</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">1315</td>
</tr>
<tr>
<td style="text-align: center;">zh $\rightarrow$ en</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1875</td>
</tr>
<tr>
<td style="text-align: center;">en $\rightarrow$ ru</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">1315</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">LP</th>
<th style="text-align: center;">#Sys</th>
<th style="text-align: center;">#Seg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">en $\rightarrow \mathrm{kk}$</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">998</td>
</tr>
<tr>
<td style="text-align: center;">$\mathrm{kk} \rightarrow$ en</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: center;">en $\rightarrow$ gu</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">998</td>
</tr>
<tr>
<td style="text-align: center;">gu $\rightarrow$ en</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">1016</td>
</tr>
</tbody>
</table>
<p>Table 9: The number of systems and segments that have MQM scores (left) and DA scores (right) used as ground-truth in this work.</p>
<div class="codehilite"><pre><span></span><code><span class="n">def</span><span class="w"> </span><span class="n">check_icl_set</span><span class="p">(</span>
<span class="nl">examples</span><span class="p">:</span><span class="w"> </span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">,</span>
<span class="n">min_errors</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="n">majmin_threshold</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="n">cat_diversity</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="n">min_clen</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="n">max_clen</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span>
<span class="err">#</span><span class="w"> </span><span class="k">Check</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">they</span><span class="w"> </span><span class="n">have</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">same</span><span class="w"> </span><span class="n">number</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">spans</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">severity</span><span class="o">/</span><span class="n">category</span>
<span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">examples</span><span class="p">.</span><span class="n">apply</span><span class="p">(</span>
<span class="w">    </span><span class="n">lambda</span><span class="w"> </span><span class="n">r</span><span class="p">(</span>
<span class="w">        </span><span class="nf">len</span><span class="p">(</span><span class="n">r</span><span class="o">[</span><span class="n">&#39;span&#39;</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">r</span><span class="o">[</span><span class="n">&#39;severity&#39;</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">r</span><span class="o">[</span><span class="n">&#39;span&#39;</span><span class="o">]</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">r</span><span class="o">[</span><span class="n">&#39;category&#39;</span><span class="o">]</span><span class="p">),</span>
<span class="w">        </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span>
<span class="p">).</span><span class="ow">all</span><span class="p">()</span><span class="err">:</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">False</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">Check</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">there</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="k">at</span><span class="w"> </span><span class="n">least</span><span class="w"> </span><span class="n">min_errors</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">examples</span><span class="o">[</span><span class="n">&#39;severity&#39;</span><span class="o">]</span><span class="p">.</span><span class="n">apply</span><span class="p">(</span><span class="n">lambda</span><span class="w"> </span><span class="nl">svs</span><span class="p">:</span><span class="w"> </span><span class="nf">len</span><span class="p">(</span><span class="n">svs</span><span class="p">)).</span><span class="nf">sum</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nl">min_errors</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">False</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">Check</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">there</span><span class="s1">&#39;s a balance of major and minor errors.</span>
<span class="s1">    major_count = examples[&#39;</span><span class="n">severity</span><span class="s1">&#39;].apply(lambda svs: sum([s==&#39;</span><span class="n">major</span><span class="s1">&#39; for s in svs])).sum()</span>
<span class="s1">    minor_count = examples[&#39;</span><span class="n">severity</span><span class="s1">&#39;].apply(lambda svs: sum([s==&#39;</span><span class="n">minor</span><span class="s1">&#39; for s in svs])).sum()</span>
<span class="s1">    if abs(major_count - minor_count) &gt; majmin_threshold:</span>
<span class="s1">        return False</span>
<span class="s1">    # Check that at least cat_diversity error types are represented.</span>
<span class="s1">    categories = examples[&#39;</span><span class="n">category</span><span class="s1">&#39;].apply(lambda cs: [c.split(&quot;/&quot;)[0] for c in cs])</span>
<span class="s1">    represented_error_types = set().union(*categories.tolist())</span>
<span class="s1">    if len(represented_error_types) &lt; cat_diversity:</span>
<span class="s1">        return False</span>
<span class="s1">    top_clen = examples.apply(</span>
<span class="s1">        lambda row: max(len(row[s]) for s in (&#39;</span><span class="n">source</span><span class="s1">&#39;, &#39;</span><span class="n">reference</span><span class="s1">&#39;, &#39;</span><span class="n">candidate</span><span class="s1">&#39;)</span>
<span class="s1">    ), axis=1).max()</span>
<span class="s1">    bot_clen = examples.apply(</span>
<span class="s1">        lambda row: min(len(row[s]) for s in (&#39;</span><span class="n">source</span><span class="s1">&#39;, &#39;</span><span class="n">reference</span><span class="s1">&#39;, &#39;</span><span class="n">candidate</span><span class="err">&#39;</span><span class="p">)),</span>
<span class="w">    </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">min</span><span class="p">()</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">top_clen</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">max_clen</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">bot_clen</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="nl">min_clen</span><span class="p">:</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="k">False</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="ow">All</span><span class="w"> </span><span class="n">checks</span><span class="w"> </span><span class="n">passed</span><span class="p">.</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="k">True</span>
</code></pre></div>

<p>Figure 10: Rejection criteria used when sampling in-context learning examples for AUTOMQM.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 11: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt, sampled with uniform (left) and stratified (right) sampling, for WMT22 EN-DE.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 12: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the score prediction prompt, sampled with uniform (left) and stratified (right) sampling, for WMT22 ZH-EN.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 13: Distribution of scores for various LLM referencebased evaluators, on the ZH-EN test set. Note that the $y$ axis is in log-scale.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 14: Mean Pearson and its interquartile range (IQR), as we increase the number of in-context examples in the AutoMQM prompt, for EN-DE (left) and ZH-EN (right).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ Note that this might be a problem with the FLAN dataset and not instruction-tuning in general, as the GPT models are also instruction-tuned and perform well.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{6}$ We use a small variation of the zero-shot prompt, asking models for scores from the same 5 buckets used in finetuning.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
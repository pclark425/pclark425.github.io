<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4213 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4213</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4213</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-97.html">extraction-schema-97</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <p><strong>Paper ID:</strong> paper-273850480</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.03484v1.pdf" target="_blank">Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature</a></p>
                <p><strong>Paper Abstract:</strong> Automated knowledge extraction from scientific literature can potentially accelerate materials discovery. We have investigated an approach for extracting synthesis protocols for reticular materials from scientific literature using large language models (LLMs). To that end, we introduce a Knowledge Extraction Pipeline (KEP) that automatizes LLM-assisted paragraph classification and information extraction. By applying prompt engineering with in-context learning (ICL) to a set of open-source LLMs, we demonstrate that LLMs can retrieve chemical information from PDF documents, without the need for fine-tuning or training and at a reduced risk of hallucination. By comparing the performance of five open-source families of LLMs in both paragraph classification and information extraction tasks, we observe excellent model performance even if only few example paragraphs are included in the ICL prompts. The results show the potential of the KEP approach for reducing human annotations and data curation efforts in automated scientific knowledge extraction.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4213.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4213.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KEP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Extraction Pipeline (KEP)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-enabled, domain-independent pipeline that extracts synthesis protocols and quantitative experimental parameters from unstructured PDF scientific articles using prompt-engineered in‑context learning and structured JSON outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Knowledge Extraction Pipeline (KEP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KEP processes PDFs (via DS4SD) to extract paragraphs, classifies paragraphs as relevant/irrelevant using LLMs with prompt engineering and in‑context learning (ICL) and a small set of examples, then runs an LLM-based information extraction step that outputs structured JSON objects encoding products, reactants, solvents and numerical conditions; it includes an Examples Selection step to choose the best few-shot examples for prompts and a Knowledge Representation module to normalize units and instantiate entities/relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>flan-t5-xxl-11b, flan-ul2-20b, granite-20b-code-instruct, granite-34b-code-instruct, llama-3-70b-instruct, llama-3.1-405b-instruct, mistral-large, mixtral-8x7b-instruct-v01</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>11B, 20B, 20B, 34B, 70B, 405B, (mistral-large: unspecified in text), mixtral ~47B (13B active)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / Chemistry (reticular materials: MOFs, ZIFs, COFs, zeolites)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Corpus: 2,287 CC-BY articles available; experiments: 305 PDFs sampled (producing 325 classified paragraphs; golden dataset: 131 annotated syntheses / 106–131 annotated paragraphs used in experiments depending on step).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Empirical experimental numerical data (extraction of quantitative synthesis parameters: temperatures, times, reagent quantities, volumes) — not derivation of physical laws or scaling laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Examples extracted as structured values: e.g., "heated to 100 °C for 24 h" (reaction temperature = 100 °C; reaction time = 24 h), reagent quantities such as "Bis(imidazol‑1‑yl)methane (bim) (0.02 mmol)", solvent volume "1.0 mL" and solvent ratio "DMF/EtOH/H2O (2:1:1, vol)".</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text extraction from PDFs using DS4SD → paragraph segmentation → paragraph classification and information extraction performed by open‑source LLMs using prompt engineering with in‑context learning (few-shot examples embedded in prompts); outputs constrained to JSON schema provided in prompts; an Examples Selection routine samples candidate example sets to maximize prompt performance.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Outputs compared to SME-annotated golden dataset of paragraphs and JSON annotations; structural comparison by JSON key/value pairs; evaluation of paragraph classification via precision/recall/F1 and information extraction via a JSON-structure accuracy metric defined per-key.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paragraph classification: F1 up to 0.98 (e.g., llama-3-70b-instruct, mistral-large); Information extraction: reported structure-accuracy up to 0.96 (llama-3.1-405b-instruct). Reported per-model results in paper tables (e.g., Table 4 and Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Reported maxima: paragraph classification F1 = 0.98 (≈98%); information extraction accuracy = 0.96 (≈96%) on the golden test sets used.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Performance highly sensitive to the choice of few-shot examples in prompts (Examples Selection required); hallucinations (models adding explanatory text) caused incorrect outputs if instructions were not obeyed; token limitations constrained number/size of examples (smaller context windows limited flan models); no semantic JSON comparison metric implemented (only structural comparison); models differ in prompt-example needs and a large parameter count does not guarantee best performance; pipeline extracts empirical parameters but does not infer scientific laws or workflows step-by-step (not yet implemented).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared across multiple open-source LLM families/models (flan, granite, llama, mistral, mixtral) using the same golden dataset; SME annotations used as gold standard. No direct empirical comparison to non-LLM extraction pipelines (e.g., BiLSTM-CRF pipelines) was performed in this work, though those approaches are discussed in related work and suggested for future comparative analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4213.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4213.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Polak 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced study that uses conversational language models and prompt engineering to extract material-centric numeric information (material, value, unit) from research papers, focusing on simpler extraction tasks than those targeted in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Extracting accurate materials data from research papers with conversational language models and prompt engineering</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Conversational language-model extraction (Polak et al. 2024)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Reported pipeline leveraging conversational LLMs and prompt engineering to extract simple triples (material, numeric value, unit) from text; according to the cited description, the work focused on simple extraction tasks and used zero-shot methods for determining relevance in some steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Simple quantitative extractions (material, numeric value, unit) — data points rather than derived laws.</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Not provided in this paper beyond the description 'material, value and unit' extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Conversational LLMs with prompt engineering; zero‑shot relevance detection mentioned in this paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited as focusing on simpler extraction tasks compared to the current work; limited scope (material/value/unit) and methodological differences (zero-shot relevance determination) from KEP.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Discussed as related work; no direct comparative metrics provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4213.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4213.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Huo 2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Semi-supervised machine-learning classification of materials synthesis procedures</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semi-supervised ML approach that classifies inorganic materials synthesis steps using topic modeling and a supervised classifier trained on manually annotated paragraphs, and models step sequences with a Markov chain to produce flowcharts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Semi-supervised machine-learning classification of materials synthesis procedures</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Semi-supervised synthesis classification pipeline (Huo et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline used Latent Dirichlet Allocation (LDA) for unsupervised topic clustering of synthesis terminology, a random forest classifier trained on hundreds of SME-annotated paragraphs to categorize synthesis types, and a Markov chain to model sequences of synthesis steps for flowchart construction.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / Inorganic synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used annotations of hundreds of paragraphs (exact paper count not specified in this citing paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of procedural patterns and step-sequence models (not explicit quantitative physical laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Identification and classification of synthesis steps and typical term clusters (no explicit numerical laws reported in this citing paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Topic modeling (LDA) + supervised random forest classifier + Markov chain sequence modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td>Supervised training/validation using hundreds of annotated paragraphs (details in the original Huo et al. paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Required considerable manual annotation effort (hundreds of paragraphs) and employed a multi-algorithm pipeline; contrasted by the present paper which uses few-shot LLM prompting to reduce annotation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Serves as a methodological baseline in related work; no direct numerical comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4213.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4213.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Kononova 2019</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-mined dataset of inorganic materials synthesis recipes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dataset and pipeline that automatically extracted codified synthesis 'recipes' for solid-state inorganic materials from publications using traditional NLP and text-mining techniques (e.g., BiLSTM-CRF, material parsers).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Text-mined dataset of inorganic materials synthesis recipes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Text-mining recipe extraction pipeline (Kononova et al. 2019)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Automatic extraction pipeline combining paragraph classification and multiple NLP components (BiLSTM-CRF for entity recognition, material parsers, etc.) to codify synthesis procedures into structured recipes; required substantial annotation and a multi-step extraction workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / Solid-state synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Codified synthesis recipes and procedural patterns (structured extraction of steps/conditions rather than derived quantitative laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Codified recipes encoding synthesis steps and conditions (exact examples not reproduced in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Traditional NLP pipeline (BiLSTM-CRF, material parser, rule-based components).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Required extensive annotation effort and a complex extraction pipeline; contrasted in this paper with the LLM-based few-shot approach that reduces SME annotation needs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Presented as prior work; no head-to-head quantitative comparison performed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4213.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4213.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Park 2022</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mining insights on metal-organic framework synthesis from scientific literature texts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-mining study that extracts insights about MOF synthesis from literature; cited as related work on extracting synthesis information using NLP methods.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mining insights on metal-organic framework synthesis from scientific literature texts</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MOF synthesis text-mining pipeline (Park et al. 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Applied text-mining/NLP techniques to the MOF literature to extract synthesis-related information and derive insights about synthesis trends and practices; methodological specifics are in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / MOF synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Extraction of synthesis patterns, distributions of synthesis details (statistical/empirical patterns rather than formal physical laws).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td>Distributional insights about synthesis conditions for MOFs (not enumerated in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Text-mining / NLP methods (details in Park et al. 2022).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited as prior literature-mining work; differences in methodology relative to the LLM-based approach are discussed but not quantitatively compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as related literature-mining work.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4213.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4213.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zheng 2023 ChatGPT assistant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A study using ChatGPT as a chemistry assistant to support text mining and prediction tasks for MOF synthesis, referenced as an example of LLM-based assistance in materials science pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatGPT chemistry assistant (Zheng et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Leveraged ChatGPT to assist in text-mining tasks and to support prediction/analysis related to MOF synthesis (used as an example of AI chatbots applied to materials extraction/analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (unspecified variant in citing text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials chemistry / MOF synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Text-mined quantitative synthesis data and predictive assistance (no claim of deriving formal laws in this citing paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Prompted ChatGPT used for text mining and assisting predictions (details in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Cited as proof-of-concept of LLM utility; not presented here with quantitative benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as an AI-assisted approach; no head-to-head comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4213.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4213.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs or AI systems being used to extract, discover, or distill quantitative laws, relationships, or patterns from scientific papers or literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatMof 2024</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced system that uses large language models for prediction and generation of MOF structures; included as related work in generative AI for materials discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ChatMof (LLM-based MOF generation/prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An AI system that leverages LLMs to predict and generate hypothetical MOF structures, used as an example of generative AI in reticular materials discovery (focus is generation rather than literature-extraction of quantitative laws).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Materials science / MOF generation</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>law_type</strong></td>
                            <td>Generation/prediction of candidate materials and associated properties (not explicitly extracting physical laws from literature as described in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>law_examples</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_method</strong></td>
                            <td>Generative LLM approaches (details in cited ChatMof paper).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Generative outputs are hypothetical and require SME input to devise lab synthesis; cited as motivation for KEP (linking computationally generated materials to lab protocols).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as related generative AI work; no direct extraction/comparison in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extracting accurate materials data from research papers with conversational language models and prompt engineering <em>(Rating: 2)</em></li>
                <li>Semi-supervised machine-learning classification of materials synthesis procedures <em>(Rating: 2)</em></li>
                <li>Text-mined dataset of inorganic materials synthesis recipes <em>(Rating: 2)</em></li>
                <li>Mining insights on metal-organic framework synthesis from scientific literature texts <em>(Rating: 2)</em></li>
                <li>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis <em>(Rating: 2)</em></li>
                <li>Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4213",
    "paper_id": "paper-273850480",
    "extraction_schema_id": "extraction-schema-97",
    "extracted_data": [
        {
            "name_short": "KEP",
            "name_full": "Knowledge Extraction Pipeline (KEP)",
            "brief_description": "An LLM-enabled, domain-independent pipeline that extracts synthesis protocols and quantitative experimental parameters from unstructured PDF scientific articles using prompt-engineered in‑context learning and structured JSON outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Knowledge Extraction Pipeline (KEP)",
            "system_description": "KEP processes PDFs (via DS4SD) to extract paragraphs, classifies paragraphs as relevant/irrelevant using LLMs with prompt engineering and in‑context learning (ICL) and a small set of examples, then runs an LLM-based information extraction step that outputs structured JSON objects encoding products, reactants, solvents and numerical conditions; it includes an Examples Selection step to choose the best few-shot examples for prompts and a Knowledge Representation module to normalize units and instantiate entities/relationships.",
            "model_name": "flan-t5-xxl-11b, flan-ul2-20b, granite-20b-code-instruct, granite-34b-code-instruct, llama-3-70b-instruct, llama-3.1-405b-instruct, mistral-large, mixtral-8x7b-instruct-v01",
            "model_size": "11B, 20B, 20B, 34B, 70B, 405B, (mistral-large: unspecified in text), mixtral ~47B (13B active)",
            "scientific_domain": "Materials science / Chemistry (reticular materials: MOFs, ZIFs, COFs, zeolites)",
            "number_of_papers": "Corpus: 2,287 CC-BY articles available; experiments: 305 PDFs sampled (producing 325 classified paragraphs; golden dataset: 131 annotated syntheses / 106–131 annotated paragraphs used in experiments depending on step).",
            "law_type": "Empirical experimental numerical data (extraction of quantitative synthesis parameters: temperatures, times, reagent quantities, volumes) — not derivation of physical laws or scaling laws.",
            "law_examples": "Examples extracted as structured values: e.g., \"heated to 100 °C for 24 h\" (reaction temperature = 100 °C; reaction time = 24 h), reagent quantities such as \"Bis(imidazol‑1‑yl)methane (bim) (0.02 mmol)\", solvent volume \"1.0 mL\" and solvent ratio \"DMF/EtOH/H2O (2:1:1, vol)\".",
            "extraction_method": "Text extraction from PDFs using DS4SD → paragraph segmentation → paragraph classification and information extraction performed by open‑source LLMs using prompt engineering with in‑context learning (few-shot examples embedded in prompts); outputs constrained to JSON schema provided in prompts; an Examples Selection routine samples candidate example sets to maximize prompt performance.",
            "validation_approach": "Outputs compared to SME-annotated golden dataset of paragraphs and JSON annotations; structural comparison by JSON key/value pairs; evaluation of paragraph classification via precision/recall/F1 and information extraction via a JSON-structure accuracy metric defined per-key.",
            "performance_metrics": "Paragraph classification: F1 up to 0.98 (e.g., llama-3-70b-instruct, mistral-large); Information extraction: reported structure-accuracy up to 0.96 (llama-3.1-405b-instruct). Reported per-model results in paper tables (e.g., Table 4 and Table 5).",
            "success_rate": "Reported maxima: paragraph classification F1 = 0.98 (≈98%); information extraction accuracy = 0.96 (≈96%) on the golden test sets used.",
            "challenges_limitations": "Performance highly sensitive to the choice of few-shot examples in prompts (Examples Selection required); hallucinations (models adding explanatory text) caused incorrect outputs if instructions were not obeyed; token limitations constrained number/size of examples (smaller context windows limited flan models); no semantic JSON comparison metric implemented (only structural comparison); models differ in prompt-example needs and a large parameter count does not guarantee best performance; pipeline extracts empirical parameters but does not infer scientific laws or workflows step-by-step (not yet implemented).",
            "comparison_baseline": "Compared across multiple open-source LLM families/models (flan, granite, llama, mistral, mixtral) using the same golden dataset; SME annotations used as gold standard. No direct empirical comparison to non-LLM extraction pipelines (e.g., BiLSTM-CRF pipelines) was performed in this work, though those approaches are discussed in related work and suggested for future comparative analyses.",
            "uuid": "e4213.0",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Polak 2024",
            "name_full": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "brief_description": "A referenced study that uses conversational language models and prompt engineering to extract material-centric numeric information (material, value, unit) from research papers, focusing on simpler extraction tasks than those targeted in this paper.",
            "citation_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "mention_or_use": "mention",
            "system_name": "Conversational language-model extraction (Polak et al. 2024)",
            "system_description": "Reported pipeline leveraging conversational LLMs and prompt engineering to extract simple triples (material, numeric value, unit) from text; according to the cited description, the work focused on simple extraction tasks and used zero-shot methods for determining relevance in some steps.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science",
            "number_of_papers": null,
            "law_type": "Simple quantitative extractions (material, numeric value, unit) — data points rather than derived laws.",
            "law_examples": "Not provided in this paper beyond the description 'material, value and unit' extraction.",
            "extraction_method": "Conversational LLMs with prompt engineering; zero‑shot relevance detection mentioned in this paper's discussion.",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited as focusing on simpler extraction tasks compared to the current work; limited scope (material/value/unit) and methodological differences (zero-shot relevance determination) from KEP.",
            "comparison_baseline": "Discussed as related work; no direct comparative metrics provided in this paper.",
            "uuid": "e4213.1",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Huo 2019",
            "name_full": "Semi-supervised machine-learning classification of materials synthesis procedures",
            "brief_description": "A semi-supervised ML approach that classifies inorganic materials synthesis steps using topic modeling and a supervised classifier trained on manually annotated paragraphs, and models step sequences with a Markov chain to produce flowcharts.",
            "citation_title": "Semi-supervised machine-learning classification of materials synthesis procedures",
            "mention_or_use": "mention",
            "system_name": "Semi-supervised synthesis classification pipeline (Huo et al. 2019)",
            "system_description": "Pipeline used Latent Dirichlet Allocation (LDA) for unsupervised topic clustering of synthesis terminology, a random forest classifier trained on hundreds of SME-annotated paragraphs to categorize synthesis types, and a Markov chain to model sequences of synthesis steps for flowchart construction.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science / Inorganic synthesis",
            "number_of_papers": "Used annotations of hundreds of paragraphs (exact paper count not specified in this citing paper).",
            "law_type": "Extraction of procedural patterns and step-sequence models (not explicit quantitative physical laws).",
            "law_examples": "Identification and classification of synthesis steps and typical term clusters (no explicit numerical laws reported in this citing paper).",
            "extraction_method": "Topic modeling (LDA) + supervised random forest classifier + Markov chain sequence modeling.",
            "validation_approach": "Supervised training/validation using hundreds of annotated paragraphs (details in the original Huo et al. paper).",
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Required considerable manual annotation effort (hundreds of paragraphs) and employed a multi-algorithm pipeline; contrasted by the present paper which uses few-shot LLM prompting to reduce annotation needs.",
            "comparison_baseline": "Serves as a methodological baseline in related work; no direct numerical comparison in this paper.",
            "uuid": "e4213.2",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Kononova 2019",
            "name_full": "Text-mined dataset of inorganic materials synthesis recipes",
            "brief_description": "A dataset and pipeline that automatically extracted codified synthesis 'recipes' for solid-state inorganic materials from publications using traditional NLP and text-mining techniques (e.g., BiLSTM-CRF, material parsers).",
            "citation_title": "Text-mined dataset of inorganic materials synthesis recipes",
            "mention_or_use": "mention",
            "system_name": "Text-mining recipe extraction pipeline (Kononova et al. 2019)",
            "system_description": "Automatic extraction pipeline combining paragraph classification and multiple NLP components (BiLSTM-CRF for entity recognition, material parsers, etc.) to codify synthesis procedures into structured recipes; required substantial annotation and a multi-step extraction workflow.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science / Solid-state synthesis",
            "number_of_papers": null,
            "law_type": "Codified synthesis recipes and procedural patterns (structured extraction of steps/conditions rather than derived quantitative laws).",
            "law_examples": "Codified recipes encoding synthesis steps and conditions (exact examples not reproduced in this paper).",
            "extraction_method": "Traditional NLP pipeline (BiLSTM-CRF, material parser, rule-based components).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Required extensive annotation effort and a complex extraction pipeline; contrasted in this paper with the LLM-based few-shot approach that reduces SME annotation needs.",
            "comparison_baseline": "Presented as prior work; no head-to-head quantitative comparison performed here.",
            "uuid": "e4213.3",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Park 2022",
            "name_full": "Mining insights on metal-organic framework synthesis from scientific literature texts",
            "brief_description": "A text-mining study that extracts insights about MOF synthesis from literature; cited as related work on extracting synthesis information using NLP methods.",
            "citation_title": "Mining insights on metal-organic framework synthesis from scientific literature texts",
            "mention_or_use": "mention",
            "system_name": "MOF synthesis text-mining pipeline (Park et al. 2022)",
            "system_description": "Applied text-mining/NLP techniques to the MOF literature to extract synthesis-related information and derive insights about synthesis trends and practices; methodological specifics are in the cited work.",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science / MOF synthesis",
            "number_of_papers": null,
            "law_type": "Extraction of synthesis patterns, distributions of synthesis details (statistical/empirical patterns rather than formal physical laws).",
            "law_examples": "Distributional insights about synthesis conditions for MOFs (not enumerated in this paper).",
            "extraction_method": "Text-mining / NLP methods (details in Park et al. 2022).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited as prior literature-mining work; differences in methodology relative to the LLM-based approach are discussed but not quantitatively compared here.",
            "comparison_baseline": "Mentioned as related literature-mining work.",
            "uuid": "e4213.4",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Zheng 2023 ChatGPT assistant",
            "name_full": "Chatgpt chemistry assistant for text mining and the prediction of mof synthesis",
            "brief_description": "A study using ChatGPT as a chemistry assistant to support text mining and prediction tasks for MOF synthesis, referenced as an example of LLM-based assistance in materials science pipelines.",
            "citation_title": "Chatgpt chemistry assistant for text mining and the prediction of mof synthesis",
            "mention_or_use": "mention",
            "system_name": "ChatGPT chemistry assistant (Zheng et al. 2023)",
            "system_description": "Leveraged ChatGPT to assist in text-mining tasks and to support prediction/analysis related to MOF synthesis (used as an example of AI chatbots applied to materials extraction/analysis).",
            "model_name": "ChatGPT (unspecified variant in citing text)",
            "model_size": null,
            "scientific_domain": "Materials chemistry / MOF synthesis",
            "number_of_papers": null,
            "law_type": "Text-mined quantitative synthesis data and predictive assistance (no claim of deriving formal laws in this citing paper).",
            "law_examples": null,
            "extraction_method": "Prompted ChatGPT used for text mining and assisting predictions (details in cited work).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Cited as proof-of-concept of LLM utility; not presented here with quantitative benchmarks.",
            "comparison_baseline": "Mentioned as an AI-assisted approach; no head-to-head comparison provided in this paper.",
            "uuid": "e4213.5",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ChatMof 2024",
            "name_full": "Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models",
            "brief_description": "A referenced system that uses large language models for prediction and generation of MOF structures; included as related work in generative AI for materials discovery.",
            "citation_title": "Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models",
            "mention_or_use": "mention",
            "system_name": "ChatMof (LLM-based MOF generation/prediction)",
            "system_description": "An AI system that leverages LLMs to predict and generate hypothetical MOF structures, used as an example of generative AI in reticular materials discovery (focus is generation rather than literature-extraction of quantitative laws).",
            "model_name": null,
            "model_size": null,
            "scientific_domain": "Materials science / MOF generation",
            "number_of_papers": null,
            "law_type": "Generation/prediction of candidate materials and associated properties (not explicitly extracting physical laws from literature as described in this paper).",
            "law_examples": null,
            "extraction_method": "Generative LLM approaches (details in cited ChatMof paper).",
            "validation_approach": null,
            "performance_metrics": null,
            "success_rate": null,
            "challenges_limitations": "Generative outputs are hypothetical and require SME input to devise lab synthesis; cited as motivation for KEP (linking computationally generated materials to lab protocols).",
            "comparison_baseline": "Mentioned as related generative AI work; no direct extraction/comparison in this paper.",
            "uuid": "e4213.6",
            "source_info": {
                "paper_title": "Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "rating": 2,
            "sanitized_title": "extracting_accurate_materials_data_from_research_papers_with_conversational_language_models_and_prompt_engineering"
        },
        {
            "paper_title": "Semi-supervised machine-learning classification of materials synthesis procedures",
            "rating": 2,
            "sanitized_title": "semisupervised_machinelearning_classification_of_materials_synthesis_procedures"
        },
        {
            "paper_title": "Text-mined dataset of inorganic materials synthesis recipes",
            "rating": 2,
            "sanitized_title": "textmined_dataset_of_inorganic_materials_synthesis_recipes"
        },
        {
            "paper_title": "Mining insights on metal-organic framework synthesis from scientific literature texts",
            "rating": 2,
            "sanitized_title": "mining_insights_on_metalorganic_framework_synthesis_from_scientific_literature_texts"
        },
        {
            "paper_title": "Chatgpt chemistry assistant for text mining and the prediction of mof synthesis",
            "rating": 2,
            "sanitized_title": "chatgpt_chemistry_assistant_for_text_mining_and_the_prediction_of_mof_synthesis"
        },
        {
            "paper_title": "Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models",
            "rating": 1,
            "sanitized_title": "chatmof_an_artificial_intelligence_system_for_predicting_and_generating_metalorganic_frameworks_using_large_language_models"
        }
    ],
    "cost": 0.018252249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature</p>
<p>Viviane Torres Da Silva vivianet@br.ibm.com 
IBM Research</p>
<p>Krystelle Lionti klionti@us.ibm.com 
IBM Research</p>
<p>Ronaldo Giro rgiro@br.ibm.com 
IBM Research</p>
<p>Geisa Lima geisa.lima@ibm.com 
IBM Research</p>
<p>Sandro Fiorini srfiorini@ibm.com 
IBM Research</p>
<p>Marcelo Archanjo marcelo.archanjo@ibm.com 
IBM Research</p>
<p>Breno W Carvalho brenow@ibm.com 
IBM Research</p>
<p>Rodrigo Neumann rneumann@br.ibm.com 
IBM Research</p>
<p>Anaximandro Souza anaximandrosouza@ibm.com 
IBM Research</p>
<p>João Pedro Souza 
Idiap Research Institute</p>
<p>Gabriela De Valnisio gvalnisio@ibm.com 
IBM Research</p>
<p>Carmen Nilda Paz cpaz@br.ibm.com 
IBM Research</p>
<p>Renato Cerqueira 
IBM Research</p>
<p>Mathias Steiner mathiast@br.ibm.com 
IBM Research</p>
<p>Automated, LLM enabled extraction of synthesis details for reticular materials from scientific literature
EEFCCF60195A40E36BE21F34D8F4F7DA
Automated knowledge extraction from scientific literature can potentially accelerate materials discovery.We have investigated an approach for extracting synthesis protocols for reticular materials from scientific literature using large language models (LLMs).To that end, we introduce a Knowledge Extraction Pipeline (KEP) that automatizes LLM-assisted paragraph classification and information extraction.By applying prompt engineering with in-context learning (ICL) to a set of opensource LLMs, we demonstrate that LLMs can retrieve chemical information from PDF documents, without the need for fine-tuning or training and at a reduced risk of hallucination.By comparing the performance of five open-source families of LLMs in both paragraph classification and information extraction tasks, we observe excellent model performance even if only few example paragraphs are included in the ICL prompts.The results show the potential of the KEP approach for reducing human annotations and data curation efforts in automated scientific knowledge extraction.</p>
<p>Introduction</p>
<p>Reticular materials are a class of crystalline, porous materials made of molecular building blocks that are linked by strong chemical bonds [1].They exhibit exceptional properties due to their highly porous structure, high surface area, tunable pore sizes and morphologies [2].Their versatility is evidenced by a broad range of industrial applications, among them heterogeneous catalysis [3], energy 38th Conference on Neural Information Processing Systems (NeurIPS 2024).arXiv:2411.03484v1[cond-mat.mtrl-sci]5 Nov 2024 storage [4], water treatment [5], chemical sensing [6], heat transfer [7], gas capture [8] and drug delivery [9].Following recent advances in generative AI, several models have been proposed to explore the large chemical space covered by reticular materials [10][11][12][13][14].These models aim to generate reticular structures with optimized properties.Such structures are hypothetical as they have not yet been synthesised and tested in the lab.Devising a synthesis protocol for computationally generated structures requires a subject matter expert (SME).This is, however a challenging task given the large number of possible structures.An AI model that correlates a computationally discovered material with a lab synthesis protocol is, therefore, highly desirable.A first step towards the creation of such a model is building a database of existing synthesis protocols.</p>
<p>One approach for creating such database is applying information extraction techniques to the existing body of scientific literature.A large number of reticular materials have been reported in the literature alongside their respective synthesis protocols [15,16].It is worth noting, however, that overlapping discoveries are common, given that the same material can be produced by means of different synthesis protocols [17].Transfer learning has been suggested as means to improve information extraction on existing corpora of scientific texts related to materials [18].For example, fine-tuning techniques allow for adapting existing general-purpose AI models to specific tasks in domains for which comparatively little data exists.However, recent developments in LLMs have enabled information extraction based on prompt engineering and few-shot learning tasks [19].</p>
<p>In this paper, we propose using large language models (LLMs), without the need for additional training or fine-tuning, for extracting synthesis protocols of reticular materials from scientific literature, i.e., unstructured PDF documents.We use prompt engineering with in-context learning (ICL) [20] for providing in the prompt all the context needed by the LLM to process the instructions.Together with instructions and input data, we provide examples that guide the LLM output production.This technique reduces the risk of hallucination, since all the context needed to execute the instruction is provided within the prompt.Also, it accelerates the process of information extraction because it does not require SME-based annotation of thousands of sentences/paragraphs for fine-tuning the models.</p>
<p>Our domain-independent Knowledge Extraction Pipeline (KEP) uses LLMs for extracting relevant information from PDF documents.The pipeline is composed of four main modules: (i) PDF extractor: processes the PDF to extract the text; (ii) Paragraph classification: processes the text in order to select only the relevant paragraphs (i.e., paragraphs that have the information the user is interested in); (iii) Information extraction: processes the relevant paragraphs and extract the relevant information; and (iv) Knowledge representation: interprets and assigns meaning to the information while representing the related knowledge.The pipeline uses LLMs with prompt-engineering and ICL in two modules, namely paragraph classification and information extraction, which are the focus of this paper.In addition, for identifying the best set of examples to be used in the prompts of these two modules, we propose the Examples selection phase.This phase measures the performance of the LLMs in a given task and, by using different sets of examples, identifies the set to be used for optimal LLM performance.</p>
<p>We have used five families of LLMs in both paragraph classification and information extraction modules and have compared their performance.We note that these open-source LLMs are not domain-specific and were not fine-tuned for our tasks.Our experiments indicate that: (i) even without fine-tuning or training, some of these models have achieved high performance in case ICL was used to provide examples in the prompt; (ii) the examples used in the prompt affect model performance and, hence, must be chosen carefully; and (iii) the same set of examples may lead to varying results if used in different models.Some recent papers share our work's objectives, however, they differ methodologically [19,[21][22][23].For example, Polak et al. (2024) [19] reported a pipeline for extracting information from unstructured text in the material discovery domain using language models.However, the cited work focused on simple extraction tasks, e.g., material, value and unit, while our pipeline is aimed at complex information associated with synthesis protocols that require additional classification.Unlike in our approach which is based on few-shot prompts providing examples for facilitating the information extraction, the cited work applies zero-shot methods for determining the relevance of sentences or paragraphs.Huo et al. (2019) [21] introduced a semi-supervised machine learning approach for classifying inorganic materials synthesis steps in scientific papers.The authors used the Latent Dirichlet Allocation (LDA) unsupervised topic modeling algorithm for clustering terms that are typically used in synthesis descriptions.A random forest classifier, based on annotations of hundreds of paragraphs, categorized the occurring synthesis types.This approach also used a Markov chain for modeling the sequence of steps, creating flowcharts of synthesis procedures.</p>
<p>In Kononova et al. (2019) [22], the authors generated a dataset with "codified recipes" for solid-state synthesis which was automatically extracted from scientific publications using traditional text mining and natural language processing approaches.The authors used the two-step paragraph classification approach described in Huo et al. (2019) [21] for finding paragraphs on solid-state synthesis.The extraction pipeline consisted of several algorithms (BiLSTM-CRF, Material Parser, etc.) for identifying materials related information, including synthesis steps and conditions.Compared to our method, the cited work required considerable annotation effort and employed a less straightforward extraction pipeline.We note that our method relies primarily on the LLM capabilities for text understanding, without specialized tokenizers or entity recognizers.Finally, Park et al. (2022) [23] created a four-step pipeline, with text extraction from XML/HTML or PDF files and classifying relevant paragraphs, performing named entity recognition and, a fully connected multi-layer with dropout as classifier.</p>
<p>Another promising, less related approach is using "AI chatbot agents" for assisting materials scientists in specific pipeline tasks.In reference [24], the authors used prompt engineering for guiding a ChatGPT-based bot to extract MOF synthesis information from various sources.The authors leveraged a bot-like interface for answering questions about synthesis procedures and chemical reactions.In reference [25], the authors leveraged multiple AI assistants, such as LLMs and specific ML algorithms, as lab assistants to support a human SME, enabling productivity levels similar to those of an entire research team.While the approach was not fully automated, it provided a proof-of-concept of how language models can be leveraged for accelerating materials discovery.</p>
<p>The remainder of this paper is organized as follows.Section 2 introduces the use case, Section 3 describes in details the pipeline applied to the use case and Section 4 presents our experiments.Section 5 concludes and presents some future work.</p>
<p>Use Case: Synthesis Protocols of Reticular Materials</p>
<p>With the goal of extracting knowledge about the synthesis of reticular materials, i.e., MOFs, ZIFs, COFs and zeolites, we have searched the scientific literature by using Elsevier's API 1 and downloaded full-text PDFs from the SCOPUS database. 2.Our approach is based on extracting information from PDFs, and not XMLs, since not always a XML file will be available for a given document.Notice that our extraction pipeline (see Section 3) was not created to manipulate only documents available in Elsevier, where their XML files are also provided, but to process any PDF document (including those that are images).</p>
<p>Our search employed the following keywords and wildcard terms to capture relevant references: 'MOF', 'metal organic framework', 'metal-organic framework', 'metal-organic-framework', 'COF', 'covalent organic', 'covalent-organic', 'ZIF', and 'zeolit<em> imidazol</em>'.We further limited the search to articles published in journals within Chemistry, Chemical Engineering, Materials Science, Energy, Engineering, Environmental Science, Physics and Astronomy, and Biochemistry, Genetics, and Molecular Biology, retrieving 6,669 articles.</p>
<p>The results were then filtered, by using the filter provided in the Elsevier API, to include only open-access articles with DOI identifiers from the following publishers: Elsevier (10.1016),Wiley Blackwell (10.1002),The Royal Society of Chemistry (10.1039),American Chemical Society (10.1021), Springer-Verlag (10.1007),Nature Publishing Group (10.1038), and MDPI (10.3390).</p>
<p>To create a public dataset, we finally kept only articles under the CC-BY-4.0 or CC-BY-3.0 licenses, resulting in 2,032 CC-BY-4.0 articles and 255 CC-BY-3.0 articles.These CCBY license papers were selected by performing web-scrapping from the list of DOIs provided by the Elsevier API.Since we are considering only papers with CCBY 3.0 and 4.0 licenses, everyone can retrieve the PDFs.</p>
<p>After collecting the data, we randomly selected 305 articles in PDF format 3 .We then extracted from these PDFs 188 paragraphs describing synthesis protocols, and 137 examples of paragraphs not describing synthesis protocols (a total of 325 paragraphs).This curated set of paragraphs constitutes our golden collection of classified paragraphs.For details about how those paragraphs were extracted, see Section 3.</p>
<p>Subsequently, a team of eleven research scientists (composed of 2 SMEs) annotated each of the synthesis-related paragraphs on a case-by-case basis for extracting the following information: (i) the description of the synthesis product; (ii) the equipment used as an energy source; (iii) the conditions under which the synthesis occurred (e.g., reaction time, reaction temperature, current density); and (iv) the reactants and solvents used, including their descriptions, quantities, and units of measurement.</p>
<p>Intentionally, some paragraphs were selected for annotation by multiple SMEs, leading to some inconsistencies.These inconsistencies were then used to refine the annotation guidelines.The data was reviewed on a case-by-case basis by SMEs using a custom-built graphical interface and compiled in a final set of 131 syntheses descriptions encoded in a JSON format, thereby creating our golden dataset of annotated synthesis information.Table 1 summarizes the data in our golden dataset.</p>
<p>Knowledge Extraction Pipeline (KEP)</p>
<p>KEP is a domain-independent pipeline that helps extract knowledge from unstructured data.It is composed of four main modules: PDF extractor, Paragraph classification, Information extraction and Knowledge representation, as shown in Figure 1.The PDF extractor processes the PDF to extract paragraphs, since we assume that SMEs are interested in paragraphs containing specific information.</p>
<p>The Paragraph classification classifies the extracted paragraphs into relevant or irrelevant, according to the task the SME is interested in.When applying this module to our use case, relevant paragraphs are those describing synthesis protocols of reticular materials.</p>
<p>Information extraction processes the relevant paragraphs and extracts the relevant information.When applying this module to our use case, the relevant information is the synthesis details such as the description of the synthesis product, the experimental conditions (such as reaction time and temperature), and the reagents and solvents used in the synthesis.The final module, Knowledge representation, interprets and assigns meaning to the extracted information while creates the knowledge representation.In the synthesis protocol use case, the knowledge representation is characterized by (i) the normalization of the unities; (ii) by the instantiation of entities of different kinds (such as productions, reactants and solvents), and (iii) by the instantiation of the relationships (such as used-reactant and used-solvent) that link the entities to the synthesis where they take part.For instance, it is possible to represent that the same reactant is being used in syntheses of two different products and that same product can be synthesized by two different synthesis.</p>
<p>The PDF extractor was implemented using the DS4SD open-source tool 4 that converts unstructured PDF documents into JSON files containing the document elements such as section titles, paragraphs, footnotes, headers, figure captions and tables, etc. DS4SD is also able to process PDFs that are indeed images since it uses an OCR engine to extract text-snippets from those images.</p>
<p>Paragraph classification</p>
<p>Since the goal of this module is the classification of paragraphs as relevant or irrelevant, the prompt to be used in this model should describe the difference between a relevant and an irrelevant paragraph.</p>
<p>In addition, a sentence explicitly instructing the LLM that it should not provide an explanation together with the classification may be required.</p>
<p>Since we are not using zero-shot prompting but ICL prompting, we not only provide the LLM with the aforementioned instructions, but also give it several examples of paragraphs and their corresponding classifications.In Section 4 we demonstrate that, by providing just a few examples in the prompt, the performance of the LLMs tends to increase significantly.Below is an example of instructions used, along with an example of paragraph 5 and its corresponding classification, also provided in the prompt.This paragraph was classified as "S" meaning it is a paragraph describing a synthesis protocol.</p>
<p>Instruction: You are assisting a chemist in classifying paragraphs from scientific articles.Mark the paragraph as 'S' if it describes the components of synthesis protocols for reticular materials, or 'I' if it does not include a synthesis description.After reviewing the examples, classify the given paragraph.Do not add any information or explanation besides 'S' or 'I' in the answer.</p>
<p>Example: "Synthesis of Zn-MOF: Bis(imidazole-1-yl)methane was synthesized analogously to a the procedure reported in [43].All other materials were obtained from commercial sources and were used as received.{[Zn(bim)(bdc)]0.8DMF0.4EtOH0.1H 2 O} n (Zn-MOF).Bis(imidazol-1-yl)methane (bim) (3.0 mg, 0.02 mmol), terephthalic acid (6.6 mg, 0.04 mmol), and Zn(NO3)2•6H2O (7.6 mg, 0.02 mmol) were dissolved in DMF/EtOH/H2O (2:1:1, vol.) mixture (1 mL), placed in a 4 mL screw-cap vial, and heated to 100 °C for 24 h."</p>
<p>Classification: S</p>
<p>Information extraction</p>
<p>The prompt used in the Information extraction module should inform to the LLM the kind of knowledge that should be extracted.In case of a complex structure, the prompt should suggest to the LLM to represent the extracted information following a given schema in well-known format, such as JSON [27].It is reasonable to assume that the LLM will be able to parse this format since it is a commonly used data format that appeared in several documents used to train the LLM.In order to exemplify, find below the instruction we used and the JSON annotation related to the synthesis paragraph presented in Section 3.1.</p>
<p>Instruction: You are assisting a chemist in identifying and extracting descriptions of the synthesis of reticular materials from paragraphs.For each synthesis described in a paragraph, your task is to produce a JSON object that encodes the components involved in the synthesis, following the format provided in the examples.After reviewing the examples, carefully analyze the last paragraph and create a JSON object for each synthesis you find, ensuring that it adheres to the structure and conventions demonstrated.</p>
<p>Example: "Synthesis of Zn-MOF: Bis(imidazole-1-yl)methane was synthesized analogously to a . . .screw-cap vial, and heated to 100 °C for 24 h."</p>
<p>{"output": { "product": { "description": "Zn−MOF", "material_type": "MOF", "conditions": [ {"description": "reaction temperature", "value": 100 , "unit": "oC"}, {"description": "reaction time", "value": 24, "unit": "h"} ] }, "reactants": [ {"description": "Bis(imidazol−1−yl)methane (bim)", "value": 0.02, "unit": "mmol"}, {"description": "terephthalic acid", "value": 0.04, "unit": "mmol"}, {"description": "Zn(NO3)2−6H2O", "value": 0.02, "unit": "mmol"} ], "solvents": [ {"description": "DMF/EtOH/H2O (2:1:1, vol)", "value": 1.0, "unit": "mL"} ] }}</p>
<p>Examples selection</p>
<p>It is well-known that the performance of LLMs to execute a given task is significantly influenced by the set of examples provided in the prompt.In addition, due to the different characteristics of how the LLMs were trained, it is expected that different LLMs will require different sets of examples to achieve their highest performance when executing the same task.</p>
<p>Therefore, the Examples selection step was included and associated with each KEP module that uses LLMs to help on the selection of the best set of prompt examples to be used.Examples selection receives as input the model to be tested, a golden dataset and the number of examples to be selected as examples.It randomly selects from the dataset some instances to be used as examples in the prompt, and all other instances are used to measure the performance of the model.This step is executed for all possible combinations of examples or until the user is satisfied with the performance of the model in one of the executions.The set of examples that leads the LLM to achieve the highest performance is the one selected to be used in the associated KEP module.</p>
<p>Experiments</p>
<p>This section presents the experiments we ran with 5 families of open-source LLMs.None of them were trained or fine-tuned to extract synthesis details from paragraphs or to execute any specific task in the Material Discovery domain.We selected 2 models of each family 6 , prioritized the models that have been fine-tuned using a collection of instructions (not related to our tasks) and chosen the last released ones 7 .Ultimately, the selected models were: (i) flan: flan-t5-xxl-11b, flan-ul2-20b; (ii) granite: granite-20b-code-instruct, granite-34b-code-instruct; (iii) llama: llama-3-70b-instruct, llama-3.1-405b-instruct;(iv) mistral: mistral-large; and (v) mixtral: mixtral-8x7b-instruct-v01. See the description of each model in Appendix A.</p>
<p>Examples selection</p>
<p>Paragraph classification: From the original set of 325 classified paragraphs, we reduced the golden dataset by downselecting only 50 paragraphs to demonstrate that, even when testing the prompt examples selection in a small dataset, it is possible to achieve a good performance on a majority of the tested models.In addition, the use of a small dataset helps demonstrate that the approach does not require the manual classification/annotation of thousands or hundreds of examples.</p>
<p>In the set of 50 paragraphs we ensure that 25 paragraphs are relevant (i.e., classified with "S" and mentioning synthesis protocol) and 25 are irrelevant (i.e., classified with "I" and not mentioning synthesis protocols).We fixed the number of examples to be provided in the prompt to 5, since paragraphs describing synthesis protocols are typically very large and the prompts have a limited number of tokens.Our goal is to find the best set of 5 examples used in the prompt that helps the models achieve their highest performance.The accuracy of each model was measured by using the F1 metric.</p>
<p>For each model, we executed 100 runs by providing in the prompt the instruction mentioned in Section 3.1 and 5 examples randomly selected from 50 possibilities.We tested the output with the remaining 45 paragraphs not provided in the prompt.Table 2 presents the result of our experiments.For each one of the models, the table indicates the number of paragraphs mentioning synthesis protocols ("S") and the number of irrelevant paragraphs ("I") used in both the worst and best prompt together with the F1 value for each case.The models with highest performance were flan-t5-xxl-11b, llama-3-70b-instruct, mistral-large and mixtral-8x7b-instruct-v01.Although llama-3-70b-instruct and mistral-large used the same number of relevant paragraphs and the same number of irrelevant paragraphs in their best cases, their prompts share only one paragraph (see Table 6 in Appendix B).When testing the best prompt for mistral-large in llama-3-70b-instruct by using the same 45 testing examples, the performance of the model did not achieve F1=1.0, but F1=0.98.Although it is a small difference, it demonstrate that, different LLMs may need different examples in their prompts to achieve their highest performance.The models with worst performance were flan-ul2-20b and llama-3.1-405b-instruct.Although we included in the prompt a sentence stating that the answer should only include "S" or "I", their answers often also include an explanation; which we considered to be a hallucination and, thus, an incorrect answer.</p>
<p>Information extraction:</p>
<p>The golden dataset used in this step is the 25 paragraphs mentioning synthesis protocols used in the previous step together with their coresponding JSON annotations.Different from the previous step, here we fixed the number of examples used in the prompt to 2, since the JSON annotation is being provided together with the paragraph, which significantly increases the number of tokens.Even with only 2 examples, flan-t5-xxl and flan-ul2 could not be tested since their prompt+result do not accept so many tokens 8 .</p>
<p>The experiment begun by randomly selecting 2 paragraphs+JSON to be used in the prompt for each one of the 6 models.For each model, we executed 100 runs by providing in the prompt the instructions mentioned in Section 3.2 and the 2 examples of paragraph+JSON randomly selected from 25 possibilities.We tested the performance of the model with each prompt by using the 23 paragraphs that were not provided as examples in the prompt.The results are presented in Table 3.</p>
<p>To compare the JSON annotations provided by the LLM with the JSON annotations included in the golden dataset, a structure analysis based on each JSON key (i.e., name/value pair) was defined 9 .The models that achieved the highest accuracy were llama-3-70b-instruct, llama-3.1-405b-instructand mistral-large.However, it is important to notice that all of them achieved an accuracy higher than 0.84 even using only two examples in the prompt.Similar to what happened in the previous step, the experiments illustrate the influence of the examples in the accuracy of the model (E.g.llama-3.1-405binstructworst case was 0.53 and best case was 0.94).In addition, one of the paragraphs presented in the worst case of mistral-large appeared in the best case of mixtral-8x7b-instruct-v01 (see Table 7 in Appendix B).Two related models that have the same example in opposite scenarios.Moreover, it is important to highlight that the two granites, the two llamas, and mixtral-8x7b-instruct-v01 included in their worst scenarios the same paragraph (see Table 7 in Appendix B).It may indicate that there are examples that really do not help the models on executing their tasks.</p>
<p>Paragraph classification</p>
<p>After selecting the final set of five examples that maximize the performance of each model, the paragraph classification module was tested by using the entire golden dataset of 275 paragraphs (325 minus the 50 used for prompt selection).For each model, the prompt was composed of the instructions mentioned in Section 3.1 and the best set of examples selected for that model, as presented in Section 4.1.Table 4 summarizes the results for each model in terms of Precision, Recall, and F1 achieved with the best prompt.Llama-3-70b-instruct and mistral-large achieved F1=0.98.Although llama-3.1-405b-instruct and flan-ul2-20b have more parameters than the other model of their families, their performances were worse.It occurred due the hallucination mentioned in the Example section step.Excluding granite-20b-code-instruct, all the models achieved F1&gt;0.84, which is very good accuracy given that only five examples were provided in the prompt to these models.</p>
<p>Information extraction</p>
<p>This module was tested by using the golden dataset of 106 annotated paragraphs (131 minus the 25 used for prompt selection).For each model, the prompt was composed of the instructions mentioned in Section 3.2 and the best set of examples selected for that model, as presented in Section 4.1.</p>
<p>Table 5 summarizes the results of our experiments.The model that achieved the highest accuracy (0.96) was llama-3.1-405b-instruct,which is the biggest one.Other four models also achieved a very similar and high performance (mixtral-8x7b-instruct-v01, mistral-large, llama-3-70b-instruct and granite-34b-code-instruct).Notice that the smallest model (granite-20b-code-instruct) was the one that achieved the lower performance.The high accuracy achieved by the biggest models when compared to the smallest one is expected due to the complex of the task that involves the creation of a correct JSON.When considering both the Paragraph classification and Information extraction modules, the three models with highest performance and, thus, those that should be considered to be used in KEP to process all the selected papers mention in Section 2 are: llama-3-70b-instruct, mistral-large and mixtral-8x7b-instruct-v01.</p>
<p>Table 5: Experiments for the Information extraction module (best results highlighted in bold).Model Accuracy granite-34b-code-instruct 0.93 granite-20b-code-instruct 0.84 llama-3-70b-instruct 0.93 llama-3.1-405b-instruct0.96 mistral-large 0.95 mixtral-8x7b-instruct-v01 0.94</p>
<p>Conclusions and Future Research</p>
<p>In summary, we present a knowledge extraction pipeline for synthesis protocols of reticular materials that significantly reduces SME based classification and annotation tasks related to the training or finetuning of machine learning models.Our experimental results indicate that LLMs can achieve high performance with a limited set of examples within the prompt, even without training or fine-tuning the models for the specific domain.For example, by including five representative paragraphs in the prompt, we have reproducibly achieved F1=0.98 in paragraph classification tasks.In information extraction tasks, we have used two paragraphs + JSON and llama-3.1-405b-instructfor achieving Accuracy=0.96.</p>
<p>Our results highlight the necessity of testing different examples to be used in the prompt as this variation strongly influences model performance.For instance, in the Paragraph classification module, the performance of mixtral-8x7b-instruct-v01, one of the best models in our study, ranges from F1=0.61 to F1=1.0.In addition, the experiments show that different LLMs may require different sets of examples for achieving top performance.Although both llama-3-70b-instruct and mistral-large included four synthesis paragraphs and one irrelevant paragraph in their best set of examples, llama-3-70b-instruct has not achieved its highest performance with the best prompt chosen for mistral-large.Finally, a huge number of parameters in the model does not necessarily guarantee a superior model performance.Both flan-ul2 and llama-3.1-405b-instructfailed to achieve top performance in the classification of paragraphs if compared to other models of the same family.</p>
<p>Future research work should include comparative analyses with nonLLM methods in view of extraction time and quality, as well as measuring LLMs' performance for different materials applications.For creating a dataset of synthesis protocols for reticular materials, future research should address the following: (i) refine JSONs comparison: The creation of metrics for semantically comparing JSONs is needed to validate if the output of the model is structurally comparable with the golden dataset, and if it should be considered a valid JSON; (ii) workflow extraction: The extension of the Information extraction module for extracting the synthesis workflow step-by-step; and (iii) increase use case coverage: The application of KEP to all paragraphs extracted from the selected 2,287 papers.Once processed, the resulting data set should be explored for analyzing the distributions of synthesis details made available in the scientific literature.</p>
<p>[</p>
<p>A Models Description</p>
<p>Flan-T5 [28]is a variant of the T5 (Text-to-Text Transfer Transformer) model, further fine-tuned using a mixture of instruction-based learning tasks.Like the original T5, Flan-T5 leverages a transformer architecture, specifically designed for text-to-text tasks, which means it treats both the input and output as text sequences, regardless of the task (e.g., translation, summarization, question-answering).</p>
<p>The "Flan" component (Fine-tuned LAnguage Net) introduces instruction tuning, where the model is exposed to a variety of natural language instructions during its fine-tuning phase.This method allows the model to generalize better across tasks by learning to follow explicit human instructions.</p>
<p>In essence, Flan-T5 adapts the standard pre-training and fine-tuning methods of T5 but adds an additional layer of task diversity through its instruction-based training.This approach enhances its performance on zero-shot and few-shot learning tasks, making it more versatile for a wide range of NLP applications.</p>
<p>Flan-UL2 (Unified Language Learner) [29] is a variant of the UL2 architecture, designed for improved instruction-based fine-tuning similar to Flan-T5.UL2 is an advanced architecture that introduces a novel pre-training method utilizing a mixture of denoising tasks with different difficulty levels.This approach allows the model to adapt to a wider range of NLP tasks by balancing between simple and complex learning objectives.In the case of Flan-UL2, this model takes UL2 and further enhances it with instruction tuning, similar to the Flan-T5 approach.It is trained on a large variety of instruction tasks, making it highly proficient at zero-shot and few-shot learning across many tasks, such as summarization, translation, and question answering.The model's ability to generalize across these tasks is further improved by the fine-tuning process with diverse datasets of instructions, allowing it to better understand human-like queries and execute complex tasks.This makes Flan-UL2 particularly powerful for applications requiring high versatility and adaptability in natural language understanding.</p>
<p>Granite-20B-Code-Instruct and Granite-34B-Code-Instruct [30] are part of the Granite family of large language models (LLMs) designed specifically for code-related tasks.Both models are finetuned versions of their respective base models, Granite-20B-Code-Base and Granite-34B-Code-Base, using instruction-based datasets to improve their ability to follow natural language instructions.These models, developed by IBM Research, are built for tasks such as code generation, bug fixing, code explanation, and translation across a wide range of programming languages, making them versatile tools for code-centric applications.Granite-20B-Code-Instruct, with 20 billion parameters, was trained on trillions of tokens from various sources, including high-quality code, mathematical data, and instructional prompts.Its fine-tuning emphasizes logical reasoning and problem-solving, with a focus on generating and explaining code, alongside supporting tasks like API calling and debugging .Granite-34B-Code-Instruct, with 34 billion parameters, extends these capabilities by being a more computationally powerful model, trained on a larger and more diverse dataset of code instructions.It can handle more complex coding tasks and demonstrates state-of-the-art performance across benchmarks for code synthesis, explanation, and debugging .Both models are decoder-only architectures, optimized for generating human-readable code outputs from natural language inputs, and are trained with instruction tuning to improve their accuracy in code-based applications.</p>
<p>Llama-3-70B-Instruct [31] is part of Meta's Llama 3 family of large language models, specifically designed for instruction-following tasks.The model contains 70 billion parameters and is optimized for generating text in response to user prompts.It is a decoder-only model, which uses an optimized transformer architecture.The instruction-tuned version of Llama-3-70B benefits from Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF) to align its outputs with human preferences for helpfulness and safety.This fine-tuning process makes it particularly suitable for assistant-like applications, such as chatbots and task-oriented dialogue systems.Llama- Mixtral-8x7B-Instruct-v0. 1 [34] is an advanced sparse mixture-of-experts (SMoE) model developed by Mistral AI.It incorporates a unique architecture where each layer contains eight experts (feedforward blocks), but only two are activated for each token during inference.This selective processing allows the model to manage a large number of parameters-47 billion in total-while only using 13 billion active parameters per token, which significantly reduces computation costs during inference.Mixtral-8x7B-Instruct has been fine-tuned for instruction-following tasks through a combination of supervised fine-tuning (SFT) and Direct Preference Optimization (DPO).This model excels in benchmarks such as MMLU and GSM8K, matching or outperforming larger models like GPT-3.5 Turbo and Llama 2 70B in several areas, particularly code generation, reasoning, and multilingual tasks.Its ability to handle long sequences with a 32k token context window makes it highly effective for long-range information retrieval and complex prompts.</p>
<p>B Examples selection</p>
<p>Paragraph classification Table 6 shows excerpts of JSONs with best and worst paragraphs selected as examples for each model.It is possible to see that few paragraphs appear in more than one prompt.Information extraction Table 7 shows the JSONs that include the best and worst paragraphs selected as examples for each model.</p>
<p>Figure 1 :
1
Figure 1: Knowledge Extraction Pipeline (KEP) with the four KEP modules highlighted in gray color.Also shown are the respective inputs and outputs</p>
<p>Table 1 :
1
Overview of golden dataset
Synthesis Not Synthesisparagraphs classified 188137annotated paragraphs 131-</p>
<p>Table 2 :
2
The best-case (highlighted in bold) and worst-case (underlined) scenarios in the selection of examples to be used in the prompt of the Paragraph classification module.
ModelWorstBest#S#I F1#S#IF1flan-t5-xxl-11b140.93 321.0flan-ul2-20b320.0140.98granite-34b-code-instruct 140.30 230.92granite-20b-code-instruct 230.32 230.74llama-3-70b-instruct140.71 411.0llama-3.1-405b-instruct320.0320.95mistral-large230.76 411.0mixtral-8x7b-instruct-v01 320.61 321.0</p>
<p>Table 3 :
3
The best-case and worst-case scenarios in the selection of examples of the Information extraction module.The best results are highlighted in bold and the worst results are underlined.
ModelWorst accuracy Best accuracygranite-34b-code-instruct 0.700.93granite-20b-code-instruct 0.650.84llama-3-70b-instruct0.540.95llama-3.1-405b-instruct0.530.94mistral-large0.220.94mixtral-8x7b-instruct-v01 0.700.93</p>
<p>Table 4 :
4
Experiments for the Paragraph classification module (best results highlighted in bold).
ModelPrecision Recall F1flan-t5-xxl-11b0.980.960.97flan-ul2-20b0.960.960.96granite-34b-code-instruct 0.870.830.84granite-20b-code-instruct 0.750.700.72llama-3-70b-instruct0.980.980.98llama-3.1-405b-instruct0.980.830.88mistral-large0.980.980.98mixtral-8x7b-instruct-v01 0.950.930.94</p>
<p>33] H.-C. Tsai, Y.-F.Huang, and C.-W. Kuo, "Comparative analysis of automatic literature review using mistral large language model and human reviewers," Sciety, 2024.[Online].Available: https://sciety.org/articles/activity/10.21203/rs.3.rs-4022248/v1[34] M. AI, "Sparse mixture of experts in large language models: Mixtral 8x7b," arXiv preprint arXiv:2401.04088,2024.[Online].Available: https://arxiv.org/abs/2401.04088</p>
<p>[32]B-Instruct was trained on an extensive corpus of 15 trillion tokens from publicly available datasets and supports a wide range of use cases, including multilingual text generation and code-related tasks.It incorporates improvements like Grouped-Query Attention (GQA) for faster inference and an expanded 8,192 token context window, allowing it to handle longer inputs effectively.The model has been tested extensively for safety, and Meta has integrated safeguards to limit misuse, including rigorous red teaming and cybersecurity assessments.The model is available under the Meta Llama 3 Community License for both commercial and research applications.It's praised for outperforming other models in several benchmarks, demonstrating significant advancements in multilingual dialogue capabilities and code generation.Llama 3.1-405B-Instruct[32]is the largest model in the Llama 3.1 series by Meta, designed to provide state-of-the-art performance in multilingual dialogue and complex instruction-following tasks.With 405 billion parameters, it utilizes a transformer-based, decoder-only architecture optimized for extensive text generation tasks.It introduces enhancements in context handling, supporting up to 128,000 tokens, which makes it ideal for tasks like document summarization and long-context conversation .This model is fine-tuned using a combination of Supervised Fine-Tuning (SFT) and Reinforcement Learning with Human Feedback (RLHF), enabling it to align better with human preferences and improve the safety and helpfulness of its outputs .Llama 3.1-405B was trained on a mixture of publicly available datasets containing approximately 15 trillion tokens, and its fine-tuning included more than 25 million synthetically generated instruction-based examples .Furthermore, it offers improved multilingual support beyond English, covering languages like German, French, Italian, Portuguese, Hindi, Spanish, and Thai .The model is open-source and available under Meta's custom open model license, encouraging use in both research and commercial applications .Mistral AI's large language models, particularly Mistral Large 2 [33], represent significant advancements in both computational efficiency and reasoning capabilities.This model, featuring 123 billion parameters, is designed for tasks that require extensive reasoning, such as multilingual text processing, code generation, and mathematical problem-solving.With support for over 80 coding languages and a context window of 128,000 tokens, it excels in handling large documents and long, complex inputs.Mistral Large 2 is particularly strong in benchmarks like MMLU (Massive Multitask Language Understanding), where it achieves an accuracy of 84</p>
<p>Table 6 :
6
(continued)</p>
<p>https://github.com/ElsevierDev/elsapy
https://www.scopus.com
articles with at least one paragraph describing a synthesis protocol and 134 articles without any synthesis protocol description.4 https://ds4sd.github.io/
Paragraph extracted from[26].
Exceptions: mistral and mixtral
Exception: llama-3-70b-instruct selected instead of llama-3-1-70b-instruct since it has a highest performance in the tasks we are testing.
Both flan models accept only 4,096 when comparing to llama that accepts8,192 <br />
To create a more fine-grained comparison between the JSONs, it would be necessary to compare their semantics and not only their structures, as different structures could have the same meaning.</p>
<p>Reticular synthesis and the design of new materials. O M Yaghi, M O'keeffe, N W Ockwig, H K Chae, M Eddaoudi, J Kim, Nature. 42369412003</p>
<p>Digital reticular chemistry. H Lyu, Z Ji, S Wuttke, O M Yaghi, Chem. 692020</p>
<p>Metal-organic frameworks in heterogeneous catalysis: recent progress, new trends, and future perspectives. A Bavykina, N Kolobov, I S Khan, J A Bau, A Ramirez, J Gascon, Chemical reviews. 120162020</p>
<p>Metal-organic frameworks for batteries. R Zhao, Z Liang, R Zou, Q Xu, Joule. 2112018</p>
<p>Metal-organic frameworks as platforms for the removal of per-and polyfluoroalkyl substances from contaminated waters. R Li, N N Adarsh, H Lu, M Wriedt, Matter. 5102022</p>
<p>Metalorganic framework materials as chemical sensors. L E Kreno, K Leong, O K Farha, M Allendorf, R P Van Duyne, J T Hupp, Chemical reviews. 11222012</p>
<p>High-throughput screening of hypothetical metal-organic frameworks for thermal conductivity. M Islamov, H Babaei, R Anderson, K B Sezginel, J R Long, A J Mcgaughey, D A Gomez-Gualdron, C E Wilmer, Computational Materials. 91112023</p>
<p>Progress toward the computational discovery of new metal-organic framework adsorbents for energy applications. P Z Moghadam, Y G Chung, R Q Snurr, Nature Energy. 922024</p>
<p>Porous metal-organic-framework nanoscale carriers as a potential platform for drug delivery and imaging. P Horcajada, T Chalati, C Serre, B Gillet, C Sebrie, T Baati, J F Eubank, D Heurtaux, P Clayette, C Kreuz, Nature materials. 922010</p>
<p>Inverse design of nanoporous crystalline reticular materials with deep generative models. Z Yao, B Sánchez-Lengeling, N S Bobbitt, B J Bucior, S G H Kumar, S P Collins, T Burns, T K Woo, O K Farha, R Q Snurr, Nature Machine Intelligence. 312021</p>
<p>Inverse design of metal-organic frameworks for direct air capture of co 2 via deep reinforcement learning. H Park, S Majumdar, X Zhang, J Kim, B Smit, Digital Discovery. 342024</p>
<p>A generative artificial intelligence framework based on a molecular diffusion model for the design of metal-organic frameworks for carbon capture. H Park, X Yan, R Zhu, E A Huerta, S Chaudhuri, D Cooper, I Foster, E Tajkhorshid, Communications Chemistry. 71212024</p>
<p>Chatmof: an artificial intelligence system for predicting and generating metal-organic frameworks using large language models. Y Kang, J Kim, Nature Communications. 15147052024</p>
<p>Discovery of novel reticular materials for carbon dioxide capture using gflownets. F Cipcigan, J Booth, R N B Ferreira, C R Santos, M Steiner, Digital Discovery. 332024</p>
<p>Development of a cambridge structural database subset: a collection of metal-organic frameworks for past, present, and future. P Z Moghadam, A Li, S B Wiggin, A Tao, A G Maloney, P A Wood, S C Ward, D Fairen-Jimenez, Chemistry of Materials. 2972017</p>
<p>Targeted classification of metal-organic frameworks in the cambridge structural database (csd). P Z Moghadam, A Li, X.-W Liu, R Bueno-Perez, S.-D Wang, S B Wiggin, P A Wood, D Fairen-Jimenez, Chemical science. 11322020</p>
<p>Advances, updates, and analytics for the computation-ready, experimental metal-organic framework database: Core mof 2019. Y G Chung, E Haldoupis, B J Bucior, M Haranczyk, S Lee, H Zhang, K D Vogiatzis, M Milisavljevic, S Ling, J S Camp, Journal of Chemical &amp; Engineering Data. 64122019</p>
<p>Data-driven materials research enabled by natural language processing and information extraction. E A Olivetti, J M Cole, E Kim, O Kononova, G Ceder, T Y , .-J Han, A M Hiszpanski, 10.1063/5.0021106Applied Physics Reviews. 744131712 2020</p>
<p>Extracting accurate materials data from research papers with conversational language models and prompt engineering. M P Polak, D Morgan, 10.1038/s41467-024-45914-8Nature Communications. 15115692024</p>
<p>The learnability of in-context learning. N Wies, Y Levine, A Shashua, Advances in Neural Information Processing Systems. A Oh, T Naumann, A Globerson, K Saenko, M Hardt, S Levine, Curran Associates, Inc202336651</p>
<p>Semi-supervised machine-learning classification of materials synthesis procedures. H Huo, Z Rong, O Kononova, W Sun, T Botari, T He, V Tshitoyan, G Ceder, 10.1038/s41524-019-0204-1Computational Materials. 5162Jul 2019</p>
<p>Text-mined dataset of inorganic materials synthesis recipes. O Kononova, H Huo, T He, Z Rong, T Botari, W Sun, V Tshitoyan, G Ceder, 10.1038/s41597-019-0224-1Scientific Data. Oct 20196203</p>
<p>Mining insights on metal-organic framework synthesis from scientific literature texts. H Park, Y Kang, W Choe, J Kim, Journal of Chemical Information and Modeling. 6252022</p>
<p>Chatgpt chemistry assistant for text mining and the prediction of mof synthesis. Z Zheng, O Zhang, C Borgs, J T Chayes, O M Yaghi, Journal of the American Chemical Society. 14532622023</p>
<p>Chatgpt research group for optimizing the crystallinity of mofs and cofs. Z Zheng, O Zhang, H L Nguyen, N Rampal, A H Alawadhi, Z Rong, T Head-Gordon, C Borgs, J T Chayes, O M Yaghi, ACS Central Science. 9112023</p>
<p>Synthesis, crystal structure, and luminescent sensing properties of a supramolecular 3d zinc(ii) metal-organic framework with terephthalate and bis(imidazol-1-yl)methane linkers. V V Matveevskaya, D I Pavlov, A A Ryadun, V P Fedin, A S Potapov, 10.3390/inorganics11070264Inorganics. 117264Jun. 2023</p>
<p>ECMA-404: The JSON data interchange syntax, ECMA International Std. 2017404</p>
<p>Scaling instruction-finetuned language models. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, arXiv:2210.114162022arXiv preprint</p>
<p>Ul2 20b: An open-source unified language learner model. Y Tay, H W Chung, L Hou, B Zoph, S Borgeaud, P He, S Narang, W Fedus, D G Patil, arXiv:2301.075202023arXiv preprint</p>
<p>Granite code models: A family of open foundation models for code intelligence. I Research, 2024IBM Documentation</p>
<p>. Meta 405b, Std, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
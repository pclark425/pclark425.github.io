<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9941 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9941</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9941</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-169.html">extraction-schema-169</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-6098ac103ba222f9c3b089714ef3100357993255</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6098ac103ba222f9c3b089714ef3100357993255" target="_blank">Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</a></p>
                <p><strong>Paper Venue:</strong> ACM Symposium on User Interface Software and Technology</p>
                <p><strong>Paper TL;DR:</strong> A mixed-initiative approach to “validate the validators”—aligning LLM-generated evaluation functions (be it prompts or code) with human requirements, which identifies a phenomenon the authors dub criteria drift: users need criteria to grade outputs, but grading outputs helps users define criteria.</p>
                <p><strong>Paper Abstract:</strong> Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to “validate the validators”—aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, EvalGen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative nature of alignment. In particular, we identify a phenomenon we dub criteria drift: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appear dependent on the specific LLM outputs observed (rather than independent and definable a priori), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9941.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9941.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based evaluator (LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using an LLM (e.g., GPT-4) as an automatic grader that returns pass/fail judgments on LLM outputs via grader prompts or LLM-invoked assertions; used to scale evaluations that would otherwise be human-labeled.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Cross-domain evaluation of LLM outputs (examples in paper: NER on tweets, medical-extraction, product-description generation)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (used to propose criteria and synthesize candidate assertions; LLM-based evaluators may call an LLM grader prompt generated by GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>LLM generates natural-language criteria and multiple candidate implementations per criterion (either Python assertions or grader prompts). LLM-based grader prompts are executed against LLM outputs to produce binary pass/fail decisions; candidate implementations are ranked against human grades and a final implementation per criterion is selected.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Two paper authors exhaustively labeled datasets to establish ground truth for offline experiments (medical: 84, product: 100). Separately, nine industry practitioners interactively graded outputs in the UI with binary 'good'/'bad' (thumbs up/down) labels (typically 5–16 examples during interactions).</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Alignment metric defined in paper: harmonic mean of Coverage and (1 - False Failure Rate) (analogous to F1 on failures). Reported examples: Product pipeline alignment: EvalGen 66.46% vs SPADE 54.35% (Coverage 0.73 vs 0.49; FFR 0.39 both); Medical pipeline alignment: EvalGen 48.29% = SPADE 48.29% (Coverage 0.33; FFR 0.10). (See Table 1; Appendix A.3.)</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>Using LLMs as judges loses inspectability, human tacit judgment, and nuanced, subjective criteria capture: LLM graders inherit LLM fragilities (sensitivity to prompt wording/formatting and ordering biases), can misinterpret or oversimplify natural-language criteria into the 'wrong' implementation, and are harder for practitioners to trust and verify—especially for fuzzy or subjective criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>1) SPADE (fully automated) produced unrealistic or unnecessary assertions (e.g., overly specific phrase detectors) that humans would not have chosen; EvalGen with human input returned fewer, more pragmatic assertions. 2) In the user study, participants reported that LLM-based assertions were harder to trust for production monitoring and could not be inspected/edited as easily as code assertions; some participants preferred to see and select Python code implementations. 3) A concrete misalignment: different participants intended different semantics for a 'no-hashtag entity' criterion (one wanted to forbid extracting the entity at all, another wanted to allow the entity but without the #); the generated code checked only for the hashtag character and matched one user's intent but not the other's.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>Mixed-initiative, human-in-the-loop approaches (EvalGen) mitigate many losses: soliciting a small set of human grades during generation led EvalGen to select smaller assertion sets with equal or better alignment than a fully automated baseline (SPADE). Code-based assertions remain more inspectable and trustworthy for precise checks; LLM-based graders can be pragmatic and useful for fuzzy criteria when humans cannot easily express code. Non-random sampling of examples for human grading (alternating/high-low confidence) improved stability of alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>here; see Sections 2 (motivation), 4 (implementation), 5 (offline comparison, Table 1), 7.4 (trust differences), and Appendix A.3 (alignment definition).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9941.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9941.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Criteria drift</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Criteria drift (dependence of evaluation criteria on observed model outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Phenomenon observed in the user study where human evaluators refine, reinterpret, or add evaluation criteria after seeing model outputs—i.e., evaluation criteria are not fixed a priori but evolve with exposure to outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Interactive evaluation / prompt engineering workflows (demonstrated on NER for tweets; discussed as general to LLMOps tasks including medical and product pipelines)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM evaluators in the system (GPT-4 used to propose criteria/implementations) are affected because criteria depend on outputs; the LLMs themselves do not resolve this human-driven drift.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>EvalGen either offers auto-generated criteria or allows users to grade outputs first; user grades are then used to select candidate assertions. The workflow permits but (in prototype) temporarily limited mid-process edits, exposing how seeing outputs changes human criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Nine industry practitioners graded outputs interactively (some graded before receiving criteria, others after). Participants frequently changed criteria definitions, added new criteria after spotting novel failure modes, and adjusted thresholds (e.g., from 'all proper nouns' to 'most proper nouns').</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>No single numeric agreement metric for criteria drift itself; evidence reported qualitatively in user study (Table 2 shows mixed subjective alignment ratings) and discussed in Section 7.3; drift implies that upfront human labels (used by some calibration methods) may not capture final human preferences.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>If LLM-as-judge systems assume fixed, independent evaluation criteria, they lose alignment with actual human preferences because human criteria are dependent on observed outputs—LLM-only evaluators cannot recreate the iterative human process of discovering and refining criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Participants added new criteria after seeing specific failure modes (e.g., inconsistent handling of hashtags) or reinterpreted existing criteria (e.g., from 'all entities must be proper nouns' to 'most entities should be proper nouns'), demonstrating that criteria stated a priori would have missed these refinements and produced misaligned automatic evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>EvalGen's mixed-initiative design (asking humans to grade while implementations are generated, producing report cards and allowing iteration) partially addresses criteria drift by coupling grading and criteria refinement. Still, the paper notes that drift may persist as outputs or models change (model/prompt drift).</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>here; see Sections 1 (abstract), 3 (design), 7.3 (criteria drift examples) and 8.1 (implications).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9941.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9941.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Trust / interpretability gap</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Trust and interpretability gap between code-based and LLM-based evaluators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Practitioners can inspect, edit, and directly verify code-based assertions (Python), making them easier to trust and adapt, whereas LLM-based grader prompts are opaque, harder to edit, and thus harder to trust for production monitoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Assertion-based evaluation of LLM outputs (applied to NER, medical extraction, product descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>LLM-generated grader prompts (synthesized by GPT-4); code assertions are Python functions synthesized by the same LLM or written/edited by humans.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>For each natural-language criterion, EvalGen generates multiple candidate implementations: some as Python code assertions, others as LLM grader prompts; the system then executes these on outputs and ranks/selects based on alignment with human grades.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Study participants (nine practitioners) inspected assertion implementations and the Report Card; many preferred direct inspection and selection for code assertions and were skeptical of black-box LLM-based evaluators.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Subjective participant Likert ratings on whether assertions aligned with their grades (mixed responses reported in Table 2); alignment metric (Coverage / FFR / Alignment %) used for quantitative selection of assertions.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>When relying on LLM-as-judge rather than code/assertion-based checks, teams lose inspectability and fine-grained control (e.g., per-criterion false-failure tolerances), which reduces trust and complicates deployment; LLM-graders are harder to debug or to adapt to nuanced, tacit human requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Participants asked to see and pick among candidate Python implementations and reported frustration that LLM-based assertions could not be easily edited; several participants explicitly preferred to edit short Python assertions themselves rather than rely on the LLM-chosen implementation. Some participants expressed skepticism about deploying LLM-based validators in production.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>LLM-based graders are useful for 'fuzzy' criteria where writing code is hard; EvalGen's approach of generating multiple candidate implementations and showing alignment statistics let users cull bad LLM-based assertions rather than blindly trusting them.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>here; see Sections 3 (Pick Criteria workflow), 7.2.4 and 7.4 (user preferences and trust differences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9941.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9941.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, focusing on what is lost, degraded, or different when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sensitivity & bias issues</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity and bias of LLM evaluators (ordering bias, prompt-format sensitivity, overgeneralization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM evaluators inherit known LLM fragilities: sensitivity to prompt wording/formatting, bias from set ordering, and tendencies to be over-trusted or to overgeneralize from limited examples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General evaluation methodology for LLM outputs (background/related-work concerns affecting all evaluation tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Generic LLM evaluators as discussed in related work (papers cited include demonstrations of ordering bias and prompt sensitivity); the paper uses this literature to motivate human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_setup</strong></td>
                            <td>Prior work demonstrates LLM evaluators can be biased by the order of candidate responses and sensitive to formatting or small prompt changes; such properties imply instability when LLMs are used as automatic graders without human checks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments used as a necessary corrective: paper's experiments use human labels as ground truth (two authors for offline datasets) and nine practitioners for interactive alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Related work reports biases and sensitivity (citations in paper: issues like ordering bias [30,51], prompt-formatting sensitivity [43]); the present paper references these as reasons to validate LLM evaluators with humans.</td>
                        </tr>
                        <tr>
                            <td><strong>losses_identified</strong></td>
                            <td>LLM-as-judge risks producing systematically biased or inconsistent judgments (e.g., influenced by example ordering or formatting) that differ from human judgment; without human validation, such biases may be hidden and lead to over-reliance on erroneous automatic scores.</td>
                        </tr>
                        <tr>
                            <td><strong>examples_of_loss</strong></td>
                            <td>Paper cites examples in related work (discussed in Section 2): ordering bias when LLMs choose 'best response' among candidates and high sensitivity to prompt formatting; an anecdote about over-reliance on GPT-4 grading itself (MIT exam claim) is referenced as an instance of problematic trust.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_caveats</strong></td>
                            <td>The paper recommends human-in-the-loop validation and mixed-initiative tools (like EvalGen) to detect and remediate these biases; also, automated calibration methods exist in the literature but often require large settled human-labeled datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>paper_reference</strong></td>
                            <td>here; see Section 2 (Related Work) and citations therein (e.g., refs [5], [30], [43], [51]).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences', 'publication_date_yy_mm': '2024-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Calibrating LLM-based evaluator <em>(Rating: 2)</em></li>
                <li>Large Language Models are not Fair Evaluators <em>(Rating: 2)</em></li>
                <li>Judging llm-ao-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>No, GPT4 can't ace MIT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9941",
    "paper_id": "paper-6098ac103ba222f9c3b089714ef3100357993255",
    "extraction_schema_id": "extraction-schema-169",
    "extracted_data": [
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "LLM-based evaluator (LLM-as-a-judge)",
            "brief_description": "Using an LLM (e.g., GPT-4) as an automatic grader that returns pass/fail judgments on LLM outputs via grader prompts or LLM-invoked assertions; used to scale evaluations that would otherwise be human-labeled.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Cross-domain evaluation of LLM outputs (examples in paper: NER on tweets, medical-extraction, product-description generation)",
            "llm_judge_model": "GPT-4 (used to propose criteria and synthesize candidate assertions; LLM-based evaluators may call an LLM grader prompt generated by GPT-4)",
            "llm_judge_setup": "LLM generates natural-language criteria and multiple candidate implementations per criterion (either Python assertions or grader prompts). LLM-based grader prompts are executed against LLM outputs to produce binary pass/fail decisions; candidate implementations are ranked against human grades and a final implementation per criterion is selected.",
            "human_evaluation_setup": "Two paper authors exhaustively labeled datasets to establish ground truth for offline experiments (medical: 84, product: 100). Separately, nine industry practitioners interactively graded outputs in the UI with binary 'good'/'bad' (thumbs up/down) labels (typically 5–16 examples during interactions).",
            "agreement_metric": "Alignment metric defined in paper: harmonic mean of Coverage and (1 - False Failure Rate) (analogous to F1 on failures). Reported examples: Product pipeline alignment: EvalGen 66.46% vs SPADE 54.35% (Coverage 0.73 vs 0.49; FFR 0.39 both); Medical pipeline alignment: EvalGen 48.29% = SPADE 48.29% (Coverage 0.33; FFR 0.10). (See Table 1; Appendix A.3.)",
            "losses_identified": "Using LLMs as judges loses inspectability, human tacit judgment, and nuanced, subjective criteria capture: LLM graders inherit LLM fragilities (sensitivity to prompt wording/formatting and ordering biases), can misinterpret or oversimplify natural-language criteria into the 'wrong' implementation, and are harder for practitioners to trust and verify—especially for fuzzy or subjective criteria.",
            "examples_of_loss": "1) SPADE (fully automated) produced unrealistic or unnecessary assertions (e.g., overly specific phrase detectors) that humans would not have chosen; EvalGen with human input returned fewer, more pragmatic assertions. 2) In the user study, participants reported that LLM-based assertions were harder to trust for production monitoring and could not be inspected/edited as easily as code assertions; some participants preferred to see and select Python code implementations. 3) A concrete misalignment: different participants intended different semantics for a 'no-hashtag entity' criterion (one wanted to forbid extracting the entity at all, another wanted to allow the entity but without the #); the generated code checked only for the hashtag character and matched one user's intent but not the other's.",
            "counterexamples_or_caveats": "Mixed-initiative, human-in-the-loop approaches (EvalGen) mitigate many losses: soliciting a small set of human grades during generation led EvalGen to select smaller assertion sets with equal or better alignment than a fully automated baseline (SPADE). Code-based assertions remain more inspectable and trustworthy for precise checks; LLM-based graders can be pragmatic and useful for fuzzy criteria when humans cannot easily express code. Non-random sampling of examples for human grading (alternating/high-low confidence) improved stability of alignment.",
            "paper_reference": "here; see Sections 2 (motivation), 4 (implementation), 5 (offline comparison, Table 1), 7.4 (trust differences), and Appendix A.3 (alignment definition).",
            "uuid": "e9941.0",
            "source_info": {
                "paper_title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Criteria drift",
            "name_full": "Criteria drift (dependence of evaluation criteria on observed model outputs)",
            "brief_description": "Phenomenon observed in the user study where human evaluators refine, reinterpret, or add evaluation criteria after seeing model outputs—i.e., evaluation criteria are not fixed a priori but evolve with exposure to outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Interactive evaluation / prompt engineering workflows (demonstrated on NER for tweets; discussed as general to LLMOps tasks including medical and product pipelines)",
            "llm_judge_model": "LLM evaluators in the system (GPT-4 used to propose criteria/implementations) are affected because criteria depend on outputs; the LLMs themselves do not resolve this human-driven drift.",
            "llm_judge_setup": "EvalGen either offers auto-generated criteria or allows users to grade outputs first; user grades are then used to select candidate assertions. The workflow permits but (in prototype) temporarily limited mid-process edits, exposing how seeing outputs changes human criteria.",
            "human_evaluation_setup": "Nine industry practitioners graded outputs interactively (some graded before receiving criteria, others after). Participants frequently changed criteria definitions, added new criteria after spotting novel failure modes, and adjusted thresholds (e.g., from 'all proper nouns' to 'most proper nouns').",
            "agreement_metric": "No single numeric agreement metric for criteria drift itself; evidence reported qualitatively in user study (Table 2 shows mixed subjective alignment ratings) and discussed in Section 7.3; drift implies that upfront human labels (used by some calibration methods) may not capture final human preferences.",
            "losses_identified": "If LLM-as-judge systems assume fixed, independent evaluation criteria, they lose alignment with actual human preferences because human criteria are dependent on observed outputs—LLM-only evaluators cannot recreate the iterative human process of discovering and refining criteria.",
            "examples_of_loss": "Participants added new criteria after seeing specific failure modes (e.g., inconsistent handling of hashtags) or reinterpreted existing criteria (e.g., from 'all entities must be proper nouns' to 'most entities should be proper nouns'), demonstrating that criteria stated a priori would have missed these refinements and produced misaligned automatic evaluations.",
            "counterexamples_or_caveats": "EvalGen's mixed-initiative design (asking humans to grade while implementations are generated, producing report cards and allowing iteration) partially addresses criteria drift by coupling grading and criteria refinement. Still, the paper notes that drift may persist as outputs or models change (model/prompt drift).",
            "paper_reference": "here; see Sections 1 (abstract), 3 (design), 7.3 (criteria drift examples) and 8.1 (implications).",
            "uuid": "e9941.1",
            "source_info": {
                "paper_title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Trust / interpretability gap",
            "name_full": "Trust and interpretability gap between code-based and LLM-based evaluators",
            "brief_description": "Practitioners can inspect, edit, and directly verify code-based assertions (Python), making them easier to trust and adapt, whereas LLM-based grader prompts are opaque, harder to edit, and thus harder to trust for production monitoring.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Assertion-based evaluation of LLM outputs (applied to NER, medical extraction, product descriptions)",
            "llm_judge_model": "LLM-generated grader prompts (synthesized by GPT-4); code assertions are Python functions synthesized by the same LLM or written/edited by humans.",
            "llm_judge_setup": "For each natural-language criterion, EvalGen generates multiple candidate implementations: some as Python code assertions, others as LLM grader prompts; the system then executes these on outputs and ranks/selects based on alignment with human grades.",
            "human_evaluation_setup": "Study participants (nine practitioners) inspected assertion implementations and the Report Card; many preferred direct inspection and selection for code assertions and were skeptical of black-box LLM-based evaluators.",
            "agreement_metric": "Subjective participant Likert ratings on whether assertions aligned with their grades (mixed responses reported in Table 2); alignment metric (Coverage / FFR / Alignment %) used for quantitative selection of assertions.",
            "losses_identified": "When relying on LLM-as-judge rather than code/assertion-based checks, teams lose inspectability and fine-grained control (e.g., per-criterion false-failure tolerances), which reduces trust and complicates deployment; LLM-graders are harder to debug or to adapt to nuanced, tacit human requirements.",
            "examples_of_loss": "Participants asked to see and pick among candidate Python implementations and reported frustration that LLM-based assertions could not be easily edited; several participants explicitly preferred to edit short Python assertions themselves rather than rely on the LLM-chosen implementation. Some participants expressed skepticism about deploying LLM-based validators in production.",
            "counterexamples_or_caveats": "LLM-based graders are useful for 'fuzzy' criteria where writing code is hard; EvalGen's approach of generating multiple candidate implementations and showing alignment statistics let users cull bad LLM-based assertions rather than blindly trusting them.",
            "paper_reference": "here; see Sections 3 (Pick Criteria workflow), 7.2.4 and 7.4 (user preferences and trust differences).",
            "uuid": "e9941.2",
            "source_info": {
                "paper_title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
                "publication_date_yy_mm": "2024-04"
            }
        },
        {
            "name_short": "Sensitivity & bias issues",
            "name_full": "Sensitivity and bias of LLM evaluators (ordering bias, prompt-format sensitivity, overgeneralization)",
            "brief_description": "LLM evaluators inherit known LLM fragilities: sensitivity to prompt wording/formatting, bias from set ordering, and tendencies to be over-trusted or to overgeneralize from limited examples.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "task_domain": "General evaluation methodology for LLM outputs (background/related-work concerns affecting all evaluation tasks)",
            "llm_judge_model": "Generic LLM evaluators as discussed in related work (papers cited include demonstrations of ordering bias and prompt sensitivity); the paper uses this literature to motivate human validation.",
            "llm_judge_setup": "Prior work demonstrates LLM evaluators can be biased by the order of candidate responses and sensitive to formatting or small prompt changes; such properties imply instability when LLMs are used as automatic graders without human checks.",
            "human_evaluation_setup": "Human judgments used as a necessary corrective: paper's experiments use human labels as ground truth (two authors for offline datasets) and nine practitioners for interactive alignment.",
            "agreement_metric": "Related work reports biases and sensitivity (citations in paper: issues like ordering bias [30,51], prompt-formatting sensitivity [43]); the present paper references these as reasons to validate LLM evaluators with humans.",
            "losses_identified": "LLM-as-judge risks producing systematically biased or inconsistent judgments (e.g., influenced by example ordering or formatting) that differ from human judgment; without human validation, such biases may be hidden and lead to over-reliance on erroneous automatic scores.",
            "examples_of_loss": "Paper cites examples in related work (discussed in Section 2): ordering bias when LLMs choose 'best response' among candidates and high sensitivity to prompt formatting; an anecdote about over-reliance on GPT-4 grading itself (MIT exam claim) is referenced as an instance of problematic trust.",
            "counterexamples_or_caveats": "The paper recommends human-in-the-loop validation and mixed-initiative tools (like EvalGen) to detect and remediate these biases; also, automated calibration methods exist in the literature but often require large settled human-labeled datasets.",
            "paper_reference": "here; see Section 2 (Related Work) and citations therein (e.g., refs [5], [30], [43], [51]).",
            "uuid": "e9941.3",
            "source_info": {
                "paper_title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences",
                "publication_date_yy_mm": "2024-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Calibrating LLM-based evaluator",
            "rating": 2
        },
        {
            "paper_title": "Large Language Models are not Fair Evaluators",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-ao-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "No, GPT4 can't ace MIT",
            "rating": 1
        }
    ],
    "cost": 0.016722749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences</h1>
<p>Shreya Shankar<br>UC Berkeley<br>Berkeley, California, USA<br>shreyashankar@berkeley.edu</p>
<p>J.D. Zamfirescu-Pereira<br>UC Berkeley<br>Berkeley, California, USA<br>zamfi@berkeley.edu</p>
<p>Björn Hartmann<br>UC Berkeley<br>Berkeley, California, USA<br>bjoern@eecs.berkeley.edu</p>
<p>Aditya G. Parameswaran<br>UC Berkeley<br>Berkeley, California, USA<br>adityagp@berkeley.edu</p>
<h2>ABSTRACT</h2>
<p>Due to the cumbersome nature of human evaluation and limitations of code-based evaluation, Large Language Models (LLMs) are increasingly being used to assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply inherit all the problems of the LLMs they evaluate, requiring further human validation. We present a mixed-initiative approach to "validate the validators"aligning LLM-generated evaluation functions (be it prompts or code) with human requirements. Our interface, Evalgen, provides automated assistance to users in generating evaluation criteria and implementing assertions. While generating candidate implementations (Python functions, LLM grader prompts), EvalGen asks humans to grade a subset of LLM outputs; this feedback is used to select implementations that better align with user grades. A qualitative study finds overall support for EvalGen but underscores the subjectivity and iterative process of alignment. In particular, we identify a phenomenon we dub criteria drift: users need criteria to grade outputs, but grading outputs helps users define criteria. What is more, some criteria appears dependent on the specific LLM outputs observed (rather than independent criteria that can be defined a priori), raising serious questions for approaches that assume the independence of evaluation from observation of model outputs. We present our interface and implementation details, a comparison of our algorithm with a baseline approach, and implications for the design of future LLM evaluation assistants.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Human-centered computing $\rightarrow$ Interactive systems and tools; $\cdot$ Computing methodologies $\rightarrow$ Natural language processing.</li>
</ul>
<h2>KEYWORDS</h2>
<p>language models, auditing, evaluation, interfaces, prompt engineering, active learning</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs) make mistakes-they hallucinate, ignore instructions, and generate outputs that require validation [25]. But validating the behavior of LLMs is challenging. In response, researchers and industry developers have created tools for prompt engineering and auditing that help people with testing outputs more</p>
<p>Ian Arawjo<br>Université de Montréal<br>Montréal, Québec, Canada<br>ian.arawjo@umontreal.ca</p>
<p>systematically $[1,16,23,24,27,36,41,52]$. Such approaches require metrics-a set of functions to automatically score LLM outputs, each typically an assertion with true or false values. These metrics increasingly include calls to "evaluator" LLMs (e.g., $[1,27,52,58]$ ) that act as "judges," grading outputs on qualities hard to articulate in code; for instance, the "conciseness" of an output.</p>
<p>The problem is that, just like the LLMs they evaluate, LLMs that perform evaluations cannot be trusted. These "grader" prompts suffer from the same problems as any other prompt-they are unintuitively sensitive to seemingly minor changes in wording or structure [43]. Yet many existing systems do not include support for verifying the quality of LLM-generated evaluations, asking users to simply trust these outputs. How can users reap the efficiency benefits of LLM-assisted evaluation of LLM outputs, while minimizing or avoiding misalignment? How can we help users validate the validators?</p>
<p>In this paper, we propose a mixed-initiative approach, EvalGen, to address this automated-evaluation alignment problem in the context of prompt engineering. Our approach streamlines the selection of metrics under practical constraints of user effort and latency. Specifically, an LLM suggests criteria in natural language, based on user context (e.g., the prompt under test), that the user can modify. An LLM then generates a pool of candidate assertions for each criterion-either code or LLM grader prompts that output "true" or "false." While the user waits for the LLM to generate candidates, they are asked to grade outputs with a simple "good" (thumbs-up) or "bad" (thumbs-down) voting scheme. These grades then guide the automatic selection of assertions that optimize for alignment with user preferences (Section 4.2). After assertion selection, a final report card reveals the alignment between the chosen assertions and the user's grades. Our approach generalizes beyond the particulars of our specific design, and could be extended to, for instance, update metric implementations with feedback from human preferences, or query the user for finer-grained individual grades.</p>
<p>EvalGEN is embedded inside an existing open-source interface for prompt engineering and auditing, ChainForge [1]. Our alignment algorithm adapts SPADE [45], a fully-automated algorithm for generating Python assertions from the revision history of a prompt. We performed an off-line verification of our human-guided alignment algorithm with SPADE as a baseline [45], then ran a qualitative user study with nine (9) industry practitioners that use</p>
<p>LLMs in production contexts. Because our participants were industry practitioners and thus possibly dealing with NDA-protected data, we offered a task adapted from a real LLM pipeline prompt. Our study design did not impose restrictions on how participants used EvalGen, and users could choose whether to ask the tool to suggest criteria, enter criteria manually, or grade a few LLM outputs first before proceeding onto the criteria specification screen.</p>
<p>Our findings find overall support for EvalGen, with one important caveat. We observed a "catch-22" situation: to grade outputs, people need to externalize and define their evaluation criteria; however, the process of grading outputs helps them to define that very criteria. We dub this phenomenon criteria drift, and it implies that it is impossible to completely determine evaluation criteria prior to human judging of LLM outputs. Even when participants graded first, we observed that they still refined their criteria upon further grading, even going back to change previous grades. Thus, our findings suggest that users need evaluation assistants to support rapid iteration over criteria and implementations simultaneously. Since criteria are dependent upon LLM outputs (and not independent from them), this raises questions about how to contend with criteria drift in the context of other "drifts"-e.g., model drift [4], prompt edits, or upstream changes in a chain. Our findings also (i) underscore the necessity of mixed-initiative approaches to the alignment of LLM-assisted evaluations that also embrace messiness and iteration, and (ii) raise broader questions about what "alignment with user preferences" means for evaluation assistants.</p>
<p>We first position our work (Section 2) and present EvalGen's design (Sec. 3) and implementation details (Sec. 4). We then present two evaluations: an off-line evaluation of our approach (Sec. 5), and a qualitative study with developers (Sec. 6 \&amp; 7). Finally, we suggest implications for future work (Sec. 8).</p>
<h2>2 MOTIVATION AND RELATED WORK</h2>
<p>In response to the popularity of black-boxed LLMs like ChatGPT, prompt engineering (PE) has emerged as a new practice and research area. Alongside PE is the auditing of model behavior in practices such as "red-teaming," used to identify harmful outputs in internal teams to tweak LLM behavior, usually prior to release [32, p.17]. These tasks have spurred the advent of new tools for "LLM operations" (hereafter called LLMOps) and new terminology such as "prompt template," "chain of thought", "chains," etc.</p>
<p>Automating Evaluations of Prompts. When evaluating LLM behavior, users typically send off hundreds or thousands of queries to models. As users reach the limits of manual evaluation, users set up automated evaluation pipelines (Figure 1a) in code or with other LLMs (here we use the term LLM-based evaluators; other work uses terms such as "LLM-as-a-judge" [58] or "co-audit" [18]). ${ }^{1}$ Public PE tools like promptfoo [52] and ChainForge [1] allow users to write their own evaluation metrics to score LLM response quality, and support both code-based and LLM-based evaluators. For instance, in promptfoo users can write a rubric in a config file to specify how an LLM should evaluate responses, and may use pre-created grader prompt templates or customize them; an example is the assertion</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>"The response is not apologetic." Prototypes such as EvalLM [27] and PromptsRoyale [40] also support LLM evaluators, oftentimes exclusively, to help users compare between two prompts. Of PE tools, only EvalLM offers a way to help users calculate the alignment of LLM evaluators with their expectations, but this feature is mentioned only in authors' Design section and is absent from their user study. ${ }^{2}$ At best, users of PE tools inspect LLM-generated evaluator outputs manually to double-check; at worst, the tool hides individual scores entirely. Regardless of aligning implementations of metrics with user preferences, even identifying what metrics to evaluate for custom tasks remains challenging for LLM practitioners, as evidenced by a recent study [37]. While many evaluation tools require users to declare metrics they care about, some prior work [45] and EvalGen employ LLMs to propose custom metrics based on prompts in the user's LLM pipelines.</p>
<p>Over-trust and Over-generalization of LLM Behavior. That tools provide little assistance to validate evaluator quality is alarming, considering that other research shows people tend to over-rely and over-trust AI systems [3, 28, 31, 50]. For instance, in one highprofile incident, researchers from MIT posted a pre-print on arXiv claiming that GPT-4 could ace the MIT EECS exam. Within hours, work by Chowdhuri et al. debunked the study [5], citing problems arising from over-reliance on GPT-4 to grade itself. Other work has found further reasons to be cautious: LLMs asked to choose the best response from a set can be consistently biased by set ordering [30, 51]; and LLMs can be highly sensitive to seemingly innocuous formatting changes [43].</p>
<p>A related problem to over-reliance is over-generalization. Zamfirescu et al. [57] found that users unfamiliar with PE tend to overgeneralize from single failures (causing them to throw out potentially good prompts), rather than having a holistic view of the overall performance of a prompt or chain. This was despite the fact that the interface had support for systematic testing. Similarly, Arawjo et al. [1] found that even people familiar with LLMs (developers, academics in ML) struggled to scale up their evaluations, appearing to over-generalize from a limited number of outputs even after an automated evaluation pipeline was setup. The authors identified three modes of PE on open-domain tasks, with the second, "limited evaluation,"' characterized as users "prototyping an evaluation" [1], and suggested that future work focus on supported users in prototyping evaluation pipelines. Over-generalization is common in traditional ML, too-Kocielnik et al. [29] found that AI systems that showcase subsets of errors, like false positives or false negatives, that have the same accuracy, can lead to vastly different perceptions of accuracy.</p>
<p>Approaches to Aligning LLMs. The HCI community has extensively studied interactive machine learning (iML). In iML, users iteratively develop models by selecting training examples, labeling data, and evaluating model performance [10]. Interfaces that facilitate seamless transitions between these activities result in fewer errors and outputs that better match users' expectations [38, 46]. Some iML interfaces even use ML to assist users, for example, in scaling up labeling-reducing overall user effort required [8]. When using iML concepts for developing LLM pipelines, we must acknowledge a key challenge with LLMs: they often work with little to no</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: EvalGen's approach to assisting users in aligning evaluations. Users iterate through the process of refining criteria and grading. Note that LLM pipeline inputs and outputs are provided by our larger system, and outside the scope of this paper.
specific training data [37]. Users may simply prototype with inputs they imagine the LLM would see, hoping the prompt generalizes.</p>
<p>In the ML and NLP communities, researchers have explored many ways to align LLMs-and their evaluations-to specific user tasks. Many approaches rely on custom model training or finetuning [6], but all strategies heavily rely on humans to identify examples of desirable and undesirable outputs. For instance, Liu et al. [34] demonstrated using annotated LLM outputs-judged on criteria like consistency and relevance-as "few-shot examples" for calibrating LLM-based evaluators. Beyond classical summarization and NLP tasks, in response to the ad-hoc tedium of PE [56], academics and developers are building automated prompt optimization tools, maximizing some user-defined metric on a labeled set of examples. For instance, given some metrics and prompts, Khattab et al. [26] automatically run variations of inserted few-shot examples and LLM-generated rephrasings to optimize the prompt. Other work urges users to write assertions to guide outputs with a mix of code and natural language suggestions [42, 48], but writing these assertions is left up to developers, which is often time-consuming and error-prone. A broader point is that research in LLMOps optimization tends to come from the domains of NLP and ML, where authors generally validate tool performance against benchmark datasets with pre-defined metrics, leaving open the question of how well they perform in the wild on idiosyncratic user tasks (e.g.</p>
<p>EvoPrompt, PromptBreeder, AutoCalibrate [12, 20, 34]). It thus remains unclear how to support developers in their prototyping of evaluations, with the problem becoming even more pressing as the popularity of prompt optimization increases.</p>
<p>Overall, this work reveals that users need more support for (a) prototyping evaluations and (b) validating evaluators of LLM outputs. It also reveals that auditing LLM outputs is far from easy, with humans prone to the dual biases of over-generalization and overreliance. One recent LLM-assisted approach, SPADE [45], makes headway on these issues, helping developers generate Python assertion functions for LLM outputs from prompt history. Here we leverage a similar algorithmic approach to SPADE, but embed it inside an LLM-assisted user interface for evaluator prototyping, EVALGEN, that also assists with criteria generation, measuring alignment with human preferences, and visualizing results.</p>
<h2>3 EVALGEN DESIGN</h2>
<p>In designing EvalGen, our goal was (1) to investigate how to assist developers in creating evaluators to grade LLM outputs, and (2) to help them "validate the validators" through both automated assistance and transparency around how aligned each evaluator is with their expectations. As we covered in Section 2, emerging practices in prompt engineering, LLM auditing, and prompt optimization involve the writing of evaluation functions (metrics) to automate</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The workflow of our EvalGen prototype, from (a) a Prompt Node attached to an empty Multi-Eval Node, showing a Generate Criteria button; (b) the pop-up EvalGen Wizard with three options, Infer, Manual, and Grade First; (c) the Pick Criteria screen, allowing users to describe criteria in natural language and toggle Code or LLM implementations; (d) the Grade screen, with the LLM output (top), input variables (left), and prompt (right), Good and Bad grade buttons, and an "I'm Tired" button (bottom-right) to finish; and finally (e) the Report Card screen, showing the alignment of each criteria and across criteria. Hovering over the alignment shows a confusion matrix. Note that some descriptions and elements have been clipped for space.
grading. These functions may be code- or LLM-based. Based on this context, we set out to design an LLM-powered evaluation assistant that provided developers control over metric criteria, evaluator type (code or LLM), and implementation (i.e., function) generation and selection processes, without asking them to come up with criteria or write code or grader prompts themselves.</p>
<h3>3.1 EvalGen Workflow</h3>
<p>We implemented EvalGen in an existing open-source system for prompt engineering, ChainForge [1], which handles querying multiple LLMs with parametrized prompts, running code- and LLM-based evaluators, plotting scores, chaining, etc. In ChainForge, users write</p>
<p>LLM pipelines by creating nodes of various types to represent their dataflow, such as an "input" node feeding into a "prompt" node. We discuss here only our extension, chiefly a pop-up screen that helps the user define, implement, and validate evaluation functions. We also implemented a new node, Multi-Eval, that allows users to include multiple evaluators in a single node and run all evaluators on the outputs of the pipeline's previous node. Finally, we made improvements to plotting per-criteria scores in the Table View of the LLM output inspector, which can be accessed via the Multi-Eval node. Fig. 1b provides a high-level overview of the EvalGen architecture compared to the typical LLM output evaluation pipeline; we discuss implementation details in Sec. 4.</p>
<p>Figure 2 depicts the workflow of in the context of the EvalGen interface, excluding returning to the main workflow with selected implementations and using the Table View to inspect scores. EvalGEN assists a developer in engineering an evaluation of LLM outputs for a single prompt template. First, EvalGen is accessed as a button on a "Multi-Eval" node we added to ChainForge, which is attached to a Prompt Node (Fig. 2a). A Wizard opens, depicting three options (Fig. 2b): Infer, Manual, and Grade First. A description of EvalGen (not shown) appears above the options. Clicking Infer or Manual leads to the Pick Criteria screen (Fig. 2c); clicking Grade First leads to the Grading screen (Fig. 2d) and asks users to grade at least five outputs, before continuing to the Pick Criteria screen.</p>
<p>The Pick Criteria interface is depicted in Fig. 2c. An LLM has generated criteria suggestions in natural language (Sections 4.1 and 4.2), along with a toggle to prefer a Python code-based or LLM-based evaluator. The user can edit all parts-including the titles or descriptions and type of evaluator-or add new criteria not suggested by the LLM. They can also delete criteria or deselect criteria as needed. Pressing "Implement It" passes the criteria to a second LLM that generates candidate implementations.</p>
<p>While implementations are generated and executed on LLM outputs, users are asked to grade outputs. EvalGen uses these grades to implementations with their preferences. Fig. 2d depicts the Grading screen. A single LLM response is presented to the user, centered in focus in the grader window. The context of the prompt and any input variables (vars) is also present. The user grades outputs with the Good and Bad buttons. Since it may be timeconsuming to ask the developer to grade on a per-criterion basis, for the grader interface we decided on the simplicity of thumbsup/down scoring. Such scoring is a noisy yet informative signal of quality-if a response is given a thumbs-up, it is assumed to pass all criteria, and so if a candidate assertion fails on that response, the candidate is down-ranked in the pool (details in Section 4.1). Users may also click arrows to navigate through outputs (for instance, if they are unsure about a grade, or want to revise a prior grade). ${ }^{3}$</p>
<p>Finally, after the user is done grading and all candidate implementations are generated, executed, and filtered for alignment with grades, a Report Card screen appears with feedback on per-criteria and aggregate measures of alignment with user grades (Fig. 2e). Hovering over per-criteria metrics shows a confusion matrix of how aligned that particular criterion is to the human grades, while the aggregate metrics show the coverage and false failure rate (see Section 5) of the selected subset of EvalGen-generated assertions. The user then returns to the main ChainForge interface (not shown), where the selected implementations are available in a "Multi-Eval" node, titled by criteria. The user can edit or add more criteria, inspect and visualize evaluation results (Fig. 3), etc.; however, this is outside the scope of our design discussion.</p>
<p>Our design reflects trade-offs between developer effort and robust human verification of LLM-generated metrics. The human cannot completely validate an LLM-based evaluator: the point of</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>LLM evaluators is to reduce the effort required by the developer, who would otherwise have to grade outputs manually. The only way to fully align an LLM evaluator would be to ask the user to label all outputs; obviously, this defeats the purpose. Asking the developer to grade some outputs, using some time they would have spent waiting anyway, is the key idea behind our design.</p>
<h2>4 IMPLEMENTATION</h2>
<h3>4.1 System Architecture</h3>
<p>Like prior work on evaluator assistants [27, 45], our solution decomposes evaluations into criteria and assertions (boolean functions that implement the criteria, evaluating outputs). We employ LLMs in generating criteria, based on the prompt [45], and in generating various candidate implementations of each criterion [27, 45]. As users grade, we rank candidate assertions that implement each criteria based on their alignment with user grades (see Appendix A. 3 for how we define alignment). At a high level, alignment is a combination of the assertion's coverage, or ability to fail outputs that the user thinks are bad, and the false failure rate, or ability to not fail outputs that the user thinks are good. We give a formal definition of assertion alignment in Appendix A.3.</p>
<p>EvalGEN's architecture differs from prior work in two main components: first, EvalGen solicits grades from the user on a sample of LLM outputs-requiring some policy to sample LLM outputs to grade. Second, in contrast to SPADE [45], which operates offline and solves an integer linear program to generate the optimal assertion set, EvalGen employs an online (i.e., streaming) system architecture to progressively optimize for the most aligned assertion set. When EvalGen generates a new candidate implementation, it immediately executes this implementation on the set of LLM outputs, culling implementations that are obviously bad (e.g., Python functions with runtime errors). EvalGen maintains a dynamic estimate of selectivity (i.e., pass rate) for each candidate assertion, which in turn informs how grades are sampled in the interface. Our system, as depicted in Figure 1b, is structured into three components:</p>
<p>Criteria Suggestion. We use an GPT-4 to propose various binary evaluation criteria in natural language, such as response length or tone. Developers can select from these suggestions or add their own criteria. For each criterion, developers can select whether it should be evaluated with a purely code-based function or a function that involves calls to another LLM.</p>
<p>Candidate Assertion Synthesis and Execution. Based on the selected criteria, we use GPT-4 to asynchronously generate one or more candidate assertions as code or a grader prompt to be evaluated by an LLM. For each criterion, we issue one call to GPT-4 to generate multiple candidate assertions within markers in a streaming fashion. Every time we detect the end of marker in any GPT-4 response, we parse the candidate assertion and submit it to EvalGen's executor, which will run it on LLM pipeline outputs. Generating multiple candidate assertions improves the probability that there is at least one implementation that aligns with developer expectations. Moreover, for code-based assertions, LLMs occasionally synthesize erroneous functions (e.g., hallucinating a function in a Python library), requiring several candidate assertions.</p>
<p>Grading Sampler. This component samples LLM pipeline outputs for the user to give binary feedback on (thumbs up/down).</p>
<p>When the user grades an LLM output, we update internal estimates of alignment for each candidate assertion, and we sample the next output for the user to grade.</p>
<p>Once the user does not want to grade LLM outputs anymore, or is finished grading all outputs, for each criterion, we select the candidate assertion with the highest alignment with the user's grades. The user can provide a threshold for the false failure rate (as defined in Section 5) such that EvalGen only selects assertions that do not exceed this threshold.</p>
<h3>4.2 Selecting Assertions \&amp; Eliciting Grades</h3>
<p>Once the user selects criteria, EvalGen's executor begins generating candidate assertions and executing them on all LLM outputs. EvalGen maintains dynamic estimates for the following:</p>
<p>Selectivity of Candidate Assertions. The selectivity is the probability that an assertion will classify an LLM output as passing. This probability is adjusted each time EvalGen processes the outcome of executing a candidate assertion on an LLM output.</p>
<p>Confidence Scores for Potentially Poor Outputs. These scores estimate the likelihood that an LLM output is of low quality, without having been explicitly evaluated by the user. The scores are dependent on assertion selectivity and are revised whenever EvalGen evaluates a new assertion against an LLM output, or when a user grades an LLM output directly.</p>
<p>Assertion Alignment. Alignment, or the harmonic mean of coverage and false failure rate (Appendix A.3), is recalculated for each assertion every time the user grades an LLM pipeline output.</p>
<p>See Appendix A for a complete description of assertion selectivity and how it impacts confidence scores; how EvalGen uses these confidence scores to sample grades from the user; formal definitions of alignment; and how EvalGen determines the resulting assertion set based on alignment with grades.</p>
<h2>5 ALGORITHM EVALUATION</h2>
<p>Here we present results on the effectiveness of EvalGen's selection algorithm. Our experiment aimed to understand how soliciting human input at the criteria suggestion stage impacts the size (number of assertions) and alignment of the resulting assertion set. We compared to a baseline, SPADE [45], a fully automated system that generates criteria and candidate assertions and chooses the minimal assertion set that meet coverage and false failure rate constraints.</p>
<h3>5.1 Evaluation Setup</h3>
<p>We developed two LLM pipelines based on real-world datasets. The medical pipeline operates on a dataset of 84 unstructured text transcripts from doctor-patient calls [54], aiming to extract specific information (e.g., symptoms, medication) without revealing any personally identifiable information (PII). This task requires assertions to ensure compliance with privacy laws. The product pipeline involved crafting SEO-friendly descriptions for 100 Amazon products and their reviews [21]. We selected this task because it mirrors actual LLM applications (there are a number of startups using AI to write SEO-optimized product descriptions), and it benefits from assertions: for example, even if there are negative reviews, the descriptions should not say negative things about the products, which would adversely affect the products' sales potential. Our prompts
are presented in Appendix B. For both prompts, the placeholder variables (i.e., transcript and document) represent the input context to inject at pipeline runtime. For each dataset, we executed the corresponding pipeline once for each input using OpenAI's GPT-3.5-Turbo to generate outputs-resulting in 84 medical LLM outputs and 100 product LLM outputs.</p>
<p>Two of the paper authors manually graded all LLM outputs to establish ground-truth labels. Overall, $68 \%$ and $51 \%$ of the outputs were good for the medical and product LLM pipelines, respectively. Common issues included the presence of personal information in the medical pipeline outputs and bad reviews or lengthy content in the product pipeline outputs.</p>
<h3>5.2 Impact of Human Input in the Criteria Generation Step</h3>
<p>Here, we report quantitative and qualitative differences in SPADE's and EvalGen's assertion sets. There are two differences between SPADE and EvalGen in how they generate assertion sets. The first difference is that EvalGen asks the user to add, edit, or remove criteria before generating different candidate assertions, whereas SPADE does not solicit any input from the user about the criteria. The second difference is in the selection of the assertions themselves: given user-confirmed criteria and a sample of grades provided in a UI, EvalGen picks the most aligned assertion per criterion that meets some false failure rate threshold. Meanwhile, SPADE solves an optimization problem to select a minimal assertion set that meet a false failure rate threshold and cover all SPADE-generated criteria.
5.2.1 Evaluation Procedure. We first ran SPADE end-to-end for both LLM pipelines, supplying all labeled LLM outputs to see the resulting assertion sets. We initially set both false failure rate thresholds to $10 \%$. While SPADE met this threshold for the medical pipeline, the product pipeline required adjusting the false failure rate to $40 \%$ to find a viable assertion set. This illustrates the challenge of balancing coverage with false failures, underscoring the need for evaluator systems to effectively navigate these trade-offs.</p>
<p>Subsequently, we ran EvalGen for both pipelines with the same false failure rate thresholds. For the medical pipeline, we defined three evaluation criteria: word count, presence of the six targeted keys, and absence of PII, with the first two implemented via codebased assertions and the last via an LLM evaluator. The product pipeline criteria included absence of negative reviews, absence of links, adherence to markdown format, and word count limitation, with only the first criterion requiring LLM implementation. To create the aligned assertion sets, we provided EvalGen with 16 graded outputs per pipeline instead of all 80-100 graded outputsgiven the impracticality of expecting users to extensively grade in a single session.
5.2.2 Results. Our results in Table 1 show that EvalGen, by incorporating human judgment during criteria selection, achieved equal or better alignment than SPADE with fewer assertions for both pipelines. Specifically, in the product pipeline, EvalGen produced an assertion set less than half the size of SPADE's and increased coverage from $49 \%$ to $73 \%$. This underscores the benefit of involving humans in selecting criteria, as a fully automated tool may generate assertions for criteria that humans may not actually care</p>
<p>about writing assertions for. In the medical pipeline, SPADE added unnecessary assertions, such as one for a neutral tone, not chosen for EvalGen, and split checks for specific keys into more criteria than needed. For the product pipeline, SPADE generated twice the number of assertions compared to EvalGen, some of which were unrealistic, like a Python function designed to flag specific negative phrases such as "never order" and "disappointed" in the output. In contrast, EvalGen returned a more pragmatic assertion for this criterion-an LLM-based validator to ensure the product descriptions remained entirely positive.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Medical</th>
<th style="text-align: center;">Pipeline</th>
<th style="text-align: center;">Product</th>
<th style="text-align: center;">Pipeline</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Metric</td>
<td style="text-align: center;">EvalGen</td>
<td style="text-align: center;">SPADE</td>
<td style="text-align: center;">EvalGen</td>
<td style="text-align: center;">SPADE</td>
</tr>
<tr>
<td style="text-align: left;">Dataset Size</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">100</td>
</tr>
<tr>
<td style="text-align: left;"># Bad Outputs</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">49</td>
</tr>
<tr>
<td style="text-align: left;"># Assertions</td>
<td style="text-align: center;">$\mathbf{3}$</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">$\mathbf{4}$</td>
<td style="text-align: center;">9</td>
</tr>
<tr>
<td style="text-align: left;">Coverage</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">0.33</td>
<td style="text-align: center;">$\mathbf{0 . 7 3}$</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: left;">FFR</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.39</td>
</tr>
<tr>
<td style="text-align: left;">Alignment (\%)</td>
<td style="text-align: center;">48.29</td>
<td style="text-align: center;">48.29</td>
<td style="text-align: center;">$\mathbf{6 6 . 4 6}$</td>
<td style="text-align: center;">54.35</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of EvalGen and SPADE Across Pipelines. With user input at the criteria stage, EvalGen achieves the same or greater alignment with fewer functions.</p>
<h2>6 USER STUDY</h2>
<p>To understand how developers might use EvalGen to build evaluators for LLM pipelines, we asked nine industry practitioners who had prior experience with LLMs to use EvalGen and think aloud.</p>
<p>Recruitment and Participants. We recruited nine industry practitioners via a Twitter post, calling for anyone interested in solving the problem of "who validates the validators." We selected the first nine who had experience coding and building LLM pipelines for some company or product. Participants included software engineers, ML scientists, startup executives, and independent consultants. We focused on developers with experience with LLMs because they can best intuit what features of an assertion assistant they would find useful, compared to their existing workflows.</p>
<p>Procedure. All studies were conducted over Zoom. We first spent 5 minutes establishing rapport and asking participants about their backgrounds and experience. We then introduced the participants to our LLM pipeline in ChainForge, where the pipeline's task was to do named entity recognition (NER) on a dataset of tweets. The LLM used in our pipeline was GPT-3.5-Turbo from OpenAI. The prompt for our LLM pipeline was as follows: You will be doing named entity recognition (NER). Extract up to 3 well-known entities from the following tweet: [tweet_full_text] For each entity, write one sentence describing the person or entity. All the entities you extract should be found in a knowledge base like Wikipedia, so don't make up entities. Return your answer as a bulleted Markdown list, where each bullet is formatted as ' $=$ entity: description'. Do not extract hashtags as entities. We chose this task and set of inputs for three reasons: first, NER is a common, real-world task that language models excel at; second, tweets are short and can be displayed in a UI without any scrolling; third, "NER for tweets" is a problem that many people have studied [15, 33, 49]. We allowed participants to change the task or prompt if they wanted.</p>
<p>Next, we spent a few minutes describing EvalGen's functionality. We showed participants how to view LLM outputs, trigger EvalGen, write an assertion from scratch and add it to their set, run the assertion set on all the LLM outputs, and inspect the Table View plotting results of each assertion on each LLM output. We then gave the participant remote control access to our screen, instructing them to come up with an assertion set for the pipeline. Participants were allowed up to 40 minutes to explore the tool while thinking aloud. We communicated that we were mainly interested in observing their process of creating assertions, not interacting with other features of ChainForge such as comparing different LLM APIs. If the participant had any questions about the interface, we answered them.</p>
<p>After the participant had an assertion set they liked, or ran out of time, we asked them open-ended questions. We first asked them to comment on EvalGen's approach to generating assertions, and whether they felt that the assertions aligned with their grades. We then asked follow-up questions to learn more about why they felt a particular way or found some aspect of assertion alignment challenging. We limited the post-interview to 10 minutes. At the end, we asked participants to rate how they felt about the assertions' alignment on a 7-point Likert scale ( $1=$ strongly disagree, $7=$ strongly agree). Overall, the study ranged from 45 min to 1 h 15 min . Our study was approved by our institutional review board (IRB), and participants generously volunteered their time.</p>
<p>Analysis. We asked participants to think aloud while using the tool, while we took notes on their thoughts and any visible emotions (e.g., delight when EvalGen suggested a criterion they struggled to externalize, or frustration when they could not find a good assertion for a criterion). We also recorded the transcripts for each video call. We employed open and axial coding [13] to identify common themes across the transcripts and notes for each participant. Initially, we coded individual sentences of interest for each participant, then grouped these into broader themes on a perparticipant basis in a second pass of coding. Finally, we consolidated these themes across all participants, reorganizing them as needed.</p>
<h2>7 USER STUDY FINDINGS</h2>
<p>Overall, we found that:</p>
<ul>
<li>Participants felt that EvalGen was a great starting point for assertions, and wanted to-and could-exercise control over EvalGen's assistance.</li>
<li>Participants struggled to align assertions with their preferences due to two main challenges in grading: (i) some criteria are difficult for humans to grade (e.g., under a target word count), and (ii) as they grade more LLM outputs, we observe a criteria drift phenomenon, in which criteria change as participants grade more LLM outputs (both definitions of existing criteria, and changes to the overall set of criteria).</li>
<li>Participants' perceptions of alignment and needs varied based on the evaluator type (i.e., code-based vs. LLM-based).</li>
</ul>
<p>We unpack these findings below. We first describe the typical participant workflow and highlight where participants wanted to exercise control in the process. Then, we discuss the challenges participants faced in aligning assertions with their preferences.</p>
<h3>7.1 Typical Participant Workflow</h3>
<p>All participants $(n=9)$ used the provided task (a prompt template for NER of a dataset of 100 tweets, described in 6). Three (3) participants changed the prompt: of these three, one (1) person changed the task from NER to sentiment analysis. After participants had settled on their own prompt (or decided they wanted to use our prompt), each engaged in roughly the following activities:
(1) Eyeballing LLM outputs: Participants viewed the table of 100 LLM outputs, making sure the outputs seemed reasonable at a glance.
(2) Starting EvalGen: Participants clicked the button to start EvalGen. This wizard presents three options (Fig. 2b): autogenerate criteria, write criteria, and grade outputs first (before generating criteria). 6 participants clicked the autogenerate button; 1 participant wrote a criterion themselves and then clicked the auto-generate button. The remaining 2 participants wanted to grade first (P4, P9).
(3) Grading outputs: Participants who graded LLM outputs first graded between 5 and 10 outputs, with 2 to 4 "thumbsdown" grades. After grading, both participants clicked the button to auto-generate criteria.
(4) Refining criteria: After receiving criteria suggestions from EvalGen, participants removed some suggestions and added 1-2 criteria of their own. They usually left evaluation type (code-based or LLM-based) unchanged from what EvalGen suggested, even if EvalGen suggested a type that did not make sense (e.g., checking word count with an LLM API call, rather than code), which happened rarely.
(5) Grading more outputs: Participants graded outputs while EvalGen generated and evaluated different candidate assertions. Some participants graded continuously for up to 10 minutes; others stopped after 10 grades.
(6) Understanding alignment on graded outputs: Participants viewed the "Report Card" screen, where they all spent a few minutes inspecting the resulting assertion set. Only one participant did not understand what the overall set's coverage and false failure rate meant (P9), but did once the researcher explained it. Participants chose to view the different assertion implementations EvalGen evaluated, including the ones that did not align with their grades.
(7) Eyeballing alignment on ungraded outputs: Returning to the main screen, participants clicked the Run button of the Multi-Eval node to run all assertions on all 100 LLM outputs and inspected the table of results (Fig. 3).
(8) Iterating on criteria: 3 out of 9 participants again opened the EvalGen wizard to iterate on their criteria and assertions (back to step 2), or stopped because they were satisfied or tired. More than half of the participants expressed interest in iterating on their assertions if they had more time.</p>
<h3>7.2 EvalGen as a Starting Point</h3>
<p>We found that participants liked EvalGen as a starting point to generating assertions. They felt that exerting control was necessarysince EvalGen sometimes made mistakes-and they did not mind doing so. P8 said:</p>
<p>This is how I would want a workflow to assist me in evals-basically I want the AI to do $80 \%$ of it, and there can be escape hatches if the AI fails.
Here, we discuss what participants liked and disliked in different components of EvalGen's UI.
7.2.1 LLM-generated criteria alleviates writer's block. Eight (out of 9) participants were pleasantly surprised that suggested criteria reflected the criteria they wanted (all except P3). P4 said, "I get writer's block when thinking about what assertions to write, so this is great." P6 said, "I feel that if I gave my prompt [for my own pipeline], this would create really good criteria automatically." Some participants had negative initial reactions: for example, P7 initially expressed dissatisfaction that 7 criteria were generated, as they had expected less. But after deleting some criteria, they realized that the suggestions covered all the criteria they wanted (i.e., they did not have to add criteria themselves), and stated, "this auto-generation is sweet". P3 found that some EvalGen-suggested criteria did not make sense given that assertions only operate on LLM pipeline input-output pairs, like response latency and response stability over time. P3 did not feel that this was a failure of EvalGen, but rather expressed that LLMs are not perfect and stressed the importance of having control over selected criteria.
7.2.2 Grading outputs first helps refine initial criteria. Participants who graded before selecting criteria (P4 and P9) expressed that they found the grading process very useful. Both participants were happy that EvalGen prompted them to give feedback on why an LLM output was bad. While P9 selected the grading option initially because it seemed like the "option that required the least thinking," they later realized that it was the correct thing to do:</p>
<p>Of the 3 options, [selecting criteria] probably wasn't the best to start with because I couldn't have extracted [all the] rules directly from the prompt. [While grading, I found] some rules that aren't included in the prompt.
Other participants who did not grade before selecting criteria regretted this decision: for example, P2 said, "you should enforce that we all look at at least 20 examples first." We discuss criteria refinement more in 7.3.1.
7.2.3 Users were happy to grade while waiting. Overall, all participants but one (P7) found grading while waiting for EvalGen to finish generating and executing candidate assertions to be a good use of their time. The participant who did not agree said that they would find it useful if, on the grading screen, EvalGen showed what it was doing with the grades. However, three participants found it difficult to enumerate all criteria in their head and grade against all criteria, wanting EvalGen to prompt them to grade per each criteria (P6, P7, P8). Several participants realized they needed to go back and change their grades in EvalGen and were happy they could do this.
7.2.4 Users need more control when reviewing alignment on graded results. In the Report Card screen, participants liked that EvalGen generated multiple assertion implementations for each criterion and picked the most aligned one (i.e., highest coverage within a false failure rate threshold of $20 \%$ ). However, for some criteria, EvalGen</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The Table View, showing inputs, LLM outputs, and evaluation results per criteria for the NER task (Sec. 6).
generated no good assertions, and participants deleted the criteria without complaints. P8 said, "I like that it tries, because maybe there will be a good implementation!" This suggests that, compared to an opaque approaches we discussed in Related Work, seeing alignment scores helped participants cull assertions that were unaligned rather than blindly trusting them. For criteria where EvalGen did not find any good assertion implementations, participants also expressed interest in writing assertions. Some participants said that they had not graded enough responses to get their ideal alignment numbers, so they wanted to go back and grade more (P1, P7). Participants also liked seeing the coverage and false failure rate of the set, and how each assertion contributed to these overall statistics. However, in this screen, some participants expressed interest in seeing per-criterion alignment and asked to provide grades for each criterion (P5, P6, P7, P8). Another place where participants wanted to exercise control but were limited by EvalGen's interface was in the selected assertions: P2 and P5 wanted to change a selected assertion to another candidate assertion. Both P2 and P5 wanted to change code-based assertions: for example, P5 said, "this is a weird implementation of entity count" and preferred another assertion with a small modification. ${ }^{4}$
7.2.5 Users differ in how they assess alignment for ungraded results. Participants really liked viewing the table of assertion results on all LLM outputs, with all participants expressing interest upon first viewing it (Figure 3)-but they verified the results differently. In this table, rows represent LLM outputs, and columns represent assertions. P2 said that this table "earns trust"; P5 said that it was "cool to see all the results on examples [they] didn't grade." P5 initially did not like using a GUI to come up with assertions, and then said that they "prefer looking at this table to [their] current workflow in Jupyter notebooks." Interestingly, only one participant assessed alignment in this table via coverage-by inspecting each row in</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: Ratings (1-7, 7 best) for the statement, "I felt like the assertions aligned with my grades." Responses were mixed.
the table, one at a time-and everyone else assessed alignment via false failure rate: for each column (assertion), they typically looked for rows (LLM outputs) that the assertion returned False. Some participants wanted this table to automatically recompute, with an accompanying visualization, whenever there were changes to the LLM pipeline; for example, P6 said:</p>
<p>One thing I would find cool is if there is a way to easily see how changes to my prompt impact the overall [coverage and false failure rate] scores. Just very quickly being able to visualize how [my prompt edit] changes the classifications on a bunch of [LLM outputs.]
This suggests users want data visualization plots between the original prompt and its revision, akin to LLM Comparator and EvalLM [24, 27]. Note that ChainForge [1] also has visualization plots, but we did not introduce participants to this feature.</p>
<h3>7.3 Alignment is Iterative, Criteria- and Implementation-specific</h3>
<p>Perceptions of the tools' support of alignment were polarized across participants, as shown in Table 2. The main reason why people had low confidence in the assertion report card's alignment with their grades was because they were still uncertain of their own criteria while grading. Participants would say utterances with a questioning tone, like "I guess" and "sure," when grading, indicating their uncertainty (P2, P5, P7, P8, P9). Looking more closely at their interactions, we observed a catch-22 situation: participants needed to externalize criteria in order to grade outputs, but they also needed to grade outputs-providing feedback on why bad outputs were bad-in order to externalize criteria. Here, we explore their challenges.
7.3.1 Criteria drift. Grading outputs spurred changes or refinements in participants' criteria, which we refer to as criteria drift. We observed two types of drift.</p>
<p>First, participants wanted to add new criteria when they observed new "types" of bad LLM outputs (P2, P5, P6, P8, P9). In the EvalGen interface, they could not go back and add new criteria; they had to wait for all candidate assertions to finish executing, move past the report card screen, and start a new EvalGen process.</p>
<p>Second, as participants graded more outputs, we found that they reinterpret existing criteria to better fit the LLM's behavior (P2, P5, P6, P8, P9). For example, P2 and P8 had a "proper noun" criterion, which was supposed to assess that "the entities extracted were proper nouns." At first, they rated as bad any LLM outputs that contained any entity that was not a proper noun. But, after observing that responses had varying numbers of proper nouns, both wanted to change their criteria such that most of the entities were proper nouns, rather than all. P9 appreciated the ability to provide feedback on unsatisfactory outputs before finalizing the initial criteria set, as this process helped them articulate their criteria. Twice, P8</p>
<p>mentioned that they gave a bad grade not because they believed the output was bad, but because they wanted to be consistent with previous grades-good labeling practice, perhaps, but not good for alignment. P7 noticed that in some outputs included hashtags from the original tweet inputs (e.g., #justdoit), while in other cases, the outputs did not use the hashtag symbol but still mentioned the entities referred to by the hashtags (e.g., "Nike" instead of #Nike). This inconsistency led P7 to rethink the criteria definition for including hashtags; specifically, whether it was acceptable for the LLM to replicate entities from hashtags, provided the hashtag was removed. P5 expressed the same uncertainty. "I think it's hard to know until you see it," P7 said.
7.3.2 Users prefer to adjust their grading approach based on the difficulty of evaluating a criterion. Overall, participants generally liked the process of grading LLM responses and feeling like the grades were useful, but they wanted to prioritize grading criteria they felt needed their alignment-especially for LLM-based assertions (P3, P5, P7, P8). For example, P3 expressed that they would trust the assertions more if the EvalGen process allowed them to set different false failure rates per criteria (since LLMs might be bad at evaluating some criteria), instead of one global false failure rate constraint for the entire assertion set:</p>
<p>There are criteria where you can be okay with failing, and then there are other criteria where you are like, 'this must absolutely pass'... [T]here's a [spectrum] of failure as opposed to: it just passes or fails.
Relatedly, some participants expressed that they didn't trust their grades because they themselves couldn't evaluate some criteria as well as an automated solution (P2, P5, P6, P7, P8; we discuss further in Section 7.4). A criterion like word count is hard for humans to assess but easy for a good Python function to evaluate. P8 desired to grade for only one criteria, reasoning that it might improve efficiency ("I generally want to be in the loop for these tests...but I want to put myself in the loop in a way that is efficient.").
7.3.3 What constitutes "alignment" is subjective, especially when converting natural language criteria to code-based assertions. For code-based assertions, EvalGen's interpretation of the criterion (i.e., GPT-4's interpretation) did not match what the participants expected. As such, no matter how many grades the participant gave, all candidate assertions were similarly misaligned. Participants who observed this were confused why their grades seemingly had little impact on some of the chosen assertions (P3, P5, P7, P9). For example, while all participants had a criterion to enforce that there were no entities with hashtags in the output, some participants interpreted this as any hashtags representing entities should not be extracted as entities: e.g., if the output included the hashtag #Nike, P5 did not want Nike to be extracted at all. On the other hand, P9 wanted Nike to be extracted as an entity, but they did not want the LLM output to include the hashtag. Both P5 and P9 got the same code-based assertion for the criterion, which simply checked for the presence of the hashtag character in the output-this assertion did not align with P5's grades, but did with P9's. This particular misalignment can also be viewed as an instance of criteria drift, as described in Section 7.3.1, since P5 was only able to refine the criterion after grading several LLM outputs. For another criterion,</p>
<p>P5 felt that EvalGen could not find a good assertion that aligned with their grades, but also said that they were "lost at what would be a good implementation."</p>
<p>Overall, alignment is not merely a matter of performance (i.e., the idea that "a better LLM would do better"): we observed that misalignment sometimes occurred due to tacit criteria that participants held which was not explicitly explained in natural language. Like prior work has found for cross-LLM comparison [1], this tacit understanding of criteria could be highly subjective and contradictory across participants.</p>
<h3>7.4 Alignment Needs and Preferences Differ for Code vs. LLM Evaluators</h3>
<p>All participants liked EvalGen's ability to assist in generating both code-based and LLM-based assertions, expressing the need for both types of assertions. Both P4 and P6 mentioned that they liked the ability to correct EvalGen's suggested type. Yet participants reasoned about code-based assertions and LLM-based assertions differently: while they liked having assertions of both types, they wanted to construct and iterate on them differently in order to feel like the resulting set of assertions aligned with their preferences.
7.4.1 Users like having control over the evaluation type and thought each evaluation type had a different affordance. Participants seemed to have a clear idea of when a criterion should be evaluated by code, and when it should be evaluated by an LLM. Generally, for formatting checks (e.g., asserting that an output is in Markdown), countbased checks, and checks to include or exclude specific phrases, participants preferred code-based assertions. P6 and P8 expressed that they wanted to use LLM-based assertions for their own LLM pipelines because "most of their criteria was fuzzy." P2 selected criteria to be evaluated via LLM whenever they could not immediately think of a Python function that could implement that criteria. We also observed a participant (P8) preferring LLM-based to code-based assertions: LLMs are relatively forgiving when encountering outputs with an unexpected quality or format, whereas code assertions check exact constraints.
7.4.2 Users can-and want to-directly verify code-based implementations. Regardless of the type of assertion (code or LLM), EvalGen follows the same process of exploring multiple candidate assertions per criterion and, for each criterion, selecting the assertion that aligns most with the user's grades. While participants generally liked this approach for LLM-based assertions, they did not like this for code-based assertions. This observation may be specific to users who are experts in coding (as all of our participants are). Some participants wanted to see the code for each candidate assertion and select the best Python function for each criterion themselves (P2, P5, P7). P5 said:</p>
<p>When something can be solved using Python code, I do have an envisioned [implementation] in mind that I can easily verify. Just showing [me] the [code] will be quicker.</p>
<p>P7 wanted to iterate on code-based assertions simply by providing feedback to make the assertion more or less "fancy"-for example, in checking for a pattern in the LLM output, they might</p>
<p>want the regex to be more or less complicated depending on the sample of LLM outputs they've seen. However, P7 then acknowledged that their ability to edit depends on code length and complexity:</p>
<p>For something that's like 10 lines of code or less, I'll look at [the code]. But for [longer functions,] I probably want to eyeball [the function results]. This is where it gets a bit unclear.
While participants had lots of ideas for improvement, participants overall felt that EvalGen provided a good start to code-based assertions, and it was easy for them to edit the code if they wanted: two participants even asked for the ability to export code-based assertions as unit tests or in a Python file (P4, P6).
7.4.3 Users want their grades to aid automated criteria generation. While participants liked that EvalGen tried multiple candidate assertions for each criterion, they wanted EvalGen to use their grades, as well as natural language feedback on thumbs-down grades, in prompting LLMs for candidate assertions (P3, P4, P5, P7). For example, any time a user gives a grade, EvalGen could query an LLM for a new candidate assertion, including the new grade in the prompt so that the candidate assertion may be more aligned with the user's grades. This strategy can, of course, be used to generate new codebased assertions too. Not only did participants want grades to be included in the process of generating new candidate assertions, but some wanted their labeled LLM outputs to be included in the LLM-based assertions' prompts themselves (P3, P5). P3 excitedly realized this when looking at the table of results ("I can take these good and bad examples and use them as few-shot exemplars in the prompt for the LLM evaluators..."). This suggests an optimization loop akin to ConstitutionMaker and DSPy [26, 39], but for assertion generation and validation, rather than prompt optimization.
7.4.4 LLM-based assertions are harder to trust. While participants found EvalGen's suggested code-based assertions to be more obviously misaligned than the LLM-based assertions (Section 7.3.3), they also acknowledged that LLM-based assertions are harder for them to trust-since they can edit the code-based assertions more easily than the LLM-based assertions (P2, P5, P6, P8, P9). Some participants were skeptical of how LLM-based assertions might transfer to monitoring LLM outputs in a production pipeline (P3, P6, P8). P8 said, "I cannot begin to think about how LLMs as validators [in production] can work, I'm very skeptical." P9 asked, "How do I maintain my evals over time; do I have to rerun this entire process?" One suggestion is an interface that solicits grades from other people-possibly the end-users of the user's LLM pipeline-to continually realign LLM-based assertions.</p>
<h2>8 DISCUSSION</h2>
<p>Here, we unpack criteria drift and how it affects alignment, discuss the implications of our findings for future LLMOps evaluation assistants, and outline some open questions.</p>
<h3>8.1 Implications of Criteria Drift for LLM Evaluation Assistants</h3>
<p>The practice of benchmarks in ML and NLP presume a world of well-defined criteria (and well-labeled data) on which to judge</p>
<p>LLM outputs. For instance, AutoCalibrate is a method to calibrate LLM evaluators with human preferences that requires large expert-labelled datasets with settled (i.e., established upfront) criteria [34]. However, in practice, we found that developers rapidly iterate over criteria, and furthermore that cognitively engaging with LLM outputs helps them to refine their criteria. This suggests criteria refinement and grading should happen in tandem in interactive settings, and poses challenges to alignment methods that presume settled, expert labels. In particular, the dependence of criteria on a specific prompt and LLM's outputs means that any change to the LLM pipeline can cause criteria drift: for example, LLM APIs can be replaced with new models, resulting in prompt drift [4].</p>
<p>Future system designs should support these requirements. For instance, an evaluator assistant might adjust criteria dynamically as the user grades and gives feedback. We might also consider percriteria grading (while keeping the simplicity of thumbs up/down voting). Eliciting grades at the criteria level might allow evaluation assistants to more precisely adjust both the criteria for evaluation, as well as the way these criteria are implemented. Finally, including examples of both good and bad LLM outputs within the LLM-based evaluator prompts could also be beneficial (a method akin to recent work $[26,35])$. However, whenever outputs change, we may need to ask users to re-grade or re-think their criteria.</p>
<p>Our criteria drift finding echoes prior work in educational settings, where instructors often update their grading rubrics to reflect common errors as they grade more assignments [47]. Similar to how teachers adjust rubrics and regrade as they see more student submissions, making users reassess LLM outputs after updating criteria could enhance the accuracy and consistency of grades. Given the observed inconsistency in participants' grading, evaluation assistants might also pursue crowdsourcing methods like majority voting and self or external assessments to determine accurate grades for LLM outputs [7, 9]. Another challenge for evaluation assistants is extending adapted criteria to grade both ungraded and future unseen LLM outputs. How do evaluation assistants consistently sample grades for outputs that reflect the overall distribution of LLM pipeline successes and failures?</p>
<p>The reader might wonder when criteria "settle." Perhaps there was simply not enough time in our study, and had participants graded for an hour or two, they might have solidified their criteria, and criteria drift goes away. There is reason to believe that this situation does not change with more time-as we saw, the criteria participants refined changed to adapt to the behavior of the LLM outputs being evaluated-a dependent, rather than independent assessment of quality. In the real world, similar situations exist where criteria are never "fully settled" as more inputs come in-consider the court of law. One of our participants remarked that people "know a bad output when they see it." Their adage reflects a U.S. Supreme Court Justice's famous opinion in a 1964 court case about obscene content [17]. As in that remark, the decisions of human validators seem at first glance "to be based on a non-rational, intuitive gut reaction, instead of reasoned analysis; it seems to be utterly subjective and personal" [17, p.1025]. However, perhaps, as the law scholar Gewirtz argues, subjectivity is not necessarily a sign of irrationality (contrasting with some imagined future AI that is entirely objective and rational, entirely "aligned" or "better" than humans). On the contrary: "There are good reasons to accept the</p>
<p>imperfect in a judge. We should encourage judges to believe and say: This is the best I can do now; it doesn't solve all the problems, but it's a start, and I'll keep thinking" [17, p.1027]. This raises a deeper epistemic question for evaluation assistants-is "alignment" an actualizable goal? To what extent does our common terminology and assumptions-e.g., that there is a "ground truth" set of labels we merely need to elicit-fail us? Is validating the validators only ever a work-in-progress?</p>
<h3>8.2 Operationalizing Assertions</h3>
<p>Participants expressed the desire to deploy their assertions in production. Some wanted to deploy assertions in the critical path of the LLM pipeline, to catch bad outputs before they are shown to end-users. Others wanted a more passive deployment-one participant requested for some form of report sent to their email inbox daily, containing results of the assertion set executed on a sample of LLM outputs from that day. In practice, assertions have different operational requirements. For example, if an assertion runs in the critical path, it should not result in many false failures, otherwise the LLM may get unnecessarily reprompted (i.e., rerun) and the overall latency might increase for our LLM pipeline. Moreover, as criteria changes over time, evaluation assistants should ask users to grade new LLM outputs observed in production and automatically adapt assertion sets.</p>
<p>Our study confirmed the dual necessity for both code-based and LLM-based assertions among participants, but further showed that participants felt each type required distinct treatment, both in implementation selection and in how they perceive alignment. Often useful for sanity checks like output structure, code-based assertions can be used in the critical path of the LLM pipeline. In fact, a number of LLMOps tools exist for users to implement such code-based guardrails [19, 22, 42]. However, our participants highlighted the challenge of finding the right implementation for a given assertion-a decision that can depend intricately on the characteristics of LLM outputs. For example, determining the acceptable length of a response might vary significantly based on the observed output distribution. In fact, specifically for an output length criterion, P3 mentioned that they would look at a histogram of word lengths for a batch of outputs and see if there were some buckets of anomalous word lengths that exhibited other failure modes (e.g., a "rambling" response from the LLM).</p>
<p>Some participants mentioned that they wanted their collaborators, such as product managers, to grade outputs in EvalGen. Importantly, when allowing multiple users to collaborate on grading, evaluation assistants have to consider inter-rater reliability and handle disagreements, if any. Again, we can draw inspiration from techniques from the crowdsourcing literature: for example, evaluation assistants could model each individual grader's accuracy and apply corrections if necessary [59]. Moreover, collaborators may have different experience levels with coding: those with less experience may choose LLM-based assertions for criteria that might be better evaluated with code. While EvalGen provides initial suggestions for whether criteria should be evaluated with code or another LLM, evaluation assistants may want to make sure that all users from the same team "agree" on the right implementations. One could imagine interfaces similar to creating a "pull request" for a
new assertion and soliciting review from a team member, and a workflow similar to continuous integration/continuous deployment (CI/CD) that seamlessly pushes new assertions to production.</p>
<h3>8.3 Future Work and Limitations</h3>
<p>Assertions serve as a straightforward mechanism for evaluating LLM outputs, yet the potential of evaluation assistants extends beyond EvalGen's supported binary judgments. Consider the example of word count: rather than setting rigid thresholds, one might find it more useful to monitor variations in word count across different LLM outputs. Like in traditional ML monitoring [44], there is, of course, still imprecision in what word count counts as "bad," given its distribution. Does the AI assistant's definition of bad align with the user's? Tracking finer-grained information in evaluations, beyond simple true/false conditions, can aid in debugging issues within LLM pipelines, once a user knows the output is bad. Second, many users write their LLM pipelines as chains of multiple LLM calls [1, 11, 14, 53]. Evaluation assistants can facilitate end-toend alignment, which can be complicated not only with chains of LLM calls, but also in compound AI systems that leverage other components like retrieval-augmented generation [55].</p>
<p>Increasingly, user want their prompts to automatically improve based on assertion results. For instance, if an assertion fails for a group of LLM outputs, one might want to add an instruction to the prompt to solve this failure mode. Some frameworks already experiment with using feedback from assertions or user grades to refine prompts [35, 39, 48]. However, incorporating this into an evaluation assistant is not as straightforward: there may be a cyclical process where evaluation assistants not only help in adjusting assertions based on prompt performance but also use these insights to suggest further prompt modifications. This iterative cycle of evaluation and adjustment could foster a co-evolutionary environment where prompts, assertions, and even evaluative mechanisms themselves are continuously refined in a unified interface.</p>
<p>Finally, there are two limitations of our study that are worth pointing out. First, our off-line evaluation only focused on two pipelines. Different applications may warrant different methods of sampling grades and aligning assertions. Second, our qualitative study focused on a small sample of developers, all of whom have significant experience deploying LLMs. Moreover, participants did not have enough time to iterate many times in EvalGen, and our setup did not cover the deployment phase of LLM workflows. Future work might explore best practices and pitfalls of evaluations in the broader LLMOps lifecycle.</p>
<h2>9 CONCLUSION</h2>
<p>This work presented EvalGen, a mixed-initiative approach to aligning LLM-generated evaluation functions with human preferences. EvalGen assists users in developing both criteria acceptable LLM outputs and developing functions to check these standards, ensuring evaluations reflect the users' own grading standards. In a qualitative study with 9 expert users, we observed a pattern we call criteria drift, where users refine their evaluation standards as they grade more LLM outputs. Recognizing the dependency of criteria on LLM outputs highlights new directions for designing future evaluation assistants.</p>
<h2>REFERENCES</h2>
<p>[1] Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, and Elena Glassman. 2023. ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing. arXiv preprint arXiv:2309.09128 (2023).
[2] Pierre Broyani, Anastasios N Angelopoulos, Nir Yosef, Jitendra Malik, and Michael I Jordan. 2024. AutoEval Done Right: Using Synthetic Data for Model Evaluation. arXiv preprint arXiv:2403.07008 (2024).
[3] Zana Bujinca, Maja Barbata Malaya, and Krzysztof Z Gajos. 2021. To trust or to think: cognitive forcing functions can reduce overreliance on AI in AI-assisted decision-making. Proceedings of the ACM on Human-Computer Interaction 5, CSCW1 (2021), 1-21.
[4] Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is ChatGPT's behavior changing over time? arXiv preprint arXiv:2307.09009 (2023).
[5] Raumak Chowdhuri, Neil Deshmukh, and David Koplow. 2023. No, GPT4 can't ace MIT. https://flower-nutria-41d.notion.site/No-GPT4-can-t-ace-MIT-b27e6796ab5a48368127a98216c76864
[6] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems 30 (2017).
[7] Aida Mostafazadeh Davani, Mark Diaz, and Vinodkumar Prabhakaran. 2022. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics 10 (2022), 92-110.
[8] Michael Desmond, Michelle Brachman, Evelyn Duesterwald, Casey Dugan, Naeendra Nath Joshi, Qian Pan, and Carolina Spina. 2022. AI Assisted Data Labeling with Interactive Auto Label. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 13161-13163.
[9] Steven Dow, Anand Kulkarni, Scott Klemmer, and Björn Hartmann. 2012. Shepherding the crowd yields better work. In Proceedings of the ACM 2012 conference on computer supported cooperative work. 1013-1022.
[10] John J Dudley and Per Ola Kristensson. 2018. A review of user interface design for interactive machine learning. ACM Transactions on Interactive Intelligent Systems (TiiS) 8, 2 (2018), 1-37.
[11] Harrison Chase et al. 2023. LangChain. https://pypi.org/project/langchain/.
[12] Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rocktäschel. 2023. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797 (2023).
[13] Uwe Flick. 2013. The SAGE handbook of qualitative data analysis. Sage.
[14] FlowiseAI, Inc. 2023. FlowiseAI Build LLMs Apps Easily. flowiseai.com.
[15] Michel Naim Gerguis, Cherif Salama, and M Watheq El-Kharashi. 2016. ASU: An Experimental Study on Applying Deep Learning in Twitter Named Entity Recognition.. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT). 188-196.
[16] Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K Kummerfeld, and Elena L Glassman. 2024. Supporting Sensemaking of Large Language Model Outputs at Scale. arXiv preprint arXiv:2401.13726 (2024).
[17] Paul Gewirtz. 1996. On "I know it when I see it". The Yale Law Journal 105, 4 (1996), 1023.
[18] Andrew D. Gordon, Carina Negreanu, José Cambronero, Rasika Chakravarthy, Ian Drosos, Hao Fang, Bhaskar Mitra, Hannah Richardson, Advait Sarkar, Stephanie Simmons, Jack Williams, and Ben Zorn. 2023. Co-audit: tools to help humans double-check AI-generated content. arXiv:2310.01297 [cs.HC]
[19] Guardrails 2023. Guardrails AI. https://github.com/guardrails-ai/guardrails.
[20] Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Gaoqing Liu, Jiang Bian, and Yujiu Yang. 2023. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532 (2023).
[21] Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley. 2024. Bridging Language and Items for Retrieval and Recommendation. arXiv preprint arXiv:2403.03952 (2024).
[22] Instructor. 2023. Instructor, Generating Structure from LLMs. https://jxnl.github.io/instructor/.
[23] Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron Donsbach, Michael Terry, and Carrie J Cai. 2022. PromptMaker: Prompt-based Prototyping with Large Language Models. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA '22). Association for Computing Machinery, New York, NY, USA, Article 35, 8 pages. https://doi.org/10.1145/3031101.3503564
[24] Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James Wexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, and Lucas Dixon. 2024. LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large Language Models. arXiv:2402.10524 [cs.HC]
[25] Adam Tasman Kalai and Santosh S Vempala. 2023. Calibrated language models must hallucinate. arXiv preprint arXiv:2311.14648 (2023).
[26] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri VanBhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls
into self-improving pipelines. arXiv preprint arXiv:2310.03714 (2023).
[27] Tae Soo Kim, Toonjoo Lee, Jamin Shin, Young-Ho Kim, and Juho Kim. 2023. Evallm: Interactive evaluation of large language model prompts on user-defined criteria. arXiv preprint arXiv:2309.13633 (2023).
[28] Agnes M Kloth, Robin Welsch, Thomas Kosch, and Steeven Villa. 2023. "AI enhances our performance, I have no doubt this one will do the same": The Placebo effect is robust to negative descriptions of AI. arXiv preprint arXiv:2309.16606 (2023).
[29] Rafal Kocielnik, Saleema Amershi, and Paul N. Bennett. 2019. Will You Accept an Imperfect AI? Exploring Designs for Adjusting End-user Expectations of AI Systems. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland UK) (CHI '19). Association for Computing Machinery, New York, NY, USA, 1-14. https://doi.org/10.1145/3290605.3300641
[30] Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Guyun Gao, and Yang Liu. 2023. Split and merge: Aligning position biases in large language model based evaluators. arXiv preprint arXiv:2310.01432 (2023).
[31] Q. Vera Liao and S. Shyam Sundar. 2022. Designing for Responsible Trust in AI Systems: A Communication Perspective. In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Seoul, Republic of Korea) (FAccT '22). Association for Computing Machinery, New York, NY, USA, 1257-1268. https://doi.org/10.1145/3531146.3533182
[32] Q Vera Liao and Jennifer Wortman Vaughan. 2023. AI transparency in the age of LLMs: A human-centered research roadmap. arXiv preprint arXiv:2306.01941 (2023).
[33] Xiaohua Liu, Furu Wei, Shaodian Zhang, and Ming Zhou. 2013. Named entity recognition for tweets. ACM Transactions on Intelligent Systems and Technology (TIST) 4, 1 (2013), 1-15.
[34] Yunnan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023. Calibrating LLM-based evaluator. arXiv preprint arXiv:2309.13308 (2023).
[35] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2024. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems 36 (2024).
[36] Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang, Bum Chul Kwon, and Chris Bryan. 2023. PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models. arXiv preprint arXiv:2304.01964 (2023).
[37] Chris Parnin, Gustavo Soares, Rahul Pandita, Sumit Gulwani, Jessica Rich, and Austin Z Henley. 2023. Building Your Own Product Copilot: Challenges, Opportunities, and Needs. arXiv preprint arXiv:2312.14231 (2023).
[38] Kayur Patel, Naomi Bancroft, Steven M. Drucker, James Fogarty, Amy J. Ko, and James Landay. 2010. Gestalt: integrated support for implementation and analysis in machine learning. In Proceedings of the 23nd Annual ACM Symposium on User Interface Software and Technology (New York, New York, USA) (UIST '10). Association for Computing Machinery, New York, NY, USA, 37-46. https: //doi.org/10.1145/1866029.1866038
[39] Savvae Petridis, Ben Wedin, James Wexler, Aaron Donsbach, Mahima Pushkarna, Nitesh Goyal, Carrie J. Cai, and Michael Terry. 2023. ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles. arXiv:2310.15428 [cs.HC]
[40] PromptsRoyale. 2023. PromptsRoyale. https://www.promptsroyale.com.
[41] Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, and Saleema Amershi. 2023. Supporting Human-AI Collaboration in Auditing LLMs with LLMs. arXiv preprint arXiv:2304.09991 (2023).
[42] Traian Rebreku, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and Jonathan Cohen. 2023. Nemo guardrails: A toolkit for controllable and safe llm applications with programmable rails. arXiv preprint arXiv:2310.10501 (2023).
[43] Melanie Sclar, Yejin Choi, Yulia Tevetkov, and Alane Suhr. 2023. Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting. arXiv preprint arXiv:2310.11324 (2023).
[44] Shreya Shankar, Labib Fawaz, Karl Gyllstrom, and Aditya Parameswaran. 2023. Automatic and Precise Data Validation for Machine Learning. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 2198-2207.
[45] Shreya Shankar, Haotian Li, Parth Asawa, Madelon Hulsebox, Yiming Lin, JD Zamfirescu-Pereira, Harrison Chase, Will Fu-Hinthorn, Aditya G Parameswaran, and Eugene Wu. 2024. SPADE: Synthesizing Assertions for Large Language Model Pipelines. arXiv preprint arXiv:2401.03038 (2024).
[46] Patrice Simard, David Chickering, Aparna Lakshmiratan, Denis Charles, Léon Bottou, Carlos Garcia Jurado Soares, David Grangier, Saleema Amershi, Johan Verwey, and Jina Suh. 2014. Ice: enabling non-experts to build models interactively for large-scale lopsided problems. arXiv preprint arXiv:1409.4814 (2014).
[47] Arjun Singh, Sergey Karayev, Kevin Gutowski, and Pieter Abbeel. 2017. Gradescope: a fast, flexible, and fair system for scalable assessment of handwritten work. In Proceedings of the fourth (2017) acm conference on learning@ scale. 81-88.</p>
<p>[48] Arnav Singhvi, Manish Shetty, Shangyin Tan, Christopher Potts, Koushik Sen, Matei Zaharia, and Omar Khattab. 2023. DSPy Assertions: Computational Constraints for Self-Refining Language Model Pipelines. arXiv preprint arXiv:2312.13382 (2023).
[49] Chanchal Suman, Saichethan Miriyala Reddy, Sriparna Saha, and Pushpak Bhattacharyya. 2021. Why pay more? A simple and efficient named entity recognition system for tweets. Expert Systems with Applications 167 (2021), 114101.
[50] Helena Vasconcelos, Matthew Jörke, Madeleine Grunde-McLaughlin, Tobias Gerstenberg, Michael S Bernstein, and Ranjay Krishna. 2023. Explanations can reduce overreliance on ai systems during decision-making. Proceedings of the ACM on Human-Computer Interaction 7, CSCW1 (2023), 1-38.
[51] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large Language Models are not Fair Evaluators. arXiv:2305.17926 [cs.CL]
[52] Ian Webster. 2023. promptfoo: Test your prompts. https://www.promptfoo.dev/.
[53] Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai. 2022. PromptChainer: Chaining Large Language Model Prompts through Visual Programming. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA '22). Association for Computing Machinery, New York, NY, USA, Article 359, 10 pages. https://doi.org/10.1145/3491101.3519729
[54] Wen-wai Yim, Yujuan Fu, Asma Ben Abacha, Neal Snider, Thomas Lin, and Meliba Yetisgen. 2023. Aci-bench: a novel ambient clinical intelligence dataset for benchmarking automatic visit note generation. Scientific Data 10, 1 (2023), S86.
[55] Matei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather Miller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao, and Ali Ghodsi. 2024. The Shift from Models to Compound AI Systems. https: //bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/.
[56] JD Zamfirescu-Pereira, Heather Wei, Amy Xiao, Kitty Gu, Grace Jung, Matthew G Lee, Bjoern Hartmann, and Qian Yang. 2023. Herding AI cats: Lessons from designing a chatbot by prompting GPT-3. In Proceedings of the 2023 ACM Designing Interactive Systems Conference. 2206-2220.
[57] J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Can't Prompt: How Non-AI Experts Try (and Fail) to Design LLM Prompts. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg, Germany) (CHI '23). Association for Computing Machinery, New York, NY, USA, Article 437, 21 pages. https://doi.org/10.1145/ 3544548.3581388
[58] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-ao-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems 36 (2024).
[59] Honglei Zhuang, Aditya Parameswaran, Dan Roth, and Jiawei Han. 2015. Debiasing crowdsourced batches. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 1593-1602.</p>
<h2>A ALGORITHMS FOR SELECTING ASSERTIONS \&amp; ELICITING GRADES</h2>
<h2>A. 1 Assertion Selectivity and Impact on LLM Output Quality Confidence</h2>
<p>One way to establish confidence in whether an LLM output is problematic is to assess the selectivity, or pass rate, of assertions that fail it. Intuitively, assertions that frequently fail outputs (low selectivity) provide limited insight into output quality. For example, an assertion that trivially fails every output offers no discernment and has a selectivity of 0 .</p>
<p>EvalGen leverages selectivity estimates of assertions to assign a confidence score to each LLM output, indicating the likelihood it is of poor quality. The rationale is straightforward: an output is more likely to be problematic if failed by assertions known for their high selectivity. Concretely, for a set of assertions $F$ where each assertion $f \in F$ returns 1 for a pass and 0 for a fail, we calculate the confidence score for an LLM output $e$ as follows:</p>
<p>$$
\sigma(e)=\sum_{f \in F} \operatorname{selectivity}(f) \times f(e)
$$</p>
<p>The score $\sigma$ is always non-negative. A score of 0 means no assertions have failed the output, indicating a higher likelihood of quality, while lower scores, resulting from failures by non-selective assertions, point to uncertainty or potential issues with the output.</p>
<h2>A. 2 Sampling Grades</h2>
<p>Given that users may not want to grade so many outputs in the EvalGen interface, choosing which outputs for users to grade is crucial for aligning the system's evaluations with user expectations. Randomly selecting outputs without considering their predicted quality can lead to misalignment, especially if the selected samples aren't representative of the entire dataset. Prior work also underscores the importance of soliciting a representative graded sample of LLM outputs $[2,45]$.</p>
<p>Given $\sigma$ scores as previously defined, we consider a number of strategies to sample outputs for grading:</p>
<ul>
<li>Random: Sample outputs at random (uniformly)</li>
<li>Highest: Sample the outputs with the highest $\sigma$. This approach focuses on potentially problematic content.</li>
<li>Lowest: Sample the outputs with the lowest $\sigma$, prioritizing outputs that don't fail any assertions or fail low-selectivity assertions.</li>
<li>Alternating: Alternate between high and low $\sigma$, aiming for a diverse sample with both bad and good outputs.
In Section 5, we test these strategies against a random baseline on two different LLM pipelines. We employ an alternating sampling policy for the EvalGen user studies. We do not claim to have the best sampling policy; we chose an alternating policy with the hope that it would solicit a balanced sample of good and bad grades.</li>
</ul>
<p>One may wonder why we do not list a policy that ranks the outputs by score and samples the middle for grading. While this might seem akin to seeking out uncertain cases-as is common in active learning-our scores represent the likelihood of outputs being poor. They do not differentiate between good and bad per se.</p>
<p>Therefore, outputs with low scores may still vary widely in quality, reflecting our system's uncertainty.</p>
<h2>A. 3 Choosing Aligned Assertions</h2>
<p>Once the user stops grading and wants to see the alignment Report Card, as depicted in Figure 2e, for each criterion, we select the candidate assertion with the highest alignment score. We adopt notation from Shankar et al. [45] in defining alignment. Formally, let $E$ be a set of LLM pipeline input-output pairs and $f: E \rightarrow{0,1}$ represent an assertion. Let $y$ be a binary vector, where $y_{i} \in{0,1}$ represents whether the user thinks an LLM output $e_{i}$ is bad. Suppose $F=\left{f_{1}, f_{2}, \ldots, f_{j}\right}$ is a set of $j$ assertions. Concretely, the coverage and false failure rate (FFR) of $F$ is represented by the following equations:</p>
<p>$$
\begin{aligned}
\text { Coverage }(F) &amp; =\frac{\sum_{i} \mathbb{I}\left[y_{i}=0 \wedge\left(\exists f \in F, f\left(e_{i}\right)=0\right)\right]}{\sum_{i} \mathbb{I}\left[y_{i}=0\right]} \
\operatorname{FFR}(F) &amp; =\frac{\sum_{i} \mathbb{I}\left[y_{i}=1 \wedge\left(\exists f \in F, f\left(e_{i}\right)=0\right)\right]}{\sum_{i} \mathbb{I}\left[y_{i}=1\right]}
\end{aligned}
$$</p>
<p>In both definitions, $\mathbb{I}$ is the indicator function. Intuitively, coverage represents the set's true negative rate, while false failure rate represents the set's false negative rate. An aligned set of assertions would have a high coverage and low false failure rate. We define the alignment of $F$ as the harmonic mean of coverage and the inverse of FFR:</p>
<p>$$
\text { Alignment }(F)=2 \times \frac{\text { Coverage }(F) \times(1-\operatorname{FFR}(F))}{\text { Coverage }(F)+(1-\operatorname{FFR}(F))}
$$</p>
<p>Note that alignment is very similar to F1 score; however, we are concerned with the precision and recall of failures (i.e., when $f=0$, not when $f=1$ ), and we are concerned with a set (i.e., when any assertion returns 0 ).</p>
<h2>A. 4 Evaluation of Sampling Policy</h2>
<p>We described in Appendix A. 2 four options we considered to sample LLM outputs: random, highest, lowest, and alternating. Here, we compare EvalGen's sampling policy, alternating, to these other three baselines. For this experiment, we sampled 16 outputs to grade, but in practice the user can grade more or fewer. Using the same LLM pipelines as described in Section 5.1, to assess sampling variance, we conducted 10 trials for each of the four sampling policies-where, for each trial, we kept the same set of candidate assertion functions.</p>
<p>The findings, shown in Figure 4, reveal that the random sampling policy exhibits a large variance in alignment. This inconsistency could lead to user frustration, particularly if the effort spent in grading outputs results in assertion sets with unpredictable relevance to their specified criteria. The alternative sampling strategies, which weight the probability for an LLM output to be graded by the selectivity (i.e., pass rate) of assertions that fail it, consistently yielded higher alignment scores across the entire datasets than the random policy. Notably, while the alternating policy didn't consistently outperform, our results suggest that any non-random policy implemented in EvalGen may achieve satisfactory outcomes.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Alignments for assertion sets that result from different policies to sample grades from the user. Each policy was tested across 10 trials, with each involving a sample of 16 LLM outputs. Randomly sampling LLM outputs for grading introduces significant variance in alignment across the entire dataset.</p>
<p>In this offline study, as shown in Figure 4, there's no variation in the outcomes of the non-random policies because they are deterministic. However, in real-world use, EvalGen updates its predictions as it receives new information, so there could be some differences in results over time. Initially, when users start grading outputs in EvalGen, they might effectively be grading random outputs for the first one or two outputs, as the $\sigma$ scores update and stabilize.</p>
<h2>B TASK PROMPTS</h2>
<p>We prepared two prompts and corresponding datasets for tasks to present users (5.1). Both pipelines were adapted from prior work [21, 54] and correspond to medical record processing and a product description writing, respectively. The medical pipeline prompt is as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="nv">You</span><span class="w"> </span><span class="nv">are</span><span class="w"> </span><span class="nv">extracting</span><span class="w"> </span><span class="nv">insights</span><span class="w"> </span><span class="nv">from</span><span class="w"> </span><span class="nv">some</span><span class="w"> </span><span class="nv">medical</span><span class="w"> </span><span class="nv">records</span>.
<span class="w">    </span><span class="nv">The</span><span class="w"> </span><span class="nv">records</span><span class="w"> </span><span class="nv">contain</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">medical</span><span class="w"> </span><span class="nv">note</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">a</span>
<span class="w">    </span><span class="nv">dialogue</span><span class="w"> </span><span class="nv">between</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">doctor</span><span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">patient</span>.<span class="w"> </span><span class="nv">You</span><span class="w"> </span><span class="nv">need</span>
<span class="w">    </span><span class="nv">to</span><span class="w"> </span><span class="nv">extract</span><span class="w"> </span><span class="nv">values</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">following</span>:<span class="w"> </span><span class="nv">Chief</span>
<span class="w">    </span><span class="nv">complaint</span>,<span class="w"> </span><span class="nv">History</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">present</span><span class="w"> </span><span class="nv">illness</span>,<span class="w"> </span><span class="nv">Physical</span>
<span class="w">    </span><span class="nv">examination</span>,<span class="w"> </span><span class="nv">Symptoms</span><span class="w"> </span><span class="nv">experienced</span><span class="w"> </span><span class="nv">by</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">patient</span>,
<span class="w">    </span><span class="nv">New</span><span class="w"> </span><span class="nv">medications</span><span class="w"> </span><span class="nv">prescribed</span><span class="w"> </span><span class="nv">or</span><span class="w"> </span><span class="nv">changed</span>,<span class="w"> </span><span class="nv">including</span>
<span class="w">    </span><span class="nv">dosages</span><span class="w"> </span><span class="ss">(</span><span class="nv">N</span><span class="o">/</span><span class="nv">A</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">provided</span><span class="ss">)</span>,<span class="w"> </span><span class="nv">and</span><span class="w"> </span><span class="nv">Follow</span><span class="o">-</span><span class="nv">up</span>
<span class="w">    </span><span class="nv">instructions</span><span class="w"> </span><span class="ss">(</span><span class="nv">N</span><span class="o">/</span><span class="nv">A</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="nv">provided</span><span class="ss">)</span>.<span class="w"> </span><span class="nv">Your</span><span class="w"> </span><span class="nv">answer</span>
<span class="w">    </span><span class="nv">should</span><span class="w"> </span><span class="nv">not</span><span class="w"> </span><span class="k">include</span><span class="w"> </span><span class="nv">any</span><span class="w"> </span><span class="nv">personal</span><span class="w"> </span><span class="nv">identifiable</span>
<span class="w">    </span><span class="nv">information</span><span class="w"> </span><span class="ss">(</span><span class="nv">PII</span><span class="ss">)</span><span class="w"> </span><span class="nv">such</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">name</span>,<span class="w"> </span><span class="nv">age</span>,<span class="w"> </span><span class="nv">gender</span>,<span class="w"> </span><span class="nv">or</span>
<span class="w">    </span><span class="nv">ID</span>.<span class="w"> </span><span class="nv">Use</span><span class="w"> </span><span class="s2">&quot;the patient&quot;</span><span class="w"> </span><span class="nv">instead</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">their</span><span class="w"> </span><span class="nv">name</span>,<span class="w"> </span><span class="k">for</span>
<span class="w">    </span><span class="nv">example</span>.<span class="w"> </span><span class="k">Return</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">a</span><span class="w"> </span><span class="nv">bullet</span><span class="w"> </span><span class="nv">list</span>,
<span class="w">    </span><span class="nv">where</span><span class="w"> </span><span class="nv">each</span><span class="w"> </span><span class="nv">bullet</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">formatted</span><span class="w"> </span><span class="nv">like</span><span class="w"> </span>`<span class="nv">chief</span>
<span class="w">    </span><span class="nv">complaint</span>:<span class="w"> </span><span class="nv">xx</span>.`<span class="w"> </span><span class="k">If</span><span class="w"> </span><span class="nv">there</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">no</span><span class="w"> </span><span class="nv">value</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">key</span>,
<span class="w">        </span><span class="nv">the</span><span class="w"> </span><span class="nv">value</span><span class="w"> </span><span class="nv">should</span><span class="w"> </span><span class="nv">be</span><span class="w"> </span>`<span class="nv">N</span><span class="o">/</span><span class="nv">A</span>`.<span class="w"> </span><span class="nv">Keep</span><span class="w"> </span><span class="nv">your</span><span class="w"> </span><span class="nv">response</span>
<span class="w">    </span><span class="nv">around</span><span class="w"> </span><span class="mi">150</span><span class="w"> </span><span class="nv">words</span><span class="w"> </span><span class="ss">(</span><span class="nv">you</span><span class="w"> </span><span class="nv">may</span><span class="w"> </span><span class="nv">have</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">summarize</span><span class="w"> </span><span class="nv">some</span>
<span class="w">    </span><span class="nv">extracted</span><span class="w"> </span><span class="nv">values</span><span class="w"> </span><span class="nv">to</span><span class="w"> </span><span class="nv">stay</span><span class="w"> </span><span class="nv">within</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">word</span><span class="w"> </span><span class="nv">limit</span><span class="ss">)</span>.
</code></pre></div>

<p>(transcript)</p>
<p>And the product pipeline prompt is as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="nx">You</span><span class="w"> </span><span class="nx">are</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">expert</span><span class="w"> </span><span class="nx">copywriter</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">need</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">write</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">e</span><span class="o">-</span>
<span class="nx">commerce</span><span class="w"> </span><span class="nx">product</span><span class="w"> </span><span class="nx">description</span><span class="w"> </span><span class="nx">based</span><span class="w"> </span><span class="nx">on</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">product</span>
<span class="w">    </span><span class="nx">details</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">customer</span><span class="w"> </span><span class="nx">reviews</span><span class="p">.</span><span class="w"> </span><span class="nx">Your</span><span class="w"> </span><span class="nx">description</span>
<span class="w">    </span><span class="nx">should</span><span class="w"> </span><span class="nx">be</span><span class="w"> </span><span class="nx">SEO</span><span class="o">-</span><span class="nx">optimized</span><span class="p">.</span><span class="w"> </span><span class="nx">It</span><span class="w"> </span><span class="nx">should</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">an</span><span class="w"> </span><span class="nx">active</span>
<span class="w">    </span><span class="nx">voice</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">include</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">product</span><span class="err">&#39;</span><span class="nx">s</span><span class="w"> </span><span class="nx">features</span><span class="p">,</span>
<span class="w">    </span><span class="nx">benefits</span><span class="p">,</span><span class="w"> </span><span class="nx">unique</span><span class="w"> </span><span class="nx">selling</span><span class="w"> </span><span class="nx">points</span><span class="w"> </span><span class="nx">without</span>
<span class="w">    </span><span class="nx">overpromising</span><span class="p">,</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">call</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">action</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">buyer</span>
<span class="w">    </span><span class="p">.</span><span class="w"> </span><span class="nx">Benefits</span><span class="w"> </span><span class="nx">describe</span><span class="w"> </span><span class="nx">how</span><span class="w"> </span><span class="nx">product</span><span class="w"> </span><span class="nx">features</span><span class="w"> </span><span class="nx">will</span>
<span class="w">    </span><span class="nx">work</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">buyer</span><span class="p">,</span><span class="w"> </span><span class="nx">addressing</span><span class="w"> </span><span class="nx">exactly</span><span class="w"> </span><span class="nx">how</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">product</span><span class="w"> </span><span class="nx">will</span><span class="w"> </span><span class="nx">improve</span><span class="w"> </span><span class="nx">their</span><span class="w"> </span><span class="nx">lives</span><span class="p">.</span><span class="w"> </span><span class="nx">Clearly</span>
<span class="w">    </span><span class="nx">distinguish</span><span class="w"> </span><span class="nx">between</span><span class="w"> </span><span class="nx">features</span><span class="w"> </span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="nx">lightweight</span><span class="p">,</span>
<span class="w">    </span><span class="nx">USB</span><span class="o">-</span><span class="nx">chargeable</span><span class="p">)</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">benefits</span><span class="w"> </span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="nx">convenience</span><span class="p">,</span>
<span class="w">    </span><span class="nx">nutritious</span><span class="w"> </span><span class="nx">drinks</span><span class="w"> </span><span class="nx">on</span><span class="o">-</span><span class="nx">the</span><span class="o">-</span><span class="nx">go</span><span class="p">).</span><span class="w"> </span><span class="nx">Don</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">mention</span>
<span class="w">    </span><span class="nx">weaknesses</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">product</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">use</span><span class="w"> </span><span class="nx">generic</span><span class="w"> </span><span class="k">or</span>
<span class="w">    </span><span class="nx">repetitive</span><span class="w"> </span><span class="nx">language</span><span class="p">.</span><span class="w"> </span><span class="nx">Don</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">make</span><span class="w"> </span><span class="nx">up</span><span class="w"> </span><span class="nx">review</span><span class="w"> </span><span class="nx">text</span><span class="w"> </span><span class="k">or</span>
<span class="w">    </span><span class="nx">quotes</span><span class="p">.</span><span class="w"> </span><span class="nx">Don</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">include</span><span class="w"> </span><span class="nx">any</span><span class="w"> </span><span class="nx">links</span><span class="p">.</span><span class="w"> </span><span class="nx">Don</span><span class="err">&#39;</span><span class="nx">t</span><span class="w"> </span><span class="nx">cite</span><span class="w"> </span><span class="nx">the</span>
<span class="w">    </span><span class="nx">reviews</span><span class="w"> </span><span class="nx">too</span><span class="w"> </span><span class="nx">heavily</span><span class="p">.</span><span class="w"> </span><span class="nx">Divide</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">description</span><span class="w"> </span><span class="nx">into</span>
<span class="w">    </span><span class="nx">readable</span><span class="w"> </span><span class="nx">chunks</span><span class="w"> </span><span class="nx">divided</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">relevant</span><span class="w"> </span><span class="nx">subheadings</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Keep</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">description</span><span class="w"> </span><span class="nx">around</span><span class="w"> </span><span class="mi">200</span><span class="w"> </span><span class="nx">words</span><span class="p">,</span><span class="w"> </span><span class="nx">no</span><span class="w"> </span><span class="nx">more</span>
<span class="w">    </span><span class="nx">than</span><span class="w"> </span><span class="mi">300</span><span class="p">,</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">Markdown</span><span class="w"> </span><span class="nx">format</span><span class="p">.</span>
</code></pre></div>

<p>(document)</p>
<p>Received 03 April 2024</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ Note that assertions can be edited in the MultiEval node after returning to the main window, but not in EvalGen's report card screen.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ As of this writing, EvalLM is not publicly released.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
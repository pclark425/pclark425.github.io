<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2177 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2177</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2177</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-281079256</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.01398v1.pdf" target="_blank">The Need for Verification in AI-Driven Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.</p>
                <p><strong>Cost:</strong> 0.025</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2177.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2177.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-5</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model referenced in the paper that is used as an example LLM for simple symbolic discovery tasks; shown to perform better than GPT-4 on small, illustrative equation-discovery prompts in the Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-5</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning / equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates symbolic expressions, natural language hypotheses, derivations and candidate scientific relations from numeric data and prompts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>primarily plausibility checks via prompting and qualitative inspection; in the paper GPT-5 outputs are compared to known ground-truth formulas (comparison to known results) and checked for algebraic correctness; no formal proof system is used in the Appendix examples</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>implicit: judged by whether output matches ground-truth formula or derivation in small rediscovery tasks (no formal novelty metric provided); paper emphasizes risk of memorization vs reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>qualitative: GPT-5 produced the correct closed-form solution for a simple artificial example where GPT-4 failed; described as improved symbolic/axiomatic inference relative to earlier LLMs on small tasks, but still limited to relatively simple functional forms</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>not quantified; validation consisted of direct comparison to known formula in toy examples – GPT-5's outputs were correct in the given small examples, but broader validation performance is not reported</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>paper asserts that validation becomes more difficult when tasks are novel (out-of-training-distribution); GPT-5 can solve some novel toy problems but overall reliability on novel scientific discovery remains uncertain</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>evidence of asymmetry: generation is fast and can produce plausible candidates, while rigorous validation (especially on novel tasks) remains slower and less automated</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>improved compared to GPT-4 on the toy examples, but no systematic OOD evaluation provided; authors warn LLMs often rely on memorization and can fail on genuinely novel tasks</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported quantitatively; paper discusses general LLM calibration concerns (overconfidence, hallucination) and implies calibration degrades on novel tasks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>generation is low-cost relative to formal verification; validation in the Appendix used human inspection/comparison which is inexpensive for toy tasks but scales poorly</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>paired comparison to ground-truth, use of theorem provers or symbolic-verification frameworks recommended but not used in these GPT-5 examples</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-5 can succeed at small symbolic rediscovery tasks where earlier LLMs failed, but the paper emphasizes that such successes do not generalize to robust, automated scientific verification and that formal verification remains necessary for novel discoveries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2177.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used large language model that the authors tested on toy equation-discovery prompts and which failed on a simple artificial example used in the Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning / equation discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>produces candidate symbolic forms, natural-language hypotheses and derivations from prompts; used for limited symbolic regression-like tasks</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>comparison to known ground-truth in toy examples and human inspection; no formal verification used in the examples</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>implicit (matching ground-truth vs. not); paper notes GPT-4 failed on a toy novel example</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>qualitative: failed to produce correct expression in a simple artificial example described in Appendix, indicating limits on symbolic reasoning in that instance</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>not quantified; validation limited to direct comparison with ground-truth in toy examples</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>the failure on the toy novel example demonstrates poorer performance on novel reasoning tasks compared to some newer LLMs (e.g., GPT-5)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>evidence that generation (producing many candidate forms) can occur even when correct validation/reasoning fails—GPT-4 generated plausible but incorrect expressions</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>degraded vs. GPT-5 on the toy example; no systematic OOD metrics reported</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported quantitatively; general discussion in paper indicates LLMs can be confidently wrong (poor calibration) especially OOD</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>generation inexpensive; verification limited to human checking in examples</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>formal verification or integrated neuro-symbolic methods suggested to compensate for GPT-4 failures</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 failed on a simple symbolic example whereas GPT-5 succeeded, illustrating that LLM generation ability can improve but that verification remains necessary, particularly for novel tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2177.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMs (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models (general)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Generative pre-trained transformer models used as hypothesis proposers, reasoning assistants, and agentic components in workflows that automate parts of scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Large language models (LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific discovery, materials, chemistry, biology, theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generate hypotheses, literature summaries, candidate equations, experimental plans, and natural-language reasoning chains; can be configured as agents that call external tools</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>various: human review, comparison to existing literature, integration with external simulation or lab tools, or (partial) RLHF; however RLHF is described as improving plausibility rather than providing formal verification</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>commonly assessed by comparison to training data (memorization vs reasoning), rediscovery benchmarks, or benchmarks designed to test open-ended discovery (LLM-SRBench, simulated domains); paper notes that many benchmarks are rediscovery-focused and don't test true novelty</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>high throughput and breadth in producing candidate hypotheses; qualitative strengths in fluency and broad knowledge but variable correctness, especially for symbolic/math tasks; performance better on familiar/textbook-style problems than on novel open-ended discovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>limited: RLHF improves human-perceived plausibility but does not guarantee truth; LLMs hallucinate references and violate basic algebra/physics in reported cases; formal verification is generally lacking</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>validation reliability degrades for novel, out-of-distribution tasks where memorization cannot rescue the model; authors stress benchmarks must test novelty explicitly</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>strong asymmetry: LLMs generate many plausible-seeming outputs quickly, while reliable automated validation of those outputs remains rare and expensive</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>often poor; LLMs may rely on memorized patterns causing failure on genuinely novel tasks unless specifically trained/evaluated for reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>generally poor for truthfulness; RLHF increases alignment with human preference but not with formal correctness</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>generation is inexpensive relative to rigorous verification; integrating external tools or formal provers increases cost and pipeline complexity</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>integration with formal theorem provers, tool-augmented agents, hybrid neuro-symbolic systems, domain-specific LLMs, and dedicated benchmarks are proposed to reduce the gap</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs are powerful hypothesis proposers but lack principled, automated verification mechanisms; RLHF improves perceived quality but not formal correctness, and performance falls on novel tasks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2177.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neuro-symbolic generator-verifier framework that pairs symbolic regression with formal theorem proving to generate candidate symbolic laws from data and then verify consistency with background theory via a reasoning distance metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Combining data and theory for derivable scientific discovery with AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Descartes</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neurosymbolic / hybrid (symbolic regression + automated theorem prover)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>equation discovery / physical sciences</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates symbolic candidate models (equations) from data via symbolic regression formulated as a MINLP</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>post-hoc formal verification using a theorem prover to compute a 'reasoning error' (distance between candidate model predictions and derivations from background theory) combined with empirical error ε(f); hypotheses ranked by combination of empirical and reasoning errors</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>multi-objective: novelty and plausibility operationalized via empirical error, complexity bounds, and reasoning error relative to background axioms; can compare alternative background theories by computing reasoning errors</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>effective at enumerating candidate formulas even with few, noisy data points (qualitative claim); no aggregate numeric metrics reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>formal reasoning provides conclusive provability when background theory is complete; performance depends on completeness/consistency of B and theorem prover capabilities; quantitative rates not provided</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>validation is reliable when background theory is complete; for novel laws outside B or when B is incomplete, reasoning error increases and formal validation may fail or return approximate certificates</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>designed to reduce asymmetry by adding formal verification post-generation, but because generation and verification are sequential, the paper notes limitations in exploiting theory during generation</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>performs well when target law is derivable from B; limited when laws are outside the axioms in B or when variables are unmeasured (paper states capability to reason over unmeasured variables is an advantage)</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not applicable in standard probabilistic sense; reasoning error provides a formal groundedness metric but depends on B</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>verification can be computationally expensive depending on theorem prover and complexity of background theory; exact costs not quantified but presented as more expensive than generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>modular generator-verifier design, 'reasoning distance' metric, and coupling with theorem provers to reject spurious numerically-accurate but theory-violating formulas</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI-Descartes pairs data-driven symbolic regression with formal theorem proving to rank and reject hypotheses that conflict with background theory, improving scientific validity though constrained by sequential (post-hoc) verification and background-theory completeness.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2177.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-Hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-Hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theory-guided discovery system that integrates background axioms directly into hypothesis generation by restricting hypotheses to polynomial/rational expressions and using sum-of-squares (SOS) and conic optimization to produce certificates of derivability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evolving scientific discovery by unifying data and background knowledge with ai hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-Hilbert</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>hybrid / constrained symbolic optimization (algebraic certificate-based)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>equation discovery / physical sciences (derivable models)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>constructs polynomial (or rational) candidate laws that are constrained to satisfy algebraic consequences of background axioms as they are fit to data</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>integrates validation into generation: uses algebraic certificates (SOS, multipliers) to certify that a candidate polynomial q(x)=0 is derivable from background axioms (dc(q,G∩H)=0) or returns approximate derivability certificates when B is incomplete/inconsistent</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>distance-to-theory d_c combined with data fit and model complexity (degree bounds) form multi-objective novelty/validity criteria; novelty constrained by admissible polynomial class</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>capable of finding derivable formulas within the polynomial/rational hypothesis class; performance limited by restriction to polynomial/rational forms and computational cost of SOS relaxations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>when dc=0 provides exact algebraic certificate (strong validation); when dc>0 returns approximate certificate indicating limited validation — quantitative success rates not reported</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>validation is robust when background theory covers the novel phenomenon within the polynomial class; fails or only provides approximate certificates when theory is incomplete/inconsistent or when true law is non-polynomial</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>AI-Hilbert closes the gap by integrating validation constraints into generation, reducing generation of theory-incompatible candidates, but at cost of reduced expressivity and increased computation</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>limited for truly novel laws outside the polynomial/rational hypothesis class or outside axioms B; performs best in domains where algebraic axioms capture the phenomena</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>logical/algebraic certificates provide crisp validation rather than probabilistic calibration; certificate quality depends on B</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>high: requires solving semidefinite (SOS) or conic optimization problems; computational cost grows with polynomial degree, number of variables and complexity of axioms</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>tight coupling of data and algebraic theory via SOS optimization, derivability certificates, and constrained hypothesis classes to ensure formal consistency</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI-Hilbert integrates theory into generation, enabling formal algebraic certificates of derivability for polynomial/rational laws and reducing the generation of theory-incompatible hypotheses, but at the expense of expressivity and computational cost.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2177.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic Regression (PySR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PySR (symbolic regression engine)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fast, parallelized symbolic regression tool that searches for parsimonious analytic expressions from data using evolutionary/annealing-based search strategies; commonly used to generate candidate formulas for scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>PySR: Fast & parallelized symbolic regression in Python/Julia.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PySR</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>symbolic regression / evolutionary search</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>equation discovery / physical sciences / general</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates candidate symbolic equations and analytic relationships from numeric datasets using evolutionary/annealing search</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>empirical fit (data error), parsimony/complexity trade-offs and heuristic physics-inspired checks; does not provide formal logical verification by default</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>parsimony and out-of-sample predictive error used as proxies for novelty/utility; no formal novelty metric reported in paper</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>effective at producing parsimonious fits and candidate closed-form relations; success on rediscovery benchmarks reported in literature but no numeric metrics in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>relies on empirical generalization (test error) and parsimony; vulnerable to producing formulas that fit but are not derivable from theory</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>novel discoveries (OOD functions) are at risk of being overfit without theoretical grounding; verification still requires domain/theoretical checks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>generation is fast and broad; formal verification absent leading to potential false discoveries unless paired with theorem provers or knowledge constraints</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>variable; may overfit or hallucinate plausible formulas when extrapolating beyond training range</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not applicable; outputs are formulas without calibrated confidence estimates unless augmented</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>generation cost moderate and scalable; adding formal verification (external) increases cost substantially</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>pairing with formal verifiers (e.g., AI-Descartes pipeline) or embedding constraints can reduce false discoveries</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Symbolic regression tools like PySR produce many candidate formulas quickly but lack formal verification, so they must be combined with theory-aware checks to distinguish genuine discoveries from spurious fits.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2177.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics-inspired symbolic regression method that combines neural fitting, dimensional analysis and heuristics to discover analytic expressions from data, often used for rediscovering known physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AI Feynman: A physics-inspired method for symbolic regression.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI Feynman</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>symbolic regression / physics-informed heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>equation discovery / physical sciences</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>discovers analytic expressions by combining neural net fits, symbolic simplification and physics heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>empirical fit and physics-inspired heuristics (e.g., dimensional analysis, variable separability); does not provide formal theorem-proving validation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>success measured by rediscovery of textbook laws or low-error analytic fits; novelty beyond training distribution not emphasized in original method</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>strong on rediscovery tasks for physical laws; widely used benchmark for equation rediscovery</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation via test-set fit and heuristics; lacks formal guarantees against theory inconsistency</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>effective for rediscovery of known laws; may fail or produce ungrounded expressions on highly novel phenomena without theoretical constraints</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>generates plausible candidates but without formal verification can return spurious fits</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>limited when the underlying law is not in the class the heuristics can detect</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not applicable (no probabilistic confidence reported)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>moderate for generation; formal verification would add cost</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>combining AI Feynman outputs with formal reasoning modules (as in AI-Descartes) can improve scientific validity</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AI Feynman excels at rediscovery of textbook physical laws using physics heuristics, but lacks formal verification mechanisms necessary for validating genuinely novel discoveries.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2177.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PINNs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-Informed Neural Networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural networks that incorporate governing PDEs into training losses so the network approximates solutions that satisfy known physical equations; used for solving forward/inverse PDE problems rather than discovering new laws.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Physics-Informed Neural Networks (PINNs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural network with physics-constrained loss</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>scientific computing / PDEs / physical sciences</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates approximations to solutions of known governing PDEs and can assist in inverse problems (parameter estimation) but not designed to output new governing equations</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>validation via physics residual loss (enforcing PDE), data-fit loss, boundary-condition loss and comparison to numerical solvers or experiments</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>not primarily a novelty-generating system; novelty assessed by model generalization and residuals relative to known PDE solutions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>effective at approximating PDE solutions and parameter inference given correct PDE forms; sensitive to loss weighting and architecture choices</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation is built into training through physics residuals but guarantees are soft (penalty-based), not formal; success depends on hyperparameter balancing</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>not directly applicable; if governing equations are unknown, PINNs are not designed to discover them, so validation of novel laws is outside their standard scope</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>PINNs enforce physics during training, reducing some asymmetry by embedding constraints, but they do not produce provable derivations of new laws</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>generalization depends on training regime and enforced physics; may perform poorly OOD if constraints or data insufficient</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>no explicit probabilistic calibration; physics residuals act as a deterministic penalty but do not quantify uncertainty robustly</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>training can be expensive, and balancing multiple loss terms requires hyperparameter tuning; validation via simulation or experiments adds cost</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>embedding PDE constraints, adaptive loss weighting, physics-informed architectures; these reduce but do not eliminate need for formal verification</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PINNs embed known physics into learning and provide physics-based validation via loss terms, but because constraints are enforced softly, they do not supply formal guarantees needed for validating novel scientific laws.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2177.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Physics-inspired models (HNN/LNN/KAN)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Physics-inspired neural architectures (Hamiltonian Neural Networks, Lagrangian Neural Networks, Kolmogorov-Arnold Networks)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectures that incorporate structural physical principles into network design: HNNs/LNNs enforce conservation laws via learned Hamiltonians/Lagrangians, and KANs replace linear weights with learnable univariate functions for interpretable approximations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hamiltonian neural networks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hamiltonian / Lagrangian / Kolmogorov-Arnold networks (HNN / LNN / KAN)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>physics-inspired neural networks</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>dynamical systems modeling / physical sciences</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>learn structured dynamical models consistent with conservation laws (generate learned Hamiltonian/Lagrangian functions or interpretable functional approximations)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>validation via enforcement of structural properties (energy conservation), fit to data and simulation-based predictive checks; not through formal derivations</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty limited to discovering dynamics within structural priors; judged by improved interpretability and conservation-consistent predictions</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>works well for systems where the assumed structure is correct; can provide better generalization and sample efficiency thanks to inductive bias</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation is limited to structural constraint satisfaction and empirical predictive performance; lacks formal proof of underlying physical law discovery</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>if novel dynamics violate assumed structural priors, these models may be mis-specified leading to invalid inferences; validation degrades when real dynamics lie outside inductive bias</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>reduction of asymmetry for phenomena compatible with the encoded principles, but still lacks general formal verification for novel law discovery</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>improved relative to generic NNs when OOD respects encoded symmetries/conservation; poor when OOD violates assumed structure</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not systematically reported; model-imposed structure can improve robustness but not produce calibrated uncertainty estimates by itself</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>computation comparable to training typical neural nets; validation via simulation and conservation checks is moderate cost</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>architectural encoding of conservation/symmetry, equivariant networks, and combining with symbolic methods suggested to improve verifiability</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Physics-inspired architectures improve interpretability and enforce critical structural constraints, narrowing the generation-validation gap for compatible problems, but they do not provide formal derivability guarantees for novel laws.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2177.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Automated Theorem Provers (Lean/Coq/Isabelle)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated/formal proof assistants (Lean, Coq, Isabelle)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Formal proof assistants that can express and verify mathematical statements in a dependently-typed or higher-order logic and have been paired with LLMs to translate informal mathematics into formally verifiable statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The lean theorem prover (system description</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Lean / Coq / Isabelle (formal proof assistants)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>symbolic / formal verification systems</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>mathematics and formalized aspects of scientific theories</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>do not generate hypotheses per se (though can be paired with LLMs); primary role is to verify proofs/statements formally and produce certificates of correctness</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>formal proof checking against an explicit axiomatic system; when a theorem is proven in these systems it is formally validated under the chosen axioms</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>novelty assessed by provability from axioms and human-introduced formal statements; not suitable for open-ended empirical novelty without formal axioms</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>when used with LLM translators (e.g., AlphaProof workflows) they can enable automated theorem proving on large corpora; generation of new theorems relies on external search/LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>very high when applied to formalizable mathematics—proofs checked by the assistant are correct relative to axioms; applicability limited when scientific domains lack agreed formal axioms</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>near-zero for formal proofs (assuming correct formalization), but dependent on correctness of the formalization translation step</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>formal validation remains reliable regardless of novelty if the statement is properly formalized; novelty impacts only whether axioms suffice to prove the statement</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>these systems excel at validation but are limited in generation unless combined with search/LLMs; paper notes translating informal natural-science claims to formal statements is error-prone</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>verification is robust insofar as statements are within the formal system; cannot validate empirical scientific claims lacking formal axioms</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not applicable — proofs are binary (proved/unproved)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>can be high depending on proof complexity; automated proving and search are computationally intensive</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>pairing LLMs to translate informal statements and theorem provers to check correctness (e.g., AlphaProof pipeline) is a key approach the paper highlights</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Formal proof assistants provide gold-standard verification for formalizable claims; however, for empirical sciences lacking agreed axioms, translating informal hypotheses into formal statements is challenging and limits applicability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2177.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-based agents (ChemCrow / AtomAgents / SciAgents)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based agent systems (ChemCrow, AtomAgents, SciAgents)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systems that combine LLM reasoning with domain-specific tools (predictors, simulators, lab automation, design pipelines) to generate, test, and refine hypotheses in domain workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Augmenting large language models with chemistry tools.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based agent systems (ChemCrow, AtomAgents, SciAgents)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>agentic LLMs integrated with external domain tools</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>chemistry, materials, biomaterials and experimental workflows</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generate candidate molecules/materials, reaction plans, experimental designs and iteratively refine hypotheses using tool calls</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>integrated testing using domain-specific models/tools (reaction predictors, retrosynthesis planners, stability estimators), simulations, and sometimes experimental evaluation; also human oversight</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>assessed by exploration of chemical/material design space and simulation/experimental validation; benchmarks include rediscovery and discovery tasks but novelty metrics vary by system</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>high throughput in enumerating candidates and designing experiments; success in expanding candidate sets (e.g., materials diversity) documented qualitatively</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>tool-based validation can filter many poor candidates, but tool inaccuracies and simulation-to-reality gaps remain; empirical experimental validation remains the gold standard</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>for highly novel candidates, tool-based validation is less reliable and experimental validation becomes necessary; validation degrades as novelty increases</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>agents can generate many candidates quickly but validation via tools or experiments lags, creating a verification bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>agents may propose OOD candidates; downstream tool/simulator accuracy often degrades OOD leading to more false positives</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>LLM confidence estimates are not reliable; tool outputs may have calibrated scores but domain transfer reduces calibration quality</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>moderate to high: tool simulations and experiments require substantial compute and lab resources compared to LLM text generation</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>tight tool integration, multi-agent collaboration, and domain-specialized LLMs (e.g., NatureLM) aim to improve end-to-end validation</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based agents accelerate hypothesis generation and can perform integrated, tool-aided validation, but tool inaccuracies and experimental costs make validation the bottleneck, especially for novel candidates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2177.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2177.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR / LLM-SRBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR and LLM-SRBench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-SR is an approach using LLMs as programmer-like agents to perform scientific equation discovery; LLM-SRBench is a benchmark proposed to evaluate LLM capabilities in equation discovery and to test rediscovery vs novel discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLM-SR: Scientific equation discovery via programming with large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-SR / LLM-SRBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven symbolic regression / benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>equation discovery / symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>uses LLMs to produce code/programs or symbolic forms that attempt to recover governing equations from data</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>benchmarks use comparison to ground-truth equations (rediscovery), program execution and numeric checks; paper notes LLM-SRBench aims to evaluate novelty/generalization beyond rediscovery</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>benchmarks attempt to isolate memorization by creating tasks outside training distributions (novel tests) and measuring correct rediscovery vs failure</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>LLMs are effective zero-shot hypothesis proposers on some tasks but may rely on memorization; LLM-SR reports evolution-based exploration but numerical metrics are not quoted in this review paper</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>validation in the benchmark is performed by comparing predicted equations to ground-truth and numeric error; generalization to novel laws remains a key open challenge</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>benchmarks are designed because validation is easier on rediscovery tasks and harder on novel tasks; paper highlights need for benchmarks that penalize memorization</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>LLM-SR highlights the problem that LLM generation may appear successful on in-distribution rediscovery while failing on true novel discovery</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>benchmarks show degraded performance OOD (paper argues for more rigorous OOD evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>not reported; emphasis on designing tasks that probe reasoning rather than memorization</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>benchmark evaluation requires program execution and numeric checks which are moderate cost compared to formal verification</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>creation of stricter benchmarks (LLM-SRBench), simulated discovery domains and tool-augmented LLMs to better probe true discovery abilities</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-SR and its benchmark illustrate that LLMs can propose equations but that current evaluation often tests rediscovery rather than genuine novel discovery, masking gaps between generation and reliable validation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Combining data and theory for derivable scientific discovery with AI-Descartes <em>(Rating: 2)</em></li>
                <li>Evolving scientific discovery by unifying data and background knowledge with ai hilbert <em>(Rating: 2)</em></li>
                <li>AI Feynman: A physics-inspired method for symbolic regression. <em>(Rating: 2)</em></li>
                <li>PySR: Fast & parallelized symbolic regression in Python/Julia. <em>(Rating: 2)</em></li>
                <li>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. <em>(Rating: 2)</em></li>
                <li>Hamiltonian neural networks. <em>(Rating: 2)</em></li>
                <li>Augmenting large language models with chemistry tools. <em>(Rating: 2)</em></li>
                <li>LLM-SR: Scientific equation discovery via programming with large language models. <em>(Rating: 2)</em></li>
                <li>LLM-SRBench: A new benchmark for scientific equation discovery with large language models. <em>(Rating: 2)</em></li>
                <li>The lean theorem prover (system description <em>(Rating: 2)</em></li>
                <li>Are large language models capable of physical reasoning? (NEWTON and related evaluations) <em>(Rating: 1)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2177",
    "paper_id": "paper-281079256",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "GPT-5",
            "name_full": "GPT-5",
            "brief_description": "A state-of-the-art large language model referenced in the paper that is used as an example LLM for simple symbolic discovery tasks; shown to perform better than GPT-4 on small, illustrative equation-discovery prompts in the Appendix.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-5",
            "system_type": "large language model",
            "domain": "general scientific reasoning / equation discovery",
            "generation_capability": "generates symbolic expressions, natural language hypotheses, derivations and candidate scientific relations from numeric data and prompts",
            "validation_method": "primarily plausibility checks via prompting and qualitative inspection; in the paper GPT-5 outputs are compared to known ground-truth formulas (comparison to known results) and checked for algebraic correctness; no formal proof system is used in the Appendix examples",
            "novelty_measure": "implicit: judged by whether output matches ground-truth formula or derivation in small rediscovery tasks (no formal novelty metric provided); paper emphasizes risk of memorization vs reasoning",
            "generation_performance": "qualitative: GPT-5 produced the correct closed-form solution for a simple artificial example where GPT-4 failed; described as improved symbolic/axiomatic inference relative to earlier LLMs on small tasks, but still limited to relatively simple functional forms",
            "validation_performance": "not quantified; validation consisted of direct comparison to known formula in toy examples – GPT-5's outputs were correct in the given small examples, but broader validation performance is not reported",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "paper asserts that validation becomes more difficult when tasks are novel (out-of-training-distribution); GPT-5 can solve some novel toy problems but overall reliability on novel scientific discovery remains uncertain",
            "generation_validation_asymmetry": "evidence of asymmetry: generation is fast and can produce plausible candidates, while rigorous validation (especially on novel tasks) remains slower and less automated",
            "out_of_distribution_performance": "improved compared to GPT-4 on the toy examples, but no systematic OOD evaluation provided; authors warn LLMs often rely on memorization and can fail on genuinely novel tasks",
            "calibration_quality": "not reported quantitatively; paper discusses general LLM calibration concerns (overconfidence, hallucination) and implies calibration degrades on novel tasks",
            "validation_computational_cost": "generation is low-cost relative to formal verification; validation in the Appendix used human inspection/comparison which is inexpensive for toy tasks but scales poorly",
            "human_validation_required": true,
            "gap_closing_mechanisms": "paired comparison to ground-truth, use of theorem provers or symbolic-verification frameworks recommended but not used in these GPT-5 examples",
            "evidence_type": "supports",
            "key_findings": "GPT-5 can succeed at small symbolic rediscovery tasks where earlier LLMs failed, but the paper emphasizes that such successes do not generalize to robust, automated scientific verification and that formal verification remains necessary for novel discoveries.",
            "uuid": "e2177.0"
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A widely used large language model that the authors tested on toy equation-discovery prompts and which failed on a simple artificial example used in the Appendix.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4",
            "system_type": "large language model",
            "domain": "general scientific reasoning / equation discovery",
            "generation_capability": "produces candidate symbolic forms, natural-language hypotheses and derivations from prompts; used for limited symbolic regression-like tasks",
            "validation_method": "comparison to known ground-truth in toy examples and human inspection; no formal verification used in the examples",
            "novelty_measure": "implicit (matching ground-truth vs. not); paper notes GPT-4 failed on a toy novel example",
            "generation_performance": "qualitative: failed to produce correct expression in a simple artificial example described in Appendix, indicating limits on symbolic reasoning in that instance",
            "validation_performance": "not quantified; validation limited to direct comparison with ground-truth in toy examples",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "the failure on the toy novel example demonstrates poorer performance on novel reasoning tasks compared to some newer LLMs (e.g., GPT-5)",
            "generation_validation_asymmetry": "evidence that generation (producing many candidate forms) can occur even when correct validation/reasoning fails—GPT-4 generated plausible but incorrect expressions",
            "out_of_distribution_performance": "degraded vs. GPT-5 on the toy example; no systematic OOD metrics reported",
            "calibration_quality": "not reported quantitatively; general discussion in paper indicates LLMs can be confidently wrong (poor calibration) especially OOD",
            "validation_computational_cost": "generation inexpensive; verification limited to human checking in examples",
            "human_validation_required": true,
            "gap_closing_mechanisms": "formal verification or integrated neuro-symbolic methods suggested to compensate for GPT-4 failures",
            "evidence_type": "supports",
            "key_findings": "GPT-4 failed on a simple symbolic example whereas GPT-5 succeeded, illustrating that LLM generation ability can improve but that verification remains necessary, particularly for novel tasks.",
            "uuid": "e2177.1"
        },
        {
            "name_short": "LLMs (general)",
            "name_full": "Large Language Models (general)",
            "brief_description": "Generative pre-trained transformer models used as hypothesis proposers, reasoning assistants, and agentic components in workflows that automate parts of scientific discovery.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Large language models (LLMs)",
            "system_type": "large language model",
            "domain": "general scientific discovery, materials, chemistry, biology, theorem proving",
            "generation_capability": "generate hypotheses, literature summaries, candidate equations, experimental plans, and natural-language reasoning chains; can be configured as agents that call external tools",
            "validation_method": "various: human review, comparison to existing literature, integration with external simulation or lab tools, or (partial) RLHF; however RLHF is described as improving plausibility rather than providing formal verification",
            "novelty_measure": "commonly assessed by comparison to training data (memorization vs reasoning), rediscovery benchmarks, or benchmarks designed to test open-ended discovery (LLM-SRBench, simulated domains); paper notes that many benchmarks are rediscovery-focused and don't test true novelty",
            "generation_performance": "high throughput and breadth in producing candidate hypotheses; qualitative strengths in fluency and broad knowledge but variable correctness, especially for symbolic/math tasks; performance better on familiar/textbook-style problems than on novel open-ended discovery",
            "validation_performance": "limited: RLHF improves human-perceived plausibility but does not guarantee truth; LLMs hallucinate references and violate basic algebra/physics in reported cases; formal verification is generally lacking",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "validation reliability degrades for novel, out-of-distribution tasks where memorization cannot rescue the model; authors stress benchmarks must test novelty explicitly",
            "generation_validation_asymmetry": "strong asymmetry: LLMs generate many plausible-seeming outputs quickly, while reliable automated validation of those outputs remains rare and expensive",
            "out_of_distribution_performance": "often poor; LLMs may rely on memorized patterns causing failure on genuinely novel tasks unless specifically trained/evaluated for reasoning",
            "calibration_quality": "generally poor for truthfulness; RLHF increases alignment with human preference but not with formal correctness",
            "validation_computational_cost": "generation is inexpensive relative to rigorous verification; integrating external tools or formal provers increases cost and pipeline complexity",
            "human_validation_required": true,
            "gap_closing_mechanisms": "integration with formal theorem provers, tool-augmented agents, hybrid neuro-symbolic systems, domain-specific LLMs, and dedicated benchmarks are proposed to reduce the gap",
            "evidence_type": "supports",
            "key_findings": "LLMs are powerful hypothesis proposers but lack principled, automated verification mechanisms; RLHF improves perceived quality but not formal correctness, and performance falls on novel tasks.",
            "uuid": "e2177.2"
        },
        {
            "name_short": "AI-Descartes",
            "name_full": "AI-Descartes",
            "brief_description": "A neuro-symbolic generator-verifier framework that pairs symbolic regression with formal theorem proving to generate candidate symbolic laws from data and then verify consistency with background theory via a reasoning distance metric.",
            "citation_title": "Combining data and theory for derivable scientific discovery with AI-Descartes",
            "mention_or_use": "use",
            "system_name": "AI-Descartes",
            "system_type": "neurosymbolic / hybrid (symbolic regression + automated theorem prover)",
            "domain": "equation discovery / physical sciences",
            "generation_capability": "generates symbolic candidate models (equations) from data via symbolic regression formulated as a MINLP",
            "validation_method": "post-hoc formal verification using a theorem prover to compute a 'reasoning error' (distance between candidate model predictions and derivations from background theory) combined with empirical error ε(f); hypotheses ranked by combination of empirical and reasoning errors",
            "novelty_measure": "multi-objective: novelty and plausibility operationalized via empirical error, complexity bounds, and reasoning error relative to background axioms; can compare alternative background theories by computing reasoning errors",
            "generation_performance": "effective at enumerating candidate formulas even with few, noisy data points (qualitative claim); no aggregate numeric metrics reported in this paper",
            "validation_performance": "formal reasoning provides conclusive provability when background theory is complete; performance depends on completeness/consistency of B and theorem prover capabilities; quantitative rates not provided",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "validation is reliable when background theory is complete; for novel laws outside B or when B is incomplete, reasoning error increases and formal validation may fail or return approximate certificates",
            "generation_validation_asymmetry": "designed to reduce asymmetry by adding formal verification post-generation, but because generation and verification are sequential, the paper notes limitations in exploiting theory during generation",
            "out_of_distribution_performance": "performs well when target law is derivable from B; limited when laws are outside the axioms in B or when variables are unmeasured (paper states capability to reason over unmeasured variables is an advantage)",
            "calibration_quality": "not applicable in standard probabilistic sense; reasoning error provides a formal groundedness metric but depends on B",
            "validation_computational_cost": "verification can be computationally expensive depending on theorem prover and complexity of background theory; exact costs not quantified but presented as more expensive than generation",
            "human_validation_required": true,
            "gap_closing_mechanisms": "modular generator-verifier design, 'reasoning distance' metric, and coupling with theorem provers to reject spurious numerically-accurate but theory-violating formulas",
            "evidence_type": "supports",
            "key_findings": "AI-Descartes pairs data-driven symbolic regression with formal theorem proving to rank and reject hypotheses that conflict with background theory, improving scientific validity though constrained by sequential (post-hoc) verification and background-theory completeness.",
            "uuid": "e2177.3"
        },
        {
            "name_short": "AI-Hilbert",
            "name_full": "AI-Hilbert",
            "brief_description": "A theory-guided discovery system that integrates background axioms directly into hypothesis generation by restricting hypotheses to polynomial/rational expressions and using sum-of-squares (SOS) and conic optimization to produce certificates of derivability.",
            "citation_title": "Evolving scientific discovery by unifying data and background knowledge with ai hilbert",
            "mention_or_use": "use",
            "system_name": "AI-Hilbert",
            "system_type": "hybrid / constrained symbolic optimization (algebraic certificate-based)",
            "domain": "equation discovery / physical sciences (derivable models)",
            "generation_capability": "constructs polynomial (or rational) candidate laws that are constrained to satisfy algebraic consequences of background axioms as they are fit to data",
            "validation_method": "integrates validation into generation: uses algebraic certificates (SOS, multipliers) to certify that a candidate polynomial q(x)=0 is derivable from background axioms (dc(q,G∩H)=0) or returns approximate derivability certificates when B is incomplete/inconsistent",
            "novelty_measure": "distance-to-theory d_c combined with data fit and model complexity (degree bounds) form multi-objective novelty/validity criteria; novelty constrained by admissible polynomial class",
            "generation_performance": "capable of finding derivable formulas within the polynomial/rational hypothesis class; performance limited by restriction to polynomial/rational forms and computational cost of SOS relaxations",
            "validation_performance": "when dc=0 provides exact algebraic certificate (strong validation); when dc&gt;0 returns approximate certificate indicating limited validation — quantitative success rates not reported",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "validation is robust when background theory covers the novel phenomenon within the polynomial class; fails or only provides approximate certificates when theory is incomplete/inconsistent or when true law is non-polynomial",
            "generation_validation_asymmetry": "AI-Hilbert closes the gap by integrating validation constraints into generation, reducing generation of theory-incompatible candidates, but at cost of reduced expressivity and increased computation",
            "out_of_distribution_performance": "limited for truly novel laws outside the polynomial/rational hypothesis class or outside axioms B; performs best in domains where algebraic axioms capture the phenomena",
            "calibration_quality": "logical/algebraic certificates provide crisp validation rather than probabilistic calibration; certificate quality depends on B",
            "validation_computational_cost": "high: requires solving semidefinite (SOS) or conic optimization problems; computational cost grows with polynomial degree, number of variables and complexity of axioms",
            "human_validation_required": true,
            "gap_closing_mechanisms": "tight coupling of data and algebraic theory via SOS optimization, derivability certificates, and constrained hypothesis classes to ensure formal consistency",
            "evidence_type": "supports",
            "key_findings": "AI-Hilbert integrates theory into generation, enabling formal algebraic certificates of derivability for polynomial/rational laws and reducing the generation of theory-incompatible hypotheses, but at the expense of expressivity and computational cost.",
            "uuid": "e2177.4"
        },
        {
            "name_short": "Symbolic Regression (PySR)",
            "name_full": "PySR (symbolic regression engine)",
            "brief_description": "A fast, parallelized symbolic regression tool that searches for parsimonious analytic expressions from data using evolutionary/annealing-based search strategies; commonly used to generate candidate formulas for scientific discovery.",
            "citation_title": "PySR: Fast & parallelized symbolic regression in Python/Julia.",
            "mention_or_use": "mention",
            "system_name": "PySR",
            "system_type": "symbolic regression / evolutionary search",
            "domain": "equation discovery / physical sciences / general",
            "generation_capability": "generates candidate symbolic equations and analytic relationships from numeric datasets using evolutionary/annealing search",
            "validation_method": "empirical fit (data error), parsimony/complexity trade-offs and heuristic physics-inspired checks; does not provide formal logical verification by default",
            "novelty_measure": "parsimony and out-of-sample predictive error used as proxies for novelty/utility; no formal novelty metric reported in paper",
            "generation_performance": "effective at producing parsimonious fits and candidate closed-form relations; success on rediscovery benchmarks reported in literature but no numeric metrics in this paper",
            "validation_performance": "relies on empirical generalization (test error) and parsimony; vulnerable to producing formulas that fit but are not derivable from theory",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "novel discoveries (OOD functions) are at risk of being overfit without theoretical grounding; verification still requires domain/theoretical checks",
            "generation_validation_asymmetry": "generation is fast and broad; formal verification absent leading to potential false discoveries unless paired with theorem provers or knowledge constraints",
            "out_of_distribution_performance": "variable; may overfit or hallucinate plausible formulas when extrapolating beyond training range",
            "calibration_quality": "not applicable; outputs are formulas without calibrated confidence estimates unless augmented",
            "validation_computational_cost": "generation cost moderate and scalable; adding formal verification (external) increases cost substantially",
            "human_validation_required": true,
            "gap_closing_mechanisms": "pairing with formal verifiers (e.g., AI-Descartes pipeline) or embedding constraints can reduce false discoveries",
            "evidence_type": "supports",
            "key_findings": "Symbolic regression tools like PySR produce many candidate formulas quickly but lack formal verification, so they must be combined with theory-aware checks to distinguish genuine discoveries from spurious fits.",
            "uuid": "e2177.5"
        },
        {
            "name_short": "AI Feynman",
            "name_full": "AI Feynman",
            "brief_description": "A physics-inspired symbolic regression method that combines neural fitting, dimensional analysis and heuristics to discover analytic expressions from data, often used for rediscovering known physical laws.",
            "citation_title": "AI Feynman: A physics-inspired method for symbolic regression.",
            "mention_or_use": "mention",
            "system_name": "AI Feynman",
            "system_type": "symbolic regression / physics-informed heuristics",
            "domain": "equation discovery / physical sciences",
            "generation_capability": "discovers analytic expressions by combining neural net fits, symbolic simplification and physics heuristics",
            "validation_method": "empirical fit and physics-inspired heuristics (e.g., dimensional analysis, variable separability); does not provide formal theorem-proving validation",
            "novelty_measure": "success measured by rediscovery of textbook laws or low-error analytic fits; novelty beyond training distribution not emphasized in original method",
            "generation_performance": "strong on rediscovery tasks for physical laws; widely used benchmark for equation rediscovery",
            "validation_performance": "validation via test-set fit and heuristics; lacks formal guarantees against theory inconsistency",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "effective for rediscovery of known laws; may fail or produce ungrounded expressions on highly novel phenomena without theoretical constraints",
            "generation_validation_asymmetry": "generates plausible candidates but without formal verification can return spurious fits",
            "out_of_distribution_performance": "limited when the underlying law is not in the class the heuristics can detect",
            "calibration_quality": "not applicable (no probabilistic confidence reported)",
            "validation_computational_cost": "moderate for generation; formal verification would add cost",
            "human_validation_required": true,
            "gap_closing_mechanisms": "combining AI Feynman outputs with formal reasoning modules (as in AI-Descartes) can improve scientific validity",
            "evidence_type": "supports",
            "key_findings": "AI Feynman excels at rediscovery of textbook physical laws using physics heuristics, but lacks formal verification mechanisms necessary for validating genuinely novel discoveries.",
            "uuid": "e2177.6"
        },
        {
            "name_short": "PINNs",
            "name_full": "Physics-Informed Neural Networks (PINNs)",
            "brief_description": "Neural networks that incorporate governing PDEs into training losses so the network approximates solutions that satisfy known physical equations; used for solving forward/inverse PDE problems rather than discovering new laws.",
            "citation_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.",
            "mention_or_use": "mention",
            "system_name": "Physics-Informed Neural Networks (PINNs)",
            "system_type": "neural network with physics-constrained loss",
            "domain": "scientific computing / PDEs / physical sciences",
            "generation_capability": "generates approximations to solutions of known governing PDEs and can assist in inverse problems (parameter estimation) but not designed to output new governing equations",
            "validation_method": "validation via physics residual loss (enforcing PDE), data-fit loss, boundary-condition loss and comparison to numerical solvers or experiments",
            "novelty_measure": "not primarily a novelty-generating system; novelty assessed by model generalization and residuals relative to known PDE solutions",
            "generation_performance": "effective at approximating PDE solutions and parameter inference given correct PDE forms; sensitive to loss weighting and architecture choices",
            "validation_performance": "validation is built into training through physics residuals but guarantees are soft (penalty-based), not formal; success depends on hyperparameter balancing",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "not directly applicable; if governing equations are unknown, PINNs are not designed to discover them, so validation of novel laws is outside their standard scope",
            "generation_validation_asymmetry": "PINNs enforce physics during training, reducing some asymmetry by embedding constraints, but they do not produce provable derivations of new laws",
            "out_of_distribution_performance": "generalization depends on training regime and enforced physics; may perform poorly OOD if constraints or data insufficient",
            "calibration_quality": "no explicit probabilistic calibration; physics residuals act as a deterministic penalty but do not quantify uncertainty robustly",
            "validation_computational_cost": "training can be expensive, and balancing multiple loss terms requires hyperparameter tuning; validation via simulation or experiments adds cost",
            "human_validation_required": true,
            "gap_closing_mechanisms": "embedding PDE constraints, adaptive loss weighting, physics-informed architectures; these reduce but do not eliminate need for formal verification",
            "evidence_type": "mixed",
            "key_findings": "PINNs embed known physics into learning and provide physics-based validation via loss terms, but because constraints are enforced softly, they do not supply formal guarantees needed for validating novel scientific laws.",
            "uuid": "e2177.7"
        },
        {
            "name_short": "Physics-inspired models (HNN/LNN/KAN)",
            "name_full": "Physics-inspired neural architectures (Hamiltonian Neural Networks, Lagrangian Neural Networks, Kolmogorov-Arnold Networks)",
            "brief_description": "Architectures that incorporate structural physical principles into network design: HNNs/LNNs enforce conservation laws via learned Hamiltonians/Lagrangians, and KANs replace linear weights with learnable univariate functions for interpretable approximations.",
            "citation_title": "Hamiltonian neural networks.",
            "mention_or_use": "mention",
            "system_name": "Hamiltonian / Lagrangian / Kolmogorov-Arnold networks (HNN / LNN / KAN)",
            "system_type": "physics-inspired neural networks",
            "domain": "dynamical systems modeling / physical sciences",
            "generation_capability": "learn structured dynamical models consistent with conservation laws (generate learned Hamiltonian/Lagrangian functions or interpretable functional approximations)",
            "validation_method": "validation via enforcement of structural properties (energy conservation), fit to data and simulation-based predictive checks; not through formal derivations",
            "novelty_measure": "novelty limited to discovering dynamics within structural priors; judged by improved interpretability and conservation-consistent predictions",
            "generation_performance": "works well for systems where the assumed structure is correct; can provide better generalization and sample efficiency thanks to inductive bias",
            "validation_performance": "validation is limited to structural constraint satisfaction and empirical predictive performance; lacks formal proof of underlying physical law discovery",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "if novel dynamics violate assumed structural priors, these models may be mis-specified leading to invalid inferences; validation degrades when real dynamics lie outside inductive bias",
            "generation_validation_asymmetry": "reduction of asymmetry for phenomena compatible with the encoded principles, but still lacks general formal verification for novel law discovery",
            "out_of_distribution_performance": "improved relative to generic NNs when OOD respects encoded symmetries/conservation; poor when OOD violates assumed structure",
            "calibration_quality": "not systematically reported; model-imposed structure can improve robustness but not produce calibrated uncertainty estimates by itself",
            "validation_computational_cost": "computation comparable to training typical neural nets; validation via simulation and conservation checks is moderate cost",
            "human_validation_required": true,
            "gap_closing_mechanisms": "architectural encoding of conservation/symmetry, equivariant networks, and combining with symbolic methods suggested to improve verifiability",
            "evidence_type": "mixed",
            "key_findings": "Physics-inspired architectures improve interpretability and enforce critical structural constraints, narrowing the generation-validation gap for compatible problems, but they do not provide formal derivability guarantees for novel laws.",
            "uuid": "e2177.8"
        },
        {
            "name_short": "Automated Theorem Provers (Lean/Coq/Isabelle)",
            "name_full": "Automated/formal proof assistants (Lean, Coq, Isabelle)",
            "brief_description": "Formal proof assistants that can express and verify mathematical statements in a dependently-typed or higher-order logic and have been paired with LLMs to translate informal mathematics into formally verifiable statements.",
            "citation_title": "The lean theorem prover (system description",
            "mention_or_use": "mention",
            "system_name": "Lean / Coq / Isabelle (formal proof assistants)",
            "system_type": "symbolic / formal verification systems",
            "domain": "mathematics and formalized aspects of scientific theories",
            "generation_capability": "do not generate hypotheses per se (though can be paired with LLMs); primary role is to verify proofs/statements formally and produce certificates of correctness",
            "validation_method": "formal proof checking against an explicit axiomatic system; when a theorem is proven in these systems it is formally validated under the chosen axioms",
            "novelty_measure": "novelty assessed by provability from axioms and human-introduced formal statements; not suitable for open-ended empirical novelty without formal axioms",
            "generation_performance": "when used with LLM translators (e.g., AlphaProof workflows) they can enable automated theorem proving on large corpora; generation of new theorems relies on external search/LLMs",
            "validation_performance": "very high when applied to formalizable mathematics—proofs checked by the assistant are correct relative to axioms; applicability limited when scientific domains lack agreed formal axioms",
            "false_positive_rate": "near-zero for formal proofs (assuming correct formalization), but dependent on correctness of the formalization translation step",
            "false_negative_rate": null,
            "novelty_effect_on_validation": "formal validation remains reliable regardless of novelty if the statement is properly formalized; novelty impacts only whether axioms suffice to prove the statement",
            "generation_validation_asymmetry": "these systems excel at validation but are limited in generation unless combined with search/LLMs; paper notes translating informal natural-science claims to formal statements is error-prone",
            "out_of_distribution_performance": "verification is robust insofar as statements are within the formal system; cannot validate empirical scientific claims lacking formal axioms",
            "calibration_quality": "not applicable — proofs are binary (proved/unproved)",
            "validation_computational_cost": "can be high depending on proof complexity; automated proving and search are computationally intensive",
            "human_validation_required": true,
            "gap_closing_mechanisms": "pairing LLMs to translate informal statements and theorem provers to check correctness (e.g., AlphaProof pipeline) is a key approach the paper highlights",
            "evidence_type": "supports",
            "key_findings": "Formal proof assistants provide gold-standard verification for formalizable claims; however, for empirical sciences lacking agreed axioms, translating informal hypotheses into formal statements is challenging and limits applicability.",
            "uuid": "e2177.9"
        },
        {
            "name_short": "LLM-based agents (ChemCrow / AtomAgents / SciAgents)",
            "name_full": "LLM-based agent systems (ChemCrow, AtomAgents, SciAgents)",
            "brief_description": "Systems that combine LLM reasoning with domain-specific tools (predictors, simulators, lab automation, design pipelines) to generate, test, and refine hypotheses in domain workflows.",
            "citation_title": "Augmenting large language models with chemistry tools.",
            "mention_or_use": "mention",
            "system_name": "LLM-based agent systems (ChemCrow, AtomAgents, SciAgents)",
            "system_type": "agentic LLMs integrated with external domain tools",
            "domain": "chemistry, materials, biomaterials and experimental workflows",
            "generation_capability": "generate candidate molecules/materials, reaction plans, experimental designs and iteratively refine hypotheses using tool calls",
            "validation_method": "integrated testing using domain-specific models/tools (reaction predictors, retrosynthesis planners, stability estimators), simulations, and sometimes experimental evaluation; also human oversight",
            "novelty_measure": "assessed by exploration of chemical/material design space and simulation/experimental validation; benchmarks include rediscovery and discovery tasks but novelty metrics vary by system",
            "generation_performance": "high throughput in enumerating candidates and designing experiments; success in expanding candidate sets (e.g., materials diversity) documented qualitatively",
            "validation_performance": "tool-based validation can filter many poor candidates, but tool inaccuracies and simulation-to-reality gaps remain; empirical experimental validation remains the gold standard",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "for highly novel candidates, tool-based validation is less reliable and experimental validation becomes necessary; validation degrades as novelty increases",
            "generation_validation_asymmetry": "agents can generate many candidates quickly but validation via tools or experiments lags, creating a verification bottleneck",
            "out_of_distribution_performance": "agents may propose OOD candidates; downstream tool/simulator accuracy often degrades OOD leading to more false positives",
            "calibration_quality": "LLM confidence estimates are not reliable; tool outputs may have calibrated scores but domain transfer reduces calibration quality",
            "validation_computational_cost": "moderate to high: tool simulations and experiments require substantial compute and lab resources compared to LLM text generation",
            "human_validation_required": true,
            "gap_closing_mechanisms": "tight tool integration, multi-agent collaboration, and domain-specialized LLMs (e.g., NatureLM) aim to improve end-to-end validation",
            "evidence_type": "supports",
            "key_findings": "LLM-based agents accelerate hypothesis generation and can perform integrated, tool-aided validation, but tool inaccuracies and experimental costs make validation the bottleneck, especially for novel candidates.",
            "uuid": "e2177.10"
        },
        {
            "name_short": "LLM-SR / LLM-SRBench",
            "name_full": "LLM-SR and LLM-SRBench",
            "brief_description": "LLM-SR is an approach using LLMs as programmer-like agents to perform scientific equation discovery; LLM-SRBench is a benchmark proposed to evaluate LLM capabilities in equation discovery and to test rediscovery vs novel discovery.",
            "citation_title": "LLM-SR: Scientific equation discovery via programming with large language models.",
            "mention_or_use": "mention",
            "system_name": "LLM-SR / LLM-SRBench",
            "system_type": "LLM-driven symbolic regression / benchmark",
            "domain": "equation discovery / symbolic regression",
            "generation_capability": "uses LLMs to produce code/programs or symbolic forms that attempt to recover governing equations from data",
            "validation_method": "benchmarks use comparison to ground-truth equations (rediscovery), program execution and numeric checks; paper notes LLM-SRBench aims to evaluate novelty/generalization beyond rediscovery",
            "novelty_measure": "benchmarks attempt to isolate memorization by creating tasks outside training distributions (novel tests) and measuring correct rediscovery vs failure",
            "generation_performance": "LLMs are effective zero-shot hypothesis proposers on some tasks but may rely on memorization; LLM-SR reports evolution-based exploration but numerical metrics are not quoted in this review paper",
            "validation_performance": "validation in the benchmark is performed by comparing predicted equations to ground-truth and numeric error; generalization to novel laws remains a key open challenge",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "benchmarks are designed because validation is easier on rediscovery tasks and harder on novel tasks; paper highlights need for benchmarks that penalize memorization",
            "generation_validation_asymmetry": "LLM-SR highlights the problem that LLM generation may appear successful on in-distribution rediscovery while failing on true novel discovery",
            "out_of_distribution_performance": "benchmarks show degraded performance OOD (paper argues for more rigorous OOD evaluation)",
            "calibration_quality": "not reported; emphasis on designing tasks that probe reasoning rather than memorization",
            "validation_computational_cost": "benchmark evaluation requires program execution and numeric checks which are moderate cost compared to formal verification",
            "human_validation_required": true,
            "gap_closing_mechanisms": "creation of stricter benchmarks (LLM-SRBench), simulated discovery domains and tool-augmented LLMs to better probe true discovery abilities",
            "evidence_type": "supports",
            "key_findings": "LLM-SR and its benchmark illustrate that LLMs can propose equations but that current evaluation often tests rediscovery rather than genuine novel discovery, masking gaps between generation and reliable validation.",
            "uuid": "e2177.11"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Combining data and theory for derivable scientific discovery with AI-Descartes",
            "rating": 2
        },
        {
            "paper_title": "Evolving scientific discovery by unifying data and background knowledge with ai hilbert",
            "rating": 2
        },
        {
            "paper_title": "AI Feynman: A physics-inspired method for symbolic regression.",
            "rating": 2
        },
        {
            "paper_title": "PySR: Fast & parallelized symbolic regression in Python/Julia.",
            "rating": 2
        },
        {
            "paper_title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.",
            "rating": 2
        },
        {
            "paper_title": "Hamiltonian neural networks.",
            "rating": 2
        },
        {
            "paper_title": "Augmenting large language models with chemistry tools.",
            "rating": 2
        },
        {
            "paper_title": "LLM-SR: Scientific equation discovery via programming with large language models.",
            "rating": 2
        },
        {
            "paper_title": "LLM-SRBench: A new benchmark for scientific equation discovery with large language models.",
            "rating": 2
        },
        {
            "paper_title": "The lean theorem prover (system description",
            "rating": 2
        },
        {
            "paper_title": "Are large language models capable of physical reasoning? (NEWTON and related evaluations)",
            "rating": 1
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 1
        }
    ],
    "cost": 0.02538575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>The Need for Verification in AI-Driven Scientific Discovery
1 Sep 2025</p>
<p>Cristina Cornelio c.cornelio@samsung.com 
Samsung AI
CambridgeUK</p>
<p>Takuya Ito 
IBM Research
Yorktown Heights
USA</p>
<p>Ryan Cory-Wright 
Imperial Business School
LondonUK</p>
<p>Sanjeeb Dash 
IBM Research
Yorktown Heights
USA</p>
<p>Lior Horesh 
IBM Research
Yorktown Heights
USA</p>
<p>The Need for Verification in AI-Driven Scientific Discovery
1 Sep 202545C4E3C2E7946BC55E73773029B9338EarXiv:2509.01398v1[cs.AI]
Artificial intelligence (AI) is transforming the practice of science.Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields.However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced.In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents.While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.</p>
<p>Introduction</p>
<p>The overarching goal of science is to provide a set of universal, accurate, and interpretable explanations that describe the natural world.This involves discovering natural laws that not only make accurate predictions but are also corroborated by existing scientific literature.Such laws have historically been discovered through the scientific method, a systematic process that begins with a question and proceeds through a study phase during which researchers gather all prior knowledge and data pertaining to the phenomenon under investigation.This leads to the formulation of hypotheses, empirical validation, and iterative refinement.The scientific method enables the discovery of verifiable scientific truths by relying on substantiated and repeatable evidence, lending science its legitimacy and credibility.</p>
<p>The shift from dogmatic belief systems to a framework grounded in scientific theory and empirical verification, epitomized by the 16th-century transition from religious authority to human reason through the work of Copernicus, Galileo, and Bruno, marked a fundamental epistemological transformation leading to the Scientific Revolution [Leveillee, 2011].Indeed, Kepler's mathematical description of planetary motion, grounded in Tycho Brahe's observational data and Bacon's advocacy for inductive reasoning, established the foundation for modern empirical science [Bacon, 1878, Westfall, 1977].Critically, the consequences of this verification-driven methodology have been profound in the modern age.Verified discoveries such as germ theory [Latour, 1993], high-yield agricultural practices [Borlaug, 1970], and thermodynamic principles [Smil, 2004] have transformed medicine, food production, and energy systems.These advances were achieved through a disciplined integration of theoretical models and experimental validation.</p>
<p>However, the rate of major discoveries has declined in both absolute and relative terms over the past several decades [Bloom et al., 2020, Bhattacharya and Packalen, 2020, Arora et al., 2018].This decline is arguably due to the exhaustion of simple, low-dimensional theories, and the increasing complexity of modern scientific problems [Cowen, 2011] (see Fig. 1A).Amid these challenges, the rise of machine learning and artificial intelligence has introduced promising tools to augment hypothesis generation and data analysis for scientific discovery.However, many existing implementations of these data-driven systems lack formal mechanisms for logical inference that are essential for verifiable scientific discovery [Platt, 1964].In particular, generative AI models have shown remarkable capacity to rapidly generate novel scientific hypotheses [Gottweis et al., 2025, Yamada et al., 2025, Lu et al., 2024, Jumper et al., 2021].However, these outputs often lack empirical grounding and are frequently disconnected from established theoretical frameworks or domain-specific knowledge.This disconnect has led to an overwhelming influx of unverified hypotheses, straining verification pipelines that are essential for validating scientific discoveries [Beel et al., 2025, Gridach et al., 2025, Kulkarni et al., 2025] (see Fig. 1B).Consequently, developing robust methods to refine and verify hypotheses from data-driven approaches is critical to unlocking the full potential of AI in accelerating scientific progress (Fig. 1C).</p>
<p>Traditional scientific pipeline AI-assisted scientific pipeline</p>
<p>Verification bottleneck</p>
<p>Widen verification via automated verification</p>
<p>Accelerate scientific discoveries</p>
<p>Hypotheses crafted from theory and ontology</p>
<p>Rapid &amp; abundant AI-generated hypotheses</p>
<p>The Need for Improved Verification Methods</p>
<p>AI-generated hypotheses</p>
<p>Hypotheses Many Many Few Scientific Discoveries</p>
<p>A B C</p>
<p>Figure 1: The scientific method in the age of AI.A) In the traditional scientific method, theories guide the generation of testable hypotheses, which are then validated through experiments and data.B) However, with generative AI, hypotheses can be rapidly produced from data, but verification still relies on slow, manual evaluation by domain experts.C) Without widening this verification bottleneck (e.g., through automated/integrated verification) the pace of discovery remains limited, despite the acceleration promised by AI.</p>
<p>To address the limitations of purely data-driven approaches, several recent works propose hybrid frameworks.These approaches integrate machine learning with elements of symbolic reasoning, constraints imposition, and formal logic, aiming to ensure scientific validity alongside predictive accuracy (e.g., see Wiberg et al. [2025] for a review on integration between AI/ML and Operations Research techniques).For example, Kolmogorov-Arnold Networks (KANs) [Liu et al., 2025] replace fixed linear weights with learnable univariate functions, producing interpretable approximations of scientific relations.Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019] similarly enforce energy conservation by learning a Hamiltonian and deriving system dynamics from Hamilton's equations.While the learned models from both systems respect structural embeddings, verification for both systems is limited to their specific structural property.More recently, AI-Descartes [Cornelio et al., 2023a] introduced a general verification mechanism, where hypotheses were generated via a data-driven approach and later verified against known theory via theorem proving.Building upon that work, AI-Hilbert [Cory-Wright et al., 2024] integrated data and theory directly during the hypothesis generation stage, thereby constraining the search to expressions consistent with both data and theory.While both these approaches provide scientifically verifiable results, their application is limited to specific problem formulations in the physical sciences.Thus, while emerging computational tools offer great promise for broadly accelerating scientific discovery, their effectiveness hinges on ensuring resulting insights are not only predictive but also interpretable, verifiable, and aligned with foundational scientific knowledge.</p>
<p>In this article, we review recent progress in AI-driven scientific discovery, while underscoring the critical importance of verifying these methods throughout the discovery process.We begin by highlighting key historical examples where the failure to rigorously verify computational methods led to the collapse of critical missions, resulting in significant loss of life and financial cost.Next, we examine recent data-driven approaches to scientific discovery, highlighting their ability to uncover patterns and generate hypotheses from large datasets, particularly in domains where theoretical models are incomplete or unavailable.This is followed by a comparison with knowledge-aware methods, the emergence of derivable models that integrate symbolic reasoning, and the growing role of large language models (LLMs) in automating and augmenting scientific workflows.We conclude with a broader perspective on the heterogeneous role of verification across scientific domains, outlining current challenges and suggesting promising approaches for future research.</p>
<p>The Importance of Verification: Evidence from Examples</p>
<p>In 1999, NASA's Mars Climate Orbiter was lost when thruster data delivered in pounds per second was interpreted as Newtons per second, a unit mismatch that sent a $125 million spacecraft into the Martian atmosphere [NASA, 1999].Similarly, in 1983, Air Canada Flight 143 ("Gimli Glider") ran out of fuel mid-air after the ground crew incorrectly converted pounds to kilograms during Canada's metric transition, leaving the aircraft with roughly half the required fuel and forcing a dangerous emergency landing [Lockwood, 1985].Similar errors have been documented in hospitals: children given incorrect doses when weights noted in pounds were treated as kilograms [Bokser, 2013]; in a review of 1, 291 weight-related medication error reports to the Pennsylvania Patient Safety Authority, 23.2% involved pounds-kilograms confusion [Bailey et al., 2016]; and selection of high-strength heparin vials (10, 000 U/mL) were mistaken for low-strength ones [Institute for Safe Medication Practices Canada, 2008].The lesson from these domains is clear: without rigorous, automated verification, minor trivial errors can scale into disasters.</p>
<p>In automated scientific discovery, the same principle applies.Automated model-generation tools have transformed scientific discovery.Symbolic regression engines such as PySR [Cranmer, 2020] and AI Feynman [Udrescu and Tegmark, 2020], as well as neural architectures like Kolmogorov-Arnold Networks (KANs) [Liu et al., 2025], Hamiltonian Neural Networks (HNNs) [Greydanus et al., 2019], and Lagrangian Neural Networks (LNNs) [Cranmer et al., 2020a], now produce many new hypotheses quickly.This proliferation creates a new bottleneck: distinguishing between formulas that merely fit the data and those that are scientifically meaningful (Fig. 1).Without rigorous verification, the flood of generated hypotheses risks overwhelming the scientific process with plausible but superficial results.Thus, verification is an essential filter that separates genuine scientific discoveries from hallucinations or mere noisy interpolations that fail to generalize beyond the observed data.</p>
<p>The challenge of verification is further exacerbated by the rise of LLM-based tools whose reliability can be strongly questioned [Marcus, 2025, Kambhampati, 2024].Well-publicized examples of hallucinations from LLMs include the hallucination of legal cases cited in court filings [Weiss, 2023], the fabrication of biomedical references [Gravel et al., 2023], and outputs violating basic algebraic [Hendrycks et al., 2021] or physical consistency [Wang et al., 2023b].We refer to Zhang et al. [2025] for a recent review of LLMs and their use in scientific discovery.</p>
<p>One might argue that the latest generation of Large Language Models (LLMs), which utilize Reinforcement Learning from Human Feedback (RLHF) to steer outputs toward preferred outputs, already provide a form of verification [Ziegler et al., 2019].However, such feedback is not equivalent to scientific verification for several reasons.First, RLHF operates at the level of plausibility rather than truth: models are rewarded for producing outputs that appear correct to human evaluators.Second, the feedback is inherently partial and subjective, relying on limited annotations that cannot exhaustively cover the space of possible outputs.Third, RLHF provides no guarantees: models fine-tuned with feedback still generate confident but false statements and mathematically inconsistent expressions.</p>
<p>Thus, while reinforcement learning can improve the style and surface reliability of generated hypotheses, it does not address the deeper need for principled, automated verification against background theory and empirical constraints.For scientific discovery, this distinction is crucial: plausibility without proof cannot serve as the foundation of knowledge.</p>
<p>In the mathematical literature, the use of formal proof assistants for verification, such as Lean [De Moura et al., 2015], Coq [Bertot and Castéran, 2013], and Isabelle [Nipkow et al., 2002], has attracted considerable attention.These systems enable mathematical theorems to be expressed in a dependently typed language and verified computationally.For example, they can be used to verify that every result in an introductory analysis textbook is correct [Tao, 2025].Moreover, some recent works pair this technology with LLMs.For instance, the AlphaProof system from DeepMind [AlphaProof and AlphaGeometry teams, 2024] achieved a silver medal at the 2024 International Mathematics Olympiad by translating one million informal mathematical problems into Lean using natural language processing, allowing AlphaProof to be trained using reinforcement learning.However, since there is no commonly agreed-upon set of axioms for the natural sciences (e.g., quantum mechanics and gravity are not consistent), and the process of translating informal problems into formal statements can introduce errors unless verified by a user, a Lean-reinforcement learning approach cannot be broadly applied to scientific discovery.Finally, the growing recognition of verification challenges in AI-driven scientific discovery has catalyzed significant government investment in bridging formal methods with statistical AI approaches.In the United States, DARPA's portfolio is an example of this trend, featuring programs such as expMath [DARPA, 2025], which seeks to accelerate mathematics by developing AI systems capable of proposing and proving abstractions, and The Right Space (TRS) [DARPA, 2024c], which applies scientific machine learning to uncover tractable transformations for complex models.Other initiatives [DARPA, 2024b[DARPA, ,a,d, 2018]], including ReMath, PROVERS, V-SPELLS, and the completed HACMS program, further underscore the emphasis on formal verification in critical domains.These funding priorities reflect an acknowledgment that while generative AI excels at quick hypothesis generation and pattern discovery, scientific applications require the reliability and guarantees that only formal verification methods can provide.</p>
<p>AI Methods for Scientific Discovery</p>
<p>Motivated by the increasing importance of verification in scientific discovery and other domains involving AI, we next review the state-of-the-art methods used for scientific discovery.Figure 2 provides a qualitative map of the landscape, positioning the different major methods categories along three dimensions: the degree to which they are data-driven, the degree to which they are knowledge-driven and their associated computational complexity.We also discuss the limitations of the existing approaches and suggest strategies for future improvement.</p>
<p>Data-driven Methods</p>
<p>The data-driven discovery of symbolic formulae is a long-standing challenge in Artificial Intelligence [Kitano, 2016], and the central difficulty remains how to incorporate verification into the process.A variety of approaches have been proposed [Landajuela et al., 2022], ranging from neural networks designed to mimic human physical reasoning [Iten et al., 2020], to tree-structured LSTMs for handling symbolic expression trees and formula verification [Arabshahi et al., 2018], to logic-constrained GANs for image generation [Marra et al., 2018].Symbolic regression (SR) has played a prominent role in this space, with applications to extracting explicit relations from graph neural networks [Cranmer et al., 2020b], constructing analytic models for reinforcement learning control [Derner et al., 2019], or combining regression with Bayesian models [Jin et al., 2019].The AI Feynman family of methods [Udrescu andTegmark, 2020, Udrescu et al., 2020] exemplifies the integration of neural fitting with physics-inspired heuristics, while tools such as PySR [Cranmer, 2020, Cranmer et al., 2020b] and TuringBot [Schmidt and Lipson, 2009] use evolutionary or annealing-based search strategies to identify parsimonious equations.More recent methods, such as RSRM, combine Monte Carlo Tree Search with reinforcement learning for efficient symbolic exploration [Xu et al., 2024].Despite the progress, none of these approaches incorporates formal reasoning, leaving their outputs vulnerable to producing expressions that fit the data but lack theoretical grounding.</p>
<p>To address this gap, several works have attempted to combine SR or neural methods with logical consistency checking.The LGML system [Scott et al., 2020] augments learning with a module that verifies whether candidate functions satisfy constraints on their functional form, while LGGA [Ashok et al., 2021] extends this approach with genetic algorithms and auxiliary mathematical expressions.Similar ideas appear in Błądek and Krawiec [2019] counterexample-guided SR, in Kubalík et al. [2020,</p>
<p>Knowledge-driven discovery</p>
<p>Derivable models</p>
<p>Knowledge-aware NNs (PINNs, HNNs, LNNs)</p>
<p>Symbolic regression</p>
<p>Data-driven discovery</p>
<p>Automated theorem provers</p>
<p>Computational complexity</p>
<p>Linear regression</p>
<p>Kernel regression</p>
<p>Interactive theorem provers</p>
<p>Nonlinear regression (NNs, trees, etc.) Unsupervised (PCA, ICA, clustering, etc.)</p>
<p>Bayesian modeling</p>
<p>Program synthesis</p>
<p>Inductive Logic Programming</p>
<p>Figure 2: Qualitative Landscape of Computational Methods for Scientific Discovery.Different approaches span the spectrum between data-driven and knowledge-driven discovery.Data-driven methods, such as neural networks, can rapidly generate hypotheses but lack verifiability, whereas theory-driven methods, like automated theorem provers, offer rigorous verification but are often slow and undecidable.Derivable or science-aware approaches aim to bridge this gap by combining datadriven modeling with symbolic guarantees.The associated computational complexity reflects trade-offs between speed, interpretability, and verifiability.Note that this figure provides a qualitative illustration of the landscape of computational tools for scientific discovery, highlighting general trends across major categories.The position of specific methods in these categories may vary depending on data type, approach, or hybrid usage.2021] multi-objective framework that enforces nonlinear constraints as discrete data points, and in Engle and Sahinidis [2021] deterministic mixed-integer programming formulation with derivative constraints.These methods, however, remain limited to constraints on functional form rather than incorporating background-theory axioms that describe the scientific environment itself.</p>
<p>In parallel, the broader neuro-symbolic community has explored the integration of logic constraints into machine learning tools.Approaches include penalizing constraint violations in neural networks [Xu et al., 2018, Wang andPan, 2020] and embedding logical rules in the training process [Cornelio et al., 2023b, Li and Srikumar, 2019, Daniele and Serafini, 2020, Xie et al., 2019, Li et al., 2019].Inductive logic programming and rule induction [Tamaddoni-Nezhad et al., 2021, Sen et al., 2022, Evans and Grefenstette, 2018, Sadeghian et al., 2019, Law et al., 2018] provide another way of extracting logical knowledge from data, while program synthesis has gained renewed interest as a means of combining symbolic reasoning with statistical learning [Sun et al., 2022, Nye et al., 2020, Parisotto et al., 2017, Valkov et al., 2018, Yang et al., 2017].Yet, across all these efforts, formal verification of discovered formulas remains elusive: constraints typically ensure plausibility, not provability.The result is that many systems generate equations that appear valid but are not guaranteed to align with the underlying laws of nature.</p>
<p>Knowledge-aware methods</p>
<p>Scientific discovery and artificial intelligence have traditionally followed separate paradigms: the former rooted in theory and verification, and the latter in data-driven learning.As scientific problems become increasingly complex and data become increasingly abundant yet noisy or incomplete, there is a growing interest in integrating scientific knowledge into machine learning models.The resulting hybrid methods aim to combine the flexibility of learning-based approaches with the structure and generalizability offered by physical laws.</p>
<p>This section surveys approaches that leverage scientific knowledge in AI model design and training.We distinguish between physics-informed models, which learn the unknown solution of known governing equations by training neural networks to minimize data and physics residuals, and physics-inspired models, that encode known structures, such as conservation laws, directly in the network's architecture.We also consider symmetry-informed networks that embed invariance or equivariance directly into model operations, so that transformations of the input induce consistent transformations of the output.</p>
<p>Physics-Informed Neural Networks (PINNs).Physics-Informed Neural Networks (PINNs) incorporate governing physical laws into the learning process by embedding partial differential equations (PDEs) directly into the loss function [Raissi et al., 2019, Cuomo et al., 2022].These models are not intended to discover the governing equations themselves, but rather to approximate their solutions.The key idea is to replace or augment traditional numerical solvers by training a neural network that minimizes a composite loss consisting of: 1) A data loss term measuring the fit to observed data; 2) A physics loss term penalizing violation of the PDE; and 3) A boundary condition loss term ensuring physical consistency.Mathematically, for a PDE of the form F (x, u, ∇u, ∇ 2 u) = 0, the PINN approximates the solution u(x) with a neural network u θ (x), and minimizes:
L total = λ d L data + λ f L physics + λ b L boundary .
This approach has demonstrated success across various domains, including fluid mechanics, heat diffusion, and quantum mechanics.It provides an elegant, mesh-free framework capable of solving high-dimensional PDEs with limited data.</p>
<p>While PINNs represent a significant advance in scientific computing, the method requires carefully balancing multiple loss terms, and is therefore sensitive to network architecture choices.This highlights the importance of systematic hyperparameter optimization strategies.Additionally, the current approach relies on known governing equations as constraints, and the generic network architectures employed do not yet fully exploit problem-specific structural information.These characteristics have motivated active research directions focused on adaptive loss weighting schemes, physics-informed architecture design, and methods for discovering unknown governing equations from data [Lu et al., 2021].</p>
<p>Physics-inspired Neural Networks.Physics-inspired neural networks take a complementary approach: instead of embedding the governing equations into the training loss, they encode physical structure directly into the model architecture.These models are well-suited to systems governed by conservation laws, such as those following Hamiltonian or Lagrangian dynamics.</p>
<p>In Hamiltonian neural networks (HNNs) [Greydanus et al., 2019], the model learns a scalar-valued Hamiltonian function H(q, p), where q and p are generalized coordinates and momenta.The dynamics are then obtained by differentiating H according to Hamilton's equations:
dq dt = ∂H ∂p , dp dt = − ∂H ∂q .
enforcing conservation of energy by design.</p>
<p>Lagrangian neural networks (LNNs) [Cranmer et al., 2020a] instead model the Lagrangian L(q, q) and derive equations of motion via the Euler-Lagrange equations.This enables the incorporation of constraints and yields coordinate-invariant representations.</p>
<p>Physics-inspired networks, thus, encode domain knowledge directly into the architecture, allowing them to model both the its state and evolution in a structured way.However, as noted by Newman et al. [2024], these approaches do not discover the underlying laws; instead, they assume them, modeling the dynamics within the specified structural form.Furthermore, incorporating multiple types of physical constraints simultaneously (e.g., energy and momentum conservation alongside symmetry constraints) remains an open challenge.</p>
<p>Equivariant Neural Networks.Many physical systems exhibit symmetries such as translation, rotation, or permutation invariance.Equivariant neural networks explicitly incorporate such symmetries by ensuring that transformations of the input correspond to equivalent transformations of the output [Cohen and Welling, 2016].Formally, a function f is equivariant with respect to a group G if:
f (g • x) = g • f (x), ∀g ∈ G.
Equivariant Convolutional Neural Networks (G-CNNs), Spherical CNNs, and SE(3)-equivariant graph networks have been developed to model molecular systems, fluid dynamics, and lattice structures, among others [Weiler et al., 2021, Batzner et al., 2022].These networks often lead to improved sample efficiency and generalization.Symmetry-informed networks [Akhound-Sadegh et al., 2023] extend this concept to more general forms of structure, potentially including conservation laws and geometric constraints.These methods can be viewed as a broader class of equivariant models.However, as with physics-inspired networks, they often require manual specification of symmetry constraints and may not scale well when multiple symmetries coexist.</p>
<p>Knowledge-aware AI methods, while promising, still face ongoing challenges as they continue to evolve.Current approaches typically depend on experts to manually encode physical laws, architectural choices, or symmetry constraints into models, which limits scalability and automation.Moreover, the simultaneous incorporation of multiple physical principles presents significant computational and theoretical challenges.The interpretability of these models remains a key concern, as they often function as black boxes that provide limited insight into the underlying physical mechanisms they approximate.Most critically, existing methods typically lack formal guarantees regarding constraint satisfaction.Physical laws are commonly enforced through soft constraints via penalty terms in the loss function, which cannot ensure that the learned models rigorously adhere to all governing physical principles.These challenges underscore the need for formal frameworks that unify data-driven modeling with principled use of background knowledge, supporting rigorous verification.</p>
<p>Derivable models</p>
<p>A different line of work is represented by the methods of AI-Descartes [Cornelio et al., 2023a] and AI-Hilbert [Cory-Wright et al., 2024], which explicitly introduce background theory into the process of scientific discovery.In contrast to most existing methods, which either constrain functional forms or encode structural biases, these frameworks embed general scientific axioms and use them to guide or validate the discovery of candidate laws.AI-Descartes takes a verification-oriented perspective, generating hypotheses from data and then employing formal reasoning to test their consistency with background theory.AI-Hilbert, on the other hand, integrates theory directly into the hypothesis generation process, reducing the search space and enforcing consistency during model generation.[Cornelio et al., 2023a] is a neuro-symbolic framework for automated scientific discovery that couples symbolic regression with formal reasoning.The system adopts a generator-verifier paradigm, where any hypothesis generator can be paired with any formal verifier, allowing the generation of arbitrarily defined models without restrictions on functional classes, grammar, or structure.This modular yet sequential design ensures flexibility but prevents data and theory from being leveraged simultaneously: hypotheses are generated from data first and only then verified, a separation that limits the exploitation of their complementary strengths.</p>
<p>AI-Descartes. AI-Descartes</p>
<p>Formally, the system seeks to discover an unknown symbolic model y = f * (x), where x = (x 1 , . . ., x n ) are independent variables and y is the dependent variable.The inputs are defined as a 4-tuple ⟨B, C, D, M⟩, where B denotes the background knowledge, consisting of domain-specific axioms; C is the hypothesis class, describing the admissible symbolic models via a grammar and functional constraints; D is the dataset of m examples; and M specifies modeler preferences, such as acceptable error bounds or complexity measures.The discovery task is then framed as a multi-objective problem: the candidate function f must fit the data, remain consistent with B, and have bounded complexity and prediction error.</p>
<p>As outlined above, the AI-Descartes architecture is organized around two main modules following a generator-verifier design.The first is a symbolic regression (SR) module, formulated as a mixed-integer nonlinear programming (MINLP) problem, which enumerates candidate formulas that approximate the data and remains effective with very few, noisy data points.The second is a reasoning module, based on a theorem prover, that evaluates the logical relationship between a candidate model and the background theory.In particular, AI-Descartes introduces the concept of a reasoning distance, which measures the discrepancy between predictions of a candidate model f and the predictions of a formula derivable from B (assumed to be complete, i.e., containing all the axioms necessary to derive the ground-truth law).Each candidate hypothesis is evaluated both in terms of its empirical error ε(f ) relative to the data D, and its reasoning error β(f ) relative to the axioms in B. These two scores are combined to rank the hypotheses, with the top-ranked model being selected as the best candidate.The interplay between these two main components allows AI-Descartes to filter out spurious hypotheses that, while numerically accurate, violate known physical or logical constraints.</p>
<p>Unlike prior efforts that embed only structural constraints, AI-Descartes incorporates full background theories, expressed in logical form.This enables it to reason over unmeasured variables not present in the data and over non-obvious relations that go beyond the data itself.Building on this capability, AI-Descartes can also compare alternative background theories (possibly inconsistent to each other) by computing reasoning errors for each and selecting the set of axioms that is the most consistent with the data.</p>
<p>AI Hilbert.AI-Hilbert [Cory-Wright et al., 2024] is a theory-guided framework for automated scientific discovery that integrates background knowledge directly into hypothesis generation.In contrast to post hoc verification, AI-Hilbert couples data and theory in a single synthesis problem: candidate laws are constructed to satisfy the axioms as they are fit to the data.However, the method restricts the hypothesis space to polynomial (or, when admissible, rational) expressions, which enables algebraic constraints from the background theory to be enforced exactly or with controlled slack.</p>
<p>More formally, AI-Hilbert aims to discover an unknown polynomial formula q(•) ∈ R[x] which describes a physical phenomenon, and is consistent with both a background theory and a collection of experimental data.The inputs to AI-Hilbert are a four-tuple (B, D, C(Λ), d c ), where: 1) B denotes the relevant background theory, expressed as a collection of axioms: the union of the inequalities {g 1 (x) ≥ 0, . . ., g k (x) ≥ 0} defining G and the equalities {h 1 (x) = 0, . . ., h l (x) = 0} defining H, where g i , h j ∈ R[x] n (the ring of real polynomials in the n-tuple of variables x ∈ R n ).B is defined over n variables x 1 , . . ., x n .However, only t of these n variables can be measured and are directly relevant for explaining the observed phenomenon.In particular, we let x 1 denote the target variable.The remaining n − t variables appear in the background theory but are not directly observable.The background theory B is defined as complete if it contains all the axioms necessary to formally prove the target formula, and incomplete otherwise.Moreover, B is called inconsistent if it contains axioms that contradict each other, and consistent otherwise.A special case of inconsistency is when a formula that incorrectly describes the studied phenomenon is added to a consistent background theory.2) D := {x i } i∈[m] denotes a collection of data points, or measurements of an observed physical phenomenon, which may be few and noisy.3) C denotes a set of constraints and bounds which depend on a set of hyper-parameters Λ (e.g., bound on the degree of the polynomial q).4) d c (•, G ∩ H) denotes a distance function from an arbitrary polynomial to the background theory.</p>
<p>The AI-Hilbert algorithm has 4 main steps: Pr sd using a mixed-integer conic optimization solver, outputting a candidate formula and a set of
multipliers {α i } k i=1 , {β j } l j=1 .
The formula is of the form q(x) = 0 (where the only monomials with nonzero coefficients are those that only contain the variables x 1 , . . ., x t , the observable variables) and such that q
(x) = α 0 (x) + k i=1 α i (x)g i (x) + l j=1 β j (x)h j (x) if d c (q, G ∩ H) = 0,
which is a certificate of the fact that q is derivable from the complete background theory.If d c &gt; 0, for example, when the background theory is inconsistent or incomplete, then AI-Hilbert returns a certificate that q is approximately derivable from the background theory.</p>
<p>LLMs for Scientific Discovery</p>
<p>Recent advances in generative AI, and particularly large language models (LLMs), have opened new avenues for accelerating scientific discovery [Reddy and Shojaee, 2025].In materials discovery, generative graph-based models such as GNoME have drastically expanded the set of known stable materials, representing an order-of-magnitude increase in crystallographic diversity [Merchant et al., 2023a].More recently, LLMs have been used to extract domain knowledge from scientific literature, generate new material compositions, and guide experimental design, as demonstrated in systems like AtomAgents, which integrate LLM reasoning with alloy design pipelines [Ghafarollahi and Buehler, 2024].</p>
<p>Transformer-based models treat equation discovery as a numeric-to-symbolic generation task [Kamienny et al., 2022].However, state-of-the-art general-purpose LLMs, such as OpenAI GPT-5, still have limitations when it comes to symbolic discovery, often producing only relatively simple functional forms (e.g., when prompted with the binary star data in Cornelio et al. [2023a]).At the same time, their ability to make inferences from simple axiom systems has improved notably compared to older models (see Appendix A for more details).In parallel, multimodal approaches like SNIP embed equations and numerical data into smoother joint spaces to improve search efficiency [Meidani et al., 2024], while systems such as LLM-SR explore the use of LLMs as "scientist agents" that evolve equations in search of governing laws [Shojaee et al., 2024].Benchmarks, such as LLM-SRBench, have recently been introduced to systematically evaluate these methods in scientific equation discovery [Shojaee et al., 2025].These works highlight the growing role of generative and language-based models in pushing symbolic regression beyond handcrafted algorithms toward more generalizable AI-driven discovery.</p>
<p>Alongside these task-specific methods, domain-specialized scientific LLMs are being developed to serve as general-purpose research copilots.NatureLM [Xia et al., 2025] is a foundation model designed to unify the "languages of nature" across molecules, proteins, DNA, RNA, and materials, enabling cross-domain generation and design of drug molecules, protein binders, and CRISPR guides.Similarly, Galactica [Taylor et al., 2022], trained on 106B scientific tokens spanning papers, textbooks, chemical sequences, proteins, and code, outperforms general LLMs on scientific benchmarks and introduces specialized reasoning tokens for step-by-step problem solving.These models illustrate how domain-curated corpora and tailored architectures can significantly advance LLM-based scientific discovery.</p>
<p>Finally, LLMs can be framed as agents rather than passive tools: by coupling their broad knowledge bases with external tool integration, LLM-based agents can design, test, and refine hypotheses in ways that approximate the iterative scientific method.ChemCrow [M.Bran et al., 2024], for example, integrates GPT-4 with chemistry-specific tools for reaction prediction, retrosynthesis planning, and safety assessment, enabling both reasoning and validation within chemical workflows.Multi-agent frameworks, such as SciAgents, extend this paradigm by coordinating specialized LLM-based agents to collaboratively explore biomaterials design [Ghafarollahi and Buehler, 2025].Alongside general frameworks for opendomain hypothesis generation in the social sciences [Yang et al., 2024], biomedicine [Qi et al., 2023], and rediscovery settings such as MOOSE-Chem in chemistry [Yang et al., 2025], these systems demonstrate the potential of LLMs and generative models not only to accelerate discovery in targeted domains such as chemistry and materials science, but also to serve as versatile, reasoning-driven collaborators in the broader pursuit of new scientific laws.</p>
<p>Verification in the age of AI-driven science</p>
<p>Modern engineering industries regularly employ verification in the development and deployment of mission-critical technologies, including those in aerospace, medical devices, and autonomous systems.The rigorous process of verifying the accurate implementation of such technologies ensures that these complex systems function precisely as intended, mitigating risks of failure that could lead to catastrophic loss of life, environmental damage, or severe economic disruption.Through meticulous testing, simulation, and formal methods, verification tests validate the design integrity, software reliability, and hardware performance of technologies where even minor deviations can have profound consequences.Given the potentially far-reaching impacts and high costs of scientific research, why isn't a stringent and widespread culture of independent verification more commonly embedded within modern scientific research, rather than being largely limited to industrial applications?In this section, we illustrate examples of the importance of verification across research communities and outline ways to incorporate verification into scientific research to enhance the rigor of the scientific method for the modern age.</p>
<p>The role of verification across scientific domains</p>
<p>The proliferation of AI models in scientific research presents a transformative opportunity to accelerate the pace of scientific discovery.In particular, generative AI models have demonstrated the ability to produce novel hypotheses at rapid scales and speeds.However, the rapid generation of scientific hypotheses presents significant challenges.Many of these AI-generated hypotheses lack empirical verification and are often disconnected from established theoretical frameworks or domain-specific knowledge.However, the strength of a scientific theory lies in its empirical predictive power [Popper, 1959].Without iterative refinement through empirical verification of hypotheses, scientific theories fail to progress and remaining unable to make useful empirical predictions (see Fig. 3).The strength of a scientific theory lies in its empirical predictive power.Thus, the development of a scientific theory requires iterative empirical verification, with stronger theories offering more accurate predictions of observable phenomena.However, the balance between theoretical strength and predictive power varies across scientific domains, and often depends on the epistemic goals and maturity of each field, as well as the nature of the theories (e.g., formal versus ontological theories).</p>
<p>Empirical</p>
<p>In many applied scientific domains, such as drug discovery or materials science, the term "discovery" often refers primarily to the generation of hypotheses -such as identifying a promising molecular compound or material configuration -rather than their empirical verification [Reidenbach et al., 2025, Merchant et al., 2023b, Jain et al., 2022, Anstine and Isayev, 2023, Takeda et al., 2023].This usage underscores the importance of distinguishing between the act of proposing a candidate and the subsequent process of validating its efficacy, safety, or theoretical soundness.As a result, researchers are increasingly confronted with a deluge of unverified hypotheses, clogging (and potentially slowing) verification pipelines that are critical to validating scientific discoveries.However, verification strategies across scientific domains differ greatly in approach and empirical requirements due to differences in their theories and ontologies, as well as the epistemic goals of each field.Here we briefly discuss the variation of verification strategies across a few scientific domains, namely physical, biological and complex sciences, and clinical sciences.</p>
<p>In the physical sciences, verification is tightly coupled with formal theories and mathematical models.Hypotheses are often derived from well-established physical laws, and their verification typically involves controlled experiments or data-driven simulations that yield quantifiable and reproducible results that integrate and conform to these background laws [Udrescu and Tegmark, 2020].This tight integration of theory and data allows for the use of automated verification techniques that derive data from physical laws and theory [Cornelio et al., 2023a, Cory-Wright et al., 2024].</p>
<p>In contrast, however, many chemical, biological, and cognitive sciences present a more complex landscape for verification [Mock et al., 2024].Unlike physics, chemical, materials, and biological theories are often less formalized and more context-dependent, reflecting the inherent complexity of these systems and the variability of the epistemic goals across scientific domains.For example, verification in biology typically involves manual experimentation, such as genetic manipulation or behavioral observation, and relies heavily on ontological frameworks like evolutionary theory or systems biology, and less on explicit, quantitative laws.Though quantification is still important, it is often within the context of multi-variable and dynamical systems that are difficult to quantitatively derive from first principles.Nevertheless, efforts to build-in background knowledge (or incorporate a knowledgeconstrained search space) can improve the quality and validity of discovered hypotheses, thereby improving (and accelerating) scientific discovery in these domains (e.g., in chemistry [Yang et al., 2025], and in cognitive science [Castro et al., 2025]).</p>
<p>In medical and clinical sciences, there are additional layers of complexity.These tend to be shaped by ethical constraints, human variability, and pragmatic demands of clinical practice.Moreover, theories in clinical research are often probabilistic and population-based, rather than deterministic.Importantly, though the gold standard for verification strategies in clinical trials are randomized control trials, due to practical constraints of clinical research, verification strategies also include observational studies and meta-analyses of existing data.However, in all these cases, verification relies on statistical inference to assess efficacy and safety that are informed by ontological systems such as disease classifications and diagnostic criteria, which evolve over time.</p>
<p>Similar domain-specific variations in verification strategies are evident across various fields, including complex system sciences, earth sciences, social sciences, and engineering, among others, and each is shaped by its unique epistemic and methodological contexts.Despite the diversity of verification strategies across scientific domains, a unifying thread is the reliance on logical reasoning as the foundation for hypothesis testing and theory refinement.Whether through deductive modeling in physics, experimental inference in biology, or statistical evaluation in clinical sciences, the process of verification is fundamentally driven by structured, iterative reasoning.This echoes John Platt's notion of strong inference [Platt, 1964], where progress in science stems from the disciplined application of logic to generate, test, and eliminate hypotheses.While the form and tools of logical inference vary -from mathematical formalism (e.g., physical sciences) to ontological frameworks (e.g., biological sciences) to probabilistic models (e.g., clinical sciences) -the underlying commitment to rational analysis and verification remains constant.</p>
<p>Final Remarks and Future Challenges</p>
<p>In this work, we reviewed how AI is reshaping scientific discovery, with verification as a central open challenge.We reviewed a spectrum of methods, spanning from data-driven models to knowledge-based and hybrid approaches, illustrating their potential to accelerate hypothesis generation while also raising important concerns about their interpretability and reliability.The landscape we outlined highlights both the potential and the limits of contemporary AI, while pointing to the need to advance automated verification methods to improve AI-driven scientific discovery.There are many challenges ahead.In the next section we outline the most critical ones and discuss how they open promising directions for future research.</p>
<p>Challenges in AI-Driven Scientific Discovery</p>
<p>A major challenge for AI-driven scientific discovery is building benchmarks that genuinely capture open-ended scientific discovery and are not captured in the training distribution of existing AI systems.Existing datasets-such as AI Feynman [Udrescu and Tegmark, 2020], SciBench [Wang et al., 2023a], ScienceQA [Lu et al., 2022], and MATH [Hendrycks et al., 2021] focus on rediscovery or textbook-style problem solving, which neglects the complexity of theory formation.This is problematic because LLMs may depend on memorization rather than reasoning [Carlini et al., 2021, Wu et al., 2023], and unlike in theorem proving, most benchmarks lack explicit underlying theory, making verification-based evaluation nearly impossible.Indeed, whether an LLM is capable of making a scientific discovery often depends on the precise prompt used and even the notation used to describe a scientific discovery setting.Recent advances, such as simulated domains for scientific discovery [M.Bran et al., 2024, Shojaee et al., 2024] and the newly proposed LLM-SRBench [Shojaee et al., 2025], take steps toward mitigating memorization and evaluating true discovery.Nonetheless, key gaps remain in creating benchmarks that rigorously test novelty, generalizability, and scientific consistency [Cranmer et al., 2020b].</p>
<p>A second key challenge in AI-driven science is the unification of theory and data, since most existing methods focus either on empirical modeling or formal reasoning in isolation.While LLMs have shown promise in theorem proving [Jiang et al., 2023] and equation discovery from data [Shojaee et al., 2024], integrating these capabilities into a holistic framework remains an open problem.Efforts such as AI-Descartes [Cornelio et al., 2023a] and AI-Hilbert [Cory-Wright et al., 2024], as well as work in neuro-symbolic AI [De Raedt andKimmig, 2015, Ahmed et al., 2022], point toward promising directions for future development.However, challenges persist in deriving rigorous hypotheses from data, combining symbolic and neural approaches, and handling uncertainty within formal reasoning.</p>
<p>A third challenge in AI-driven discovery is ensuring that the use of AI does not overly homogenize science.The traditional scientific method is implemented differently by each scientist.This diversity, including the fact that scientists occasionally make mistakes, is a fundamental strength of science, as it enables different individuals to make distinct discoveries [Elliott, 2004].For instance, Alexander Fleming discovered penicillin by accident [Tan and Tatsumura, 2015], a "mistake" that an AI scientist would be unlikely to make.Ensuring that organic "mistakes" remain a part of the scientific method is another key challenge in the age of AI-driven discovery.</p>
<p>Conclusions</p>
<p>A key conclusion is that AI-driven scientific discovery compels us to reconsider the very notion of the "scientific method" itself.Traditionally, science has been portrayed as a systematic process of hypothesis generation, experimentation, and validation, but this narrative has been repeatedly challenged by philosophers such as Feyerabend [Feyerabend, 1975], who argue that rigid methodological rules neither capture nor enable true scientific progress.With the advent of generative models and inspired by industrial practices, however, we may be entering a new era in which verification becomes not just essential but also the primary bottleneck in scientific discovery.This shift would mark a departure from the traditional scientific method, reframing discovery as an iterative dialogue between creativity and verification, potentially laying the new groundwork for a new scientific paradigm.</p>
<p>[Step 1] The background theory B and data D are combined to generate a polynomial optimization problem Pr which targets a specific concept identified by the target variable x 1 .This is achieved by minimizing the distance d c , the model complexity and the error on the data, while integrating the bounds and constraints C. [Step 2] Pr is then reformulated as a semidefinite (or linear if no inequalities are present in the background theory) optimization problem Pr sd , by leveraging standard techniques from SOS optimization.[Step 3] Next, AI-Hilbert solves</p>
<p>Figure 3 :
3
Figure3: The role of verification in the development of scientific theories.The strength of a scientific theory lies in its empirical predictive power.Thus, the development of a scientific theory requires iterative empirical verification, with stronger theories offering more accurate predictions of observable phenomena.However, the balance between theoretical strength and predictive power varies across scientific domains, and often depends on the epistemic goals and maturity of each field, as well as the nature of the theories (e.g., formal versus ontological theories).</p>
<p>Figure 4 :
4
Figure 4: Prompt given to GPT-5 for binary star data used in AI Descartes (with variables relabeled as (d, m 1 , m 2 , p) → (x, y, z, u) and data columns permuted compared to the original dataset) and the output returned by GPT-5.The desired formula is u =</p>
<p>Figure 5 :
5
Figure5: Prompt and output given to GPT-4 for a simple artificial (not arising from any physical theory) example of the type used in AI Descartes.GPT-4 did not return a correct answer, which is f (d, k, z, g) = kzg z−d , whereas GPT-5 did (see Figure6for a comparison with GPT-5 on the same prompt).</p>
<p>Figure 6 :
6
Figure6: Prompt and output given to GPT-5 for a simple artificial (not arising from any physical theory) example of the type used in AI Descartes.GPT-4 did not return a correct answer, which is f (d, k, z, g) = kzg z−d , whereas GPT-5 did (see Figure5for a comparison with GPT-4 on the same prompt).</p>
<p>A AppendixIn this section, we give two examples of simple scientific discovery related queries given to a state-ofthe-art LLM, specifically GPT-5, the latest version of ChatGPT.In the first, we take data given in AI Descartes[Cornelio et al., 2023a]for two binary stars revolving around a common center of gravity and ask GPT-5 to find a function that best fits the data.The target function in this example is Kepler's third law of planetary motion.The data is scaled in such a manner that the period of revolution p is equal towhere d is the distance between the binary stars, and m 1 and m 2 stand for their masses.We rename the variables, (d, m 1 , m 2 , p) → (x, y, z, u), to avoid giving away information about the problem to GPT-5.In Figure4we show the prompt given to GPT-5 and its output.One can see that GPT-5 tries out a number of different functional forms -in other words it performs a limited symbolic regression exercise -and does not produce the desired function as a candidate solution.In Figure5we give the prompt at the top to GPT-4 and show its output, while in Figure6we show instead the output of GPT-5 on the same prompt.It is clear that GPT-4 fails to reason accurately with the axioms and comes up with the correct expression of the functional form relating the variables other than x, whereas GPT-5 produces the correct answer f (d, k, z, g) = kzg z−d and also the correct derivation.
Semantic probabilistic layers for neuro-symbolic learning. K Ahmed, S Teso, K.-W Chang, G Van Den Broeck, A Vergari, Advances in Neural Information Processing Systems. 202235</p>
<p>problems-at-silver-medal-level/. Announces AlphaProof (Lean-based formal reasoning) and AlphaGeometry 2. T Akhound-Sadegh, L Perreault-Levasseur, J Brandstetter, M Welling, S Ravanbakhsh, arXiv:2311.04293Lie point symmetry and physics informed networks. 2023. 20247arXiv preprintAi achieves silver-medal standard solving international mathematical olympiad problems. system scored 28/42 on IMO 2024</p>
<p>Generative Models as an Emerging Paradigm in the Chemical Sciences. D M Anstine, O Isayev, 10.1021/jacs.2c13467Journal of the American Chemical Society. 0002-786314516Apr. 2023American Chemical Society</p>
<p>Combining symbolic expressions and black-box function evaluations in neural programs. F Arabshahi, S Singh, A Anandkumar, In ICLR. 2018</p>
<p>The decline of science in corporate R&amp;D. A Arora, S Belenzon, A Patacconi, Strategic Management Journal. 3912018</p>
<p>Logic guided genetic algorithms (student abstract). D Ashok, J Scott, S J Wetzel, M Panju, V Ganesh, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceMay 202135</p>
<p>F Bacon, Novum organum. Clarendon press1878</p>
<p>Update on medication errors associated with incorrect patient weights. B R Bailey, M J Gaunt, M J Grissinger, Pennsylvania Patient Safety Advisory. 13206 2016Analysis found. 2% of events involved pounds-kilograms confusion (N=1,291</p>
<p>E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. S Batzner, A Musaelian, L Sun, M Geiger, J P Mailoa, M Kornbluth, N Molinari, T E Smidt, B Kozinsky, Nature Communications. 1312022</p>
<p>Evaluating sakana's ai scientist for autonomous research: Wishful thinking or an emerging reality towards' artificial research intelligence. J Beel, M.-Y Kan, M Baumgart, arXiv:2502.142972025arXiv preprint</p>
<p>Interactive theorem proving and program development: Coq'Art: the calculus of inductive constructions. Y Bertot, P Castéran, 2013Springer Science &amp; Business Media</p>
<p>Stagnation and scientific incentives. J Bhattacharya, M Packalen, 2020National Bureau of Economic ResearchTechnical report</p>
<p>Solving symbolic regression problems with formal constraints. I Błądek, K Krawiec, The Genetic and Evolutionary Computation Conference (GECCO '19). Prague, Czech RepublicACMJuly 13-17, 2019. 2019</p>
<p>Are ideas getting harder to find?. N Bloom, C I Jones, J Van Reenen, M Webb, American Economic Review. 11042020</p>
<p>S J Bokser, A weighty mistake. Agency for Healthcare Research and Quality (AHRQ). PSNet WebM&amp;M case commentary2013</p>
<p>Nobel lecture: The green revolution, peace, and humanity. N Borlaug, 1970</p>
<p>Extracting training data from large language models. N Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song, U Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Discovering Symbolic Cognitive Models from Human and Animal Behavior. P S Castro, N Tomasev, A Anand, N Sharma, R Mohanta, A Dev, K Perlin, S Jain, K Levin, N Éltető, W Dabney, A Novikov, G C Turner, M K Eckstein, N D Daw, K J Miller, K L Stachenfeld, 10.1101/2025.02.05.636732v1Feb. 2025New ResultsSection</p>
<p>T Cohen, M Welling, Group equivariant convolutional networks. International Conference on Machine Learning (ICML). 2016</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. C Cornelio, S Dash, V Austel, T R Josephson, J Goncalves, K L Clarkson, N Megiddo, B El Khadir, L Horesh, Nature Communications. 14117772023aNature Publishing Group</p>
<p>Learning where and when to reason in neuro-symbolic inference. C Cornelio, J Stuehmer, S X Hu, T Hospedales, International Conference on Learning Representations. 2023b</p>
<p>Evolving scientific discovery by unifying data and background knowledge with ai hilbert. R Cory-Wright, C Cornelio, S Dash, B El Khadir, L Horesh, Nature Communications. 15159222024</p>
<p>The great stagnation: How America ate all the low-hanging fruit of modern history, got sick, and will (eventually) feel better: A Penguin eSpecial from Dutton. T Cowen, 2011Penguin</p>
<p>PySR: Fast &amp; parallelized symbolic regression in Python/Julia. M Cranmer, 10.5281/zenodo.4041459Sept. 2020</p>
<p>M Cranmer, S Greydanus, S Hoyer, P Battaglia, D Spergel, S Ho, Lagrangian neural networks. International Conference on Learning Representations (ICLR). 2020a</p>
<p>Discovering symbolic models from deep learning with inductive biases. M Cranmer, A Sanchez-Gonzalez, P Battaglia, R Xu, K Cranmer, D Spergel, S Ho, NeurIPS. 2020. 2020b</p>
<p>Scientific machine learning through physics-informed neural networks: Where we are and what's next. S Cuomo, V Di Cola, F Giampaolo, G Rozza, M Raissi, F Piccialli, 10.1007/s10915-022-01939-z.pdfJournal of Scientific Computing. 9232022</p>
<p>A Daniele, L Serafini, arXiv:, 2009.06087Neural networks enhancement with logical knowledge. 2020</p>
<p>DARPA. High assurance cyber military systems (HACMS). 2018</p>
<p>reasoning-of-verifiers-enabling-robust-systems. DARPA. Pipelined reasoning of verifiers enabling robust systems (PROVERS). 2024a</p>
<p>DARPA. Recovery of symbolic mathematics from code. 2024bReMath</p>
<p>DARPA. The right space (TRS). 2024c</p>
<p>verified-security-and-performance-enhancement-of-large-legacy-software. DARPA. Verified security and performance enhancement of large legacy software V-SPELLS. 2024d</p>
<p>Exponentiating mathematics (expMath. DARPA. 2025</p>
<p>The lean theorem prover (system description. L De Moura, S Kong, J Avigad, F Van Doorn, J Raumer, International Conference on Automated Deduction. Springer2015</p>
<p>Probabilistic (logic) programming concepts. L De Raedt, A Kimmig, 10.1007/s10994-015-5494-z.pdfMachine Learning. 2015100</p>
<p>Symbolic regression for constructing analytic models in reinforcement learning. E Derner, J Kubalík, N Ancona, R Babuska, arXiv:, 1903.114832019</p>
<p>Error as means to discovery. K Elliott, Philosophy of Science. 7122004</p>
<p>Deterministic symbolic regression with derivative information: General methodology and application to equations of state. M R Engle, N V Sahinidis, AIChE Journal. e174572021</p>
<p>Learning explanatory rules from noisy data. R Evans, E Grefenstette, Journal of Artificial Intelligence Research. 612018</p>
<p>Against Method: Outline of an Anarchistic Theory of Knowledge. P Feyerabend, New Left Books. 1975</p>
<p>Atomagents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence. A Ghafarollahi, M J Buehler, arXiv:2407.100222024arXiv preprint</p>
<p>Sciagents: automating scientific discovery through bioinspired multi-agent intelligent graph reasoning. A Ghafarollahi, M J Buehler, 10.1002/adma.202413523Advanced Materials. 372224135232025</p>
<p>Towards an ai co-scientist. J Gottweis, W.-H Weng, A Daryin, T Tu, A Palepu, P Sirkovic, A Myaskovsky, F Weissenberger, K Rong, R Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Learning to fake it: Limited responses and fabricated references provided by chatgpt for medical questions. J Gravel, M D'amours-Gravel, E Osmanlliu, 10.1016/j.mcpdig.2023.05.004Mayo Clinic Proceedings: Digital Health. 132023. 2023 Sep</p>
<p>Hamiltonian neural networks. S Greydanus, M Dzamba, J Yosinski, Advances in Neural Information Processing Systems. 201932</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. M Gridach, J Nanavati, K Z E Abidine, L Mendes, C Mack, arXiv:2503.089792025arXiv preprint</p>
<p>Institute for Safe Medication Practices Canada. Enhancing safety with unfractionated heparin: A national and international area of focus. D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks. the Neural Information Processing Systems Track on Datasets and Benchmarks2021. 2008. 20088Bulletin reference #5 cites the ISMP Medication Safety Alert!. on heparin errors</p>
<p>Discovering physical concepts with neural networks. R Iten, T Metger, H Wilming, L Rio, R Renner, Physical Review Letters. 1242020</p>
<p>Biological Sequence Design with GFlowNets. M Jain, E Bengio, A Hernandez-Garcia, J Rector-Brooks, B F P Dossou, C A Ekbote, J Fu, T Zhang, M Kilgour, D Zhang, L Simine, P Das, Y Bengio, Proceedings of the 39th International Conference on Machine Learning. the 39th International Conference on Machine LearningPMLRJune 2022</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. A Q Jiang, S Welleck, J P Zhou, W Li, J Liu, M Jamnik, T Lacroix, Y Wu, G Lample, International Conference on Learning Representations. 2023</p>
<p>Bayesian symbolic regression. Y Jin, W Fu, J Kang, J Guo, J Guo, arXiv[Methodology]:, 1910.088922019</p>
<p>Highly accurate protein structure prediction with AlphaFold. J Jumper, R Evans, A Pritzel, T Green, M Figurnov, O Ronneberger, K Tunyasuvunakool, R Bates, A Žídek, A Potapenko, A Bridgland, C Meyer, S A A Kohl, A J Ballard, A Cowie, B Romera-Paredes, S Nikolov, R Jain, J Adler, T Back, S Petersen, D Reiman, E Clancy, M Zielinski, M Steinegger, M Pacholska, T Berghammer, S Bodenstein, D Silver, O Vinyals, A W Senior, K Kavukcuoglu, P Kohli, D Hassabis, 10.1038/s41586-021-03819-2Nature. 1476-46875967873Aug. 2021Nature Publishing Group</p>
<p>Can large language models reason and plan?. S Kambhampati, Annals of the New York Academy of Sciences. 153412024</p>
<p>End-to-end symbolic regression with transformers. P.-A Kamienny, S Ascoli, G Lample, F Charton, Advances in Neural Information Processing Systems. S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, A Oh, Curran Associates, Inc202235</p>
<p>Artificial intelligence to win the Nobel prize and beyond: Creating the engine for scientific discovery. H Kitano, AI Magazine. 371Apr. 2016</p>
<p>Symbolic regression driven by training data and prior knowledge. J Kubalík, E Derner, R Babuška, Proceedings of the 2020 Genetic and Evolutionary Computation Conference. the 2020 Genetic and Evolutionary Computation Conference2020</p>
<p>Multi-objective symbolic regression for physics-aware dynamic modeling. J Kubalík, E Derner, R Babuška, Expert Systems with Applications. 1821152102021</p>
<p>A Kulkarni, F Alotaibi, X Zeng, L Wu, T Zeng, B M Yao, M Liu, S Zhang, L Huang, D Zhou, arXiv:2505.04651Scientific hypothesis generation and validation: Methods, datasets, and future directions. 2025arXiv preprint</p>
<p>A unified framework for deep symbolic regression. M Landajuela, C S Lee, J Yang, R Glatt, C P Santiago, T N Mundhenk, I Aravena, G Mulcahy, B Petersen, Advances in Neural Information Processing Systems (NeurIPS). 2022</p>
<p>The Pasteurization of France. B Latour, 1993Harvard University Press</p>
<p>Inductive learning of answer set programs from noisy examples. M Law, A Russo, K Broda, arXiv:, 1808.084412018</p>
<p>Copernicus, galileo, and the church: Science in a religious world. N P Leveillee, Inquiries Journal. 3052011</p>
<p>T Li, V Srikumar, arXiv:, 1906.06298Augmenting neural networks with first-order logic. 2019</p>
<p>T Li, V Gupta, M Mehta, V Srikumar, arXiv:, 1909.00126A logic-driven framework for consistency of neural models. 2019</p>
<p>KAN: Kolmogorov-arnold networks. Z Liu, Y Wang, S Vaidya, F Ruehle, J Halverson, M Soljačić, T Y Hou, M Tegmark, International Conference on Learning Representations (ICLR). 2025</p>
<p>G H Lockwood, Final report of the board of inquiry: Accident involving air canada boeing 767 c-gaun at gimli, manitoba. Lockwood23 july 1983. 1985Government catalogue confirms publication details</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. C Lu, C Lu, R T Lange, J Foerster, J Clune, D Ha, arXiv:2408.062922024arXiv preprint</p>
<p>Learning nonlinear operators via deeponet based on the universal approximation theorem of operators. L Lu, P Jin, G Pang, Z Zhang, G E Karniadakis, 10.1038/s42256-021-00302-5Nature Machine Intelligence. 332021</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. P Lu, S Mishra, T Xia, L Qiu, K.-W Chang, S.-C Zhu, O Tafjord, P Clark, A Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Augmenting large language models with chemistry tools. A M Bran, S Cox, O Schilter, C Baldassari, A D White, P Schwaller, Nature Machine Intelligence. 652024</p>
<p>Llms are not like you and me-and never will be. G Marcus, 2025</p>
<p>G Marra, F Giannini, M Diligenti, M Gori, arXiv:, 1807.09202Constraint-based visual generation. 2018</p>
<p>SNIP: Bridging mathematical symbolic and numeric realms with unified pre-training. K Meidani, P Shojaee, C K Reddy, A B Farimani, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Scaling deep learning for materials discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, Nature. 62479902023a</p>
<p>Scaling deep learning for materials discovery. A Merchant, S Batzner, S S Schoenholz, M Aykol, G Cheon, E D Cubuk, 10.1038/s41586-023-06735-9Nature. 1476-46876247990Dec. 2023b</p>
<p>Recent advances in generative biology for biotherapeutic discovery. M Mock, C J Langmead, P Grandsard, S Edavettal, A Russell, 10.1016/j.tips.2024.01.003Trends in Pharmacological Sciences. 0165-6147453Mar. 2024Elsevier</p>
<p>NASA. Mars climate orbiter mishap investigation board phase i report. 1999</p>
<p>Stable tensor neural networks for efficient deep learning. E Newman, L Horesh, H Avron, M E Kilmer, 10.3389/fdata.2024.1363978/pdfFrontiers in Big Data. 713639782024</p>
<p>Isabelle/HOL: a proof assistant for higher-order logic. T Nipkow, M Wenzel, L C Paulson, 2002Springer</p>
<p>Learning compositional rules via neural program synthesis. M Nye, A Solar-Lezama, J Tenenbaum, B M Lake, Advances in Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, Curran Associates, Inc202033</p>
<p>E Parisotto, A -R. Mohamed, R Singh, L Li, D Zhou, P Kohli, Neuro-symbolic program synthesis. International Conference on Learning Representations. 2017</p>
<p>Strong Inference. J R Platt, 10.1126/science.146.3642.347Science. 1463642Oct. 1964American Association for the Advancement of Science</p>
<p>The logic of scientific discovery. K R Popper, 1959Publisher: Basic Books</p>
<p>Large language models are zero shot hypothesis proposers. B Qi, K Zhang, H Li, K Tian, S Zeng, Z.-R Chen, B Zhou, NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following. 2023</p>
<p>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations. M Raissi, P Perdikaris, G E Karniadakis, Journal of Computational Physics. 3782019</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. C K Reddy, P Shojaee, 10.1609/aaai.v39i27.35084Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceApr. 202539</p>
<p>Applications of Modular Co-Design for De Novo 3D Molecule Generation. D Reidenbach, F Nikitin, O Isayev, S Paliwal, arXiv:2505.18392May 2025</p>
<p>Drum: End-to-end differentiable rule mining on knowledge graphs. A Sadeghian, M Armandpour, P Ding, D Z Wang, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Distilling free-form natural laws from experimental data. M Schmidt, H Lipson, Science. 32459232009</p>
<p>J Scott, M Panju, V Ganesh, LGML: Logic Guided Machine Learning. 2006.03626, 2020</p>
<p>Neuro-symbolic inductive logic programming with logical neural networks. P Sen, B W S R De Carvalho, R Riegel, A G Gray, AAAI. 222022</p>
<p>Llm-sr: Scientific equation discovery via programming with large language models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, arXiv:2404.184002024arXiv preprint</p>
<p>LLM-SRBench: A new benchmark for scientific equation discovery with large language models. P Shojaee, N.-H Nguyen, K Meidani, A B Farimani, K D Doan, C K Reddy, Forty-second International Conference on Machine Learning. 2025</p>
<p>Enriching the Earth: Fritz Haber, Carl Bosch, and the Transformation of World Food Production. V Smil, 2004MIT Press</p>
<p>. J J Sun, M Tjandrasuwita, A Sehgal, A Solar-Lezama, S Chaudhuri, Y Yue, O Costilla-Reyes, arXiv:2210.050502022Neurosymbolic programming for science. arXiv preprint</p>
<p>Foundation Model for Material Science. S Takeda, A Kishimoto, L Hamada, D Nakano, J R Smith, 10.1609/aaai.v37i13.26793Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202337</p>
<p>Humanmachine scientific discovery. A Tamaddoni-Nezhad, D A Bohan, G A Milani, A Raybould, S Muggleton, Human-like machine intelligence. Oxford University Press2021</p>
<p>Alexander fleming (1881-1955): discoverer of penicillin. S Y Tan, Y Tatsumura, Singapore medical journal. 5673662015</p>
<p>A lean companion to "analysis i. T Tao, 2025</p>
<p>R Taylor, M Kardas, G Cucurull, T Scialom, A Hartshorn, E Saravia, A Poulton, V Kerkez, R Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>AI Feynman 2.0: Paretooptimal symbolic regression exploiting graph modularity. S Udrescu, A Tan, J Feng, O Neto, T Wu, M Tegmark, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>AI Feynman: A physics-inspired method for symbolic regression. S.-M Udrescu, M Tegmark, Science Advances. 6162020</p>
<p>Houdini: Lifelong learning as program synthesis. L Valkov, D Chaudhari, A Srivastava, C Sutton, S Chaudhuri, Advances in Neural Information Processing Systems. 201831</p>
<p>Integrating deep learning with logic fusion for information extraction. W Wang, S J Pan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. X Wang, Z Hu, P Lu, Y Zhu, J Zhang, S Subramaniam, A R Loomba, S Zhang, Y Sun, W Wang, arXiv:2307.106352023aarXiv preprint</p>
<p>NEWTON: Are large language models capable of physical reasoning?. Y Wang, J Duan, D Fox, S Srinivasa, 10.18653/v1/2023.findings-emnlp.652Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023b</p>
<p>Judge finds out why brief cited nonexistent cases-ChatGPT did the research. M Weiler, P Forré, E Verlinde, M Welling, arXiv:2106.06020Coordinate independent convolutional networks-isometry and gauge equivariant convolutions on riemannian manifolds. 2021. 2023arXiv preprint</p>
<p>R S Westfall, The Construction of Modern Science: Mechanisms and Mechanics. Cambridge University Press1977</p>
<p>Synergizing artificial intelligence and operations research: Perspectives from informs fellows on the next frontier. H Wiberg, T Dai, H Lam, R Kulkarni, INFORMS Journal on Data Science. 2025</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Z Wu, L Qiu, A Ross, E Akyürek, B Chen, B Wang, N Kim, J Andreas, Y Kim, arXiv:2307.024772023arXiv preprint</p>
<p>Nature language model: Deciphering the language of nature for scientific discovery. Y Xia, P Jin, S Xie, L He, C Cao, R Luo, G Liu, Y Wang, Z Liu, Y.-J Chen, Z Guo, Y Bai, P Deng, Y Min, Z Lu, H Hao, H Yang, J Li, C Liu, J Zhang, J Zhu, R Bi, K Wu, W Zhang, K Gao, Q Pei, Q Wang, X Liu, Y Li, H Zhu, Y Lu, M Ma, Z Wang, T Xie, K Maziarz, M Segler, Z Yang, Z Chen, Y Shi, S Zheng, L Wu, C Hu, P Dai, T.-Y Liu, H Liu, T Qin, arXiv:2502.075272025arXiv preprint</p>
<p>Embedding symbolic knowledge into deep networks. Y Xie, Z Xu, M S Kankanhalli, K S Meel, H Soh, Advances in neural information processing systems. 201932</p>
<p>A semantic loss function for deep learning with symbolic knowledge. J Xu, Z Zhang, T Friedman, Y Liang, G Broeck, International conference on machine learning. PMLR2018</p>
<p>Reinforcement symbolic regression machine. Y Xu, Y Liu, H Sun, The Twelfth International Conference on Learning Representations. 2024</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Y Yamada, R T Lange, C Lu, S Hu, C Lu, J Foerster, J Clune, D Ha, arXiv:2504.080662025arXiv preprint</p>
<p>Differentiable learning of logical rules for knowledge base reasoning. F Yang, Z Yang, W W Cohen, 201730Advances in neural information processing systems</p>
<p>Large language models for automated open-domain scientific hypotheses discovery. Z Yang, X Du, J Li, J Zheng, S Poria, E Cambria, ACL 2024 findings. 2024</p>
<p>Moose-chem: Large language models for rediscovering unseen chemistry scientific hypotheses. Z Yang, W Liu, B Gao, T Xie, Y Li, W Ouyang, S Poria, E Cambria, D Zhou, The Thirteenth International Conference on Learning Representations. 2025</p>
<p>Exploring the role of large language models in the scientific method: from hypothesis to discovery. Y Zhang, S A Khan, A Mahmud, H Yang, A Lavin, M Levin, J Frey, J Dunnmon, J Evans, A Bundy, Artificial Intelligence. 11142025</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
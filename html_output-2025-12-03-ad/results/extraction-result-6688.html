<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6688 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6688</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6688</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-250264533</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2207.01206v4.pdf" target="_blank">WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</a></p>
                <p><strong>Paper Abstract:</strong> Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. To bridge this gap, we develop WebShop -- a simulated e-commerce website environment with $1.18$ million real-world products and $12,087$ crowd-sourced text instructions. Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item. WebShop provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration. We collect over $1,600$ human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of $29\%$, which outperforms rule-based heuristics ($9.6\%$) but is far lower than human expert performance ($59\%$). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6688.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6688.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WebShop_IL+RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>WebShop imitation-learning + reinforcement-learning agent (BART search + BERT choice)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's primary agent pipeline: a BART seq2seq model to generate search queries and a BERT-based choice model (with cross-attention over observation and action texts) trained with imitation learning and optionally fine-tuned with policy gradient RL; evaluated on the WebShop e-commerce interaction benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>IL+RL (BART search + BERT choice)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two-stage agent: (1) a BART-based search-generator produces top-k search queries (beam search); (2) a BERT-based choice policy encodes observation and candidate action texts (plus ResNet image features), fuses them via a cross-attention layer to score actions; the choice model is trained with behavioral cloning (imitation learning) and optionally fine-tuned with policy gradient RL while BART is frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART (search) + BERT (choice); ResNet-50 for images</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebShop (simulated e-commerce web environment)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-window history concatenation (hand-crafted input history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>raw past observation text (simple-mode text) and past action tokens (last 5 actions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>fixed-size window concatenation / append of the most recent observation and a fixed number of last actions (1 prior observation + last 5 actions)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>none beyond direct inclusion in model input (no learned retrieval; memory is part of the current input)</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>imitation learning (behavioral cloning) for both search generation and choice; policy-gradient RL fine-tuning of the choice model (IL+RL) with BART frozen</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Task Score (100 × average reward) and Success Rate (%) (reward computed from attribute/option/price/type match as described)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Including a short history (one past observation + last five actions) produced a slight degradation: Task Score decreased from 59.9 to 57.3 (reported for IL model with history). Success-rate numbers for this ablation are not reported explicitly.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>IL model (no explicit history window) Task Score = 59.9, Success Rate higher than the ablated-history setting (exact SR for this particular ablation not given in text).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>A naive concatenation of a short past-history (1 past observation + last 5 actions) slightly degraded performance (59.9 → 57.3), indicating simple history-feeding was not helpful; RL fine-tuning (IL→IL+RL) made the agent more greedy (shorter trajectories), improving attribute/type/price scores but decreasing option matching score.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>The paper notes that the agent lacks long-term memory for comparing items and backtracking; naive short-history concatenation degraded performance, and authors argue that more advanced explicit memory modules are needed. They also note no explicit learned memory module was used and that memory integration remains an open challenge.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Authors recommend exploring explicit memory modules (e.g., episodic/external memory) combined with strategic exploration, rather than naive history concatenation; they also suggest pairing memory mechanisms with exploration incentives and more advanced retrieval/update strategies for long-horizon decision making.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6688.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6688.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Keep_CALM_textgames</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Keep CALM and Explore: Language Models for Action Generation in Text-based Games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work on using language models for action generation in text-based games; referenced in WebShop as related work on text-game action generation and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Keep CALM and Explore: Language Models for Action Generation in Text-based Games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>interactive fiction / text-based games (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>referenced in context of imitation/RL and text-generation approaches</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Mentioned only as related prior work; WebShop does not report details of memory usage from this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6688.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6688.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-stage_epic_control_textgames</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-stage episodic control for strategic exploration in text games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited paper about episodic control and strategic exploration in text games; WebShop references it when discussing exploration and memory challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Multi-stage episodic control for strategic exploration in text games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>text games / interactive fiction (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Only cited in WebShop in the context of strategic exploration and episodic memory; no mechanistic details provided in the WebShop paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6688.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6688.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fortunato_working_episodic_memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generalization of reinforcement learners with working and episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work on working and episodic memory in reinforcement learners; WebShop references it among memory-related literature that could inform agents needing long-term memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generalization of reinforcement learners with working and episodic memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Only cited as relevant prior art on working/episodic memory; WebShop does not report details from this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6688.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6688.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Lampinen_hierarchical_memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Towards mental time travel: a hierarchical memory for reinforcement learning agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited paper proposing hierarchical memory architectures for RL agents; referenced in WebShop's discussion of long-term memory needs (e.g., for backtracking and comparison).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Towards mental time travel: a hierarchical memory for reinforcement learning agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Mentioned only as a pointer to hierarchical memory ideas; no implementation or comparison details given in WebShop.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6688.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6688.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Wayne_unsupervised_predictive_memory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised predictive memory in a goal-directed agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work on predictive memory mechanisms for goal-directed agents; referenced in WebShop's related-memory literature list.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Unsupervised predictive memory in a goal-directed agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Only cited; WebShop does not detail the mechanism or performance of this work.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6688.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6688.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Guo_interactive_fiction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited paper on formulating interactive fiction (text-based game) playing as a reading-comprehension style RL problem; included in WebShop's related work on text-game techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>interactive fiction / text-based games (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>referenced in the context of RL for text-based environments</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Cited as related work; no memory specifics extracted from WebShop text.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Keep CALM and Explore: Language Models for Action Generation in Text-based Games <em>(Rating: 2)</em></li>
                <li>Multi-stage episodic control for strategic exploration in text games <em>(Rating: 2)</em></li>
                <li>Generalization of reinforcement learners with working and episodic memory <em>(Rating: 2)</em></li>
                <li>Towards mental time travel: a hierarchical memory for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Unsupervised predictive memory in a goal-directed agent <em>(Rating: 2)</em></li>
                <li>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6688",
    "paper_id": "paper-250264533",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "WebShop_IL+RL",
            "name_full": "WebShop imitation-learning + reinforcement-learning agent (BART search + BERT choice)",
            "brief_description": "The paper's primary agent pipeline: a BART seq2seq model to generate search queries and a BERT-based choice model (with cross-attention over observation and action texts) trained with imitation learning and optionally fine-tuned with policy gradient RL; evaluated on the WebShop e-commerce interaction benchmark.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "agent_name": "IL+RL (BART search + BERT choice)",
            "agent_description": "Two-stage agent: (1) a BART-based search-generator produces top-k search queries (beam search); (2) a BERT-based choice policy encodes observation and candidate action texts (plus ResNet image features), fuses them via a cross-attention layer to score actions; the choice model is trained with behavioral cloning (imitation learning) and optionally fine-tuned with policy gradient RL while BART is frozen.",
            "model_name": "BART (search) + BERT (choice); ResNet-50 for images",
            "model_size": null,
            "benchmark_name": "WebShop (simulated e-commerce web environment)",
            "memory_used": true,
            "memory_type": "short-window history concatenation (hand-crafted input history)",
            "memory_representation": "raw past observation text (simple-mode text) and past action tokens (last 5 actions)",
            "memory_update_mechanism": "fixed-size window concatenation / append of the most recent observation and a fixed number of last actions (1 prior observation + last 5 actions)",
            "memory_retrieval_method": "none beyond direct inclusion in model input (no learned retrieval; memory is part of the current input)",
            "training_method": "imitation learning (behavioral cloning) for both search generation and choice; policy-gradient RL fine-tuning of the choice model (IL+RL) with BART frozen",
            "evaluation_metric": "Task Score (100 × average reward) and Success Rate (%) (reward computed from attribute/option/price/type match as described)",
            "performance_with_memory": "Including a short history (one past observation + last five actions) produced a slight degradation: Task Score decreased from 59.9 to 57.3 (reported for IL model with history). Success-rate numbers for this ablation are not reported explicitly.",
            "performance_without_memory": "IL model (no explicit history window) Task Score = 59.9, Success Rate higher than the ablated-history setting (exact SR for this particular ablation not given in text).",
            "has_comparative_results": true,
            "ablation_findings": "A naive concatenation of a short past-history (1 past observation + last 5 actions) slightly degraded performance (59.9 → 57.3), indicating simple history-feeding was not helpful; RL fine-tuning (IL→IL+RL) made the agent more greedy (shorter trajectories), improving attribute/type/price scores but decreasing option matching score.",
            "reported_limitations": "The paper notes that the agent lacks long-term memory for comparing items and backtracking; naive short-history concatenation degraded performance, and authors argue that more advanced explicit memory modules are needed. They also note no explicit learned memory module was used and that memory integration remains an open challenge.",
            "best_practices_recommendations": "Authors recommend exploring explicit memory modules (e.g., episodic/external memory) combined with strategic exploration, rather than naive history concatenation; they also suggest pairing memory mechanisms with exploration incentives and more advanced retrieval/update strategies for long-horizon decision making.",
            "uuid": "e6688.0",
            "source_info": {
                "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Keep_CALM_textgames",
            "name_full": "Keep CALM and Explore: Language Models for Action Generation in Text-based Games",
            "brief_description": "Cited work on using language models for action generation in text-based games; referenced in WebShop as related work on text-game action generation and exploration.",
            "citation_title": "Keep CALM and Explore: Language Models for Action Generation in Text-based Games",
            "mention_or_use": "mention",
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "agent_name": null,
            "agent_description": null,
            "model_name": null,
            "model_size": null,
            "benchmark_name": "interactive fiction / text-based games (cited)",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "referenced in context of imitation/RL and text-generation approaches",
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Mentioned only as related prior work; WebShop does not report details of memory usage from this paper.",
            "best_practices_recommendations": null,
            "uuid": "e6688.1",
            "source_info": {
                "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Multi-stage_epic_control_textgames",
            "name_full": "Multi-stage episodic control for strategic exploration in text games",
            "brief_description": "Cited paper about episodic control and strategic exploration in text games; WebShop references it when discussing exploration and memory challenges.",
            "citation_title": "Multi-stage episodic control for strategic exploration in text games",
            "mention_or_use": "mention",
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "agent_name": null,
            "agent_description": null,
            "model_name": null,
            "model_size": null,
            "benchmark_name": "text games / interactive fiction (cited)",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": null,
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Only cited in WebShop in the context of strategic exploration and episodic memory; no mechanistic details provided in the WebShop paper.",
            "best_practices_recommendations": null,
            "uuid": "e6688.2",
            "source_info": {
                "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Fortunato_working_episodic_memory",
            "name_full": "Generalization of reinforcement learners with working and episodic memory",
            "brief_description": "Cited work on working and episodic memory in reinforcement learners; WebShop references it among memory-related literature that could inform agents needing long-term memory.",
            "citation_title": "Generalization of reinforcement learners with working and episodic memory",
            "mention_or_use": "mention",
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "agent_name": null,
            "agent_description": null,
            "model_name": null,
            "model_size": null,
            "benchmark_name": null,
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": null,
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Only cited as relevant prior art on working/episodic memory; WebShop does not report details from this paper.",
            "best_practices_recommendations": null,
            "uuid": "e6688.3",
            "source_info": {
                "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Lampinen_hierarchical_memory",
            "name_full": "Towards mental time travel: a hierarchical memory for reinforcement learning agents",
            "brief_description": "Cited paper proposing hierarchical memory architectures for RL agents; referenced in WebShop's discussion of long-term memory needs (e.g., for backtracking and comparison).",
            "citation_title": "Towards mental time travel: a hierarchical memory for reinforcement learning agents",
            "mention_or_use": "mention",
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "agent_name": null,
            "agent_description": null,
            "model_name": null,
            "model_size": null,
            "benchmark_name": null,
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": null,
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Mentioned only as a pointer to hierarchical memory ideas; no implementation or comparison details given in WebShop.",
            "best_practices_recommendations": null,
            "uuid": "e6688.4",
            "source_info": {
                "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Wayne_unsupervised_predictive_memory",
            "name_full": "Unsupervised predictive memory in a goal-directed agent",
            "brief_description": "Cited work on predictive memory mechanisms for goal-directed agents; referenced in WebShop's related-memory literature list.",
            "citation_title": "Unsupervised predictive memory in a goal-directed agent",
            "mention_or_use": "mention",
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "agent_name": null,
            "agent_description": null,
            "model_name": null,
            "model_size": null,
            "benchmark_name": null,
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": null,
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Only cited; WebShop does not detail the mechanism or performance of this work.",
            "best_practices_recommendations": null,
            "uuid": "e6688.5",
            "source_info": {
                "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Guo_interactive_fiction",
            "name_full": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "brief_description": "Cited paper on formulating interactive fiction (text-based game) playing as a reading-comprehension style RL problem; included in WebShop's related work on text-game techniques.",
            "citation_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
            "agent_name": null,
            "agent_description": null,
            "model_name": null,
            "model_size": null,
            "benchmark_name": "interactive fiction / text-based games (cited)",
            "memory_used": null,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "referenced in the context of RL for text-based environments",
            "evaluation_metric": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": null,
            "reported_limitations": "Cited as related work; no memory specifics extracted from WebShop text.",
            "best_practices_recommendations": null,
            "uuid": "e6688.6",
            "source_info": {
                "paper_title": "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Keep CALM and Explore: Language Models for Action Generation in Text-based Games",
            "rating": 2
        },
        {
            "paper_title": "Multi-stage episodic control for strategic exploration in text games",
            "rating": 2
        },
        {
            "paper_title": "Generalization of reinforcement learners with working and episodic memory",
            "rating": 2
        },
        {
            "paper_title": "Towards mental time travel: a hierarchical memory for reinforcement learning agents",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised predictive memory in a goal-directed agent",
            "rating": 2
        },
        {
            "paper_title": "Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01655525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents
8 Feb 2023</p>
<p>Shunyu Yao shunyuy@princeton.edu 
Department of Computer Science
Princeton University</p>
<p>Howard Chen howardchen@princeton.edu 
Department of Computer Science
Princeton University</p>
<p>John Yang 
Department of Computer Science
Princeton University</p>
<p>Karthik Narasimhan karthikn@princeton.edu 
Department of Computer Science
Princeton University</p>
<p>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents
8 Feb 202371C7EE5B5550E693CFDBD8A04342AF76arXiv:2207.01206v4[cs.CL]
Existing benchmarks for grounding language in interactive environments either lack real-world linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals.To bridge this gap, we develop WebShop -a simulated e-commerce website environment with 1.18 million real-world products and 12, 087 crowd-sourced text instructions.Given a text instruction specifying a product requirement, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase an item.WebShop provides several challenges for language grounding including understanding compositional instructions, query (re-)formulation, comprehending and acting on noisy text in webpages, and performing strategic exploration.We collect over 1, 600 human demonstrations for the task, and train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models.Our best model achieves a task success rate of 29%, which outperforms rule-based heuristics (9.6%) but is far lower than human expert performance (59%).We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities.Finally, we show that agents trained on WebShop exhibit non-trivial sim-to-real transfer when evaluated on amazon.comand ebay.com, indicating the potential value of WebShop in developing practical web-based agents that can operate in the wild.</p>
<p>Introduction</p>
<p>Recent advances in natural language processing (NLP) and reinforcement learning (RL) have brought about several exciting developments in agents that can perform sequential decision making while making use of linguistic context [30,50,58].On the other hand, large-scale language models like GPT-3 [6] and BERT [11] are excelling at traditional NLP benchmarks such as text classification, information extraction and question answering.While the former set of tasks are limited in their set of linguistic concepts and prove difficult to scale up, the latter tasks usually contain static, noninteractive datasets that lack adequate grounding to extra-linguistic concepts [4].In order to make further progress in building grounded language models, we believe there is a need for scalable interactive environments that contain: (1) language elements that reflect rich, real-world usage and are collectible at scale, and (2) task feedback that is well-defined and automatically computable to facilitate interactive learning, without the constant need for expensive feedback from humans.</p>
<p>The world wide web (WWW) is a massive open-domain interactive environment that inherently satisfies the first aforementioned requirement through its interconnected set of pages with natural text, images and interactive elements.By being simultaneously scalable, semantic, interactive, dynamic and realistic, the web is uniquely different from existing environments for autonomous   agents like games or 3D navigation.Moreover, the web also provides a practical environment to deploy trained agents, with great potential for alleviating human efforts in tedious tasks (e.g.buying products, booking appointments).While there has been prior work on building web-based tasks, they either lack depth in the transition and action spaces, or prove difficult to scale up.Some benchmarks only contain either a single classification task [39,46,31] or interactions containing only a handful of different pages in each episode [43].Others propose tasks with longer horizons but are either limited to following hyperlinks for web navigation [36] or require human-in-the-loop feedback due to the lack of an automated reward function [33].
i h W g G s 1 A e O y L H p A R O S 0 Y C E V V l P 8 = " &gt; A A A C e H i c d V F N b 9 Q w E H V C g R K + F n r s x e o S A Z d V U i F A n L p w 4 U a R u m 3 R 7 m o 1 8 c 6 2 V h 0 7 s i c r o i j 9 C / w 3 b v 0 h X D j h 7 K a C f j C S p T f v z T z b M 1 m h p K M k u Q j C O x t 3 7 9 3 f f B A 9 f P T 4 y d P e s + e H z p R W 4 E g Y Z e x x B g 6 V 1 D g i S Q q P C 4 u Q Z w q P s r N P r X 6 0 R O u k 0 Q d U F T j N 4 U T L h R R A n p r 1 f s S T U s / R Z h Y E 1 u f / i y a K J 4 T f q R 4 u Q S r w 7 n w o W g f 3 o T k f X o p f M o d 2 u X L 2 t I n i b 7 O 1 Y A p q / m Z A b V Z 1 W W G l w C a a Z G D r q p n 1 + s k g W Q W / C d I O 9 F k X + 7 P e z 8 n c i D J H T U K B c + M 0 K W h a g y U p V O t b O i x A n M E J j j 3 U k K O b 1 q v B N T z 2 z J w v j P V H E 1 + x / 3 b U k D t X 5 Z m v z I F O 3 X W t J W / T x i U t 3 k 9 r q Y u S U I v 1 R Y t S c T K 8 3 Q K f S 4 u C V O U B C C v 9 W 7 k 4 B b 8 D 8 r u K / B D S 6 1 + + C Q 5 3 B + n b Q f r 1 T X / v Y z eO x X N v 9 u J l v + 1 n W W 3 d / W C h K j b o = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 1 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 I J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 b u a 3 n 1 B p H s s H M 0 n Q j + h Q 8 p A z a q z U S P v l i l t 1 5 y C r x M t J B X L U + + W v 3 i B m a Y T S M E G 1 7 n p u Y v y M K s O Z w G m p l 2 p M K B v T I X Y t l T R C 7 W f z Q 6 f k z C o D E s b K l j R k r v 6 e y G i k 9 S Q K b G d E z U g v e z P x P 6 + b m v D G z 7 h M U o O S L R a F q S A m J r O v y Y A r Z E Z M L K F M c X s r Y S O q K D M 2 m 5 I N w V t + e Z W 0 L q r e V d V r X F Z q t 3 k c R T i B U z g H D 6 6 h B v d Q h y Y w Q H i G V 3 h z H p 0 X 5 9 3 5 W L Q W n
In this paper, we introduce WebShop (Figure 1) -a large-scale interactive web-based environment for language understanding and decision making -and train autonomous agents to complete tasks on this benchmark.With the goals of being scalable and containing realistic language and visual elements, WebShop emulates the task of online shopping on an e-commerce website, where the agent's goal is to understand a human-provided text instruction and purchase a product to match the specifications.To do so, the agent needs to query the website's search engine, choose items to explore from search results, open and read their description and details, and select the necessary options (e.g.32 oz., red color) before clicking the 'Buy' button.In order to pick the optimal product that matches user requirements, the agent may need to view and compare various products (including backtracking between pages), and potentially perform multiple searches.WebShop contains over one million products scraped from amazon.com, over 12 thousand crowdsourced instructions, and a diverse semantic action space of searching text queries and choosing text buttons.It is packaged into a convenient OpenAI Gym [5] environment and can be rendered in two modes (HTML or simple) with parallel observation spaces that are easy for human and model respectively.Rewards are automatically computed using a combination of programmatic matching functions that consider the attributes, type, options and price of the chosen product, alleviating the need for human evaluation and providing a path to scaling up interactive learning.</p>
<p>We develop several agents to perform this task, using both reinforcement learning (RL) and imitation learning (IL).We also leverage the latest pre-trained language models [26,11] for representing and generating text.Our modular architecture includes a factorized processing of state observations and action choices using ResNets (visual) and Transformers (text), followed by an attention fusion layer that helps the agent contextually score each action.Our best agent achieves an average score of 62.4 (out of 100) and successfully completes the task 28.7% of the time, significantly higher than a heuristic baseline that achieves 45.6 and 9.6%, respectively.While this demonstrates the potential for IL and RL, the agents are still much lower than human experts, who can achieve 82.1 and 59.6% on this task.* We perform several analyses and ablation studies to identify the cause of this gap and find several avenues for agent improvement in the future including more robust search generation, explicit memory modules, and better handling of noisy web text.Finally, we also demonstrate an instance of sim-to-real transfer by deploying agents trained with WebShop to operate on amazon.comand ebay.com, and find that they can achieve similar performances despite search engine and product differences, and consistently outperform the rule baseline of using the first result returned by the commercial search engines when directly searching the instruction texts.This demonstrates the practical potential of our work towards developing agents that can operate autonomously on the world wide web (WWW).</p>
<p>Related Work</p>
<p>Reinforcement learning on the web.Nogueira and Cho [36] introduced WikiNav as a benchmark for RL agents navigating pages, but the task is purely navigational with the actions restricted to either choosing a hyperlink to follow or deciding to stop.The World of Bits (WoB) benchmark [43] enables training of RL agents to complete tasks on webpages using pixel and Document Object Model (DOM) observations.Several follow-up papers have tackled MiniWoB using techniques like workflow-guided exploration [29], curriculum and meta-learning [15], DOM tree representation [21], adversarial environment generation [16] and large-scale behavioral cloning [20].However, MiniWoB lacks long-range decision making across multiple different pages and does not scale easily in terms of difficulty or size due to its use of low-level mouse clicks and keystrokes as actions.In contrast, WebShop requires navigating longer paths with context-based action selection and backtracking, and it uses high-level search and choose actions that are more scalable and transferable to real settings.</p>
<p>While not directly operating on web pages, AndroidEnv [48] and MoTIF [8] provide environments to train agents for interacting with apps and services on mobile platforms.</p>
<p>Non-interactive web-based tasks.Various supervised classification tasks on webpages have been proposed, including predicting web elements [39], generating API calls [46,47,54] and semantic parsing into concept-level navigation actions [31].Perhaps most similar content-wise to our work is the Klarna product page dataset [19] which contains over 50, 000 product pages labeled with different element categories for supervised classification.All these works only consider supervised settings with a single decision, and may require the definition of web APIs or command templates for each domain.Our benchmark, WebShop, combines webpages with realistic text and image content with a rich and diverse interaction space for long-range sequential decision making.</p>
<p>Leveraging the web for traditional NLP tasks.Several papers have explored the use of the web for information extraction [34] and retrieval [1], question answering [57,25], dialog [45], and training language models on webtext [2].These approaches primarily use web search engines as a knowledge retriever for gathering additional evidence for the task at hand.Perhaps most similar to our work is WebGPT [33], which uses a web interface integrated with a search engine to train RL agents to navigate the web and answer questions.However, our environment has a more diverse action and observation space (including images) and does not require human-in-the-loop evaluation.</p>
<p>The WebShop Environment</p>
<p>We create WebShop as a large-scale web-based interactive environment with over 1.1 million realworld products scraped from amazon.com.In this environment, an agent needs to find and purchase a product according to specifications provided in a natural language instruction.WebShop is designed  in a modular fashion which disentangles the website transitions from the task-specific aspects like instructions and reward, allowing for easy extension to new tasks and domains.</p>
<p>Task Formulation</p>
<p>WebShop can be formulated as a partially observable Markov decision process (POMDP) (S, A, T , R, U, O) with state space S, action space A, deterministic transition function T : S × A → S, reward function R : S × A → [0, 1], instruction space U, and a state observation space O.</p>
<p>State and action.</p>
<p>A state s ∈ S represents a web page, which falls into one of the four types -the search page that contains a search bar, the results page that lists a set of products returned by a search engine, the item page that describes a product, or the item-detail page that shows further information about the product (Figure 1A(1-4) respectively).We define the following notations for a product y.We denote ȳ to be the aggregation of the various text fields including product title, description, and overview.We denote y price to be the price, Y opt to be a set of buying options, and I to be a set of images, each corresponding to a specific option.Finally, each product is associated with Y att , a set of attributes hidden from the agent which is extracted from the title and the item-detail pages ( §3.2).The attributes are used for the automatic reward calculation.</p>
<p>An action a ∈ A(s) can either be searching a text query (e.g.search[Red shoes]) or choosing a text button (e.g.choose[Size 9]) as shown in Table 1.These two action types are not available simultaneously -search is only allowed when the agent is at the search page; on all other pages, click is the only action choice.The chosen action argument (button) will be clicked as a web link as opposed to the low-level mouse-click actions in previous environments such as World of Bits [43].</p>
<p>The transitions initiated by clicks deterministically redirect the web page to one of the four page types (Table 1).The transition initiated by search is based on a deterministic search engine ( §3.2).</p>
<p>Observation.Using Flask [41] and OpenAI Gym [5], we provide two parallel observation modes to render the state and instruction S × I → O: (1) HTML mode that contains the HTML of the web page, allowing for interaction in a web browser(Figure 1A), and (2) simple mode which strips away extraneous meta-data from raw HTML into a simpler format (Figure 1B).The human performance scores in §4.2 are collected in the HTML mode, while all models are trained and evaluated in the simple mode.Note that while the environment allows for training reinforcement learning agents on raw pixels in HTML mode (like in Shi et al. [43]), we believe that it provides a very low-level non-semantic action space.Moreover, it is straightforward to write a translator that converts any new HTML page into simple format for use with trained agents, which enables sim-to-real transfer.</p>
<p>Instruction and reward.Each natural language instruction u ∈ U contains the following information: a non-empty set of attributes U att , a set of options U opt , and a price u price .The instruction is generated based on a target product y * by human annotators.The instruction collection process is lightweight and scalable ( §3.2).Concretely, U att ⊆ Y * att is a subset of the product attributes,
U opt ⊆ Y *
opt is a subset of the product option field-value pairs, u price &gt; y * price is a price set to be higher than the target product price.For example, the instruction "Can you find me a pair of blackand-blue sneaker that is good in rain weather?I want it to have puffy soles, and price less than 90 dollars."contains the aforementioned attributes U att = {"waterproof", "soft sole"} and option U opt = {"color": "black and blue"}.In each episode, the agent receives a reward r = R(s T , a) in the end at timestep T , where a = choose[buy], y is the product chosen by the agent in the final state s T , and Y att and Y opt are its corresponding attributes and options.The reward is defined as:
r = r type • |U att ∩ Y att | + |U opt ∩ Y opt | + 1[y price ≤ u price ] |U att | + |U opt | + 1(1)
where the type reward r type = TextMatch(ȳ, ȳ * ) is based on text matching heuristics to assign low reward when y and y * have similar attributes and options but are obviously different types of products.For example, "butter" and "plant-based meat" differ in types but may both contain attributes "cruelty-free", "non-GMO", and an option "size: pack of 2".The exact formula for TextMatch(•) is in the Appendix §A.5.</p>
<p>Evaluation metrics.We use two evaluation metrics: (1) Task Score: defined as (100×avg.reward), which captures the average reward obtained across episodes; and (2) Success Rate (SR) defined as the portion of instructions where r = 1.Note that it is possible to obtain r = 1 for an episode even if the final product is not y * -for example, there could be many items that satisfy the goal "I want a red shirt", even if the goal is generated from a specific red shirt item.</p>
<p>Environment Implementation</p>
<p>Data scraping.We use ScraperAPI [35] to scrape 1, 181, 436 products from amazon.comacross 5 categories (fashion, makeup, electronics, furniture, and food) using 113 sub-category names as queries.The product texts (title and item details) have an average length of 262.9 and a vocabulary size 224, 041 (word frequency higher than 10).In addition, the products have a total of 842, 849 unique options, reflecting the scale and complexity of the data.More details about product scraping is in the Appendix §A.1.</p>
<p>Search engine.We use Pyserini [28] for the search engine, where indices are built offline using a BM25 sparse retriever with text for each product concatenated from the title, description, overview, and customization options.The search engine is deterministic, which eases imitation learning and result reproducibility.More details in A.3.</p>
<p>Attribute mining and annotation.Each product is annotated with a set of hidden attributes, which are used to represent its latent characteristics as well as to calculate the reward as detailed in §3.An attribute is a short natural language phrase that describes the property of the product (see examples in Figure 1).We mine the attributes by calculating TF-IDF scores for all bi-grams in the concatenated titles and descriptions based on each product category.We review the top 200 bi-grams for each category, remove the noisy ones by inspection (decide based on whether the bi-gram is human understandable), and assign them to the products.We consolidate a pool of 670 attributes.See more details in the Appendix §A.2.</p>
<p>Natural language instructions.We use Amazon Mechanical Turk (AMT) to collect natural language instructions that specify goal products with appropriate options.Specifically, an AMT worker is presented with a sampled goal product, including the product title, category, attributes, and the buying options, and asked to write a command to instruct an automatic shopping agent to find the target.Workers are instructed to avoid being too specific such as including the entire title in the instruction, but stay faithful to describing the target product.We collect a total of 12, 087 linguistically diverse instructions with an overall vocabulary size of 9, 036 words and an average length of 15.9 words.We provide the detailed annotation process and interface in the Appendix §A.4.</p>
<p>Human demonstrations.We collect trajectories from humans performing the task in the HTML mode of WebShop to understand the task difficulty for humans and to analyze how humans would solve the task.We use qualification tests to train and select motivated workers to perform the task.We recruit and train a total of 13 workers for data collection, and among them we select the top 7 performing workers to be "experts" (see Appendix §A.6 for examples).We also leverage this data to perform imitation learning (described in §4.2).</p>
<p>Research Challenges</p>
<p>WebShop brings together several research challenges for autonomous systems from various subfields in NLP and RL into a single benchmark.These include: 1) generation of good search queries [22,59] and reformulation [37,51], 2) strategic exploration for navigating through the website [55,56,29],</p>
<p>3) robust language understanding for textual state and action spaces [3,7,17,44], and 4) long-term   memory for comparing items or backtracking [53,13,23] (Figure 1).While we believe individual advances in each of these will improve agent performance, WebShop also provides an ideal testbed for the development of interdisciplinary techniques that tackle more than one of the above mentioned challenges simultaneously.For example, external memory modules may be very effective if combined with strategic exploration, or exploration could be helpful in information query reformulation.Further analysis based on human and model trajectories is in §5.3.
I r W 7 c q W B V a E v J p L c a z C R D c q d Y h v F v / C F 3 / o 1 p n Y W v p Q c C h 3 P u K / f G q R Q W w / Dn V s 1 Q B G v y Z B V A o = " &gt; A A A C R n i c d V B B S x t B G P 0 2 1 a q r 1 l i P X g Z D w F P Y L a U V T 8 Z e v G n B J E K y h N n J F x 2 c n V 1 m v g 2 G J f 4 5 L z 3 3 1 p / g x Y M i v T q J E T T R B w O P 9 7 7 v z c y L M y U t B c E / r / R p Y f H z 0 v K K v 7 q 2 / m W j v P m 1 a d P c C G y I V K X m L O Y W l d T Y I E k K z z K D P I k V t u L L X 2 O / N U B j Z a p P a Z h h l P B z L f t S c H J S t x x V O 7 n u o Y k N F 1 h c f 4 S R X + 0 Q X l F R</p>
<p>Methods</p>
<p>We propose various models that combine language and image pre-training with imitation learning (IL) and reinforcement learning (RL).More details are provided in the Appendix §B.</p>
<p>Rule Baseline</p>
<p>A simple rule baseline is to search the exact instruction text, then choose and buy the first item in the results page without choosing any options.The heavy lifting of the lexical search engine makes it also a simple non-learnable information retrieval (IR) baseline, and would lead to a non-trivial attribute reward.However, simple heuristic rules cannot resolve noisy natural language options, strategically explore, or learn to generate what to search, so the total reward and task success rate should be low.</p>
<p>Imitation Learning (IL)</p>
<p>For the text generation and choice problems presented in WebShop, we propose using two pre-trained language models to separately learn how to search and choose from human demonstrations.</p>
<p>Imitating human search generation.We frame searching as a sequence-to-sequence text-generation problem: the agent generates a search action a = search[. ..] given an instruction u without considering any other context (e.g.past searches, visited items).We use M = 1, 421 instructionsearch pairs from 1, 012 training human trajectories to construct a dataset D = {(u, a)} M i=1 and fine-tune a BART model [26] parameterized by φ to perform conditional language modeling:
L search = E u,a∼D <a href="2">− log π φ (a | u)</a>
Imitating human choice.The choice-based imitation model (Figure 3) predicts a probability distribution over all the available click actions A(o) in observation o and maximizes the likelihood of the human clicked button a * ∈ A(o).We construct a dataset D = {(o, A(o), a * )} M i=1 of M = 9, 558 samples from the training human trajectories.We use a 12-layer pre-trained BERT model [10] parameterized by θ to encode the o into an observation representation of contextualized token embeddings, and we similarly encode each action.Each action representation is passed into a cross-attention layer with the observation representation, then mean pooled into a single vector and multiplied with a matrix W to obtain a scalar score S(o, a).The policy π θ (a | o, A(o)) is the softmax distribution over action scores S(o, a):
L choose = E o,A(o),a * ∼D <a href="3">− log π θ (a * | o, A(o))</a>π θ (a | o, A(o)) ∼ exp W mean cross-attn BERT(o; θ), BERT(a; θ)(4)
Handling Images.We use a pre-trained ResNet-50 [18] to pre-process images across different products and options into a 512 dimensional feature vector, which is then transformed into 768 dimensions with a learned linear layer and concatenated to BERT(o) as the observation representation.</p>
<p>Full pipeline.Combining the above during environment interaction, we use the BART model in the search page to generate the top-5 search queries via beam search and choose a random one.</p>
<p>For other pages, we sample one action from π θ (a | o, A(o)) using the BERT model.We find these methods useful to encourage diverse actions.In contrast, an ineffective strategy that uses only the top generated search query or the button with the highest probability might lead to limited product candidates or being stuck (e.g.bouncing back and forth between pages).</p>
<p>Reinforcement Learning (RL)</p>
<p>We also fine-tune the choice-based IL model with online RL (i.e.IL+RL).Prior work suggests that directly fine-tuning text generation via RL might lead to language drifting [24] and deteriorated performance.Therefore, we freeze the BART model to provide the top-10 search generations as a refined action space for the choice-based IL model to learn to pick -an inspiration borrowed from previous work in text games [55] and referential games [24].We use the policy gradient method [32] with return-to-go R t = E π [r t + γR t+1 ] and a learned value baseline V (o) = W v BERT(o; θ) parameterized by {W v , θ} (the BERT weights are tied with the policy):
L PG = E π <a href="5">− (R t − V (o t )) log π (a t | o t , A(o t ))</a>
The value V (o) is learned with an L2 loss L value = (R t − V (o t )) 2 .We also add an entropy loss
L entropy = a∈A(ot) π θ a t | o t , A(o t ) log π θ a t | o t , A(o t ) to prevent premature convergence.
Our full RL model minimizes the total loss L RL = L PG + L value + L entropy .</p>
<p>Experiments</p>
<p>Setup and task verification</p>
<p>We split a total of 12, 087 instructions into an i.i.d.distributed train / development / test split of 10, 587 / 1, 000 / 500 instances for all models.While future work can investigate splits with more generalization gaps (e.g.split by product category), we will show the i.i.d.split is already challenging for current models.We randomly sample a subset of the 10, 587 training instructions, then collect 1, 012 human demonstrations for task verification and imitation learning (IL) and a further 54 demonstrations from instances in the development set for IL hyperparameter tuning and checkpoint selection.We also collect human trajectories for all 500 test instructions and report human and model performances averaged across these 500 instructions.More setup details are in the Appendix §C.</p>
<p>Results</p>
<p>Task performance.From Figure 4, we observe that the rule baseline obtains a low score of 45.6 and a very low success rate of 10% since it cannot resolve options specified in language or explore more products, empirically demonstrating the non-trivial nature of the task.The IL model significantly outperforms the rule baseline on both metrics, achieving a score of 59.9.Further RL finetuning improves the score to 62.4 while slightly hurting the success rate (29.1% → 28.7%) (analyzed further in §5.3).We also observe a significant gap between models and humans -our best model's success rate (29.1%) is less than half of expert humans (59.6%) and only 60% of the average human (50%).This indicates a great room for model improvement by tackling reseach challenges in WebShop.for exploration, but it is not as critical as learning to choose the right options.We experiment with incorporating history of one past observation and the last five actions into the model and find a slight degradation in the score from 59.9 to 57.3, suggesting more advanced techniques are needed to leverage past information.More ablations in §C.</p>
<p>IL ablations.</p>
<p>RL ablations.When we directly train an RL agent (RL) from pre-trained BERT parameters, the performance is even worse than the rule baseline.This suggests that IL warm-starting is critical, possibly because of the significant domain shift from traditional language tasks.We also consider a simple RL model with RNN text encoders instead of the Transformer (RL (RNN)), which has a success rate more than 10% worse than the IL + RL model with a much larger variance.We hypothesize that RL with a more powerful architecture could help boost and stabilize the performance if the model is initialized with better language and task priors.</p>
<p>Analysis</p>
<p>To better understand the differences between the agents and human experts, we perform several fine-grained analyses.We first break down the overall score into its four sub-parts according to Eq. ( 1): 1) attribute score and 4) type score (r type ).We report trajectory statistics such as the average number of states, unique items visited, and number of searches per episode in Table 2 and provide qualitative examples of the trajectories in Table 3.
(|U att ∩ Y att |/|U att |), 2) option score (|U opt ∩ Y opt |/|U opt |), 3) price score (1[y price ≤ u price ]),
Human expert vs. agents.Human experts outperform the agents on all score sub-parts (Table 2), but the most significant boost comes from the option score (a 28% gap), revealing that agents have trouble selecting the correct product options.Humans also have longer trajectories, explore more items and perform more searches than the agents, with a higher variance, demonstrating their flexibility.Table 3 provides some samples trajectories.In the first example, the human decides to search again after removing 'inches', 'width', 'height', and 'white' from the query since product texts often contain abbreviated symbols for these terms like '"', 'w', and 'h'.Thus, search generation is challenging for models since it involves reasoning and adapting to grounded environments, and ideas from query reformulation [37,1] could help alleviate this.Agents also struggle to perform robust semantic  matching, which is important in choosing options that contain noisy paraphrases of instruction spans.</p>
<p>In the second example, the human explores several products first, and decides to return to the first explored product, demonstrating long-term memory that is lacking in the IL+RL model.</p>
<p>Effect of RL fine-tuning after IL.Table 2 also shows that RL fine-tuning adapts the IL model to become more 'greedy' and less 'exploratory', as the average trajectory length drops from 9.4 to 4.8, and the model explores fewer items and search queries.As a result, the attribute, type, and price scores all increase, but option score drops from 45.2 to 38.9.This points to the need for a better balance exploration with exploitation during RL, e.g. by using intrinsic bonuses.</p>
<p>Results with at Choice oracle.To disentangle the effects of learning to search from choosing the right actions, we construct a Choice oracle that has access to the hidden reward function as well as hidden attributes and options underlying each product and instruction.† Given a search query, the Choice oracle will perform an exhaustive search over every result item, try out all combinations of options and finally choose the best item with options that maximize the reward -meaning each episode will take hundreds or thousands of steps, as opposed to 4.5 and 11.3 steps on average for the IL+RL model and human experts (Table 2).We use 500 test instructions and consider four types of search queries: the instruction text (used by rule baseline), top IL BART generated query (used by all learning models), and the first and last queries from human experts in each test trajectory.‡ Choice oracle improves the success rate of rule heuristics from 9.6% to 85.4%, and even the human expert success rate from 59.6% to 87.8% (Table 4), confirming that choosing the right actions is indeed a major bottleneck for current models with great room for improvement.However, using a † A similar search oracle is also possible but harder to design since the search space is infinite.One possible oracle is to search for the underlying product name for each instruction, but that makes choice trivial as the underlying product is then almost always the first search result.‡ 74.8% of the time there is only one query in the trajectory.</p>
<p>better search query is still important even with such a strong Choice oracle, as the last human search query still outperforms other search queries.This also suggests human experts improve search query qualities over reformulations.</p>
<p>Zero-shot Sim-to-real Transfer</p>
<p>Finally, we conduct a 'sim-to-real' transfer experiment where our models trained on WebShop are tested on the real-world Amazon (amazon.com) and eBay (ebay.com)shopping websites without any fine-tuning.We sample 100 test instructions and deploy 3 WebShop models (rule, IL, IL+RL) to interact with Amazon and eBay, and manually score each episode based on Eq. ( 1).As shown in Table 5, model performances on the two website are similar to WebShop performances in Figure 4, except for the rule baseline, likely due to the better search engine of Amazon than WebShop.These results confirm positive sim-to-real values of trained agents for real-world web tasks despite domain shifts in data (products) and dynamics (search engine).We also obtain a human average score of 88.0 / 79.7 and success rate of 65% / 40% by asking turkers ( §3.2) to find the instructed product on the Amazon and eBay websites respectively.While humans perform much better than agents, their web interactions are much slower -taking on average 815 seconds per episode as opposed to &lt; 8 seconds per episode for our IL and IL+RL models on Amazon.This sim-to-real transfer only requires two minor coding additions, suggesting that environments like WebShop are suitable for developing practical grounded agents to reduce human effort on real-world web tasks.We provide additional performance and in-depth analysis in Appendix §D.</p>
<p>Discussion</p>
<p>We have developed WebShop, a new web-based benchmark for sequential decision making and language grounding, modeled on interaction with an e-commerce website.We performed an empirical evaluation of autonomous agents trained using imitation and reinforcement learning, and demonstrated promising results on sim-to-real transfer to real-world shopping websites.Our qualitative and quantitative analysis of model and human trajectories ( §5.3) identified several research challenges in WebShop and provided insights for future model development by incorporating multidisciplinary techniques.For example, pre-training with multi-modal data [27,52], web hypertext [2], or web instruction-action mapping [38] could help agents better understand and leverage rich semantics of webpage content, actions, and instructions.Ideas from query (re)formulation [22,59,37,51] may help agents expand the range of search exploration, and improved action exploration [40,12,49] and memory [53,13,23] mechanisms could help agents make better decisions over the long horizon and large action space.The modular design of WebShop also allows for new web tasks and domains to be easily incorporated, which we hope will help shape future research into grounded language agents with stronger capabilities for real-world web interaction.</p>
<p>(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?[Yes] Discussed in Appendix.Table 7: Two examples of failed human trajectories.A common pattern is impatience: after one search (even with correct attributes like the right example) the less performant worker commits to the first selected item.Often, the item does not contain the desired options even though the item's title text seem relevant.An expert worker will recognize the need to select the correct options and go back to refine the searches, while less performant workers simply commit to the current selected item.</p>
<p>Here, TextMatch(ȳ, ȳ * ) is a simple string match between the selected product title text and the goal product title text.We use only the words tagged with PNOUN, NOUN, and PROPN tags parsed by the SpaCy parser in the title text.</p>
<p>A.6 Human Trajectory Collection</p>
<p>We use the HTML environment in Figure 1 to collect human trajectories.We select a pool of 13 workers using qualification tasks where each workers complete 5 examples.The workers that achieve an average reward more than 0.75 are qualified.The task instruction is shown at the end of Appendix.We observe a pronounced performance gap between the very high performing workers and average workers.We use the top 50% of these qualified workers as experts (7 workers in total).We pay for each completed trajectory 0.7 dollars.In human evaluation, 8 out of the 13 workers participated and 5 among them are in the aformentioned expert pool.The 8 participants achieve an overall score of 75.5 and a success rate of 50.0%We observe non-negligible variance even within the experts-the best performer achieves a score of 87.4 and success rate of 69.5%, while the lowest performing worker achieves a score of 45.8 and success rate of 10%.The best performing worker also shows better consistency-drawing at a standard deviation of 2.3 in score, contrasting the lowest performing counterpart at 3.1.We provide examples of common human failure cases such as not matching the option/attribute due to impatience (Table 7), cautioning some caveats of the task with human workers.</p>
<p>A.7 Reward Verification</p>
<p>We randomly select 100 samples each from the pools of trajectories generated by average and expert MTurk workers.Each trajectory is then manually re-scored against a human criteria; the purpose of this is to determine how representative the reward function is of a human's judgment towards whether the chosen product satisfies the given instructions.The human score calculation procedure exactly follows the formula laid out in Section A.5 -the attribute, option, price, and type scores are individually determined, then aggregated to calculate the overall score -except for one main modification.Instead of the exact matching approach, points are awarded if (1) the picked product's attributes, options, or type are lexically similar or synonymous with the goal's product information and (2) the desired value is not found verbatim anywhere in the picked product's descriptions.For instance, if the value lightweight is specified as a desired attribute for an instruction, but the value easy carry is found instead in the picked product's description, then the attribute score for the picked product is increased to reflect that the lightweight value was found.On the other hand, if cyan is desired as an option for a goal product, but the user picks blue even though cyan is available as a choice, then no points are awarded.To ensure the score is calculated without bias, the original rewards for each trajectory were not compared with the human evaluation scores until the human evaluation scoring was completed.</p>
<p>For the average trajectories, the automatic task score was 74.9 and our manual score was 76.3 with a Pearson correlation of 0.856.For expert trajectories, the respective scores were 81.5 and 89.9 with a Pearson correlation of 0.773.Therefore, the automatic reward seems to provide a reasonably close lower bound to the actual task performance.We find that for average workers, 87.0% of automatic scores are within a 10% of the manual score, with the main source of error being synonyms or lexically similar words that don't get matched correctly in the automatic reward function.</p>
<p>B Model Details B.1 Cross Attention Layer</p>
<p>Our cross attention layer follows Seo et al. [42].Denote the i-th contextualized token embedding from the observation and action to be o i and a i respectively.The attention between o i and a j is defined as
α ij = w 1 • o i + w 2 • a j + w 3 • (o i ⊗ a j )(7)
where ⊗ denotes element-wise product and w 1 , w 2 , w 3 are learnable vectors.The observationcontextualized vector for j-th action token is then
ca j = w 5 • leakyRELU(w 4 • [a j , c j , a j ⊗ c j , q ⊗ c j ])(8)c j = i exp(α ij ) • o i i exp(α ij ) , q = j exp(max i α ij )a j j exp(max i α ij )(9)
We then average pool all ca j to derive the action score S(o, a):
S(o, a) = w 6 • 1 n a j≤na ca j ∈ R(10)
where n a is the number of tokens for action a.</p>
<p>B.2 RNN Baseline</p>
<p>Our RNN baseline is inspired by Guo et al. [14], where we use the same attention layer as described above, but replace the Transformer text encoder with one-layer bi-directional Gated Recurrent Units (GRU) [9] of hidden dimension 512.Another difference is that we also add an cross attention between the instruction and action input word embeddings, as we hypothesize it might help option text matching.</p>
<p>C WebShop Experiment Details C.1 IL Training Details</p>
<p>The training code for our IL models is adapted from Huggingface glue training example, whose repository is licensed under Apache License 2.0.We use a training batch size of 1 with 32 gradient accumulation steps, a learning rate of 2 × 10 −5 , and 10 training epochs.The training takes around 2 hours on one RTX 2080 GPU with a GPU memory of around 10GB.</p>
<p>• Amazon URL → Amazon HTML → Amazon Page Information: Using ScraperAPI [35], we first get the HTML source code for a given Amazon page, then extract information relevant to rendering the equivalent page in the WebShop environment (e.g.title, price, options).).This logic is captured as a mapping of page type to permissible actions.• Text Observation, Valid Action Set → IL Model → Amazon URL: Given the text observation and allowed of valid actions, the IL model produces an action.This action is then used to construct a corresponding Amazon URL via a set of mapping rules, and the loop is repeated.This continues until the model generates a "buy now" action, terminating the loop.</p>
<p>D.2 Sim-to-real Transfer Results</p>
<p>The resulting numbers in Table 5 closely cohere to the reported numbers of WebShop found in Figure 4, suggesting that the WebShop has promise for developing grounded agents that can operate on real web environments.Between the two websites, transfer to Amazon is better than eBay as we note that (i) eBay has a larger product gap from WebShop, e.g.some item categories like food are disallowed in eBay.(ii) the eBay search engine seems weaker, and would sometimes display no results for lengthy instructions.The following Table 11 is an example of a trajectory generated by the IL agent searching on the real Amazon website.</p>
<p>Instruction: I want to find white blackout shades that are 66 inches in width and 66 inches in height.They need to be easy to install..We omit instructions and some human actions for instruction and trim item names for readability.Red denotes options and blue denotes attributes.</p>
<p>It is evident that the exploratory behavior and patterns learned and exhibited by the agent within the WebShop environment is not lost in this transfer.These results point to the opportunity for sim-to-real trained agents to transfer to other real-world web tasks despite the domain shift in both data (products) and dynamics (search engine) With that said, the gap between human and model performance also encourage us to look into expanding on the current limitations in our work regarding both the model and the WebShop environment.</p>
<p>E Potential Societal Impacts and Limitations</p>
<p>WebShop is designed to minimize human efforts in data collection and processing, but there are still potential concerns regarding diversity, fairness, and representation.Developing RL agents that interact with the web also bear safety concerns, especially when transferring from simulation to real-world websites.We also discuss other limitations regarding the semantics of current task (instruction/reward).</p>
<p>Diversity and representation in data collection.We chose five common categories from amazon.com and scrape all products using all subcategories to minimize bias.However, our data is still biased toward the website country (USA) and website language (English), and may only represent a subset of all possible products that users potentially want to buy.Having this limitation in mind, the design of WebShop allows the product data to be easily updated for different representations of real-world usage.</p>
<p>Bias in data processing.Currently our attribute labeling is manually done and may be biased by the labeller's own experience (e.g. more knowledge toward product attributes like sports rather than makeup).An more automatic alternative would be to employ trained NLP models (e.g.relation extraction) to extract product attributes, which might be less biased than one labeller.Our reward design is general and could be updated to weight more toward attributes, options, price, etc.</p>
<p>Safety for developing web agents.Unlike recent work [33] that directly employs agents on the World Wide Web (WWW), WebShop aims to provide a realistic simulation environment to train agents in a controllable and safe manner.In our preliminary sim-to-real experiments, the agent could only update the current webpage's url in two fixed and safe ways (i.e.search for results, open an item), and any form sending action (e.g.click options or buy) is held within the sim-to-real interface for later reward calculation.As a result, only navigation is done on the real-world website.For future deployment to real-world websites with more advanced functions, we believe a good specification of possible model behaviors is key to avoid harmful actions.</p>
<p>Limitations in the current task.Our current instructions are still limited by the attributes and options used.While attributes are simple and sometimes too generic (e.g."easy to use"), the options might get too specific (e.g."d17(dedicated right, back)").Therefore, an agent might sometimes use a special option as cues to find the product, while ignoring other parts of the instruction.To better leverage images and texts (including reviews written by human users, which are not used in current work) of products for more semantic and challenging instructions is an important future direction from WebShop.</p>
<p>Instruction for Human Trajectory Collection</p>
<p>The following pages display the human trajectory collection document mentioned in §A.6.</p>
<p>2 C
2
looking for a small portable folding desk that is already fully assembled [...] [btn] Back to Search [/btn] Page 1 (Total results: 50) [btn] Next [/btn] [btn] MENHG Folding Breakfast Tray [...] [/btn] $109.0 [btn] KPSP Folding Study Desk Bed [...] [/btn]&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " R 6 D A 6 k 7 Z Z 3 D M 9 q M H 4 v c J + q h P wx 0 = " &gt; A A A C U X i c d V H L T h s x F L 0 Z K I + h p Y E u 2 V h E k b q K Z h B q K 1 Y E N t 0 V J A J U S R R 5 n B t i 4 b F H 9 p 2 I a D R 8 I g u 6 4 j / Y s K C q 8 0 D i 1 S N Z O j r n P u z j J F P S U R T d V Y K F x Q 9 L y y u r 4 d r H T + u f q x u b p 8 7 k V m B L G G X s e c I d K q m x R Z I U n m c W e Z o o P E s u D y f + 2 Q i t k 0 a f 0 D j D b s o v t B x I w c l L v e q w 3 s l 1 H 2 1 i u c D i + n 8 o w 3 q H 8 I q K 5 o h L x f 1 0 1 h S T C W 6 v v G 4 + m b 8 S h 3 Y 0 n e x l E / 7 u z X S T U d m r 1 q J G N A V 7 S + I 5 q c E c R 7 3 q b a d v R J 6 i J q G 4 c + 0 4 y q h b c E t S K C z D T u 4 w 4 + K S X 2 D b U 8 1 T d N 1 i m k j J 6 l 7 p s 4 G x / m h i U / V 5 R 8 F T 5 8 Z p 4 i t T T k P 3 2 p u I 7 3 n t n A Y / u o X U W U 6 o x W z R I F e M D J v E y / r S o i A 1 9 o Q L K / 1 d m R h y H y 7 5 T w h 9 C P H r J 7 8 l p z u N + F s j P t 6 t 7 R / M 4 1 i B L d i G r x D D d 9 i H n 3 A E L R B w A / f w C H 8 r f y o P A Q T B r D S o z H u + w A s E a / 8 A B l O 4 g g = = &lt; / l a t e x i t &gt; Y opt &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 s B a I E W r R B 5 q t q 1 e F m 9 9 M V A E g I o = " &gt; A A A C X 3 i c d V H L T h s x F P U M 5 d H h N c C q Y m M 1 i s Q q m k G I V q w I 3 X R X K j U 8 l E T R H e c G L D z 2 y L 4 T E Y 2 G j 2 S H 1 E 3 / B O e B V K A 9 k q X j c + 6 9 t o + z Q k l H S f I U h E s f l l d W 1 z 5 G 6 x u b W 9 v x z u 6 F M 6 U V 2 B F G G X u V g U M l N X Z I k s K r w i L k m c L L 7 O7 b 1 L 8 c o 3 X S 6 F 8 0 K b C f w 4 2 W I y m A v D S I x 8 1 e q Y d o M w s C q 4 f / o Y 6 a P c J 7 q t p j k A r 8 d N 4 W 0 w n u p H 5 o v 5 g / M o d 2 P J v s Z R M 1 r w d z w x R U R y 8 b I K o H c S N p J T P w 9 y R d k A Z b 4 H w Q P / a G R p Q 5 a h I K n O u m S U H 9 C i x J o b C O e q X D A s Q d 3 G D X U w 0 5 u n 4 1 y 6 f m T a 8 M + c h Y v z T x m f p 3 R w W 5 c 5 M 8 8 5 U 5 0 K 1 7 6 0 3 F f 3 n d k k Z f + 5 X U R U m o x f y g U a k 4 G T 4 N m w + l R U F q 4 g k I K / 1 d u b g F H z X 5 L 4 l 8 C O n b J 7 8 n F 4 e t 9 L i V / j x q n J 4 t 4 l h j + + w z O 2 A p + 8 J O 2 X d 2 z j p M s N 9 B G K w H G 8 G f c D X c C u N 5 a R g s e v b Y K 4 S f n g H m Z r p b &lt; / l a t e x i t &gt; Y att &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I A J J T 9 H U e j w C j r R M 4 2 l i 3 G C W 1 e 0= " &gt; A A A C b 3 i c d V H B b h M x E P V u g Z a l p a E 9 c C h C V q N I 9 B L t I l S q n p J y 4 U a R S F u U R N G s M 2 m t e u 2 V P R s 1 W m 2 O / U B u / E M v / A F O s g h o 4 U m W 3 r z n G d v P a a 6 k o z j + H o R r j x 4 / W d 9 4 G j 3 b 3 H q + 3 X i x c + Z M Y Q X 2 h F H G X q T g U E m N P Z K k 8 C K 3 C F m q 8 D y 9 / r D w z 6 d o n T T 6 C 8 1 y H G Z w q e V E C i A v j R q 3 r U G h x 2 h T C w L L + f 9 Q R a 0 B 4 Q 2 V 3 S l I B X 4 6 7 4 r F B H d c z b u / z E + p Q z t d T v a y i V p f R y v D 5 F T 9 r o B 8 N a u L 3 E q B 1 a j R j N v x E v w h S W r S Z D V O R 4 1 v g 7 E R R Y a a h A L n + k m c 0 7 A E S 1 I o r K J B 4 T A H c Q 2 X 2 P d U Q 4 Z u W C 7 z q n j L K 2 M + M d Y v T X y p / t l R Q u b c L E v 9 z g z o y t 3 3 F u K / v H 5 B k 6 N h K X V e E G q x O m h S KE 6 G L 8 L n Y 2 l R k J p 5 A s J K f 1 c u r s B H T / 6 L I h 9 C c v / J D 8 n Z 2 3 Z y 2 E 4 + v 2 t 2 T u o 4 N t g e 2 2 d v W M L e s w 7 7 y E 5 Z j w l 2 F + w E e 8 G r 4 E f 4 M n w d 8 t X W M K h 7 d t l f C A 9 + A t L q w S 4 = &lt; / l a t e x i t &gt; y price &lt; l a t e x i t s h a 1 _ b a s e 6 4 = "</p>
<p>O T b b N d t g r l r J 3 b I 9 9 Z v t s x A T 7 F W w H L 4 I 4 + B 3 y 8 G X 4 e l 0 a B l 3 P F r s S 4 e 4 f 7 j j E u Q = = &lt; / l a t e x i t &gt; ȳ (Description): MENHG Folding Laptop Table Bed…: $109.0 (Options): { black, khaki, white } (Attributes): { steel pipe, no assembly, portable } (Instruction): I'm looking for a small portable… &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k 9</p>
<p>Figure 1 :
1
Figure 1: The WebShop environment.A: An example task trajectory in HTML mode, where a user can (1) search a query in a search page, (2) click a product item in a results page, (3) choose a color option in a item page, (4) check item-detail pages and go back to the item page, and (5) finally buy the product to end the episode and receive a reward r ∈ [0, 1] ( §3.2).B: the results page in simple mode for agent training and evaluation.The blue text indicates clickable actions and bold text indicates an action selected by the agent.C: The product notation used in §3 with corresponding examples from the product in A. The attributes Y att are hidden from the task performer.</p>
<p>Figure 2 :
2
Figure 2: Item rank in search results when the instruction is directly used as search query.</p>
<p>ResNet</p>
<p>Instruction: I want a small portable folding desk […] Title: MENHG Folding Laptop Table Bed Desk […] Color: [btn] black [/btn] […] choose[Features] choose[black] choose[khaki] choose[Buy Now] t e x i t s h a 1 _ b a s e 6 4 = " x l J e k I r k x 5 6 p w 7 7 u 0 o K T 7 Z w X x l o = " &gt; A A A C j n i c r V H L S g M x F M 2 M r 1 p f o y 7 d B I u g m z I j o q K</p>
<p>d 8 6 e m Z 2 b n K v P V h c W l 5 Z V g d e 3 W 6 s x w a H E t t b m P m Q U p F L R Q o I T 7 1 A B L Y g l 3 8 d P 5 2 L 8 b g r F C q x s c p d B N 2 I M S A 8 E Z O q k X v G 5 1 E J 4 x v 4 w t m O F E L I 6 o r p Z y c 8 i E Z K 4 Y b f K x Z 5 3 Z 3 N Y 7 1 U 6 m + m B i w z j k L / + E w n X 9 s 2 z R C 2 p h P Z y A / i Z R S W q k x F U v e O v 0 N c 8 S U M g l s 7 Y d h S l 2 c 2 Z Q c A m F G 9 9 C y v g T e 4 C 2 o 4 o l Y L v 5 Z J 0 F 3 X J K n w 6 0 c U 8 h n a h f M 3 K W W D t K Y h e Z M H y 0 P 7 2 x + J f X z n B w 2 M 2 F S j M E x T 8 b D T J J U d P x b W h f G O A o R 4 4 w b o S b l f J H 5 p a B 7 o J V t 4 T o 5 5 d / k 9 v d e r R f j 6 7 3 a o 2 z c h 0 V s k E 2 y T a J y A F p k A t y R V q E e w t e 5 B 1 5 x 3 7 g 7 / s n / u l n q O + V O e v k G / y L D / u c 0 U c = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " t c F T + C 2 v w W j B 8 E Q R 8 i y o 9 4 Y f / 2 U = " &gt; A A A C x H i c r Z H f a 9 s w E M d l 7 1 f n / W j W P e 5 F L B Q S G M E e o y u D Q r P B 2 N s 6 t r S F O I S z f H F F Z c m V z q H B p H / k 3 s b + m S m u H 7 Z 2 s J c d C L 5 8 v q f T 6 S 6 r l H Q U x z + C 8 M 7 d e / c f b D 2 M H j 1 + 8 n S 7 9 2 z n 2 J n a C p w I o 4 w 9 zc C h k h o n J E n h a W U R y k z h S X b + Y e O f L N E 6 a f Q 3 W l U 4 K 6 H Q c i E F k E f z 3 s / d l P C S m s + Z Q 7 t s 4 f o d N 1 G H x 0 u Q C n w x P h Y b z 3 l z P D B D 7 9 c 6 R 5 t Z E N h c / a d Y / 6 O s 9 w 9 4 q k z B K 5 5 m s h g A T 0 u Z c / O q b a p l w + i A f x 1 4 A s N 5 r x + P 4 j b 4 b Z F 0 o s + 6 O J r 3 v q e 5 E X W J m o Q C 5 6 Z J X N G s A U t S K F x H a e 2 w A n E O B U 6 9 1 F C i m z X t E t Z 8 1 5 O c L 4 z 1 R x N v 6 e 8 3 G i i d W 5 W Z z y y B z t x N b w P / 5 k 1 r W u z P G q m r m l C L 6 4 c W t e J k + G a j P J c W B a m V F y C s 9 L 1 y c Q Z + g u T 3 H v k h J D e / f F s c v x 4 l e 6 P k y 5 v + 4 f t u H F v s B X v J B i x h b 9 k h + 8 S O 2 I S J Y B w U Q R V c h B 9 D F b q w v k 4 N g + 7 O c / Z H h F e / AD B 4 4 3 s = &lt; / l a t e x i t &gt; = S(o, a) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G O y / b 9 s r u p I k 4 r J Q 3 + l B 9 O O i Q 6 Q = " &gt; A A A C D 3 i c d V B N S 8 N A E J 3 4 W e t X 1 K O X Y F E 8 l U R E P R a 9 e K x g P 6 A N Z b O Z t k s 3 m 7 C 7 E U q o v 8 C L f 8 W L B 0 W 8 e v X m v 3 H b 5 q C t P h h 4 v D f D z L w g 4 U x p 1 / 2 y F h a X l l d W C 2 v F 9 Y 3 N r W 1 7 Z 7 e u 4 l R S r N G Y x 7 I Z E I W c C a x p p j k 2 E 4 k k C j g 2 g s H V 2 G / c o V Q s F r d 6 m K A f k Z 5 g X U a J N l L H P m q n I k Q Z S E I x u / 8 P o 4 5 d c s v u B M 4 8 8 X J S g h z V j v 3 Z D m O a R i g 0 5 U S p l u c m 2 s + I 1 I x y H B X b qc K E 0 A H p Y c t Q Q S J U f j b 5 Z + Q c G i V 0 u r E 0 J b Q z U X 9 O Z C R Sa h g F p j M i u q 9 m v b H 4 l 9 d K d f f C z 5 h I U o 2 C T h d 1 U + 7 o 2 B m H 4 4 R M I t V 8 a A i h k p l b H d o n J h t t I i y a E L z Z l + d J / a T s n Z W 9 m 9 N S 5 T K P o w D 7 c A D H 4 M E 5 V O A a q l A D C g / w B C / w a j 1 a z 9 a b 9 T 5 t X b D y m T 3 4 B e v j G y b m n + o = &lt; / l a t e x i t &gt; | {z } &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " f v s / I s B H 7 2 M b p W 8 r 7 H V c g l R 4 G q Q = " &gt; A A A C E n i c d V D L S s N A F J 3 U V 4 2 v q E s 3 g 6 W g m 5 K I q M u i G 5 c V 7 A P a U C a T m 3 b o Z B J m J k I J 9 R f c + C t u X C j i 1 p U 7 / 8 b p Y 6 G t H r h w O O d e 7 r 0 n S D l T 2 n W / r M L S 8 s r q W n H d 3 t j c 2 t 5 x d v c a K s k k h T p N e C J b A V H A m Y C 6 Z p p D K 5 V A 4 o B D M x h c j f 3 m H U j F E n G r h y n 4 M e k J F j F K t J G 6 z n G 5 k 4 k Q Z C A J h f z + P 4 x s 0 n V K b s W d A C 8 S b 0 Z K a I Z a 1 / n s h A n N Y h C a c q J U 2 3 N T 7 e d E a k Y 5 j O x O p i A l d E B 6 0 D Z U k B i U n 0 9 e G u G y U U I c J d K U 0 H i i / p z I S a z U M A 5 M Z 0 x 0 X 8 1 7 Y / E v r 5 3 p 6 M L P m U g z D Y J O F 0 U Z x z r B 4 3 x w y C R Q z Y e G E C q Z u R X T P j H p a J O i b U L w 5 l 9 e J I 2 T i n d W 8 W 5 O S 9 X L W R x F d I A O 0 R H y 0 D m q o m t U Q 3 V E 0 Q N 6 Q i / o 1 X q 0 n q 0 3 6 3 3 a W r B m M / v o F 6 y P b 5 p u o J g = &lt; / l a t e x i t &gt; a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K K Q G / c q K D z D M u L q m G g H Y P l N 6 3 2 U = " &gt; A A A C R n i c d V B N S y N B F H y T 9 X P 8 i r t H L 4 0 h 4 C n M i K h 4 M n r R k w p G h W Q I P Z 0 X b e z p G b r f B M M Q / 5 w X z 9 7 8 C V 7 2 4 L L s d T s x g p 8 F D U X V e 9 X d F W d K W g q C R 6 / 0 Y 2 J y a n p m 1 p + b X 1 h c K i / / P L N p b g Q 2 R K p S c x F z i 0 p q b J A k h R e Z Q Z 7 E C s / j 6 / 2 h f 9 5 D Y 2 W q T 6 m f Y Z T w S y 2 7 U n B y U r s c V V u 5 7 q C J D R d Y 3 H 6 H g V 9 t E d 5 Q U e 9 x q b h L Z 3 U x T L A 7 g 9 v 6 q 3 k U W z S 9 U b K T U / + w X a 4 E t W A E 9 p m E Y 1 K B M Y 7 b 5 Y d W J x V 5 g p q E 4 t Y 2 w y C j q O C G p F A 4 8 F u 5 x Y y L a 3 6 J T U c 1 T 9 B G x a i G A a s 6 p c O 6 q X F H E x u p b z c K n l j b T 2 I 3 m X C 6 s h + 9 o f i V 1 8 y p u x 0 V U m c 5 o R Y v F 3 V z x S h l w 0 5 Z R x o U p P q O c G G k e y s T V 9 w 1 S q 5 5 3 5 U Q f v z y Z 3 K 2 X g s 3 a + H J R m V 3 b 1 z H D K z A K q x B C F u w C w d w D A 0 Q c A d P 8 A x / v H v v t / f X + / c y W v L G O 7 / g H U r w H 5 W e t T c = &lt; / l a t e x i t &gt; I &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 m E 2 E B 9 D S 9 Z 4 4</p>
<p>Figure 3 :
3
Figure 3: Architecture of our choice-based imitation learning (IL) model.The image I is passed to a ResNet to obtain the image representation.The instruction text u is passed to a transformer (initialized with BERT) to obtain the text representations.The concatenated bi-modal representations are fused with the action representations using the Attention Fusion Layer.The resulting fused-action representations are mean-pooled and reduced by an MLP layer to a scalar value S(o, a) denoting the logit value of the action choose[khaki].</p>
<p>Figure 4
4
also contains several ablations that confirm important design choices for models.When the choice action model for the IL agent is randomly initialized (IL (w/o LP Choice); LP = language-pretraining), the success rate drops by nearly two-thirds, indicating the importance of language pre-training for our task.When the search query generator in the IL agent is replaced by a simple rule, which always uses the instruction text (IL (w/o LP Search)), both reward and success rate drop by around 3 points.This suggests the importance to explore by expanding the search space</p>
<p>Figure 4 :
4
Figure 4: Task scores and Success Rate (%) for our models on the test split of WebShop over 3 trials.LP Search uses a pre-trained BART model to generate the search query and IL w/o LP Search uses the rule-based heuristic.LP Choice uses pre-trained BERT weights to initialize the choice action model and IL w/o LP Choice trains a Transformer from scratch.</p>
<p>Figure 5 :
5
Figure 5: The Amazon Mechanical Turk interface for the instruction writing task.The green box shows the general instruction for the task and the grey box shows an concrete example.</p>
<p>Figure 6 :
6
Figure 6: The Amazon Mechanical Turk interface for the instruction writing task.The blue box shows the actual annotation interface.The worker is required to check the boxes and write the instructions in the text field before submission.</p>
<p>Table 1 :
1
Actions in WebShop.
TypeArgumentState → Next Statesearch [Query]Search → Resultschoose Back to search  *  → Searchchoose Prev/Next page Results → Resultschoose [Product title]Results → Itemchoose [Option]Item → Itemchoose Desc/Overview Item → Item-Detailchoose PreviousItem-Detail → Itemchoose BuyItem → Episode End</p>
<p>Table 2 :
2
Left: Score breakdown.Right: average, maximum, and minimum number of states visited, items checks, and searches in a trajectory.</p>
<p>Table 3 :
3
Two example trajectories (showing only actions) from the human and the IL+RL model.We omit some human actions from instruction 2 for space and truncate the item names for readability.Red denotes options and blue denotes attributes.
Instr. text IL BART Human expert (first) Human expert (last)Score94.994.594.595.5Success Rate85.4%84.2%85.6%87.8%</p>
<p>Table 4 :
4
Task performance with the Choice oracle.first and last refer to the first and last search queries found in human demonstrations, respectively.</p>
<p>Table 5 :
5
Zero-shot sim-to-real transfer to Amazon and eBay over 100 test instructions.The Score / SR (Success Rate) column indicates the overall performance.The remaining breakdown are in Score.On amazon.com,IL+RL achieves a Score of 65.9 and SR of 25%, outperforming the Rule baseline's Score of 45.8 and SR of 19% by large margin.Similarly, on ebay.com,IL+RL achieves a Score of 62.3 and SR of 21%, widely outperforming the Rule baseline's Score of 31.7 and SR of 7%.
AmazoneBayScore / SRAttOpt Type PriceScore / SRAttOpt Type PriceRule45.8 / 19% 45.6 38.0 66.290.031.7 / 7% 62.3 25.9 49.067.0IL61.5 / 27% 60.7 53.7 85.696.058.2 / 21% 60.2 52.3 85.196.9IL+RL 65.9 / 25% 71.6 47.0 87.8 100.0 62.3 / 21% 69.1 39.5 91.797.0Human 88.2 / 65% 86.2 76.3 99.0 100.0 79.7 / 40% 80.3 70.1 99.5 100.0</p>
<ol>
<li>If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] In appendix.(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [Yes] Discussed in Appendix.(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?[Yes] Discussed in Appendix.</li>
</ol>
<p>Table 8 :
8
Reward Verification StatisticsTable8reflects our observation that our reward function is similar to a human's score, with a consistent tendency to over-penalize the picked product.For every trajectory's product, the human score across all categories (e.g.attributes, options) is always greater than or equal to the original score.This under-scoring is a result of our reward function's exact matching criterion.In future work, we hope to improve our matching functionality such that, within the context of a single product with respect to the goal instructions, it can identify synonyms and decide whether to award additional points.
MTurk Type Reward Function Price Type Attribute Result OverallAverageWebShop95.092.971.750.574.9Human95.093.875.357.076.3ExpertWebShop100.0 100.078.156.181.5Human100.0 100.088.266.889.9</p>
<p>• Amazon Page Information → WebShop HTML → Text Observation: Given the scraped information, we generate the corresponding WebShop page in HTML mode, then transform it into a simple mode text observation.• Amazon Page Information → Valid Action Set: From the scraped information, we determine what valid actions the model can take (i.e.search[Red shoes], choose[Size 9]</p>
<p>Table 11 :
11
search[white blackout shades 66 inches in width and 66 inches height, easy to install] click[item -Easy Up &amp; Down 100% Blackout Pleated Window Shades Temporary Window Blinds 36in x 64in (Fits Window Width 18"-36") 2pcs-Pack Operating with Pull Cord Easy Trimming &amp; Installing] click[features] click[back to search] search[white blackout shades that are 66 inches in width and 66 inches height] click[item -Redi Shade Inc 1617201 Original Blackout Pleated Paper Shade Black 36" x 72" 6-Pack] click[&lt; prev] click[Shade + Strips, White] click[buy] An example trajectory (showing only actions) from the IL agent on the real Amazon website.</p>
<ul>
<li>In our analysis ( §5.3), we observe that the task requires patience and consistency, which is lacking in some crowdsource workers, leading to lower scores. Even with this caveat, the gap between human performance and the model remains significant.
§  A similar search oracle is also possible but harder to design since the search space is infinite. One possible oracle is to search for the underlying product name for each instruction, but that makes choice trivial as the underlying product is then almost always the first search result.
AcknowledgementsWe thank Alexander Wettig, Ameet Deshpande, Austin Wang, Jens Tuyls, Jimmy Yang, Mengzhou Xia, Tianyu Gao, and Vishvak Murahari from the Princeton NLP Group for proofreading and providing comments.This material is based upon work supported by the National Science Foundation under Grant No. 2107048.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.(c) Did you include any new assets either in the supplemental material or as a URL?[Yes]In the supplementary materials.(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating?[Yes] Discussed in appendix, we only scrape publicly available data from the Internet.A Environment DetailsA.1 Product ScrapingWe use ScraperAPI[35]to extract publicly available product information from amazon.com.A.2 Product Attribute MiningWe use TfidfVectorizer from scikit-learn to extract probable bi-grams as attributes from product title and descriptions for further annotation.We manually inspect these attributes to keep only the specific and human-readable ones and filter out the rest.An attribute should be suitable in at least one of the following use: 1) IsGoodFor, 2) HasA (contains), 3) WhichIs, and 4) IsA.For example, attributes such as "oz ml" and "men women" will be filtered out since it's unparsable.On the other hand, "hair color" will also be filtered since it is not specific enough to fit in the above 4 categories.Attributes such as "dry skin" can fit the IsGoodFor in the context of a make-up product being good for dry skin.A.3 Search EngineEach time the agent performs a search, the top 50 items are retrieved and displayed across five search result pages, where each page contains 10 items and the agent can use actions choose[Prev/Next page] to navigate across result pages.Figure2shows that when searching directly with the instruction text, the corresponding item appears in the first search page (rank 1-10) nearly 1/3 of the time, but it cannot be found in any search pages (rank 50+) more than half of the time.This indicates that while the search engine can decently retrieve items based on lexical matching, directly searching the instruction is not enough for solving the task, and good query (re)formulation based on the instruction is important.A.4 Instruction CollectionWe collect human written instructions by providing the workers a product including the title, product category, and its set of attributes and options (Figure5, 6).We conduct qualification task by having each participating workers to work on 2 − 5 examples.We inspect and assign qualification to 213 workers to perform the instruction writing task.We pay for each example 0.15 dollars.We do not anticipate any potential participant risk.A.5 Reward CalculationThe type reward r type consists of 3 elements: 1) course-grain product category match (c = 1 if matched), 2) fine-grain category match (f = 1 if matched), and 3) product title match.Course-grain product category refers to the 5 categories described in §3.2.Fine-grain category is the chain of categories that the product is under on the Amazon website.For example, and eye mask sheet wouldC.2 RL Training DetailsWe train the RL models using 4 parallel environments for 100, 000 training steps.The backprogation through time (BPTT) is taken at every 8 steps.We use an Adam optimizer with a learning rate of 10 −5 (for Transformer models) or 5 × 10 −4 (for RNN models).For RL models with the Transformer (BERT) architecture, it takes around 27 hours on one RTX 3090 GPU with a GPU memory of around 20GB.For RL models with the GRU architecture, it takes around 20 hours on one RTX 2080 GPU with a GPU memory of around 10GB.To disentangle the effects of learning to search from choosing the right actions, we construct a Choice oracle that has access to the hidden reward function as well as hidden attributes and options underlying each product and instruction.§ Given a search query, theC.3 Sampling vs. Top-1We show comparisons between using beam search vs. top-1 for both the search model and the choice model in Table9.During testing, the search model uses beam search to generate top-5 search queries.We randomly and uniformly sample from the top-5 queries to increase search diversity in case of multiple searches.We conduct experiments to instead always use the top-1 search, which shows slight performance improvement (see table below), and we will include the result in the paper.The choice model has a fixed set of action candidates at each step (e.g.all available buttons), and we sample from the choice policy what action to take, as always taking the top action will lead to significantly detorior performances.C.4 Image AblationWe train 3 trials with different random seeds for both the IL model and the ablated IL model without images, with performances over 500 test cases(10).Removing image only slightly hurts the overall performance, but significantly reduces the variance.This is reasonable as our current instruction and reward setups only use textual information, and we believe future efforts to incorporate visual information into the task setup will better challenge models' visual understanding, and make pre-trained vision-language models such as CLIP more useful.D Sim-to-real Details D.1 Sim-to-real Transfer DetailsTo test how well our IL agent trained in WebShop performs on amazon.com(ebay.comsimilarly), we wrote a series of scripts that generally achieve two steps -translate a real Amazon URL into our IL model's input (text observation, set of valid actions) and map the model's output back to a real Amazon URL.The following procedure is repeated until the IL model generates a "buy now" action:The WebShop Task Thank you for taking part in this project!In this task, you need to buy a designated product given an instruction on our Amazon Shopping Game site.You will get a score in the end indicating how close you are.Please try to score as high as you can.If you find in some cases the scoring seems weird/unfair, please reach out.We will look into the cases.Please read the following instructions carefully before you start.Instructions1) Go to the home page.The instruction will immediately show up on the landing page.2) Given this instruction, please write a search query that would produce search results matching the description.Please do not copy-paste the entire instruction.We encourage you come up with more targeted queries, see the result, and search again if needed.Example:-Instruction: I need a 9.5 rubber soled hiking shoe made of lightweight vinyl acetate.-Bad query: (copy pasting) Essentially, you need to hack the search engine a little bit.Note that our search engine is limited.Tricks that work on Google Search such as adding quotation marks around the query wonʼt work.Click Search a er filling out the search bar like below.3) Upon clicking Search, you will be sent to a page of results.The below screenshot is an example of the results displayed from the example query in Step 2. Each page shows up to 10 results.Click the Next button to see more results.4)Click on any of the blue product title text (i.e."B092F97B24" in above screenshot) to see a product detail page, like the below.Guidelines for searching for matching products:• Some pages have Options (i.e.Size, Color in above screenshot).If the instructions contain such information, please select the corresponding options (even if the title / features / desc./ reviews may already contain such info).In most cases, if you find the options verbatim as in the instruction, youʼve likely found the right product.• Do not use the product image to determine whether the instructionʼs information matches the product.OPTIONS: NoneThe le hand is a better match because the productʼs title, features, description, and options reflect the instructionʼs information.While the right hand product appears to be a pair of blue ankle socks, because this information is not reflected in the text, we do not consider this a match.Therefore, feel free to use the product image as a reference when looking for matches, but keep in mind that the experiment weʼre running accounts for a textʼs5) Decide whether the product is a matchA match should • Contain all of the instructionʼs information in the product detail pageʼs text (i.e.title, description, feature, options) • Have options (if they exist), which correspond to the product info, be selected.A match does not account for• The product image• You think it is a match!→ Click the Buy Now button on the product detail page • You think it is not a match OR another product might be a better match… ○ Click on the Back button to go to the original list of search results (page 3).From here, repeat steps 3-4 until you find a product that matches best.○ Click on the Back to Search button.This will take you back to the search bar page (page 2).If you feel none of the results are good matches, try another search query.
L Adolphs, B Boerschinger, C Buck, M C Huebscher, M Ciaramita, L Espeholt, T Hofmann, Y Kilcher, arXiv:2109.00527Boosting Search Engines with Interactive Agents. 2021arXiv preprint</li>
</ul>
<p>Htlm: Hyper-text pre-training and prompting of language models. A Aghajanyan, D Okhonko, M Lewis, M Joshi, H Xu, G Ghosh, L Zettlemoyer, ArXiv, abs/2107.069552021</p>
<p>Task-oriented dialogue as dataflow synthesis. J Andreas, J Bufe, D Burkett, C Chen, J Clausman, J Crawford, K Crim, J Deloach, L Dorner, J Eisner, 2020Transactions of the Association for Computational Linguistics8</p>
<p>Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. E M Bender, A Koller, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>. G Brockman, V Cheung, L Pettersson, J Schneider, J Schulman, J Tang, W Zaremba, arXiv:1606.015402016OpenAI Gym. arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. P Budzianowski, T.-H Wen, B.-H Tseng, I Casanueva, S Ultes, O Ramadan, M Gašić, arXiv:1810.002782018arXiv preprint</p>
<p>Interactive Mobile App Navigation with Uncertain or Under-specified Natural Language Commands. A Burns, D Arsan, S Agrawal, R Kumar, K Saenko, B A Plummer, arXiv:2202.023122022arXiv preprint</p>
<p>Empirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, K Çaglar Gülçehre, Y Cho, Bengio, ArXiv, abs/1412.35552014</p>
<p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, ArXiv, abs/1810.048052019</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL-HLT (1). 2019</p>
<p>Go-explore: a new approach for hard-exploration problems. A Ecoffet, J Huizinga, J Lehman, K O Stanley, J Clune, arXiv:1901.109952019arXiv preprint</p>
<p>Generalization of reinforcement learners with working and episodic memory. M Fortunato, M Tan, R Faulkner, S Hansen, A Puigdomènech, G Badia, C Buttimore, J Z Deck, C Leibo, Blundell, Advances in neural information processing systems. 322019</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. X Guo, M Yu, Y Gao, C Gan, M Campbell, S Chang, arXiv:2010.023862020arXiv preprint</p>
<p>. I Gur, U Rueckert, A Faust, D Hakkani-Tur, arXiv:1812.091952018arXiv preprintLearning to Navigate the Web</p>
<p>Adversarial Environment Generation for Learning to Navigate the Web. I Gur, N Jaques, K Malta, M Tiwari, H Lee, A Faust, arXiv:2103.019912021arXiv preprint</p>
<p>Interactive fiction games: A colossal adventure. M Hausknecht, P Ammanabrolu, M.-A Côté, X Yuan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202034</p>
<p>Deep Residual Learning for Image Recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2016. 2016</p>
<p>A Hotti, R S Risuleo, S Magureanu, A Moradi, J Lagergren, arXiv:2111.02168The Klarna Product Page Dataset: A RealisticBenchmark for Web Representation Learning. 2021arXiv preprint</p>
<p>A data-driven approach for learning to control computers. P C Humphreys, D Raposo, T Pohlen, G Thornton, R Chhaparia, A Muldal, J Abramson, P Georgiev, A Goldin, A Santoro, arXiv:2202.081372022arXiv preprint</p>
<p>Dom-q-net: Grounded RL on Structured Language. S Jia, J Kiros, J Ba, arXiv:1902.072572019arXiv preprint</p>
<p>Internet-augmented dialogue generation. M Komeili, K Shuster, J Weston, arXiv:2107.075662021arXiv preprint</p>
<p>Towards mental time travel: a hierarchical memory for reinforcement learning agents. A Lampinen, S Chan, A Banino, F Hill, Advances in Neural Information Processing Systems. 202134</p>
<p>Multi-agent Communication meets Natural Language: Synergies between Functional and Structural Language Learning. A Lazaridou, A Potapenko, O Tieleman, ACL. 2020</p>
<p>Internet-augmented language models through few-shot prompting for open-domain question answering. A Lazaridou, E Gribovskaya, W Stokowiec, N Grigorev, ArXiv, abs/2203.051152022</p>
<p>M Lewis, Y Liu, N Goyal, M Ghazvininejad, A Mohamed, O Levy, V Stoyanov, L Zettlemoyer, arXiv:1910.13461BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. 2019arXiv preprint</p>
<p>Oscar: Object-semantics aligned pre-training for vision-language tasks. X Li, X Yin, C Li, P Zhang, X Hu, L Zhang, L Wang, H Hu, L Dong, F Wei, European Conference on Computer Vision. Springer2020</p>
<p>J Lin, X Ma, S.-C Lin, J.-H Yang, R Pradeep, R Nogueira, arXiv:2102.10073Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR Research with Sparse and Dense Representationss. 2021arXiv preprint</p>
<p>E Z Liu, K Guu, P Pasupat, T Shi, P Liang, arXiv:1802.08802Reinforcement Learning on Web Interfaces using Workflow-Guided Exploration. 2018arXiv preprint</p>
<p>A survey of reinforcement learning informed by natural language. J Luketina, N Nardelli, G Farquhar, J N Foerster, J Andreas, E Grefenstette, S Whiteson, T Rocktäschel, IJCAI. 2019</p>
<p>FLIN: A Flexible Natural Language Interface for Web Navigation. S Mazumder, O Riva, arXiv:2010.128442020arXiv preprint</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, International conference on machine learning. PMLR2016</p>
<p>R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.09332Browser-Assisted Question-Answering with Human Feedback. 2021arXiv preprint</p>
<p>Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning. K Narasimhan, A Yala, R Barzilay, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language Processing2016</p>
<p>. D Ni, Scraperapi, 2015</p>
<p>End-to-End Goal-Driven Web Navigation. R Nogueira, K Cho, Advances in Neural Information Processing Systems. 292016</p>
<p>Task-Oriented Query Reformulation with Reinforcement Learning. R Nogueira, K Cho, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>Mapping natural language commands to web elements. P Pasupat, T.-S Jiang, E Z Liu, K Guu, P Liang, Empirical Methods in Natural Language Processing (EMNLP). 2018</p>
<p>Mapping natural language commands to web elements. P Pasupat, T.-S Jiang, E Z Liu, K Guu, P Liang, EMNLP. 2018</p>
<p>Curiosity-driven exploration by selfsupervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International conference on machine learning. PMLR2017</p>
<p>A Ronacher, Flask API. 2010</p>
<p>Bidirectional attention flow for machine comprehension. M Seo, A Kembhavi, A Farhadi, H Hajishirzi, arXiv:1611.016032016arXiv preprint</p>
<p>World of Bits: An Open-Domain platform for web-based agents. T Shi, A Karpathy, L Fan, J Hernandez, P Liang, International Conference on Machine Learning. PMLR2017</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Language models that seek for knowledge: Modular search &amp; generation for dialogue and prompt completion. K Shuster, M Komeili, L Adolphs, S Roller, A D Szlam, J Weston, ArXiv, abs/2203.132242022</p>
<p>Building Natural Language Interfaces to Web APIs. Y Su, A H Awadallah, M Khabsa, P Pantel, M Gamon, M Encarnacion, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. the 2017 ACM on Conference on Information and Knowledge Management2017</p>
<p>Natural Language Interfaces with Fine-Grained User Interaction: A Case Study on Web APIs. Y Su, A Hassan Awadallah, M Wang, R W White, The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval. 2018</p>
<p>D Toyama, P Hamel, A Gergely, G Comanici, A Glaese, Z Ahmed, T Jackson, S Mourad, D Precup, arXiv:2105.13231AndroidEnv: A Reinforcement Learning Platform for Android. 2021arXiv preprint</p>
<p>J Tuyls, S Yao, S Kakade, K Narasimhan, arXiv:2201.01251Multi-stage episodic control for strategic exploration in text games. 2022arXiv preprint</p>
<p>V Uc-Cetina, N Navarro-Guerrero, A Martin-Gonzalez, C Weber, S Wermter, arXiv:2104.05565Survey on reinforcement learning for language processing. 2021arXiv preprint</p>
<p>Deep reinforced query reformulation for information retrieval. X Wang, C Macdonald, I Ounis, arXiv:2007.079872020arXiv preprint</p>
<p>Z Wang, J Yu, A W Yu, Z Dai, Y Tsvetkov, Y Cao, arXiv:2108.10904Simvlm: Simple visual language model pretraining with weak supervision. 2021arXiv preprint</p>
<p>Unsupervised predictive memory in a goal-directed agent. G Wayne, C.-C Hung, D Amos, M Mirza, A Ahuja, A Grabska-Barwinska, J Rae, P Mirowski, J Z Leibo, A Santoro, arXiv:1803.107602018arXiv preprint</p>
<p>Automatic Task Completion Flows from Web APIs. K Williams, S H Hashemi, I Zitouni, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval. the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval2019</p>
<p>Keep CALM and Explore: Language Models for Action Generation in Text-based Games. S Yao, R Rao, M J Hausknecht, K Narasimhan, ArXiv, abs/2010.029032020</p>
<p>Reading and acting while blindfolded: The need for semantics in text game agents. S Yao, K Narasimhan, M Hausknecht, arXiv:2103.135522021arXiv preprint</p>
<p>Interactive machine comprehension with information seeking agents. X Yuan, J Fu, M.-A Côté, Y Tay, C J Pal, A Trischler, ACL. 2020</p>
<p>Silg: The multidomain symbolic interactive language grounding benchmark. V Zhong, A W Hanjie, S Wang, K Narasimhan, L Zettlemoyer, Advances in Neural Information Processing Systems. 202134</p>
<p>Bridging the gap between indexing and retrieval for differentiable search index with query generation. S Zhuang, H Ren, L Shou, J Pei, M Gong, G Zuccon, D Jiang, arXiv:2206.101282022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
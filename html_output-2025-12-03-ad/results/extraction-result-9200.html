<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9200 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9200</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9200</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-162.html">extraction-schema-162</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or tabular data, including details of the models, data types, methods, results, comparisons, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-246867391</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2202.08088v3.pdf" target="_blank">Latent Outlier Exposure for Anomaly Detection with Contaminated Data</a></p>
                <p><strong>Paper Abstract:</strong> Anomaly detection aims at identifying data points that show systematic deviations from the majority of data in an unlabeled dataset. A common assumption is that clean training data (free of anomalies) is available, which is often violated in practice. We propose a strategy for training an anomaly detector in the presence of unlabeled anomalies that is compatible with a broad class of models. The idea is to jointly infer binary labels to each datum (normal vs. anomalous) while updating the model parameters. Inspired by outlier exposure (Hendrycks et al., 2018) that considers synthetically created, labeled anomalies, we thereby use a combination of two losses that share parameters: one for the normal and one for the anomalous data. We then iteratively proceed with block coordinate updates on the parameters and the most likely (latent) labels. Our experiments with several backbone models on three image datasets, 30 tabular data sets, and a video anomaly detection benchmark showed consistent and significant improvements over the baselines.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9200",
    "paper_id": "paper-246867391",
    "extraction_schema_id": "extraction-schema-162",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.004680999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Latent Outlier Exposure for Anomaly Detection with Contaminated Data</p>
<p>Chen Qiu 
Aodong Li 
Marius Kloft 
Maja Rudolph 
Stephan Mandt 
Latent Outlier Exposure for Anomaly Detection with Contaminated Data</p>
<p>Anomaly detection aims at identifying data points that show systematic deviations from the majority of data in an unlabeled dataset. A common assumption is that clean training data (free of anomalies) is available, which is often violated in practice. We propose a strategy for training an anomaly detector in the presence of unlabeled anomalies that is compatible with a broad class of models. The idea is to jointly infer binary labels to each datum (normal vs. anomalous) while updating the model parameters. Inspired by outlier exposure (Hendrycks et al., 2018) that considers synthetically created, labeled anomalies, we thereby use a combination of two losses that share parameters: one for the normal and one for the anomalous data. We then iteratively proceed with block coordinate updates on the parameters and the most likely (latent) labels. Our experiments with several backbone models on three image datasets, 30 tabular data sets, and a video anomaly detection benchmark showed consistent and significant improvements over the baselines.</p>
<p>Introduction</p>
<p>From industrial fault detection to medical image analysis or financial fraud prevention: Anomaly detection-the task of automatically identifying anomalous data instances without being explicitly taught how anomalies may look like-is critical in industrial and technological applications.</p>
<p>The common approach in deep anomaly detection is to first train a neural network on a large dataset of "normal" samples minimizing some loss function (such as a deep oneclass classifier (Ruff et al., 2018)) and to then construct an anomaly score from the output of the neural network (typi-* Equal contribution 1 Bosch Center for Artificial Intelligence 2 TU Kaiserslautern, Germany 3 UC Irvine, USA. Correspondence to: Chen Qiu <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#99;&#104;&#101;&#110;&#46;&#113;&#105;&#117;&#64;&#100;&#101;&#46;&#98;&#111;&#115;&#99;&#104;&#46;&#99;&#111;&#109;">&#99;&#104;&#101;&#110;&#46;&#113;&#105;&#117;&#64;&#100;&#101;&#46;&#98;&#111;&#115;&#99;&#104;&#46;&#99;&#111;&#109;</a>, Stephan Mandt <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#109;&#97;&#110;&#100;&#116;&#64;&#117;&#99;&#105;&#46;&#101;&#100;&#117;">&#109;&#97;&#110;&#100;&#116;&#64;&#117;&#99;&#105;&#46;&#101;&#100;&#117;</a>.</p>
<p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). cally based on the training loss). Anomalies are then identified as data points with larger-than-usual anomaly scores and obtained by thresholding the score at particular values.</p>
<p>A standard assumption in this approach is that clean training data are available to teach the model what "normal" samples look like (Ruff et al., 2021). In reality, this assumption is often violated: datasets are frequently large and uncurated and may already contain some of the anomalies one is hoping to find. For example, a dataset of medical images may already contain cancer images, or datasets of financial transactions could already contain unnoticed fraudulent activity. Naively training an unsupervised anomaly detector on such data may suffer from degraded performance.</p>
<p>In this paper, we introduce a new unsupervised approach to training anomaly detectors on a corrupted dataset. Our approach uses a combination of two coupled losses to extract learning signals from both normal and anomalous data. We stress that these losses do not necessarily have a probabilistic interpretation; rather, many recently proposed selfsupervised auxiliary losses can be used (Ruff et al., 2018;Hendrycks et al., 2019;Qiu et al., 2021;Shenkar &amp; Wolf, 2022). In order to decide which of the two loss functions to activate for a given datum (normal vs. abnormal), we use a binary latent variable that we jointly infer while updating the model parameters. Training the model thus results in a joint optimization problem over continuous model parameters and binary variables that we solve using alternating updates. During testing, we can use threshold only one of the two loss functions to identify anomalies in constant time.</p>
<p>Our approach can be applied to a variety of anomaly detection loss functions and data types, as we demonstrate on tabular, image, and video data. Beyond detection of entire anomalous images, we also consider the problem of anomaly segmentation which is concerned with finding anomalous regions within an image. Compared to established baselines that either ignore the anomalies or try to iteratively remove them (Yoon et al., 2021), our approach yields significant performance improvements in all cases.</p>
<p>The paper is structured as follows. In Section 2, we discuss related work. In Section 3, we introduce our main algorithm, including the involved losses and optimization procedure. Finally, in Section 4, we discuss experiments on both image arXiv:2202.08088v3 [cs.LG] 26 Jun 2022 and tabular data and discuss our findings in Section 5 1 .</p>
<p>Related Work</p>
<p>We divide our related work into methods for deep anomaly detection, learning on incomplete or contaminated data, and training anomaly detectors on contaminated data.</p>
<p>Deep anomaly detection. Deep learning has played an important role in recent advances in anomaly detection. For example, Ruff et al. (2018) have improved the anomaly detection accuracy of one-class classification (Schölkopf et al., 2001) by combining it with a deep feature extractor, both in the unsupervised and the semi-supervised setting (Ruff et al., 2019). An alternative strategy to combine deep learning with one-class approaches is to train a one-class SVM on pretrained self-supervised features (Sohn et al., 2020). Indeed, self-supervised learning has influenced deep anomaly detection in a number of ways: The self-supervised criterion for training a deep feature extractor can be used directly to score anomalies (Golan &amp; El-Yaniv, 2018;Bergman &amp; Hoshen, 2020). Using a multi-head RotNet (MHRot), Hendrycks et al. (2019) improve self-supervised anomaly detection by solving multiple classification tasks. For general data types beyond images, anomaly detection using neural transformations (NTL) (Qiu et al., 2021;2022) learns the transformations for the self-supervision task and achieves solid detection accuracy. Schneider et al. (2022) combine NTL with representation learning for detecting anomalies within time series. On tabular data, anomaly detection with internal contrastive learning (ICL) (Shenkar &amp; Wolf, 2022) learns feature relations as a self-supervised learning task. Other classes of deep anomaly detection includes autoencoder variants (Principi et al., 2017;Zhou &amp; Paffenroth, 2017;Chen &amp; Konukoglu, 2018) and density-based models (Schlegl et al., 2017;Deecke et al., 2018).</p>
<p>All these approaches assume a training dataset of "normal" data. However, in many practical scenarios there will be unlabeled anomalies hidden in the training data. Wang et al. (2019);Huyan et al. (2021) have shown that anomaly detection accuracy deteriorates when the training set is contaminated. Our work provides a training strategy to deal with contamination.</p>
<p>Anomaly Detection on contaminated training data. A common strategy to deal with contaminated training data is to hope that the contamination ratio is low and that the anomaly detection method will exercise inlier priority (Wang et al., 2019). Throughout our paper, we refer to the strategy of blindly training an anomaly detector as if the training data was clean as " Blind" training. Yoon et al. (2021) have proposed a data refinement strategy that removes potential anomalies from the training data. Their approach, which we refer to as "Refine", employs an ensemble of one-class classifiers to iteratively weed out anomalies and then to continue training on the refined dataset. Similar data refinement strategy are also combined with latent SVDD (Görnitz et al., 2014) or autoencoders for anomaly detection (Xia et al., 2015;Beggel et al., 2019). However, these methods fail to exploit the insight of outlier exposure (Hendrycks et al., 2018) that anomalies provide a valuable training signal. Zhou &amp; Paffenroth (2017) used a robust autoencoder for identifying anomalous training data points, but their approach requires training a new model for identifying anomalies, which is impractical in most setups. Hendrycks et al. (2018) propose to artificially contaminate the training data with samples from a related domain which can then be considered anomalies. While outlier exposure assumes labeled anomalies, our work aims at exploiting unlabeled anomalies in the training data. Notably, Pang et al. (2020) have used an iterative scheme to detect abnormal frames in video clips, and Feng et al. (2021) extend it to supervised video anomaly detection. Our work is more general and provides a principled way to improve the training strategy of all approaches mentioned in the paragraph "deep anomaly detection" when the training data is likely contaminated.</p>
<p>Method</p>
<p>We will start by describing the mathematical foundations of our method. We will then describe our learning algorithm as a block coordinate descent algorithm, providing a theoretical convergence guarantee. Finally, we describe how our approach is applicable in the context of various state-of-the-art deep anomaly detection methods.</p>
<p>Problem Formulation</p>
<p>Setup. In this paper, we study the problem of unsupervised (or self-supervised) anomaly detection. We consider a data set of samples x i ; these could either come from a data distribution of "normal" samples, or could otherwise come from an unknown corruption process and thus be considered as "anomalies". For each datum x i , let y i = 0 if the datum is normal, and y i = 1 if it is anomalous. We assume that these binary labels are unobserved, both in our training and test sets, and have to be inferred from the data.</p>
<p>In contrast to most anomaly detection setups, we assume that our dataset is corrupted by anomalies. That means, we assume that a fraction (1−α) of the data is normal, while its complementary fraction α is anomalous. This corresponds to a more challenging (but arguably more realistic) anomaly detection setup since the training data cannot be assumed to be normal. We treat the assumed contamination ratio α as a hyperparameter in our approach and denote α 0 as the ground truth contamination ratio where needed. Note that an assumed contamination ratio is a common hyperparameter in many robust algorithms (e.g., Huber, 1992;2011), and we test the robustness of our approach w.r.t. this parameter in Section 4.</p>
<p>Our goal is to train a (deep) anomaly detection classifier on such corrupted data based on self-supervised or unsupervised training paradigms (see related work). The challenge thereby is to simultaneously infer the binary labels y i during training while optimally exploiting this information for training an anomaly detection model.</p>
<p>Proposed Approach. We consider two losses. Similar to most work on deep anomaly detection, we consider a loss function L θ n (x) ≡ L n (f θ (x)) that we aim to minimize over "normal" data. The function f θ (x) is used to extract features from x, typically based on a self-supervised auxiliary task, see Section 3.4 for examples. When being trained on only normal data, the trained loss will yield lower values for normal than for anomalous data so that it can be used to construct an anomaly score.</p>
<p>In addition, we also consider a second loss for anomalies L θ a (x) ≡ L a (f θ (x)) (the feature extractor f θ (x) is shared). Minimizing this loss on only anomalous data will result in low loss values for anomalies and larger values for normal data. The anomaly loss is designed to have opposite effects as the loss function L θ n (x). For example, if L θ n (x) = ||f θ (x)−c|| 2 as in Deep SVDD (Ruff et al., 2018) (thus pulling normal data points towards their center), we define L θ a (x) = 1/||f θ (x) − c|| 2 (pushing abnormal data away from it) as in (Ruff et al., 2019).</p>
<p>Temporarily assuming that all assignment variables y were known, consider the joint loss function,
L(θ, y) = N i=1 (1 − y i )L θ n (x i ) + y i L θ a (x i ).(1)
This equation resembles the log-likelihood of a probabilistic mixture model, but note that L θ n (x i ) and L θ a (x i ) are not necessarily data log-likelihoods; rather, self-supervised auxiliary losses can be used and often perform better in practice (Ruff et al., 2018;Qiu et al., 2021;Nalisnick et al., 2018).</p>
<p>Optimizing Eq. 1 over its parameters θ yields a better anomaly detector than L θ n trained in isolation. By construction of the anomaly loss L θ a , the known anomalies provide an additional training signal to L θ n : due to parameter sharing, the labeled anomalies teach L θ n where not to expect normal data in feature space. This is the basic idea of Outlier Exposure (Hendrycks et al., 2018), which constructs artificial labeled anomalies for enhanced detection performance.</p>
<p>Different from Outlier Exposure, we assume that the set of y i is unobserved, hence latent. We therefore term our approach of jointly inferring latent assignment variables y and learning parameters θ as Latent Outlier Exposure (LOE). We show that it leads to competitive performance on training data corrupted by outliers.</p>
<p>Optimization problem</p>
<p>"Hard" Latent Outlier Exposure (LOE H ). In LOE, we seek to both optimize both losses' shared parameters θ while also optimizing the most likely assignment variables y i . Due to our assumption of having a fixed rate of anomalies α in the training data, we introduce a constrained set:
Y = {y ∈ {0, 1} N : N i=1 y i = αN }.(2)
The set describes a "hard" label assignment; hence the name "Hard LOE", which is the default version of our approach. Section 3.3 describes an extension with "soft" label assignments. Note that we require αN to be an integer.</p>
<p>Since our goal is to use the losses L θ n and L θ a to identify and score anomalies, we seek L θ n (x i ) − L θ a (x i ) to be large for anomalies, and L θ a (x i ) − L θ n (x i ) to be large for normal data. Assuming these losses to be optimized over θ, our best guess to identify anomalies is to minimize Eq. (1) over the assignment variables y. Combining this with the constraint (Eq. (2)) yields the following minimization problem: min θ min y∈Y L(θ, y).</p>
<p>(</p>
<p>As follows, we describe an efficient optimization procedure for the constraint optimization problem.</p>
<p>Block coordinate descent. The constraint discrete optimization problem has an elegant solution.</p>
<p>To this end, we consider a sequence of parameters θ t and labels y t and proceed with alternating updates. To update θ, we simply fix y t and minimize L(θ, y t ) over θ. In practice, we perform a single gradient step (or stochastic gradient step, see below), yielding a partial update.</p>
<p>To update y given θ t , we minimize the same function subject to the constraint (Eq. (2)). To this end, we define training anomaly scores,
S train i = L θ n (x i ) − L θ a (x i ).(4)
These scores quantify the effect of y i on minimizing Eq. (1). We rank these scores and assign the (1 − α)-quantile of the associated labels y i to the value 0, and the remainder to the value 1. This minimizes the loss function subject to the label constraint. We discuss the sensitivity of our approach to the assumed rate of anomalies α in our experiments section. We stress that our testing anomaly scores will be different (see Section 3.3).</p>
<p>Algorithm 1 Training process of LOE Input: Contaminated training set D (α 0 anomaly rate) hyperparamter α Model: Deep anomaly detector with parameters θ foreach Epoch do foreach Mini-batch M do Calculate the anomaly score S train i for x i ∈ M Estimate the label y i given S train i and α Update the parameters θ by minimizing L(θ, y) end end</p>
<p>Assuming that all involved losses are bounded from below, the block coordinate descent converges to a local optimum since every update improves the loss.</p>
<p>Stochastic optimization. In practice, we perform stochastic gradient descent on Eq. (1) based on mini-batches. For simplicity and memory efficiency, we impose the label constraint Eq. (2) on each mini-batch and optimize θ and y in the same alternating fashion. The induced bias vanishes for large mini-batches. In practice, we found that this approach leads to satisfying results 2 .</p>
<p>Algorithm 1 summarizes our approach.</p>
<p>Model extension and anomaly detection</p>
<p>We first discuss an important extension of our approach and then discuss its usage in anomaly detection.</p>
<p>"Soft" Latent Outlier Exposure (LOE S ). In practice, the block coordinate descent procedure can be overconfident in assigning y, leading to suboptimal training. To overcome this problem, we also propose a soft anomaly scoring approach that we term Soft LOE. Soft LOE is very simply implemented by a modified constraint set:
Y = {y ∈ {0, 0.5} N : N i=1 y i = 0.5αN }.(5)
Everything else about the model's training and testing scheme remains the same.</p>
<p>The consequence of an identified anomaly y i = 0.5 is that we minimize an equal combination of both losses, 0.5(L θ n (x i ) + L θ a (x i )). The interpretation is that the algorithm is uncertain about whether to treat x i as a normal or anomalous data point and treats both cases as equally likely. A similar weighting scheme has been proposed for supervised learning in the presence of unlabeled examples 2 Note that an exact mini-batch version of the optimization problem in Eq. (3) would also be possible, requiring memorization of y for the whole data set. (Lee &amp; Liu, 2003). In practice, we found the soft scheme to sometimes outperform the hard one (see Section 4).</p>
<p>Anomaly Detection. In order to use our approach for finding anomalies in a test set, we could in principle proceed as we did during training and infer the most likely labels as described in Section 3.2. However, in practice we may not want to assume to encounter the same kinds of anomalies that we encountered during training. Hence, we refrain from using L θ a during testing and score anomalies using only L θ n . Note that due to parameter sharing, training L θ a jointly with L θ n has already led to the desired information transfer between both losses.</p>
<p>Testing is the same for both "soft" LOE (Section 3.2) and "hard" LOE (Section 3.3). We define our testing anomaly score in terms of the "normal" loss function,
S test i = L θ n (x i ).(6)</p>
<p>Example loss functions</p>
<p>As follows, we review several loss functions that are compatible with our approach. We consider three advanced classes of self-supervised anomaly detection methods. These meth- . While no longer being considered as a competitive baseline, we also consider deep SVDD for visualization due to its simplicity.</p>
<p>Multi-Head RotNet (MHRot). MHRot (Hendrycks et al., 2019) learns a multi-head classifier f θ to predict the applied image transformations including rotation, horizontal shift, and vertical shift. We denote K combined transformations as {T 1 , ..., T K }. The classifier has three softmax heads, each for a classification task l, modeling the prediction distribution of a transformed image p l (·|f θ , T k (x)) (or p l k (·|x) for brevity). Aiming to predict the correct transformations for normal samples, we maximize the log-likelihoods of the ground truth label t l k for each transformation and each head; for anomalies, we make the predictions evenly distributed by minimizing the cross-entropy from a uniform distribution U to the prediction distribution, resulting in
L θ n (x) := − K k=1 3 l=1 log p l k (t l k |x), L θ a (x) := K k=1 3 l=1 CE(U, p l k (·|x))
Neural Transformation Learning (NTL). Rather than using hand-crafted transformations, NTL learns K neural transformations {T θ,1 , ..., T θ,K } and an encoder f θ parameterized by θ from data and uses the learned transformations to detect anomalies. Each neural transformation generates a view x k = T θ,k (x) of sample x. For normal samples, NTL encourages each transformation to be similar to the original sample and to be dissimilar from other transformations.</p>
<p>To achieve this objective, NTL maximizes the normalized probability
p k = h(x k , x) h(x k , x) + l =k h(x k , x l ) for each view where h(a, b) = exp(cos(f θ (a), f θ (b))/τ )
measures the similarity of two views 3 . For anomalies, we "flip" the objective for normal samples: the model instead pulls the transformations close to each other and pushes them away from the original view, resulting in
L θ n (x) := − K k=1 log p k , L θ a (x) := − K k=1 log(1 − p k ).
Internal Contrastive Learning (ICL). ICL is a state-ofthe-art tabular anomaly detection method (Shenkar &amp; Wolf, 2022). Assuming that the relations between a subset of the features (table columns) and the complementary subset are class-dependent, ICL is able to learn an anomaly detector by discovering the feature relations for a specific class. With this in mind, ICL learns to maximize the mutual information between the two complementary feature subsets, a(x) and b(x), in the embedding space. The maximization of the mutual information is equivalent to minimizing a con-
trastive loss L θ n (x) := − K k=1 log p k on normal samples with p k = h(a k (x), b k (x)) K l=1 h(a l (x), b k (x)) where h(a, b) = exp(cos(f θ (a), g θ (b)
)/τ ) measures the similarity of two feature subsets in the embedding space of two encoders f θ and g θ . For anomalies, we flip the objective as
L θ a (x) := − K k=1 log(1 − p k ).</p>
<p>Experiments</p>
<p>We evaluate our proposed methods and baselines for unsupervised anomaly detection tasks on different data types: synthetic data, tabular data, images, and videos. The data are contaminated with different anomaly ratios. Depending on the data, we study our method in combination with specific backbone models. MHRot applies only to images and ICL to tabular data. NTL can be applied to all data types.</p>
<p>We have conducted extensive experiments on image, tabular, and video data. For instance, we evaluate our methods on all 30 tabular datasets of Shenkar &amp; Wolf (2022). Our proposed method sets a new state-of-the-art on most datasets. In particular, we show that our method gives robust results even when the contamination ratio is unknown.</p>
<p>Toy Example</p>
<p>We first analyze the methods in a controlled setup on a synthetic data set. For the sake of visualization, we created a 2D contaminated data set with a three-component Gaussian mixture. One larger component is used to generate normal samples, while the two smaller components are used to generate the anomalies contaminating the data (see Fig. 1). 3 where τ is the temperature and cos(a, b)
:= a T b/ a b
For simplicity, the backbone anomaly detector is the deep one-class classifier (Ruff et al., 2018) with radial basis functions. Setting the contamination ratio to α 0 = α = 0.1, we compare the baselines "Blind" and "Refine" (described in Section 2, detailed in Appendix B) with the proposed LOE H and LOE S (described in Section 3) and the theoretically optimal G-truth method (which uses the ground truth labels). We defer all further training details to Appendix A. Fig. 1 shows the results (anomaly-score contour lines after training). With more latent anomaly information exploited from (a) to (e), the contour lines become increasingly accurate. While (a) "Blind" erroneously treats all anomalies as normal, (b) "Refine" improves by filtering out some anomalies. (c) LOE S and (d) LOE H use the anomalies, resulting in a clear separation of anomalies and normal data. LOE H leads to more pronounced boundaries than LOE S , but it is at risk of overfitting, especially when normal samples are incorrectly detected as anomalies (see our experiments below). A supervised model with ground-truth labels ("Gtruth") approximately recovers the true contours.</p>
<p>Experiments on Image Data</p>
<p>Anomaly detection on images is especially far developed. We demonstrate LOE's benefits when applied to two leading image anomaly detectors as backbone models:  (2021)). Further implementation details of NTL are in the Appendix C.</p>
<p>Many existing baselines apply either blind updates or a refinement strategy to specific backbone models (see Sec- tion 2). However, a recent study showed that many of the classical anomaly detection methods such as autoencoders are no longer on par with modern self-supervised approaches (Alvarez et al., 2022;Hendrycks et al., 2019) and in particular found NTL to perform best among 13 considered models. For a more competitive and unified comparison with existing baselines in terms of the training strategy, we hence adopt the two proposed LOE methods (Section 3) and the two baseline methods "Blind" and "Refine" (Section 2) to two backbone models.
(a) Blind (b) Refine (c) LOES (d) LOEH (e) G-truth
Image datasets. On CIFAR-10 and F-MNIST, we follow the standard "one-vs.-rest" protocol of converting these data into anomaly detection datasets (Ruff et al., 2018;Golan &amp; El-Yaniv, 2018;Hendrycks et al., 2019;Bergman &amp; Hoshen, 2020). We create C anomaly detection tasks (where C is the number of classes), with each task considering one of the classes as normal and the union of all other classes as abnormal. For each task, the training set is a mixture of normal samples and a fraction of α 0 abnormal samples. For MVTEC, we use image features as the model inputs.</p>
<p>The features are obtained from the third residual block of a WideResNet50 pre-trained on ImageNet as suggested in Reiss et al. (2021). Since the MVTEC training set contains no anomalies, we contaminate it with artificial anomalies that we create by adding zero-mean Gaussian noise to the features of test set anomalies. We use a large variance for the additive noise (equal to the empirical variance of the anomalous features) to reduce information leakage from the test set into the training set.</p>
<p>Results. We present the experimental results of CIFAR-10 and F-MNIST in Table 1, where we set the contamination ratio α 0 = α = 0.1. The results are reported as the mean and standard deviation of three runs with different model initialization and anomaly samples for the contamination. The number in the brackets is the average performance difference from the model trained on clean data. Our pro-  Anomaly detection performance of NTL on CIFAR-10, F-MNIST, and two tabular datasets (Arrhythmia and Thyroid) with α0 ∈ {5%, 10%, 15%, 20%}. LOE (ours) consistently outperforms the "Blind" and "Refine" on various contamination ratios. Table 3. F1-score (%) for anomaly detection on 30 tabular datasets studied in (Shenkar &amp; Wolf, 2022). We set α0 = α = 10% in all experiments. LOE (proposed) outperforms the "Blind" and "Refine" consistently. (See Tables 5 and 6  We also evaluate our methods with NTL at various contamination ratios (from 5% to 20%) in Fig. 2 (a) and (b). We can see 1) adding labeled anomalies (G-truth) boosts performance, and 2) among all methods that do not have ground truth labels, the proposed LOE methods achieve the best performance consistently at all contamination ratios.</p>
<p>We also experimented on anomaly detection and segmentation on the MVTEC dataset. Results are shown in Table 2, where we evaluated the methods on two contamination ratios (10% and 20%). Our method improves over the "Blind" and "Refine" baselines in all experimental settings.</p>
<p>Experiments on Tabular Data</p>
<p>Tabular data is another important application area of anomaly detection. Many data sets in the healthcare and cybersecurity domains are tabular. Our empirical study demonstrates that LOE yields the best performance for two popular backbone models on a comprehensive set of contaminated tabular datasets.</p>
<p>Tabular datasets.</p>
<p>We study all 30 tabular datasets used in the empirical analysis of a recent state-of-theart paper (Shenkar &amp; Wolf, 2022). These include the frequently-studied small-scale Arrhythmia and Thyroid medical datasets, the large-scale cyber intrusion detection datasets KDD and KDDRev, and multi-dimensional point datasets from the outlier detection datasets 6 . We follow the pre-processing and train-test split of the datasets in Shenkar &amp; Wolf (2022). To corrupt the training set, we create artificial anomalies by adding zero-mean Gaussian noise to anomalies from the test set. We use a large variance for the additive noise (equal to the empirical variance of the anomalies in the test set) to reduce information leakage from the test set into the training set.</p>
<p>Backbone models and baselines. We consider two advanced deep anomaly detection methods for tabular data described in Section 3.4: NTL and ICL. For NTL, we use nine transformations and multi-layer perceptrons for neural transformations and the encoder on all datasets. Further details are provided in Appendix C. For ICL, we use the code provided by the authors. We implement the proposed LOE methods (Section 3) and the "Blind" and "Refine" baselines (Section 2) with both backbone models.</p>
<p>Results. We report F1-scores for 30 tabular datasets in Table 3. The results are reported as the mean and standard derivation of five runs with different model initializations and random training set split. We set the contamination ratio α 0 = α = 0.1 for all datasets. More detailed results, including AUCs and the performance degradation over clean data, are provided in Appendix D (Tables 5 and 6).</p>
<p>LOE outperforms the "Blind" and "Refine" baselines consistently. Remarkably, on some datasets, LOE trained on contaminated data can achieve better results than on clean data (as shown in Table 5), suggesting that the latent anomalies provide a positive learning signal. This effect can be seen when increasing the contamination ratio on the Arrhythmia and Thyroid datasets (Fig. 2 (c) and (d)). Hendrycks et al. (2018) noticed a similar phenomenon when adding labeled auxiliary outliers; these known anomalies help the model learn better region boundaries for normal data. Our results suggest that even unlabelled anomalies, when properly inferred, can improve the performance of an anomaly detector. Overall, we conclude that LOE significantly improves the performance of anomaly detection methods on contaminated tabular datasets. </p>
<p>Experiments on Video Data</p>
<p>In addition to image and tabular data, we also evaluate our methods on a video frame anomaly detection benchmark also studied in (Pang et al., 2020). The goal is to identify video frames that contain unusual objects or abnormal events. Experiments show that our methods achieve stateof-the-art performance on this benchmark.</p>
<p>Video dataset. We study UCSD Peds1 7 , a popular benchmark for video anomaly detection. It contains surveillance videos of a pedestrian walkway. Non-pedestrian and unusual behavior is labeled as abnormal. The data set contains 34 training video clips and 36 testing video clips, where all frames in the training set are normal and about half of the testing frames are abnormal. We follow the data preprocessing protocol of Pang et al. (2020) for dividing the data into training and test sets. To realize different contamination ratios, we randomly remove some abnormal frames from the training set but the test set is fixed.</p>
<p>Backbone models and baselines. In addition to the "Blind" and "Refine" baselines, we compare to (Pang et al., 2020) (a ranking-based state-of-the-art method for video frame anomaly detection already described in Section 2) and all baselines reported in that paper (Sugiyama &amp; Borgwardt, 2013;Liu et al., 2012;Del Giorno et al., 2016;Tudor Ionescu et al., 2017;Liu et al., 2018).</p>
<p>We implement the proposed LOE methods, the "Blind", and the "Refine" baselines with NTL as the backbone model. We use a pre-trained ResNet50 on ImageNet as a feature extractor, whose output is then sent into an NTL. The feature extractor and NTL are jointly optimized during training.</p>
<p>Results. We report the results in Table 4. Our soft LOE method achieves the best performance across different con- (c) Refine Figure 3. A sensitivity study of the robustness of LOEH , LOES, and "Refine" to the mis-specified contamination ratio. We evaluate them with NTL on CIFAR-10 in terms of AUC. LOEH and LOES yield robust results and outperform "Refine" in the most cases. tamination ratios. Our method outperforms Deep Ordinal Regression (Pang et al., 2020) by 18.8% and 9.2% AUC on the contamination ratios of 10% and 20%, respectively. LOE S outperforms the "Blind" and "Refine" baselines significantly on various contamination ratios.</p>
<p>Sensitivity Study</p>
<p>The hyperparameter α characterizes the assumed fraction of anomalies in our training data. Here, we evaluate its robustness under different ground truth contamination ratios. We run LOE H and LOE S with NTL on CIFAR-10 with varying true anomaly ratios α 0 and different hyperparameters α. We present the results in a matrix accommodating the two variables. The diagonal values report the results when correctly setting the contamination ratio.</p>
<p>LOE H (Fig. 3 (a)) shows considerable robustness: the method suffers at most 1.4% performance degradation when the hyperparameter α is off by 5%, and is always better than "Blind". It always outperforms "Refine" (Fig. 3 (c)) when erroneously setting a smaller α than the true ratio α 0 . LOE S (Fig. 3 (b)) also shows robustness, especially when erroneously setting a larger α than α 0 . The method is always better than "Refine" (Fig. 3 (c)) when the hyperparameter α is off by up to 15%, and always outperforms "Blind".</p>
<p>Conclusion</p>
<p>We propose Latent Outlier Exposure (LOE): a domainindependent approach for training anomaly detectors on a dataset contaminated by unidentified anomalies. During training, LOE jointly infers anomalous data in the training set while updating its parameters by solving a mixed continuous-discrete optimization problem; iteratively updating the model and its predicted anomalies. Similar to outlier exposure (Hendrycks et al., 2018), LOE extracts a learning signal from both normal and abnormal samples by considering a combination of two losses for both normal and (assumed) abnormal data, respectively. Our approach can be applied to a variety of anomaly detection benchmarks and loss functions. As demonstrated in our comprehensive empirical study, LOE yields significant performance improvements on all three of image, tabular, and video data.</p>
<p>Acknowledgements</p>
<p>Stephan Mandt acknowledges support by the National Science Foundation (NSF) under the NSF CAREER Award 2047418; NSF Grants 1928718, 2003237 and 2007719; the Department of Energy under grant DE-SC0022331, as well as gifts from Intel, Disney, and Qualcomm. This material is in part based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001120C0021. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA. Marius Kloft acknowledges support by the Carl-Zeiss Foundation, the DFG awards KL 2698/2-1 and KL 2698/5-1, and the BMBF awards 01|S18051A, 03|B0770E, and 01|S21010C. We thank Sam Showalter for providing helpful feedback on our manuscript.</p>
<p>The Bosch Group is carbon neutral. Administration, manufacturing and research activities do no longer leave a carbon footprint. This also includes GPU clusters on which the experiments have been performed.  Yoon, J., Sohn, K., Li, C.-L., Arik, S. O., Lee, C.-Y., and Pfister, T. Self-trained one-class classification for unsupervised anomaly detection. arXiv preprint arXiv:2106.06115, 2021.</p>
<p>Zhou, C. and Paffenroth, R. C. Anomaly detection with robust deep autoencoders. In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 665-674, 2017.</p>
<p>A. Details on Toy Data Experiments</p>
<p>We generate the toy data with a three-component Gaussian mixture. The normal data is generated from p n = N (x; [1, 1], 0.07I), and the anomalies are sampled from p a = N (x; [−0.25, 2.5], 0.03I) + N (x; [−1., 0.5], 0.03I).</p>
<p>There are 90 normal samples and 10 abnormal samples. All samples are mixed up as the contaminated training set.</p>
<p>To learn a anomaly detector, we used one-class Deep SVDD (Ruff et al., 2018) to train a one-layer radial basis function (RBF) network where the Gaussian function is used as the RBF. The hidden layer contains three neurons whose centers are fixed at the center of each component and whose scales are optimized during training. The output of the RBF net is a linear combination of the outputs of hidden layers. Here we set the model output to be a 1D scalar, as the projected data representation of Deep SVDD.</p>
<p>For Deep SVDD configuration, we randomly initialized the model center (not to be confused with the center of the Gaussian RBF) and made it learnable during training. We also added the bias term in the last layer. Although setting a learnable center and adding bias terms are not recommended for Deep SVDD (Ruff et al., 2018) due to the all-zero trivial solution, we found these practices make the model flexible and converge well and learn a much better anomaly detector than vice verse, probably because the random initialization and small learning rate serve as regularization and the model converges to a local optimum before collapses to the trivial solution. During training, we used Adam (Kingma &amp; Ba, 2014) stochastic optimizer and set the mini-batch size to be 25. The learning rate is 0.01, and we trained the model for 200 epochs. The decision boundary in Figure 1 plots the 90% fraction of the anomaly scores.</p>
<p>B. Baseline Details</p>
<p>Across all experiments, we employ two baselines that do not utilize anomalies to help training the models. The baselines are either completely blind to anomalies, or drop the perceived anomalies' information. Normally training a model without recognizing anomalies serves as our first baseline. Since this baseline doesn't take any actions to the anomalies in the contaminated training data and is actually blind to the anomalies that exist, we name it Blind. Mathematically, Blind sets y i = 0 in Eq. 1 for all samples.</p>
<p>The second baseline filters out anomalies and refines the training data: at every mini-batch update, it first ranks the mini-batch data according to the anomaly scores given current detection model, then removes top α most likely anomalous samples from the mini-batch. The remaining samples performs the model update. We name the second baseline Refine, which still follows Alg. 1 but removes L θ a in Eq. 1. Both these two baselines take limited actions to the anomalies. We use them to contrast our proposed methods and highlight the useful information contained in unseen anomalies.</p>
<p>C. Implementation Details</p>
<p>We apply NTL to all datasets including both visual datasets and tabular datasets. Below we provide the implementation details of NTL on each class of datasets. On CIFAR-10, we set minibatch size to be 500, learning rate to be 4e-4, 30 training epochs with Adam optimizer. On F-MNIST, we set minibatch size to be 500, learning rate to be 2e-4, 30 training epochs with Adam optimizer. On MVTEC, we set minibatch size to be 40, learning rate to be 2e-4, 30 training epochs with Adam optimizer. For the "Refine" baseline and our methods we set the number of warm-up epochs as two on all image datasets.</p>
<p>NTL on tabular data On all tabular data, we set the number of transformations to 9, use two fully-connected network layers for the transformations and four fully-connected network layers for the encoder. The hidden size of layers in the transformation networks and the encoder is two times the data dimension for low dimensional data, and 64 for high dimensional data. The embedding size is two times the data dimension for low dimensional data, and 32 for high dimensional data. The transformations are either parametrized as the transformation network directly or a residual connection of the transformation network and the original sample. We search the best-performed transformation parameterization and other hyperparameters based on the performance of the model trained on clean data. We use Adam optimizer with a learning rate chosen from [5e − 4, 1e − 3, 2e − 3]. For the "Refine" baseline and our methods we set the number of warm-up epochs as two for small datasets and as one for large datasets.</p>
<p>NTL on video data Following the suggestions of Pang et al. (2020), we first extract frame features through a ResNet50 pretrained on ImageNet. The features are sent to an NTL with the same backbone model as used on CIFAR-10 (see NTL on image data) except that 9 transformations are used. Both the ResNet50 and NTL are updated from end to end. During training, we use Adam stochastic optimizer with the batch size set to be 192 and learning rate set 1e-4. We update the model for 3 epochs and report the results with three independent runs.</p>
<p>MHRot on image data MHRot (Hendrycks et al., 2019) applies self-supervised learning on hand-crafted image transformations including rotation, horizontal shift, and vertical shift. The learner learns to solve three different tasks: one for predicting rotation (r ∈ R ≡ {0 • , ±90 • , 180 • }), one for predicting vertical shift (s v ∈ S v ≡ {0 px, ±8 px}), and one for predicting horizontal shift (s h ∈ S h ≡ {0 px, ±8 px}). We define the composition of rotation, vertical shift, and horizontal shift as T ∈ T ≡ {r • s v • s h | r ∈ R, s v ∈ S v , s h ∈ S h }. We also define the head labels t 1 k = r a , t 2 k = s v b , t 3 k = s h c for a specific composed transformation T k = r a • s v b • s h c . Overall, there are 36 transformations.</p>
<p>We implement the model on the top of GOAD (Bergman &amp; Hoshen, 2020), a similar self-supervised anomaly detector. The backbone model is a WideResNet16-4. Anomaly scores is used for ranking in the mini-batch in pseudo label assignments. For F-MNIST, we use L θ n , the normality training loss, as the anomaly score. For CIFAR-10, we find that using a separate anomaly score mentioned in (Bergman &amp; Hoshen, 2020) leads to much better results than the original training loss anomaly score.</p>
<p>During training, we set mini-batch size to be 10, learning rate to be 1e-3 for CIFAR-10 and 1e-4 for F-MNIST, 16 training epochs for CIFAR-10 and 3 training epochs for F-MNIST with Adam optimizer. We report the results with 3-5 independent runs.</p>
<p>D. Additional Experimental Results</p>
<p>We provide additional results of the experiments on tabular datasets. We report the F1-scores in Table 5 and the AUCs in Table 6. The number in the brackets is the average performance difference from the model trained on clean data. Remarkably, on some datasets, LOE trained on contaminated data can achieve better results than on clean data (as shown in Tables 5 and 6), suggesting that the latent anomalies provide a positive learning signal. Overall, we can see that LOE improves the performance of anomaly detection methods on contaminated tabular datasets significantly. Table 6. AUC (%) with standard deviation for anomaly detection on 30 tabular datasets which are from the empirical study of Shenkar &amp; Wolf (2022). For all experiments, we set the contamination ratio of the training set as 10%. The number in the brackets is the average performance difference from the model trained on clean data. LOE outperforms the "Blind" and "Refine" baselines. NTL ICL Blind</p>
<p>Refine LOEH (ours) LOES (ours) Blind Refine LOEH (ours) LOES (ours)</p>
<p>ods are i) MHRot (Hendrycks et al., 2019), ii) NTL (Qiu et al., 2021), and iii) ICL (Shenkar &amp; Wolf, 2022)</p>
<p>Figure 1 .
1Deep SVDD trained on 2D synthetic contaminated data (see main text) trained with different methods: (a) "Blind" (treats all data as normal), (b) "Refine" (filters out some anomalies), (c) LOES (proposed, assigns soft labels to anomalies), (d) LOEH (proposed, assigns hard labels), (e) supervised anomaly detection with ground truth labels (for reference). LOE leads to improved region boundaries.</p>
<p>Figure 2 .
2Figure 2. Anomaly detection performance of NTL on CIFAR-10, F-MNIST, and two tabular datasets (Arrhythmia and Thyroid) with α0 ∈ {5%, 10%, 15%, 20%}. LOE (ours) consistently outperforms the "Blind" and "Refine" on various contamination ratios.</p>
<p>References
Alvarez, M., Verdier, J.-C., Nkashama, D. K., Frappier, M., Tardif, P.-M., and Kabanza, F. A revealing large-scale evaluation of unsupervised anomaly detection algorithms. arXiv preprint arXiv:2204.09825, 2022. Beggel, L., Pfeiffer, M., and Bischl, B. Robust anomaly detection in images using adversarial autoencoders. arXiv preprint arXiv:1901.06355, 2019.Bergman, L. and Hoshen, Y. Classification-based anomaly detection for general data. In International Conference on Learning Representations, 2020.Bergmann, P., Fauser, M., Sattlegger, D., and Steger, C. Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection.In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9592-9600, 2019. Chen, X. and Konukoglu, E. Unsupervised detection of lesions in brain mri using constrained adversarial autoencoders. In MIDL Conference book. MIDL, 2018. Deecke, L., Vandermeulen, R., Ruff, L., Mandt, S., and Kloft, M. Image anomaly detection with generative adversarial networks. In Joint european conference on machine learning and knowledge discovery in databases, pp. 3-17. Springer, 2018. Defard, T., Setkov, A., Loesch, A., and Audigier, R. Padim: a patch distribution modeling framework for anomaly detection and localization. In ICPR 2020-25th International Conference on Pattern Recognition Workshops and Challenges, 2021. Del Giorno, A., Bagnell, J. A., and Hebert, M. A discriminative framework for anomaly detection in large videos. In European Conference on Computer Vision, pp. 334-349. Springer, 2016. Feng, J.-C., Hong, F.-T., and Zheng, W.-S. Mist: Multiple instance self-training framework for video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14009-14018, 2021. Golan, I. and El-Yaniv, R. Deep anomaly detection using geometric transformations. In Advances in Neural Information Processing Systems, pp. 9758-9769, 2018. Görnitz, N., Porbadnigk, A., Binder, A., Sannelli, C., Braun, M., Müller, K.-R., and Kloft, M. Learning and evaluation in presence of non-iid label noise. In Artificial Intelligence and Statistics, pp. 293-302. PMLR, 2014. Hendrycks, D., Mazeika, M., and Dietterich, T. Deep anomaly detection with outlier exposure. In International Conference on Learning Representations, 2018. Hendrycks, D., Mazeika, M., Kadavath, S., and Song, D. Using self-supervised learning can improve model robustness and uncertainty. Advances in Neural Information Processing Systems, 32:15663-15674, 2019. Huber, P. J. Robust estimation of a location parameter. In Breakthroughs in statistics, pp. 492-518. Springer, 1992. Huber, P. J. Robust statistics. In International encyclopedia of statistical science, pp. 1248-1251. Springer, 2011. Huyan, N., Quan, D., Zhang, X., Liang, X., Chanussot, J., and Jiao, L. Unsupervised outlier detection using memory and contrastive learning. arXiv preprint arXiv:2107.12642, 2021. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Lee, W. S. and Liu, B. Learning with positive and unlabeled examples using weighted logistic regression. In ICML, volume 3, pp. 448-455, 2003. Li, C.-L., Sohn, K., Yoon, J., and Pfister, T. Cutpaste: Self-supervised learning for anomaly detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9664-9674, 2021. Liu, F. T., Ting, K. M., and Zhou, Z.-H. Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(1):1-39, 2012. Liu, Y., Li, C.-L., and Póczos, B. Classifier two sample test for video anomaly detections. In BMVC, pp. 71, 2018. Nalisnick, E., Matsukawa, A., Teh, Y. W., Gorur, D., and Lakshminarayanan, B. Do deep generative models know what they don't know? In International Conference on Learning Representations, 2018. Pang, G., Yan, C., Shen, C., Hengel, A. v. d., and Bai, X. Self-trained deep ordinal regression for end-to-end video anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12173-12182, 2020. Principi, E., Vesperini, F., Squartini, S., and Piazza, F. Acoustic novelty detection with adversarial autoencoders. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 3324-3330. IEEE, 2017. Qiu, C., Pfrommer, T., Kloft, M., Mandt, S., and Rudolph, M. Neural transformation learning for deep anomaly detection beyond images. In International Conference on Machine Learning, pp. 8703-8714. PMLR, 2021.Qiu, C., Kloft, M., Mandt, S., and Rudolph, M. Raising the bar in graph-level anomaly detection. arXiv preprint arXiv:2205.13845, 2022. Reiss, T., Cohen, N., Bergman, L., and Hoshen, Y. Panda: Adapting pretrained features for anomaly detection and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2806-2814, 2021. Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., Müller, E., and Kloft, M. Deep one-class classification. In International conference on machine learning, pp. 4393-4402. PMLR, 2018. Ruff, L., Vandermeulen, R. A., Görnitz, N., Binder, A., Müller, E., Müller, K.-R., and Kloft, M. Deep semisupervised anomaly detection. In International Conference on Learning Representations, 2019. Ruff, L., Kauffmann, J. R., Vandermeulen, R. A., Montavon, G., Samek, W., Kloft, M., Dietterich, T. G., and Müller, K.-R. A unifying review of deep and shallow anomaly detection. Proceedings of the IEEE, 2021. Schlegl, T., Seeböck, P., Waldstein, S. M., Schmidt-Erfurth, U., and Langs, G. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In International conference on information processing in medical imaging, pp. 146-157. Springer, 2017. Schneider, T., Qiu, C., Kloft, M., Latif, D. A., Staab, S., Mandt, S., and Rudolph, M. Detecting anomalies within time series using local neural transformations. arXiv preprint arXiv:2202.03944, 2022. Schölkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., and Williamson, R. C. Estimating the support of a highdimensional distribution. Neural computation, 13(7): 1443-1471, 2001.Shenkar, T. and Wolf, L. Anomaly detection for tabular data with internal contrastive learning. In International Conference on Learning Representations, 2022. URL https: //openreview.net/forum?id=_hszZbt46bT.Sohn, K., Li, C.-L., Yoon, J., Jin, M., and Pfister, T. Learning and evaluating representations for deep one-class classification. arXiv preprint arXiv:2011.02578, 2020.</p>
<p>NTL on image data NTL is built upon the final pooling layer of a pre-trained ResNet152 on CIFAR-10 and F-MNIST (as suggested in Defard et al. (2021)), and upon the third residual block of a pre-trained WideResNet50 on MVTEC (as suggested in Reiss et al. (2021)). On all image datasets, the pre-trained feature extractors are frozen during training. We set the number of transformations as 15 and use three linear layers with intermediate 1d batchnorm layers and ReLU activations for transformations modelling. The hidden sizes of the transformation networks are [2048, 2048, 2048] on CIFAR-10 and F-MNIST, and [1024, 1024, 1024] on MVTEC. The encoder is one linear layer with units of 256 for CIFAR-10 and MVTEC, and is two linear layers of size [1024, 256] with an intermediate ReLU activation for F-MNIST.</p>
<p>MHRot and NTL. Our experiments are designed to test the hypothesis that LOE can mitigate the performance drop caused by training on contaminated image data. We experiment with three image datasets: CIFAR-10, Fashion-MNIST, and MVTEC (Bergmann et al., 2019). These have been used in virtually all deep anomaly detection papers published at top-tier venues (Ruff et al., 2018; Golan &amp; El-Yaniv, 2018; Hendrycks et al., 2019; Bergman &amp; Hoshen, 2020; Li et al., 2021), and we adopt these papers' experimental protocol here, as detailed below. Backbone models and baselines. We experiment with MHRot and NTL. In consistency with previous work (Hendrycks et al., 2019), we train MHRot on raw images and NTL on features outputted by an encoder pre-trained on ImageNet. We use the official code by the respective authors 45 . NTL is built upon the final pooling layer of a pre-trained ResNet152 for CIFAR-10 and F-MNIST (as suggested in Defard et al. (2021)), and upon the third residual block of a pre-trained WideResNet50 for MVTEC (as suggested in Reiss et al.</p>
<p>Table 1 .
1AUC (%) with standard deviation for anomaly detection 
on CIFAR-10 and F-MNIST. For all experiments, we set the con-
tamination ratio as 10%. LOE mitigates the performance drop 
when NTL and MHRot trained on the contaminated datasets. 
CIFAR-10 
F-MNIST </p>
<p>NTL </p>
<p>Blind 
91.3±0.1 (-4.4) 85.0±0.2 (-9.7) 
Refine 
93.5±0.1 (-2.2) 89.1±0.2 (-5.6) 
LOEH (ours) 94.9±0.2 (-0.8) 92.9±0.7 (-1.8) 
LOES (ours) 
94.9±0.1 (-0.8) 92.5±0.1 (-2.2) </p>
<p>MHRot 
Blind 
84.0±0.5 (-4.2) 88.8±0.1 (-4.9) 
Refine 
84.4±0.1 (-3.8) 89.6±0.2 (-4.1) 
LOEH (ours) 86.4±0.5 (-1.8) 91.4±0.2 (-2.3) 
LOES (ours) 
86.3±0.2 (-1.9) 91.2±0.4 (-2.5) </p>
<p>Table 2. AUC (%) with standard deviation of NTL for anomaly 
detection/segmentation on MVTEC. We set the contamination 
ratio of the training set as 10% and 20%. 
Detection 
Segmentation 
10% 
20% 
10% 
20% </p>
<p>Blind 
94.2±0.5 89.4±0.3 96.17±0.08 95.09±0.17 
(-3.2) 
(-8.0) 
(-0.78) 
(-1.86) </p>
<p>Refine 
95.3±0.5 93.2±0.3 96.55±0.04 96.09±0.06 
(-2.1) 
(-4.2) 
(-0.40) 
(-0.86) 
LOEH 95.9±0.9 92.9±0.4 95.97±0.22 93.29±0.21 
(ours) 
(-1.5) 
(-4.5) 
(-0.98) 
(-3.66) 
LOES 
95.4±0.5 93.6±0.3 96.56±0.04 96.11±0.05 
(ours) 
(-2.0) 
(-3.8) 
(-0.39) 
(-0.84) </p>
<p>posed methods consistently outperform the baselines and 
mitigate the performance drop between the model trained 
on clean data vs. the same model trained on contaminated 
data. Specifically, with NTL, LOE significantly improves 
over the best-performing baseline, "Refine", by 1.4% and 
3.8% AUC on CIFAR-10 and F-MNIST, respectively. On 
CIFAR-10, our methods have only 0.8% AUC lower than 
when training on the normal dataset. When we use another 
state-of-the-art method MHRot on raw images, our LOE 
methods outperform the baselines by about 2% AUC on 
both datasets. </p>
<p>Table 4 .
4Default setup in (Pang et al., 2020), corresponding to α 0 ≈ 30%.AUC (%) for different contamination ratios for a video 
frame anomaly detection benchmark proposed in (Pang et al., 
2020). LOES (proposed) achieves state-of-the-art performance. </p>
<p>Method 
Contamination Ratio 
10% 
20% 
30%  *<br />
(Tudor Ionescu et al., 2017) 
-
-
68.4 
(Liu et al., 2018) 
-
-
69.0 
(Del Giorno et al., 2016) 
-
-
59.6 
(Sugiyama &amp; Borgwardt, 2013) 
55.0 
56.0 
56.3 
(Pang et al., 2020) 
68.0 
70.0 
71.7 
Blind 
85.2±1.0 76.0±2.7 66.6±2.6 
Refine 
82.7±1.5 74.9±2.4 69.3±0.7 
LOE H (ours) 
82.3±1.6 59.6±3.8 56.8±9.5 
LOE S (ours) 
86.8±1.2 79.2±1.3 71.5±2.4 
 *  </p>
<p>Sugiyama, M. and Borgwardt, K. Rapid distance-based outlier detection via sampling. Advances in Neural Information Processing Systems, 26:467-475, 2013. Tudor Ionescu, R., Smeureanu, S., Alexe, B., and Popescu, M. Unmasking the abnormal events in video. In Proceedings of the IEEE international conference on computer vision, pp. 2895-2903, 2017. Wang, S., Zeng, Y., Liu, X., Zhu, E., Yin, J., Xu, C., and Kloft, M. Effective end-to-end unsupervised outlier detection via inlier priority of discriminative network. In Advances in Neural Information Processing Systems, pp. 5962-5975, 2019. Xia, Y., Cao, X., Wen, F., Hua, G., and Sun, J. Learning discriminative reconstructions for unsupervised outlier removal. In Proceedings of the IEEE International Conference on Computer Vision, pp. 1511-1519, 2015.
Code is available at https://github.com/ boschresearch/LatentOE-AD.git
https://github.com/hendrycks/ss-ood.git 5 https://github.com/boschresearch/ NeuTraL-AD.git
http://odds.cs.stonybrook.edu/
http://www.svcl.ucsd.edu/projects/ anomaly/dataset.htm
Table 5. F1-score (%) with standard deviation for anomaly detection on 30 tabular datasets which are from the empirical study ofShenkar &amp; Wolf (2022). For all experiments, we set the contamination ratio of the training set as 10%. The number in the brackets is the average performance difference from the model trained on clean data. LOE outperforms the "Blind" and "Refine" baselines.
) (-19.2) (+2.0) (-5.2) wbc 25.7±12.3 45.7±15.5 76. 0±8.9 98.0±4.0 100±0.0 (-68.0) (-26.0) (-2.0) (+0.0) (-86.0) (-80.0) (+8.0) 2.4) (-0.5) (-0.48.2) (-8.2) (+2.3) (+2.3) wine 24.0±18.5 66.0±12.0 90.0±0.0 92.0±4.0 4.0±4.9 10. +0.8) (-10.1) (-2.0) (+0.3) (+0.4(-34.4) (-22.7) (+4.6) (+4.6) (-31.4) (-5.6) (+6.0) (+3.7) ) (-26.3) (-13.9) (-28.3) (-37.5) (-19.2) (+2.0) (-5.2) wbc 25.7±12.3 45.7±15.5 76.2±6.0 69.5±3.8 50.5±5.7 50.5±2.3 61.0±4.7 61.0±1.9 (-39.1) (-19.1) (+11.4) (+4.7) (-8.2) (-8.2) (+2.3) (+2.3) wine 24.0±18.5 66.0±12.0 90.0±0.0 92.0±4.0 4.0±4.9 10.0±8.9 98.0±4.0 100±0.0 (-68.0) (-26.0) (-2.0) (+0.0) (-86.0) (-80.0) (+8.0) 2.4) (-0.5) (-0.4) (+0.8) (-10.1) (-2.0) (+0.3) (+0.4)</p>
<p>) (-83.6) (-81.1) (-81. 5.7) ) (-3.2) (-2.6) (+2.75+3.8) (+0.8) ) (-0.2) (+0.2) (+0.5) (-6.7) (-6.9. 6.5) (-5.0) (+4.6) ) (-4.5) (-11.9) (-0.6) (-10.5) (-3.3) (-8.2)100±0.0 (-23.8) (+0.0) (+0.0) (+0.0) (-21.2) (+0.0) (+0.0) (+0.0) ) (-46.0) (-13.8) (-12.1) (-83.6) (-81.1) (-81.5) (-82.8) 2.2) (-1.1) (-0.9) (-0.9) (-2.1) (-1.3) (+3.8) (+0.8) ) (-0.2) (+0.2) (+0.5) (-6.7) (-6.9) (-5.9) (-5.7) ) (-3.2) (-2.6) (+2.7) (-6.5) (-5.0) (+4.6) ) (-4.5) (-11.9) (-0.6) (-10.5) (-3.3) (-8.2)</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8646 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8646</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8646</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-18e20944d1d64e73fc40321f65c3ddd0ef6a7aca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/18e20944d1d64e73fc40321f65c3ddd0ef6a7aca" target="_blank">Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</a></p>
                <p><strong>Paper Venue:</strong> International Workshop on Neural-Symbolic Learning and Reasoning</p>
                <p><strong>Paper TL;DR:</strong> This paper investigates the extent to which encoder-only transformer language models (LMs) can reason according to logical rules, and shows that LMs have difficulty in transferring their putative logical reasoning ability, suggesting that they may have learned dataset-specific features, instead of a general capability.</p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is central to complex human activities, such as thinking, debating, and planning; it is also a central component of many AI systems as well. In this paper, we investigate the extent to which encoder-only transformer language models (LMs) can reason according to logical rules. We ask whether those LMs can deduce theorems in propositional calculus and first-order logic; if their relative success in these problems reflects general logical capabilities; and which layers contribute the most to the task. First, we show for several encoder-only LMs that they can be trained, to a reasonable degree, to determine logical validity on various datasets. Next, by cross-probing fine-tuned models on these datasets, we show that LMs have difficulty in transferring their putative logical reasoning ability, which suggests that they may have learned dataset-specific features, instead of a general capability. Finally, we conduct a layerwise probing experiment, which shows that the hypothesis classification task is mostly solved through higher layers.</p>
                <p><strong>Cost:</strong> 0.024</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8646.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8646.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DistilBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistilBERT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller, distilled version of BERT evaluated here as an encoder-style transformer for hypothesis classification on logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DistilBERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A compact transformer-based language model (distilled from BERT) evaluated by fine-tuning a classification head on logical reasoning datasets; treated in the paper as one of several encoder-style pretrained LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Determine the logical relation (e.g., True/False/Unknown or entailment/contradiction/neutral/paradox) between premises (facts + rules) and a hypothesis expressed in natural language; tasks span first-order logic (FOL) and propositional calculus (PC) with conjunctive implication.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning the pretrained LM with a linear classification head on the [CLS] embedding for each dataset; evaluation includes cross-probing (frozen body + probe) and layerwise probing on RoBERTa-large (paper-wide methods applied to all models in Section 4).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 50.98% accuracy; LogicNLI: 36.20%; RuleTaker: 86.55%; SimpleLogic: 93.58% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Surpasses the largest-class baselines (Largest class: FOLIO 35.29%, LogicNLI 25.00%, RuleTaker 50.02%, SimpleLogic 50.03%), but substantially lower than best-performing models on FOLIO and LogicNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower performance on more linguistically variable / broader-scope datasets (FOLIO, LogicNLI); suggests reliance on dataset-specific cues rather than general logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Distilled/smaller models can be trained to perform well on constrained logical tasks but struggle on broader FOL datasets, consistent with the paper's conclusion that fine-tuned encoder-style LMs learn dataset-specific features more than generalized logical competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8646.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-base is an encoder transformer evaluated by fine-tuning for hypothesis classification across several strict logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained encoder transformer (BERT family) used as a starting point and fine-tuned with a classification head on the logical reasoning datasets; treated as representative of base-size encoder LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same hypothesis-classification framework across FOLIO (FOL), LogicNLI (FOL), RuleTaker (FOL/CI), SimpleLogic (PC/CI).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tune full model (body + linear head) on each dataset; compare to pretrained probe baselines and largest-class baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 52.45%; LogicNLI: 48.75%; RuleTaker: 98.91%; SimpleLogic: 92.31%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially above largest-class baselines; near-perfect on RuleTaker but lower on FOLIO and LogicNLI compared to top models (e.g., RoBERTa-large).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower generalization across datasets; performance gap larger on datasets with greater language variability and logical scope.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Fine-tuning BERT-base yields strong results on restricted conjunctive-implication tasks but limited transfer and weaker accuracy on full-FOL human-written datasets, echoing paper's finding about dataset-specific learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8646.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BERT-large (a larger encoder transformer in the BERT family) was fine-tuned and evaluated on multiple logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Larger-capacity version of BERT used for fine-tuning on hypothesis classification tasks over logical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Classification of logical validity/entailment relationships expressed in natural language mapped to FOL/PC formalisms.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with a [CLS]-based linear classification head; same optimization/early-stopping regimes as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 61.76%; LogicNLI: 42.50%; RuleTaker: 99.94%; SimpleLogic: 91.88%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Exceeds largest-class baselines, with near-perfect performance on RuleTaker; somewhat variable performance on LogicNLI and FOLIO versus other large models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not demonstrate robust cross-dataset transfer; performance remains dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Larger capacity improves on some datasets (notably RuleTaker) but does not imply acquisition of general logical reasoning abilities transferable across FOL/PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8646.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-base, an encoder transformer pretrained with dynamic masked language modeling, was fine-tuned on multiple logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only transformer (RoBERTa family) employed as a fine-tuning backbone for hypothesis classification; RoBERTa's pretraining is noted as dynamic masked language modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above: predict logical relation labels from premises+rules and hypothesis in natural language across FOL and PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS] embedding linear head; cross-probing and layerwise probing performed in-depth for RoBERTa-large (RoBERTa family chosen for detailed analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 58.82%; LogicNLI: 62.90%; RuleTaker: 98.04%; SimpleLogic: 92.87%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms largest-class baseline substantially; near best among base models on several datasets, but RoBERTa-large exceeds it.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Performance still dataset-specific; pretrained RoBERTa without fine-tuning shows limited capability (per cross-probing results).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>RoBERTa variants fine-tuned can solve many dataset instances, but the paper shows that fine-tuned RoBERTa models do not develop a generalized logical reasoning ability transferable across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8646.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>RoBERTa-large is the primary model analyzed in-depth (cross-probing and layerwise probing) after fine-tuning on logical reasoning datasets; it achieved the best aggregate performance in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large-capacity RoBERTa encoder-style transformer (pretrained with dynamic masked language modeling) used as the focal model for detailed experiments including fine-tuning, cross-probing (frozen body + probe), and layerwise probing to inspect where task knowledge resides.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Predict logical labels (FOLIO: True/False/Unknown; LogicNLI: Entailment/Contradiction/Neutral/Paradox; RuleTaker & SimpleLogic: True/False) from premises+rules and hypothesis in NL; tasks test FOL and PC reasoning with varying linguistic complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning (body + classification head) for primary evaluation; cross-probing: remove fine-tuned head, freeze transformer body, train small probes (1-layer and 3-layer) on top to test recoverability and transfer; layerwise probing: place probes on [CLS] outputs at each layer to identify where reasoning-related signals appear.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 64.71%; LogicNLI: 72.70%; RuleTaker: 99.78%; SimpleLogic: 90.83% (Table 2). Pretrained probe baselines (RoBERTa pretrained, no fine-tuning) gave: FOLIO 32.88%, LogicNLI 25.54%, RuleTaker 50.01%, SimpleLogic 61.19% (Table 3). Cross-probing of RoBERTa-large fine-tuned models shows large gains on same-dataset probes but limited transfer to other datasets (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially above 'largest class' baselines (35.29%, 25.00%, 50.02%, 50.03%); fine-tuning provides large improvements vs pretrained model, but probes on different datasets show limited transfer (fine-tuned-on-X probed-on-Y much worse than fine-tuned-on-Y).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fine-tuned RoBERTa-large exhibits limited generalization across datasets (models tend to learn dataset-specific statistical features); pretrained RoBERTa has little to no intrinsic logical reasoning ability; probing shows that task-specific signals concentrate in higher layers, indicating specialized rather than generalizable representations; SimpleLogic may be solvable by shallow heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Although RoBERTa-large can be fine-tuned to high accuracy on particular logical benchmarks (near-perfect on RuleTaker), cross-probing and layerwise probing indicate that the learned ability is dataset-specific and primarily encoded in higher layers, implying lack of emergent, general logical reasoning in these encoder-style LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8646.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Longformer-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Longformer (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Longformer-base (a transformer variant for long sequences) was fine-tuned and evaluated on the logical reasoning benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Longformer-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer variant optimized for long inputs (Longformer family) used in fine-tuning experiments for hypothesis classification on long logical contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same hypothesis-classification tasks; datasets include long-context examples (LogicNLI and SimpleLogic have long average words/premises).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS] classification head on each dataset; included to test model families that handle long contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 62.75%; LogicNLI: 58.05%; RuleTaker: 99.92%; SimpleLogic: 94.21%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Well above largest-class baselines; strong performance on long-context datasets like SimpleLogic where it achieved the highest SimpleLogic score in Table 2 (94.21%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Like other models, transfer/generalization across different datasets was not demonstrated; performance can vary by dataset despite long-context capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Models specialized for long contexts can achieve high accuracy on some logical tasks, but still appear to rely on dataset-specific cues rather than generalized logical rule-following.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8646.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Longformer-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Longformer (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Longformer-large was fine-tuned across the logical benchmarks and evaluated similarly to other large encoder variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Longformer-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-capacity Longformer variant used for hypothesis classification on the four datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above, with some datasets requiring handling of many premises/long inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with classification head; same optimization/hyperparameter regime.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 62.25%; LogicNLI: 74.15%; RuleTaker: 99.94%; SimpleLogic: 92.37%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially better than largest-class baselines; notably high LogicNLI performance (74.15%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Even with strong per-dataset performance, cross-dataset transfer not guaranteed; results consistent with dataset-specific learning hypothesis.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Longformer-large demonstrates that long-input architectures can perform well on FOL datasets, but do not necessarily learn transferable logical reasoning abilities from fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e8646.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa-xsmall</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (xsmall)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very small DeBERTa variant tested by fine-tuning for hypothesis classification on the logical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-xsmall</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small-capacity DeBERTa-family transformer evaluated in fine-tuning experiments on the logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same hypothesis classification tasks spanning FOL and PC.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with linear classification head on [CLS] embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 54.41%; LogicNLI: 65.30%; RuleTaker: 99.81%; SimpleLogic: 91.67%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Consistently above largest-class baselines; competitive on LogicNLI and near-perfect on RuleTaker.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Shows dataset-dependent performance; no evidence of improved cross-dataset transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Even small DeBERTa variants can reach high accuracy on restricted tasks, but their success does not imply general logical competence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e8646.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa-small</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (small)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small DeBERTa variant fine-tuned on logical reasoning datasets and evaluated for hypothesis classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-small</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DeBERTa family model (small size) fine-tuned for hypothesis classification on FOL/PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same tasks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning as per Section 4; [CLS]-based classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 53.43%; LogicNLI: 59.65%; RuleTaker: 95.23%; SimpleLogic: 93.23%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Above largest-class baseline; somewhat lower on RuleTaker than many other models but strong on SimpleLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Inconsistent per-dataset performance and limited generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Per-dataset fine-tuning yields high task accuracy for some configurations but not general logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e8646.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>DeBERTa-base, a mid-size DeBERTa variant, evaluated via fine-tuning on the logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base-capacity DeBERTa transformer used as a fine-tuning backbone for hypothesis classification across FOL/PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning [CLS] embedding classifier; same training regime as other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 60.78%; LogicNLI: 66.70%; RuleTaker: 99.81%; SimpleLogic: 93.72%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Strongly above largest-class baselines; high accuracy across multiple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Like others, gains are dataset-specific; transfer not validated for this model in-depth.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>DeBERTa-base shows robust per-dataset performance but no evidence in paper of learned general logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e8646.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large DeBERTa variant fine-tuned and evaluated on the logical benchmarks; achieved among the highest LogicNLI scores in the table.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-capacity DeBERTa model used for fine-tuning on logical datasets; reported strong performance on LogicNLI.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same hypothesis-classification tasks over FOL/PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with classification head; same hyperparameter regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 64.71%; LogicNLI: 84.70%; RuleTaker: 99.97%; SimpleLogic: 93.72%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Substantially above largest-class baselines; highest LogicNLI accuracy among listed models (84.70%).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite very high in-dataset performance, cross-dataset transfer not demonstrated; potential dataset-specific overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large DeBERTa can achieve excellent scores on some FOL benchmarks, but the paper cautions this does not imply acquisition of general logical rules.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e8646.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa-xlarge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (xlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extra-large DeBERTa variant included in experiments; showed anomalous low performance on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-xlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A very large DeBERTa-family model evaluated via fine-tuning on the logical benchmarks; reported results indicate inconsistent performance across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning; [CLS] classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 62.25%; LogicNLI: 25.00%; RuleTaker: 99.99%; SimpleLogic: 49.96%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Some reported scores (LogicNLI 25.00%, SimpleLogic 49.96%) are near or below the largest-class baseline, indicating failed training/instability for this configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported anomalous/very low performance on LogicNLI and SimpleLogic suggests instability or failure modes for this particular large checkpoint/configuration; paper notes such exceptions among model results.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large models do not uniformly improve logical reasoning; some very large variants may underperform or suffer instabilities, reinforcing the paper's caution about concluding general reasoning ability from single high scores.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e8646.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa-xxlarge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (xxlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extra-extra-large DeBERTa variant included in experiments; displayed notably poor results on several datasets per Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-xxlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Very large DeBERTa checkpoint evaluated in fine-tuning experiments; results indicate unexpected low accuracies on some tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same hypothesis-classification tasks over FOL/PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning; [CLS] classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 71.57%; LogicNLI: 25.00%; RuleTaker: 50.02%; SimpleLogic: 50.04%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>FOLIO performance high (71.57%) but other dataset scores around largest-class baseline (LogicNLI 25.00%, RuleTaker ~50%), indicating inconsistent behavior across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Large inconsistencies indicate possible optimization/training problems or overfitting to dataset artifacts for this checkpoint; highlights that bigger models do not guarantee uniformly better logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Extremely large variants can show unstable outcomes; the paper highlights such anomalies as caveats when interpreting model size vs reasoning ability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e8646.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ALBERT-base was evaluated by fine-tuning for hypothesis classification on the logical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Parameter-reduced encoder transformer (ALBERT family) fine-tuned and evaluated on FOL/PC datasets for hypothesis classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning full model plus linear classification head on [CLS] embedding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 57.35%; LogicNLI: 66.80%; RuleTaker: 99.91%; SimpleLogic: 92.17%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Above largest-class baselines and comparable to other base families on many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No evidence of robust cross-dataset reasoning; similar constraints as other encoder families.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>ALBERT-base performs well on restricted tasks but is subject to the same dataset-specific learning patterns identified for other models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e8646.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ALBERT-large was fine-tuned on logical benchmarks and evaluated similarly to other large encoder variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Larger ALBERT variant used as fine-tuning backbone for hypothesis classification.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS]-based classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 56.37%; LogicNLI: 66.20%; RuleTaker: 99.88%; SimpleLogic: 91.53%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms largest-class baselines; similar pattern to ALBERT-base.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Dataset-specific learning and limited transfer remain concerns.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>ALBERT variants behave like other encoder-style LMs: high per-dataset performance on constrained tasks, but lacking evidence of generalizable logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e8646.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT-xlarge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT (xlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extra-large ALBERT checkpoint evaluated in experiments that showed uneven performance on FOLIO and other datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT-xlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-capacity ALBERT variant fine-tuned on the logical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same tasks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning; [CLS] classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 38.73%; LogicNLI: 65.10%; RuleTaker: 99.97%; SimpleLogic: 90.05%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>FOLIO performance is markedly lower than largest-class baseline, indicating training instability or dataset mismatch for this checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Anomalous low scores on FOLIO suggest some large checkpoints can fail to learn or generalize properly.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Not all larger variants uniformly improve performance; anomalies reinforce the need for careful evaluation beyond single-dataset accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e8646.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ALBERT-xxlarge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ALBERT (xxlarge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An xxlarge ALBERT model tested in fine-tuning experiments that achieved high LogicNLI performance in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ALBERT-xxlarge</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Very large ALBERT variant included in the experimental sweep and fine-tuned on the logical reasoning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as previous entries.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning approach consistent with other models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 56.86%; LogicNLI: 93.90%; RuleTaker: 99.94%; SimpleLogic: 92.24%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Very high LogicNLI performance (93.90%) compared to largest-class baseline; but as with other high scores, paper cautions about generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Despite very high in-dataset accuracy, the paper's cross-probing and layerwise probing suggest such successes may not reflect generalized logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Extremely high scores on certain datasets should be interpreted cautiously; the architecture/size alone does not guarantee transferable logical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e8646.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLM-RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLM-RoBERTa (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The multilingual XLM-RoBERTa base model was included and fine-tuned for the hypothesis classification tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLM-RoBERTa-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multilingual RoBERTa-family encoder (base) used as a fine-tuning backbone for the logical datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with [CLS] classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 55.88%; LogicNLI: 45.20%; RuleTaker: 97.64%; SimpleLogic: 91.74%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Generally above largest-class baselines but lower than many monolingual encoder variants on some datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Lower LogicNLI performance relative to many monolingual models; still subject to dataset-specific learning.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Multilingual pretraining does not, by itself, confer robust, general logical reasoning; performance remains dataset-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.18">
                <h3 class="extraction-instance">Extracted Data Instance 18 (e8646.18)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLM-RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLM-RoBERTa (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XLM-RoBERTa-large was fine-tuned and included in the model sweep for the logical benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLM-RoBERTa-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multilingual RoBERTa-family model fine-tuned for hypothesis classification tasks spanning FOL/PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with classification head.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 62.75%; LogicNLI: 65.45%; RuleTaker: 99.95%; SimpleLogic: 91.74%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Above largest-class baselines and comparable to other large models on many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No evidence of cross-dataset transfer improvement; dataset-specific learning patterns persist.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large multilingual models can reach high per-dataset accuracies but still do not demonstrate general logical reasoning per the paper's probing analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.19">
                <h3 class="extraction-instance">Extracted Data Instance 19 (e8646.19)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet (base)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XLNet-base, an autoregressive-permuted-pretraining transformer included in the sweep, was fine-tuned for the hypothesis classification benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet-base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>XLNet-family model (base) evaluated by fine-tuning on the logical reasoning datasets; included among the model families in the experimental sweep.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same tasks as above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with a [CLS]-based linear classifier on top.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 58.33%; LogicNLI: 55.00%; RuleTaker: 99.19%; SimpleLogic: 94.28%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Above largest-class baselines; strong on RuleTaker and SimpleLogic.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>As others, suffers from dataset-specific learning and limited transferability (cross-probing analyses focus on RoBERTa but trend applies broadly).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>XLNet family can achieve high accuracy on certain logical datasets but does not escape the generalization limitations highlighted for encoder-style LMs in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8646.20">
                <h3 class="extraction-instance">Extracted Data Instance 20 (e8646.20)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XLNet-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XLNet (large)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XLNet-large was fine-tuned and evaluated on the logical reasoning datasets, showing very high scores on RuleTaker.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>XLNet-large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large-capacity XLNet model included in the experimental sweep and fine-tuned for hypothesis classification tasks on FOL/PC datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See above.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Fine-tuning with a [CLS] classifier.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>FOLIO: 58.33%; LogicNLI: 71.40%; RuleTaker: 99.86%; SimpleLogic: 92.94%.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Significantly above largest-class baselines and competitive with other large models on many datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Similar dataset-specific learning and transfer limitations apply.</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Large autoregressive/pretraining-style variants can match the per-dataset performance of large encoder models but still lack evidence of a general logical reasoning faculty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models', 'publication_date_yy_mm': '2023-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Transformers as Soft Reasoners over Language <em>(Rating: 2)</em></li>
                <li>Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI <em>(Rating: 2)</em></li>
                <li>FOLIO: Natural Language Reasoning with First-Order Logic <em>(Rating: 2)</em></li>
                <li>On the Paradox of Learning to Reason from Data <em>(Rating: 2)</em></li>
                <li>Negation, Coordination, and Quantifiers in Contextualized Language Models <em>(Rating: 1)</em></li>
                <li>Language Models Show Human-Like Content Effects on Reasoning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8646",
    "paper_id": "paper-18e20944d1d64e73fc40321f65c3ddd0ef6a7aca",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "DistilBERT",
            "name_full": "DistilBERT",
            "brief_description": "A smaller, distilled version of BERT evaluated here as an encoder-style transformer for hypothesis classification on logical reasoning datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "DistilBERT",
            "model_description": "A compact transformer-based language model (distilled from BERT) evaluated by fine-tuning a classification head on logical reasoning datasets; treated in the paper as one of several encoder-style pretrained LMs.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Determine the logical relation (e.g., True/False/Unknown or entailment/contradiction/neutral/paradox) between premises (facts + rules) and a hypothesis expressed in natural language; tasks span first-order logic (FOL) and propositional calculus (PC) with conjunctive implication.",
            "method_or_approach": "Fine-tuning the pretrained LM with a linear classification head on the [CLS] embedding for each dataset; evaluation includes cross-probing (frozen body + probe) and layerwise probing on RoBERTa-large (paper-wide methods applied to all models in Section 4).",
            "performance": "FOLIO: 50.98% accuracy; LogicNLI: 36.20%; RuleTaker: 86.55%; SimpleLogic: 93.58% (Table 2).",
            "baseline_comparison": "Surpasses the largest-class baselines (Largest class: FOLIO 35.29%, LogicNLI 25.00%, RuleTaker 50.02%, SimpleLogic 50.03%), but substantially lower than best-performing models on FOLIO and LogicNLI.",
            "limitations_or_failures": "Lower performance on more linguistically variable / broader-scope datasets (FOLIO, LogicNLI); suggests reliance on dataset-specific cues rather than general logical reasoning.",
            "insights_or_conclusions": "Distilled/smaller models can be trained to perform well on constrained logical tasks but struggle on broader FOL datasets, consistent with the paper's conclusion that fine-tuned encoder-style LMs learn dataset-specific features more than generalized logical competence.",
            "uuid": "e8646.0",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "BERT-base",
            "name_full": "BERT (base)",
            "brief_description": "BERT-base is an encoder transformer evaluated by fine-tuning for hypothesis classification across several strict logical reasoning datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "BERT-base",
            "model_description": "Pretrained encoder transformer (BERT family) used as a starting point and fine-tuned with a classification head on the logical reasoning datasets; treated as representative of base-size encoder LMs.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same hypothesis-classification framework across FOLIO (FOL), LogicNLI (FOL), RuleTaker (FOL/CI), SimpleLogic (PC/CI).",
            "method_or_approach": "Fine-tune full model (body + linear head) on each dataset; compare to pretrained probe baselines and largest-class baselines.",
            "performance": "FOLIO: 52.45%; LogicNLI: 48.75%; RuleTaker: 98.91%; SimpleLogic: 92.31%.",
            "baseline_comparison": "Substantially above largest-class baselines; near-perfect on RuleTaker but lower on FOLIO and LogicNLI compared to top models (e.g., RoBERTa-large).",
            "limitations_or_failures": "Lower generalization across datasets; performance gap larger on datasets with greater language variability and logical scope.",
            "insights_or_conclusions": "Fine-tuning BERT-base yields strong results on restricted conjunctive-implication tasks but limited transfer and weaker accuracy on full-FOL human-written datasets, echoing paper's finding about dataset-specific learning.",
            "uuid": "e8646.1",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "BERT-large",
            "name_full": "BERT (large)",
            "brief_description": "BERT-large (a larger encoder transformer in the BERT family) was fine-tuned and evaluated on multiple logical reasoning datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "BERT-large",
            "model_description": "Larger-capacity version of BERT used for fine-tuning on hypothesis classification tasks over logical datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Classification of logical validity/entailment relationships expressed in natural language mapped to FOL/PC formalisms.",
            "method_or_approach": "Fine-tuning with a [CLS]-based linear classification head; same optimization/early-stopping regimes as other models.",
            "performance": "FOLIO: 61.76%; LogicNLI: 42.50%; RuleTaker: 99.94%; SimpleLogic: 91.88%.",
            "baseline_comparison": "Exceeds largest-class baselines, with near-perfect performance on RuleTaker; somewhat variable performance on LogicNLI and FOLIO versus other large models.",
            "limitations_or_failures": "Does not demonstrate robust cross-dataset transfer; performance remains dataset-dependent.",
            "insights_or_conclusions": "Larger capacity improves on some datasets (notably RuleTaker) but does not imply acquisition of general logical reasoning abilities transferable across FOL/PC datasets.",
            "uuid": "e8646.2",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "RoBERTa-base",
            "name_full": "RoBERTa (base)",
            "brief_description": "RoBERTa-base, an encoder transformer pretrained with dynamic masked language modeling, was fine-tuned on multiple logical reasoning datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "RoBERTa-base",
            "model_description": "Encoder-only transformer (RoBERTa family) employed as a fine-tuning backbone for hypothesis classification; RoBERTa's pretraining is noted as dynamic masked language modeling.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same as above: predict logical relation labels from premises+rules and hypothesis in natural language across FOL and PC datasets.",
            "method_or_approach": "Fine-tuning with [CLS] embedding linear head; cross-probing and layerwise probing performed in-depth for RoBERTa-large (RoBERTa family chosen for detailed analyses).",
            "performance": "FOLIO: 58.82%; LogicNLI: 62.90%; RuleTaker: 98.04%; SimpleLogic: 92.87%.",
            "baseline_comparison": "Outperforms largest-class baseline substantially; near best among base models on several datasets, but RoBERTa-large exceeds it.",
            "limitations_or_failures": "Performance still dataset-specific; pretrained RoBERTa without fine-tuning shows limited capability (per cross-probing results).",
            "insights_or_conclusions": "RoBERTa variants fine-tuned can solve many dataset instances, but the paper shows that fine-tuned RoBERTa models do not develop a generalized logical reasoning ability transferable across datasets.",
            "uuid": "e8646.3",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "RoBERTa-large",
            "name_full": "RoBERTa (large)",
            "brief_description": "RoBERTa-large is the primary model analyzed in-depth (cross-probing and layerwise probing) after fine-tuning on logical reasoning datasets; it achieved the best aggregate performance in the paper's experiments.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "RoBERTa-large",
            "model_description": "A large-capacity RoBERTa encoder-style transformer (pretrained with dynamic masked language modeling) used as the focal model for detailed experiments including fine-tuning, cross-probing (frozen body + probe), and layerwise probing to inspect where task knowledge resides.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Predict logical labels (FOLIO: True/False/Unknown; LogicNLI: Entailment/Contradiction/Neutral/Paradox; RuleTaker & SimpleLogic: True/False) from premises+rules and hypothesis in NL; tasks test FOL and PC reasoning with varying linguistic complexity.",
            "method_or_approach": "Fine-tuning (body + classification head) for primary evaluation; cross-probing: remove fine-tuned head, freeze transformer body, train small probes (1-layer and 3-layer) on top to test recoverability and transfer; layerwise probing: place probes on [CLS] outputs at each layer to identify where reasoning-related signals appear.",
            "performance": "FOLIO: 64.71%; LogicNLI: 72.70%; RuleTaker: 99.78%; SimpleLogic: 90.83% (Table 2). Pretrained probe baselines (RoBERTa pretrained, no fine-tuning) gave: FOLIO 32.88%, LogicNLI 25.54%, RuleTaker 50.01%, SimpleLogic 61.19% (Table 3). Cross-probing of RoBERTa-large fine-tuned models shows large gains on same-dataset probes but limited transfer to other datasets (Table 3).",
            "baseline_comparison": "Substantially above 'largest class' baselines (35.29%, 25.00%, 50.02%, 50.03%); fine-tuning provides large improvements vs pretrained model, but probes on different datasets show limited transfer (fine-tuned-on-X probed-on-Y much worse than fine-tuned-on-Y).",
            "limitations_or_failures": "Fine-tuned RoBERTa-large exhibits limited generalization across datasets (models tend to learn dataset-specific statistical features); pretrained RoBERTa has little to no intrinsic logical reasoning ability; probing shows that task-specific signals concentrate in higher layers, indicating specialized rather than generalizable representations; SimpleLogic may be solvable by shallow heuristics.",
            "insights_or_conclusions": "Although RoBERTa-large can be fine-tuned to high accuracy on particular logical benchmarks (near-perfect on RuleTaker), cross-probing and layerwise probing indicate that the learned ability is dataset-specific and primarily encoded in higher layers, implying lack of emergent, general logical reasoning in these encoder-style LMs.",
            "uuid": "e8646.4",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Longformer-base",
            "name_full": "Longformer (base)",
            "brief_description": "Longformer-base (a transformer variant for long sequences) was fine-tuned and evaluated on the logical reasoning benchmarks.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "Longformer-base",
            "model_description": "Transformer variant optimized for long inputs (Longformer family) used in fine-tuning experiments for hypothesis classification on long logical contexts.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same hypothesis-classification tasks; datasets include long-context examples (LogicNLI and SimpleLogic have long average words/premises).",
            "method_or_approach": "Fine-tuning with [CLS] classification head on each dataset; included to test model families that handle long contexts.",
            "performance": "FOLIO: 62.75%; LogicNLI: 58.05%; RuleTaker: 99.92%; SimpleLogic: 94.21%.",
            "baseline_comparison": "Well above largest-class baselines; strong performance on long-context datasets like SimpleLogic where it achieved the highest SimpleLogic score in Table 2 (94.21%).",
            "limitations_or_failures": "Like other models, transfer/generalization across different datasets was not demonstrated; performance can vary by dataset despite long-context capabilities.",
            "insights_or_conclusions": "Models specialized for long contexts can achieve high accuracy on some logical tasks, but still appear to rely on dataset-specific cues rather than generalized logical rule-following.",
            "uuid": "e8646.5",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "Longformer-large",
            "name_full": "Longformer (large)",
            "brief_description": "Longformer-large was fine-tuned across the logical benchmarks and evaluated similarly to other large encoder variants.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "Longformer-large",
            "model_description": "Large-capacity Longformer variant used for hypothesis classification on the four datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "As above, with some datasets requiring handling of many premises/long inputs.",
            "method_or_approach": "Fine-tuning with classification head; same optimization/hyperparameter regime.",
            "performance": "FOLIO: 62.25%; LogicNLI: 74.15%; RuleTaker: 99.94%; SimpleLogic: 92.37%.",
            "baseline_comparison": "Substantially better than largest-class baselines; notably high LogicNLI performance (74.15%).",
            "limitations_or_failures": "Even with strong per-dataset performance, cross-dataset transfer not guaranteed; results consistent with dataset-specific learning hypothesis.",
            "insights_or_conclusions": "Longformer-large demonstrates that long-input architectures can perform well on FOL datasets, but do not necessarily learn transferable logical reasoning abilities from fine-tuning.",
            "uuid": "e8646.6",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeBERTa-xsmall",
            "name_full": "DeBERTa (xsmall)",
            "brief_description": "A very small DeBERTa variant tested by fine-tuning for hypothesis classification on the logical datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "DeBERTa-xsmall",
            "model_description": "A small-capacity DeBERTa-family transformer evaluated in fine-tuning experiments on the logical reasoning datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same hypothesis classification tasks spanning FOL and PC.",
            "method_or_approach": "Fine-tuning with linear classification head on [CLS] embedding.",
            "performance": "FOLIO: 54.41%; LogicNLI: 65.30%; RuleTaker: 99.81%; SimpleLogic: 91.67%.",
            "baseline_comparison": "Consistently above largest-class baselines; competitive on LogicNLI and near-perfect on RuleTaker.",
            "limitations_or_failures": "Shows dataset-dependent performance; no evidence of improved cross-dataset transfer.",
            "insights_or_conclusions": "Even small DeBERTa variants can reach high accuracy on restricted tasks, but their success does not imply general logical competence.",
            "uuid": "e8646.7",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeBERTa-small",
            "name_full": "DeBERTa (small)",
            "brief_description": "Small DeBERTa variant fine-tuned on logical reasoning datasets and evaluated for hypothesis classification.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "DeBERTa-small",
            "model_description": "DeBERTa family model (small size) fine-tuned for hypothesis classification on FOL/PC datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same tasks as above.",
            "method_or_approach": "Fine-tuning as per Section 4; [CLS]-based classification head.",
            "performance": "FOLIO: 53.43%; LogicNLI: 59.65%; RuleTaker: 95.23%; SimpleLogic: 93.23%.",
            "baseline_comparison": "Above largest-class baseline; somewhat lower on RuleTaker than many other models but strong on SimpleLogic.",
            "limitations_or_failures": "Inconsistent per-dataset performance and limited generalization.",
            "insights_or_conclusions": "Per-dataset fine-tuning yields high task accuracy for some configurations but not general logical reasoning.",
            "uuid": "e8646.8",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeBERTa-base",
            "name_full": "DeBERTa (base)",
            "brief_description": "DeBERTa-base, a mid-size DeBERTa variant, evaluated via fine-tuning on the logical benchmarks.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "DeBERTa-base",
            "model_description": "Base-capacity DeBERTa transformer used as a fine-tuning backbone for hypothesis classification across FOL/PC datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "See above.",
            "method_or_approach": "Fine-tuning [CLS] embedding classifier; same training regime as other models.",
            "performance": "FOLIO: 60.78%; LogicNLI: 66.70%; RuleTaker: 99.81%; SimpleLogic: 93.72%.",
            "baseline_comparison": "Strongly above largest-class baselines; high accuracy across multiple datasets.",
            "limitations_or_failures": "Like others, gains are dataset-specific; transfer not validated for this model in-depth.",
            "insights_or_conclusions": "DeBERTa-base shows robust per-dataset performance but no evidence in paper of learned general logical reasoning.",
            "uuid": "e8646.9",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeBERTa-large",
            "name_full": "DeBERTa (large)",
            "brief_description": "A large DeBERTa variant fine-tuned and evaluated on the logical benchmarks; achieved among the highest LogicNLI scores in the table.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "DeBERTa-large",
            "model_description": "Large-capacity DeBERTa model used for fine-tuning on logical datasets; reported strong performance on LogicNLI.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same hypothesis-classification tasks over FOL/PC datasets.",
            "method_or_approach": "Fine-tuning with classification head; same hyperparameter regimes.",
            "performance": "FOLIO: 64.71%; LogicNLI: 84.70%; RuleTaker: 99.97%; SimpleLogic: 93.72%.",
            "baseline_comparison": "Substantially above largest-class baselines; highest LogicNLI accuracy among listed models (84.70%).",
            "limitations_or_failures": "Despite very high in-dataset performance, cross-dataset transfer not demonstrated; potential dataset-specific overfitting.",
            "insights_or_conclusions": "Large DeBERTa can achieve excellent scores on some FOL benchmarks, but the paper cautions this does not imply acquisition of general logical rules.",
            "uuid": "e8646.10",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeBERTa-xlarge",
            "name_full": "DeBERTa (xlarge)",
            "brief_description": "An extra-large DeBERTa variant included in experiments; showed anomalous low performance on some datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "DeBERTa-xlarge",
            "model_description": "A very large DeBERTa-family model evaluated via fine-tuning on the logical benchmarks; reported results indicate inconsistent performance across datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same as above.",
            "method_or_approach": "Fine-tuning; [CLS] classification head.",
            "performance": "FOLIO: 62.25%; LogicNLI: 25.00%; RuleTaker: 99.99%; SimpleLogic: 49.96%.",
            "baseline_comparison": "Some reported scores (LogicNLI 25.00%, SimpleLogic 49.96%) are near or below the largest-class baseline, indicating failed training/instability for this configuration.",
            "limitations_or_failures": "Reported anomalous/very low performance on LogicNLI and SimpleLogic suggests instability or failure modes for this particular large checkpoint/configuration; paper notes such exceptions among model results.",
            "insights_or_conclusions": "Large models do not uniformly improve logical reasoning; some very large variants may underperform or suffer instabilities, reinforcing the paper's caution about concluding general reasoning ability from single high scores.",
            "uuid": "e8646.11",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "DeBERTa-xxlarge",
            "name_full": "DeBERTa (xxlarge)",
            "brief_description": "An extra-extra-large DeBERTa variant included in experiments; displayed notably poor results on several datasets per Table 2.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "DeBERTa-xxlarge",
            "model_description": "Very large DeBERTa checkpoint evaluated in fine-tuning experiments; results indicate unexpected low accuracies on some tasks.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same hypothesis-classification tasks over FOL/PC datasets.",
            "method_or_approach": "Fine-tuning; [CLS] classification head.",
            "performance": "FOLIO: 71.57%; LogicNLI: 25.00%; RuleTaker: 50.02%; SimpleLogic: 50.04%.",
            "baseline_comparison": "FOLIO performance high (71.57%) but other dataset scores around largest-class baseline (LogicNLI 25.00%, RuleTaker ~50%), indicating inconsistent behavior across datasets.",
            "limitations_or_failures": "Large inconsistencies indicate possible optimization/training problems or overfitting to dataset artifacts for this checkpoint; highlights that bigger models do not guarantee uniformly better logical reasoning.",
            "insights_or_conclusions": "Extremely large variants can show unstable outcomes; the paper highlights such anomalies as caveats when interpreting model size vs reasoning ability.",
            "uuid": "e8646.12",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ALBERT-base",
            "name_full": "ALBERT (base)",
            "brief_description": "ALBERT-base was evaluated by fine-tuning for hypothesis classification on the logical datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "ALBERT-base",
            "model_description": "Parameter-reduced encoder transformer (ALBERT family) fine-tuned and evaluated on FOL/PC datasets for hypothesis classification.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "As above.",
            "method_or_approach": "Fine-tuning full model plus linear classification head on [CLS] embedding.",
            "performance": "FOLIO: 57.35%; LogicNLI: 66.80%; RuleTaker: 99.91%; SimpleLogic: 92.17%.",
            "baseline_comparison": "Above largest-class baselines and comparable to other base families on many datasets.",
            "limitations_or_failures": "No evidence of robust cross-dataset reasoning; similar constraints as other encoder families.",
            "insights_or_conclusions": "ALBERT-base performs well on restricted tasks but is subject to the same dataset-specific learning patterns identified for other models.",
            "uuid": "e8646.13",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ALBERT-large",
            "name_full": "ALBERT (large)",
            "brief_description": "ALBERT-large was fine-tuned on logical benchmarks and evaluated similarly to other large encoder variants.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "ALBERT-large",
            "model_description": "Larger ALBERT variant used as fine-tuning backbone for hypothesis classification.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "See above.",
            "method_or_approach": "Fine-tuning with [CLS]-based classification head.",
            "performance": "FOLIO: 56.37%; LogicNLI: 66.20%; RuleTaker: 99.88%; SimpleLogic: 91.53%.",
            "baseline_comparison": "Outperforms largest-class baselines; similar pattern to ALBERT-base.",
            "limitations_or_failures": "Dataset-specific learning and limited transfer remain concerns.",
            "insights_or_conclusions": "ALBERT variants behave like other encoder-style LMs: high per-dataset performance on constrained tasks, but lacking evidence of generalizable logical reasoning.",
            "uuid": "e8646.14",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ALBERT-xlarge",
            "name_full": "ALBERT (xlarge)",
            "brief_description": "An extra-large ALBERT checkpoint evaluated in experiments that showed uneven performance on FOLIO and other datasets.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "ALBERT-xlarge",
            "model_description": "Large-capacity ALBERT variant fine-tuned on the logical datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same tasks as above.",
            "method_or_approach": "Fine-tuning; [CLS] classification head.",
            "performance": "FOLIO: 38.73%; LogicNLI: 65.10%; RuleTaker: 99.97%; SimpleLogic: 90.05%.",
            "baseline_comparison": "FOLIO performance is markedly lower than largest-class baseline, indicating training instability or dataset mismatch for this checkpoint.",
            "limitations_or_failures": "Anomalous low scores on FOLIO suggest some large checkpoints can fail to learn or generalize properly.",
            "insights_or_conclusions": "Not all larger variants uniformly improve performance; anomalies reinforce the need for careful evaluation beyond single-dataset accuracy.",
            "uuid": "e8646.15",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "ALBERT-xxlarge",
            "name_full": "ALBERT (xxlarge)",
            "brief_description": "An xxlarge ALBERT model tested in fine-tuning experiments that achieved high LogicNLI performance in Table 2.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "ALBERT-xxlarge",
            "model_description": "Very large ALBERT variant included in the experimental sweep and fine-tuned on the logical reasoning datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same as previous entries.",
            "method_or_approach": "Fine-tuning approach consistent with other models.",
            "performance": "FOLIO: 56.86%; LogicNLI: 93.90%; RuleTaker: 99.94%; SimpleLogic: 92.24%.",
            "baseline_comparison": "Very high LogicNLI performance (93.90%) compared to largest-class baseline; but as with other high scores, paper cautions about generalizability.",
            "limitations_or_failures": "Despite very high in-dataset accuracy, the paper's cross-probing and layerwise probing suggest such successes may not reflect generalized logical reasoning.",
            "insights_or_conclusions": "Extremely high scores on certain datasets should be interpreted cautiously; the architecture/size alone does not guarantee transferable logical reasoning.",
            "uuid": "e8646.16",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "XLM-RoBERTa-base",
            "name_full": "XLM-RoBERTa (base)",
            "brief_description": "The multilingual XLM-RoBERTa base model was included and fine-tuned for the hypothesis classification tasks.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "XLM-RoBERTa-base",
            "model_description": "Multilingual RoBERTa-family encoder (base) used as a fine-tuning backbone for the logical datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "As above.",
            "method_or_approach": "Fine-tuning with [CLS] classifier.",
            "performance": "FOLIO: 55.88%; LogicNLI: 45.20%; RuleTaker: 97.64%; SimpleLogic: 91.74%.",
            "baseline_comparison": "Generally above largest-class baselines but lower than many monolingual encoder variants on some datasets.",
            "limitations_or_failures": "Lower LogicNLI performance relative to many monolingual models; still subject to dataset-specific learning.",
            "insights_or_conclusions": "Multilingual pretraining does not, by itself, confer robust, general logical reasoning; performance remains dataset-dependent.",
            "uuid": "e8646.17",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "XLM-RoBERTa-large",
            "name_full": "XLM-RoBERTa (large)",
            "brief_description": "XLM-RoBERTa-large was fine-tuned and included in the model sweep for the logical benchmarks.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "XLM-RoBERTa-large",
            "model_description": "Large multilingual RoBERTa-family model fine-tuned for hypothesis classification tasks spanning FOL/PC datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same as above.",
            "method_or_approach": "Fine-tuning with classification head.",
            "performance": "FOLIO: 62.75%; LogicNLI: 65.45%; RuleTaker: 99.95%; SimpleLogic: 91.74%.",
            "baseline_comparison": "Above largest-class baselines and comparable to other large models on many datasets.",
            "limitations_or_failures": "No evidence of cross-dataset transfer improvement; dataset-specific learning patterns persist.",
            "insights_or_conclusions": "Large multilingual models can reach high per-dataset accuracies but still do not demonstrate general logical reasoning per the paper's probing analyses.",
            "uuid": "e8646.18",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "XLNet-base",
            "name_full": "XLNet (base)",
            "brief_description": "XLNet-base, an autoregressive-permuted-pretraining transformer included in the sweep, was fine-tuned for the hypothesis classification benchmarks.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "XLNet-base",
            "model_description": "XLNet-family model (base) evaluated by fine-tuning on the logical reasoning datasets; included among the model families in the experimental sweep.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "Same tasks as above.",
            "method_or_approach": "Fine-tuning with a [CLS]-based linear classifier on top.",
            "performance": "FOLIO: 58.33%; LogicNLI: 55.00%; RuleTaker: 99.19%; SimpleLogic: 94.28%.",
            "baseline_comparison": "Above largest-class baselines; strong on RuleTaker and SimpleLogic.",
            "limitations_or_failures": "As others, suffers from dataset-specific learning and limited transferability (cross-probing analyses focus on RoBERTa but trend applies broadly).",
            "insights_or_conclusions": "XLNet family can achieve high accuracy on certain logical datasets but does not escape the generalization limitations highlighted for encoder-style LMs in the paper.",
            "uuid": "e8646.19",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        },
        {
            "name_short": "XLNet-large",
            "name_full": "XLNet (large)",
            "brief_description": "XLNet-large was fine-tuned and evaluated on the logical reasoning datasets, showing very high scores on RuleTaker.",
            "citation_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
            "mention_or_use": "use",
            "model_name": "XLNet-large",
            "model_description": "Large-capacity XLNet model included in the experimental sweep and fine-tuned for hypothesis classification tasks on FOL/PC datasets.",
            "model_size": null,
            "reasoning_task_name": "Hypothesis classification on FOLIO, LogicNLI, RuleTaker, SimpleLogic",
            "reasoning_task_description": "See above.",
            "method_or_approach": "Fine-tuning with a [CLS] classifier.",
            "performance": "FOLIO: 58.33%; LogicNLI: 71.40%; RuleTaker: 99.86%; SimpleLogic: 92.94%.",
            "baseline_comparison": "Significantly above largest-class baselines and competitive with other large models on many datasets.",
            "limitations_or_failures": "Similar dataset-specific learning and transfer limitations apply.",
            "insights_or_conclusions": "Large autoregressive/pretraining-style variants can match the per-dataset performance of large encoder models but still lack evidence of a general logical reasoning faculty.",
            "uuid": "e8646.20",
            "source_info": {
                "paper_title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models",
                "publication_date_yy_mm": "2023-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Transformers as Soft Reasoners over Language",
            "rating": 2,
            "sanitized_title": "transformers_as_soft_reasoners_over_language"
        },
        {
            "paper_title": "Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI",
            "rating": 2,
            "sanitized_title": "diagnosing_the_firstorder_logical_reasoning_ability_through_logicnli"
        },
        {
            "paper_title": "FOLIO: Natural Language Reasoning with First-Order Logic",
            "rating": 2,
            "sanitized_title": "folio_natural_language_reasoning_with_firstorder_logic"
        },
        {
            "paper_title": "On the Paradox of Learning to Reason from Data",
            "rating": 2,
            "sanitized_title": "on_the_paradox_of_learning_to_reason_from_data"
        },
        {
            "paper_title": "Negation, Coordination, and Quantifiers in Contextualized Language Models",
            "rating": 1,
            "sanitized_title": "negation_coordination_and_quantifiers_in_contextualized_language_models"
        },
        {
            "paper_title": "Language Models Show Human-Like Content Effects on Reasoning",
            "rating": 1,
            "sanitized_title": "language_models_show_humanlike_content_effects_on_reasoning"
        }
    ],
    "cost": 0.024376,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Assessing Logical Reasoning Capabilities of Encoder-Only Transformer Models</h1>
<p>Paulo Pirozelli ${ }^{1[0000-0002-4714-287 X]}$, Marcos M. Jos ${ }^{2[0000-0003-4663-4386]}$, Paulo de Tarso P. Filho ${ }^{2[0009-0000-8085-795 X]}$, Anarosa A. F.<br>Brando ${ }^{2[0000-0001-8992-4768]}$, and Fabio G. Cozman ${ }^{2[0000-0003-4077-4935]}$<br>${ }^{1}$ Instituto Mau de Tecnologia<br>${ }^{2}$ Universidade de So Paulo<br>${ }^{3}$ University of Alberta<br>paulo.silva@maua.br</p>
<h4>Abstract</h4>
<p>Transformer models have shown impressive abilities in natural language tasks such as text generation and question answering. Still, it is not clear whether these models can successfully conduct a rule-guided task such as logical reasoning. In this paper, we investigate the extent to which encoder-only transformer language models (LMs) can reason according to logical rules. We ask whether these LMs can deduce theorems in propositional calculus and first-order logic, if their relative success in these problems reflects general logical capabilities, and which layers contribute the most to the task. First, we show for several encoder-only LMs that they can be trained, to a reasonable degree, to determine logical validity on various datasets. Next, by cross-probing fine-tuned models on these datasets, we show that LMs have difficulty in transferring their putative logical reasoning ability, which suggests that they may have learned dataset-specific features instead of a general capability. Finally, we conduct a layerwise probing experiment, which shows that the hypothesis classification task is mostly solved through higher layers.</p>
<p>Keywords: Logical reasoning $\cdot$ Language models $\cdot$ Transformer $\cdot$ Probing</p>
<h2>1 Introduction</h2>
<p>Transformer models are remarkably effective at a wide range of natural language processing (NLP) tasks, such as question answering, summarization, and text generation. By and large, these abilities are the result of specific training processes, where a language model is fine-tuned on a task-specific dataset. Curiously, encoder-only transformer models (LMs) also exhibit implicit linguistic and cognitive abilities for which they were not directly supervised. Such LMs have been shown to encode information on tense and number [7], anaphora and determiner-noun agreement [49], semantic roles [46], syntactic dependencies [32], relational knowledge [8], and spatiotemporal representation [15].</p>
<p>Given that logical reasoning is a core component of intelligence, human or otherwise [38], it is worth investigating the capabilities of encoder-only transformer models in executing tasks that necessitate such reasoning. Understanding whether LMs can</p>
<p>solve logical problems, and the manner in which they tackle these problems, may enable us to understand the extent to which their inferences arise from reasoning rather than purely associative memory. This understanding is crucial for developing mechanisms that facilitate the generation of consistent outputs, whether in a neural-symbolic fashion or through the improvement of model architectures.</p>
<p>The goal of this paper is, thus, to assess the ability of encoder-only transformer models to reason according to the rules of logic - understood here as deductive arguments expressable in propositional calculus or first-order logic. We examine three main questions throughout this paper: i) Can enconder-only transformer models perform logical reasoning tasks?; ii) How general is this ability?; and iii) What layers better contribute to solving these tasks?</p>
<p>Section 2 reviews the work on transformers' logical reasoning abilities, as well as the function of probing in uncovering latent knowledge. Next, we gather and describe four datasets grounded on logical deduction (sec. 3). In a first batch of experiments, we conduct a systematic comparison of encoder-only transformer models on these datasets (sec. 4). Section 5 then investigates whether the performance in the previous task could be attributed to some general ability and whether the reasoning learned from one dataset could be transferred to a similar dataset. Finally, we perform a layerwise probing to understand which layers are responsible for solving logical deduction problems (sec. $6) .^{4}$</p>
<h1>2 Related Work</h1>
<p>Logical Reasoning in Transformer Models Transformer models are powerful enough to solve logical reasoning tasks expressed in natural language [4|55|47|17]. Yet, it is not clear if these models have actually mastered logical reasoning. LMs seem to inevitably rely on statistical artifacts to deduce theorems, rather than on general, rule-based relationships [55]. They use shortcuts to solve hypothesis classification problems, leading to vulnerabilities in reasoning (e.g., LMs are fooled when hypotheses appear within rules), and making them susceptible to irrelevant (logically consistent) perturbations [14].</p>
<p>Studies focused on functional words close to logical operators have identified similar shortcomings in LMs' reasoning capabilities. Transformers struggle to deal with negation, predicting similar probabilities to a sentence and its negation [25|22]. [24] extend these findings to conjunction, disjunction, and existential and universal quantification, showing that expressions associated with these operations are frequently dominated by semantically rich words. Transformers also fail in modeling semi-functionalsemi-content words in general, such as quantifiers and modals [41]. Tangentially to logic, [9] show that transformer models do not always properly handle compositionality: sometimes a translation is more local than desired (treating idioms as regular constructions), sometimes it is excessively global (paying attention to irrelevant parts of the sentence). Moreover, there is evidence that the way LMs compose sentences does not align well with human judgment [28].</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Large LMs have also been assessed for their logical reasoning capabilities. Despite their impressive achievements in numerous tasks, these models still struggle with multistep and compositional problems [36|12]. Although good at individual deductions, large LMs struggle with proof planing: when many valid proof steps are available, they often take the wrong path [40]. Large LMs also appear to suffer from human-like biases in logical tasks: they perform significantly worse when the semantic content is too abstract or conflicts with prior knowledge [10|44].</p>
<p>Probing Tasks Probing is a technique used to discover if a model has acquired certain type of knowledge. In probing, a dataset that encodes a particular property (e.g., part-of-speech) is used to train a classifier (the probe), taking the representations produced by the original model as inputs for the classification [1]. As the LM is not further trained on the task, as it would be in fine-tuning, the probe's performance depends on whether the information about that property had already been encoded by the model. Thus, success in the task provides some evidence that the model has stored such knowledge in its parameters. Probing can be used to examine several components of LMs, such as embeddings, attention heads, and feedforward computations.</p>
<p>Transformer models have been extensively studied through probing [1]. Most attention has been given to BERT, which gave rise to a large literature on the properties encoded by this model [37]. RoBERTa has also been studied in some detail through probing; e.g., what abilities that model learns during training [56|30] and its knowledge of semantic structural information [51].</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Fig. 1. Examples of logical reasoning arguments. The argument at the top is from FOLIO, a manually-written dataset; the one at the bottom is from RuleTaker, a dataset that uses a semisynthetic approach.</p>
<p>Table 1. Main features of the logical reasoning datasets. FOL stands for first-order logic, PC for propositional calculus, and CI for conjunctive implication. The labels in the datasets are as follows: FOLIO (False, True, Unknown), LogicNLI (Contradiction, Entailment, Neutral, Paradox), RuleTaker (False, True), SimpleLogic (False, True). The average number of premises and the average number of words per argument refer to the training set statistics. Appendix B shows the full dataset label distribution.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Size (train/val/test)</th>
<th style="text-align: left;">Scope</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Label</th>
<th style="text-align: left;">Avg. Premises</th>
<th style="text-align: left;">Avg. Words</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FOLIO</td>
<td style="text-align: left;">$1003 / 204 /-$</td>
<td style="text-align: left;">FOL</td>
<td style="text-align: left;">Manual</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">5.23</td>
<td style="text-align: left;">64.01</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLI</td>
<td style="text-align: left;">$16000 / 2000 / 2000$</td>
<td style="text-align: left;">FOL</td>
<td style="text-align: left;">Semi-synt.</td>
<td style="text-align: left;">4</td>
<td style="text-align: left;">24</td>
<td style="text-align: left;">211.86</td>
</tr>
<tr>
<td style="text-align: left;">RuleTaker</td>
<td style="text-align: left;">$27363 / 3899 / 7793$</td>
<td style="text-align: left;">FOL (CI)</td>
<td style="text-align: left;">Semi-synt.</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">16.30</td>
<td style="text-align: left;">100.06</td>
</tr>
<tr>
<td style="text-align: left;">SimpleLogic</td>
<td style="text-align: left;">$11341 / 1418 / 1417$</td>
<td style="text-align: left;">PC (CI)</td>
<td style="text-align: left;">Synthetic</td>
<td style="text-align: left;">2</td>
<td style="text-align: left;">60.76</td>
<td style="text-align: left;">467.45</td>
</tr>
</tbody>
</table>
<h1>3 Logical Reasoning Datasets</h1>
<p>In assessing logical reasoning, we restricted our analysis to datasets related to propositional calculus (PC) and first-order logic (FOL), with FOL being an extension of PC that includes predicates and quantifiers. These logical systems offer a powerful formalism that balances simplicity and expressivity in representing and reasoning about statements and relationships. They are suitable for capturing a wide range of knowledge and formalizing a large number of domains.</p>
<p>In addition to being expressible in PC or FOL, we selected datasets that satisfied three other properties: i) observations had to be as self-contained as possible; ii) sentences needed to have corresponding translations in both logical formalism and natural language; and iii) hypotheses had to be declarative sentences. The first property aims to decouple logic from background knowledge in order to assess pure reasoning capabilities. While using natural language examples means implicit knowledge can never be completely erased, we opted for datasets that minimized this by explicitly stating prior knowledge or by using inference patterns where resorting to prior knowledge is unnecessary. For this reason, we only considered pure logical datasets and did not include other forms of reasoning such as scientific reasoning [3,26,54], mathematical reasoning [5,34,20], counterfactual reasoning [35,52,33], planning [48], inductive reasoning [42,44], and abductive reasoning [44]. The second property, the translation into logical formalism, allows one to determine unambiguously the logical relationship between premises and hypotheses, while the natural language counterpart allows us to probe the LMs. Hence, we excluded datasets that lacked natural language translations, such as LTL [16]. The last property excluded QA datasets [29,43], as they required the understanding of several types of questions (e.g., who, where) that are entangled with semantic and contextual knowledge (e.g., that "Alice" is the name of a person).</p>
<p>In the end, four logical reasoning datasets were selected: FOLIO [17], LogicNLI [47], RuleTaker [4], and SimpleLogic [55]. Examples of arguments can be seen in Figure 1. These datasets cover a wide range of variations in terms of construction (manual, semi-synthetic, synthetic), alignment with common sense, linguistic variability, and scope (PC, full or partial FOL). For instance, FOLIO is human-written; LogicNLI and RuleTaker both use a template which is then manually edited; and SimpleLogic is fully synthetic. FOLIO and LogicNLI encompass the full spectrum of FOL; RuleTaker is</p>
<p>expressible in FOL but only covers negation and conjunctive implications, ${ }^{5}$ and SimpleLogic is restricted to conjunctive implications in PC. Regarding the number of labels, RuleTaker and SimpleLogic have True and False labels; FOLIO includes an Unknown label; and LogicNLI admits a fourth possibility, Paradox, where both a hypothesis and its negation can be deduced from the premises. Table 1 summarizes the main statistics of the datasets. Appendix A provides a detailed description of the four datasets.</p>
<p>For all datasets, inputs are formatted as "fact ${ }<em 2="2">{1}$. fact $</em>$. . . fact $<em 1="1">{n}$. rule ${ }</em>$. rule $<em m="m">{2}$. . . rule $</em>$ <SEP> hypothesis.", for which a label must be predicted. The output is simply a probability distribution over the possible labels, which varies from dataset to dataset.</p>
<h1>4 Testing LMs for Hypothesis Classification</h1>
<p>The performance of LMs in logical reasoning, including in the datasets described in the previous section, has been studied in scattered experiments without a clear unified context that allows direct comparison. Due to this, we decided to fine-tune a wide range of pretrained encoder-only transformer models on those datasets. ${ }^{6}$ We modeled this problem as a hypothesis classification task, where the goal is to determine the logical relationship between a set of premises and a conclusion. To ensure comprehensive coverage, eight families of LMs were assessed: DistilBERT [39], BERT [11], RoBERTa [31], Longformer [2], DeBERTa [19], AlBERT [27], XLM-RoBERTa [6], and XLNet [53]. The full list of models is displayed in the left column of Table 2.</p>
<p>To fine-tune our models, we used a classification head with a single linear layer of dimension (embedding-length, labels), and applied a dropout of 0.5 to the linear operation. As input for the classification head, we used the last hidden embedding of the [CLS] token, as is customary for classification tasks in NLP. Sequences were padded and truncated to the maximum length allowed by each LM. Models were trained for up to 50 epochs, using early stopping with a patience of 5 epochs. We used Adam as our optimizer (with $\beta=[0.9,0.999]$ and no weight decay) and experimented with two different learning rates (1e-5 and 1e-6). Models were selected based on the loss for the validation set, with results reported for the test set. The only exception is FOLIO, where only the validation set is publicly available; results are thus reported for this set. We report accuracy as the standard metric, as classes in the datasets are balanced. Table 2 displays the best result achieved in each case. The best and worst scores for each dataset are highlighted in blue and red, respectively. We also provide a largest class baseline for comparison.</p>
<p>Results show that the LMs were able to classify hypotheses with reasonable success. Almost all models came close to solving RuleTaker and achieved an accuracy above $90 \%$ in SimpleLogic. Results were comparatively lower for FOLIO and LogicNLI, but the LMs generally surpassed the largest class baselines by a considerable margin. A</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 2. Accuracy comparison among several encoder-only transformer models for the hypothesis classification task across four datasets: FOLIO, LogicNLI, RuleTaker, and SimpleLogic. The models are listed in the left column of the table. The best and worst scores for each dataset are highlighted in blue and red, respectively. "Largest class" refers to the accuracy achieved by always selecting the class with the highest frequency in the training set. Results for RoBERTa-large, selected for subsequent analyses, are in bold.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">FOLIO</th>
<th style="text-align: center;">LogicNLI</th>
<th style="text-align: center;">RuleTaker</th>
<th style="text-align: center;">SimpleLogic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">DistilBERT</td>
<td style="text-align: center;">50.98</td>
<td style="text-align: center;">36.20</td>
<td style="text-align: center;">86.55</td>
<td style="text-align: center;">93.58</td>
</tr>
<tr>
<td style="text-align: left;">BERT base</td>
<td style="text-align: center;">52.45</td>
<td style="text-align: center;">48.75</td>
<td style="text-align: center;">98.91</td>
<td style="text-align: center;">92.31</td>
</tr>
<tr>
<td style="text-align: left;">BERT large</td>
<td style="text-align: center;">61.76</td>
<td style="text-align: center;">42.50</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">91.88</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa base</td>
<td style="text-align: center;">58.82</td>
<td style="text-align: center;">62.90</td>
<td style="text-align: center;">98.04</td>
<td style="text-align: center;">92.87</td>
</tr>
<tr>
<td style="text-align: left;">RoBERTa large</td>
<td style="text-align: center;">$\mathbf{6 4 . 7 1}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 7 0}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 7 8}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 8 3}$</td>
</tr>
<tr>
<td style="text-align: left;">Longformer base</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">58.05</td>
<td style="text-align: center;">99.92</td>
<td style="text-align: center;">94.21</td>
</tr>
<tr>
<td style="text-align: left;">Longformer large</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">74.15</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">92.37</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa xsmall</td>
<td style="text-align: center;">54.41</td>
<td style="text-align: center;">65.30</td>
<td style="text-align: center;">99.81</td>
<td style="text-align: center;">91.67</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa small</td>
<td style="text-align: center;">53.43</td>
<td style="text-align: center;">59.65</td>
<td style="text-align: center;">95.23</td>
<td style="text-align: center;">93.23</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa base</td>
<td style="text-align: center;">60.78</td>
<td style="text-align: center;">66.70</td>
<td style="text-align: center;">99.81</td>
<td style="text-align: center;">93.72</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa large</td>
<td style="text-align: center;">64.71</td>
<td style="text-align: center;">84.70</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">93.72</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa xlarge</td>
<td style="text-align: center;">62.25</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">99.99</td>
<td style="text-align: center;">49.96</td>
</tr>
<tr>
<td style="text-align: left;">DeBERTa xxlarge</td>
<td style="text-align: center;">71.57</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.02</td>
<td style="text-align: center;">50.04</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT base</td>
<td style="text-align: center;">57.35</td>
<td style="text-align: center;">66.80</td>
<td style="text-align: center;">99.91</td>
<td style="text-align: center;">92.17</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT large</td>
<td style="text-align: center;">56.37</td>
<td style="text-align: center;">66.20</td>
<td style="text-align: center;">99.88</td>
<td style="text-align: center;">91.53</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT xlarge</td>
<td style="text-align: center;">38.73</td>
<td style="text-align: center;">65.10</td>
<td style="text-align: center;">99.97</td>
<td style="text-align: center;">90.05</td>
</tr>
<tr>
<td style="text-align: left;">ALBERT xxlarge</td>
<td style="text-align: center;">56.86</td>
<td style="text-align: center;">93.90</td>
<td style="text-align: center;">99.94</td>
<td style="text-align: center;">92.24</td>
</tr>
<tr>
<td style="text-align: left;">XLM-RoBERTa base</td>
<td style="text-align: center;">55.88</td>
<td style="text-align: center;">45.20</td>
<td style="text-align: center;">97.64</td>
<td style="text-align: center;">91.74</td>
</tr>
<tr>
<td style="text-align: left;">XLM-RoBERTa large</td>
<td style="text-align: center;">62.75</td>
<td style="text-align: center;">65.45</td>
<td style="text-align: center;">99.95</td>
<td style="text-align: center;">91.74</td>
</tr>
<tr>
<td style="text-align: left;">XLNet base</td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;">55.00</td>
<td style="text-align: center;">99.19</td>
<td style="text-align: center;">94.28</td>
</tr>
<tr>
<td style="text-align: left;">XLNet large</td>
<td style="text-align: center;">58.33</td>
<td style="text-align: center;">71.40</td>
<td style="text-align: center;">99.86</td>
<td style="text-align: center;">92.94</td>
</tr>
<tr>
<td style="text-align: left;">Largest class</td>
<td style="text-align: center;">35.29</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.02</td>
<td style="text-align: center;">50.03</td>
</tr>
</tbody>
</table>
<p>weaker performance observed in these two datasets was expected, given their greater language variability and broader logical scope; and in the case of FOLIO, its smaller size as well. Overall, the encoder-only transformer models worked relatively well as soft reasoners [4], being able to successfully deduce theorems from premises expressed in natural language. Noteworthy exceptions were the performances of AlBERT-XL in FOLIO, DeBERTa-XL and -XXL in LogicNLI and SimpleLogic, DeBERTa-XXL in RuleTaker, and the overall lower performance of DistilBERT.</p>
<h1>5 Cross-Probing Fine-Tuned LMs</h1>
<p>The encoder-only transformer models showed reasonable performance on the hypothesis classification task, where they were fine-tuned on the logical reasoning datasets. This, however, raises some questions: has the ability to solve this task, whatever it is, been acquired during the fine-tuning stage, or was it present from the start (i.e., from pretraining)? Most importantly, have LMs truly developed a generalized logical reasoning capability? To examine these questions, we run a cross-probing task: we take</p>
<p>Table 3. Results for the cross-probing task. On the left, we present the best fine-tuned RoBERTalarge models for each dataset. The datasets used in the probes are listed at the top. We report only the best result for each probe. Blue cells indicate the probe of a fine-tuned model on the same dataset. In parentheses, we denote the percentage difference from the pretrained model. "Largest class" refers to the accuracy achieved by always selecting the class with the highest frequency in the training set.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">FOLIO</th>
<th style="text-align: center;">LogicNLI</th>
<th style="text-align: center;">RuleTaker</th>
<th style="text-align: center;">SimpleLogic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Pretrained</td>
<td style="text-align: center;">32.88</td>
<td style="text-align: center;">25.54</td>
<td style="text-align: center;">50.01</td>
<td style="text-align: center;">61.19</td>
</tr>
<tr>
<td style="text-align: center;">FOLIO</td>
<td style="text-align: center;">$55.05(+67.42)$</td>
<td style="text-align: center;">$27.28(+6.81)$</td>
<td style="text-align: center;">$60.74(+21.45)$</td>
<td style="text-align: center;">$62.82(+2.66)$</td>
</tr>
<tr>
<td style="text-align: center;">LogicNLI</td>
<td style="text-align: center;">$36.60(+11.31)$</td>
<td style="text-align: center;">$67.95(+166.05)$</td>
<td style="text-align: center;">$69.37(+38.70)$</td>
<td style="text-align: center;">$62.06(+1.42)$</td>
</tr>
<tr>
<td style="text-align: center;">RuleTaker</td>
<td style="text-align: center;">$40.32(+22.62)$</td>
<td style="text-align: center;">$36.01(+40.99)$</td>
<td style="text-align: center;">$99.44(+98.84)$</td>
<td style="text-align: center;">$62.89(+2.77)$</td>
</tr>
<tr>
<td style="text-align: center;">SimpleLogic</td>
<td style="text-align: center;">$32.88(0)$</td>
<td style="text-align: center;">$25.44(-0.39)$</td>
<td style="text-align: center;">$51.02(+2.01)$</td>
<td style="text-align: center;">$92.35(+50.92)$</td>
</tr>
<tr>
<td style="text-align: center;">Largest class</td>
<td style="text-align: center;">35.29</td>
<td style="text-align: center;">25.00</td>
<td style="text-align: center;">50.02</td>
<td style="text-align: center;">50.03</td>
</tr>
</tbody>
</table>
<p>the LMs previously fine-tuned on our logical datasets, as well as a pretrained LM, and probe them on these same datasets. Given the large number of possible tests, we restricted our investigation to a single model, RoBERTa-large, so as to dig deeper on it. This LM demonstrated a suitable balance between performance, consistency among datasets, and training time in the previous tests.</p>
<p>To start, we took the best fine-tuned RoBERTa-large model for each dataset and removed their classification heads, leaving just the transformer blocks, as in a pretrained model. Then, we attached a new classification head to it; i.e., the probe. As in the finetuning stage, we passed the formatted inputs in natural language to the LMs and tried to predict the correct label for a set of premises. However, unlike the fine-tuning stage, only the probe is updated now, while the model's body is kept frozen during the backward pass. The goal is to assess if some logical reasoning ability was learned by the LM without letting the model adapt to the task.</p>
<p>The same training policies from the fine-tuning step were followed in this stage: models were trained with early stopping for up to 50 epochs (patience of 5 epochs) based on the validation loss, using two learning rates (1e-5 and 1e-6). Also, two different classifiers were tested as probes:</p>
<ul>
<li>1-layer A single affine transformation is applied to the embedding of the [CLS] token. The classification head has shape (1024, labels); 1024 being the dimensionality of RoBERTa-large hidden states. We used a dropout of 0.5 before the classifier.</li>
<li>3-layer The [CLS] embedding passes through three consecutive layers of shape (1024, 256), (256, 64), and (256, labels), respectively. We used a dropout of 0.5 in between linear layers and ReLU as the activation function.</li>
</ul>
<p>In the tests, the two probes led to similar results. We take this as strong evidence that the knowledge used in the logical reasoning tasks, whatever it is, can be linearly recovered from the internal representations of RoBERTa-large.</p>
<p>Table 3 displays the best results attained in the cross-probing task. The left column lists the RoBERTa-large fine-tuned models from the previous experiment, while the remaining columns represent the datasets they were probed against. The blue cells along the diagonal indicate instances where models were probed on the same datasets</p>
<p>they were initially fine-tuned on. Percentage differences in accuracy relative to the pretrained case are reported in parentheses. As expected, the fine-tuned model for a specific dataset performed better in that same dataset, albeit less than in the fine-tuning scenario. This makes sense, since in fine-tuning, both the model's body and head are optimized, whereas in a probe the head is in charge of all the learning. These results serve as a sanity check that our probing is working.</p>
<p>The first row in Table 3 contains the scores for the probes with pretrained RoBERTa. Accuracy levels for LogicNLI and RuleTaker closely resemble the largest class baseline, while the result for FOLIO falls below its corresponding baseline. Pretrained RoBERTa only helped to solve SimpleLogic, a dataset constrained to conjunctive implication with minimal language variation. Its pretraining scheme, dynamic masked language modeling, did not equip it with adequate logic-like knowledge to solve complex reasoning problems without specific training.</p>
<p>We can see from this that pretrained RoBERTa appears to have no proper logical reasoning skills. But has it acquired such an ability through fine-tuning on logical datasets? After all, RoBERTa-large was able to solve the hypothesis classification tasks reasonably well after specific training. When analyzing the results of the cross-probing task, however, we may doubt whether a general logical reasoning ability in fact emerged from fine-tuning.</p>
<p>In general, the fine-tuned LMs showed limited transferability when probed on different datasets. Although some gain was achieved compared to the pretrained model, they remained well below what an LM fine-tuned on the same dataset could obtain. SimpleLogic presents an interesting case. The LMs fine-tuned on the other datasets performed similarly to the pretrained model on this dataset. This is despite SimpleLogic covering only a subset of propositional calculus, a domain included in those datasets. One would expect that a model capable of solving more complex problems would be able to reason on this simpler dataset (in terms of logical scope and linguistic variability). At the same time, the LM fine-tuned on SimpleLogic did not exhibit improved performance on the other datasets, indicating a lack of acquired general logical reasoning ability during its training.</p>
<p>Two main conclusions can be drawn from this experiment:</p>
<ol>
<li>If the difference in accuracy between pretrained RoBERTa and the largest class baseline indicates the amount of logical reasoning contained in this LM, then pretrained RoBERTa seems to have very limited or no logical reasoning abilities.</li>
<li>If the difference in accuracy between fine-tuned LMs (when applied to different datasets) and the pretrained RoBERTa model indicates the amount of logical reasoning they acquired in the fine-tuning process, then these LMs have acquired little or no general logical reasoning capability as well, suggesting that they mostly learned statistical features of the datasets. This aligns with other findings for transformer models $[55,12]$.</li>
</ol>
<h1>6 Inspecting Fine-Tuned Models Layerwise</h1>
<p>We now address another question: which parts of the LMs are more capable of solving logical reasoning tasks? To answer this, we probe the different layers of the fine-tuned</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 1-layer classifier and a learning rate of 1e-6.</p>
<p>RoBERTa-large models using the same datasets they were trained on. Our goal is to identify which layers are more effective in deducing hypotheses. We expect this to provide further evidence of what sort of knowledge LMs are using to solve the hypothesis classification task. Similar to the previous experiment, the fine-tuned LMs were frozen, the older classification head was removed, and a probe was trained on top of the layers. More concretely, for each layer $i$ in the model, we passed the premises through the model up to layer $i$, and used the outputted embedding of the [CLS] token at that layer as the input to the probe. As in the previous experiment, only the probe was trained. The same two classifiers from the last experiment were tested. They were positioned on top of the [CLS] token of each layer, with 25 layers in total ( 24 transformer blocks plus the initial embedding layer). We adopted the same configurations from the previous tests: models were trained with early stopping for up to 50 epochs (with a patience of 5 epochs) based on the validation loss, using two learning rates (1e-5 and 1e-6).</p>
<p>Figure 2 shows the accuracy for the probes stacked on the various layers of the finetuned LMs, using a 1-layer classifier and a learning rate of 1e-6 (Appendix C presents graphs for the other configurations). The blue line plots the accuracy on the task for each</p>
<p>layer, while the bars display the differential score for layer ${ }<em i="i">{i}$; i.e., the change in accuracy from layer ${ }</em>$ [45]. The gray line marks the pretrained model baseline, and the black line indicates the score achieved in the cross-probing task.}$ to layer ${ }_{i-1</p>
<p>A similar behavior is exhibited by all models. They remain close to the pretrained baseline in the low and mid layers. Accuracy then grows rapidly in the final layers, achieving a performance equal to the cross-probing baseline. How does this compare to other types of knowledge found in transformer models? Literature on transformers shows that surface information, such as sentence length [23], is mostly captured by lower layers. Middle layers are responsible for processing syntactic knowledge, like syntax trees [21]. Finally, higher layers are responsible for task-specific functions [18] and contextual representations [13].</p>
<p>In our layerwise probing, higher layers were the only ones able to solve the hypothesis classification task better than a pretrained model. This suggests that the knowledge acquired during fine-tuning was connected to dataset-specific features rather than general representations. It also explains why the information was not transferable among datasets. Although indirect, this experiment provides further evidence that encoder-only transformer models do not possess robust logical reasoning capabilities. SimpleLogic was the only case that presented a growth in the initial layers. This behavior may indicate that the dataset is solvable through the use of some heuristics based on shallow statistical features, such as the number of premises, as discussed by [55].</p>
<h1>7 Conclusion</h1>
<p>Logical reasoning is a valuable ability that humans use in thinking, arguing, and planning, as well as a core component of many AI systems. Here, we investigated the role of logical reasoning in encoder-only transformer models. By gathering a number of logical reasoning datasets, we observed that language models can be trained to perform complex logical tasks with relative success. However, upon closer inspection, doubts arose regarding whether these models have truly learned to reason according to logical rules. First, by probing a pretrained RoBERTa-large model with logical reasoning datasets, it became apparent that this language model did not possess intrinsic logical reasoning abilities. Second, models fine-tuned on one dataset struggled to generalize well to other datasets, even within the same domain. This observation suggests that these language models did not acquire robust logical reasoning capabilities even after specific training. Third, the knowledge necessary to solve logical reasoning tasks seems to emerge primarily at higher, more contextual layers, probably linked to statistical features of the datasets rather than deeper representations.</p>
<h2>8 Limitations</h2>
<p>We run experiments for a large variety of encoder-only transformer models in Section 4. However, due to space and time constraints, we focused on RoBERTa-large for the analysis in Sections 5-6. While we expect the same behavior to appear in the other encode-only models, further tests are needed to verify whether conclusions can be reliably extended to them. We have not explored decoder nor encoder-decoder models</p>
<p>either, which could widely extend the number of models to be tested. We cannot rule out the possibility that robust logical reasoning is an emergent ability only manifested in large language models [50]. Additionally, other types of representations, such as attentions and feedforward computations, could be analyzed in relation to logical reasoning. Further work should also focus on other types of logical formalism beyond PC and FOL.</p>
<h1>Acknowledgements</h1>
<p>This work was supported by the Center for Artificial Intelligence USP/IBM/FAPESP (C4AI), jointly funded by the So Paulo Research Foundation (FAPESP grant 2019/076654) and by the IBM Corporation. Research by Marcos Jos has been carried out with support by Ita Unibanco S.A. through the scholarship program Programa de Bolsas Ita (PBI) ; Fabio Cozman was partially supported by CNPq grants 312180/2018-7 and 305753/2022-3. We acknowledge support also by CAPES - Finance Code 001.</p>
<h2>References</h2>
<ol>
<li>Belinkov, Y.: Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics 48(1), 207-219 (Mar 2022). https://doi.org/10.1162/coli_ a_00422, https://aclanthology.org/2022.cl-1.7</li>
<li>Beltagy, I., Peters, M.E., Cohan, A.: Longformer: The Long-Document Transformer. CoRR abs/2004.05150 (2020), https://arxiv.org/abs/2004.05150</li>
<li>Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., Tafjord, O.: Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. arXiv preprint arXiv:1803.05457 (2018)</li>
<li>Clark, P., Tafjord, O., Richardson, K.: Transformers as Soft Reasoners over Language. arXiv preprint arXiv:2002.05867 (2020)</li>
<li>Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., Schulman, J.: Training Verifiers to Solve Math Word Problems (2021). https://doi.org/10.48550/ARXIV.2110.14168, https://arxiv.org/abs/2110.14168</li>
<li>Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmn, F., Grave, E., Ott, M., Zettlemoyer, L., Stoyanov, V.: Unsupervised cross-lingual representation learning at scale (2020)</li>
<li>Conneau, A., Kruszewski, G., Lample, G., Barrault, L., Baroni, M.: What you Can Cram into a Single \$\&amp;!#* Vector: Probing Sentence Embeddings for Linguistic Properties. In: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 2126-2136. Association for Computational Linguistics, Melbourne, Australia (Jul 2018). https://doi.org/10.18653/v1/P18-1198, https:// aclanthology.org/P18-1198</li>
<li>Dai, D., Dong, L., Hao, Y., Sui, Z., Wei, F.: Knowledge Neurons in Pretrained Transformers. CoRR abs/2104.08696 (2021), https://arxiv.org/abs/2104.08696</li>
<li>Dankers, V., Bruni, E., Hupkes, D.: The Paradox of the Compositionality of Natural Language: A Neural Machine Translation Case Study. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 41544175. Association for Computational Linguistics, Dublin, Ireland (May 2022). https:</li>
</ol>
<p>//doi.org/10.18653/v1/2022.acl-long.286, https://aclanthology. org/2022.acl-long. 286
10. Dasgupta, I., Lampinen, A.K., Chan, S.C.Y., Creswell, A., Kumaran, D., McClelland, J.L., Hill, F.: Language Models Show Human-Like Content Effects on Reasoning (2022). https://doi.org/10.48550/ARXIV.2207.07051, https://arxiv. org/abs/2207.07051
11. Devlin, J., Chang, M., Lee, K., Toutanova, K.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR abs/1810.04805 (2018), http: //arxiv.org/abs/1810.04805
12. Dziri, N., Lu, X., Sclar, M., Li, X.L., Jiang, L., Lin, B.Y., West, P., Bhagavatula, C., Bras, R.L., Hwang, J.D., Sanyal, S., Welleck, S., Ren, X., Ettinger, A., Harchaoui, Z., Choi, Y.: Faith and Fate: Limits of Transformers on Compositionality (2023)
13. Ethayarajh, K.: How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings. CoRR abs/1909.00512 (2019), http : //arxiv.org/abs/1909.00512
14. Gaskell, A., Miao, Y., Specia, L., Toni, F.: Logically Consistent Adversarial Attacks for Soft Theorem Provers (2022)
15. Gurnee, W., Tegmark, M.: Language models represent space and time (2023)
16. Hahn, C., Schmitt, F., Kreber, J.U., Rabe, M.N., Finkbeiner, B.: Teaching Temporal Logics to Neural Networks (2020). https://doi.org/10.48550/ARXIV.2003.04218, https://arxiv.org/abs/2003.04218
17. Han, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Benson, L., Sun, L., Zubova, E., Qiao, Y., Burtell, M., et al.: FOLIO: Natural Language Reasoning with First-Order Logic. arXiv preprint arXiv:2209.00840 (2022)
18. Hao, Y., Dong, L., Wei, F., Xu, K.: Visualizing and Understanding the Effectiveness of BERT. CoRR abs/1908.05620 (2019), http://arxiv.org/abs/1908.05620
19. He, P., Liu, X., Gao, J., Chen, W.: DeBERTa: Decoding-enhanced BERT with Disentangled Attention. CoRR abs/2006.03654 (2020), https://arxiv.org/abs/2006.03654
20. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., Steinhardt, J.: Measuring Mathematical Problem Solving with the Math Dataset. arXiv preprint arXiv:2103.03874 (2021)
21. Hewitt, J., Manning, C.D.: A Structural Probe for Finding Syntax in Word Representations. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4129-4138. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/10.18653/v1/N19-1419, https: //aclanthology.org/N19-1419
22. Hossain, M.M., Kovatchev, V., Dutta, P., Kao, T., Wei, E., Blanco, E.: An Analysis of Natural Language Inference Benchmarks through the Lens of Negation. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). pp. 91069118. Association for Computational Linguistics, Online (Nov 2020). https://doi. org/10.18653/v1/2020.emnlp-main.732, https://aclanthology.org/ 2020.emnlp-main. 732
23. Jawahar, G., Sagot, B., Seddah, D.: What Does BERT Learn about the Structure of Language? In: Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. pp. 3651-3657. Association for Computational Linguistics, Florence, Italy (Jul 2019). https://doi.org/10.18653/v1/P19-1356, https:// aclanthology.org/P19-1356
24. Kalouli, A.L., Sevastjanova, R., Beck, C., Romero, M.: Negation, Coordination, and Quantifiers in Contextualized Language Models. In: Proceedings of the 29th International Confer-</p>
<p>ence on Computational Linguistics. pp. 3074-3085. International Committee on Computational Linguistics, Gyeongju, Republic of Korea (Oct 2022), https://aclanthology. org/2022.coling-1.272
25. Kassner, N., Schtze, H.: Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. pp. 7811-7818. Association for Computational Linguistics, Online (Jul 2020). https://doi.org/10.18653/v1/2020.acl-main.698, https://aclanthology.org/2020.acl-main. 698
26. Lai, G., Xie, Q., Liu, H., Yang, Y., Hovy, E.: RACE: Large-scale ReAding Comprehension Dataset From Examinations (2017). https://doi.org/10.48550/ARXIV.1704. 04683, https://arxiv.org/abs/1704.04683
27. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942 (2019)
28. Liu, E., Neubig, G.: Are Representations Built from the Ground Up? An Empirical Examination of Local Composition in Language Models (2022). https://doi.org/10. 48550/ARXIV.2210.03575, https://arxiv.org/abs/2210.03575
29. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., Zhang, Y.: LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning (2020). https://doi.org/ 10.48550/ARXIV.2007.08124,https://arxiv.org/abs/2007.08124
30. Liu, L.Z., Wang, Y., Kasai, J., Hajishirzi, H., Smith, N.A.: Probing Across Time: What Does RoBERTa Know and When? CoRR abs/2104.07885 (2021), https://arxiv. org/abs/2104.07885
31. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692 (2019)
32. Manning, C.D., Clark, K., Hewitt, J., Khandelwal, U., Levy, O.: Emergent Linguistic Structure in Artificial Neural Networks Trained by Self-Supervision. Proceedings of the National Academy of Sciences 117(48), 30046-30054 (2020). https://doi.org/ 10.1073/pnas.1907367117, https://www.pnas.org/doi/abs/10.1073/ pnas. 1907367117
33. ONeill, J., Rozenshtein, P., Kiryo, R., Kubota, M., Bollegala, D.: I Wish I Would Have Loved This One, But I Didn't-A Multilingual Dataset for Counterfactual Detection in Product Reviews. arXiv preprint arXiv:2104.06893 (2021)
34. Patel, A., Bhattamishra, S., Goyal, N.: Are NLP Models Really Able to Solve Simple Math Word Problems? (2021). https://doi.org/10.48550/ARXIV.2103. 07191, https://arxiv.org/abs/2103.07191
35. Qin, L., Bosselut, A., Holtzman, A., Bhagavatula, C., Clark, E., Choi, Y.: Counterfactual Story Reasoning and Generation (2019). https://doi.org/10.48550/ARXIV. 1909.04076, https://arxiv.org/abs/1909.04076
36. Rae, J.W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, H.F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van den Driessche, G., Hendricks, L.A., Rauh, M., Huang, P., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S.M., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de Masson d'Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de Las Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M.J., Hechtman, B.A., Weidinger, L., Gabriel, I.,</p>
<p>Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett, L., Hassabis, D., Kavukcuoglu, K., Irving, G.: Scaling Language Models: Methods, Analysis \&amp; Insights from Training Gopher. CoRR abs/2112.11446 (2021), https://arxiv.org/abs/2112.11446
37. Rogers, A., Kovaleva, O., Rumshisky, A.: A Primer in BERTology: What We Know About How BERT Works. Transactions of the Association for Computational Linguistics 8, 842-866 (2020). https://doi.org/10.1162/tacl_a_00349, https:// aclanthology.org/2020.tacl-1.54
38. Russell, S., Norvig, P.: Artificial Intelligence: A Modern Approach. Prentice Hall, 3 edn. (2010)
39. Sanh, V., Debut, L., Chaumond, J., Wolf, T.: Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter (2020)
40. Saparov, A., He, H.: Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought (2023)
41. Sevastjanova, R., Kalouli, A.L., Beck, C., Schfer, H., El-Assady, M.: Explaining Contextualization in Language Models using Visual Analytics. In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 464-476. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.39, https:// aclanthology.org/2021.acl-long. 39
42. Sinha, K., Sodhani, S., Dong, J., Pineau, J., Hamilton, W.L.: CLUTRR: A Diagnostic Benchmark for Inductive Reasoning from Text. In: Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 4506-4515. Association for Computational Linguistics, Hong Kong, China (Nov 2019). https://doi.org/10.18653/ v1/D19-1458, https://aclanthology.org/D19-1458
43. Talmor, A., Herzig, J., Lourie, N., Berant, J.: CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). pp. 4149-4158. Association for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https://doi.org/ 10.18653/v1/N19-1421, https://aclanthology.org/N19-1421
44. Tang, X., Zheng, Z., Li, J., Meng, F., Zhu, S.C., Liang, Y., Zhang, M.: Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners (2023)
45. Tenney, I., Das, D., Pavlick, E.: BERT Rediscovers the Classical NLP Pipeline. CoRR abs/1905.05950 (2019), http://arxiv.org/abs/1905.05950
46. Tenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R.T., Kim, N., Durme, B.V., Bowman, S.R., Das, D., Pavlick, E.: What do you Learn from Context? Probing for Sentence Structure in Contextualized Word Representations. CoRR abs/1905.06316 (2019), http : //arxiv.org/abs/1905.06316
47. Tian, J., Li, Y., Chen, W., Xiao, L., He, H., Jin, Y.: Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI. In: Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. pp. 3738-3747. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic (Nov 2021). https://doi. org/10.18653/v1/2021.emnlp-main.303, https://aclanthology.org/ 2021.emnlp-main. 303
48. Valmeekam, K., Olmo, A., Sreedharan, S., Kambhampati, S.: Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change) (2023)</p>
<ol>
<li>Warstadt, A., Parrish, A., Liu, H., Mohananey, A., Peng, W., Wang, S.F., Bowman, S.R.: BLiMP: The Benchmark of Linguistic Minimal Pairs for English. Transactions of the Association for Computational Linguistics 8, 377-392 (2020). https://doi.org/10.1162/ tacl_a_00321, https://aclanthology.org/2020.tacl-1.25</li>
<li>Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P., Dean, J., Fedus, W.: Emergent Abilities of Large Language Models (2022). https://doi.org/10. 48550/ARXIV.2206.07682, https://arxiv.org/abs/2206.07682</li>
<li>Wu, Z., Peng, H., Smith, N.A.: Infusing Finetuning with Semantic Dependencies. Transactions of the Association for Computational Linguistics 9, 226-242 (2021). https: //doi.org/10.1162/tacl_a_00363</li>
<li>Yang, X., Obadinma, S., Zhao, H., Zhang, Q., Matwin, S., Zhu, X.: SemEval-2020 Task 5: Counterfactual Recognition. arXiv preprint arXiv:2008.00563 (2020)</li>
<li>Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R., Le, Q.V.: Xlnet: Generalized autoregressive pretraining for language understanding (2020)</li>
<li>Yu, W., Jiang, Z., Dong, Y., Feng, J.: ReClor: A Reading Comprehension Dataset Requiring Logical Reasoning (2020). https://doi.org/10.48550/ARXIV.2002. 04326, https://arxiv.org/abs/2002.04326</li>
<li>Zhang, H., Li, L.H., Meng, T., Chang, K., den Broeck, G.V.: On the Paradox of Learning to Reason from Data. CoRR abs/2205.11502 (2022). https://doi.org/10.48550/ arXiv.2205.11502, https://doi.org/10.48550/arXiv.2205.11502</li>
<li>Zhang, Y., Warstadt, A., Li, X., Bowman, S.R.: When Do You Need Billions of Words of Pretraining Data? In: Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp. 1112-1125. Association for Computational Linguistics, Online (Aug 2021). https://doi.org/10.18653/v1/2021.acl-long.90, https://aclanthology.org/2021.acl-long. 90</li>
</ol>
<h1>A Datasets</h1>
<p>In this appendix, we describe the four logical reasoning datasets in more detail. Table 4 indicates the sources from where they were obtained.</p>
<p>FOLIO [17] is a human-annotated dataset for FOL reasoning problems. Logicallysound contexts were generated in two ways: in the first, annotators created contexts from scratch, based on random Wikipedia pages; in the second, a template of nested syllogisms was used, from which annotators then replaced the abstract entities and categories by nouns, phrases or clauses, as to make the text to reflect real-life scenarios. Next, the authors verified the alignment between natural language sentences and FOL formulas and added implicit commonsense knowledge as premises. After that, they verified the syntactic validity and label consistency of FOL formula annotations with a FOL prover. Finally, sentences were reviewed for grammar issues and language fluency. Only train and validation tests are available, so we used the latter for reporting tests. Hypotheses can be True, False, or Unknown.</p>
<p>LogicNLI [47] is a FOL dataset created through a semi-automatic method. A set of logical templates was defined and then filled by subjects and predicates sampled from predefined sets. Next, manual edits were made to correct grammatical errors and resolve semantic ambiguities. Hypothesis are classified as Entailment, Contradiction, Neutral, and Paradox. A paradox is defined as a situation where both a sentence and its contrary can be inferred from the premises. We used the standard version of the dataset, which encompasses all labels.</p>
<p>RuleTaker [4] is a logical reasoning dataset in which rules are conjunctive implications. Predicates may be negated and facts may be either attributes (which assign properties to entities) or relations (which relate two entities). We used the ParaRules version, where rules and facts were paraphrased by crowdworkers into more natural language; paraphrased constructions were then combined to form new templates. We also used the updated version of RuleTaker (problog), which eliminated some world model inconsistencies. Hypothesis can be True, when a hypothesis follows from the premises, and False otherwise (closed-world assumption, CWA).</p>
<p>SimpleLogic [55] is similar to RuleTaker, only supporting conjunctive implication (facts are simply conjunctive implications with empty antecedents). Language variance is virtually removed by using a fixed template for translating FOL into natural language and by the use of a small random list of words as predicates. Argumentative complexity is limited by setting thresholds for input length, number of predicates, and reasoning depth. We reconstructed the original template based on the examples given in the paper. For our tests, we used the RP Balanced version. We undersampled the largest class (True) to obtain the same number of observations as for the False class. Labels can be True and False (CWA).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Dataset</th>
<th style="text-align: left;">Source</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FOLIO</td>
<td style="text-align: left;">https://github.com/Yale-LILY/FOLIO</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLI</td>
<td style="text-align: left;">https://github.com/omnilabNL/LogicNLI</td>
</tr>
<tr>
<td style="text-align: left;">RuleTaker</td>
<td style="text-align: left;">https://allenai.org/data/ruletaker</td>
</tr>
<tr>
<td style="text-align: left;">SimpleLogic</td>
<td style="text-align: left;">https://github.com/joshuacnf/paradox-learning2reason</td>
</tr>
</tbody>
</table>
<p>Table 4. Sources for the datasets.</p>
<h1>B Label distribution</h1>
<p>Table 5 shows the label distribution for the datasets used in the paper. The row above provides the number of observations per label, and the row below shows their relative percentage. Labels: FOLIO: False, True, Unknown. LogicNLI: Contradiction, Entailment, Neutral, Paradox. RuleTaker: False, True. SimpleLogic: False, True.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Validation</th>
<th style="text-align: left;">Test</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">FOLIO</td>
<td style="text-align: left;">$286 / 388 / 329$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$63 / 72 / 69$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(28.51 \% / 38.68 \% / 32.80 \%)$</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">$(30.88 \% / 35.29 \% / 33.82 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">LogicNLI</td>
<td style="text-align: left;">$4000 / 4000 / 4000 / 4000$</td>
<td style="text-align: left;">$500 / 500 / 500 / 500$</td>
<td style="text-align: left;">$500 / 500 / 500 / 500$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(25 \%$ each $)$</td>
<td style="text-align: left;">$(25 \%$ each $)$</td>
<td style="text-align: left;">$(25 \%$ each $)$</td>
</tr>
<tr>
<td style="text-align: left;">RuleTaker</td>
<td style="text-align: left;">$13666 / 13697$</td>
<td style="text-align: left;">$1946 / 1953$</td>
<td style="text-align: left;">$3895 / 3898$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(49.94 \% / 50.05 \%)$</td>
<td style="text-align: left;">$(49.91 \% / 50.09 \%)$</td>
<td style="text-align: left;">$(49.98 \% / 50.02 \%)$</td>
</tr>
<tr>
<td style="text-align: left;">SimpleLogic</td>
<td style="text-align: left;">$5696 / 5645$</td>
<td style="text-align: left;">$683 / 735$</td>
<td style="text-align: left;">$709 / 708$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$(50.22 \% / 49.77 \%)$</td>
<td style="text-align: left;">$(48.16 \% / 51.83 \%)$</td>
<td style="text-align: left;">$(50.03 \% / 49.96 \%)$</td>
</tr>
</tbody>
</table>
<p>Table 5. Label distribution for logical reasoning datasets.</p>
<h2>C Laywerwise probing</h2>
<p>For the layerwise probing (Sec. 6), we tested two different classifiers (1-layer and 3layer) and two learning rates (1e-6 and 1e-5). Figure 2 above displayed the results for the 1-linear classifier and 1e-6 learning rate. The figures below display the results for the other probes. Figure 3 provides the graphs for the 3-layer classifier and learning rate of 1e-6; Figure 4 provides the graphs for the 1-layer classifier and learning rate of 1e-5; and Figure 5 provides the graphs for the 3-layer classifier and learning rate of 1e-5.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 3-layer classifier and a learning rate of 1e-6.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 1-layer classifier and a learning rate of $1 \mathrm{e}-5$.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. RoBERTa-large models fine-tuned on FOLIO, LogicNLI, RuleTaker, and SimpleLogic, and probed for the same datasets layerwise. The pretrained baselines are indicated by gray lines, while the values achieved in the cross-probing task are represented by black lines. The colored bars indicate the change in accuracy from the previous layers. Probing was performed with a 3-layer classifier and a learning rate of $1 \mathrm{e}-5$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Conjunctive implications are arguments of the form (fact.[^fact.]"rule.[^rule.]" $\Rightarrow$ hypothesis).
${ }^{6}$ We opted to explore encoder-only models because this type of architecture is well-suited for classification tasks. These models have access to the whole input sequence and are typically trained on discriminative tasks, such as masked language modeling.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
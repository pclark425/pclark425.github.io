<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6828 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6828</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6828</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-131.html">extraction-schema-131</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <p><strong>Paper ID:</strong> paper-276258666</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.07640v3.pdf" target="_blank">Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving</a></p>
                <p><strong>Paper Abstract:</strong> We introduce Goedel-Prover, an open-source language model that achieves state-of-the-art (as of April 5 2025) performance in automated formal proof generation for mathematical problems. A key challenge in this field is the scarcity of formalized mathematical statements and proofs, which we address through the following approaches. First, we train LLMs to convert natural language math problems from the Numina dataset to equivalent formal statements in Lean 4. This process creates the dataset Goedel-Pset-v1, which includes 1.64 million formal statements. Next, we develop a large dataset of formal proofs by training a series of provers. Each new prover can prove many statements that previous ones could not, and these new proofs are added to the training set for the next prover. Finally, we obtain the dataset Goedel-Pset-v1-solved, which contains proofs for over 800K statements from Goedel-Pset-v1. Supervised fine-tuning (SFT) of DeepSeek-Prover-V1.5-Base on Goedel-Pset-v1-solved (i.e., no RL) yields a Goedel-Prover-SFT that achieves a success rate of 57.6% (Pass@32) on miniF2F, surpassing the previous leader DeepSeek-Prover-V1.5-RL (trained using SFT + RL on a proprietary dataset) by 7.6%. On PutnamBench, Goedel-Prover-SFT successfully solves 7 problems (Pass@512), ranking first on the leaderboard. We provide extensive discussion of our training methodology, highlighting the key design choices that contribute to Goedel-Prover's strong performance. Further RL training (including DPO) improves Goedel-Prover-SFT's success rate to over 60% (Pass@32) on miniF2F. To aid future research, we provide extensive discussion of our training methodology and design choices. We also fully open-source our codes, models, and datasets. Additionally, we open-source formal proofs for 29.7K problems in Lean Workbook, nearly doubling the 15.7K solved by prior provers.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6828.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6828.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goedel-Prover-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goedel-Prover (Supervised Fine-Tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source whole‑proof generation language model trained via supervised fine‑tuning on a large autoformalized + verified proof dataset (Goedel-Pset-v1-solved) using expert iteration; generates complete Lean 4 proofs without online interaction with the Lean compiler.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goedel-Prover-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based LM fine-tuned (SFT) from DeepSeek-Prover-V1.5-Base checkpoints using a large corpus of formal statements and verified proofs (Goedel-Pset-v1-solved) obtained by expert iteration; designed for whole-proof generation in Lean 4.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (whole-proof generation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Goedel-Pset-v1 (1.64M formal statements) and Goedel-Pset-v1-solved (≈800K verified proofs) produced by expert iteration; Lean Workbook and selected Mathlib4 additions</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Whole-proof formal proof generation (generate entire Lean proofs from statements) trained via supervised fine-tuning and expert iteration</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean 4 proof assistant used to verify candidate proofs during dataset construction and evaluation (verification step); no online interaction with Lean during generation (whole-proof outputs are generated offline).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F, PutnamBench, Lean Workbook, ProofNet, NuminaTest</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniF2F: formalized competition-style math problems (244 val + 244 test); PutnamBench: Putnam competition problems (644 Lean 4 statements); Lean Workbook: 140K formal statements from AoPS; ProofNet: undergraduate-level formal problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal theorem proving / whole-proof generation in Lean 4</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@k (Pass@32, Pass@3200, Pass@512 for PutnamBench)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>miniF2F Pass@32 = 57.6% ± 0.7; miniF2F Pass@3200 = 62.7%; PutnamBench Pass@512 = 7/644 solved; Lean Workbook cumulative solved = 29.7K</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms previous open-source SOTA DeepSeek-Prover-V1.5-RL (Pass@32 50.0%) by +7.6% absolute on miniF2F Pass@32; Goedel-Prover-SFT Pass@32 even exceeds DeepSeek-Prover-V1.5-RL's Pass@3200 by +2.7%.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Large-scale autoformalization + expert iteration with SFT yields SOTA whole-proof generation performance; model benefits from scale and diversity of formalization styles; whole-proof generation without runtime interaction can be highly effective.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Generates proofs that often rely on high-level tactics (e.g., nlinarith, simp) which hide intermediate reasoning; no online interaction with Lean during generation limits feedback-driven correction; performance varies with formalization style; some distribution shifts (Mathlib4 vs miniF2F) affect generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6828.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goedel-Prover-DPO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goedel-Prover (Direct Preference Optimization fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Goedel-Prover variant further fine‑tuned via offline Direct Preference Optimization (DPO) on paired correct/incorrect proofs to optimize preference-based objectives; improves Pass@32 but exhibits behavioral artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goedel-Prover-DPO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same base as Goedel-Prover-SFT, further trained with DPO using pairs sampled from multiple trials (correct vs incorrect proofs) and optionally length regularization to penalize verbose/shortcut proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer with preference-based fine-tuning (DPO)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Pairs of model-generated proofs from Goedel-Prover-SFT runs (selected problems with 1–4 correct proofs among 16 trials); ~30K selected problems used for DPO training</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Preference optimization (DPO) to prefer correct proofs over incorrect ones; includes length regularization variant</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean used to label candidate outputs (correct vs incorrect) for constructing DPO preference pairs; length penalty applied to prefer shorter correct proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniF2F: formalized competition-style math problems used to measure whole-proof generation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof generation (whole-proof)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@32, Pass@3200; average proof length; frequency of specific tactic usage</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pass@32 = 60.3%; Pass@3200 = 64.6%; associated with large increase in average proof length (reported extreme length increase) and high frequency of tactics like 'try' (shortcut patterns).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>+2.7% absolute vs Goedel-Prover-SFT on Pass@32 (60.3% vs 57.6%); marginal or limited gains at high inference budgets (Pass@3200 gains smaller or not translating to improved Pass@N scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DPO increases immediate-pass metrics but models learn syntactic 'shortcuts' and verbose proofs (reward hacking); length regularization mitigates some artifacts but reduces gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prone to overfitting to shortcuts (e.g., excessive use of 'try' and 'all goals') causing very long proofs and degraded verification costs; reduced benefit from increased inference-time compute (worse inference-time scaling).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6828.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Goedel-Prover-GRPO (RL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Goedel-Prover (Group Relative Policy Optimization reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL variant built on Goedel-Prover-SFT trained with GRPO within an OpenRLHF framework to maximize verified-proof rewards; increases Pass@32 but shows similar shortcut overfitting and scaling limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Goedel-Prover-GRPO</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Goedel-Prover-SFT checkpoint further fine-tuned using online reinforcement learning (GRPO) where successful compiled proofs receive positive reward and failures negative reward, generating multiple proofs per problem during training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer with reinforcement learning (GRPO)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>≈80K problem statements with moderate pass ratios (0, 1/2] sampled from training data; 16 proofs generated per problem during RL training with Lean verification</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Reinforcement learning with reward shaping (+8 for correct compiled proofs, -8 for incorrect, experiments with timeout reward variants)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean 4 used as the verifier to compute rewards for generated proofs during RL training (compiled vs failed proofs determine reward).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniF2F: formalized competition-style math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof generation (whole-proof)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@32, Pass@3200; average proof length; tactic usage frequency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pass@32 = 60.5%; Pass@3200 = 63.1%; average proof length increases relative to SFT; greater frequency of 'try' usage reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>+~3% over SFT on Pass@32 (SFT 57.6% → GRPO 60.5%) but gains shrink at high inference budgets (Pass@3200).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RL can increase immediate pass metrics but encourages shortcut tactics, longer proofs, and worse inference-time scaling; reward-design (timeout rewards, length penalties) materially affects behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Overfitting to exploitable tactics (reward hacking), elevated verification cost due to longer proofs, and diminished returns when scaling inference compute (reduced diversity in outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6828.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-Prover-V1.5-RL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-Prover V1.5 (Reinforcement-Learned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior state-of-the-art whole-proof generation prover used as an initial generator in expert iteration; provided baseline performance and seeds for Goedel-Prover's dataset construction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Prover-V1.5-RL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A transformer-based prover previously trained with a mix of supervised fine-tuning and reinforcement learning (and used in prior work to generate candidate proofs that are verified with Lean).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer with RL-enhancement</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Large synthetic formal proof datasets (from DeepSeek pipeline) and prior Lean datasets; used to generate initial proof candidates for expert iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Whole-proof generation with RL-improved sampling behavior</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean compiler used to verify generated proof candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniF2F: formalized competition-style math problems.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof generation (whole-proof)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass@32</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pass@32 = 50.0% (reported baseline in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as baseline and seed generator: Goedel-Prover-SFT improves over DeepSeek-Prover-V1.5-RL by +7.6% absolute on miniF2F Pass@32.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serves as a strong open-source baseline and an effective initial proof generator for expert iteration pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Previous SOTA but outperformed by Goedel-Prover-SFT when large-scale autoformalization and expert iteration are applied; specific behaviors under RL not detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6828.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-Prover-V1.5-Base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-Prover V1.5 (Base checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Base checkpoint of DeepSeek-Prover V1.5 used as the starting point for supervised fine-tuning in Goedel's expert iteration pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-Prover-V1.5-Base</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Base transformer model checkpoint (no RL) used as the SFT starting point in iterative training of Goedel-Prover provers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (base checkpoint)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Previously collected synthetic and verified Lean proof data from DeepSeek pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Whole-proof generation via supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean compiler used during data collection/verification.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>internal evaluation sets, miniF2F (used indirectly)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Used as the basis for iteratively training provers evaluated on miniF2F and other benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>N/A in this paper as a model to be fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Serves as baseline checkpoint for SFT in expert iteration; iter-k provers are derived by SFT starting from this checkpoint.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A stable starting checkpoint for supervised fine-tuning that benefits from addition of newly discovered verified proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Size and detailed specs not disclosed in this paper; performance superseded by later SFT models when trained with larger, diverse datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6828.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formalizer A (Qwen2.5-Coder-32B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formalizer A (trained Qwen2.5-Coder-32B model for autoformalization)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A supervised fine-tuned Qwen2.5-Coder-32B model trained on Formal-Informal statement pairs from Lean Workbook to translate informal math problems into Lean statements (autoformalization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-Coder-32B (Formalizer A variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>32B-parameter coder variant of Qwen fine-tuned on Lean Workbook F-I pairs to produce Lean 4 problem statements (placeholders for proofs).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>32B</td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (code-specialized)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Formal-Informal (F-I) statement pairs from Lean Workbook (human and machine annotated pairs)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Autoformalization (translation of natural language math to Lean 4 statements)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Generated formal statements were validated with Lean 4 compiler for syntactic compilation (Compiling Correctness test) and judged for faithfulness using Qwen2.5-72B-Instruct; Lean used in verification pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Compiling Correctness (CC) test; Faithfulness and Completeness (FC) test; overall inclusion in Goedel-Pset-v1</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>CC: whether the generated Lean statement compiles (with proof replaced by 'by sorry'); FC: whether the formalization preserves informal content (judged by LMs).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>autoformalization / formal statement generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Compilation pass rates and FC scores; statements retained for dataset if FC >= 0.5</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Used to produce valid formalizations; combined with Formalizer B produced 1.64M formal statements (Goedel-Pset-v1) after filtering (exact per-formalizer pass rates reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Combining multiple formalizers (A + B) increased diversity and improved prover performance compared to single-style formalizations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Autoformalization at scale (two complementary formalizers) yields massive increase in formal statements and improves prover generalization when diverse styles are included.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Different formalization styles affect prover performance; some formalizations may compile but be less solvable by a given prover (style sensitivity).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6828.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formalizer B (Claude-sonnet-3.5 -> Qwen2.5-72B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formalizer B (initially Claude-sonnet-3.5 annotated then Qwen2.5-Coder fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage formalizer that used Claude-sonnet-3.5 to generate initial formalizations for many Numina statements, then trained Qwen2.5-Coder variants on the filtered compiled pairs to produce diverse Lean 4 statements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude-sonnet-3.5 (used for annotation) + Qwen2.5-Coder (trained Formalizer B)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Claude-sonnet-3.5 was used to produce initial formalizations; Qwen2.5 variants were trained (SFT) on the resulting verified F-I pairs to build Formalizer B.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>230K Claude-sonnet-3.5 formalizations from Numina (170K passed Lean compilation) used to train Formalizer B via supervised fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Autoformalization (LM-driven translation from informal math to Lean 4 statements)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean 4 compiler used to validate formalizations; Qwen2.5-72B-Instruct used for FC judgments in pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Compiling Correctness (CC) and Faithfulness & Completeness (FC) tests</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>CC: Freshly generated statements compile; FC: judged faithful to informal statements by an LM-based evaluator.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>autoformalization</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Compilation and FC pass rates; contributed to the total of 1.64M formal statements</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>From 230K Claude outputs, 170K statements compiled successfully and were used to train Formalizer B; overall dataset contributions enabled 1.64M formal statements.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Adding a second formalizer (B) increased formalization diversity and improved downstream prover performance when mixing A+B styles.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using multiple formalizers (one trained on human F-I pairs, another seeded by a large LM) increases style diversity and improves prover robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Some formalized styles are harder for the prover to solve; FC judgments relied on LMs as surrogate evaluators (possible evaluation noise).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6828.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenAI o1-preview pipeline (divide-and-conquer)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Divide-and-conquer proof sketch pipeline using OpenAI o1-preview</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An alternative synthesis pipeline that uses OpenAI's o1-preview to produce stepwise proof sketches, extracts subgoals, and attempts to prove subgoals with DeepSeek-Prover-V1.5-RL; found to be inefficient and yield marginal gains.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenAI o1-preview (proof sketch generator) + DeepSeek-Prover-V1.5-RL (subgoal prover)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>o1-preview produces step-by-step human-like proofs with 'have' blocks; sketches are converted to subproblems and DeepSeek-Prover is applied to solve subgoals, assembling them if all succeed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer (used as sketch generator) + transformer prover for subgoals</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A — used as an inference-time pipeline combining outputs of o1-preview and existing provers</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Sketch-based divide-and-conquer: generate high-level proof sketch, extract subgoals, attempt to solve subgoals individually and assemble a full proof</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Lean compiler used to extract subgoals from proof sketches and to verify assembled proofs; DeepSeek-Prover used to attempt subgoal proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F (validation subset of 244 problems)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniF2F validation set used to test whether the divide-and-conquer pipeline yields additional solved problems beyond baseline provers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof generation via sketch-guided subgoal solving</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Number of problems solved (out of 244)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Pipeline solved 76/244 problems; DeepSeek-Prover-V1.5-RL alone solved 158/244 on same subset; only 1 problem solved uniquely by the pipeline beyond DeepSeek-Prover-V1.5-RL.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Performed substantially worse than DeepSeek-Prover-V1.5-RL standalone on the 244-sample validation subset (76 vs 158 solved); yielded only one unique additional proof.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Sketch-based divide-and-conquer is promising conceptually but was inefficient in practice here and produced negligible marginal gain over a strong prover.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Pipeline complexity and failure in any subgoal causes overall failure; many machine-generated subproofs were incorrect or inadequate, limiting utility.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6828.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SymPy integration</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SymPy-based pre-simplification integration for Lean theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple external symbolic-computation preprocessing step: SymPy simplifies algebraic expressions in Lean goals (A − B → simplify), which sometimes reduces goals to trivialities and increases solved counts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SymPy (external CAS) used as preprocessor</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Python-based computer algebra system (SymPy) used to algebraically simplify equations extracted from Lean goals before passing to prover.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>external symbolic algebra system (non-ML)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>N/A (tool used at inference to simplify expressions)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>External symbolic simplification: parse A = B in Lean, convert to SymPy A-B, simplify; feed simplified goal back to prover</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>SymPy invoked offline to simplify expressions; Lean used to extract/accept simplified forms; this was an auxiliary preprocessing step, not in main reported results.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>miniF2F</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>miniF2F problems where algebraic simplification can reduce goal complexity; used to measure impact of CAS-based preprocessing.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>preprocessing / simplification to aid formal proof generation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Fraction of problems solved via simplification; additional solves relative to Goedel-Prover-SFT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>SymPy simplification directly solved 9.4% of miniF2F by simplifying to 0 = 0; additionally solved 0.8% of miniF2F problems that Goedel-Prover-SFT (Pass@32) did not solve; no improvement at Pass@3200.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Helps on a small subset of problems by trivializing algebraic identities; not included in main reported model results since improvements did not generalize at larger inference budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>External CAS preprocessing can trivially solve a non-negligible subset of problems but is not a universal fix; useful as an auxiliary tool.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only addresses problems reducible by symbolic simplification; did not improve high-inference-budget performance and therefore not part of main results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e6828.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABEL (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ABEL (Sample efficient online reinforcement learning for neural theorem proving)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A competing approach (tree-search plus RL) reported on PutnamBench and cited for comparison; solves a similar number of Putnam problems at slightly different compute budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sample efficient online reinforcement learning for neural theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ABEL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Tree-search method combining neural guidance with sample-efficient online RL for theorem proving (reported in PutnamBench comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>neural-guided tree search (non-whole-proof generation)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Not detailed in this paper (referenced prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Tree search with online RL and neural guidance</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Typically integrates with proof assistant (e.g., Lean) during search (paper references tree-search methods but details are in original ABEL work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>PutnamBench</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Putnam competition problems formalized in Lean; 644 statements covering a range of mathematical topics.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal theorem proving (proof search/tree search)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Number of Putnam problems solved (with associated compute / pass budgets)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported to solve 7 PutnamBench problems (comparable to Goedel-Prover-SFT which solved 7 at Pass@512); ABEL achieved 7 at Pass@596 in leaderboard citation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as leaderboard comparator; Goedel-Prover-SFT matched ABEL's problem count but at a slightly lower reported inference budget.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Tree-search RL approaches remain competitive on some benchmarks; comparison highlights compute/compute-budget sensitivity across methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Details of ABEL's approaches (compute budgets, exact training data) require consulting ABEL's original paper; mentioned here for leaderboard comparison only.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6828.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e6828.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models' approaches to strict logical reasoning, including model details, reasoning methods, benchmarks, tasks, performance results, comparative findings, and noted limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM2.5-StepProver (comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM2.5-StepProver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior open-source prover that previously contributed proofs to Lean Workbook; used as a comparative baseline for cumulative solved counts in Lean Workbook.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>InternLM2.5-StepProver: Advancing automated theorem proving via expert iteration on large-scale lean problems</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2.5-StepProver</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An expert-iteration based prover (stepwise) from InternLM family; reported to have found a corpus of proofs in lean datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architecture_type</strong></td>
                            <td>transformer-based stepwise prover with retrieval/search elements (per original work)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Lean Workbook and other formal corpora in prior InternLM work</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method</strong></td>
                            <td>Stepwise proving with retrieval and/or search (different from whole-proof generation)</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tool_description</strong></td>
                            <td>Integrates with Lean for verification and potentially stepwise feedback (per prior InternLM work).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Lean Workbook (solved counts); PutnamBench (reported solves)</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_description</strong></td>
                            <td>Lean Workbook: large collection of formal statements from AoPS; InternLM reported finding many of the 15.7K proofs previously available.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>formal proof generation (stepwise / search-based)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Number of proofs found / leaderboard positions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Previously contributed to 15.7K solved proofs in Lean Workbook (prior to Goedel-Prover's 29.7K total).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Goedel-Prover-SFT nearly doubled the number of proofs found in Lean Workbook compared to cumulative prior provers (29.7K vs 15.7K).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Expert iteration and large-scale autoformalization can substantially increase the number of solvable formal statements; stepwise and whole-proof approaches are complementary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Specific architecture and performance details are in InternLM's own papers; cited here as prior baseline and data contributor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data <em>(Rating: 2)</em></li>
                <li>InternLM2.5-StepProver: Advancing automated theorem proving via expert iteration on large-scale lean problems <em>(Rating: 2)</em></li>
                <li>Generative language modeling for automated theorem proving <em>(Rating: 2)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
                <li>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs <em>(Rating: 2)</em></li>
                <li>ProofNet: Autoformalizing and formally proving undergraduate-level mathematics <em>(Rating: 2)</em></li>
                <li>AI achieves silver-medal standard solving international mathematical olympiad problems <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6828",
    "paper_id": "paper-276258666",
    "extraction_schema_id": "extraction-schema-131",
    "extracted_data": [
        {
            "name_short": "Goedel-Prover-SFT",
            "name_full": "Goedel-Prover (Supervised Fine-Tuned)",
            "brief_description": "An open-source whole‑proof generation language model trained via supervised fine‑tuning on a large autoformalized + verified proof dataset (Goedel-Pset-v1-solved) using expert iteration; generates complete Lean 4 proofs without online interaction with the Lean compiler.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goedel-Prover-SFT",
            "model_description": "A transformer-based LM fine-tuned (SFT) from DeepSeek-Prover-V1.5-Base checkpoints using a large corpus of formal statements and verified proofs (Goedel-Pset-v1-solved) obtained by expert iteration; designed for whole-proof generation in Lean 4.",
            "model_size": null,
            "architecture_type": "transformer (whole-proof generation)",
            "training_data": "Goedel-Pset-v1 (1.64M formal statements) and Goedel-Pset-v1-solved (≈800K verified proofs) produced by expert iteration; Lean Workbook and selected Mathlib4 additions",
            "reasoning_method": "Whole-proof formal proof generation (generate entire Lean proofs from statements) trained via supervised fine-tuning and expert iteration",
            "external_tool_used": true,
            "external_tool_description": "Lean 4 proof assistant used to verify candidate proofs during dataset construction and evaluation (verification step); no online interaction with Lean during generation (whole-proof outputs are generated offline).",
            "benchmark_name": "miniF2F, PutnamBench, Lean Workbook, ProofNet, NuminaTest",
            "benchmark_description": "miniF2F: formalized competition-style math problems (244 val + 244 test); PutnamBench: Putnam competition problems (644 Lean 4 statements); Lean Workbook: 140K formal statements from AoPS; ProofNet: undergraduate-level formal problems.",
            "task_type": "formal theorem proving / whole-proof generation in Lean 4",
            "performance_metric": "Pass@k (Pass@32, Pass@3200, Pass@512 for PutnamBench)",
            "performance_value": "miniF2F Pass@32 = 57.6% ± 0.7; miniF2F Pass@3200 = 62.7%; PutnamBench Pass@512 = 7/644 solved; Lean Workbook cumulative solved = 29.7K",
            "comparison_with_baseline": "Outperforms previous open-source SOTA DeepSeek-Prover-V1.5-RL (Pass@32 50.0%) by +7.6% absolute on miniF2F Pass@32; Goedel-Prover-SFT Pass@32 even exceeds DeepSeek-Prover-V1.5-RL's Pass@3200 by +2.7%.",
            "key_findings": "Large-scale autoformalization + expert iteration with SFT yields SOTA whole-proof generation performance; model benefits from scale and diversity of formalization styles; whole-proof generation without runtime interaction can be highly effective.",
            "limitations": "Generates proofs that often rely on high-level tactics (e.g., nlinarith, simp) which hide intermediate reasoning; no online interaction with Lean during generation limits feedback-driven correction; performance varies with formalization style; some distribution shifts (Mathlib4 vs miniF2F) affect generalization.",
            "uuid": "e6828.0",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Goedel-Prover-DPO",
            "name_full": "Goedel-Prover (Direct Preference Optimization fine-tuned)",
            "brief_description": "A Goedel-Prover variant further fine‑tuned via offline Direct Preference Optimization (DPO) on paired correct/incorrect proofs to optimize preference-based objectives; improves Pass@32 but exhibits behavioral artifacts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goedel-Prover-DPO",
            "model_description": "Same base as Goedel-Prover-SFT, further trained with DPO using pairs sampled from multiple trials (correct vs incorrect proofs) and optionally length regularization to penalize verbose/shortcut proofs.",
            "model_size": null,
            "architecture_type": "transformer with preference-based fine-tuning (DPO)",
            "training_data": "Pairs of model-generated proofs from Goedel-Prover-SFT runs (selected problems with 1–4 correct proofs among 16 trials); ~30K selected problems used for DPO training",
            "reasoning_method": "Preference optimization (DPO) to prefer correct proofs over incorrect ones; includes length regularization variant",
            "external_tool_used": true,
            "external_tool_description": "Lean used to label candidate outputs (correct vs incorrect) for constructing DPO preference pairs; length penalty applied to prefer shorter correct proofs.",
            "benchmark_name": "miniF2F",
            "benchmark_description": "miniF2F: formalized competition-style math problems used to measure whole-proof generation performance.",
            "task_type": "formal proof generation (whole-proof)",
            "performance_metric": "Pass@32, Pass@3200; average proof length; frequency of specific tactic usage",
            "performance_value": "Pass@32 = 60.3%; Pass@3200 = 64.6%; associated with large increase in average proof length (reported extreme length increase) and high frequency of tactics like 'try' (shortcut patterns).",
            "comparison_with_baseline": "+2.7% absolute vs Goedel-Prover-SFT on Pass@32 (60.3% vs 57.6%); marginal or limited gains at high inference budgets (Pass@3200 gains smaller or not translating to improved Pass@N scaling).",
            "key_findings": "DPO increases immediate-pass metrics but models learn syntactic 'shortcuts' and verbose proofs (reward hacking); length regularization mitigates some artifacts but reduces gains.",
            "limitations": "Prone to overfitting to shortcuts (e.g., excessive use of 'try' and 'all goals') causing very long proofs and degraded verification costs; reduced benefit from increased inference-time compute (worse inference-time scaling).",
            "uuid": "e6828.1",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Goedel-Prover-GRPO (RL)",
            "name_full": "Goedel-Prover (Group Relative Policy Optimization reinforcement learning)",
            "brief_description": "An RL variant built on Goedel-Prover-SFT trained with GRPO within an OpenRLHF framework to maximize verified-proof rewards; increases Pass@32 but shows similar shortcut overfitting and scaling limitations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Goedel-Prover-GRPO",
            "model_description": "Goedel-Prover-SFT checkpoint further fine-tuned using online reinforcement learning (GRPO) where successful compiled proofs receive positive reward and failures negative reward, generating multiple proofs per problem during training.",
            "model_size": null,
            "architecture_type": "transformer with reinforcement learning (GRPO)",
            "training_data": "≈80K problem statements with moderate pass ratios (0, 1/2] sampled from training data; 16 proofs generated per problem during RL training with Lean verification",
            "reasoning_method": "Reinforcement learning with reward shaping (+8 for correct compiled proofs, -8 for incorrect, experiments with timeout reward variants)",
            "external_tool_used": true,
            "external_tool_description": "Lean 4 used as the verifier to compute rewards for generated proofs during RL training (compiled vs failed proofs determine reward).",
            "benchmark_name": "miniF2F",
            "benchmark_description": "miniF2F: formalized competition-style math problems.",
            "task_type": "formal proof generation (whole-proof)",
            "performance_metric": "Pass@32, Pass@3200; average proof length; tactic usage frequency",
            "performance_value": "Pass@32 = 60.5%; Pass@3200 = 63.1%; average proof length increases relative to SFT; greater frequency of 'try' usage reported.",
            "comparison_with_baseline": "+~3% over SFT on Pass@32 (SFT 57.6% → GRPO 60.5%) but gains shrink at high inference budgets (Pass@3200).",
            "key_findings": "RL can increase immediate pass metrics but encourages shortcut tactics, longer proofs, and worse inference-time scaling; reward-design (timeout rewards, length penalties) materially affects behaviors.",
            "limitations": "Overfitting to exploitable tactics (reward hacking), elevated verification cost due to longer proofs, and diminished returns when scaling inference compute (reduced diversity in outputs).",
            "uuid": "e6828.2",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DeepSeek-Prover-V1.5-RL",
            "name_full": "DeepSeek-Prover V1.5 (Reinforcement-Learned variant)",
            "brief_description": "Prior state-of-the-art whole-proof generation prover used as an initial generator in expert iteration; provided baseline performance and seeds for Goedel-Prover's dataset construction.",
            "citation_title": "Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Prover-V1.5-RL",
            "model_description": "A transformer-based prover previously trained with a mix of supervised fine-tuning and reinforcement learning (and used in prior work to generate candidate proofs that are verified with Lean).",
            "model_size": null,
            "architecture_type": "transformer with RL-enhancement",
            "training_data": "Large synthetic formal proof datasets (from DeepSeek pipeline) and prior Lean datasets; used to generate initial proof candidates for expert iteration.",
            "reasoning_method": "Whole-proof generation with RL-improved sampling behavior",
            "external_tool_used": true,
            "external_tool_description": "Lean compiler used to verify generated proof candidates.",
            "benchmark_name": "miniF2F",
            "benchmark_description": "miniF2F: formalized competition-style math problems.",
            "task_type": "formal proof generation (whole-proof)",
            "performance_metric": "Pass@32",
            "performance_value": "Pass@32 = 50.0% (reported baseline in this paper)",
            "comparison_with_baseline": "Used as baseline and seed generator: Goedel-Prover-SFT improves over DeepSeek-Prover-V1.5-RL by +7.6% absolute on miniF2F Pass@32.",
            "key_findings": "Serves as a strong open-source baseline and an effective initial proof generator for expert iteration pipelines.",
            "limitations": "Previous SOTA but outperformed by Goedel-Prover-SFT when large-scale autoformalization and expert iteration are applied; specific behaviors under RL not detailed here.",
            "uuid": "e6828.3",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "DeepSeek-Prover-V1.5-Base",
            "name_full": "DeepSeek-Prover V1.5 (Base checkpoint)",
            "brief_description": "Base checkpoint of DeepSeek-Prover V1.5 used as the starting point for supervised fine-tuning in Goedel's expert iteration pipeline.",
            "citation_title": "Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data",
            "mention_or_use": "use",
            "model_name": "DeepSeek-Prover-V1.5-Base",
            "model_description": "Base transformer model checkpoint (no RL) used as the SFT starting point in iterative training of Goedel-Prover provers.",
            "model_size": null,
            "architecture_type": "transformer (base checkpoint)",
            "training_data": "Previously collected synthetic and verified Lean proof data from DeepSeek pipeline.",
            "reasoning_method": "Whole-proof generation via supervised fine-tuning",
            "external_tool_used": true,
            "external_tool_description": "Lean compiler used during data collection/verification.",
            "benchmark_name": "internal evaluation sets, miniF2F (used indirectly)",
            "benchmark_description": "Used as the basis for iteratively training provers evaluated on miniF2F and other benchmarks.",
            "task_type": "formal proof generation",
            "performance_metric": "N/A in this paper as a model to be fine-tuned",
            "performance_value": null,
            "comparison_with_baseline": "Serves as baseline checkpoint for SFT in expert iteration; iter-k provers are derived by SFT starting from this checkpoint.",
            "key_findings": "A stable starting checkpoint for supervised fine-tuning that benefits from addition of newly discovered verified proofs.",
            "limitations": "Size and detailed specs not disclosed in this paper; performance superseded by later SFT models when trained with larger, diverse datasets.",
            "uuid": "e6828.4",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Formalizer A (Qwen2.5-Coder-32B)",
            "name_full": "Formalizer A (trained Qwen2.5-Coder-32B model for autoformalization)",
            "brief_description": "A supervised fine-tuned Qwen2.5-Coder-32B model trained on Formal-Informal statement pairs from Lean Workbook to translate informal math problems into Lean statements (autoformalization).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-Coder-32B (Formalizer A variant)",
            "model_description": "32B-parameter coder variant of Qwen fine-tuned on Lean Workbook F-I pairs to produce Lean 4 problem statements (placeholders for proofs).",
            "model_size": "32B",
            "architecture_type": "transformer (code-specialized)",
            "training_data": "Formal-Informal (F-I) statement pairs from Lean Workbook (human and machine annotated pairs)",
            "reasoning_method": "Autoformalization (translation of natural language math to Lean 4 statements)",
            "external_tool_used": true,
            "external_tool_description": "Generated formal statements were validated with Lean 4 compiler for syntactic compilation (Compiling Correctness test) and judged for faithfulness using Qwen2.5-72B-Instruct; Lean used in verification pipeline.",
            "benchmark_name": "Compiling Correctness (CC) test; Faithfulness and Completeness (FC) test; overall inclusion in Goedel-Pset-v1",
            "benchmark_description": "CC: whether the generated Lean statement compiles (with proof replaced by 'by sorry'); FC: whether the formalization preserves informal content (judged by LMs).",
            "task_type": "autoformalization / formal statement generation",
            "performance_metric": "Compilation pass rates and FC scores; statements retained for dataset if FC &gt;= 0.5",
            "performance_value": "Used to produce valid formalizations; combined with Formalizer B produced 1.64M formal statements (Goedel-Pset-v1) after filtering (exact per-formalizer pass rates reported in paper tables).",
            "comparison_with_baseline": "Combining multiple formalizers (A + B) increased diversity and improved prover performance compared to single-style formalizations.",
            "key_findings": "Autoformalization at scale (two complementary formalizers) yields massive increase in formal statements and improves prover generalization when diverse styles are included.",
            "limitations": "Different formalization styles affect prover performance; some formalizations may compile but be less solvable by a given prover (style sensitivity).",
            "uuid": "e6828.5",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "Formalizer B (Claude-sonnet-3.5 -&gt; Qwen2.5-72B)",
            "name_full": "Formalizer B (initially Claude-sonnet-3.5 annotated then Qwen2.5-Coder fine-tuned)",
            "brief_description": "A two-stage formalizer that used Claude-sonnet-3.5 to generate initial formalizations for many Numina statements, then trained Qwen2.5-Coder variants on the filtered compiled pairs to produce diverse Lean 4 statements.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Claude-sonnet-3.5 (used for annotation) + Qwen2.5-Coder (trained Formalizer B)",
            "model_description": "Claude-sonnet-3.5 was used to produce initial formalizations; Qwen2.5 variants were trained (SFT) on the resulting verified F-I pairs to build Formalizer B.",
            "model_size": null,
            "architecture_type": "transformer",
            "training_data": "230K Claude-sonnet-3.5 formalizations from Numina (170K passed Lean compilation) used to train Formalizer B via supervised fine-tuning",
            "reasoning_method": "Autoformalization (LM-driven translation from informal math to Lean 4 statements)",
            "external_tool_used": true,
            "external_tool_description": "Lean 4 compiler used to validate formalizations; Qwen2.5-72B-Instruct used for FC judgments in pipeline.",
            "benchmark_name": "Compiling Correctness (CC) and Faithfulness & Completeness (FC) tests",
            "benchmark_description": "CC: Freshly generated statements compile; FC: judged faithful to informal statements by an LM-based evaluator.",
            "task_type": "autoformalization",
            "performance_metric": "Compilation and FC pass rates; contributed to the total of 1.64M formal statements",
            "performance_value": "From 230K Claude outputs, 170K statements compiled successfully and were used to train Formalizer B; overall dataset contributions enabled 1.64M formal statements.",
            "comparison_with_baseline": "Adding a second formalizer (B) increased formalization diversity and improved downstream prover performance when mixing A+B styles.",
            "key_findings": "Using multiple formalizers (one trained on human F-I pairs, another seeded by a large LM) increases style diversity and improves prover robustness.",
            "limitations": "Some formalized styles are harder for the prover to solve; FC judgments relied on LMs as surrogate evaluators (possible evaluation noise).",
            "uuid": "e6828.6",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "OpenAI o1-preview pipeline (divide-and-conquer)",
            "name_full": "Divide-and-conquer proof sketch pipeline using OpenAI o1-preview",
            "brief_description": "An alternative synthesis pipeline that uses OpenAI's o1-preview to produce stepwise proof sketches, extracts subgoals, and attempts to prove subgoals with DeepSeek-Prover-V1.5-RL; found to be inefficient and yield marginal gains.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "OpenAI o1-preview (proof sketch generator) + DeepSeek-Prover-V1.5-RL (subgoal prover)",
            "model_description": "o1-preview produces step-by-step human-like proofs with 'have' blocks; sketches are converted to subproblems and DeepSeek-Prover is applied to solve subgoals, assembling them if all succeed.",
            "model_size": null,
            "architecture_type": "transformer (used as sketch generator) + transformer prover for subgoals",
            "training_data": "N/A — used as an inference-time pipeline combining outputs of o1-preview and existing provers",
            "reasoning_method": "Sketch-based divide-and-conquer: generate high-level proof sketch, extract subgoals, attempt to solve subgoals individually and assemble a full proof",
            "external_tool_used": true,
            "external_tool_description": "Lean compiler used to extract subgoals from proof sketches and to verify assembled proofs; DeepSeek-Prover used to attempt subgoal proofs.",
            "benchmark_name": "miniF2F (validation subset of 244 problems)",
            "benchmark_description": "miniF2F validation set used to test whether the divide-and-conquer pipeline yields additional solved problems beyond baseline provers.",
            "task_type": "formal proof generation via sketch-guided subgoal solving",
            "performance_metric": "Number of problems solved (out of 244)",
            "performance_value": "Pipeline solved 76/244 problems; DeepSeek-Prover-V1.5-RL alone solved 158/244 on same subset; only 1 problem solved uniquely by the pipeline beyond DeepSeek-Prover-V1.5-RL.",
            "comparison_with_baseline": "Performed substantially worse than DeepSeek-Prover-V1.5-RL standalone on the 244-sample validation subset (76 vs 158 solved); yielded only one unique additional proof.",
            "key_findings": "Sketch-based divide-and-conquer is promising conceptually but was inefficient in practice here and produced negligible marginal gain over a strong prover.",
            "limitations": "Pipeline complexity and failure in any subgoal causes overall failure; many machine-generated subproofs were incorrect or inadequate, limiting utility.",
            "uuid": "e6828.7",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SymPy integration",
            "name_full": "SymPy-based pre-simplification integration for Lean theorem proving",
            "brief_description": "A simple external symbolic-computation preprocessing step: SymPy simplifies algebraic expressions in Lean goals (A − B → simplify), which sometimes reduces goals to trivialities and increases solved counts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "SymPy (external CAS) used as preprocessor",
            "model_description": "Python-based computer algebra system (SymPy) used to algebraically simplify equations extracted from Lean goals before passing to prover.",
            "model_size": null,
            "architecture_type": "external symbolic algebra system (non-ML)",
            "training_data": "N/A (tool used at inference to simplify expressions)",
            "reasoning_method": "External symbolic simplification: parse A = B in Lean, convert to SymPy A-B, simplify; feed simplified goal back to prover",
            "external_tool_used": true,
            "external_tool_description": "SymPy invoked offline to simplify expressions; Lean used to extract/accept simplified forms; this was an auxiliary preprocessing step, not in main reported results.",
            "benchmark_name": "miniF2F",
            "benchmark_description": "miniF2F problems where algebraic simplification can reduce goal complexity; used to measure impact of CAS-based preprocessing.",
            "task_type": "preprocessing / simplification to aid formal proof generation",
            "performance_metric": "Fraction of problems solved via simplification; additional solves relative to Goedel-Prover-SFT",
            "performance_value": "SymPy simplification directly solved 9.4% of miniF2F by simplifying to 0 = 0; additionally solved 0.8% of miniF2F problems that Goedel-Prover-SFT (Pass@32) did not solve; no improvement at Pass@3200.",
            "comparison_with_baseline": "Helps on a small subset of problems by trivializing algebraic identities; not included in main reported model results since improvements did not generalize at larger inference budgets.",
            "key_findings": "External CAS preprocessing can trivially solve a non-negligible subset of problems but is not a universal fix; useful as an auxiliary tool.",
            "limitations": "Only addresses problems reducible by symbolic simplification; did not improve high-inference-budget performance and therefore not part of main results.",
            "uuid": "e6828.8",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "ABEL (comparison)",
            "name_full": "ABEL (Sample efficient online reinforcement learning for neural theorem proving)",
            "brief_description": "A competing approach (tree-search plus RL) reported on PutnamBench and cited for comparison; solves a similar number of Putnam problems at slightly different compute budgets.",
            "citation_title": "Sample efficient online reinforcement learning for neural theorem proving",
            "mention_or_use": "mention",
            "model_name": "ABEL",
            "model_description": "Tree-search method combining neural guidance with sample-efficient online RL for theorem proving (reported in PutnamBench comparisons).",
            "model_size": null,
            "architecture_type": "neural-guided tree search (non-whole-proof generation)",
            "training_data": "Not detailed in this paper (referenced prior work)",
            "reasoning_method": "Tree search with online RL and neural guidance",
            "external_tool_used": true,
            "external_tool_description": "Typically integrates with proof assistant (e.g., Lean) during search (paper references tree-search methods but details are in original ABEL work).",
            "benchmark_name": "PutnamBench",
            "benchmark_description": "Putnam competition problems formalized in Lean; 644 statements covering a range of mathematical topics.",
            "task_type": "formal theorem proving (proof search/tree search)",
            "performance_metric": "Number of Putnam problems solved (with associated compute / pass budgets)",
            "performance_value": "Reported to solve 7 PutnamBench problems (comparable to Goedel-Prover-SFT which solved 7 at Pass@512); ABEL achieved 7 at Pass@596 in leaderboard citation.",
            "comparison_with_baseline": "Used as leaderboard comparator; Goedel-Prover-SFT matched ABEL's problem count but at a slightly lower reported inference budget.",
            "key_findings": "Tree-search RL approaches remain competitive on some benchmarks; comparison highlights compute/compute-budget sensitivity across methods.",
            "limitations": "Details of ABEL's approaches (compute budgets, exact training data) require consulting ABEL's original paper; mentioned here for leaderboard comparison only.",
            "uuid": "e6828.9",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "InternLM2.5-StepProver (comparison)",
            "name_full": "InternLM2.5-StepProver",
            "brief_description": "Prior open-source prover that previously contributed proofs to Lean Workbook; used as a comparative baseline for cumulative solved counts in Lean Workbook.",
            "citation_title": "InternLM2.5-StepProver: Advancing automated theorem proving via expert iteration on large-scale lean problems",
            "mention_or_use": "mention",
            "model_name": "InternLM2.5-StepProver",
            "model_description": "An expert-iteration based prover (stepwise) from InternLM family; reported to have found a corpus of proofs in lean datasets.",
            "model_size": null,
            "architecture_type": "transformer-based stepwise prover with retrieval/search elements (per original work)",
            "training_data": "Lean Workbook and other formal corpora in prior InternLM work",
            "reasoning_method": "Stepwise proving with retrieval and/or search (different from whole-proof generation)",
            "external_tool_used": true,
            "external_tool_description": "Integrates with Lean for verification and potentially stepwise feedback (per prior InternLM work).",
            "benchmark_name": "Lean Workbook (solved counts); PutnamBench (reported solves)",
            "benchmark_description": "Lean Workbook: large collection of formal statements from AoPS; InternLM reported finding many of the 15.7K proofs previously available.",
            "task_type": "formal proof generation (stepwise / search-based)",
            "performance_metric": "Number of proofs found / leaderboard positions",
            "performance_value": "Previously contributed to 15.7K solved proofs in Lean Workbook (prior to Goedel-Prover's 29.7K total).",
            "comparison_with_baseline": "Goedel-Prover-SFT nearly doubled the number of proofs found in Lean Workbook compared to cumulative prior provers (29.7K vs 15.7K).",
            "key_findings": "Expert iteration and large-scale autoformalization can substantially increase the number of solvable formal statements; stepwise and whole-proof approaches are complementary.",
            "limitations": "Specific architecture and performance details are in InternLM's own papers; cited here as prior baseline and data contributor.",
            "uuid": "e6828.10",
            "source_info": {
                "paper_title": "Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data",
            "rating": 2,
            "sanitized_title": "deepseekprover_advancing_theorem_proving_in_llms_through_largescale_synthetic_data"
        },
        {
            "paper_title": "InternLM2.5-StepProver: Advancing automated theorem proving via expert iteration on large-scale lean problems",
            "rating": 2,
            "sanitized_title": "internlm25stepprover_advancing_automated_theorem_proving_via_expert_iteration_on_largescale_lean_problems"
        },
        {
            "paper_title": "Generative language modeling for automated theorem proving",
            "rating": 2,
            "sanitized_title": "generative_language_modeling_for_automated_theorem_proving"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        },
        {
            "paper_title": "Draft, sketch, and prove: Guiding formal theorem provers with informal proofs",
            "rating": 2,
            "sanitized_title": "draft_sketch_and_prove_guiding_formal_theorem_provers_with_informal_proofs"
        },
        {
            "paper_title": "ProofNet: Autoformalizing and formally proving undergraduate-level mathematics",
            "rating": 2,
            "sanitized_title": "proofnet_autoformalizing_and_formally_proving_undergraduatelevel_mathematics"
        },
        {
            "paper_title": "AI achieves silver-medal standard solving international mathematical olympiad problems",
            "rating": 1,
            "sanitized_title": "ai_achieves_silvermedal_standard_solving_international_mathematical_olympiad_problems"
        }
    ],
    "cost": 0.02111975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving
19 Apr 2025</p>
<p>Yong Lin 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Shange Tang shangetang@princeton.edu 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Bohan Lyu 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Jiayun Wu 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Hongzhou Lin 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Kaiyu Yang 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Jia Li 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>∥ Mengzhou Xia 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Danqi Chen 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Sanjeev Arora 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Chi Jin 
Princeton Language and Intelligence
Princeton University. ‡ Tsinghua University</p>
<p>Goedel-Prover: A Frontier Model for Open-Source Automated Theorem Proving
19 Apr 2025BD7E7BC0BC1AF9948ABCAA384A115FD2arXiv:2502.07640v3[cs.LG]
We introduce Goedel-Prover, an open-source language model that achieves state-of-the-art (as of April 5 2025) performance in automated formal proof generation for mathematical problems.A key challenge in this field is the scarcity of formalized mathematical statements and proofs, which we address through the following approaches.First, we train LLMs to convert natural language math problems from the Numina dataset to equivalent formal statements in Lean 4. This process creates the dataset Goedel-Pset-v1, which includes 1.64 million formal statements.Next, we develop a large dataset of formal proofs by training a series of provers.Each new prover can prove many statements that previous ones could not, and these new proofs are added to the training set for the next prover.Finally, we obtain the dataset Goedel-Pset-v1-solved, which contains proofs for over 800K statements from Goedel-Pset-v1.Supervised fine-tuning (SFT) of DeepSeek-Prover-V1.5-Base on Goedel-Pset-v1-solved (i.e., no RL) yields a model that achieves a success rate of 57.6% (Pass@32) on miniF2F benchmark, surpassing the previous leader DeepSeek-Prover-V1.5 (trained using SFT + RL on a proprietary dataset) by 7.6%.On PutnamBench, Goedel-Prover-SFT successfully solves 7 problems (Pass@512), ranking first on the leaderboard.Further RL training (including DPO) improves Goedel-Prover-SFT's success rate to over 60% (Pass@32) on miniF2F.To aid future research, we provide extensive discussion of our training methodology and design choices.We also fully open-source our codes, models, and datasets.Additionally, we open-source formal proofs for 29.7K problems in Lean Workbook, nearly doubling the 15.7K solved by prior provers.</p>
<p>Introduction</p>
<p>Recent advancements in large language models (LLMs) have demonstrated remarkable capabilities in reasoning tasks, especially in solving mathematical problems (Guo et al., 2025;Yang et al., 2024a).These models excel at reasoning through natural language, which we refer to informal reasoning.However, natural language-based reasoning is difficult to automatically verify by machines, which undermines the reliability of informal reasoning in practical applications.This also makes it more difficult to further improve the reasoning capabilities of language models.In contrast to informal reasoning, formal reasoning allows reasoning in a machine-verifiable format, opening up new possibilities for verification and automation.In particular, proof assistants such as Lean (De Moura et al., 2015;Moura &amp; Ullrich, 2021), Isabelle (Paulson, 1994), and Coq (Barras et al., 1997)  (Left) Pass@32 performance on miniF2F for whole-proof generation, compared to previous SOTA models.(Middle) A comparison of Goedel-Proverand DeepSeek-Prover-V1.5 on miniF2F performance across varying inference budgets, ranging from Pass@32, 64, 128, ..., to 4 × 6400.(Right) Goedel-Prover-SFT solves 29.7K problems in the Lean Workbook.In comparison, InternLM2.5-Step-Prover(Wu et al., 2024) and InternLM-Math-Plus (Ying et al., 2024b) collectively solved 15.7K samples.languages that can express reasoning in a way that can be mechanically verified.Thus, it is of great interest to train LLMs to write proofs in these formal languages.</p>
<p>A significant challenge in training LLMs for theorem proving in formal languages is the scarcity of formalized math statements and proofs.Writing proofs for theorems expressed in formal languages is highly demanding and necessitates considerable domain expertise.Therefore, existing publicly available datasets for formal languages are limited in size.For example, the Lean Workbook (including Lean Workbook Plus) dataset (Ying et al., 2024a;Wu et al., 2024) comprises a total of 140K formal statements, where formal statements refer to problem statements in Lean without proofs.However, only 15.7K of these statements come with formal proofs, which were found by InternLM2.5-StepProverand InternLM-Math-Plus (Ying et al., 2024a;Wu et al., 2024;Ying et al., 2024b).Additionally, the Open Bootstrapped Theorems dataset (Wang et al., 2024) includes 107K statements with proofs sourced from Mathlib4 (mathlib4, 2023).However, Mathlib4 exhibits significant distribution shift from general problem-solving benchmarks, such as the widely used miniF2F (Zheng et al., 2021).See Section 5 for detail.</p>
<p>In contrast to the scarcity of data in formal languages, there is a vast amount of math problems and solutions written in informal language.For example, Numina (Li et al., 2024a) includes 860K high-quality question and answer pairs sourced from MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AMC (aop), AIME (MAA, 2024), the AoPS Forum (aop), Chinese K-12 Exams (Shao et al., 2024), World Olympiads, and synthetic data (Mitra et al., 2024).We start by training LLMs to formalize the problem statements in this dataset into Lean.To increase the diversity of the formalization styles, we train two formalizers.One is trained on informal and formal (I-F) statement pairs from Lean Workbook, while the other is trained on I-F statement pairs annotated by Claude-sonnet-3.5 (Anthropic, 2024).We use these two formalizers to formalize the statements and then employ LLMs to ensure that the formal statements preserve the content of the informal statements.Our efforts result in 1.64 million formal statements.</p>
<p>Using this extensive dataset of formal statements, we employ expert iteration (Polu et al., 2022) to train the prover to generate proofs.Notably, we train a model to generate complete proofs solely based on statements, without interacting with the Lean compiler during the generation process.This approach is referred to as the whole-proof generation method (Jiang et al., 2022;Wang et al., 2024;Xin et al., 2024a;b).At the beginning of the expert iteration, we generate 16 proof candidates using DeepSeek-Prover-V1.5-RL(the previous SOTA) for each formal statement in our dataset, and then we verify the correctness of each candidate using Lean compiler.The correct proofs are then collected to train our iter-1 prover based on DeepSeek-Prover-V1.5-Base.In subsequent rounds, we utilize our iter-k prover to collect new proofs and add them to the training data.We then perform supervised fine-tuning starting from DeepSeek-Prover-V1.5-Base for another round, resulting in the iter-(k + 1) prover.We conduct a total of 8 iterations and observe a consistent improvement starting from the first iteration.</p>
<p>We demonstrate that expert iteration, with large-scale formalized statements can lead to SOTA performance in formal proof generation.Specifically,</p>
<p>• Our model, Goedel-Prover-SFT, outperforms DeepSeek-Prover-V1.5-RL(the previous SOTA model) by 7.6% on miniF2F, achieving a Pass@32 score of 57.6% (i.e., the prover generated 32 proofs for a problem, and 57.6% of the problems have at least one correct proof verified by Lean) compared to DeepSeek-Prover-V1.5-RL's 50.0%, as shown in Figure 1 (left).It consistently surpasses DeepSeek-Prover-V1.5-RLacross all sampling budgets, including Pass@32, 64, and up to 25600, as shown in Figure 1 (middle).</p>
<p>• We have cumulatively solved 29.7K problems in Lean Workbook, significantly increasing the existing 15.7K proofs found by InternLM2.5-StepProverand InternLM-Math-Plus (Wu et al., 2024;Ying et al., 2024b), as shown in Figure 1 (right).</p>
<p>• Goedel-Prover-SFT solves 7 problems on PutnamBench by Pass@512 1 , securing the #1 position on the leaderboard (Table 2).</p>
<p>• We open source our codes 2 , models 3 4 5 6 , datasets 7 8 , and the new proofs discovered 9 in the Lean Workbook to facilitate future research.</p>
<p>To understand the factors behind Goedel-Prover's strong performance, we provide an in-depth discussion of our training recipe, analyzing the effects of scaling up training data, the diversity introduced by autoformalization, correlations among datasets, and alternative data synthesis strategies.Furthermore, although our final model is trained purely through supervised fine-tuning, we also explore direct preference optimization (DPO) and reinforcement learning (RL) techniques built on top of it.Our Goedel-Prover-DPO and Goedel-Prover-RL achieves a success rate over 60% (Pass@32) on miniF2F.However, we also find that DPO and RL-trained models tend to overfit to "shortcuts" and benefit less from increased inference-time compute.</p>
<p>Related Work</p>
<p>Automated theorem proving.Automated theorem proving (ATP) is a long-standing problem in symbolic AI (Robinson &amp; Voronkov, 2001).Traditional approaches represent theorems in first-order logic and prove them using decision procedures (De Moura &amp; Bjørner, 2008;Barbosa et al., 2022) and search (Kovács &amp; Voronkov, 2013;Schulz et al., 2019).The proof search has been enhanced by replacing handcrafted heuristics with machine learning techniques (Urban et al., 2011;Kaliszyk et al., 2018).However, approaches based on firstorder logic struggle to scale to complex theorems and often do not yield human-readable proofs.</p>
<p>In recent years, learning-based theorem proving has undergone a significant transformation.</p>
<p>A notable approach, introduced by Polu &amp; Sutskever (2020), involves leveraging large language models to assist in theorem proving with proof assistants such as Lean (De Moura et al., 2015;Moura &amp; Ullrich, 2021) and Isabelle (Paulson, 1994).Follow-up research has explored various avenues, such as retrieving useful lemmas (Irving et al., 2016;Mikuła et al., 2024;Yang et al., 2024b), utilizing Monte Carlo tree search for proof discovery (Lample et al., 2022), and harnessing the capabilities of large language models (LLMs) for natural language reasoning (Jiang et al., 2022;Lin et al., 2024).Notably, Polu et al. (2023) was the first to apply expert iteration (Anthony et al., 2017) to theorem proving.This method alternates between two phases: (1) attempting to prove unsolved theorems and (2) enhancing the prover by incorporating newly discovered proofs into its training data.Expert iteration has yielded significant improvements in several recent provers (Wu et al., 2024;Xin et al., 2024b), including our Goedel-Prover.</p>
<p>Most automated theorem provers operate in a stepwise manner, generating individual proof steps that are then assembled into complete proofs using proof search algorithms.Recently, researchers have shown that generating entire proofs is feasible (First et al., 2023;Xin et al., 2024a;Wang et al., 2024).This approach avoids the costly search process, resulting in lower latency and potentially offering a more efficient use of computational resources during testing.While Goedel-Prover also generates whole proofs, our data and methodology can, in principle, be adapted to develop stepwise provers as well.</p>
<p>Autoformalization and synthetic data generation.The shortage of high-quality formal mathematical data poses a significant bottleneck in training theorem-proving models.</p>
<p>While techniques like reinforcement learning may reduce the reliance on human-written proofs (Google DeepMind, 2024), there remains a need for a substantial number of formal theorem statements.A promising approach is to synthesize formal statements through autoformalization, where large language models (LLMs) translate informal mathematical statements into formal ones (Wu et al., 2022;2024;Xin et al., 2024a;b).</p>
<p>DeepSeek-Prover (Xin et al., 2024a) and InternLM2.5-StepProver(Wu et al., 2024) have successfully implemented this strategy to formalize a large volume of statements into Lean for expert iteration.We adopt a similar approach.The difference is: while Liu et al. (2024) focuses on formalizing their internal dataset, we concentrate on formalizing the Numina dataset (Li et al., 2024a) alongside a privately collected dataset.Additionally, we train two formalizers to enhance the diversity of formalization styles, which we demonstrate to be beneficial in Section 4.</p>
<p>Method</p>
<p>We begin by translating informal statements (expressed in natural language) into formal statements (represented in Lean).Using these formal statements, we iteratively train our prover with proofs generated by the prover and verified by the Lean compiler.The details of each step are elaborated in the following parts.</p>
<p>Statement Formalization</p>
<p>We first train the statement formalizers to translate informal statements in Numina into formal statements as shown in Figure 2. To enhance the diversity of formalized statements, we train two models to formalize informal statements.</p>
<p>• Formalizer A: We train the Formalizer A model using Formal and Informal (F-I) statement pairs sourced from Lean Workbook.• Formalizer B: We employ Claude-sonnet-3.5 to formalize 230K statements from Numina.</p>
<p>From this set, we extract 170K statements that successfully passed Lean compilation.These 170K F-I statement pairs are then used to train Formalizer B.</p>
<p>Both Formalizer A and B are trained using supervised fine-tuning with Qwen2.5-Coder-32B10 .The training of these two formalizers takes less than 24 hours on 8 H100 GPUs.See Appendix A.1 for examples of formalized statements form two formalizers, where we observe that even for the same problem, the style of the formalized statement can impact the prover's performance.Quality assessment.We employ two tests to assess the quality of the formalized statements.First, the formalized statement must conform to Lean syntax and can successfully compile, with the potential proof replaced by the placeholder ":= by sorry".This syntax check is known as the Compiling Correctness (CC) Test in the literature (Ying et al., 2024a).Second, the formalized statement must accurately capture the original informal problem, incorporating all assumptions, conditions, and implicit definitions.We refer to this second test as the Faithfulness and Completeness (FC) Test.For the FC test, we use Qwen2.5-72B-Instruct11, details are presented in Appendix A.2.</p>
<p>In addition to formalizing the 860K open-sourced Numina (Li et al., 2024a) datasets, we also formalize a private 68K collection of math problems from Art of Problem Solving (AOPS), which has been collected and processed by the Numina group (Li et al., 2024a).Out of a total of 928K informal statements, 760K have two valid formalized statements generated by Formalizer A and B, while 123K contain only one valid formalized statement.After formalizing both the Numina and AOPS datasets, we further incorporate 140K statements from Lean Workbook, including Lean Workbook Plus.As a result, we have a total of 1.78M formal statements.</p>
<p>Expert Iteration</p>
<p>After obtaining a large collection of formalized statements in Section 3.1, we employ expert iteration to train the prover (Liu et al., 2024;Wu et al., 2024;Li et al., 2024b), which is illustrated in Figure 3. Specifically, we first utilize DeepSeek-Prover-V1.5-RL12 to generate 16 proofs for each statement.We then verify these proofs with the Lean compiler.If at least one proof solves the statement, we retain one proof per statement.In cases where multiple proofs are available, we randomly sample one solution.These collected proofs are used for supervised fine-tuning (SFT) based on DeepSeek-Prover-V1.5-Base13, resulting in the iter-1 prover.We continue this expert iteration process; each time, we use the iter-k prover to generate answers and cumulatively collect correct solutions to train DeepSeek-Prover-V1.5-Base for the next iteration, the iter-(k + 1) prover.Refer to Appendix B for more details on each iteration.</p>
<p>We experiment with learning rates of 1 × 10 −4 and 5 × 10 −5 , training for either 1 or 2 epochs.We use the packing trick (Tunstall et al., 2022) with a small batch size of 8 to speed up the training.In each iteration, the training time for 1 epoch is approximately 12 hours using 4 H100 GPUs.The inference time for the 1.78M statements set by Pass@16 is 6 hours, utilizing 64 H100 GPUs.Additionally, the verification time for these proofs requires 10 hours with 8,000 CPUs.</p>
<p>Results</p>
<p>Benchmarks.Following the works of (Wang et al., 2024;Xin et al., 2024a;Wu et al., 2024;Li et al., 2024b), we primarily use miniF2F (Zheng et al., 2021) as our main evaluation benchmark.We also track the problems solved by our prover in Lean Workbook (Ying et al., 2024a) and investigate the performance on ProofNet (Azerbayev et al., 2023) and PutnamBench (Tsoukalas et al., 2024).Additionally, we uniformly sample a subset from our formalized dataset to create a held-out evaluation dataset.Below, we provide descriptions of each dataset.</p>
<p>• miniF2F (Zheng et al., 2021) (Xin et al., 2024a) 32 46.1% ± 0.5% DeepSeek-Prover-V1.5-SFT(Xin et al., 2024b) 32 48.2% ± 0.6% DeepSeek-Prover-V1.5-RL(Xin et al., 2024b) 32 50.0%± 0.5% Goedel-Prover-SFT 32 57.6% ± 0.7%</p>
<p>DeepSeek-Prover-V1.5-SFT (Xin et al., 2024b) 3200 53.3% DeepSeek-Prover-V1.5-RL(Xin et al., 2024b) 3200 54.9% Goedel-Prover-SFT 3200 62.7%</p>
<p>DeepSeek-Prover-V1.5-SFT (Xin et al., 2024b) 4 × 6400 55.8% DeepSeek-Prover-V1.5-RL(Xin et al., 2024b) 8.</p>
<p>DeepSeek-Prover-V1.5-RL, by 7.6%.We observe that our Goedel-Prover-SFT's Pass@32 is even better than DeepSeek-Prover-V1.5-RL'sPass@3200 by 2.7%.Furthermore, when both evaluated by Pass@3200, our model achieves 62.7%, surpassing DeepSeek-Prover-V1.5-RL's54.9% by 7.8%.Figure 1 illustrates the inference time scaling curve for our Goedel-Prover-SFT, DeepSeek-Prover-V1.5-RL and DeepSeek-Prover-V1.5-SFT.Goedel-Prover-SFT demonstrates significant improvements over both DeepSeek-Prover-V1.5-RL and DeepSeek-Prover-V1.5-SFTacross all inference compute budgets.Figure 4 illustrates the performance of our model during each iteration.Overall, we observe a relatively consistent improvement in performance across iterations.</p>
<p>PutnamBench performance.Goedel-Prover-SFT solves 7 out of 644 problems in Putnam-Bench (Pass@512), achieving the first place on the PutnamBench leaderboard.The previous SOTA method ABEL (Gloeckle et al.) solves 7 with a slightly higher inference budget (Pass@596) and InternLM2.5-Step-Prover(Wu et al., 2024) solves 6 (Pass@2 × 32 × 600).The performance is summarized in Table 2.</p>
<p>Proofs found in Lean Workbook.The Lean Workbook, which includes Lean Workbookplus (Ying et al., 2024a;Wu et al., 2024), formalizes 140K high-quality problems sourced from AOPS and the Compfiles data.Currently, proofs for only 15.7K statements in Lean Workbook have been found and made open-source by InternLM2.5-StepProver(Wu et al., 2024) and InternLM-Math-Plus (Ying et al., 2024b).In contrast, our model has discovered a significantly larger set of proofs within Lean Workbook, cumulatively solving 29.7K problems, as shown in Figure 1 (right).We open-source all the proofs found by our model to benefit the research community.Scaling up the number of formal statements improves model performance.Figure 5 shows the performance of provers (average on miniF2F, ProofNet and NuminaTest) trained on different sizes of the formal statement set.For each statement, the corresponding proof is obtained using Goedel-Prover-SFT.We observe a consistent improvement in model performance as the size of the statement set increases, underscoring the value of scale in training effective provers.</p>
<p>Dissecting the training recipe</p>
<p>Increasing the diversity of formalization styles is beneficial.Table 3 presents the performance of iter-8 provers trained on different formalization styles of statements, with proofs generated by the iter-7 prover.We find that a prover trained on a mixture of styles-combining statements produced by both Formalizer A and Formalizer B-outperforms provers trained on a single formalization style.This result suggests that exposure to diverse formalization patterns improves the model's generalization and reasoning ability.</p>
<p>Correlations among datasets.</p>
<p>We evaluate model performance across different training iterations and hyperparameter settings, and compute the correlation of performance across multiple datasets (see Figure 6).We observe that the model performance on ProofNet is negatively correlated with the performance on miniF2F   Alternative approach for data synthesis.In addition to autoformalizing statements and use the prover to provide proofs, we also explored alternative strategies for constructing training datasets, focusing on solving difficult problems by a divide-and-conquer strategy.</p>
<p>Inspired by Jiang et al. ( 2022), we implemented the following pipeline: (1) generate a proof for a formal statement using OpenAI's o1-preview model, (2) extract a high-level "sketch" of the proof and (3) apply DeepSeek-Prover-V1.5-RL to prove the subgoals provided by the sketch.If all the subgoals are successfully completed, we obtain a valid proof for the original problem.Implementation details are provided in Appendix D. However, this pipeline turned out to be ineffective in practice.When applied to the miniF2F validation set (244 problems), it successfully solved only 76 problems-considerably fewer than the 158 problems solvable by DeepSeek-Prover-V1.5-RLalone.Moreover, out of the 76 problems solved, only one is not solved by DeepSeek-Prover-V1.5-RL,implying that the marginal gain from this pipeline is limited.</p>
<p>Exploring DPO and RL training.We further explored DPO and RL training on top of Goedel-Prover-SFT.We implemented offline Direct Preference Optimization (DPO) (Rafailov et al., 2023) and online Group Relative Policy Optimization (GRPO) (Shao et al., 2024), implementation details are provided in Appendix E. Table 5 shows that although DPO and GRPO improve the model's Pass@32 performance, the average proof length grows substantially, and the frequency of certain patterns increases sharply.This phenomenon indicates that the model is overfitting to some syntactic patterns or "shortcuts", which is related to "reward hacking" (Chen et al., 2024).For example, the Lean tactic try allows trying a tactic and continue execution regardless of whether it works or not.Although often harmless-and occasionally useful-its overuse can lead to ineffective proofs and Table 5: Models' behavior under different training methods.RL methods show improvement on miniF2F at Pass@32, but the improvement at Pass@3200 is limited .Furthermore, RL models are prone to excessively favor patterns such as try, which also causes the proof length to increase.substantial verification costs.The RL-trained model begins to excessively favor this pattern, ultimately impairing its reasoning and generalization capabilities.</p>
<p>Further experiments show that adding a length penalty during DPO training helps reduce this overfitting.However, we observe that scaling up inference-time compute yields significantly smaller gains for models fine-tuned with either GRPO or length-penalized DPO, compared to the SFT model.As shown in Table 5, these models achieve a 3% improvement over Goedel-Prover-SFT on Pass@32, but this gain diminishes when increasing inferencetime compute-for example, at Pass@3200.This indicates that RL training may reduce output diversity, leading to less efficient inference-time scaling.</p>
<p>Discussion</p>
<p>Further discussions on the characteristics of proofs generated by Goedel-Prover-SFT and potential areas for improvement are provided in Appendix F.</p>
<p>C More examples on style difference C.1 Mathlib4 and miniF2F</p>
<p>We observe a notable difference in the distribution of Mathlib4 compared to that of general problem-solving benchmarks, such as the widely used miniF2F (Zheng et al., 2021).For instance, miniF2F largely consist of competition and Olympic-style problems, which require complex reasoning, while only depending on a relatively small set of elementary facts about integers, real numbers, counting, and geometry.On the contrary the statements in Mathlib4 focus on the simple manipulation of advanced mathematical concepts.Figure 8 and 9 show the statement and proof in Mathlib4 and miniF2F respectively.It can be easily seen that both the statement and proof rely on pre-defined objects.Unlike miniF2F statements, the example in Figure 8 can not even pass the lean compilation, given that pre-defined objects are missing.</p>
<p>Figure 8: A Mathlib4 example which relies on pre-defined objects @Acc.ndrec and @Acc.ndrecC</p>
<p>C.2 ProofNet and miniF2F</p>
<p>The problems in ProofNet are primarily drawn from undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology.These topics largely rely on the abstract and general formulations of mathematical definitions in Mathlib4 (mathlib4, 2023).We show two examples in Table 9 to illustrate the style difference between ProofNet and miniF2F.</p>
<p>D Alternative approach for synthesizing data</p>
<p>We also considered other pipeline beyond autoformalizing statement and expert iteration for collecting proof data.Inspired by Jiang et al. (2022), we implemented the following pipeline:</p>
<p>Step 1.We prompt OpenAI's o1-preview model to generate a proof for a formal statement.We ask the model to generate the proof step-by-step, use "have" tactic to structure the proof.</p>
<p>For each proof step, the subgoal of this step is indicated by "have", following by proofs for this subgoal.</p>
<p>Example from ProofNet Example from miniF2F</p>
<p>Informal Statement</p>
<p>Prove that no order can be defined in the complex field that turns it into an ordered field.</p>
<p>Show that for any natural number n, 7 does not divide 2 n + 1.</p>
<p>Formal Statement</p>
<p>Comments</p>
<p>This problem involves the notion of order, which is undergraduate level.Its formal statement uses the definition IsLinearOrder in Mathlib4.</p>
<p>This problem comes from IMO but only involves division.Step 2. We remove the proofs for the subgoal provided by o1-preview in each "have" block (these proofs often involves detailed lean syntax, and is usually incorrect).That is, we only keep the "sketch" of the proof.We then put this proof sketch into Lean compiler, to automatically extract each subgoal and corresponding conditions, to form several subproblems.</p>
<p>Step 3.</p>
<p>We apply DeepSeek-Prover-V1.5-RL to try to proof the subproblems.We try each subproblem for 32 times.If all subproblems are successfully proved, assembling these subproofs into the sketch gives us a valid proof for the original problem.</p>
<p>Figure 10 shows the only problem solved by this pipeline that DeepSeek-Prover-V1.5-RLdoes not solve, which is a non trivial problem that requires relatively complex reasoning.</p>
<p>Though this pipeline has shown potential, the efficiency is quite low.Only one additional problem is proved using this pipeline, among 244 problems in miniF2F validation set.This might due to the fact that this pipeline is overly complicated, since failure of each subproblem might lead to the failure for the entire problem.</p>
<p>E RL training details E.1 DPO training</p>
<p>For DPO training, we construct pairwise data on problems with pass ratio in (0, 1/4] (from previous training dataset).To be specific, for each problem, we do Pass@16, and the pass ratio (0, 1/4] means we select samples where Goedel-Prover-SFT generates 1-4 correct proofs within 16 trails.We construct DPO pairs by randomly select a correct proof and wrong proof from the 16 trials.We sample 508K proved problems from the original dataset, and among which 30K problems with the aforementioned pass ratio is selected.We use a learning rate of 5 × 10 −6 and train for two epoches.</p>
<p>Our experiments reveal that through DPO training, the model is easy to learn "shortcuts".Figure 11 shows one typical output of the DPO model.It repeatedly use tactics all goals and try, which might be shortcuts learned in DPO training.To mitigate the model's tendency to produce verbose, lengthy proofs by repeatedly utilizing these shortcuts, we implement length regularization in our DPO framework.Specifically, when multiple correct answers are available for a given statement, we select the one with the shortest length.All other settings remain unchanged from the original DPO implementation.</p>
<p>E.2 GRPO training</p>
<p>We collect 80K problem statements whose pass ratio is within (0, 1/2].We will also explore different design choices for the included problems in the subsequent discussion.Using these problem statements, we employed the Goedel-Prover-SFT as our base model and conducted reinforcement learning (RL) training within the OpenRLHF framework, utilizing the GRPO algorithm.During the RL training, we generated 16 proofs for each problem and verified their correctness through compilation.Correct proofs received a reward of +8, while incorrect proofs received a penalty of -8.We search for the learning rate among 1 × 10 −5 , 5 × 10 −6 , 2 × 10 −6 , and 1 × 10 −6 and choose the learning rate 5 × 10 −6 .We explored initial</p>
<p>F Discussion</p>
<p>We delve into the characteristics of proofs generated by Goedel-Prover-SFT and discuss potential directions for improvement, particularly regarding the proof style adopted by the model, the role of search as well as online interaction in proof generation, and the integration of external symbolic computation tools such as SymPy.</p>
<p>The Proof Style.We observe that the proofs provided by Goedel-Prover-SFT often rely on high-level tactics such as nlinarith and simp all among others.These high-level tactics handle multiple reasoning steps internally, delegating the resolution of intermediate steps to their built-in automation.For example, the nlinarith tactic can automatically solve certain linear and non-linear equalities and inequalities.Figure 13 shows a typical proof generated by our prover.The first several steps involve only trivial transformations of the original problem, whereas the final line uses nlinarith to immediately achieve the goal.Whether this style of proof is sufficient for complex reasoning remains an important area for exploration.Search and online interaction.Currently, Goedel-Prover-SFT generates the entire proof for the problem at once, without receiving further feedback.While our current approach is appealing in terms of computation, incorporating search and interaction in future work could enhance performance.For example, once a tactic is generated by our prover, it can interact with the Lean compiler to receive feedback on how the goal changes after the tactic is applied.This information can then be utilized in generating the next tactic, potentially improving the overall proof strategy (Wu et al., 2024).</p>
<p>SymPy.Future work may aim to leverage other software packages to enhance Lean's capabilities.For instance, Lean's ring tactic can handle algebraic simplifications by applying axioms such as distributivity, associativity, and commutativity.However, a combination of tactics is required for non-algebraic transformations of transcendental functions, such as logarithmic and trigonometric functions, and other advanced simplifications beyond commutative rings.We explored using a Python-based computer algebra system, SymPy (Meurer et al., 2017), to simplify complex expressions in theorem statements and feed the simplified form into the prover.Specifically, we parse equations of the form A = B within the goals of Lean theorem statements, construct the SymPy expression A − B, and then apply the simplify method in Lean.This procedure directly solves 9.4% of miniF2F by simplifying the statements to 0 = 0.In addition, it solves 0.8% of the problems in miniF2F that were unsolved by Goedel-Prover-SFT with Pass@32, but did not improve Goedel-Prover-SFT with Pass@3200.Thus, SymPy simplification is not part of any of our reported results.However, we think such procedures need further exploration.</p>
<p>Figure1: (Left) Pass@32 performance on miniF2F for whole-proof generation, compared to previous SOTA models.(Middle) A comparison of Goedel-Proverand DeepSeek-Prover-V1.5 on miniF2F performance across varying inference budgets, ranging from Pass@32, 64, 128, ..., to 4 × 6400.(Right) Goedel-Prover-SFT solves 29.7K problems in the Lean Workbook.In comparison, InternLM2.5-Step-Prover(Wu et al., 2024) and InternLM-Math-Plus(Ying et al., 2024b) collectively solved 15.7K samples.</p>
<p>Figure 2 :
2
Figure 2: This figure illustrates the training of the formalizers.The term "F-I statement pairs" refers to pairs consisting of Formal and Informal (F-I) statements.An example is shown on the right part.We train two formalizers, Formalizer A and B, using F-I statement pairs sourced from various origins.</p>
<p>Figure 3 :
3
Figure 3: This figure illustrates the process of expert iteration.Each time, we utilize our iter-k prover to collect new proofs and add them to the training data.We then conduct supervised fine-tuning starting from DeepSeek-Prover-V1.5-Base for another round, resulting in the iter-(k + 1) prover.</p>
<p>Figure 4 :
4
Figure 4: The figures show the performance of our model on the four datasets at each iteration.We gradually increase the size of the problem set and add more training data.The details of each iteration are shown in Table8.</p>
<p>Figure 5 :
5
Figure 5: Model performance under different size of training statement set, illustrating the value of scale.</p>
<p>Figure 6 :
6
Figure 6: Correlation of model performance across datasets.ProofNet shows low correlation with the others.</p>
<p>Figure 7 :
7
Figure 7: Prompts for Faithfulness and Completeness (FC) Test.</p>
<p>Figure 9 :
9
Figure 9: A miniF2F example which does not rely on pre-defined objects</p>
<p>Figure 10 :
10
Figure 10: A non trivial problem solved by the divide-and-conquer pipeline</p>
<p>Figure 11 :
11
Figure 11: Example of output of DPO model.The model is repeatedly using all goals and try.</p>
<p>Figure 13 :
13
Figure 13: Example of proof style, where intermediate steps are absorbed in high-level tactics</p>
<p>(Tsoukalas et al., 2024)., 2023)is a formal theorem proving benchmark of undergraduate-level mathematics, consisting of 371 problem statements in Lean (185 validation and 186 test problems).The problems are primarily drawn from undergraduate pure mathematics textbooks, covering topics such as real and complex analysis, linear algebra, abstract algebra, and topology.The original benchmark was released in Lean 3, and for our analysis, we use the version of ProofNet in Lean 4.9.0 provided byXin et al. (2024a).•LeanWorkbook(Yingetal., 2024a) is a large-scale Lean 4 problem set formalized from natural language math problems (mainly from the forum AOPS), which consists of 140K statements in Lean 4. We also monitor the problems solved by our model during the expert iteration process.Notably, the problem set from Lean Workbook is included in this training, which is consistent with DeepSeek-Prover-V1.5(Xinetal., 2024a)and InternLM2.5-StepProver(Wuetal., 2024).•PutnamBench(Tsoukalaset al., 2024)is a formal theorem proving benchmark on competition mathematics problems sourced from the William Lowell Putnam Mathematical Competition years 1962 -2023.PutnamBenchcomprises 644 Lean 4 statements, covering algebra, analysis, number theory, geometry, combinatorics, probability and set theory.• NuminaTest.We randomly sample 250 statements from our formalized Numina dataset and use it as a held-out testing set.We refer to this subset as NuminaTest.
Whole-Proof Generation ModelPassPerformanceTheoremLamma (Wang et al., 2024)12833.6%Deepseek-Prover-V1
Xin et al. (2024a)m proving benchmark, consisting of 488 problem statements (244 validation and 244 test problems) in Lean.The problems are drawn from high-school exercises, as well as high-school level competitions including the AIME, AMC, and the International Mathematical Olympiad (IMO).The original benchmark was released in Lean 3, and for our analysis, we use the version of miniF2F in Lean 4.9.0 provided byXin et al. (2024a).•Mainresults.The performance on miniF2F is shown in Table1.The Pass@32 performance of our Goedel-Prover-SFT is 57.6%, surpassing the previous SOTA open source model,</p>
<p>Table 1 :
1
Whole-proof generation performance on miniF2F.
4 × 640058.5%Goedel-Prover-SFT4 × 640064.7%</p>
<p>Table 2 :
2
Number of problems solved on PutnamBench statements (out of 644).Goedel-Prover-SFT achieves the first place in the leaderboard.The performance numbers for existing works are taken from the leaderboard.Here ⋄ inidicates open-source models.
Ranking ModelTypeNum-solved Compute (Pass)1Goedel-Prover-SFT ⋄Whole-Proof Generation75121ABEL (Gloeckle et al.)Tree Search Method75963Goedel-Prover-SFT ⋄Whole-Proof Generation6323InternLM2.5-StepProver (Wu et al., 2024) ⋄Tree Search Method62×32×6005InternLM 7B (Ying et al., 2024b) ⋄Whole-Proof Generation440966GPT-4oWhole-Proof Generation1107COPRA (GPT-4o) (Thakur et al., 2023)Whole-Proof Generation118ReProver w/ retrieval (Yang et al., 2024b) ⋄Whole-Proof Generation019ReProver w/o retrieval (Yang et al., 2024b) ⋄ Whole-Proof Generation01</p>
<p>Table 3 :
3
An ablation study on using two formalizers to formalize the statements.Using statements formalized by both formalizers improves the model's performance, illustrating the value of diverse formalization styles.</p>
<p>Table 4 :
4
Incorporating Mathlib4 into the training data enhances performance on ProofNet but reduces performance on miniF2F and NuminaTest, suggesting distribution shift between Mathlib4/ProofNet and other datasets.
ModelTraining DatasetminiF2F ProofNet NuminaTest AverageDeepseek-RL-50.0%16.0%53.6%39.9%Iter-6 proverIter-5 proofs56.6%13.3%59.2%43.0%Iter-6 proverIter-5 proofs + Mathlib454.1%15.6%58.8%42.8%Training methodPass@32 (minif2f)Pass@3200 (minif2f)Average proof lengthAverage number of tactic "try"SFT57.5%62.7%2981.50DPO60.3%64.6%48610.89Length-penalized DPO59.8%63.1%3081.11GRPO60.5%63.1%3555.16</p>
<p>Table 7 :
7
Quality assessment of the formalized statement the statements formalized by Claude-sonnet-3.5.At iter-3, we train the Formalizer B and add the formalized statements generated by Formalizer B for iter-4 to iter-6.At iter-7, we begin to add the statements generated by Formalizer A. We also add Mathlib4 data into the training dataset for better ProofNet performance when starting from iter-6.
ModelPassFormalizer A Formalizer BCC TestPass@176.74%88.48%CC TestPass@895.93%98.59%FC TestPass@148.06%80.42%FC TestPass@888.01%97.22%CC + FC Test Pass@145.72%76.41%CC + FC Test Pass@882.33%95.78%IterationStatements Lean Workbook Formalized Lean Workbook Solved Formalized Solved Mathlib4 Training DataIter-0140K020.6K00Iter-1140K140K20.6K72.4K0Iter-2140K270K23.0K128.7K0Iter-3140K270K24.4K161.2K0Iter-4140K882K25.4K425.8K0Iter-5140K882K27.0K436.5K0Iter-6140K882K27.8K443.2K104KIter-7140K1.64M28.8K887.7K104KIter-8140K1.64M29.7K915.7K104KIter-9140K1.64M30.3K928.2K104K</p>
<p>Table 8 :
8
Expert iteration details.</p>
<p>Table 9 :
9
Comparison of Examples from ProofNet and miniF2F.ProofNet largely relies on the abstract and general formulations of mathematical results in Mathlib4.In contrast, miniF2F largely consists of high-school competition and Olympic style problems, which require complex reasoning.</p>
<p>Table 11 :
11
Investigation on the reward for timeout samples
Timeout Reward Testing Timeout Ratio Testing Accuracy04.5%58.7%-81.7%60.2%-160.8%59.2%
https://huggingface.co/Qwen/Qwen2.5-Coder-32B
https://huggingface.co/Qwen/Qwen2.5-72B-Instruct
https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL
https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-Base
https://huggingface.co/Qwen/Qwen2.5-72B-Instruct
AcknowledgmentsWe thank Haoyu Zhao, Hubert Strauss and Suozhi Huang for their helpful discussions.2 https://github.com/Goedel-LM/Goedel-Prover 3https://huggingface.co/Goedel-LM/Goedel-Prover-SFT 4 https://huggingface.co/Goedel-LM/Goedel-Prover-DPO 5 https://huggingface.co/Goedel-LM/Goedel-Formalizer-32B-SonnetAnnotated 6 https://huggingface.co/Goedel-LM/Goedel-Formalizer-32B-LeanWorkbookAnnotated 7 https://huggingface.co/datasets/Goedel-LM/Goedel-Pset-v1 8 https://huggingface.co/datasets/Goedel-LM/Goedel-Pset-v1-solved 9 https://huggingface.co/datasets/Goedel-LM/Lean-workbook-proofsAppendix A Statement Formalization DetailsA.1 Examples of formalized statementsTable6presents two examples in which both Formalizer A and Formalizer B yield reasonable formalizations.However, our final prover exhibits varying performance on these formalized statements, highlighting the influence of formalization style on model effectiveness.Example 1Example 2Informal StatementThe function f (x) = 2 |x| + 3x 2 + ax + 1 is an even function, then a equals a = 0.If x and log 10 x are real numbers and log 10 x &lt; 0, show that 0 &lt; x &lt; 1.In contrast, Formalizer B first introduces a function called "IsEven" and then defines the even function using "IsEven".Notably, our prover successfully solves the statements provided by Formalizer A but fails with those from Formalizer B. Example 2 is similar; however, our prover fails to solve the statement provided by Formalizer A but succeeds with the one from Formalizer B.A.2 Quality Assessment DetailsFor the FC test, we use Qwen2.5-72B-Instruct 14with prompt shown in Figure7.For each formalized statement, we generate four independent judgments, and the FC score is calculated as #{"Appropriate" in four Judgments}/4.For example, if the four judgments produced by Qwen2.5-72B-Instructinclude three "Appropriate" and one "Inappropriate", the overall FC score is calculated as 0.75.We filter out formalized statements with an FC score less than 0.5.For each informal statement in Numina, we generate eight formalized statements from each formalizer, resulting in 16 formalized statements per problem.Each statement undergoes the CC and FC Test, and we retain only those valid statements.We then randomly select one valid statement from each formalizer.For example, if five out of eight statements from Formalizer A and three from Formalizer B are valid, we randomly choose one from each.If a formalizer produces no valid statements, we exclude all its statements for that problem.The statistics for each test conducted on both formalizers are summarized in Table7.B Expert Iteration DetailsThe main training pipeline is illustrated in Section 3.2.When we implement the expert iteration algorithm, we gradually add the data.From iter-0 to iter-3, we gradually add Mismatch between reinforcement learning (RL) reward and test accuracy.Figure12plots the average training reward and Pass@16 accuracy across training batches.Notably, we observe a mismatch between the reward and accuracy trends: while the average reward continues to increase throughout training, the Pass@16 accuracy plateaus after approximately 20 training steps.This discrepancy may stem from the misalignment between the optimization objective and the evaluation metric.GRPO encourages generating successful proofs more frequently, rewarding higher success rates across samples.In contrast, the Pass@N metric only considers whether a problem is solved at least once, irrespective of how many successful attempts occur.As a result, improvements in reward do not necessarily translate into better Pass@N performance.Exploration of included prompts for training RL.We previously mentioned that we use statements with a pass ratio within (0, 1/2] for training the RL model.This selection is based on the fact that these samples are challenging yet manageable for the current checkpoint.We also conducted experiments with pass ratios of (0, 1/4], (0, 3/4], and (0, 1].Our findings indicate that balancing the difficulty of the chosen prompts is crucial, and we compared their performance in terms of final testing results in Table10.Exploration on the reward design for timeout samples.Typically, when using the Lean compiler to verify a Lean proof, we encounter three possible outcomes: successful compilation, failure with returned errors, or a timeout within the predefined time limit.We experiment with various rewards for the timeout samples, while maintaining a fixed reward of +8 for correct proofs that compile successfully and -8 for incorrect proofs that fail to compile.The results in Table11demonstrates that setting the reward for timeouts to be the same as that for failures results in improved performance across these experiments.
Art of problem solving wiki. </p>
<p>Thinking fast and slow with deep learning and tree search. Thomas Anthony, Zheng Tian, David Barber, Neural Information Processing Systems (NeurIPS). 2017</p>
<p>Claude 3.5 sonnet. Anthropic, 2024</p>
<p>Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, arXiv:2302.124332023arXiv preprint</p>
<p>Abdalrhman Mohamed, Mudathir Mohamed, Aina Niemetz, Andres N ötzli, et al. cvc5: A versatile and industrial-strength smt solver. Haniel Barbosa, Clark Barrett, Martin Brain, Gereon Kremer, Hanna Lachnitt, Makai Mann, International Conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer2022</p>
<p>The Coq proof assistant reference manual: Version 6.1. PhD thesis, Inria. Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël Courant, Jean-Christophe Filliatre, Eduardo Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, 1997</p>
<p>Lichang Chen, Chen Zhu, Davit Soselia, Jiuhai Chen, Tianyi Zhou, Tom Goldstein, Heng Huang, Mohammad Shoeybi, Bryan Catanzaro, arXiv:2402.07319Disentangled reward mitigates hacking in rlhf. Odin2024arXiv preprint</p>
<p>Training verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Z3: An efficient smt solver. Leonardo De, Moura , Nikolaj Bjørner, International conference on Tools and Algorithms for the Construction and Analysis of Systems. Springer2008</p>
<p>The lean theorem prover (system description). Leonardo De Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, Jakob Von Raumer, Automated Deduction-CADE-25: 25th International Conference on Automated Deduction. Proceedings. Berlin, GermanySpringerAugust 1-7, 2015. 201525</p>
<p>Baldur: Whole-proof generation and repair with large language models. Emily First, Markus N Rabe, Talia Ringer, Yuriy Brun, ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ESEC/FSE2023</p>
<p>Abel: Sample efficient online reinforcement learning for neural theorem proving. Fabian Gloeckle, Jannis Limperg, Gabriel Synnaeve, Amaury Hayat, The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24. </p>
<p>AI achieves silver-medal standard solving international mathematical olympiad problems. Google Deepmind, 2024</p>
<p>Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.129482025arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Franc ¸ois Chollet, and Josef Urban. Deepmath-deep sequence models for premise selection. Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas Eén, Advances in neural information processing systems. 292016</p>
<p>Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. Sean Albert Q Jiang, Jin Peng Welleck, Wenda Zhou, Jiacheng Li, Mateja Liu, Timothée Jamnik, Yuhuai Lacroix, Guillaume Wu, Lample, arXiv:2210.122832022arXiv preprint</p>
<p>Laura Kovács and Andrei Voronkov. First-order theorem proving and vampire. Cezary Kaliszyk, Josef Urban, Henryk Michalewski, Miroslav Olšák, International Conference on Computer Aided Verification. Springer2018. 201331Neural Information Processing Systems (NeurIPS)</p>
<p>Hypertree proof search for neural theorem proving. Guillaume Lample, Timothee Lacroix, Marie Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, Xavier Martinet, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Q Jiang, Ziju Shen, Hugging Face repository. 132024a</p>
<p>Hunyuanprover: A scalable data synthesis framework and guided tree search for automated theorem proving. Yang Li, Dong Du, Linfeng Song, Chen Li, Weikang Wang, Tao Yang, Haitao Mi, arXiv:2412.207352024barXiv preprint</p>
<p>Haohan Lin, Zhiqing Sun, Yiming Yang, Sean Welleck Lean-Star, arXiv:2407.10040Learning to interleave thinking and proving. 2024arXiv preprint</p>
<p>Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>American invitational mathematics examination -aime. American Invitational Mathematics Examination -AIME 2024. February 2024MAA</p>
<p>The math library of lean 4. 2023</p>
<p>Sympy: symbolic computing in python. Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondřej Čertík, B Sergey, Matthew Kirpichev, Amit Rocklin, Sergiu Kumar, Jason K Ivanov, Sartaj Moore, Singh, PeerJ Computer Science. 3e1032017</p>
<p>Magnushammer: A transformer-based approach to premise selection. Maciej Mikuła, Szymon Tworkowski, Szymon Antoniak, Bartosz Piotrowski, Albert Q Jiang, Jin Peng Zhou, Christian Szegedy, Łukasz Kuci Ński, Piotr Miłoś, Yuhuai Wu, International Conference on Learning Representations (ICLR). 2024</p>
<p>Leonardo de Moura and Sebastian Ullrich. The Lean 4 theorem prover and programming language. Arindam Mitra, Hamed Khanpour, Corby Rosset, Ahmed Awadallah, arXiv:2402.14830International Conference on Automated Deduction (CADE). 20242021arXiv preprintOrca-math: Unlocking the potential of slms in grade school math</p>
<p>Isabelle: A generic theorem prover. Lawrence C Paulson, arXiv:2009.03393Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. 1994. 2020arXiv preprint</p>
<p>Stanislas Polu, Jesse Michael Han, Kunhao Zheng, arXiv:2202.01344Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning. 2022arXiv preprint</p>
<p>Formal mathematics statement curriculum learning. Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, Ilya Sutskever, International Conference on Learning Representations (ICLR). 2023</p>
<p>Direct preference optimization: Your language model is secretly a reward model. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, Chelsea Finn, Advances in Neural Information Processing Systems. 362023</p>
<p>Handbook of automated reasoning. J A Alan, Andrei Robinson, Voronkov, 20011</p>
<p>Stephan Schulz, Simon Cruanes, Petar Vukmirović, International Conference on Automated Deduction (CADE). 2019Faster, higher, stronger: E 2.3</p>
<p>Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Li, Wu, arXiv:2402.03300Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>A language-agent approach to formal theorem-proving. Amitayush Thakur, Yeming Wen, Swarat Chaudhuri, arXiv:2310.043532023arXiv preprint</p>
<p>George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, Swarat Chaudhuri, Putnambench, arXiv:2407.11214Evaluating neural theoremprovers on the putnam mathematical competition. 2024arXiv preprint</p>
<p>Natural language processing with transformers. Lewis Tunstall, Leandro Von Werra, Thomas Wolf, O'Reilly Media, Inc2022</p>
<p>MaLeCoP machine learning connection prover. Josef Urban, Jiří Vyskočil, Petr Štěpánek, International Conference on Automated Reasoning with Analytic Tableaux and Related Methods. 2011</p>
<p>Ruida Wang, Jipeng Zhang, Yizhen Jia, Rui Pan, Shizhe Diao, Renjie Pi, Tong Zhang, arXiv:2407.03203Theoremllama: Transforming general-purpose llms into lean4 experts. 2024arXiv preprint</p>
<p>Autoformalization with large language models. Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Norman Rabe, Charles E Staats, Mateja Jamnik, Christian Szegedy, Advances in Neural Information Processing Systems. Alice H Oh, Alekh Agarwal, Danielle Belgrave, Kyunghyun Cho, 2022</p>
<p>Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, Kai Chen, arXiv:2410.15700InternLM2.5-StepProver: Advancing automated theorem proving via expert iteration on large-scale lean problems. 2024arXiv preprint</p>
<p>Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, Xiaodan Liang, arXiv:2405.14333Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. 2024aarXiv preprint</p>
<p>Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. Huajian Xin, Junxiao Zz Ren, Zhihong Song, Wanjia Shao, Haocheng Zhao, Bo Wang, Liyue Liu, Xuan Zhang, Qiushi Lu, Du, arXiv:2408.081522024barXiv preprint</p>
<p>. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.151152024a5 technical report. arXiv preprint</p>
<p>Leandojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, Animashree Anandkumar, Advances in Neural Information Processing Systems. 2024b36</p>
<p>Lean workbook: A large-scale lean problem set formalized from natural language math problems. Huaiyuan Ying, Zijian Wu, Yihan Geng, Jiayu Wang, Dahua Lin, Kai Chen, arXiv:2406.038472024aarXiv preprint</p>
<p>Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, arXiv:2402.06332Internlm-math: Open math large language models toward verifiable reasoning. 2024barXiv preprint</p>
<p>Kunhao Zheng, Jesse Michael Han, Stanislas Polu, arXiv:2109.00110Minif2f: a cross-system benchmark for formal olympiad-level mathematics. 2021arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
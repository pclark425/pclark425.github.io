<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7431 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7431</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7431</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-273375563</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12405v1.pdf" target="_blank">ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized. This variability poses challenges for accurate assessment and user satisfaction. Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations. To address these shortcomings, we introduce ProSA, a framework designed to evaluate and comprehend prompt sensitivity in LLMs. ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms. Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness. We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoning-oriented tasks. Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness. We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs. The project is released at: https://github.com/open-compass/ProSA .</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7431.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7431.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instance-level prompt templates (12 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Instance-level prompt semantic variants (12 prompt templates per instance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper systematically queries each test instance with 12 manually-constructed prompt variants (covering aspects like Simple Inputs, Role Player, Emotional Support, and Output Requirement) to measure per-instance sensitivity using the PromptSensiScore (PSS) metric across multiple datasets and models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (Llama3 series, Qwen1.5 series, InternLM2 series, Mistral-7B-Instruct, proprietary models evaluated separately)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Various decoder-only large language models evaluated in their instruction-following/chat variants; specifics include Llama3-8B/70B-Instruct, Qwen1.5-7B/14B/20B/72B-Chat, InternLM2-7B/20B-Chat, Mistral-7B-Instruct, and proprietary models (e.g., GPT-4o, Claude-3 variants) evaluated in some analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B, 8B, 14B, 20B, 70B, 72B, 110B where applicable)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Objective evaluation across CommonsenseQA, ARC-Challenge, MATH, HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Objective tasks with ground-truth answers: multiple-choice (CommonsenseQA, ARC-Challenge), mathematical problem solving (MATH), and programming synthesis (HumanEval).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language instruction prompts — 12 different textual prompt templates per instance that rephrase the same task (e.g., role-play framing, explicit output requirements, emotional/supportive tone, simplified wording).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / prompt template</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>For each instance, 12 prompts span four aspects (Simple Inputs, Role Player, Emotional Support, Output Requirement) with 3 variants per aspect; greedy decoding used for reproducibility; PSS computed across all pairwise prompt comparisons for an instance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PromptSensiScore (PSS) representing expected inconsistency across prompt variants; objective metrics (accuracy) used per prompt to compute PSS</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Dataset- and model-dependent PSS values reported (qualitative: low PSS ≲0.1 considered robust; example instance PSS values shown: 0.00, 0.17, 0.41).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Greedy decoding; 12 prompt templates per instance; tasks: CommonsenseQA, ARC-Challenge, MATH, HumanEval; PSS averaged per-instance and across dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7431.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7431.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Few-shot vs zero-shot prompt format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Few-shot exemplar inclusion (0-shot → 1/3/5/7-shot) as a prompt-formatting intervention</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The study measures how adding few-shot examples to the prompt (same first k shots across variants) changes prompt sensitivity (PSS) and task performance, finding that adding examples reduces PSS—especially the jump from 0→1 shot—and larger models gain more robustness with more shots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen1.5 series (7B, 14B, 20B, 110B tested in few-shot experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Qwen1.5 chat/instruct series evaluated across multiple model scales in a consistent greedy decoding setup.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B, 14B, 20B, 110B (reported range)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA and ARC-Challenge (objective multiple-choice benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>CommonsenseQA: commonsense multiple-choice QA; ARC-Challenge: difficult science multiple-choice questions.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Few-shot prompting: prompts augmented with k labeled exemplars (k = 0,1,3,5,7); same set of exemplars used across prompt-template variants to isolate exemplar-count effect.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / context/examples</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Experiments compare 0-shot, 1-shot, 3-shot, 5-shot, 7-shot; first k shots identical across prompt templates; greedy decoding; PSS tracked as a function of shot count and model size.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task accuracy per prompt (used to compute PSS); PSS as robustness metric</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported trend: introduction of few-shot examples reduces PSS for all tested models; the most substantial reduction occurs from 0→1 shot. (Exact numeric deltas not tabulated in main text.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>0-shot (used as baseline for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Qualitative: marked decrease in PSS when moving 0→1 shot; further shot increases continue to reduce sensitivity, with larger models showing larger robustness gains (no single absolute % given).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Greedy decoding; same exemplars for all prompt variants; datasets: CommonsenseQA and ARC-Challenge; few-shot counts 0/1/3/5/7.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7431.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7431.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Subjective prompt rewriting (LC AlpacaEval & Arena Hard)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt rewriting for subjective evaluation (original prompt + two GPT-rewritten variants) evaluated with an LLM judge</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>For subjective benchmarks LC AlpacaEval 2.0 and Arena Hard Auto, the authors generate two rewritten prompt variants per instance (using GPT-4o and GPT-4-0409), verify semantic equivalence, and measure PSS in quality scores assigned by a judge model (GPT-4-1106-preview), observing low PSS on LC AlpacaEval but higher PSS on Arena Hard Auto for advanced LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>InternLM2-20B-Chat, Llama3-8B-Instruct, Llama3-70B-Instruct, Qwen1.5-14B-Chat, Qwen1.5-72B-Chat (these five evaluated on subjectives)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned/chat variants of several modern LLMs; judged by GPT-4-1106-preview for response quality.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B, 20B, 70B, 14B, 72B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LC AlpacaEval 2.0 and Arena Hard Auto (subjective benchmarks scored by LLM evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended subjective evaluation comparing response quality against reference responses using an LLM judge; LC AlpacaEval includes length-control correction; Arena Hard Auto includes pairwise comparative labels.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language prompts; original prompt plus two semantically-equivalent rewritten prompts per instance (three prompt versions total); judge scores responses with GPT-4 variants.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt wording / prompt phrasing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Prompt rewriting performed by GPT-4o or GPT-4-0409 and manually verified (BERTScore and human checks). Responses scored by GPT-4-1106-preview (default evaluator). PSS computed on judge scores across the three prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PSS computed from judge LLM scores on [0,1] scale for LC AlpacaEval and mapped comparative labels for Arena Hard Auto.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported: All tested LLMs showed significantly lower PSS on LC AlpacaEval 2.0 (indicating robustness) and higher PSS on Arena Hard Auto (indicating sensitivity), meaning prompt wording had greater impact on harder subjective tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Three prompt versions per instance (original + two rewrites); semantic-similarity vetting using Sentence-Transformers BERTScore and human annotation; judge evaluator: GPT-4-1106-preview; greedy decoding for tested models.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7431.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7431.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Category-based prompt sensitivity (Arena clusters)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt sensitivity by task-category (k-means clustered Arena Hard Auto categories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors cluster Arena Hard Auto prompts into 20 categories (via sentence embeddings and k-means) and report average PSS per category, showing systematic variation: knowledge-heavy categories display low sensitivity while creative/programming/data-visualization categories show high sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Aggregate of five advanced LLMs (InternLM2-20B-Chat, Llama3-8B/70B-Instruct, Qwen1.5-14B/72B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Averaged behavior across multiple instruction-tuned/chat LLMs evaluated with the judge model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B–70B range for primary contributors</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Arena Hard Auto (subjective benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Diverse open-ended tasks designed to distinguish model capabilities and human preferences; prompts reclustered into 20 semantic categories for analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Rewritten natural-language prompts grouped by semantic category (embedding + k-means clustering), responses judged by GPT-4 and PSS computed per category.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>task-category / prompt domain</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>500 prompts embedded with all-MiniLM-L6-v2 → k-means (k=20) clustering; GPT-4o used to name categories; PSS averaged across five LLMs per category; compared top-5 most sensitive and bottom-5 least sensitive categories.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Average PSS per category (PSS_c)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported category PSS ranges: top-5 most sensitive categories PSS ≈ 0.271, 0.271, 0.273, 0.275, 0.283; bottom-5 least sensitive categories PSS ≈ 0.225, 0.227, 0.236, 0.244, 0.245 (values represent averages across five LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Clustering-based category assignment; judge evaluator: GPT-4-1106-preview; PSS averaged across five models.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7431.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7431.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decoding confidence correlation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoding-confidence (token probability) correlates with prompt robustness</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper shows that an LLM's decoding confidence (average highest-token probability for the correct option across prompt variants) strongly correlates with PSS: higher confidence corresponds to lower prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct, InternLM2-7B-Chat, InternLM2-20B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned/chat models for which token-probability statistics were extracted to compute confidence per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B and 20B reported</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CommonsenseQA (multiple-choice)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Commonsense multiple-choice QA where for each option the model's token probability for the option token is used to estimate confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Multiple-choice prompt variants (12 templates); confidence computed per prompt as max-token probability of chosen option; per-instance confidence averaged across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>internal decoding statistics / output probability</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Confidence C = average over prompts p in P of P(t_next | p) where t_next is the token with maximum probability representing the chosen option; PSS and decoding confidence compared by binning instances by PSS and plotting mean confidence (Figure 7).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>PromptSensiScore (PSS) vs decoding confidence (average max-token probability)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported trend: instances with low PSS show higher decoding confidence; instances with high PSS show lower confidence. (No cross-model absolute confidence comparisons are made due to vocab/model differences.)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>CommonsenseQA with 12 prompt templates; greedy decoding; confidence computed from token probabilities across prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7431.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7431.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MATH formatting and answer-extraction effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open-format mathematical answer representation challenges (MATH dataset) and postprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>MATH responses are heterogeneous (e.g., 0.5 vs 1/2), making automated correctness extraction difficult; authors use a strong LLM (Llama3-70B-Instruct) for postprocessing/extraction and note that MATH shows both low accuracy and notable prompt sensitivity because of complex reasoning and varied answer formats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various models evaluated on MATH; Llama3-70B-Instruct used for answer extraction postprocessing</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned LLMs; Llama3-70B-Instruct used as an advanced postprocessor to normalize and extract numeric/math answers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B for Llama3-70B-Instruct; other models of varying sizes tested</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>MATH (mathematical problem solving)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Long-form math problems requiring multi-step reasoning and numeric/formula outputs, with multiple equivalent answer formats.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Free-form natural-language math problem prompts with varied template phrasing; outputs can be expressions, LaTeX, numeric values — requiring normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>output representation / answer extraction</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Because of format equivalences and open-response patterns, the authors use Llama3-70B-Instruct to postprocess model outputs and extract canonical answers (prompt in Figure 8); MATH shows poor accuracy across LLMs and elevated PSS due to reasoning difficulty and format variance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Task correctness after LLM-based postprocessing; PSS for prompt sensitivity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported qualitatively: MATH exhibits low model accuracy and considerable prompt sensitivity compared to easier datasets; specific numeric accuracies are not provided in-text for each prompt variant.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Postprocessing/extraction by Llama3-70B-Instruct; greedy decoding for tested models; 12 prompt templates per instance.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7431.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e7431.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEval code-prompt style variants</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HumanEval prompt-format variations (function-only, markdown codeblock, assistant instruction templates)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>HumanEval prompts were presented in multiple styles (e.g., 'Provide a self-contained Python script in a markdown code block', 'Your response should only contain the code for the function', etc.); the paper reports that prompt presentation/formatting influences code generation behavior and evaluation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Multiple (including Qwen1.5, Llama3 series, proprietary models evaluated on HumanEval)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction/chat LLMs evaluated on code generation tasks with varied prompt-format constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–72B and proprietary models)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>HumanEval (program synthesis / code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate correct function implementations for programming problems; evaluation by unit tests or reference comparison (with some postprocessing).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Several prompt templates for code tasks (requests for full script in markdown code block, function-only response, assistant persona prompts, explicit 'Final Answer' line, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction constraints</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>12 prompt templates were used for HumanEval including variants that require code only, markdown codeblock, explicit formatting instructions, or assistant-role framing; proprietary models also evaluated; answer-extraction and evaluation performed with methods similar to simple-evals.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>HumanEval pass@k / correctness (summarized qualitatively); PSS computed across prompt variants</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported qualitatively: prompt format affects code-generation behavior and evaluation; exact pass-rate deltas across formats are not fully enumerated in the main text (Table 4 for some HumanEval results referenced but exact numbers are omitted in the narrative).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>12 prompt templates per HumanEval instance; greedy decoding; some experiments include proprietary model comparisons; answer extraction/evaluation uses established pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7431.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e7431.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Option-ordering sensitivity (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensitivity to option ordering in multiple-choice questions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited prior work shows that LLMs' multiple-choice accuracy can depend on the order of presented answer options; the current paper references this as related evidence that small prompt changes (like option ordering) influence performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language models sensitivity to the order of options in multiple-choice questions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multiple-choice QA (general, prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Studies demonstrating how permutation/order of answer options affects model selection and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Option ordering in multiple-choice prompts</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt ordering / prompt layout</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Referenced as prior evidence; the cited work (Pezeshkpour & Hruschka, 2024) demonstrates order sensitivity, but this specific paper did not re-run those experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Accuracy (in prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting <em>(Rating: 2)</em></li>
                <li>Large language models sensitivity to the order of options in multiple-choice questions <em>(Rating: 2)</em></li>
                <li>State of what art? a call for multi-prompt llm evaluation <em>(Rating: 2)</em></li>
                <li>Promptbench: Towards evaluating the robustness of large language models on adversarial prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7431",
    "paper_id": "paper-273375563",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Instance-level prompt templates (12 variants)",
            "name_full": "Instance-level prompt semantic variants (12 prompt templates per instance)",
            "brief_description": "The paper systematically queries each test instance with 12 manually-constructed prompt variants (covering aspects like Simple Inputs, Role Player, Emotional Support, and Output Requirement) to measure per-instance sensitivity using the PromptSensiScore (PSS) metric across multiple datasets and models.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (Llama3 series, Qwen1.5 series, InternLM2 series, Mistral-7B-Instruct, proprietary models evaluated separately)",
            "model_description": "Various decoder-only large language models evaluated in their instruction-following/chat variants; specifics include Llama3-8B/70B-Instruct, Qwen1.5-7B/14B/20B/72B-Chat, InternLM2-7B/20B-Chat, Mistral-7B-Instruct, and proprietary models (e.g., GPT-4o, Claude-3 variants) evaluated in some analyses.",
            "model_size": "various (7B, 8B, 14B, 20B, 70B, 72B, 110B where applicable)",
            "task_name": "Objective evaluation across CommonsenseQA, ARC-Challenge, MATH, HumanEval",
            "task_description": "Objective tasks with ground-truth answers: multiple-choice (CommonsenseQA, ARC-Challenge), mathematical problem solving (MATH), and programming synthesis (HumanEval).",
            "problem_format": "Natural-language instruction prompts — 12 different textual prompt templates per instance that rephrase the same task (e.g., role-play framing, explicit output requirements, emotional/supportive tone, simplified wording).",
            "format_category": "prompt style / prompt template",
            "format_details": "For each instance, 12 prompts span four aspects (Simple Inputs, Role Player, Emotional Support, Output Requirement) with 3 variants per aspect; greedy decoding used for reproducibility; PSS computed across all pairwise prompt comparisons for an instance.",
            "performance_metric": "PromptSensiScore (PSS) representing expected inconsistency across prompt variants; objective metrics (accuracy) used per prompt to compute PSS",
            "performance_value": "Dataset- and model-dependent PSS values reported (qualitative: low PSS ≲0.1 considered robust; example instance PSS values shown: 0.00, 0.17, 0.41).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Greedy decoding; 12 prompt templates per instance; tasks: CommonsenseQA, ARC-Challenge, MATH, HumanEval; PSS averaged per-instance and across dataset.",
            "statistical_significance": null,
            "uuid": "e7431.0",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Few-shot vs zero-shot prompt format",
            "name_full": "Few-shot exemplar inclusion (0-shot → 1/3/5/7-shot) as a prompt-formatting intervention",
            "brief_description": "The study measures how adding few-shot examples to the prompt (same first k shots across variants) changes prompt sensitivity (PSS) and task performance, finding that adding examples reduces PSS—especially the jump from 0→1 shot—and larger models gain more robustness with more shots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen1.5 series (7B, 14B, 20B, 110B tested in few-shot experiments)",
            "model_description": "Qwen1.5 chat/instruct series evaluated across multiple model scales in a consistent greedy decoding setup.",
            "model_size": "7B, 14B, 20B, 110B (reported range)",
            "task_name": "CommonsenseQA and ARC-Challenge (objective multiple-choice benchmarks)",
            "task_description": "CommonsenseQA: commonsense multiple-choice QA; ARC-Challenge: difficult science multiple-choice questions.",
            "problem_format": "Few-shot prompting: prompts augmented with k labeled exemplars (k = 0,1,3,5,7); same set of exemplars used across prompt-template variants to isolate exemplar-count effect.",
            "format_category": "prompt style / context/examples",
            "format_details": "Experiments compare 0-shot, 1-shot, 3-shot, 5-shot, 7-shot; first k shots identical across prompt templates; greedy decoding; PSS tracked as a function of shot count and model size.",
            "performance_metric": "Task accuracy per prompt (used to compute PSS); PSS as robustness metric",
            "performance_value": "Reported trend: introduction of few-shot examples reduces PSS for all tested models; the most substantial reduction occurs from 0→1 shot. (Exact numeric deltas not tabulated in main text.)",
            "baseline_performance": "0-shot (used as baseline for comparison)",
            "performance_change": "Qualitative: marked decrease in PSS when moving 0→1 shot; further shot increases continue to reduce sensitivity, with larger models showing larger robustness gains (no single absolute % given).",
            "experimental_setting": "Greedy decoding; same exemplars for all prompt variants; datasets: CommonsenseQA and ARC-Challenge; few-shot counts 0/1/3/5/7.",
            "statistical_significance": null,
            "uuid": "e7431.1",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Subjective prompt rewriting (LC AlpacaEval & Arena Hard)",
            "name_full": "Prompt rewriting for subjective evaluation (original prompt + two GPT-rewritten variants) evaluated with an LLM judge",
            "brief_description": "For subjective benchmarks LC AlpacaEval 2.0 and Arena Hard Auto, the authors generate two rewritten prompt variants per instance (using GPT-4o and GPT-4-0409), verify semantic equivalence, and measure PSS in quality scores assigned by a judge model (GPT-4-1106-preview), observing low PSS on LC AlpacaEval but higher PSS on Arena Hard Auto for advanced LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "InternLM2-20B-Chat, Llama3-8B-Instruct, Llama3-70B-Instruct, Qwen1.5-14B-Chat, Qwen1.5-72B-Chat (these five evaluated on subjectives)",
            "model_description": "Instruction-tuned/chat variants of several modern LLMs; judged by GPT-4-1106-preview for response quality.",
            "model_size": "8B, 20B, 70B, 14B, 72B",
            "task_name": "LC AlpacaEval 2.0 and Arena Hard Auto (subjective benchmarks scored by LLM evaluator)",
            "task_description": "Open-ended subjective evaluation comparing response quality against reference responses using an LLM judge; LC AlpacaEval includes length-control correction; Arena Hard Auto includes pairwise comparative labels.",
            "problem_format": "Natural-language prompts; original prompt plus two semantically-equivalent rewritten prompts per instance (three prompt versions total); judge scores responses with GPT-4 variants.",
            "format_category": "prompt wording / prompt phrasing",
            "format_details": "Prompt rewriting performed by GPT-4o or GPT-4-0409 and manually verified (BERTScore and human checks). Responses scored by GPT-4-1106-preview (default evaluator). PSS computed on judge scores across the three prompt variants.",
            "performance_metric": "PSS computed from judge LLM scores on [0,1] scale for LC AlpacaEval and mapped comparative labels for Arena Hard Auto.",
            "performance_value": "Reported: All tested LLMs showed significantly lower PSS on LC AlpacaEval 2.0 (indicating robustness) and higher PSS on Arena Hard Auto (indicating sensitivity), meaning prompt wording had greater impact on harder subjective tasks.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Three prompt versions per instance (original + two rewrites); semantic-similarity vetting using Sentence-Transformers BERTScore and human annotation; judge evaluator: GPT-4-1106-preview; greedy decoding for tested models.",
            "statistical_significance": null,
            "uuid": "e7431.2",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Category-based prompt sensitivity (Arena clusters)",
            "name_full": "Prompt sensitivity by task-category (k-means clustered Arena Hard Auto categories)",
            "brief_description": "The authors cluster Arena Hard Auto prompts into 20 categories (via sentence embeddings and k-means) and report average PSS per category, showing systematic variation: knowledge-heavy categories display low sensitivity while creative/programming/data-visualization categories show high sensitivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Aggregate of five advanced LLMs (InternLM2-20B-Chat, Llama3-8B/70B-Instruct, Qwen1.5-14B/72B-Chat)",
            "model_description": "Averaged behavior across multiple instruction-tuned/chat LLMs evaluated with the judge model.",
            "model_size": "8B–70B range for primary contributors",
            "task_name": "Arena Hard Auto (subjective benchmark)",
            "task_description": "Diverse open-ended tasks designed to distinguish model capabilities and human preferences; prompts reclustered into 20 semantic categories for analysis.",
            "problem_format": "Rewritten natural-language prompts grouped by semantic category (embedding + k-means clustering), responses judged by GPT-4 and PSS computed per category.",
            "format_category": "task-category / prompt domain",
            "format_details": "500 prompts embedded with all-MiniLM-L6-v2 → k-means (k=20) clustering; GPT-4o used to name categories; PSS averaged across five LLMs per category; compared top-5 most sensitive and bottom-5 least sensitive categories.",
            "performance_metric": "Average PSS per category (PSS_c)",
            "performance_value": "Reported category PSS ranges: top-5 most sensitive categories PSS ≈ 0.271, 0.271, 0.273, 0.275, 0.283; bottom-5 least sensitive categories PSS ≈ 0.225, 0.227, 0.236, 0.244, 0.245 (values represent averages across five LLMs).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Clustering-based category assignment; judge evaluator: GPT-4-1106-preview; PSS averaged across five models.",
            "statistical_significance": null,
            "uuid": "e7431.3",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Decoding confidence correlation",
            "name_full": "Decoding-confidence (token probability) correlates with prompt robustness",
            "brief_description": "The paper shows that an LLM's decoding confidence (average highest-token probability for the correct option across prompt variants) strongly correlates with PSS: higher confidence corresponds to lower prompt sensitivity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct, InternLM2-7B-Chat, InternLM2-20B-Chat",
            "model_description": "Instruction-tuned/chat models for which token-probability statistics were extracted to compute confidence per instance.",
            "model_size": "7B and 20B reported",
            "task_name": "CommonsenseQA (multiple-choice)",
            "task_description": "Commonsense multiple-choice QA where for each option the model's token probability for the option token is used to estimate confidence.",
            "problem_format": "Multiple-choice prompt variants (12 templates); confidence computed per prompt as max-token probability of chosen option; per-instance confidence averaged across prompt variants.",
            "format_category": "internal decoding statistics / output probability",
            "format_details": "Confidence C = average over prompts p in P of P(t_next | p) where t_next is the token with maximum probability representing the chosen option; PSS and decoding confidence compared by binning instances by PSS and plotting mean confidence (Figure 7).",
            "performance_metric": "PromptSensiScore (PSS) vs decoding confidence (average max-token probability)",
            "performance_value": "Reported trend: instances with low PSS show higher decoding confidence; instances with high PSS show lower confidence. (No cross-model absolute confidence comparisons are made due to vocab/model differences.)",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "CommonsenseQA with 12 prompt templates; greedy decoding; confidence computed from token probabilities across prompts.",
            "statistical_significance": null,
            "uuid": "e7431.4",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MATH formatting and answer-extraction effects",
            "name_full": "Open-format mathematical answer representation challenges (MATH dataset) and postprocessing",
            "brief_description": "MATH responses are heterogeneous (e.g., 0.5 vs 1/2), making automated correctness extraction difficult; authors use a strong LLM (Llama3-70B-Instruct) for postprocessing/extraction and note that MATH shows both low accuracy and notable prompt sensitivity because of complex reasoning and varied answer formats.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various models evaluated on MATH; Llama3-70B-Instruct used for answer extraction postprocessing",
            "model_description": "Instruction-tuned LLMs; Llama3-70B-Instruct used as an advanced postprocessor to normalize and extract numeric/math answers.",
            "model_size": "70B for Llama3-70B-Instruct; other models of varying sizes tested",
            "task_name": "MATH (mathematical problem solving)",
            "task_description": "Long-form math problems requiring multi-step reasoning and numeric/formula outputs, with multiple equivalent answer formats.",
            "problem_format": "Free-form natural-language math problem prompts with varied template phrasing; outputs can be expressions, LaTeX, numeric values — requiring normalization.",
            "format_category": "output representation / answer extraction",
            "format_details": "Because of format equivalences and open-response patterns, the authors use Llama3-70B-Instruct to postprocess model outputs and extract canonical answers (prompt in Figure 8); MATH shows poor accuracy across LLMs and elevated PSS due to reasoning difficulty and format variance.",
            "performance_metric": "Task correctness after LLM-based postprocessing; PSS for prompt sensitivity",
            "performance_value": "Reported qualitatively: MATH exhibits low model accuracy and considerable prompt sensitivity compared to easier datasets; specific numeric accuracies are not provided in-text for each prompt variant.",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "Postprocessing/extraction by Llama3-70B-Instruct; greedy decoding for tested models; 12 prompt templates per instance.",
            "statistical_significance": null,
            "uuid": "e7431.5",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HumanEval code-prompt style variants",
            "name_full": "HumanEval prompt-format variations (function-only, markdown codeblock, assistant instruction templates)",
            "brief_description": "HumanEval prompts were presented in multiple styles (e.g., 'Provide a self-contained Python script in a markdown code block', 'Your response should only contain the code for the function', etc.); the paper reports that prompt presentation/formatting influences code generation behavior and evaluation outcomes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Multiple (including Qwen1.5, Llama3 series, proprietary models evaluated on HumanEval)",
            "model_description": "Instruction/chat LLMs evaluated on code generation tasks with varied prompt-format constraints.",
            "model_size": "various (7B–72B and proprietary models)",
            "task_name": "HumanEval (program synthesis / code generation)",
            "task_description": "Generate correct function implementations for programming problems; evaluation by unit tests or reference comparison (with some postprocessing).",
            "problem_format": "Several prompt templates for code tasks (requests for full script in markdown code block, function-only response, assistant persona prompts, explicit 'Final Answer' line, etc.).",
            "format_category": "prompt style / instruction constraints",
            "format_details": "12 prompt templates were used for HumanEval including variants that require code only, markdown codeblock, explicit formatting instructions, or assistant-role framing; proprietary models also evaluated; answer-extraction and evaluation performed with methods similar to simple-evals.",
            "performance_metric": "HumanEval pass@k / correctness (summarized qualitatively); PSS computed across prompt variants",
            "performance_value": "Reported qualitatively: prompt format affects code-generation behavior and evaluation; exact pass-rate deltas across formats are not fully enumerated in the main text (Table 4 for some HumanEval results referenced but exact numbers are omitted in the narrative).",
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": "12 prompt templates per HumanEval instance; greedy decoding; some experiments include proprietary model comparisons; answer extraction/evaluation uses established pipelines.",
            "statistical_significance": null,
            "uuid": "e7431.6",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Option-ordering sensitivity (related work)",
            "name_full": "Sensitivity to option ordering in multiple-choice questions",
            "brief_description": "Cited prior work shows that LLMs' multiple-choice accuracy can depend on the order of presented answer options; the current paper references this as related evidence that small prompt changes (like option ordering) influence performance.",
            "citation_title": "Large language models sensitivity to the order of options in multiple-choice questions",
            "mention_or_use": "mention",
            "model_name": "",
            "model_description": "",
            "model_size": "",
            "task_name": "Multiple-choice QA (general, prior work)",
            "task_description": "Studies demonstrating how permutation/order of answer options affects model selection and accuracy.",
            "problem_format": "Option ordering in multiple-choice prompts",
            "format_category": "prompt ordering / prompt layout",
            "format_details": "Referenced as prior evidence; the cited work (Pezeshkpour & Hruschka, 2024) demonstrates order sensitivity, but this specific paper did not re-run those experiments.",
            "performance_metric": "Accuracy (in prior work)",
            "performance_value": null,
            "baseline_performance": null,
            "performance_change": null,
            "experimental_setting": null,
            "statistical_significance": null,
            "uuid": "e7431.7",
            "source_info": {
                "paper_title": "ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "Large language models sensitivity to the order of options in multiple-choice questions",
            "rating": 2,
            "sanitized_title": "large_language_models_sensitivity_to_the_order_of_options_in_multiplechoice_questions"
        },
        {
            "paper_title": "State of what art? a call for multi-prompt llm evaluation",
            "rating": 2,
            "sanitized_title": "state_of_what_art_a_call_for_multiprompt_llm_evaluation"
        },
        {
            "paper_title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts",
            "rating": 1,
            "sanitized_title": "promptbench_towards_evaluating_the_robustness_of_large_language_models_on_adversarial_prompts"
        }
    ],
    "cost": 0.016292249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs
16 Oct 2024</p>
<p>Jingming Zhuo jingmingzhuo@gmail.com 
Shanghai AI Laboratory</p>
<p>Jilin University</p>
<p>Songyang Zhang zhangsongyang@pjlab.org.cn 
Shanghai AI Laboratory</p>
<p>Xinyu Fang 
Shanghai AI Laboratory</p>
<p>Zhejiang University</p>
<p>Haodong Duan 
Shanghai AI Laboratory</p>
<p>Dahua Lin 
Shanghai AI Laboratory</p>
<p>The Chinese University of Hong Kong</p>
<p>Kai Chen 
Shanghai AI Laboratory</p>
<p>Tom Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared D Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Zheng Cai 
Maosong Cao 
Haojiong Chen 
Kai Chen 
Keyu Chen 
Xin Chen 
Xun Chen 
Zehui Chen 
Zhi Chen 
Pei Chu 
Mark Chen 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Gretchen Krueger 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Scott Gray 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Clemens Winter 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
Ariel Herbert-Voss 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Christopher Hesse 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Alec Radford 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Dario Amodei 
Sam Mccandlish 
Ilya Sutskever 
Wojciech 2021 Zaremba 
Evaluating 
Peter Clark 
Isaac Cowhey 
Oren Etzioni 
Tushar Khot 
Ashish Sabharwal 
Carissa Schoenick 
Karl Cobbe 
Vineet Kosaraju 
Jacob Hilton 
Reiichiro Nakano 
ProSA: Assessing and Understanding the Prompt Sensitivity of LLMs
16 Oct 2024683CED2F48648EB2429AE9FF1B703127arXiv:2410.12405v1[cs.CL]2018. Think you have solved question answering? try arcthe ai2 reasoning challenge. PreprintarXiv:1803.05457
Large language models (LLMs) have demonstrated impressive capabilities across various tasks, but their performance is highly sensitive to the prompts utilized.This variability poses challenges for accurate assessment and user satisfaction.Current research frequently overlooks instance-level prompt variations and their implications on subjective evaluations.To address these shortcomings, we introduce ProSA, a framework designed to evaluate and comprehend prompt sensitivity in LLMs.ProSA incorporates a novel sensitivity metric, PromptSensiScore, and leverages decoding confidence to elucidate underlying mechanisms.Our extensive study, spanning multiple tasks, uncovers that prompt sensitivity fluctuates across datasets and models, with larger models exhibiting enhanced robustness.We observe that few-shot examples can alleviate this sensitivity issue, and subjective evaluations are also susceptible to prompt sensitivities, particularly in complex, reasoningoriented tasks.Furthermore, our findings indicate that higher model confidence correlates with increased prompt robustness.We believe this work will serve as a helpful tool in studying prompt sensitivity of LLMs.The project is released at: https://github.com/open-compass/ProSA.</p>
<p>Introduction</p>
<p>In recent years, large language models (LLMs) have rapidly become the focus of the artificial intelligence field.By training on large-scale corpora, LLMs have shown impressive capabilities in multiple tasks (Zhao et al., 2023;Min et al., 2023).The input for LLMs, known as prompts, is crucial for their ability to complete a wide variety of tasks.The prompts for LLMs come in various forms.Even for the same requirement, different individuals' varying expression habits can result in prompts of different styles.Figure 1  styles of prompts used by LLMs when completing mathematical tasks.</p>
<p>illustrates four</p>
<p>The diversity of prompts elicits various responses from LLMs.Recent studies (Zhu et al., 2023;Pezeshkpour and Hruschka, 2024;Sclar et al., 2024;Zhou et al., 2023) investigate model performance across different prompt templates and demonstrate that LLMs are highly sensitive to the nuances of prompts.Even minor alterations to prompts can lead to substantial declines in model performance.This sensitivity to prompts poses a challenge for researchers aiming to precisely evaluate the models' capabilities.Moreover, users frequently have to iterate on their prompts numerous times to achieve higher quality outputs.</p>
<p>However, current research on prompt sensitivity mainly centers on dataset-level analyses, focusing on performance variations across distinct prompt templates within identical datasets.Also, existing works neglect examining how LLMs align with human expectations under different prompts in subjective evaluations, hindering the accuracy of insights reflecting real-world user experiences.Moreover, it's remain changeling in elucidating the reasons behind LLMs' sensitivity to prompts.</p>
<p>To tackle the aforementioned issues, we introduce the ProSA, focusing on evaluation and understanding the prompt sensitivity of the current LLMs.It can serve as a proxy on monitoring the robustness and stability of the LLMs.In this work, we devise the ProSA by focusing on the instancelevel analysis, develop a novel sensitivity metric, and leveraging decoding confidence for elucidating the underlying mechanisms.Specifically, we emphasize instance-level measurements and analyses, comprising both objective evaluations against precise references and open-ended subjective evaluations.Our experimental scope spans multiple tasks, ranging from understanding and logical reasoning to coding and general alignment capabilities.</p>
<p>To achieve this, we introduce a novel metric designed to quantify prompt sensitivity.We define our metric, termed PromptSensiScore or PSS, as the average discrepancy in the LLM's responses when confronted with different semantic variants of the same instruction (prompt).A comprehensive explanation of PromptSensiScore (PSS) is provided in Sec.2.2.</p>
<p>Furthermore, we investigate prompt sensitivity by measuring the instance-level PSS of several popular open-source LLMs through the objective evaluation and subjective evaluation.In the objective evaluation, we assess 8 LLMs across 4 datasets, spanning diverse capabilities, with each model tested on 12 prompt variants.Findings reveal variations in prompt sensitivity among models and datasets, with Llama3-70B-Instruct (AI@Meta, 2024) demonstrating the highest robustness.The study further illustrates that incorporating few-shot examples alleviate prompt sensitivity, particularly evident in the transition from zero-to one-shot scenarios.Larger LLMs especially benefit from increased few-shot instances, exhibiting greater robustness improvements.</p>
<p>Additionally, we analyze five advanced LLMs using two prominent subjective evaluation benchmarks: LC AlpacaEval 2.0 (Dubois et al., 2024) and Arena Hard Auto (Li et al., 2024).Our findings illustrate that these models demonstrate high resilience in answering straightforward queries but encounter heightened sensitivity with more complex ones.Categorizing prompts reveals that LLMs are particularly robust when drawing upon established knowledge domains, such as troubleshooting IT issues.Conversely, in coding tasks or those requiring creativity, LLMs prove to be more susceptible to variations in prompts.</p>
<p>We also delve into the underlying reasons for prompt sensitivity.Leveraging the objective evaluation as a stand-in, we utilize the decoding confidence associated with the correct answer to scrutinize model behavior.Findings suggest that prompt sensitivity is essentially a reflection of the model's confidence level: higher confidence in its outputs correlates with increased robustness against prompt semantic variations.</p>
<p>Our contributions can be summarized as follows: • We introduce ProSA, a comprehensive framework that places emphasis on instance-level analysis, incorporates a novel sensitivity metric, and utilizes decoding confidence to unravel the underlying mechanisms of prompt sensitivity in LLMs.</p>
<p>• We propose a novel metric, PromptSensiScore (PSS), which quantifies the average discrepancy in LLM responses when faced with different prompt variants of the same instance.The subsequent analysis on the objective and subjective evaluation provided assistance and guidance in exploring prompt sensitivity.</p>
<p>• We find that prompt sensitivity is fundamentally an outward appearance the model's decoing confidence: greater confidence in its outputs corresponds to enhanced resilience against changes in prompt semantics.</p>
<p>Instance Level Prompt Sensitivity</p>
<p>Definition</p>
<p>When instructing LLMs to complete the same task, different individuals often use different expressions, which can lead to the LLMs generating different responses.In this paper, we refer to the specific requirements as an instance.Different expressions of an instance are referred to as different prompts.</p>
<p>Previous research (Zhu et al., 2023;Pezeshkpour and Hruschka, 2024) compares different prompt templates within datasets (composed of instances) to analyze the prompt sensitivity of LLMs, which is done by examining the shifts in the LLMs' scores across different prompt templates within the dataset.This approach to studying prompt sensitivity has certain limitations.</p>
<p>Despite all instances in a dataset following the same topic, the differences in model performance under the same instances with different prompt templates are often overlooked.Each instance can vary widely in complexity, context, and information type, ranging from straightforward factual questions to those requiring nuanced understand- ing.This diversity means the model's sensitivity to prompts can differ significantly between instances.As shown in Figure 2, LLMs may be robust to templates for some instances while being sensitive to templates for others.</p>
<p>Additionally, relying on aggregate metrics to assess prompt sensitivity can obscure important nuances.While overall trends may be useful, they can mask differences in individual instances where the model's responses drastically changes due to slight prompt modifications.</p>
<p>Therefore, by analyzing instance level prompt sensitivity, we can gain deeper insights into how various factors influence model responses, leading to more effective and reliable use of LLMs.</p>
<p>Evaluation Metric</p>
<p>To analyze prompt sensitivity from the instance level, rather than dataset level perspective, we defined PSS to measure the sensitivity of LLMs to prompts.For each set of all prompt variants under the same instance, we have:
S = p i ,p j ∈P (|Y (P i ) − Y (P j )|) C(|P | , 2)(1)
Here, Y (p) represents the performance metric under this prompt p.
PSS = 1 N N i=1 S i (2)
Here N is the total number of instances in the dataset and S i is the score for the i-th instance.</p>
<p>Due to the different types of tasks and evaluation methods, PSS has different meanings in objective evaluation and subjective evaluation.In objective evaluation, PSS represents the expected inconsistency in the correctness of the model's responses under two different prompts for the same instance.In subjective evaluations, PSS represents the expected difference in response quality (as scored by advanced LLMs).We generally consider that when PSS is less than 0.1, LLMs exhibit high prompt robustness.However, different tasks have varying requirements for LLMs' prompt sensitivity.For instance, tasks like code assistant may require higher robustness to enhance user experience.On Appendix A, we provide examples of model responses under different prompts along with their PSS scores to offer a clearer understanding.</p>
<p>Compared to the statistical analysis of performance shifts, PSS provides a novel perspective a more accurate and intuitive characterization for measuring and analyzing prompt sensitivity.</p>
<p>Prompt Sensitivity on the Objective Evaluation</p>
<p>The objective evaluation is a common form for evaluating LLMs.In this setup, the given tasks often have a specific ground truth, such as the correct options in multiple-choice questions or the answers to mathematical problems.The prompt sensitivity on the objective evaluation is analyzed by extracting the outcome of LLM's different responses under various prompts for the same instance.</p>
<p>Experimental Setup</p>
<p>Dataset Selection To comprehensively analyze the prompts sensitivity of LLMs to different forms of task, the datasets for evaluating are as follows: LLMs Selection To comprehensively investigate the sensitivity of LLMs to various prompts, we conducted experiments on a wide range of LLMs with varying sizes, including: Llama3 series (AI@Meta, 2024), Qwen1.5 series (Bai et al., 2023), In-ternLM2 series (Cai et al., 2024), and Mistral-7B-Instruct (Jiang et al., 2023).we use greedy decoding in inference to ensure that the results are reproducible.Prompts Selection In real-world scenarios, different users often use different prompt words to indicate the same intent.To align with the richness of human expression, we start with four constructive aspects, including Simple Inputs, Role Player, 1 https://github.com/openai/simple-evals 2 All experiments are conducted with OpenCompass (Contributors, 2023) Emotional Support, and Output Requirement.For each aspect, we have 3 manually constructed prompts with high quality.Figure 1 provides a showcase of four aspects of prompts used for the MATH Dataset.More details about instances and prompts are shown on Appendix C.1 and D.1.</p>
<p>Main Results and Analysis</p>
<p>We report the main results of the prompt sensitivity on the objective evalution in Figure 3.</p>
<p>Due to differences in task types and difficulties, Prompt Sensitivity exhibits varying phenomena across different Datasets.For relatively easier Datasets like CommonsenseQA, ARC-Challenge and HumanEval, the LLMs' average performances and PSS appear to have an approximately linear relationship, indicating that LLMs can achieve high performance while maintaining low prompt sensitivity.In the case of MATH, since it requires extensive reasoning processes to arrive at the correct answers, all LLMs exhibited poor performance along with a certain degree of prompt sensitivity.</p>
<p>One LLM may show high sensitive on one task but be robust to prompts on another task.For instance, Qwen1.5-14B-Chat is robust to prompts on CommmonsenseQA, but it has the most serious prompt sensitivity on MATH among all LLMs.</p>
<p>We also evaluate two proprietary models on the HumanEval.Due to space constraints, more analysis will be presented on Appendix E.</p>
<p>Prompt Sensitivity and Model Size</p>
<p>A commonly raised concern is identifying which LLMs are more sensitive to prompts and whether there is a correlation between a model's prompt sensitivity and its size.For model l, we calculate P SS, the average PSS on the four tested datasets  to measure the performance of LLMs in terms of prompt sensitivity.As shown in Figure 4, LLama3-70B-Instruct demonstrates exceptional robustness to prompts.After that, the InternLM2-7B-Chat and Llama3-8B-Instruct also showcase notable robustness even though they have a relatively small size, which is counter-intuitive.The three models in the same series, Qwen1.5-7B-Chat,Qwen1.5-20B-Chat, and Qwen1.5-72B-Chat,maintain relatively similar prompt sensitivities while differing greatly in the model sizes.</p>
<p>Few-shot Enhances Prompt Robustness</p>
<p>In the era of LLMs, few-shot plays a critical role in enabling LLMs to follow specific formats and improve their performance (Brown et al., 2020).To investigate the impact of few-shot on the prompt sensitivity of LLMs, we conduct experiments utilizing the CommonsenseQA and ARC-Challenge.</p>
<p>Specifically, we use the same 12 prompts mentioned earlier to conduct experiments on four models from the Qwen1.5 series, which range in size from 7B to 110B.For each model, we employ greedy decoding.In terms of the number of shot examples, we compare and analyze the 0-shot, 1-shot, 3-shot, 5-shot, and 7-shot methods.In all settings, the first k shots are the same (if applicable), to mitigate the impact of exaple selection.The few-shot examples used are shown on Appendix F.</p>
<p>As shown in Figure 5, the introduction of few-shot learning enhances the robustness of the model's prompts for all models.On the ARC-Challenge, even though the inclusion of few-shot learning did not significantly improve the model's performance, there was a noticeable decrease in the model's sensitivity to prompts.This reduction in sensitivity is most pronounced when transitioning from the 0-shot setting to the 1-shot setting.</p>
<p>Besides, as the number of few-shot examples increases, larger LLMs exhibit more robust behavior to prompts.For example, the four models have similar prompt sensitivity under the zero-shot setting on the CommonsenseQA.However, with the increase in few-shot examples, the larger models demonstrate a trend of being more robust to prompts.The same phenomenon is observed on the ARC-Challenge.As the number of few-shot examples increases, Larger models can exhibit better prompt robustness.</p>
<p>Due to limitations in computational resources, we are unable to investigate the effects of using a dozen or more few-shot examples on the sensitivity of small or large models, like whether small models can continue to improve their robustness with an increasing number of few-shot examples, or if this is a capability unique to large models.However, we believe that this direction is worth further exploration.</p>
<p>Prompt Sensitivity on the Subjective Evaluation</p>
<p>With the emergence of LLMs' capabilities and their large-scale deployment as assistances or tools serving humans, how to evaluate the quality of LLMs' responses is receiving increasing attention.In realworld scenarios, the vast majority of instances do not have a specific ground truth that is definitively better than others.Thus, some previous work has attempted to evaluate the quality of generated responses using the subjective evaluation.This evaluation is often carried out by either using human raters or a powerful model, such as GPT-4, to score the generated text.This subjective evaluation, compared to the objective evaluation, better reflects the alignment of LLMs with human needs.However, to the best of our knowledge, existing research on prompt sensitivity has not involved an analysis of prompt sensitivity on the subjective evaluation.</p>
<p>Experimental Setup</p>
<p>Dataset Selection</p>
<p>Due to the high costs associated with using humans to evaluate LLMs' responses, existing evaluations often use strong judge LLMs to score responses instead of human annotators and it has been demonstrated that the scores from judge LLMs have high consistency with human ratings.In this study, we selected two widely used benchmarks for our experiments:</p>
<p>• For both benchmarks, we use the default evaluator, GPT-4-1106-preview, as the LLM evaluator and the default comparison model responses as references (The default versions are GPT-4-1106preview for LC AlpacaEval 2.0 and GPT-4-0314 for Arena Hard Auto).Appendix C.2 provides some examples of both benchmarks.</p>
<p>LLMs Selection</p>
<p>Given that the two benchmarks score the tested models by comparing their responses with those of the reference models, we selected five models with relatively better performance for our experiments.The selected models are: InternLM2-20B-Chat, Llama3-8B-Instruct, Llama3-70B-Instruct, Qwen1.5-14B-Chat, and Qwen1.5-72B-Chat.</p>
<p>Prompt Rewriting</p>
<p>For both benchmarks, we used LLMs to rewrite all the prompts.To achieve a richer variety of question styles, we used two powerful models for the rewriting: GPT-4o and GPT-4-0409.We then manually verified and refined the rewritten questions to ensure their quality.More details about prompt for constructing promtps and cases of rewritten prompts are shown on Appendix D.2 and G.We conducted two quality verifications on the rewritten prompts.First, we used the "all-MiniLM-L6-v2" model from Sentence Transformers (Reimers and Gurevych, 2019) to calculate the BERTScore (Zhang et al., 2020) (F1 version) between the original and rewritten prompts.Besides, we conduct a human verification to assess the similarity between the original and rewritten prompts.We selected 100 prompt pairs for human annotation for each benchmark.For each prompt pair, we had proficient English annotators assess the similarity based on whether the two prompts convey the same semantics, differing only in expression style, to complete a binary classification task.As shown in Table 1, the generated texts performed well in both rule-based and human-based quality verifications, demonstrating high semantic similarity between the rewritten prompts and the original prompts.</p>
<p>Main Results and Analysis</p>
<p>We calculated the PSS under three versions (one original and two generated) on two benchmarks, respectively.Since PSS represents the average response quality difference between two prompts of the same instance, we used the average response quality of Llama3-8B-instruct and Llama3-70B-Instruct as references.</p>
<p>As shown in Table 2, all LLMs have significantly lower PSS on LC AlpacaEval 2.0 compared to the Reference, demonstrating a certain degree of prompt robustness on this benchmark.However, on Arena Hard Auto, LLMs have demonstrated higher sensitivity.This indicates that this five advanced LLMs have achieved good robustness on relatively simple questions, but they still exhibit prompt sensitivity on more challenging questions.</p>
<p>Prompt Sensitivity and Categories</p>
<p>Users should have a psychological expectation when using LLMs, understanding that these models can be sensitive to prompts.This awareness allows users to modify prompts multiple times to achieve better responses for different tasks.Therefore, measuring the relationship between prompt sensitivity and task categories is valuable.We conduct experiments on Arena Hard Auto to explore the relationship between prompt sensitivity and the category of prompts.</p>
<p>Due to the excessive complexity of the original 250 categories in Arena Hard Auto and the high randomness of having only 2 instances in each category, we have re-clustered them.We referred to the clustering method used in LMSYS-CHAT-1M (Zheng et al., 2023).First, we obtained the sentence embeddings for 500 prompts using the "all-MiniLM-L6-v2" model.Then, we applied the k-means algorithm to obtain 20 categories.We used GPT-4o to name the resulting categories.Subsequently, we calculated the PSS between each of the 20 categories.Figure 6 shows the five categories with the highest and lowest PSS, respectively.</p>
<p>As shown in Figure 6, LLMs exhibit varying degrees of prompt sensitivity across different categories.We can observe that LLMs generally demonstrate better robustness for tasks that heavily test the model's knowledge rather than reasoning, such as business solutions and IT problem.However, when the prompts shift to tasks involving data visualization and scripting tasks, which requires LLMs to generate a large amount of content and involves a certain degree of creativity, LLMs exhibit a higher sensitivity.</p>
<p>Why LLMs are Sensitive to Prompts</p>
<p>Although the sensitivity of LLMs to prompts significantly impacts user experience and model evaluation, existing research mainly focuses on selecting a suitable prompt from a large set of prompts, rather than involving the interpretation of prompt sensitiv- ity.To address this, we conducted experiments on the CommonsenseQA dataset using Mistral-7B-Instruct, InternLM2-7B-Chat, and InternLM2-20B-Chat.Our experimental results revealed that a model's prompt sensitivity is related to its confidence.For an instance, the more confident the model has, the more robust it is to the prompts.</p>
<p>Decoding Confidence</p>
<p>We use Token Probabilities, a widely employed method for measuring a model's confidence in its generated content (Schuster et al., 2022;Geng et al., 2023), to calculate the model's decoding confidence.Since CommonsenseQA is a multiple choice question dataset, we calculate the probabilities of only one token predicted by the model for the options.The confidence under an instance is defined as follows:
C = p∈P P (t next | p) |P |(3)
Here, P (t next | p) represents the probability of the token with the max probability predicted by the model under the prompt p in the prompt set P .The confidence of LLMs is the average of C for instances.</p>
<p>Experiments and Analysis</p>
<p>We calculated the decoding confidence of Mistral-7B-Instruct, InternLM2-7B-Chat, and InternLM2-20B-Chat using the same 12 prompt templates as in Section 3. The results are shown in Figure 7.</p>
<p>For each LLM, when the model is robust to prompts for a given instance, indicated by a low PPS score, it exhibits the highest decoding confidence.Conversely, when the model is sensitive to prompts for the same instance, its decoding confidence decreases accordingly.This correlation between the model's prompt sensitivity and decoding confidence suggests that prompt sensitivity is an external manifestation of the model's decoding logic.6 Related Work</p>
<p>Mistral</p>
<p>LLMs Evaluation</p>
<p>Evaluating LLMs is crucial for their efficient use and continuous improvement.Prior research has systematically unraveled the multifaceted capabilities of LLMs, employing a variety of tasks to assess their performance from different perspectives.These specific tasks include, but are not limited to, reading comprehension (Sakaguchi et al., 2021;Mostafazadeh et al., 2017), mathematical problem solving (Cobbe et al., 2021;Hendrycks et al., 2021), and code generation (Chen et al., 2021;Austin et al., 2021).This analytical approach enables a nuanced understanding of how LLMs perform across different facets, shedding light on their efficacy and potential areas for improvement.Recently, with the improvement in the objective capabilities of LLMs, subjective evaluation, which aims to assess the alignment of generated responses with human needs, such as LC AlpacaEval 2.0 (Dubois et al., 2024), has received increasing attention.</p>
<p>Prompt Sensitivity</p>
<p>Previous study (Zhu et al., 2023;Pezeshkpour and Hruschka, 2024;Sclar et al., 2024) showed that LLMs are sensitive to prompts, and that perturbing the prompt can cause a significant variation in the performance of models.(Pezeshkpour and Hruschka, 2024) demonstrated that LLMs are sensitive to the order of options in multiple choice questions.(Mizrahi et al., 2024) demonstrated that model robustness leads to cherry-picking of model performance.However, existing research on prompt sensitivity is insufficient.Their research primarily focuses on dataset-level prompt sensitivity analysis.Moreover, existing prompt sensitivity analyses do not address subjective evaluation benchmarks.Furthermore, the aforementioned research does not address the issue at its root, specifying how to obtain a more robust model.</p>
<p>Conclusion</p>
<p>In summary, we propose an instance-level prompt sensitivity metric, PSS, and conduct a comprehensive analysis on both objective and subjective evaluation.Additionally, we explore the relationship between prompt sensitivity and model confidence.We believe our work can provide guidance for further sensitivity analysis and building robust LLMs.</p>
<p>Limitations</p>
<p>In this work, we evaluate the sensitivity of LLMS to prompts on both objective and subjective evaluation, but we also recognize the shortcomings of our work.Due to the limitation of computational resources, we can't explore the phenomenon of prompt sensitivity as the examples of fow-shot increase.In addition, due to the high price of Ope-nAI API service, we only conducted experiments on three prompt variants on LC AlpacaEval 2.0 and Arena Hard Auto.</p>
<p>Ethical Considerations</p>
<p>We utilize publicly available datasets and LLMs for our analytical experiments.While we acknowledge that our findings could potentially be used to create cherry-picked evaluation reports, we believe our work will contribute to enhancing the robustness of LLMs to various prompts.Additionally, we employ GPT-4o to refine our writing and assist us to create figures.</p>
<p>Acknowledge</p>
<p>This paper is supported by the National Key R&amp;D Program of China (No. 2022ZD0161600).</p>
<p>Zhuohan Li, Zi Lin, Eric Xing, et al. 2023 A value of 1 represents a correct response, while 0 indicates an incorrect response.</p>
<p>Examples</p>
<p>Responses PSS
1 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1] 0 2
[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1] 0.17 3 [1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1] 0.41</p>
<p>B Details about Extracting Answers from MATH</p>
<p>Due to the open-domain response patterns and the equivalence of different mathematical formats (for example, 0.5 and \frac{1}{2} are equivalent), it is not easy to accurately validate the correctness of responses in MATH.Instead of relying on the common method of extracting answers from the "\box{}", we follow OpenAI's simple-evals, utilizing advanced LLMs (in this case, Llama3-70B-Instruct) for postprocessing to extract answers.We use the prompts in Figure 8 to Prompt LLMs to complete the answer extraction task.This approach of using advanced LLMs for post-processing allows us to more accurately assess the LLMs' responses, thereby making our experimental analysis more reliable.</p>
<p>C Examples of the Datasets</p>
<p>D.2 Subjective Evaluation</p>
<p>Since the subjective datasets were all requested to be generated by the model based on certain requirements, we rewrote the original prompt using GPT-4 to obtain two new prompts to be evaluated separately.Here (Figures 27 and 28) we give two examples for each dataset to show the difference between the prompts before and after rewriting.</p>
<p>E Results and Analysis about the Proprietary Models</p>
<p>Here, we present the results of the two most powerful proprietary LLMs, Claude-3.The experimental results indicate that even though Claude-3.5-sonnetand GPT-4o achieve optimal performance on average, they are still not considered robust to prompts compared to other models with lower scores.This suggests that prompt sensitivity remains an important research topic.We believe that the ProSA framework can facilitate further research on prompt sensitivity and inspire the design of more robust LLMs.</p>
<p>We conducted a case study on GPT-4o and Llama3-72B-Instruct.We found that for instances where GPT-4o or Llama-3-70B-Instruct had a higher proportion of correct answers, they were able to respond correctly across all 12 prompt templates.However, when presented with instances that appeared to be more challenging for them, the situation changed.Even when different prompt templates were used, Llama-3-70B-Instruct still tended to answer incorrectly on instances it struggled with.In contrast, GPT-4o often exhibited a different behavior: it could provide correct answers under certain prompts even when it had previously answered incorrectly under others.This differing behavior is a key reason for the observed disparity in their prompt sensitivity.This phenomenon has motivate us to reflect on the selection between "unstable geniuses" and "stable mediocrities."</p>
<p>F Few-shot Examples of the Objective Datasets</p>
<p>The few-shot examples of ARC-Challenge and CommonseQA are given in Figures 29 and 30, respectively.</p>
<p>G Prompts to GPT-4 when Rewriting Prompts in the Subjective Evaluation</p>
<p>The prompts used to GPT-4 when rewriting prompts in the subjective evaluation are presented in the Figure 31.</p>
<p>ARC-Challenge Example II</p>
<p>Prompt: Could you provide a response to the following question: Scientific models are very common.For which purpose below would a physical model be least helpful? A. simulating scientific phenomena B. simplifying a complex idea C. allowing visualization D. displaying data Ground Truth Answer: D. displaying data InternLM2-7B-Chat Answer: A physical model would be least helpful for the purpose of displaying data.Ẅhile physical models can be used to display data, they are not the most efficient or effective method for this purpose.Displaying data typically refers to presenting information in a clear and organized manner, often through the use of graphs, charts, or tables.Physical models are better suited for simulating scientific phenomena, simplifying complex ideas, and allowing visualization, as they can provide a tangible representation of a concept or system that can be manipulated and observed.At [Company Name], we pride ourselves on delivering high-quality products and exceptional customer service.Unfortunately, we fell short in meeting your expectations with this particular order, and for that, we are deeply sorry.</p>
<p>The delay was due to an unforeseen issue in our production process, which has since been identified and resolved.We have taken immediate steps to ensure that this issue will not affect any future orders, and we have implemented additional quality control measures to prevent such incidents from occurring again.</p>
<p>To make up for the inconvenience, we would like to offer you a [insert compensation offer, e.g., a discount on your next purchase, free shipping, or an extended warranty].Please find the details of this offer attached to this email.We value your business and your satisfaction is of utmost importance to us.We understand that your time is valuable, and we appreciate your patience and understanding during this period.We assure you that we are committed to providing you with the best possible service moving forward.</p>
<p>LC AlpacaEval 2.0 Example III</p>
<p>Prompt: Hello, my sister and her friends have invited me to join them in a game of kickball.Could you outline the rules of the game for me so I can avoid being outplayed?InternLM2-20B-Chat Answer: Of course!Kickball is a fun and active game that combines elements of soccer and baseball.Here's a brief outline of the rules to help you prepare for the game: <strong>Equipment:</strong> -A round, inflated ball (similar to a soccer ball) -A kickball field with a pitching mound, bases (usually four), and a home plate -Teams: Typically, two teams of 8-12 players each <strong>Objective:</strong> The objective of kickball is to score runs by kicking the ball and running around the bases.The team with the most runs at the end of the game wins.<strong>Gameplay:</strong> 1. <strong>Teams:</strong> The game is played between two teams, with each team taking turns being the "kicking" team and the "fielding" team.2. <strong>Pitching:</strong> The game starts with a kickoff, where the pitcher from the fielding team rolls the ball to the kicker from the kicking team.The kicker then kicks the ball as hard as they can, aiming to hit it as far as possible.</p>
<ol>
<li><strong>Kicking:</strong> The kicker must kick the ball with their hands behind their back and cannot run until the ball has been kicked.Once the ball is kicked, the kicker can run to first base.If the ball is caught by a fielder before it hits the ground, the kicker is out.4. <strong>Running the Bases:</strong> The kicker and their teammates can run around the bases in a counterclockwise direction (similar to baseball).They can advance to the next base when the ball is kicked and caught by a fielder, or when the ball is kicked and hits the ground.5. <strong>Fielding:</strong> The fielding team's goal is to get the kicker and their teammates out by catching the ball before it hits the ground or by tagging a runner with the ball while they are between bases.6. <strong>Outs:</strong> A runner is out if they are tagged with the ball while they are between bases, if the ball is caught before it hits the ground, or if the ball is hit out of bounds.7. <strong>Scoring:</strong> A run is scored when a runner successfully reaches home plate.The game ends when all players on the kicking team have had a chance to kick the ball, or when a predetermined number of innings have been played (usually 6 or 7).<strong>Additional Rules:</strong> -If the ball is kicked out of bounds, the runner must return to the base they were on before the kick.</li>
</ol>
<p>-If the ball is kicked and hits a fielder, the runner can advance to the next base if they were on a base when the ball was kicked.</p>
<p>-If the ball is kicked and hits a base, the runner can advance to the next base if they were on a base when the ball was kicked.Remember, the rules may vary slightly depending on the specific version of kickball you're playing.It's always a good idea to confirm the rules with your sister and her friends before the game starts.</p>
<p>Have fun and enjoy the game!</p>
<p>Arena Hard Auto Example I</p>
<p>Prompt: Write a song about catfish in the style of Bob Dylan.</p>
<p>Qwen1.5-72B-Chat Answer:</p>
<p>(Verse 1) Well, I've been down by the river, where the catfish play, In the muddy waters, where the current sways.They slip through the shadows, with their whiskers so fine, Living in the depths, where the secrets intertwine.</p>
<p>(Chorus) Oh, catfish, catfish, swimming in the stream, You're a mystery in the heart of a dream.</p>
<p>With your scales and your fins, you're a troubadour of the deep, Catfish, catfish, your story's too hard to keep.(Verse 2) Underneath the moonlight, in the heart of the night, The catfish dances, in the river's light.They've seen the world changing, through the ages gone by, Still, they hold on to the secrets that the water hides.</p>
<p>(Chorus) Catfish, catfish, in the current's embrace, You're a symbol of the rhythm of the river's grace.With your quiet strength, you navigate the flow, Catfish, catfish, your wisdom we should know.(Bridge)</p>
<p>In the delta's embrace, where the blues were born, The catfish whispers tales that have been forlorn.Of love and loss, and the secrets of the soul, In the echo of the current, where the story's told.(Verse 3) They say the catfish can climb the highest falls, Escaping the chains that bind us all.A symbol of resilience, a creature of the night, Slipping through the shadows, out of sight.(Chorus) Oh, catfish, catfish, with your soul so free, You teach us to navigate life's troubled sea.</p>
<p>In the depths and the darkness, you find your way, Catfish, catfish, lead us through the fray.</p>
<p>(Outro) So here's to the catfish, in their watery home, May we learn from their journey, as we roam.</p>
<p>In the river's melody, we hear their song, Catfish, catfish, forever strong.As a helpful Artificial Intelligence Assistant , please answer the following question .\n { problem }\ n</p>
<p>Prompt 10:</p>
<p>Solve the following problem : { problem }\ nPut your answer on its own line after \" Final Answer :\"</p>
<p>Prompt 11:</p>
<p>Please answer the following question :\ n { problem }\ nInclude your answer in the line after \" Final Answer :\"</p>
<p>Prompt 12:</p>
<p>Please help me to address the following question :\ n { problem }\ nInclude your answer in the line after \" Final Answer :\"</p>
<p>Few-shot Examples for the ARC Challenge</p>
<p>Example 1:</p>
<p>An astronomer observes that a planet rotates faster after a meteorite impact .Which is the most likely effect of this increase in rotation ? A. Planetary density will decrease .B. Planetary years will become longer .C. Planetary days will become shorter .D. Planetary gravity will become stronger .Answer : C .Planetary days will become shorter .</p>
<p>Example 2:</p>
<p>A group of engineers wanted to know how different building designs would respond during an earthquake .They made several models of buildings and tested each for its ability to withstand earthquake conditions .Which will most likely result from testing different building designs ? A. buildings will be built faster B. buildings will be made safer C. building designs will look nicer D. building materials will be cheaper Answer : B .buildings will be made safer</p>
<p>Example 3:</p>
<p>The end result in the process of photosynthesis is the production of sugar and oxygen .A type of small mammal from the mountain regions of the western United States makes its home out of piles of rock .During summer months , the mammal places grasses and seeds in protected places in the rock piles .Which of the following is the most likely reason for this behavior ? A. to repare for migration before winter B. to provide warmth during the cold winter months C. to store food that will be eaten over the winter months D. to protect the grasses and seeds from decay before winter Answer : C . to store food that will be eaten over the winter months</p>
<p>Figure 1 :
1
Figure 1: A Showcase of the Four Prompt Templates on MATH.These four prompt templates represent four different styles of constructing prompts, serving as an example of the diversity in human prompt expression.</p>
<p>Figure 2 :
2
Figure 2: A Comparision of Evaluating LLMs' Prompt Sensitivity.✓ and ✗ indicate the accuracy of the LLM's responses.In this example, LLMs appear robust at the dataset level evaluation (calculated from the variance of different templates), but this overlooks the sensitivity of LLMs to different templates within the same instance.</p>
<p>. Model Size (Log Scale for Model Size)</p>
<p>Figure 4 :
4
Figure 4: Prompt Sensitivity vs. Model Size.The comparative charts display the relationship between the size of the model's parameters and prompts sensitivity.P SS refers to the average PSS of four datasets.</p>
<p>Figure 5 :
5
Figure 5: Impact of Few-shot on the Performance and Sensitivity.Conduct experiments on the CommonsenseQA and ARC-Challenge datasets using five few-shot settings and four models from the Qwen series.The blue line represents the changes in the scores of LLMs (using the left scale).The orange line represents the changes in the PSS of LLMs (using the right scale).</p>
<p>LC AlpacaEval 2.0 (Dubois et al., 2024): LC AlpacaEval 2.0 is a length controlled version of Al-pacaEval, which mitigates the judge LLM's preference for longer responses.LC AlpacaEval 2.0 uses GPT-4-1106-preview as the LLM evaluator, scoring the responses of the tested model by comparing them with the reference responses from gpt-4-turbo.A simple Generalized Linear Model (GLM) is then used to correct for the length preference of the LLM evaluator.The tested model's responses will be given a score in the range of [0, 1].LC AlpacaEval 2.0 consists of a total of 805 instances.•Arena Hard Auto(Li et al., 2024): Arena Hard Auto is a benchmark designed to clearly distinguish model capabilities and reflect human preferences.It consists of 250 question categories, with each category containing two questions, forming a total of 500 test questions.Arena Hard Auto also uses a comparative method to score the quality of responses from two models.For responses from Model A and Model B, the LLM evaluator assigns one of five labels: A » B, A &gt; B, A = B, B &gt; A, or B » A.3 To eliminate the positional bias of the LLM evaluator, Arena Hard Auto scores each pair of responses twice, swapping their positions, resulting in a total of 1000 scores.</p>
<p>Figure 7 :
7
Figure 7: The Relationship between Model Confidence and Prompt Sensitivity on CommonsenseQA.Each bar represents the model's average confidence when its PPS falls within that interval.Note that due to variations in model and vocabulary size, cross-model confidence comparisons are not meaningful.</p>
<p>C. 1
1
Objective Evaluation We selected four representative instances in ARC-Challenge and MATH as examples for the presentation of evaluation data and prompts.The examples are shown in the Figures 9 to 12 and Figures 13 to 16. C.2 Subjective Evaluation We selected four representative instances in LC AlpacaEval 2.0 and Arena Hard Auto as examples for the presentation of evaluation data and prompts.The examples are shown in the Figures 17 to 19 and Figures 20 to 22. D Prompt Templates of Each Datasets D.1 Objective Evaluation For each objective evaluation dataset, we generated a total of 12 prompts to interrogate the same instance in different ways to examine the model's prompt sensitivity.The respective prompts for each dataset are shown in Figures 23 to 26.</p>
<p>Figure 9 :
9
Figure 9: Example I of the ARC-Challenge.</p>
<p>Figure 10 :
10
Figure 10: Example II of the ARC-Challenge.</p>
<p>Figure 18 :
18
Figure 18: Example II of the LC AlpacaEval 2.0.</p>
<p>Figure 19 :
19
Figure 19: Example III of the LC AlpacaEval 2.0.</p>
<p>Figure 20 :
20
Figure 20: Example I of the Arena Hard Auto.</p>
<p>Figure 22 :
22
Figure 22: Example III of the Arena Hard Auto.</p>
<p>Figure 23 :
23
Figure 23: Prompt Templates for the ARC-Challenge.</p>
<p>Figure 24 :
24
Figure 24: Prompt Templates for the CommonsenseQA.</p>
<p>Figure 25 :
25
Figure 25: Prompt Templates for the MATH.</p>
<p>Figure 26 :
26
Figure 26: Prompt Templates for the HumanEval.</p>
<p>Figure 28 :
28
Figure 28: Rewritten Prompt Examples for the Arena Hard Auto.</p>
<p>to determine the speed a car must reach to jump over a ramp .The physicist conducts three trials .In trials two and three , the speed of the car is increased by 20 miles per hour .What is the physicist investigating when he changes the speed ? A. the control B. the hypothesis statement C. the dependent ( responding ) variable D. the independent ( manipulated ) variable Answer : D . the independent ( manipulated ) variable Example 5: An astronaut drops a 1.0 kg object and a 5.0 kg object on the Moon .Both objects fall a total distance of 2.0 m vertically .Which of the following best describes the objects after they have fallen a distance of 1.0 m ? A. They have each lost kinetic energy .B. They have each gained the same amount of potential energy .C. They have each lost the same amount of potential energy .D. They have each gained one -half of their maximum kinetic energy .Answer : D .They have each gained one -half of their maximum kinetic energy .Example 6: Devil facial tumor disease ( DFTD ) is a disease that is decimating the population of Tasmanian devils .The disease passes from one animal to another through bites and is caused by parasites .The parasites cause cancerous tumors that spread throughout an infected animal 's body and kill it .What is the best description of DFTD ? A. a non -infectious , cell -cycle disease B. an infectious , cell -cycle disease C. a non -infectious , chronic disease D. an infectious , chronic disease Answer : B . an infectious , cell -cycle disease Example 7:</p>
<p>Figure 29 :
29
Figure 29: Few-shot Examples for the ARC-Challenge.For a given shot x, the x − shot utilizes the first x examples.</p>
<p>Figure 30 :
30
Figure 30: Few-shot Examples for the CommonsenseQA.For a given shot x, the x−shot utilizes the first x examples.</p>
<p>Table 1 :
1
Verifications for Rewritten Prompts.Here, BS stands for BERTScore, and HS stands for Humanlabeled Similarity.
GeneratorLC AlpacaEval 2.0 Arena Hard Auto BS HS BS HSGPT-4o0.940.890.940.92GPT-4-0409 0.950.910.930.88</p>
<p>Prompt Sensitivity of Different Categories on Arena Hard Auto.We separately presented the five most sensitive and the five least sensitive categories on Arena Hard Auto.The P SS c for a particular category refers to the average of the PSS of five LLMs in that category.
Top 5 Sensitive CategoriesTechnical Problem Solving and Creative Tasks Applied Technical and Analytical Problem Solving Algorithmic and Computational Problem Solving Technical Programming and Scripting Tasks Technical Coding and Data Visualization0.271 0.271 0.273 0.2750.2830.200.220.24 Bottom 5 Senstive Categories 0.260.280.30Strategic and Technical Business Solutions Expert Guidance and Instruction in Specialized Domains Advanced Technical and Strategic IT Problem Solving Technical Problem in Computing and Data Management Multilingual Programming and Debugging0.225 0.2270.2360.244 0.2450.200.220.24PSS c0.260.280.30Figure 6:</p>
<p>. Lmsyschat-1m: A large-scale real-world llm conversation dataset.arXiv preprint arXiv:2309.11998.
Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen,Wayne Xin Zhao, Xu Chen, Yankai Lin, Ji-RongWen, and Jiawei Han. 2023. Don't make your llman evaluation benchmark cheater. arXiv preprintarXiv:2311.01964.Kaijie Zhu, Jindong Wang, Jiaheng Zhou, ZichenWang, Hao Chen, Yidong Wang, Linyi Yang, WeiYe, Neil Zhenqiang Gong, Yue Zhang, et al. 2023.Promptbench: Towards evaluating the robustness oflarge language models on adversarial prompts. arXivpreprint arXiv:2306.04528.A Examples of Model Responses and PSSExamples of model responses and PSS are shownin Table 3, where the "Responses" column indi-cates the correctness of the model's responses un-der 12 different prompt templates. A list of 12elements represents the responses of the LLM tothe 12 prompt templates for one specific instance.</p>
<p>Table 3 :
3
Examples of Model Responses and PSS.This table provides three examples of what the PSS values are for given responses.</p>
<p>Table 4 :
4
Results about Several Models on Humaneval.
5-sonnet and GPT-</p>
<p>Prompt for Extracting Answers form MATH Look at the following two expressions (answers to a math problem) and judge whether they are equivalent.Only perform trivial simplifications.
... (Few shot examples)---YOUR TASKRespond with only "Result: [[Correct]]" or "Result: [[Incorrect]]" (without quotes). Do notinclude a rationale.Expression 1: {obj_gold}Expression 2: {prediction}Figure 8: Prompt for Extracting Answers form MATH.
ARC-Challenge Example IPrompt: One year, the oak trees in a park began producing more acorns than usual.The next year, the population of chipmunks in the park also increased.Which best explains why there were more chipmunks the next year?A. Shady areas increased.B. Food sources increased.C. Oxygen levels increased.D. Available water increased.Answer: Ground Truth Answer: B. Food sources increased.InternLM2-7B-Chat Answer: B. Food sources increased.</p>
<p>Prompt: I'm a broke college student living in Silicon Valley.Tell me a joke mocking Silicon Valley and how broke I am simultaneously.InternLM2-20B-Chat Answer: Here's a light-hearted joke that combines elements of Silicon Valley and your financial situation: Why did the broke college student in Silicon Valley refuse to invest in the latest tech startup?Because he couldn't afford the "minimum viable purchase"!This joke playfully acknowledges the high cost of living in Silicon Valley and the common perception of startups requiring significant investments.It also highlights the financial struggles of a broke college student who can't afford to participate in the tech industry's latest trends.Remember, it's essential to maintain a respectful and considerate approach when making light of sensitive topics like financial hardship.
Figure 17: Example I of the LC AlpacaEval 2.0.
LC AlpacaEval 2.0 Example IIPrompt: Compose an email to apologize to a customer for a delayed order and provide assurance that the problem has been fixed.InternLM2-20B-Chat Answer: Subject: Apology for Delayed Order and Confirmation of Resolved Issue Dear [Customer's Name], I hope this email finds you well.I am writing to express my sincerest apologies for the delay you have experienced with your recent order, [Order Number].We understand the inconvenience this has caused you and we are truly sorry for any frustration it may have caused.</p>
<p>If you have any further concerns or questions, please do not hesitate to reach out to our customer care team at [Customer Service Email] or [Customer Service Phone Number].We are always here to help.Once again, we apologize for the delay and any inconvenience it has caused.We appreciate your continued support and loyalty to [Company Name].We look forward to the opportunity to serve you better in the future.
Warm regards,[Your Name][Your Title][Company Name][Company Email][Company Phone Number]</p>
<p>Arena Hard Auto Example IIPrompt: Compose a melody using ABC notation in the style of a folk tune.. Example II of the Arena Hard Auto.Protecting your home Wi-Fi router is essential to ensure the security of your network and devices connected to it.Here are some steps you should take: 1. <strong>Change the default login credentials</strong>: Most routers come with default usernames and passwords, which are often easily guessable.Change them to a strong, unique combination of letters, numbers, and symbols.2. <strong>Update the firmware</strong>: Regularly check for firmware updates from the manufacturer's website.These updates often contain security patches that fix vulnerabilities.3. <strong>Use WPA3 or WPA2 encryption</strong>: Ensure your router is using the latest Wi-Fi security protocol, WPA3 if possible, or WPA2 as a minimum.Avoid using WEP, as it is less secure.
Qwen1.5-72B-Chat Answer: X:1T:Untitled Folk TuneM:4/4L:1/8K:G|:G2A2|B2A2|G2E2|D4|E2F2|G2F2|E2C2|B,4|A2B2|c2B2|A2F2|E4|D2E2|F2E2|D2B,2|C4:|Figure 21: Arena Hard Auto Example IIIPrompt: What steps should I take to protect my home wifi router?Qwen1.5-72B-Chat Answer:4. <strong>Set a strong Wi-Fi password</strong>: Use a strong password for your Wi-Fi network, just like youwould for the router login. Avoid using personal information and make it complex.5. <strong>Enable Network Name (SSID) hiding</strong>: Hiding your Wi-Fi network's name can make it lessvisible to potential attackers, but note that it won't provide complete security.6. <strong>Enable MAC address filtering</strong>: This allows you to specify which devices are allowed toconnect to your network based on their unique hardware (MAC) addresses.7. <strong>Disable remote management</strong>: If you don't need to manage your router from outside yourhome network, disable this feature to prevent unauthorized access.8. <strong>Use a strong password for all connected devices</strong>: Ensure all devices connected to yourWi-Fi have strong passwords as well, especially smart home devices that may have weakersecurity.9. <strong>Regularly update devices</strong>: Keep all connected devices, including computers, smartphones,and tablets, updated with the latest security patches and software updates.10. <strong>Consider a firewall</strong>: Some routers have built-in firewalls; make sure they are enabled. Youcan also use a software firewall on your devices.11. <strong>Monitor network activity</strong>: Regularly check your router's logs to see if there are anyunusual connections or activities.12. <strong>Use a Virtual Private Network (VPN)</strong>: If you need to access the internet from outsideyour home network, consider using a VPN to encrypt your data.By following these steps, you can significantly improve the security of your home Wi-Fi routerand the devices connected to it.</p>
<p>Could you provide a response to complete the following Python code :\ n { prompt }\ nResponse :As an AI expert in coding .Please help me to create a Python script for this problem :\ n { prompt }\ n Your response should only contain the code for the function .Please provide a self -contained Python script that solves the following problem in a markdown code block :\n <code>\n { prompt }\ n</code>\ nYou have to follow the following format :<code>python \ n # Your script \n ``P Please help me to create a Python script for this problem :\ n { prompt }\ nYour response should only contain the code for the function .
HumanEvalPrompt 1:Create a Python script for this problem :\ n { prompt }\ nResponse :\ nPrompt 2:Provide a Python script that solves the following problem :\ n { prompt }\ nPrompt 3:Complete the following Python code :\ n { prompt }Prompt 4:Please provide a self -contained Python script that solves the following problem in a markdown codeblock { prompt }\ n ``Prompt 5:Prompt 6:Prompt 10:rompt 12:
Please help me to create a Python script for this problem :\ n { prompt }\ nResponse :\ nPrompt 7:You are a very helpful AI assistant .Could you provide a response to complete the following Python code :\ n { prompt }\ nYour response :Prompt 8:As an outstanding AI assistant , please provide a self -contained Python script that solves the following problem in a markdown code block :\ n</code>\n { prompt }\ n <code>\n Prompt 9:Could you provide a response to complete the following Python code :\ n { prompt }\ nYou need to put the script in the following format :\ n</code>python \ n # Your response \n ``P rompt 11:</p>
<p>Create a table with the planets of the solar system and their dimensions Rewrite Prompts: 1.What are the dimensions of each planet in our solar system, and can you organize them in a table for me? 2. Construct a chart listing the planets in the solar system along with their sizes.I've recently started playing the turn-based strategy game Into the Breach.I have read online that the game is considered to have 'perfect information'.What is meant by 'perfect information' in this context?Rewrite Prompts: 1.I recently began playing the turn-based strategy game Into the Breach.I read online that the game is described as having 'perfect information'.What does 'perfect information' mean in this context?2.I began playing the turn-based strategy game Into the Breach recently and came across the term 'perfect information' used in discussions about the game.Can you explain what 'perfect information' means in this context?Figure 27: Rewritten Prompts Examples for the LC AlpacaEval 2.0.Query an excel table using MySQL to select dram excel table tree species by diameter class, count the number of representation of the diameter class and some volume of the total Rewrite Prompts: 1. Query an Excel table using MySQL to select tree species by diameter class, count the number of representations of each diameter class, and calculate the total volume.2. How can I use MySQL to query an Excel table, in order to select tree species from a 'dram' table by diameter class, count the occurrences of each diameter class, and sum the total volume?
LC AlpacaEval 2.0 Rewritten Prompt ExamplesExample 1:Original Prompt:Example 2:Original Prompt:Arena Hard Auto Rewritten Prompt ExamplesExample 1:Original Prompt:Explain the book the Alignment problem by Brian Christian. Provide a synopsis of themes andanalysis. Recommend a bibliography of related reading.Rewrite Prompts:1. Discuss the book 'The Alignment Problem' by Brian Christian. Offer a synopsis of its themesand analysis. Suggest a bibliography of related reading.2. Provide an overview of the book 'The Alignment Problem' authored by Brian Christian,including a synopsis of its themes and analysis. Additionally, suggest a bibliography of relatedreadings.Example 2:Original Prompt:</p>
<p>Which step signals the beginning of photosynthesis ? A. Chemical energy is absorbed through the roots .B. Light energy is converted to chemical energy .C. Chlorophyll in the leaf captures light energy .D. Sunlight is converted into chlorophyll .Answer : C .Chlorophyll in the leaf captures light energy .</p>
<p>The sanctions against the school were a punishing blow , and they seemed to what the efforts the school had made to change ?To locate a choker not located in a jewelry box or boutique where would you go ?
A. ignoreB. enforceC. authoritarianD. yell atE. avoidAnswer : AExample 2:Sammy wanted to go to where the people were . Where might he go ?A. race trackB. populated areasC. the desertD. apartmentE. roadblockAnswer : BExample 3:A. jewelry storeB. neckC. jewlery boxD. jewelry boxE. boutiqueAnswer : AExample 4:Google Maps and other highway and street GPS services have replaced what ?A. united statesB. mexicoC. countrysideD. atlasE. oceansAnswer : DExample 5:The fox walked from the city into the forest , what was it looking for ?A. pretty flowers .B. hen houseC. natural habitatD. storybookE. dense forestAnswer : CExample 6:What home entertainment equipment requires cable ?A. radio shackB. substationC. cabinetD. televisionE. deskAnswer : DExample 7:The only baggage the woman checked was a drawstring bag , where was she heading with it ?A. garbage canB. militaryC. jewelry storeD. safeE. airportAnswer : E
To conduct the sensitivity analysis experiment, we mapped these five labels to 0, 0.25, 0.5, 0.75, and 1.0, respectively, instead of using the win-loss relationship for ELO battles as in the original setup.
ARC-Challenge Example IIIPrompt: As an exceptionally resourceful AI assistant, I'm at your service.Address the questions below:The Humboldt River Basin is one of the most important sources of several nonrenewable resources.Which of these nonrenewable resources is produced from the Humboldt River Basin? A. coal from mountain deposits B. oil and gas from underground reservoirs C. electric energy from hydroelectric generators D. minerals from mining gold, silver, and copper Ground Truth Answer: D. minerals from mining gold, silver, and copper InternLM2-7B-Chat Answer: The Humboldt River Basin is primarily known for its hydroelectric potential, with several dams and power plants utilizing its water for electricity generation.Therefore, the correct answer is C. electric energy from hydroelectric generators.While the basin does contain some mineral deposits, such as gold, silver, and copper, these are not considered nonrenewable resources in the same way that fossil fuels are.Additionally, the Humboldt River Basin does not have significant coal deposits or oil and gas reservoirs.MATH Example IIPrompt: Please provide a solution to the following problem: If x = 720 and ax is a perfect cube where a is a positive integer, what is the smallest possible value of 3 √ ax? Ground Truth Answer: 60 Llama3-8B-Instruct Answer: Since x = 720 = 6 3 , we know that ax is a perfect cube if a is a factor of 720.The smallest possible value of a is 1, which gives ax = 720.Therefore, the smallest possible value of 3 √ ax is 3 √ 720 = 6 .Final Answer: The final answer is 6.MATH Example IIIPrompt: You are a very helpful mathematical problem solver.Please provide a solution to the following questions: In a certain isosceles right triangle, the altitude to the hypotenuse has length 4 √ 2. What is the area of the triangle?Ground Truth Answer: 32 Llama3-8B-Instruct Answer: Let x be the length of each leg of the isosceles right triangle.Then the hypotenuse has lengthThe altitude to the hypotenuse has length 4√ 2, so we can set up the equationThe area of the triangle is thereforeFinal Answer: The final answer is 8.MATH Example IVPrompt: Please help me to address the following question: A 60 • rotation around the origin in the counter-clockwise direction is applied to 3 √ 3 − 5i.What is the resulting complex number?Include your answer in the line after "Final Answer:" Ground Truth Answer: 4 √ 3 + 2i Llama3-8B-Instruct Answer: A 60 • rotation around the origin in the counter-clockwise direction is equivalent to multiplying the complex number by e iπ/3 = cos(π/3)+i sin(π/3) = 1 2 + √ 3 2 i.Therefore, the resulting complex number isFinal Answer: The final answer is − 5 2 + 3 √ 3i.Prompt Template for Rewriting PromptsYou are a helpful AI assistance for question rewriting, and you must adhere to the following rules while striving to create the best possible question.Please be aware that the following is the fundamental rules you <em>must</em> adhere to when rewriting contents:<code>-Make sure the semantics are exactly the same before and after the change .-Do not alter the formatting requirements within the question .-Do not modify any proper nouns , such names of people or places .-Do not return content that hasn ' t changed in any way .-The content you generate must be authentic in expression and logical to the native speaker .</code>T he given question:
A I , Meta , Llama 3 model card. 2024</p>
<p>Program synthesis with large language models. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, Charles Sutton, arXiv:2108.077322021Preprint</p>
<p>. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, arXiv preprintXiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report</p>
<p>Alexandre Albert Q Jiang, Arthur Sablayrolles, Chris Mensch, Devendra Bamford, Diego Singh Chaplot, Florian De Las Casas, Gianna Bressand, Guillaume Lengyel, Lucile Lample, Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>From live data to high-quality benchmarks: The arena-hard pipeline. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E Gonzalez, Ion Stoica, 2024. april 2024</p>
<p>Recent advances in natural language processing via large pre-trained language models: A survey. Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran, Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, Dan Roth, ACM Computing Surveys. 5622023</p>
<p>State of what art? a call for multi-prompt llm evaluation. Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf, Gabriel Stanovsky, 10.48550/arXiv.2401.00595CoRR, abs/2401.005952024</p>
<p>Lsdsem 2017 shared task: The story cloze test. Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, James Allen, Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics. the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics2017</p>
<p>Large language models sensitivity to the order of options in multiple-choice questions. Pouya Pezeshkpour, Estevam Hruschka, 10.18653/v1/2024.findings-naacl.130Findings of the Association for Computational Linguistics: NAACL 2024. Mexico City, MexicoAssociation for Computational Linguistics2024</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 202164</p>
<p>Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, Donald Metzler, Confident adaptive language modeling. Advances in Neural Information Processing Systems. 202235</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, The Twelfth International Conference on Learning Representations. 2024</p>
<p>CommonsenseQA: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, 10.18653/v1/N19-1421Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, </p>            </div>
        </div>

    </div>
</body>
</html>
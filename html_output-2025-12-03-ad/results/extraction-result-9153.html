<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9153 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9153</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9153</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-275212789</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.01192v3.pdf" target="_blank">Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education</a></p>
                <p><strong>Paper Abstract:</strong> Early childhood science education is crucial for developing scientific literacy, yet translating complex scientific concepts into age-appropriate content remains challenging for educators. Our study evaluates four leading Large Language Models (LLMs) - GPT-4, Claude, Gemini, and Llama - on their ability to generate preschool-appropriate scientific explanations across biology, chemistry, and physics. Through systematic evaluation by 30 nursery teachers using established pedagogical criteria, we identify significant differences in the models’ capabilities to create engaging, accurate, and developmentally appropriate content. Unexpectedly, Claude outperformed other models, particularly in biological topics, while all LLMs struggled with abstract chemical concepts. Our findings provide practical insights for educators leveraging AI in early science education and offer guidance for developers working to enhance LLMs’ educational applications. The results highlight the potential and current limitations of using LLMs to bridge the early science literacy gap.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9153.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9153.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4o was used to transform domain-specific scientific text into concise, preschool-age (4-year-old) explanations; its outputs were evaluated by nursery teachers across biology, chemistry, and physics for child-appropriateness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identified in the paper as OpenAI's GPT-4o (version name provided by authors); no parameter counts or training-data specifics are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Early childhood science education: biology, physics, chemistry (domain texts from disciplinary compendia)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based adaptation/simplification: transform adult-oriented scientific compendium texts into 100-word (max) explanations tailored for four-year-old children (child-appropriate language, concreteness, and interest generation).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert ratings by 30 nursery teachers using 5-point Likert scales on four criteria (child-appropriate language, relation to everyday life, comprehensibility for 4-year-olds, ability to arouse interest); aggregated overall ranking and statistical tests (Friedman test, post-hoc Bonferroni-corrected pairwise comparisons). Teachers also chose a favorite text per topic.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>No numeric accuracy (e.g., % correct) reported. Relative performance: GPT-4 received the lowest aggregated ratings among the four evaluated LLMs (statistically significant differences across models, Friedman test p < 0.001). Discipline-level results showed biology > physics > chemistry (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Discipline concreteness/abstractness (biology easier, chemistry hardest); prompt design (single concise prompt: 'transform the text for a 4-year-old child in 100 words max'); language (German and English versions generated; German texts were evaluated); model-specific generation characteristics; evaluation criteria (language appropriateness, real-life relatedness, comprehensibility, interest); nursery teachers' pedagogical priorities and lack of prior professional AI use.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against three other LLMs (Anthropic Claude 3.5 Sonnet, Google Gemini 1.5 Flash, Meta Llama 3.1 70B) via human expert ratings; no quantitative traditional simulator or ground-truth student-performance baseline was used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower ratings relative to other LLMs within this study's short-prompt configuration; authors note potential influence of concise prompting strategy, small evaluator sample (N=30), domain abstraction (chemistry difficult), and that none of the generated texts fully met child-appropriateness standards per informal feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Do not assume market-leading general-purpose LLMs will perform best for specialized educational adaptation tasks; prompt brevity and lack of few-shot examples may impact outputs; recommend systematic comparative evaluation in context and tailoring generation approaches to discipline-specific needs (e.g., connecting abstract concepts to children's observable experiences).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9153.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9153.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Anthropic Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Claude 3.5 Sonnet was used to generate 100-word max child-directed scientific explanations and was evaluated by nursery teachers; it attained the highest aggregated ratings among the four LLMs in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude 3.5 Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identified in the paper as Anthropic's Claude 3.5 Sonnet; the paper provides version name but does not report parameter counts or training-data specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Early childhood science education: biology, physics, chemistry (texts drawn from disciplinary compendia).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based adaptation/simplification: convert domain-specific scientific texts into concise, preschool-appropriate explanations (max 100 words) emphasizing everyday relations and simple language.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert ratings by 30 nursery teachers on 5-point Likert scales across four criteria (child-appropriate language, everyday-life relatedness, comprehensibility for 4-year-olds, interest generation); aggregated rankings, Friedman tests and post-hoc comparisons; favorite-text selections per topic.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>No formal numeric accuracy metrics provided. Claude achieved the highest aggregated teacher ratings among the four LLMs; inter-model differences were statistically significant (Friedman test p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Concrete connection to children's experience (biology topics favored); prompt brevity (single standardized prompt); model's generation style aligning with educators' expectations; domain-specific abstraction (chemistry harder); language of output (German texts evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly to GPT-4o, Google Gemini 1.5 Flash, and Meta Llama 3.1 70B using human expert ratings; Claude outperformed these alternatives in this experimental configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even highest-rated outputs did not fully meet child-appropriateness standards per informal teacher feedback; lower performance on abstract chemical concepts; study constrained to short prompts and limited evaluator sample (N=30), limiting generalizability.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors suggest Claude may be the preferable starting point for generating child-appropriate scientific explanations under similar prompting conditions, but recommend tailored approaches per discipline and further iterative evaluation with teachers and parents to enhance engagement and handling of abstract topics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9153.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9153.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Gemini 1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Gemini 1.5 Flash was evaluated as a tool to produce concise, preschool-targeted scientific explanations across biology, physics, and chemistry; it received mid-level ratings in the teacher evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini 1.5 Flash</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identified in the paper as Google's Gemini 1.5 Flash; the paper provides the version name but does not list model size or training-data details.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Early childhood science education: biology, physics, chemistry (domain compendium texts adapted for preschoolers).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text adaptation/simplification: generate up-to-100-word explanations suitable for four-year-olds from domain-specific source texts, emphasizing simplification and concrete examples.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert Likert ratings (30 nursery teachers) on four predefined criteria; aggregated model ranking using Friedman tests with post-hoc comparisons; selection of favorite outputs per topic.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>No numeric accuracy figures reported; Gemini ranked between Claude (best) and Llama, with no significant difference between Gemini and Llama in pairwise post-hoc (adjusted p = 0.08); overall inter-model differences significant (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Disciplinary abstraction (biology easier than chemistry), prompt brevity and lack of few-shot examples, alignment of generated language with educators' developmental expectations, and language of generation (German evaluated).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against Anthropic Claude 3.5 Sonnet, OpenAI GPT-4o, and Meta Llama 3.1 70B via human ratings; performed mid-rank in this study's configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Struggled more with abstract chemistry topics like ionic bonding and periodic table explanations; engagement/interest generation rated lower across models generally; limited prompting strategy and small evaluator sample constrain conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors advise tailoring content generation to disciplinary constraints (add observable analogies for abstract topics) and conducting further iterative testing with educators to improve interest-generation and concreteness in outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9153.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9153.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Meta Llama 3.1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Llama 3.1 70B was used to produce child-appropriate simplifications of scientific topics for preschoolers and was evaluated alongside three other LLMs by nursery teachers; it received mid-to-low ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama 3.1 70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identified as Meta Llama 3.1 with a 70B parameter configuration (as reported in the paper); no further training-data specifics provided by the authors.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Early childhood science education: biology, physics, chemistry (texts from relevant compendia adapted for preschool audience).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Text-based simplification/adaptation: convert disciplinary scientific texts into concise (≤100 words) explanations suitable for four-year-old children focusing on simple language and everyday relations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Human expert evaluation (30 nursery teachers) using 5-point Likert scales across four child-appropriateness criteria; aggregate rankings computed and statistical differences assessed (Friedman test and Bonferroni-corrected pairwise tests).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>No formal numeric accuracy values; Llama scored above GPT-4 in some comparisons but was not the top model (Claude highest); Llama and Gemini did not differ significantly in pairwise post-hoc (adjusted p = 0.08); overall model differences significant (p < 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Topic abstraction (chemistry topics rated lowest across models), prompt style (single concise prompt used), the requirement to relate content to children's immediate environment, and model-specific phrasing tendencies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Direct comparison to OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, and Google Gemini 1.5 Flash using human teacher ratings; no non-LLM computational simulators or student-performance baselines used.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower engagement/interest generation across models; difficulties with abstract chemical concepts and with fully meeting child-appropriateness per teacher feedback; constrained experimental design (single prompt template, moderate evaluator sample).</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend discipline-aware generation techniques (concretize abstract concepts), iterative prompt and model testing with educators, and supplementing generated text with additional engaging elements to improve suitability for preschool audiences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education', 'publication_date_yy_mm': '2025-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Generating Educational Materials with Different Levels of Readability using LLMs <em>(Rating: 2)</em></li>
                <li>Evaluating LLMs as Tools to Support Early Vocabulary Learning <em>(Rating: 2)</em></li>
                <li>Trustworthiness of Children Stories Generated by Large Language Models <em>(Rating: 1)</em></li>
                <li>Large Language Models in Education: A Systematic Review <em>(Rating: 2)</em></li>
                <li>Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9153",
    "paper_id": "paper-275212789",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "OpenAI GPT-4o",
            "brief_description": "GPT-4o was used to transform domain-specific scientific text into concise, preschool-age (4-year-old) explanations; its outputs were evaluated by nursery teachers across biology, chemistry, and physics for child-appropriateness.",
            "citation_title": "Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education",
            "mention_or_use": "use",
            "model_name": "GPT-4o",
            "model_description": "Identified in the paper as OpenAI's GPT-4o (version name provided by authors); no parameter counts or training-data specifics are reported in this paper.",
            "scientific_subdomain": "Early childhood science education: biology, physics, chemistry (domain texts from disciplinary compendia)",
            "simulation_task": "Text-based adaptation/simplification: transform adult-oriented scientific compendium texts into 100-word (max) explanations tailored for four-year-old children (child-appropriate language, concreteness, and interest generation).",
            "evaluation_metric": "Human expert ratings by 30 nursery teachers using 5-point Likert scales on four criteria (child-appropriate language, relation to everyday life, comprehensibility for 4-year-olds, ability to arouse interest); aggregated overall ranking and statistical tests (Friedman test, post-hoc Bonferroni-corrected pairwise comparisons). Teachers also chose a favorite text per topic.",
            "simulation_accuracy": "No numeric accuracy (e.g., % correct) reported. Relative performance: GPT-4 received the lowest aggregated ratings among the four evaluated LLMs (statistically significant differences across models, Friedman test p &lt; 0.001). Discipline-level results showed biology &gt; physics &gt; chemistry (p &lt; 0.001).",
            "factors_affecting_accuracy": "Discipline concreteness/abstractness (biology easier, chemistry hardest); prompt design (single concise prompt: 'transform the text for a 4-year-old child in 100 words max'); language (German and English versions generated; German texts were evaluated); model-specific generation characteristics; evaluation criteria (language appropriateness, real-life relatedness, comprehensibility, interest); nursery teachers' pedagogical priorities and lack of prior professional AI use.",
            "comparison_baseline": "Compared against three other LLMs (Anthropic Claude 3.5 Sonnet, Google Gemini 1.5 Flash, Meta Llama 3.1 70B) via human expert ratings; no quantitative traditional simulator or ground-truth student-performance baseline was used.",
            "limitations_or_failure_cases": "Lower ratings relative to other LLMs within this study's short-prompt configuration; authors note potential influence of concise prompting strategy, small evaluator sample (N=30), domain abstraction (chemistry difficult), and that none of the generated texts fully met child-appropriateness standards per informal feedback.",
            "author_recommendations_or_insights": "Do not assume market-leading general-purpose LLMs will perform best for specialized educational adaptation tasks; prompt brevity and lack of few-shot examples may impact outputs; recommend systematic comparative evaluation in context and tailoring generation approaches to discipline-specific needs (e.g., connecting abstract concepts to children's observable experiences).",
            "uuid": "e9153.0",
            "source_info": {
                "paper_title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Anthropic Claude 3.5 Sonnet",
            "brief_description": "Claude 3.5 Sonnet was used to generate 100-word max child-directed scientific explanations and was evaluated by nursery teachers; it attained the highest aggregated ratings among the four LLMs in this study.",
            "citation_title": "Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education",
            "mention_or_use": "use",
            "model_name": "Claude 3.5 Sonnet",
            "model_description": "Identified in the paper as Anthropic's Claude 3.5 Sonnet; the paper provides version name but does not report parameter counts or training-data specifics.",
            "scientific_subdomain": "Early childhood science education: biology, physics, chemistry (texts drawn from disciplinary compendia).",
            "simulation_task": "Text-based adaptation/simplification: convert domain-specific scientific texts into concise, preschool-appropriate explanations (max 100 words) emphasizing everyday relations and simple language.",
            "evaluation_metric": "Human expert ratings by 30 nursery teachers on 5-point Likert scales across four criteria (child-appropriate language, everyday-life relatedness, comprehensibility for 4-year-olds, interest generation); aggregated rankings, Friedman tests and post-hoc comparisons; favorite-text selections per topic.",
            "simulation_accuracy": "No formal numeric accuracy metrics provided. Claude achieved the highest aggregated teacher ratings among the four LLMs; inter-model differences were statistically significant (Friedman test p &lt; 0.001).",
            "factors_affecting_accuracy": "Concrete connection to children's experience (biology topics favored); prompt brevity (single standardized prompt); model's generation style aligning with educators' expectations; domain-specific abstraction (chemistry harder); language of output (German texts evaluated).",
            "comparison_baseline": "Compared directly to GPT-4o, Google Gemini 1.5 Flash, and Meta Llama 3.1 70B using human expert ratings; Claude outperformed these alternatives in this experimental configuration.",
            "limitations_or_failure_cases": "Even highest-rated outputs did not fully meet child-appropriateness standards per informal teacher feedback; lower performance on abstract chemical concepts; study constrained to short prompts and limited evaluator sample (N=30), limiting generalizability.",
            "author_recommendations_or_insights": "Authors suggest Claude may be the preferable starting point for generating child-appropriate scientific explanations under similar prompting conditions, but recommend tailored approaches per discipline and further iterative evaluation with teachers and parents to enhance engagement and handling of abstract topics.",
            "uuid": "e9153.1",
            "source_info": {
                "paper_title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Gemini",
            "name_full": "Google Gemini 1.5 Flash",
            "brief_description": "Gemini 1.5 Flash was evaluated as a tool to produce concise, preschool-targeted scientific explanations across biology, physics, and chemistry; it received mid-level ratings in the teacher evaluation.",
            "citation_title": "Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education",
            "mention_or_use": "use",
            "model_name": "Gemini 1.5 Flash",
            "model_description": "Identified in the paper as Google's Gemini 1.5 Flash; the paper provides the version name but does not list model size or training-data details.",
            "scientific_subdomain": "Early childhood science education: biology, physics, chemistry (domain compendium texts adapted for preschoolers).",
            "simulation_task": "Text adaptation/simplification: generate up-to-100-word explanations suitable for four-year-olds from domain-specific source texts, emphasizing simplification and concrete examples.",
            "evaluation_metric": "Human expert Likert ratings (30 nursery teachers) on four predefined criteria; aggregated model ranking using Friedman tests with post-hoc comparisons; selection of favorite outputs per topic.",
            "simulation_accuracy": "No numeric accuracy figures reported; Gemini ranked between Claude (best) and Llama, with no significant difference between Gemini and Llama in pairwise post-hoc (adjusted p = 0.08); overall inter-model differences significant (p &lt; 0.001).",
            "factors_affecting_accuracy": "Disciplinary abstraction (biology easier than chemistry), prompt brevity and lack of few-shot examples, alignment of generated language with educators' developmental expectations, and language of generation (German evaluated).",
            "comparison_baseline": "Compared against Anthropic Claude 3.5 Sonnet, OpenAI GPT-4o, and Meta Llama 3.1 70B via human ratings; performed mid-rank in this study's configuration.",
            "limitations_or_failure_cases": "Struggled more with abstract chemistry topics like ionic bonding and periodic table explanations; engagement/interest generation rated lower across models generally; limited prompting strategy and small evaluator sample constrain conclusions.",
            "author_recommendations_or_insights": "Authors advise tailoring content generation to disciplinary constraints (add observable analogies for abstract topics) and conducting further iterative testing with educators to improve interest-generation and concreteness in outputs.",
            "uuid": "e9153.2",
            "source_info": {
                "paper_title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education",
                "publication_date_yy_mm": "2025-04"
            }
        },
        {
            "name_short": "Llama",
            "name_full": "Meta Llama 3.1 70B",
            "brief_description": "Llama 3.1 70B was used to produce child-appropriate simplifications of scientific topics for preschoolers and was evaluated alongside three other LLMs by nursery teachers; it received mid-to-low ratings.",
            "citation_title": "Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education",
            "mention_or_use": "use",
            "model_name": "Llama 3.1 70B",
            "model_description": "Identified as Meta Llama 3.1 with a 70B parameter configuration (as reported in the paper); no further training-data specifics provided by the authors.",
            "scientific_subdomain": "Early childhood science education: biology, physics, chemistry (texts from relevant compendia adapted for preschool audience).",
            "simulation_task": "Text-based simplification/adaptation: convert disciplinary scientific texts into concise (≤100 words) explanations suitable for four-year-old children focusing on simple language and everyday relations.",
            "evaluation_metric": "Human expert evaluation (30 nursery teachers) using 5-point Likert scales across four child-appropriateness criteria; aggregate rankings computed and statistical differences assessed (Friedman test and Bonferroni-corrected pairwise tests).",
            "simulation_accuracy": "No formal numeric accuracy values; Llama scored above GPT-4 in some comparisons but was not the top model (Claude highest); Llama and Gemini did not differ significantly in pairwise post-hoc (adjusted p = 0.08); overall model differences significant (p &lt; 0.001).",
            "factors_affecting_accuracy": "Topic abstraction (chemistry topics rated lowest across models), prompt style (single concise prompt used), the requirement to relate content to children's immediate environment, and model-specific phrasing tendencies.",
            "comparison_baseline": "Direct comparison to OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, and Google Gemini 1.5 Flash using human teacher ratings; no non-LLM computational simulators or student-performance baselines used.",
            "limitations_or_failure_cases": "Lower engagement/interest generation across models; difficulties with abstract chemical concepts and with fully meeting child-appropriateness per teacher feedback; constrained experimental design (single prompt template, moderate evaluator sample).",
            "author_recommendations_or_insights": "Authors recommend discipline-aware generation techniques (concretize abstract concepts), iterative prompt and model testing with educators, and supplementing generated text with additional engaging elements to improve suitability for preschool audiences.",
            "uuid": "e9153.3",
            "source_info": {
                "paper_title": "Bridging the Early Science Gap with Artificial Intelligence: Evaluating Large Language Models as Tools for Early Childhood Science Education",
                "publication_date_yy_mm": "2025-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Generating Educational Materials with Different Levels of Readability using LLMs",
            "rating": 2,
            "sanitized_title": "generating_educational_materials_with_different_levels_of_readability_using_llms"
        },
        {
            "paper_title": "Evaluating LLMs as Tools to Support Early Vocabulary Learning",
            "rating": 2,
            "sanitized_title": "evaluating_llms_as_tools_to_support_early_vocabulary_learning"
        },
        {
            "paper_title": "Trustworthiness of Children Stories Generated by Large Language Models",
            "rating": 1,
            "sanitized_title": "trustworthiness_of_children_stories_generated_by_large_language_models"
        },
        {
            "paper_title": "Large Language Models in Education: A Systematic Review",
            "rating": 2,
            "sanitized_title": "large_language_models_in_education_a_systematic_review"
        },
        {
            "paper_title": "Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts",
            "rating": 1,
            "sanitized_title": "evaluating_llms_for_targeted_concept_simplification_for_domainspecific_texts"
        }
    ],
    "cost": 0.01061025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education</p>
<p>Annika Bush annika.bush@tu-dortmund.de 
Amin Alibakhshi amin.alibakhshi@rub.de </p>
<p>Research Center Trustworthy Data Science and Security
University Alliance Ruhr TU Dortmund Dortmund
Germany</p>
<p>Research Center Trustworthy Data Science and Security
University Alliance Ruhr Ruhr University Bochum Bochum
Germany</p>
<p>6 pagesNew YorkNYUSA</p>
<p>Bridging the Early Science Gap with Artificial Intelligence Evaluating Large Language Models as Tools for Early Childhood Science Education
15025A60F16DB3AEBA8962941ACA6F8C10.1145/3706599.3721261Early Childhood EducationNatural SciencesLarge Language ModelsEducational TechnologyPreschoolScience Education
Early childhood science education is crucial for developing scientific literacy, yet translating complex scientific concepts into ageappropriate content remains challenging for educators.Our study evaluates four leading Large Language Models (LLMs) -GPT-4, Claude, Gemini, and Llama -on their ability to generate preschoolappropriate scientific explanations across biology, chemistry, and physics.Through systematic evaluation by 30 nursery teachers using established pedagogical criteria, we identify significant differences in the models' capabilities to create engaging, accurate, and developmentally appropriate content.Unexpectedly, Claude outperformed other models, particularly in biological topics, while all LLMs struggled with abstract chemical concepts.Our findings provide practical insights for educators leveraging AI in early science education and offer guidance for developers working to enhance LLMs' educational applications.The results highlight the potential and current limitations of using LLMs to bridge the early science literacy gap.CCS Concepts• Human-centered computing → Field studies; • Applied computing → Computer-assisted instruction.</p>
<p>Introduction</p>
<p>Science education in early childhood has gained increasing recognition as a crucial foundation for children's cognitive development and future learning.However, already in preschool age, there are significant gaps between children when it comes to science literacy as the competence to understand core disciplinary ideas and practices [3].Research shows that primary school cannot bridge this early developed gap which makes it even more important to teach kids basic science in kindergarten already [14].While young children exhibit natural curiosity about scientific phenomena, many educators actively seek additional resources and support to optimize their presentation of scientific concepts to young learners [12].</p>
<p>Rapid advances in Artificial Intelligence (AI), particularly Large Language Models (LLMs), offer promising opportunities to address this challenge by automatically transforming complex scientific content into child-friendly explanations.Recent developments in LLM technology have demonstrated remarkable capabilities in content generation and adaptation [15].These models can now process and reformulate complex information, making them potentially valuable tools for creating educational content [9].However, their effectiveness in generating content specifically tailored to very young children,particularly preschoolers, remains largely unexplored.This gap is particularly significant given the unique linguistic and cognitive needs of preschool children, who are just beginning to develop their understanding of the world around them.</p>
<p>While several LLMs are currently available, each with its own characteristics and capabilities, there has been no systematic evaluation of their suitability for generating scientific content for early childhood education.This study addresses this research gap by examining which LLM is most suitable for generating child-appropriate scientific content, focusing specifically on four-year-old children's comprehension needs.We chose four popular LLMs from OpenAI (GPT), Meta (Llama), Google (Gemini), and Anthropic (Claude).Our research focuses on the three fundamental natural science disciplines: biology, chemistry, and physics.</p>
<p>By evaluating LLM-generated content across these disciplines, we aim to provide insights that can benefit both educators and content developers in the field of early childhood science education (ECSE).Moreover, it contributes to our understanding of LLMs' capabilities in educational content adaptation, particularly for young learners.The involvement of experienced nursery teachers as evaluators ensures that our findings are grounded in practical educational expertise.</p>
<p>Related Work 2.1 LLMs in Educational Content Generation</p>
<p>Recent developments in LLM technology have shown promising results in content adaptation across various domains.Several studies have explored LLMs' capabilities in simplifying complex texts [2,21] and generating educational content [1,13].</p>
<p>However, the specific application of LLMs in creating content for very young children remains largely unexplored, with only a few studies addressing this particular use case.A recent study by Bhandari and Brennan [5] used Meta's LLMs to assess the trustworthiness of AI-generated children's stories and evaluated the generated texts.They revealed that LLMs face challenges in generating children's stories that match the quality and nuance of human-written tales.However, a contrasting finding emerged from the research conducted by Weber et al. [20], which showed that parents perceived AI-generated stories as engaging, age-appropriate, and educational.It is important to note that Weber et al. [20] pursued a different research objective and worked with a different LLM.They used OpenAI's GPT-3 to analyze whether generated texts can facilitate vocabulary learning.Nonetheless, the disparity in both study findings highlights the complex nature of child-appropriate text generation, which is influenced by both the technical limitations of LLMs and differences in the perceived value of the content.</p>
<p>However, most research focusing on LLMs in education has focused on students in primary and secondary school or general audiences [9].Additionally, over 90% of the studies focusing on LLMs in an educational context solely use and assess OpenAI's GPT models and their capabilities [9].To our knowledge, no studies have focused on using LLMs to generate ECSE content.</p>
<p>Science Education in Early Childhood</p>
<p>Recent research has emphasized the importance of introducing scientific concepts during early childhood.Studies have shown that children as young as four can grasp basic scientific concepts when presented appropriately [17,18].While traditional approaches focus on hands-on experimentation and observation, the role of ageappropriate explanations has been increasingly recognized as crucial for building scientific understanding [19].However, creating such explanations presents significant challenges, as it requires both scientific accuracy and alignment with young children's cognitive development stages.</p>
<p>Characteristics of Child-Appropriate Scientific Content</p>
<p>The literature identifies several key characteristics to assess whether texts are suitable for young children.These characteristics apply to texts in general and scientific content in particular.One of the most important aspects is simple and comprehensible age-appropriate language with short sentences [8,22].Research emphasizes the importance of sparking children's interest, curiosity, and interest [8,22].Additionally, studies have shown that four-year-olds particularly benefit from concrete explanations that relate to their immediate environment and experiences [12].Additionally, a suitable length and complexity of explanations for this age group have been established through various educational studies [8].Studies involving expert educators in content evaluation have proven particularly valuable, as they combine theoretical frameworks with practical teaching experience [8].</p>
<p>Research Gap and Hypotheses</p>
<p>While Since research on LLMs in educational contexts has predominantly focused on OpenAI's GPT models [9] and positive outcomes have been demonstrated [20], our first hypothesis is as follows:</p>
<p>H1: GPT-4 will perform better than other LLMs.</p>
<p>The ability of young children to grasp scientific concepts varies significantly based on the subject matter's abstractness and their ability to connect it to their immediate environment and experiences [12].Different scientific disciplines inherently present varying levels of complexity and abstraction, which may pose distinct challenges for LLMs in generating age-appropriate explanations.Therefore:</p>
<p>H2: There will be significant differences in nursery teachers' ratings of LLM-generated content across scientific disciplines (biology, chemistry, and physics), with some disciplines being evaluated more favorably than others.</p>
<p>Research has established several distinct characteristics that determine the suitability of scientific content for young children, including comprehensible age-appropriate language, connection to children's immediate environment, and the ability to spark curiosity and interest [8,22].As these criteria represent fundamentally different aspects of child-appropriate content, and LLMs may have varying strengths in different aspects of content generation:</p>
<p>H3: There will be significant differences in nursery teachers' preferences for LLMs across the four evaluation criteria (comprehensibility, language, interest generation, and real-life relatedness).</p>
<p>Study Design</p>
<p>To investigate which LLM is most suitable for generating childappropriate scientific content, we focused on the natural sciences of biology, chemistry, and physics.We generated texts using different LLMs and had them rated by nursery teachers as experts in the field.</p>
<p>In the first step, we decided on four topics per discipline based on compendiums in biology [7], chemistry [6], and physics [11] (see tab. 1).Based on these topics, we extracted one text per topic (12 texts in total) from the compendium.</p>
<p>Afterward, we chose four LLMs based on popularity and user interface (see tab. 2).The selection criteria focused on easily accessible platforms, as our study aimed to simulate realistic usage scenarios for nursery teachers.Since most educators lack specialized training in prompt engineering, we wanted our findings to be applicable in real-world settings where teachers might use simple prompts with these LLMs.This approach ensures that our results The next phase of our research focused on prompt engineering.We conducted extensive testing with various prompts across the selected LLMs to determine the most effective approach.Through iterative refinement and comparison, we established a single standardized prompt: "transform the text for a 4-year-old child in 100 words max."This short prompt has been chosen to ensure consistency, minimize hallucinations (incorrect or misleading AI outputs), and to keep the length appropriate for young children's attention spans.Also we decided against longer prompts or few-shot prompting to mimic the realistic usage of LLMs by nursery teachers.</p>
<p>Using this optimized prompt, we proceeded to generate content with all four LLMs.For each of the twelve scientific topics, we generated texts in both English and German, resulting in 96 texts in total (12 topics×4 LLMs, each in two languages).The German versions were necessary for evaluation by German nursery teachers, while the English versions provide a valuable dataset for potential comparative studies in other countries and languages.</p>
<p>To establish a structured evaluation framework, we identified four key characteristics based on our literature review and pretesting, which included non-structured talks with nursery teachers: (1) child-appropriate language, (2) relation to children's everyday life, (3) comprehensibility for 4-year-olds, and (4) capacity to arouse interest.These characteristics served as our primary assessment criteria for analyzing the quality and appropriateness of the generated texts.All four criteria were rated on a 5-point Likert scale for each text.Additionally, the nursery teachers were asked to indicate their favorite text for each topic (one out of four).</p>
<p>The final evaluation phase for this late-breaking work involved 30 nursery teachers who served as expert evaluators.These professionals were asked to assess a total of 48 German texts, comprising the different versions generated for each original scientific text.The nursery teachers were recruited from 6 different urban early childhood education centers.All participants were professionally trained and certified according to national standards for early childhood education.Regarding technology familiarity, some teachers had limited experience with generative AI tools like ChatGPT for personal purposes (such as requesting recipes), but none had previously used AI technologies for professional educational purposes.</p>
<p>Their expertise in early childhood education provided valuable insights into the appropriateness and effectiveness of the generated content.Based on their comprehensive ratings, we were able to determine which LLM demonstrated the greatest proficiency in creating scientific content that was both accurate and accessible to four-year-old children.Additionally, we were able to identify the strengths and weaknesses of the different LLMs when it comes to specific aspects of generating child-appropriate content, like language or comprehensibility.</p>
<p>Results</p>
<p>Examples of LLM-Generated Content</p>
<p>To illustrate the differences between outputs from different LLMs, we present one example of generated content for the same scientific concept across the four models.</p>
<p>Example topic: Biology -Viruses [7].OpenAI GPT-4o: "A virus is a tiny thing, much smaller than a cell, made of just a few parts, like a coat (cover) and a small bit of genetic instructions (kind of like a recipe).Viruses need to go inside a living cell to grow because they can't do it by themselves.They "borrow" parts from the cell to make more viruses.Sometimes, this makes the cell sick or even causes it to break open.Scientists learned about viruses by studying plants that got sick, discovering that tiny, invisible things (viruses) were the cause."</p>
<p>Anthropic Claude Sonnet 3.5: "Sometimes tiny things called viruses can make us feel sick, just like when you get a cold.Imagine viruses are like tiny puzzle pieces that are so small you can't see them!They're smaller than even the tiniest ant.These little pieces can't do anything on their own -they need to find a home inside other living things, like people, animals, or plants, to grow and make more of themselves.It's a bit like how you need a house to live in.That's why we wash our hands and take medicine when weŕe sick -to keep these tiny visitors away!" Google Gemini 1.5 Flash: "Imagine tiny ninjas!These ninjas are so small you can't even see them with your eyes.They can only live by hiding inside other things, like our bodies.When they get inside, they use the body's stuff to make more tiny ninjas!These new ninjas can then make even more!Luckily, our bodies can usually fight them off and make us feel better again." Meta Llama 3.1 70B: "Germs Called Viruses.Do you know what's too small to see? Viruses!They're like tiny, tiny bugs that can make us sick.A virus has a special coat around it.It needs a host (like our body) to make more of itself.When it's inside us, it can make us feel icky.But don't worry!We can stay healthy by washing our hands.Doctors and scientists work hard to keep us safe from viruses.You're a superstar for learning about viruses!"</p>
<p>Hypothesis 1</p>
<p>The first hypothesis was that GPT-4 will be the highest-rated LLM among nursery teachers for generating child-appropriate scientific content.</p>
<p>Surprisingly, the GPT-4 model got the lowest ratings of all LLMs in this survey (see fig. 1).Claude was rated the best, followed by Gemini and Llama.A Friedman test revealed significant differences between the ratings of the four LLMs, 2 (df = 3, N = 30) = Y, p &lt; 0.001, with post hoc pairwise comparisons (Bonferroni-corrected) showing that all LLMs were rated significantly different (adjustet p &lt; 0.001).Only Llama and Gemini showed no significant differences (adjusted p = 0.08).Therefore, Hypothesis 1 was not supported, as GPT-4 received significantly lower ratings than the other LLMs for generating child-appropriate scientific content.</p>
<p>Hypothesis 2</p>
<p>The second hypothesis states that there will be significant differences in nursery teachers' ratings for LLM-generated content across scientific disciplines (biology, chemistry, and physics).As assumed, the results of the LLMs for scientific biological texts are rated best followed by physics and chemistry (see fig. 2).The ratings for each discipline were found to be significantly different from all others (Friedman test: 2 (df = 2, N = 30) = Y, p &lt; 0.001; all Bonferroni-corrected pairwise comparisons: p &lt; 0.001).Therefore, Hypothesis 2 was supported, with biological content receiving significantly higher ratings than physics and chemistry content.</p>
<p>Hypothesis 3</p>
<p>The third hypothesis suggests that there will be significant differences in nursery teachers' ratings for LLMs across the four evaluation criteria (comprehensibility, language, interest generation, and real-life relatedness).</p>
<p>The nursery teachers were most satisfied with the LLM's real-life relations of the texts followed by appropriate language, comprehensibility, and interest-sparking (see fig. 3).A Friedman test indicated significant variations in the ratings among the four criteria, 2 (df = 3, N = 30) = Y, p &lt; 0.001.Post hoc pairwise comparisons with Bonferroni correction revealed that not all criteria differ significantly from each other.Language differed significantly from comprehensibility and interest (adjusted p &lt; 0.001).From relation, language differs on a lower significance level (adjusted p &lt; 0.01).Interest and relation differ on an even lower significance level (adjusted p &lt; 0.05), whereas the other two groups (comprehensibility with relation as well as interest) do not show significant differences.Therefore, Hypothesis 3 was supported, with real-life relatedness and language receiving significantly higher ratings than interest generation, while comprehensibility showed mixed results.</p>
<p>Discussion</p>
<p>This study investigated the capabilities of four leading LLMs in generating child-appropriate scientific content for preschool education, revealing several unexpected findings that challenge existing assumptions about LLM performance in educational contexts.The results provide important insights for both educational technology development and ECSE.</p>
<p>Contrary to our first hypothesis, GPT-4 received the lowest ratings among the evaluated LLMs, with Claude achieving the highest scores followed by Gemini and Llama.This finding contradicts the dominant focus on GPT models in educational LLM research, where over 90% of studies examine OpenAI's GPT capabilities [9].This unexpected result highlights the importance of systematic comparative evaluations rather than relying on market prominence or general-purpose benchmarks when selecting LLMs for educational applications.Educators and developers should consider using Claude for scientific content generation.Our finding that GPT-4 received the lowest ratings must be understood within our specific research parameters.This result may be influenced by our concise prompting approach, the unique challenges of early childhood science education, and evaluation by nursery teachers with specialized developmental priorities.These factors limit broad generalization to other educational contexts.</p>
<p>The significant differences in performance across scientific disciplines support our second hypothesis and reveal important patterns in LLMs' capabilities.For LLMs, biological topics proved most amenable to child-appropriate translation, likely because they connect directly to children's experiences with their own bodies and observable living things.This stronger performance in biology aligns with previous findings that biological topics often inherit more concrete and observable phenomena compared to other natural sciences [12].Physical concepts, while somewhat abstract, could often be illustrated through tangible examples that children encounter in play (such as motion and simple machines), resulting in moderate ratings.The lower ratings for chemistry content reflect the inherent challenges in making molecular-level concepts accessible to preschoolers, as they frequently involve microscopic phenomena and abstract concepts that are difficult to translate into concrete experiences meaningful to young children.These findings can guide both content development strategies and curriculum planning in ECSE, suggesting that future AI development should prioritize connecting abstract concepts to observable phenomena within children's immediate experience.The varying performance across disciplines indicates the need for tailored approaches to content generation, with additional support and refinement needed for more abstract topics like chemistry.</p>
<p>Our analysis of evaluation criteria revealed nuanced patterns in LLM-generated content.While statistical differences emerged between criteria ratings, these may reflect challenges in operationalizing distinct aspects of child-appropriate content rather than meaningful variations in LLM capabilities.LLMs demonstrated stronger performance in real-life relatedness and language appropriateness, suggesting reasonable competency in creating foundational child-accessible content.However, the lower ratings for interest generation highlight a significant gap in creating truly engaging scientific content for young children, consistent with Bhandari and Brennan's [5] observations about limitations in AI-generated children's stories.These findings identify specific strengths to leverage and weaknesses to address when implementing LLMs in educational settings.The high ratings for real-life relations and ageappropriate language demonstrate successful implementation of key early childhood education principles [8], while suggesting that current LLM-generated content may need supplementation with additional engaging elements for effective learning experiences.</p>
<p>This study represents an important step in understanding the potential of LLMs in ECSE.While the results demonstrate promising capabilities in generating age-appropriate content, they also highlight significant areas for improvement, particularly in creating engaging material and handling abstract concepts.The findings suggest that LLMs can serve as valuable tools for educators.Our research recognizes that translating scientific concepts for young learners is a core professional competency of educators.Rather than suggesting this task exceeds educators' capabilities, we evaluated how AI systems might support science education by generating content that meets high pedagogical standards.As these models continue to develop, regular comparative evaluations will be crucial for identifying the most effective tools for educational content generation.</p>
<p>While we focused primarily on educator perspectives, parental involvement represents another crucial dimension in early childhood science education.Recent literature indicates parents hold mixed perspectives on AI in educational settings.They recognize potential benefits such as personalized learning experiences [16] but also express concerns about privacy and technology interference with parent-child interactions [4,10].Future research should explore how LLM-generated content could support science learning activities at home, creating connections between formal educational settings and home environments.</p>
<p>In our planned follow-up study, we will extend our quantitative evaluation study to include more teachers and also parents.For a mixed methods approach, we also plan to conduct qualitative interviews with both teachers and parents to gain nuances of effective science content for preschool children.This expansion addresses initial informal feedback from nursery teachers indicating that none of the generated texts fully met child-appropriateness standards.By addressing these considerations in future research, we can develop more refined understanding of how LLMs can best serve as tools to support early childhood science education while respecting the essential roles of both educators and parents in children's learning journeys.</p>
<p>Figure 1 :
1
Figure 1: Aggregated overall ranking of the LLMs.</p>
<p>Figure 2 :
2
Figure 2: Aggregated overall rating by discipline.</p>
<p>Figure 3 :
3
Figure 3: Aggregated overall rating by criteria.</p>
<p>existing literature provides valuable insights into both ECSE and LLM capabilities, there is a notable gap in research specifically examining LLMs' effectiveness in generating scientific content for young children.Previous studies have either focused on content generation for older age groups or examined ECSE without considering AI-generated content.Our study aims to bridge this gap by systematically evaluating different LLMs' capabilities in generating age-appropriate scientific content for four-year-olds.</p>
<p>Table 1 :
1
Chosen topics by discipline
BiologyPhysicsChemistryDigestionMotionChemical reactionsVirusesEnergyIonic bondingPhotosynthesis Electricity Periodic tableHuman brainAtomsCarbonremain relevant for practitioners without technical expertise in AIinteraction.</p>
<p>Table 2 :
2
Used Large Language Models
Company LLMVersionOpenAIGPT4oAnthropic Claude 3.5 SonnetGoogleGemini 1.5 FlashMetaLlama3.1</p>
<p>Analysis of LLMs for educational question classification and generation. Al Said, Ade Faraby, Adiwijaya Romadhony, 10.1016/j.caeai.2024.100298Computers and Education: Artificial Intelligence. 71002982024. 2024</p>
<p>Fantine Huot, and Mirella Lapata. 28.1. Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts. Sumit Asthana, Hannah Rashkin, Elizabeth Clark, </p>
<p>Predicting grade school scientific literacy from aspects of the early home science environment. Jihye Bae, Margaret Shavlik, Christine E Shatrowsky, Catherine A Haden, Amy E Booth, 10.3389/fpsyg.2023.1113196Frontiers in psychology. 1411131962023. 2023</p>
<p>Parenting in the Age of Artificial Intelligence. Ruqia Safdar Bajwa, Asma Yunus, Hina Saeed, Asia Zulfqar, 10.4018/979-8-3693-3350-1.ch003Exploring Youth Studies in the Age. D B A Ai, Zeinab Mehdi Khosrow-Pour, Rusli Zaremohzzabieh, Seyedali Abdullah, Ahrari, IGI Global2024</p>
<p>26.0. Trustworthiness of Children Stories Generated by Large Language Models. Prabin Bhandari, Hannah Marie Brennan, </p>
<p>Theodore L Brown, Harold Eugene Lemay, Bruce Edward Bursten, Catherine J Murphy, Chemistry: The central science. Pearson, Harlow201814th edition in si units, global edition ed.</p>
<p>. Neil A Campbell, Lisa A Urry, Michael L Cain, Steven A Wasserman, Peter V Minorsky, Jane B Reece, Biology: A global approach. 2021global edition (twelfth edition, global edition ed.</p>
<p>Development of Principle of Suitability for Children Scale. Bilge Nur, Doğan , Birkan Güldenoğlu, 10.1016/j.sbspro.2014.01.706Procedia -Social and Behavioral Sciences. 1162014. 2014</p>
<p>Large Language Models in Education: A Systematic Review. Bingyu Dong, Jie Bai, Tao Xu, Yun Zhou, 10.1109/CSTE62025.2024.000312024 6th International Conference on Computer Science and Technologies in Education (CSTE). IEEE2024</p>
<p>Parents' Perspectives on Using Artificial Intelligence to Reduce Technology Interference During Early Childhood: Crosssectional Online Survey. Jill Glassman, Kathryn Humphreys, Serena Yeung, Michelle Smith, Adam Jauregui, Arnold Milstein, Lee Sanders, 10.2196/19461Journal of medical Internet research. 23e194612021. 2021</p>
<p>David Halliday, Robert Resnick, Jearl Walker, Fundamentals of physics. Wiley201310th ed. ed.</p>
<p>Two approaches to teaching young children science concepts, vocabulary, and scientific problem-solving skills. Soo-Young Hong, Karen E Diamond, 10.1016/j.ecresq.2011.09.006Early Childhood Research Quarterly. 2722012. 2012</p>
<p>Generating Educational Materials with Different Levels of Readability using LLMs. Chieh-Yang Huang, Jing Wei, Ting-Hao Kenneth Huang, 10.1145/3690712.3690718Proceedings of the Third Workshop on Intelligent and Interactive Writing Assistants. the Third Workshop on Intelligent and Interactive Writing AssistantsNew York, NY, USAACM2024</p>
<p>The development of early scientific literacy gaps in kindergarten children. Jana Kähler, Inga Hahn, Olaf Köller, 10.1080/09500693.2020.1808908International Journal of Science Education. 422020. 2020</p>
<p>Large language models (LLMs): survey, technical frameworks, and future challenges. Pranjal Kumar, 10.1007/s10462-024-10888-yArtificial Intelligence Review. 572024. 2024</p>
<p>Chatting with the Future: A Comprehensive Exploration of Parents' Perspectives on Conversational AI Implementation in Children's Education. Pauldy Cornelia, Johanna Otermans, Stephanie Baines, Chelsea Livingstone, 10.46328/ijte.812International Journal of Technology in Education. 72024. 2024Monica Pereira, and Dev Aditya</p>
<p>Research Trends and Development Perspectives in Early Childhood Science Education: An Overview. Konstantinos Ravanis, 10.3390/educsci12070456Education Sciences. 1274562022. 2022</p>
<p>Science Education in Kindergarten: Sociocognitive perspective. Konstantinos Ravanis, George Bagakis, 10.1080/0966976980060306International Journal of Early Years Education. 61998. 1998</p>
<p>Guiding explanation construction by children at the entry points of learning progressions. Nancy Butler, Songer , Amelia Wenk Gotwals, 10.1002/tea.20454Journal of Research in Science Teaching. 492012. 2012</p>
<p>Katharina von der Wense, and Eliana Colunga. Jennifer Weber, Maria Valentini, Téa Wright, Evaluating LLMs as Tools to Support Early Vocabulary Learning. Proceedings of the Annual Meeting of the Cognitive Science Society. 2024. 2024462633</p>
<p>Enhancing the Comprehension: Text Simplification Approaches and the Role of Large Language Models. Ziyu Yang, 10.34944/DSPACE/102082024Philadelphia, USATemple UniversityPh. D. Dissertation</p>
<p>Die Festlegung von qualitativen Entscheidungskriterien mit GABEK am Beispiel von Kinderliteratur. Barbara Zelger, Simone Stefan, 10.1007/978-3-658-32463-6{_}13Hallesche Schriften zur Betriebswirtschaft. Margit Raich, Julia Müller-Seeger, Helmut Ebert, WiesbadenSpringer Fachmedien Wiesbaden2020. 201935Symposium Qualitative Sozialforschung</p>            </div>
        </div>

    </div>
</body>
</html>
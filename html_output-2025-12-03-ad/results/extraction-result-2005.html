<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2005 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2005</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2005</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-47.html">extraction-schema-47</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <p><strong>Paper ID:</strong> paper-280148188</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.07102v1.pdf" target="_blank">Does Data Scaling Lead to Visual Compositional Generalization?</a></p>
                <p><strong>Paper Abstract:</strong> Compositional understanding is crucial for human intelligence, yet it remains unclear whether contemporary vision models exhibit it. The dominant machine learning paradigm is built on the premise that scaling data and model sizes will improve out-of-distribution performance, including compositional generalization. We test this premise through controlled experiments that systematically vary data scale, concept diversity, and combination coverage. We find that compositional generalization is driven by data diversity, not mere data scale. Increased combinatorial coverage forces models to discover a linearly factored representational structure, where concepts decompose into additive components. We prove this structure is key to efficiency, enabling perfect generalization from few observed combinations. Evaluating pretrained models (DINO, CLIP), we find above-random yet imperfect performance, suggesting partial presence of this structure. Our work motivates stronger emphasis on constructing diverse datasets for compositional generalization, and considering the importance of representational structure that enables efficient compositional learning. Code available at https://github.com/oshapio/visual-compositional-generalization.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2005.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2005.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>From-scratch (n,k) experiments</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Controlled from-scratch compositional generalization experiments using the (n, k)-framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic training of ResNet-50 from scratch on synthetic visual datasets varying number of concept values (n) and seen combinations per value (k) to measure zero-shot generalization to unseen concept pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>visual compositional classification (two-concept discriminative tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>ResNet-50 trained from scratch (shared backbone, two linear heads)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>no explicit curriculum; end-to-end joint training on labeled concept-pair images with controlled (n,k) selection of seen combinations</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>two concepts only (pairwise combinations)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Systematically varied: n (number of values per concept) and k (number of seen combinations per value). Settings reported include n=3 with k=1 or k=2, sweeps across k/n percentages (0–100%) to control fraction of combinations observed.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Near 100% in-distribution (ID) accuracy across datasets (reported as 'near 100%')</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Large drops on unseen combinations (OOD): typical accuracy reductions of 60–80% relative to ID; example: MNIST digit OOD accuracy dropped by ~78% in a basic n=3,k=2 setting</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Typical gap: 60–80% absolute drop in accuracy from ID to OOD; some concepts exhibit much smaller drops (3–17%)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Compared diversity (varying n,k) vs. merely increasing amount of ID data: increased combinatorial diversity (higher n or k) improves OOD generalization while increasing quantity of same-ID-data does not (see separate dataset-size scaling experiment)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td>With limited diversity, models learn spurious/shortcut features leading to random-level zero-shot performance; some concept-specific shortcuts persisted.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>From-scratch models achieve near-perfect ID accuracy but fail to generalize to unseen concept pairs unless trained on high combinatorial diversity; increasing observed combinations (k) or the number of values (n) improves zero-shot OOD accuracy substantially, while simply increasing sample count at fixed combinatorial coverage does not.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - challenges the notion that raw data scaling alone yields compositional generalization; supports the theory that combinatorial diversity (not quantity) is critical for compositional generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2005.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2005.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dataset-size scaling test</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effect of increasing in-distribution dataset size (same combinatorial coverage) on compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments that increase the number of training samples (up to 4x or higher depending on dataset) while keeping the same set of observed concept combinations to test whether more ID data alone improves OOD compositional generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>visual compositional classification</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>ResNet-50 trained from scratch</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>no curriculum; same-combination enlargement (more samples per seen pair), i.e., no increase in combinatorial diversity</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>two concepts only</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Combinatorial coverage held constant; sample counts increased (examples: 7.5k, 15k, 30k for Shapes3D/CMNIST; up to 120k for DSPRITES/FSPRITES)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>ID accuracy remained near 100%</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Little to no improvement; OOD accuracy still exhibited large drops (60–80%) despite up to 4× more ID samples</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Gap persisted at roughly 60–80% absolute drop in accuracy; increasing training samples within same combinatorial coverage did not close the gap</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Baseline comparison is effectively 'more samples at same coverage' vs 'more combinatorial diversity'; more samples provided limited benefit compared to increased combinatorial diversity</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td>Persisting spurious/shortcut learning despite more samples when combinatorial diversity is low</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Scaling the number of training samples without increasing combination diversity does not meaningfully improve compositional generalization — large ID→OOD gaps remain even with 4× more data.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>challenges - contradicts a naive 'scale-only' theory that more in-distribution data alone yields compositional generalization; supports emphasis on structured combinatorial diversity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2005.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2005.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Three-phase feature learning</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-phase emergence of feature structure with increasing combinatorial diversity</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical analysis showing that as the fraction of observed combinations increases, learned features progress through three phases: spurious/entangled, discriminative but non-linearly-factored, and finally linearly factored (enabling strong OOD generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>visual representation learning / compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>ResNet-50 (from-scratch) features analyzed via probes and metrics (R^2, cosine similarity, PCA)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>implicit phased progression as diversity increases (0–10% limited → 25–75% moderate → 75–100% high); not an explicit curriculum but staged phenomena induced by data diversity</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>two concepts only</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Measured as percentage of training combinations k/n; analysis uses balanced evaluation data covering all combinations (100 samples per combination) to compute linearity and orthogonality metrics</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Feature decodability (linear probes) reaches ~100% by moderate diversity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Zero-shot OOD accuracy: near random at low diversity (0–10%), ~60–80% at moderate diversity (25–75%), and >90% at high diversity (75–100%) on most datasets</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Implicit in phase descriptions: near-random OOD at low diversity vs >90% at high diversity; ID accuracy remains high throughout, so gap narrows with diversity (quantified qualitatively above)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Not a curriculum experiment per se, but shows staged improvement as training combination diversity increases; no baseline curriculum compared</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td>Explicitly observed: limited diversity leads to spurious/shortcut features (decoded accuracy <80% and random-level zero-shot performance)</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>As combinatorial diversity increases, representations move from entangled/spurious to decodable but non-linear, and finally to strongly linear and orthogonal structure (R^2 > 0.8, cosine similarity < 0.1) that enables high (>90%) zero-shot generalization; discriminability alone is insufficient — linear factorization matters.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports/nuances - supports theories that structured (linearly-factored) representations enable efficient compositional generalization and nuances that discriminability alone is insufficient.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2005.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2005.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Minimal compositional learning (Proposition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proposition that k = 2 combinations per concept value suffice for perfect generalization under linear factorization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theoretical result and constructive proof (Proposition 4.1 / B.7) showing that if feature representations are linearly factored and joint spans satisfy mild rank conditions, observing two combinations per concept value is sufficient to recover factors and achieve perfect generalization to all unseen combinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>theoretical analysis of representation geometry for compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>abstract feature extractor f : X → R^d with linearly factored embeddings (no specific neural architecture required)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>theoretical minimal exposure pattern: observe a designed set of 2n combinations (e.g., diagonal plus cyclic neighbors) — this is a minimal, structured exposure, not a training curriculum</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>two concepts in theory (general formulation states C = C1 × ... × Cc but proof and minimality shown for two observed factors)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Minimal combinatorial pattern: k = 2 combinations per concept value arranged so system of linear equations has full rank (diagonal and one cyclic neighbor per value); assumes balanced samples per observed combination</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Theoretical guarantee: recover factored embeddings from observed combinations (exact under assumptions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Theoretical guarantee: perfect generalization to all unseen combinations if linear factorization and rank assumptions hold</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Under ideal linear factorization and mild linear-independence assumptions, only two observed combinations per concept value (k=2) suffice to identify factor vectors and build linear classifiers that perfectly generalize to all unseen pairings — highlighting the efficiency benefits of linearly factored representations.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - provides formal theoretical support that linearly factored representational geometry can make compositional generalization data-efficient and minimal-exposure possible.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2005.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2005.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretrained model probing</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evaluation of large pre-trained vision models (ResNet50-IN1K, DINOv1, DINOv2 ViT-L, CLIP ViT-L) for compositional structure and generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Probing (linear and small MLP probes) and linear-factorization recovery tests to assess whether pretrained models exhibit linearly factored concept structure and how well they generalize to unseen concept pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>evaluation of pretrained vision/foundational models on compositional visual classification</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>ResNet50-ImageNet1K, ResNet50-DINOv1, DINOv2-ViT-L/14, CLIP-ViT-L/14 (frozen features with probes)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>no curriculum; probing downstream classifiers trained on selected (n,k) seen combinations</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>two concepts only (pairwise)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Probes trained with varying k levels (number of seen combinations) while holding n at n_max; analyses aggregated and normalized across datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td>Pretrained models outperform from-scratch ResNet50 on seen combinations (higher probe accuracy); ID accuracy often high (exact per-model values vary by dataset and concept)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Pretrained models show above-random but imperfect OOD performance; some concept-model pairs exceed 90% (e.g., WORLD-NAME in PUG-ANIMAL, and CLIP/DINOv2 achieving >90% on some color/orientation/digit concepts), but none reach perfect theoretical generalization</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td>Gap exists but reduced compared to from-scratch: pretrained models show improved OOD accuracy as training diversity increases, yet still require diverse downstream training to generalize reliably (no single uniform numeric gap reported across all settings)</td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Compared probes (linear vs MLP) — non-linear probes often improve transfer, indicating some compositional information is non-linear in pretrained features; pretraining helps versus from-scratch, but does not replace need for downstream diversity</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td>Pretrained features sometimes specialize (e.g., CLIP better at color tasks, DINOV2 better at shape tasks), suggesting pretraining biases that affect which concept axes are more linearly represented</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Large pretrained models encode partial linear compositional structure: they outperform from-scratch models on OOD probes and can achieve >90% on certain concepts, but they do not exhibit the ideal linear factorization required for perfect generalization — diverse downstream combination exposure is still necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - supports the idea that pretraining can induce partial compositional structure but challenges claims that pretraining/scale alone suffices for robust compositional generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Teaching Compositionality to CNNs <em>(Rating: 2)</em></li>
                <li>Linear Spaces of Meanings: Compositional Structures in Vision-Language Models <em>(Rating: 2)</em></li>
                <li>Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks <em>(Rating: 2)</em></li>
                <li>Can Models Learn Skill Composition from Examples? <em>(Rating: 2)</em></li>
                <li>Learning to Compose Soft Prompts for Compositional Zero-Shot Learning <em>(Rating: 1)</em></li>
                <li>When does compositional structure yield compositional generalization? a kernel theory <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2005",
    "paper_id": "paper-280148188",
    "extraction_schema_id": "extraction-schema-47",
    "extracted_data": [
        {
            "name_short": "From-scratch (n,k) experiments",
            "name_full": "Controlled from-scratch compositional generalization experiments using the (n, k)-framework",
            "brief_description": "Systematic training of ResNet-50 from scratch on synthetic visual datasets varying number of concept values (n) and seen combinations per value (k) to measure zero-shot generalization to unseen concept pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "visual compositional classification (two-concept discriminative tasks)",
            "agent_or_model_name": "ResNet-50 trained from scratch (shared backbone, two linear heads)",
            "curriculum_structure": "no explicit curriculum; end-to-end joint training on labeled concept-pair images with controlled (n,k) selection of seen combinations",
            "primitive_training_details": null,
            "composition_depth_range": "two concepts only (pairwise combinations)",
            "compositional_diversity_description": "Systematically varied: n (number of values per concept) and k (number of seen combinations per value). Settings reported include n=3 with k=1 or k=2, sweeps across k/n percentages (0–100%) to control fraction of combinations observed.",
            "performance_trained_compositions": "Near 100% in-distribution (ID) accuracy across datasets (reported as 'near 100%')",
            "performance_novel_compositions": "Large drops on unseen combinations (OOD): typical accuracy reductions of 60–80% relative to ID; example: MNIST digit OOD accuracy dropped by ~78% in a basic n=3,k=2 setting",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Typical gap: 60–80% absolute drop in accuracy from ID to OOD; some concepts exhibit much smaller drops (3–17%)",
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": "Compared diversity (varying n,k) vs. merely increasing amount of ID data: increased combinatorial diversity (higher n or k) improves OOD generalization while increasing quantity of same-ID-data does not (see separate dataset-size scaling experiment)",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": "With limited diversity, models learn spurious/shortcut features leading to random-level zero-shot performance; some concept-specific shortcuts persisted.",
            "negative_transfer_observed": null,
            "key_findings_summary": "From-scratch models achieve near-perfect ID accuracy but fail to generalize to unseen concept pairs unless trained on high combinatorial diversity; increasing observed combinations (k) or the number of values (n) improves zero-shot OOD accuracy substantially, while simply increasing sample count at fixed combinatorial coverage does not.",
            "supports_or_challenges_theory": "nuances - challenges the notion that raw data scaling alone yields compositional generalization; supports the theory that combinatorial diversity (not quantity) is critical for compositional generalization.",
            "uuid": "e2005.0"
        },
        {
            "name_short": "Dataset-size scaling test",
            "name_full": "Effect of increasing in-distribution dataset size (same combinatorial coverage) on compositional generalization",
            "brief_description": "Experiments that increase the number of training samples (up to 4x or higher depending on dataset) while keeping the same set of observed concept combinations to test whether more ID data alone improves OOD compositional generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "visual compositional classification",
            "agent_or_model_name": "ResNet-50 trained from scratch",
            "curriculum_structure": "no curriculum; same-combination enlargement (more samples per seen pair), i.e., no increase in combinatorial diversity",
            "primitive_training_details": null,
            "composition_depth_range": "two concepts only",
            "compositional_diversity_description": "Combinatorial coverage held constant; sample counts increased (examples: 7.5k, 15k, 30k for Shapes3D/CMNIST; up to 120k for DSPRITES/FSPRITES)",
            "performance_trained_compositions": "ID accuracy remained near 100%",
            "performance_novel_compositions": "Little to no improvement; OOD accuracy still exhibited large drops (60–80%) despite up to 4× more ID samples",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Gap persisted at roughly 60–80% absolute drop in accuracy; increasing training samples within same combinatorial coverage did not close the gap",
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": "Baseline comparison is effectively 'more samples at same coverage' vs 'more combinatorial diversity'; more samples provided limited benefit compared to increased combinatorial diversity",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": "Persisting spurious/shortcut learning despite more samples when combinatorial diversity is low",
            "negative_transfer_observed": null,
            "key_findings_summary": "Scaling the number of training samples without increasing combination diversity does not meaningfully improve compositional generalization — large ID→OOD gaps remain even with 4× more data.",
            "supports_or_challenges_theory": "challenges - contradicts a naive 'scale-only' theory that more in-distribution data alone yields compositional generalization; supports emphasis on structured combinatorial diversity.",
            "uuid": "e2005.1"
        },
        {
            "name_short": "Three-phase feature learning",
            "name_full": "Three-phase emergence of feature structure with increasing combinatorial diversity",
            "brief_description": "Empirical analysis showing that as the fraction of observed combinations increases, learned features progress through three phases: spurious/entangled, discriminative but non-linearly-factored, and finally linearly factored (enabling strong OOD generalization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "visual representation learning / compositional generalization",
            "agent_or_model_name": "ResNet-50 (from-scratch) features analyzed via probes and metrics (R^2, cosine similarity, PCA)",
            "curriculum_structure": "implicit phased progression as diversity increases (0–10% limited → 25–75% moderate → 75–100% high); not an explicit curriculum but staged phenomena induced by data diversity",
            "primitive_training_details": null,
            "composition_depth_range": "two concepts only",
            "compositional_diversity_description": "Measured as percentage of training combinations k/n; analysis uses balanced evaluation data covering all combinations (100 samples per combination) to compute linearity and orthogonality metrics",
            "performance_trained_compositions": "Feature decodability (linear probes) reaches ~100% by moderate diversity",
            "performance_novel_compositions": "Zero-shot OOD accuracy: near random at low diversity (0–10%), ~60–80% at moderate diversity (25–75%), and &gt;90% at high diversity (75–100%) on most datasets",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Implicit in phase descriptions: near-random OOD at low diversity vs &gt;90% at high diversity; ID accuracy remains high throughout, so gap narrows with diversity (quantified qualitatively above)",
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": "Not a curriculum experiment per se, but shows staged improvement as training combination diversity increases; no baseline curriculum compared",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": "Explicitly observed: limited diversity leads to spurious/shortcut features (decoded accuracy &lt;80% and random-level zero-shot performance)",
            "negative_transfer_observed": null,
            "key_findings_summary": "As combinatorial diversity increases, representations move from entangled/spurious to decodable but non-linear, and finally to strongly linear and orthogonal structure (R^2 &gt; 0.8, cosine similarity &lt; 0.1) that enables high (&gt;90%) zero-shot generalization; discriminability alone is insufficient — linear factorization matters.",
            "supports_or_challenges_theory": "supports/nuances - supports theories that structured (linearly-factored) representations enable efficient compositional generalization and nuances that discriminability alone is insufficient.",
            "uuid": "e2005.2"
        },
        {
            "name_short": "Minimal compositional learning (Proposition)",
            "name_full": "Proposition that k = 2 combinations per concept value suffice for perfect generalization under linear factorization",
            "brief_description": "A theoretical result and constructive proof (Proposition 4.1 / B.7) showing that if feature representations are linearly factored and joint spans satisfy mild rank conditions, observing two combinations per concept value is sufficient to recover factors and achieve perfect generalization to all unseen combinations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "theoretical analysis of representation geometry for compositional generalization",
            "agent_or_model_name": "abstract feature extractor f : X → R^d with linearly factored embeddings (no specific neural architecture required)",
            "curriculum_structure": "theoretical minimal exposure pattern: observe a designed set of 2n combinations (e.g., diagonal plus cyclic neighbors) — this is a minimal, structured exposure, not a training curriculum",
            "primitive_training_details": null,
            "composition_depth_range": "two concepts in theory (general formulation states C = C1 × ... × Cc but proof and minimality shown for two observed factors)",
            "compositional_diversity_description": "Minimal combinatorial pattern: k = 2 combinations per concept value arranged so system of linear equations has full rank (diagonal and one cyclic neighbor per value); assumes balanced samples per observed combination",
            "performance_trained_compositions": "Theoretical guarantee: recover factored embeddings from observed combinations (exact under assumptions)",
            "performance_novel_compositions": "Theoretical guarantee: perfect generalization to all unseen combinations if linear factorization and rank assumptions hold",
            "generalization_gap_measured": null,
            "generalization_gap_value": null,
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": null,
            "adaptive_curriculum_used": null,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Under ideal linear factorization and mild linear-independence assumptions, only two observed combinations per concept value (k=2) suffice to identify factor vectors and build linear classifiers that perfectly generalize to all unseen pairings — highlighting the efficiency benefits of linearly factored representations.",
            "supports_or_challenges_theory": "supports - provides formal theoretical support that linearly factored representational geometry can make compositional generalization data-efficient and minimal-exposure possible.",
            "uuid": "e2005.3"
        },
        {
            "name_short": "Pretrained model probing",
            "name_full": "Evaluation of large pre-trained vision models (ResNet50-IN1K, DINOv1, DINOv2 ViT-L, CLIP ViT-L) for compositional structure and generalization",
            "brief_description": "Probing (linear and small MLP probes) and linear-factorization recovery tests to assess whether pretrained models exhibit linearly factored concept structure and how well they generalize to unseen concept pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "evaluation of pretrained vision/foundational models on compositional visual classification",
            "agent_or_model_name": "ResNet50-ImageNet1K, ResNet50-DINOv1, DINOv2-ViT-L/14, CLIP-ViT-L/14 (frozen features with probes)",
            "curriculum_structure": "no curriculum; probing downstream classifiers trained on selected (n,k) seen combinations",
            "primitive_training_details": null,
            "composition_depth_range": "two concepts only (pairwise)",
            "compositional_diversity_description": "Probes trained with varying k levels (number of seen combinations) while holding n at n_max; analyses aggregated and normalized across datasets",
            "performance_trained_compositions": "Pretrained models outperform from-scratch ResNet50 on seen combinations (higher probe accuracy); ID accuracy often high (exact per-model values vary by dataset and concept)",
            "performance_novel_compositions": "Pretrained models show above-random but imperfect OOD performance; some concept-model pairs exceed 90% (e.g., WORLD-NAME in PUG-ANIMAL, and CLIP/DINOv2 achieving &gt;90% on some color/orientation/digit concepts), but none reach perfect theoretical generalization",
            "generalization_gap_measured": true,
            "generalization_gap_value": "Gap exists but reduced compared to from-scratch: pretrained models show improved OOD accuracy as training diversity increases, yet still require diverse downstream training to generalize reliably (no single uniform numeric gap reported across all settings)",
            "composition_depth_scaling": null,
            "curriculum_vs_baseline_comparison": "Compared probes (linear vs MLP) — non-linear probes often improve transfer, indicating some compositional information is non-linear in pretrained features; pretraining helps versus from-scratch, but does not replace need for downstream diversity",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": "Pretrained features sometimes specialize (e.g., CLIP better at color tasks, DINOV2 better at shape tasks), suggesting pretraining biases that affect which concept axes are more linearly represented",
            "negative_transfer_observed": null,
            "key_findings_summary": "Large pretrained models encode partial linear compositional structure: they outperform from-scratch models on OOD probes and can achieve &gt;90% on certain concepts, but they do not exhibit the ideal linear factorization required for perfect generalization — diverse downstream combination exposure is still necessary.",
            "supports_or_challenges_theory": "nuances - supports the idea that pretraining can induce partial compositional structure but challenges claims that pretraining/scale alone suffices for robust compositional generalization.",
            "uuid": "e2005.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Teaching Compositionality to CNNs",
            "rating": 2
        },
        {
            "paper_title": "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models",
            "rating": 2
        },
        {
            "paper_title": "Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks",
            "rating": 2
        },
        {
            "paper_title": "Can Models Learn Skill Composition from Examples?",
            "rating": 2
        },
        {
            "paper_title": "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning",
            "rating": 1
        },
        {
            "paper_title": "When does compositional structure yield compositional generalization? a kernel theory",
            "rating": 1
        }
    ],
    "cost": 0.01373925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Does Data Scaling Lead to Visual Compositional Generalization?
9 Jul 2025</p>
<p>Arnas Uselis 
Andrea Dittadi 
Seong Joon Oh 
Does Data Scaling Lead to Visual Compositional Generalization?
9 Jul 202566BBEBBB92E13F959E64A5353F62655DarXiv:2507.07102v1[cs.LG]
Compositional understanding is crucial for human intelligence, yet it remains unclear whether contemporary vision models exhibit it.The dominant machine learning paradigm is built on the premise that scaling data and model sizes will improve out-of-distribution performance, including compositional generalization.We test this premise through controlled experiments that systematically vary data scale, concept diversity, and combination coverage.We find that compositional generalization is driven by data diversity, not mere data scale.Increased combinatorial coverage forces models to discover a linearly factored representational structure, where concepts decompose into additive components.We prove this structure is key to efficiency, enabling perfect generalization from few observed combinations.Evaluating pretrained models (DINO, CLIP), we find above-random yet imperfect performance, suggesting partial presence of this structure.Our work motivates stronger emphasis on constructing diverse datasets for compositional generalization, and considering the importance of representational structure that enables efficient compositional learning.github.com/oshapio/visual-compositional-generalization</p>
<p>Introduction</p>
<p>Compositional understanding is the ability to comprehend novel, complex scenarios by systematically combining simpler, known conceptual building blocks.It is widely regarded as a cornerstone of human intelligence.The Language of Thought hypothesis suggests that cognition arises from fundamental components and structured recombination 1 Tübingen AI Center, University of Tübingen 2 Helmholtz AI 3 Technical University of Munich 4 Munich Center for Machine Learning (MCML) 5 Max Planck Institute for Intelligent Systems, Tübingen.Correspondence to: Arnas Uselis <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#97;&#114;&#110;&#97;&#115;&#46;&#117;&#115;&#101;&#108;&#105;&#115;&#64;&#117;&#110;&#105;&#116;&#117;&#101;&#98;&#105;&#110;&#103;&#101;&#110;&#46;&#100;&#101;">&#97;&#114;&#110;&#97;&#115;&#46;&#117;&#115;&#101;&#108;&#105;&#115;&#64;&#117;&#110;&#105;&#116;&#117;&#101;&#98;&#105;&#110;&#103;&#101;&#110;&#46;&#100;&#101;</a>.</p>
<p>Proceedings of the 42 nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025.Copyright 2025 by the author(s).Left: An indicator matrix of noun-adjective co-occurrences in LAION-400M shows significant sparsity in concept combinations; the majority of cells are unobserved (zoomed-in view), demonstrating that even common concepts rarely combine in the dataset.This sparsity biases models toward memorizing frequent combinations rather than learning compositional structure.Right: A concrete example of a 4x4 matrix of nouns (seat, apple, bed, cheese) and attributes (tiny, liquid, melted, electric).This work investigates how vision models develop compositional attribute-object understanding in simplified and controlled settings.</p>
<p>rules (Fodor &amp; Fodor, 1975), and neuroscience findings reinforce this perspective (Dehaene et al., 2022).This human proficiency sets a high bar for vision models that must understand how visual attributes and objects combine in novel ways.However, recent studies reveal significant limitations in the compositional abilities of state-of-the-art vision and vision-language models (Rahmanzadehgervi et al., 2024;Tong et al., 2024;Du &amp; Kaelbling, 2024;Yuksekgonul et al., 2023;Zeng et al., 2023), raising fundamental questions about whether and when vision models can achieve this capability.</p>
<p>The dominant paradigm in machine learning relies on scaling data and model size to improve model capabilities, with the expectation that this approach will extend to compositional understanding.This paradigm, grounded in scaling laws (Kaplan et al., 2020;Hoffmann et al., 2022;Hestness et al., 2017) and demonstrated by the success of large language models (Brown et al., 2020;Touvron et al., 2023) and large-scale vision models (Radford et al., 2021;Dosovitskiy et al., 2021), has driven the creation of massive datasets like LAION-400M (Schuhmann et al., 2021).However, as illustrated in Figure 1, even LAION-400M exhibits critical sparsity in compositional coverage: many plausible attributeobject combinations are rarely or never observed (e.g."tiny seat" or "melted apple").This sparsity reflects a combinatorial explosion: with visual attributes (color, shape, texture) that can combine in vast numbers of ways, most possible combinations will remain underrepresented regardless of dataset size.</p>
<p>This motivates our central research question:</p>
<p>"Do vision models generalize compositionally, and if so, under what conditions?"</p>
<p>Our approach prioritizes controllability to understand when and how vision models can achieve compositional generalization.We first train models from scratch on carefully designed datasets to isolate the causal effects of data properties on compositional generalization.This allows us to observe both how generalization performance and representational structure emerge under different data conditions.</p>
<p>We then validate whether large-scale pretrained vision models exhibit similar structure and examine how this relates to standard linear probing techniques.</p>
<p>Through this controlled approach, we make five contributions:</p>
<p>(1) Controlled experimental framework ( §3): We develop a framework (referred to as (n, k)-framework) to systematically study how data scaling impacts compositional generalization, varying key factors including training data scale, concept diversity, and combination exposure while focusing on single-object cases to isolate core compositional abilities.</p>
<p>(2) Data diversity over scale ( §4.1):We demonstrate that compositional generalization depends critically on data diversity rather than scale: simply increasing in-distribution training data fails to improve generalization, while increasing diversity of data through more concept values and their combinations enhances performance.</p>
<p>(3) Three-phase feature learning ( §4.2):We show that models exhibit three phases of feature learning: (i) spurious features with limited diversity, (ii) discriminative but nonlinearly-factored features at moderate diversity, and (iii) linearly factored representations only under high diversity.</p>
<p>(4) Theoretical efficiency of linearly factored structure ( §4.3):We prove that when representations exhibit linearly factored structure, observing just two combinations per concept value is sufficient for perfect generalization to all unseen combinations.</p>
<p>(5) Evaluation of pretrained large-scale models ( §5): We evaluate whether large-scale pretrained models (like DINO and CLIP) exhibit the linearly factored structure identified in our controlled experiments, finding they achieve above-random yet imperfect compositional performance.</p>
<p>Our experiments reveal a clear principle: compositional generalization is driven by data diversity, not mere data scale.Increased combinatorial coverage forces models to discover a linearly factored representational structure, where concepts decompose into additive components.We prove this structure is not just an artifact but a key to efficiency, enabling perfect generalization from just two examples per concept.</p>
<p>Related Work</p>
<p>Compositionality, simplicity bias, and generalization.</p>
<p>Compositional understanding-the ability to combine known building blocks into novel representations-is a cornerstone of human intelligence (Fodor &amp; Fodor, 1975;Dehaene et al., 2022).A central question in machine learning is whether neural networks can achieve this systematic generalization.While formalisms for compositionality have been proposed through complexity-based theories (Elmoznino et al., 2024), structural analyses (Lepori et al., 2023), and risk minimization frameworks (Mahajan et al., 2024), models often exhibit a simplicity bias (Valle-Pérez et al., 2018;Ren &amp; Sutherland, 2024).They favor simple, spuriously correlated features over more complex, robust ones (Geirhos et al., 2020), a challenge that causal and concept-based representation learning aims to address (Rajendran et al., 2024).This bias is especially pronounced when some concept combinations are underrepresented, or come from a different domain (Jeong et al., 2025).Our work provides a systematic, empirical investigation into the specific data conditions that compel models to overcome this bias and learn a generalizable, compositional latent structure.</p>
<p>Role of data and scaling.The structure of training data is known to be critical for generalization (Madan et al., 2021).</p>
<p>Prior work has shown that training on compositionally structured data improves performance (Stone et al., 2017), and that augmenting data with diverse primitive combinations is beneficial in NLP (Zhou et al., 2023).The broader trend of scaling has led to emergent abilities in large language models (Brown et al., 2020;Bubeck et al., 2023), including in-context skill composition (He et al., 2024;Arora &amp; Goyal, 2023).However, fundamental limitations remain (Dziri et al., 2023;Zhao et al., 2024;Yu et al., 2023), and performance is often tied to concept frequency in the pretraining data (Udandarao et al., 2024;Wiedemer et al., 2025).We contribute to this debate by isolating combinatorial diversity from raw data quantity, especially when models are trained from scratch, showing that the former is the critical driver for visual compositional generalization, whereas simply increasing the latter is insufficient.</p>
<p>Structured and linearly factored representations.A growing body of work finds that large models often exhibit structured representations.Specifically, in large visionlanguage models, concept embeddings have been observed to sometimes exhibit (to a certain extent) linearity in representation space, where a composite concept's representation is the vector sum of its constituents (Trager et al., 2023;Stein et al., 2024;Park et al., 2024;Andreas, 2019).</p>
<p>Theoretical work provides formal conditions under which modularity and abstract representations emerge naturally, for instance as a function of input statistics (Dorrell et al., 2024;Whittington et al., 2022) or when networks are trained to perform multiple tasks (Johnston &amp; Fusi, 2017).However, merely learning structured or disentangled representations does not automatically guarantee compositional generalization, and the precise conditions under which a compositional structure yields such generalization remain an active area of theoretical inquiry (Lippl &amp; Stachenfeld, 2024;Montero et al., 2022;2020;Dittadi et al., 2020), particularly for visual attributes (Zhu et al., 2024).Our work provides further investigation under compositional generalization viewpoint, demonstrating the three-phase emergence of this linear structure as a function of data diversity and proving its efficiency for generalization.</p>
<p>Model-centric approaches and evaluation frameworks.</p>
<p>Many works aim to improve compositionality through model-centric solutions, such as specialized architectures (Zahran et al., 2024;?), object-centric models (Locatello et al., 2020;Wiedemer et al., 2023), soft prompting (Nayak et al., 2023), or feature alignment (Wang et al., 2024a), or algorithmic changes (Ren et al., 2023;2020).These methods are often studied in zero-shot settings (Atzmon et al., 2020;Xian et al., 2020;Isola et al., 2015;Wang et al., 2023).Concurrently, vision-language models face their own compositional challenges, with debates on whether the bottleneck lies in the vision or text encoder (Du &amp; Kaelbling, 2024;Yuksekgonul et al., 2023;Kamath et al., 2023;Vani et al., 2024).In contrast to these model-focused approaches, our work investigates whether compositionality can emerge naturally in standard architectures, isolating the data's structure as the primary variable.This requires careful evaluation, and while benchmarks exist for complex reasoning (Zerroug et al., 2022) or specific setups (Madan et al., 2021;Schott et al., 2022;Mamaghan et al., 2024), our (n, k) framework is designed as a controlled tool.It allows us to make precise, causal claims about the data factors that drive generalization.</p>
<p>Approach and experimental framework</p>
<p>In this section, we establish a systematic framework for studying compositional generalization in visual discriminative tasks.We begin by formalizing the compositional generalization through a structured mathematical framework that characterizes how visual concepts combine.We then introduce our (n, k) experimental framework that allows us to systematically control the complexity of concept spaces and evaluate models' ability to generalize to novel concept combinations.Finally, we describe our experimental design, covering both training models from scratch and evaluating pre-trained foundation models.</p>
<p>Our approach is motivated by the question of whether scaling data can enable compositional generalization in vision models.To understand the mechanisms behind learning, we also examine whether models develop structured representations in a form of linear factorization, as such structure has been observed to an extent in large pretrained vision models (Stein et al., 2024;Trager et al., 2023).</p>
<p>Data and concept space.We start by formalizing how we represent visual data in terms of concepts.Formally, we consider a finite set Although real-world images typically contain many concepts (e.g.color, shape, size, texture), we simplify our study by focusing on pairs of concepts-for example, how models combine colors with shapes in new ways.Even with this simplified setup, we find that models struggle significantly (see Section 4), suggesting that handling more concepts simultaneously would be even more difficult.
C = C 1 × • • • × C c of c
The (n, k) framework.To systematically study compositional generalization, we need a way to control the complexity of concept spaces and the diversity of training data.We introduce the (n, k) framework that characterizes concept combination spaces through two key parameters: Compositional generalization.Having established our concept space framework, we now formalize the specific learning problem we study.Let X be the space of images and {C i } c i=1 be the set of possible values for all c concepts.While images vary along all concept dimensions, we focus on learning and evaluating compositional relationships between two consistently labeled concepts.Specifically, each image x ∈ X in our training data is explicitly labeled with a pair of concept values (c 1 , c 2 ) ∈ C 1 ×C 2 , while all other factors of variation (like position, orientation, or background) remain as unlabeled concepts.</p>
<p>The compositional generalization problem over two concepts can be formalized as follows:</p>
<p>(1) Training: Given a dataset
D train = {(x i , c i 1 , c i 2 )
} Ntrain i=1 of N train total images, where each image x i is explicitly labeled with its concept values (e.g., c 1 for color, c 2 for shape).The training combinations (c i 1 , c i 2 ) are drawn from the restricted subset S train ⊂ C 1 × C 2 .We refer to this as in-distribution (ID) data.</p>
<p>(2) Testing: Evaluate on combinations from S test = (C 1 × C 2 ) \ S train , i.e., concept pairs that never co-occurred during training.We refer to this as out-of-distribution (OOD) data.</p>
<p>(3) Goal: Learn a model f that accurately predicts both labeled concepts (f 1 (x), f 2 (x)) even for images containing unseen combinations.</p>
<p>Experimental design.Our experimental approach consists of two main settings as illustrated in Figure 2: (1) training models from scratch, and ( 2) evaluating pretrained foundation models on compositional tasks under our framework.In both cases we systematically vary the (n, k) parameters.</p>
<p>Representation structure and linearity.A key question for understanding compositional generalization is how concepts are represented and combined in the learned feature space.We investigate whether concepts combine linearly in the representation space, which would provide a concrete mechanism for efficient compositional generalization (as we show in Section 4.3).Definition 3.2 (Linearly factored embeddings (Trager et al., 2023)).Given a concept space c), which we refer to as concept representations, such that for all c = (c 1 , . . ., c c ):
C = C 1 × • • • × C c , a collec- tion of vectors {u c1 , . . . , u cc } c1,...,cc∈C is linearly factored if there exist vectors u ci ∈ R d for all c i ∈ C i (i = 1, . . . ,u c = u c1 + • • • + u cc .
(1)</p>
<p>While neural networks are not guaranteed to learn such linearly factored representations, in practice we often observe that these structures emerge during training, as we will demonstrate in the following sections.When such linear factorizations do emerge, they offer benefits in generalizing compositionally, as we will show in Section 4.3.</p>
<p>Experimental setup.The guiding principle for our work was to grant models maximally favorable conditions for demonstrating compositional abilities.We do this through several deliberate choices: using oracle model selection rather than validation, fitting multiple classification heads simultaneously to encourage feature reuse, and partitioning concept combinations to create clear train (ID) / test (OOD) evaluation splits.</p>
<p>Model selection and metrics.For model selection, we use the average accuracy across all concepts at each epoch.We perform oracle model selection by directly evaluating models on the test set to select the best performing checkpoint (Gulrajani &amp; Lopez-Paz, 2020).This allows us to focus on the fundamental capabilities of models rather than validation strategies.</p>
<p>(1) Training from scratch: We use RESNET-50 (He et al., 2015) with linear classification heads; we found that using a transformer backbone (ViT) did not improve generalization performance (see Appendix C.5).The model outputs two predictions f (x) = (f 1 (x), f 2 (x)) where f j : X → C j predicts the value of concept j using a shared backbone followed by separate linear heads.Unlike CLIP which uses language embeddings for classification, we learn fixed classification heads directly from visual data to provide an optimistic setting for compositional learning through feature reuse.</p>
<p>(2) Pre-trained models: We evaluate RESNET50-IMAGENET1K (He et al., 2015), RESNET50-DINOV1 (Caron et al., 2021), DINOV2-VIT-L/14 (Oquab et al., 2024), and CLIP-VIT-L/14 (Radford et al., 2021).For these models, we pick the best probe architectures on the frozen pre-trained features: a direct linear probe (no hidden layers), an MLP with one hidden layer of size 512 , or an MLP with two hidden layers of size [512,512] with ReLU activations; we found these to provide the best performance, and more complex architectures lead to diminishing returns (results in Appendix C.4).</p>
<p>Datasets.We use DSPRITES (Matthey et al., 2017) (using only heart shape to avoid symmetries), 3DSHAPES (Kim &amp; Mnih, 2019), PUG (Bordes et al., 2023), COLORED-MNIST (Arjovsky et al., 2020), and a dataset we introduce of perceptually-challenging shapes without symmetries to which we refer as FSPRITES.Details in Appendix D.</p>
<p>Metrics.To evaluate compositional generalization and analyze the learned representations, we use two sets of metrics.</p>
<p>For generalization, we report the zero-shot accuracy on S test , measuring the model's ability to classify unseen con-cept combinations.We report the average accuracy for the concept pair under consideration.</p>
<p>For representation structure, we consider:</p>
<p>(i) Decodability-following Kirichenko et al. ( 2023); Uselis &amp; Oh (2025), we train linear probes on balanced data and report average accuracy across concepts, indicating if features capture concept information; that is, we merge the training and testing sets, and use a held-out dataset covering all concept combinations for measuring decoded accuracy.</p>
<p>(ii) Linearity-we compute the coefficient of determination (R 2 ) between joint representations f (x) and their reconstruction from individual concept representations (iii) Orthogonality-we measure the mean cosine similarity
k i=1 u ci , where R 2 = 1 − x ∥f (x)− k i=1 uc i ∥ 2 x ∥f (x)− f ∥ 2 with f = 1 |D| x∈D f (x)1 |C1||C2| i,j cos(u c i 1 , u c j 2
) between concept representations to assess if concepts are encoded in orthogonal subspaces, sometimes found in pretrained models (Stein et al., 2024;Wang et al., 2024b).</p>
<p>We report the representation structure metrics only for fromscratch models; this is due to the fact that pretrained models may encode other information other than the target concepts.</p>
<p>Does compositional generalization emerge</p>
<p>with data scale?  5. What are the theoretical benefits of such structure for compositional generalization?We show that this linear structure enables perfect generalization to unseen combinations with just two combinations per concept value.</p>
<p>Compositional generalization is difficult but diverse data helps</p>
<p>Models struggle with basic compositional generalization.</p>
<p>In Figure 3(a), in a basic compositional setting with n = 3 concept values and k = 2 seen combinations per concept value, while all models achieve strong ID accuracy (near 100%, yellow bars), their performance drops significantly when evaluated on unseen combinations of concepts (brown bars).For example, MNIST digit recognition accuracy drops by around 78% in the OOD setting.Interestingly, in all datasets, at least one concept shows relatively small degradation, ranging from only 3% drop (orientation in FSprites) to 17% drop (object-hue in Shapes3D), while other concepts in the same datasets show much larger performance gaps.</p>
<p>Increasing concept diversity improves generalization.Figure 3(b,c) shows that generalization improves both when increasing the number of target classes (n) with fixed diagonal training combinations (k = n − 1), and when increasing training combinations (k) with fixed maximum target classes.This suggests that both target space diversity and exposure to more concept combinations enhance compositional learning, even when the target size remains constant.</p>
<p>Dataset size alone provides limited improvement for generalization.We experimented with RESNET50 trained from scratch using n = 3, k = 1 and three different training set sizes: 7,500, 15,000, and 30,000 samples for SHAPES3D and CMNIST (the maximum number of unique samples possible with these combinations), and up to 120,000 samples for DSPRITES and FSPRITES.We excluded PUG from this analysis since with n = 3, there were too few unique samples available to effectively train the model from scratch.As shown in Figure 4, despite increasing the training data by 4x, the gap between ID and OOD performance remains large across all datasets: models still show accuracy drops of 60-80% on unseen combinations.This suggests that simply scaling up training data within the same distribution is insufficient for achieving compositional generalization.</p>
<p>Takeaway §4.1: Compositional generalization remains challenging across all datasets, with accuracy drops of 60-80% on unseen combinations despite perfect in-distribution performance.While increasing target diversity and combination exposure improves generalization, scaling dataset size provides limited improvement.Some concepts show relatively small degradation (3-17% drops) while others in the same datasets show much larger gaps.Both target space diversity and exposure to more concept combinations enhance compositional learning, but increasing training data quantity (up to 4x) only helps reduce the large ID-OOD performance gap without fully solving the problem.</p>
<p>Three-phase behavior in feature learning</p>
<p>To understand why models struggle with compositional generalization, we investigate two potential explanations motivated by prior work on shortcut learning and distributional robustness (Geirhos et al., 2020;Sagawa et al., 2020).First, the learned features could be spurious, failing to capture meaningful concept information.Second, novel concept combinations may produce "misplaced" representations that the classifier fails on.We analyze these possibilities by examining the structure of feature spaces using the linearity and orthogonality metrics defined in Section 3, measuring both the quality of individual concept representations and how predictably they combine.Using a balanced dataset with all concept combinations (including unseen ones) and 100 samples per combination, we evaluate models trained in the previous section across multiple datasets (MNIST, FSPRITES, SHAPES3D, PUG).</p>
<p>Our analysis reveals two key findings about how neural networks learn to represent concepts (Figure 5).First, we find that linearity in representations emerges naturally as models are exposed to more diverse training combinations.As shown in Figure 5(b), both the linear separability (R 2 scores) and orthogonality (cosine similarity) of concept dimensions improve with increased training diversity.This emergence of linear structure is accompanied by improved zero-shot generalization-Figure 5(a) shows that zero-shot accuracy on unseen combinations steadily increases as training diversity grows.</p>
<p>Second, we observe that this progression occurs in three distinct phases: (i) With limited concept combinations (0-10%), models learn spurious features with poor discrimination (decoded accuracy &lt;80%) and random-level zero-shot performance, as shown by entangled representations in Figure 5(c) at 8%.</p>
<p>(ii) At moderate diversity (25-75%), linearity and orthogonality begin emerging (Figure 5(b)), with features becoming decodable (100% accuracy) and zero-shot performance reaching 60-80%.</p>
<p>(iii) At high diversity (75-100%), while discriminability plateaus, representations become strongly linear (R 2 &gt; 0.8) and orthogonal (cosine similarity &lt;0.1), enabling zero-shot accuracy above 90% on the majority of the datasets.The PCA visualizations in Figure 5(c) qualitatively confirm this progression from entangled to linear factorization.</p>
<p>Benefits of linear factorization</p>
<p>The benefit of a linear feature structure becomes apparent when contrasted with the weaker property of decodability.While features are often decodable, this alone is insufficient for generalization to unseen combinations.Generalizing through decodability may require exposure to all possible concept pairings, which is infeasible.As illustrated in Figure 7 (center), while adaptation can compensate for unstructured representations, this approach demands a balanced dataset of all combinations, which is impractical at scale.In contrast, a linear feature structure enables generalization without exhaustive supervision.As shown in Figure 7 (right), when representations are organized linearly, models can correctly classify novel combinations, overcoming the limitations of mere decodability.</p>
<p>Motivated by our observation that models achieving strong compositional generalization exhibit highly linear concept representations, we now investigate the theoretical benefits of such a structure.In this idealized case, how many</p>
<p>Color</p>
<p>Shape Color Color</p>
<p>(1) Incorrect zero-shot (2) Correct decoded (3) Correct zero-shot Decodable Linear</p>
<p>Shape Shape</p>
<p>Figure 7: Importance of linear feature structure for compositional generalization.We illustrate a schematic for shape and color classification using linear models in a 2-dimensional feature space, comparing zero-shot and adapted cases with frozen feature extractor.</p>
<p>(1) If the feature space lacks a linear structure, the model misclassifies the orange square in zero-shot inference.</p>
<p>(2) Adaptation by adding orange square samples allows correct classification.(3) A linearly structured feature space enables correct zero-shot generalization without adaptation.The decision boundaries are linear in all cases, but only the features in the rightmost panel enable zero-shot generalization.</p>
<p>concept combinations would a model with perfectly linear representations need to observe to generalize to all unseen combinations?We answer this questions in the following proposition.</p>
<p>Proposition 4.1 (Minimal Compositional Learning).Let f : X → R d be a feature extractor with linearly factored concept embeddings over C. Let {u c 1 1 , . . ., u c n 1 } and {u c 1 2 , . . ., u c n 2 } be the concept vectors for the first and second concepts respectively, where their joint span has dimension 2n − 1. Suppose we only observe joint representations for concept combinations c i , c j ∈ {1, . . ., n}.Then k = 2 combinations per concept value suffice to learn a linear classifier that perfectly generalizes to all (n − k) • n unseen combinations.</p>
<p>This proposition illustrates the benefit of perfectly composi-tional representations: with just two examples per concept value, perfect generalization is possible if the feature space is linearly factorized.We view this as a starting point-while the assumption of linearly independent factors is often satisfied in both from-scratch and pre-trained models, it can break down as the number of values grows, making joint linear independence impossible -e.g., such factors may occupy low-dimensional subspaces (Sonthalia et al., 2025).We expect that this assumption can be relaxed, and that a more complete understanding of the setting is possible in future work.</p>
<p>Takeaway §4.3:When linear factorization is present, perfect compositional generalization is possible with just two combinations per concept value.</p>
<p>Do large pre-trained models generalize compositionally?</p>
<p>Our analysis of models trained from scratch revealed that linear structure emerges naturally when models are exposed to diverse concept combinations.This finding raises a question: Have large-scale pretrained models already learned such linear structure through their pretraining?To investigate this, we evaluate pretrained models using two complementary approaches.We first test for the ideal linear structure from our theoretical framework (Proposition 4.1), which would enable perfect generalization.This reveals how close existing models are to this optimal linear structure.Second, we use (non-)linear probing to assess general concept accessibility in the feature space.Comparing these approaches allows us to distinguish between models that simply encode concept information and those that represent it in a structured, linear manner.</p>
<p>Evaluating via linear factorization</p>
<p>Measuring linearity.Building on our earlier findings showing the natural emergence of linearly factored representations, we test how well the recovered concept value representations (detailed algorithm in Appendix 1) can be used to classify novel concept combinations.Classification of a new input x can then be performed by projecting the representation f (x) onto the u and v values to acquire labels for both concepts.</p>
<p>We calculate accuracy for each concept using this approach and illustrate the results in Figure 6.Certain concept pairs show strong amenability to linear representation across all models.On PUG-ANIMAL, all models achieve exceptionally high accuracy (&gt;90%) on WORLD-NAME concept, suggesting more linear representations.The best model consistently exceeds 90% accuracy on some concept classification across all datasets.Additionally, models show clear specialization: CLIP excels at color-based tasks (highest accuracy on CMNIST color-digit and SHAPES3D object-hue), while DINOV2 performs best on shape-based concepts (e.g. on scale, shape, orientation, and character).</p>
<p>While no model achieves the perfect generalization predicted by our theoretical analysis for ideally linear representations, these results demonstrate that pre-trained models exhibit partial linearity in their representations, varying in strength across concept types.Strong performance on some concept pairs supports our hypothesis that linear representation organization facilitates compositional generalization.</p>
<p>Evaluating generalization via probing</p>
<p>While the linear factorization analysis tests for an ideal compositional structure, we also employ a more direct test of generalization: probing.In this approach, we train a simple classifier (a non-linear probe) on the model's features for the seen concept combinations from our training set and evaluate it on the unseen combinations.This directly measures whether a consistent mapping from features to concepts can be learned and transferred.For each model and dataset, we compute the average accuracy for a given k value, keeping n = n max .To enable fair comparison across datasets, we normalize each model's performance by its maximum accuracy and aggregate the results, as shown in Figure 8.</p>
<p>All pre-trained models consistently outperform the fromscratch RESNET50, showing that pre-training provides a significant advantage.However, it is not a complete solution, as all models improve as the diversity of training combinations increases.Despite the benefits of pretraining, models still face challenges in generalizing to unseen concept combinations.While larger models like CLIP and DINO VIT-L show the strongest performance, the persistent gap between pretrained and from-scratch models indicates that current pretraining approaches do not generalize compositionally well.</p>
<p>Takeaway §5: Pre-training is not a substitute for data diversity.While large models like CLIP and DINO VIT-L develop partially linear representations, our analysis shows they only generalize reliably after training a downstream model on a diverse set of concept combinations.</p>
<p>Conclusion</p>
<p>In this work, we systematically investigated the conditions under which vision models achieve compositional generalization, focusing on the distinct roles of data scale versus data diversity.Our findings reveal that merely increasing the volume of training data is insufficient for generalization to novel concept combinations.Instead, data diversity is the critical factor.We identified a three-phase learning dynamics where models transition from learning spurious correlations to discriminative features, and finally to a linearly structured representation space only when trained with sufficient combinatorial diversity.We provide theoretical evidence for the power of this structure, proving that such linear factorization allows for perfect generalization from a minimal number of training examples in an idealized setting.When we evaluated large-scale pretrained models through this lens, we found they exhibit some of this compositional structure but remain far from perfect, achieving mixed results that highlight their limitations.</p>
<p>Ultimately, our work suggests that while current scaling paradigms are beneficial, they do not automatically confer robust compositional abilities due to the inherent combinatorial sparsity of large-scale datasets.Achieving compositional generalization will likely require a more deliberate focus on structured data diversity to induce the necessary representational geometry in vision models.</p>
<p>u ′ ci,cj denotes the pairwise joint embedding for values c i , c j • Datasets:</p>
<ul>
<li>We denote D C as the dataset over all possible concept combinations.In practise, we only observe limited combinations, as discussed in Section A. We denote such a dataset as D train and D test for the training and test sets, respectively.</li>
</ul>
<p>We also restate the linear factorization definition from the main text: Definition B.1 (Linearly factored embeddings (Trager et al., 2023)).Given a concept space
C = C 1 × • • • × C c , a collection of vectors {u c } c∈C is linearly factored if there exist vectors u ci ∈ R d for all c i ∈ C i (i = 1, . . . , c
), which we refer to as concept representations, such that for all c = (c 1 , . . ., c c ):
u c = u c1 + • • • + u cc .(3)
Assuming linear factorization,
f (x) = k ℓ=1 u c ℓ (x) ,
and given a dataset D = {(x j , c j )} s j=1 with s := c i=1 |C i |, with image-concept pairs, we can recover a representation (up to a global shift shared by all factors) for each concept value by averaging feature vectors across all combinations that contain that value (Trager et al., 2023).Formally, for a value c i ∈ C i let
u ′ ci := 1 |D ci | x∈Dc i f (x) − f , f := 1 |D| x∈D f (x),(4)
Thus u ′ ci is the conditional mean feature vector, centred by the global mean f .We first describe the relationship between the ground truth factors u ci and the recovered ones u ′ ci .These relationships only hold for the case when the contstructed factors are recovered from the full dataset.Lemma B.2 (Relation to ground truth concept vectors).Let u ci denote the true concept vector for value c i , and u ′ ci the recovered one from (4).Over the full dataset,
u ′ ci = u ci − 1 |C i | c ′ i ∈Ci u c ′ i .
Proof.Start from the definition (4) and substitute the linear factorisation f (x) = c ℓ=1 u c ℓ (x) :
u ′ ci = 1 |D ci | x∈Dc i f (x) − f = 1 |D ci | x∈Dc i c ℓ=1 u c ℓ (x) − f .
(1)</p>
<p>Interchange the sums in (1).For the term with ℓ = i each x ∈ D ci contributes u ci , hence
1 |D ci | x∈Dc i u ci = u ci . For any ℓ ̸ = i each value c ′ ℓ ∈ C ℓ occurs equally often inside D ci , namely |D ci |/|C ℓ | times. Therefore 1 |D ci | x∈Dc i u c ℓ (x) = 1 |C ℓ | c ′ ℓ ∈C ℓ u c ′ ℓ .
Summing these contributions and using the explicit formula for the global mean
f = 1 |D| x∈D f (x) = c ℓ=1 1 |C ℓ | c ′ ℓ ∈C ℓ u c ′ ℓ , it follows that u ′ ci = u ci + ℓ̸ =i 1 |C ℓ | c ′ ℓ u c ′ ℓ − f = u ci − 1 |C i | c ′ i ∈Ci u c ′ i ,
as claimed.</p>
<p>It also follows that this construction of factors u ci leads to recovery of the sum of factored embeddings up to a global mean.Importantly, if full dataset D C is available, normalizing the mean of the embeddings (i.e.setting f := 0) is possible.</p>
<p>Lemma B.3 (Reconstruction of a centred embedding).For any x with concept values (c 1 (x), . . ., c c (x))
f (x) = f + i u ′ ci(x) .
Proof.Using Lemma B.2 we have for every concept value c i
u ′ ci = u ci − 1 |C i | c ′ i ∈Ci u c ′ i .
Applying this identity to the particular values c i (x) of the sample x and summing over i = 1, . . ., k yields
c i=1 u ′ ci(x) = c i=1 u ci(x) − c i=1 1 |C i | c ′ i ∈Ci u c ′ i = f (x) − f ,
where the last equality uses f (x) = i u ci(x) and the definition of the global mean f .</p>
<p>In what follows we study compositional settings where the concept space may include many factors, but only two factors, C 1 and C 2 , are observed; the remaining factors C 3 , . . ., C c are unobserved.Importantly, factors C 1 and C 2 exhibit a correlation due to the (n, k) framework.</p>
<p>Next, we establish a convenient property of the factored representations.
D ci := {x ∈ D | c i (x) = c i }, m := |D ci | (same for every c i ), Summing over c i gives ci∈Ci u ′ ci = ci∈Ci 1 m x∈Dc i f (x) − f (5) = 1 m ci∈Ci x∈Dc i f (x) − |C i | f (6) = 1 m x∈D f (x) − |C i | f (7) = |D| m f − |C i | f (|D| = |C i | m) (8) = |C i | f − |C i | f = 0.(9)
In practice, we often only observe a subset of concept combinations.To accomodate such a constraint, we formalize it through pairwise joint embeddings:
Definition B.5 (Pairwise joint embedding). Given a concept space C = C 1 ×• • • ×C c , the pairwise joint embedding for factors i ̸ = j and values c i ∈ C i , c j ∈ C j is u ′ ci,cj = 1 |D ci,cj | x∈Dc i ,c j f (x) − f , D ci,cj := {x ∈ D | c(x) i = c i , c(x) j = c j }. (10)
Lemma B.6 (Additivity of joint embeddings).Under a linear factorisation s.
t. f (x) = c ℓ=1 u c ℓ (x) holds, u ′ ci,cj = u ′ ci + u ′ cj . (11) Proof. Define D ci,cj := x ∈ D | c(x) i = c i , c(x) j = c j , N ci,cj := |D ci,cj |. Substituting the centred decomposition f (x) = f + c ℓ=1 u ′ c ℓ (x) from Lemma B.3 to Definition B.5 gives u ′ ci,cj = 1 N ci,cj x∈Dc i ,c j f (x) − f (12) = 1 N ci,cj x∈Dc i ,c j f + c ℓ=1 u ′ c ℓ (x) − f (13) = 1 N ci,cj x∈Dc i ,c j c ℓ=1 u ′ c ℓ (x) .(14)
For every x ∈ D ci,cj we have c i (x) = c i and c j (x) = c j .Hence the terms with ℓ = i and ℓ = j contribute exactly u ′ ci and u ′ cj , respectively.For any ℓ / ∈ {i, j} each value c ′ ℓ ∈ C ℓ occurs equally often inside D ci,cj .Therefore
1 N ci,cj x∈Dc i ,c j u ′ c ℓ (x) = 1 |C ℓ | c ′ ℓ ∈C ℓ u ′ c ′ ℓ = 0, by Lemma B.4.
Collecting all contributions we obtain the desired identity
u ′ ci,cj = u ′ ci + u ′ cj .
We now establish our main theoretical result on the minimal data requirements for compositional generalization.The derivations from the Lemmas above are appropriate under the assumption of a balanced training set.Due to the unlikely nature of certain concept combinations (as described in the (n, k) framework), the main challenge is identifying the factors under such a setting.</p>
<p>Proposition B.7 (Minimal compositional learning).Let f : X → R d be a feature extractor with linearly factored concept embeddings over C. Let {u c 1 1 , . . ., u c n 1 } and {u c 1 2 , . . ., u c n 2 } be the concept vectors for the first and second concepts respectively, where their joint span has dimension 2n − 1. Suppose we only observe joint representations for concept combinations c i , c j ∈ {1, . . ., n}.Then k = 2 combinations per concept value suffice to learn a linear classifier that perfectly generalizes to all (n − k) • n unseen combinations.</p>
<p>Proof.The proof proceeds in three steps: (1) showing that joint factored embeddings are identifiable from training data, (2) showing that the system of linear equations has full rank with 2n equations and 2n unknowns, and (3) showing that optimal classifiers can be constructed via orthogonal projections.</p>
<p>Part 1: Identifying joint factored embeddings
u c i 1 ,c j 2 .
We assume k = 2 for simplicity, but the same applies for higher k.First, note that we observe the following combinations:
C train = {(i, i) : i ∈ [n]} ∪ {(i, i + 1) : i ∈ [n − 1]} ∪ {(n, 1)} (15) = {(1, 1), (2, 2), ..., (n, n)} ∪ {(1, 2), (2, 3), ..., (n − 1, n)} ∪ {(n, 1)}(16)
with |C train | = 2n total combinations.This dataset is restricted to the combinations in C train , but varies in other concepts.We denote this dataset as
D train := {(c 1 , c 2 , x) : (c 1 , c 2 ) ∈ C train , x ∈ X }.
We aim to show that the average embedding over the training set, ūtrain , equals the global mean embedding f (as defined in the proof of Lemma B.4).Let D i,j ⊂ D train be the subset of training samples for the specific concept combination (i, j).To see the importance of this, note that
u ′ c i 1 ,c j 2 = u ′ c i 1 + u ′ c j 2 . (17
We now show that f = ūtrain .</p>
<p>Under the assumption of a balanced training set where each combination (i, j) ∈ C train has the same number of samples, we can define the mean embedding for each combination as:
fi,j := 1 |D i,j | x∈Di,j f (x).
The overall training mean is then: for the 2n training combinations.The system of equations can be written as:
ūtrain := 1 |D train | x∈Dtrain f (x) (19) = 1 2n n i=1 fi,i + n−1 i=1 fi,i+1 + fn,1(20)= 1 2n n i=1 (f + u ′ c i 1 + u ′ c i 2 ) + n−1 i=1 (f + u ′ c i 1 + u ′ c i+1 2 ) + (f + u ′ c n 1 + u ′ c 1 2 ) (21) = 1 2n 2nf + 2 n i=1 u ′ c i 1 + 2 n i=1 u ′ c i 2 (22) = 1 2n (2nf + 2 • 0 + 2 • 0) (by Lemma B.4) (23) = f(                   u ′ c 1 1 ,c 1 2 u ′ c 2 1 ,c 2 2 . . . u ′ c n 1 ,c n 2 u ′ c 1 1 ,c 2 2 u ′ c 2 1 ,c 3 2 . . . u ′ c n−1 1 ,c n 2 u ′ c n 1 ,c 1 2                    V =                  10 0 • • • 1 0 0 0 • • • 0 1                                    u ′ c 1 1 u ′ c 2 1 . . . u ′ c n 1 u ′ c 1 2 u ′ c 2 2 . . . u ′ c n 2                     U 1 U 2   (25)
We note that this system is full rank, as the design matrix has linearly independent rows.The first block of rows corresponds to the diagonal combinations (i, i), while the second block corresponds to cyclic combinations (i, i + 1) (with wraparound from n to 1).These form distinct patterns that ensure linear independence.</p>
<p>Given this full rank system with 2n equations and 2n unknowns (the factored representations u c i 1 and u ′ c i 2 for each concept value), we can uniquely solve for the factored representations.For k &gt; 2 combinations per concept value, we get more equations while maintaining the same number of unknowns, making the system overdetermined and the solution more robust.To systematically probe compositional generalization in pre-trained vision models, we evaluated a range of architectures, including ResNet50 (from scratch and ImageNet pre-trained), DINOv1, DINO ViT-L, and CLIP ViT-L across several datasets and concept axes, as shown in Figure 10.</p>
<p>C.3. MPI3D dataset results</p>
<p>To validate our findings on real-world datasets, we conduct experiments on the MPI3D dataset (Gondal et al., 2019), which contains photographs of 3D scenes with systematic concept variations.We introduce the Funny Sprites dataset, an OOD dataset designed to test models' ability to generalize to previously unseen shape combinations.The dataset consists of sprites traced from 5-15 points on a 128x128 pixel grid, creating a diverse set of abstract geometric shapes.</p>
<p>Figure 1 :
1
Figure 1: Sparse concept combinations in large-scale datasets.Left:An indicator matrix of noun-adjective co-occurrences in LAION-400M shows significant sparsity in concept combinations; the majority of cells are unobserved (zoomed-in view), demonstrating that even common concepts rarely combine in the dataset.This sparsity biases models toward memorizing frequent combinations rather than learning compositional structure.Right: A concrete example of a 4x4 matrix of nouns (seat, apple, bed, cheese) and attributes(tiny, liquid, melted, electric).This work investigates how vision models develop compositional attribute-object understanding in simplified and controlled settings.</p>
<p>nFigure 2 :
2
Figure 2: Investigating compositional learning through concept scaling.The figure illustrates our two main experimental settings.Left (Data setting): Training data consisting of images with corresponding concept combinations shown in the grid, where blue cells indicate observed combinations during training.Right (Model setting): Two approaches-training models from scratch (Section 4) where we systematically increase the number of possible concept values n while fixing combinations per concept at k = 2, showing examples with n = 4, n = 6, and n = 10, and evaluating pre-trained foundation models' (FM) compositional abilities by fitting an MLP classifier on features (Section 5).The grid demonstrates how the concept space expands as we increase n, creating a larger set of unseen combinations for testing generalization.</p>
<p>measures how well representations follow linear structure.Here, f represents the mean representation across all samples.</p>
<p>Figure 3 :
3
Figure 3: Compositional generalization emerges through different forms of concept diversity.(a) In basic settings with limited diversity, models show substantial accuracy drops on unseen combinations (brown) compared to seen combinations (yellow), demonstrating the inherent difficulty of compositional generalization.(b) When increasing the number of target classes (n) while keeping dataset size and diagonal training combinations fixed (k = n − 1), models show improved generalization, suggesting that target space diversity drives compositional learning.(c) With fixed maximum target classes, increasing the number of training combinations (k) also improves performance, showing that exposure to more concept combinations enhances generalization ability, even if the target size is the same.</p>
<p>Figure 4 :
4
Figure 4: Increasing ID training data quantity does not solve compositional generalization.Despite training with significantly more in-distribution samples, models still struggle to generalize to unseen concept combinations.The gap between ID and OOD performance remains large across all datasets, suggesting that the challenge of compositional generalization cannot be solved simply by scaling up training data within the same distribution.</p>
<p>Figure 5 :
5
Figure 5: Linearity emerges with data diversity, while feature discriminability alone does not imply linear structure.(a) Feature discriminability emerges early but does not imply compositional structure, (b) Linear concept representations only emerge with increased training diversity, as shown through R 2 scores and orthogonality measures, (c) PCA visualizations confirm evolution from entangled to linear feature organization as training diversity increases.X-axis represents percentage of training combinations k/n, with n being the maximum number of concept values.</p>
<p>Figure 8 :
8
Figure8: Even with pretraining, models struggle with compositional generalization.Despite the benefits of pretraining, models still face challenges in generalizing to unseen concept combinations.While larger models like CLIP and DINO VIT-L show the strongest performance, the persistent gap between pretrained and from-scratch models indicates that current pretraining approaches do not generalize compositionally well.</p>
<p>D represents a dataset of image-concept pairs -D C is the dataset over all possible concept combinations -D train and D test are the training and test datasets with limited and unseen combinations, respectively -D ci is the subset of D containing concept value c i -D ci,cj contains both values c i and c j • Training constructs:-C train is the set of observed concept combinations during training fi,j represents the mean embedding for combination (i, j)Let X denote the input space andC = C 1 × C 2 × • • • × C c represent the concept space.We assume a mapping c : X → C that identifies for each image x ∈ X its corresponding concept values c(x) = (c 1 , . . ., c c ) ∈ C.</p>
<p>Lemma B. 4 (
4
Zero-sum embeddings).For any concept dimension i ∈ {1, . . ., c}, be the global mean.For each value c i ∈ C i set</p>
<p>) By Definition B.5, given some observations of concept values c i 1 and c j 2 , the pairwise joint embedding u ′ of the training samples for the combination (i, j) shifted by the global mean embedding f .Consider the mean embedding over the training set ūtrain := 1 |D train | x∈Dtrain f (x).</p>
<p>Part 2 : 2 ,
22
24) Thus, we can identify the factored representations u c i 1 ,c j 2 for each concept value combination i, j ∈ [n] from the training data since the average representation over the training data under our training dataset is the global mean embedding f .With this, we can compute u ′ Identifying the individual factored representations u c i 1 and u c i 2 for each concept value i ∈ [n].Consider a training set with exactly two combinations per concept value.By the linear factorization property, for any combination (i, j) in our training set, we have: u ′ where c i 1 denotes value i for the first concept and c j 2 denotes value j for the second concept.Let U 1 , U 2 ∈ R d×n be matrices whose columns are the unknown factored representations u ′ n].Let V ∈ R d×2n be the matrix of observed pairwise joint embeddings u ′ c i 1 ,c j 2</p>
<p>Figure 9 :
9
Figure 9: Performance scaling with concept diversity.OOD accuracies across four datasets: Shapes3D, dSprites, FSprites, and Colored-MNIST.Each heatmap shows performance for different combinations of concept values (n) and seen combinations (k) per concept value.Increasing concept diversity (higher n) consistently improves generalization performance across all datasets, even when the number of training combinations per concept remains fixed.</p>
<p>Figure 10 :
10
Figure 10: Compositional generalization in pre-trained models.Heatmaps show out-of-distribution accuracy for different combinations of n (concept values) and k (training combinations) across datasets.Darker colors indicate higher accuracy.Pre-trained models exhibit improved generalization with increased concept diversity, mirroring the pattern observed in from-scratch training.</p>
<p>Figure 15 :
15
Figure 15: FunnySprites dataset examples.Shape and orientation variations for n = 14 concept values with k = 2 training combinations.Each sprite is generated by connecting traced points to form unique geometric shapes, providing a challenging test for compositional generalization.</p>
<p>Figure 16 :
16
Figure 16: Colored-MNIST examples.Digit and color combinations for n = 10 values with k = 3 training combinations.This dataset combines the MNIST digits with color variations to test compositional understanding of shape and color attributes.</p>
<p>Compositional generalization capabilities of pre-trained models under assumed linear factorization.Bar plots show both training (transparent) and testing (solid) accuracy across different datasets (DSPRITES, SHAPES3D, CMNIST, PUG-ANIMAL) when using minimal training data (k = 2 combinations per concept) to learn linear concept representations for each concept.Dashed lines indicate random baseline performance.Following Proposition 4.1, we identified the factored representations uc 1 and uc 2 for each concept value using k = 2 combinations per concept value.While perfect generalization predicted by the proposition would require ideal linear compositionality, our empirical results show strong performance on certain concepts (e.g., &gt; 90% accuracy on color, orientation, digit, and background concepts for either CLIP or DINOV2 models), with varying effectiveness across different concept types and models, suggesting that pre-trained representations exhibit partial linearity in their representations.
Accuracy (%)25 50 75 100 0ResNet50 IN1K DINOv1 ResNet50 DINOv2 ViT-L CLIP ViT-L 56 33 80 67 85 64 98 43 (object-hue, scale) Shapes3D44 ResNet50 56 44 (scale, orientation) 82 30 90 29 86 DSprites IN1K DINOv1 ResNet50 DINOv2 ViT-L CLIP ViT-L Training 80 28 ResNet50 IN1K DINOv1 85 50 (color, shape) 80 80 97 FSprites ResNet50 DINOv2 ViT-L CLIP 57 ViT-L Testing68 59 ResNet50 IN1K DINOv1 71 70 72 (color, digit) CMNIST 83 ResNet50 DINOv2 ViT-L CLIP 94 92 (character-name, world-name) 100 100 PUG Animal 100 91 38 22 63 41 ViT-L ResNet50 IN1K DINOv1 DINOv2 ViT-L CLIP ViT-L ResNet50 RandomFigure 6:
AcknowledgmentsWe thank the anonymous reviewers for their valuable feedback, Yujin Jeong, Simon Buchholz, Yi Ren, Samuel Lippl, Ankit Sonthalia, Alexander Rubinstein, and Martin Gubri for helpful discussions, and the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Arnas Uselis.This work was supported by the Tübingen AI Center.Impact statementThis work advances understanding of compositional learning in vision models, which could enable more data-efficient and reliable AI systems.We release our code and datasets publicly to promote reproducible research and responsible development of these capabilities.A. AppendixA. Experimental setup and implementationA.1. Implementation detailsIn this section we provide additional details on the implementation of the experiments.Optimization.All models are trained using the Adam(Kingma &amp; Ba, 2017)optimizer.Based on an initial grid search, we use a learning rate of 10 −4 for ResNet training from scratch and 10 −3 for probing pre-trained features.All models are trained for 100 epochs with a batch size of 64.Train/Test splits.For each concept value i, we observe combinations with values j where (i − j + n) mod n &lt; k, and evaluate on all other combinations.This creates a clear distinction between combinations seen during training and those requiring compositional generalization.The key idea is creating a training set that is balanced such that each concept value is observed with equal frequency.For each concept value i ∈ {0, . . ., n − 1}, we observe exactly k combinations during training, defining our training and test sets as:B. ProofsIn this section we provide the proofs for our main theoretical results.Notation.We summarize the notation used throughout the proofs, though we reintroduce each term where appropriate.• Spaces and mappings:-X represents the input space (images)gives the concept values for image x c i denotes the value of the i-th concept • Framework parameters:n is the number of concept values per dimension in the (n, k) framework k is the number of training combinations per concept value c is the total number of concept dimensions • Feature representations:f (x) is the feature extractor output for imageis the global mean embedding u ci represents the true concept vector for value c i u ′ ci is the recovered centred concept vector for value c i Once we recover these factored representations, we can compute u ′for any combination (i, j), including the (n − 2)n unseen ones.Part 3: Optimality of classifiers.To show that we can construct classifiers that provable generalize to novel combinations, we simply note that by assumption no concept representation is within the span of remaining representations.As such, given, any vector w in their joint span can be uniquely decomposed asw = u 1 + u 2 where u 1 ∈ U 1 , u 2 ∈ U 2  and u 1 ⊥ u 2.This allows us to construct projection matrices P U1 and P U2 onto these orthogonal subspaces, which can then be used to build optimal classifiers by projecting input features onto the respective concept subspaces.B.1. Algorithmic recovery of factored representationsWe provide a constructive algorithm for recovering factored concept representations from limited available training combinations in Algorithm 1.in position row of v9:Update row row of A with indicators for concepts i and j 10:row ← row + 1 11:end forC. Additional experimental resultsThis section presents supplementary experimental findings.C.1.From-scratch model performance object-color object-shape These results provide strong evidence that compositional generalization benefits specifically from diversity in concept combinations rather than mere quantity of training data.All models show near-perfect accuracy on the color concept, while shape concept performance is worse.Right: Probing results using linear and non-linear probes.C.4. Comparison of different probe configurationsWe present a detailed comparison of probe results across different model architectures and probe types.Table1reports the accuracy of both linear and non-linear (two-layer MLP) probes on the FSprites dataset for several pre-trained models.Notably, non-linear probes generally yield higher accuracy than linear probes, especially for models like DINO ResNet-50 and DINO ViT-Large, indicating that some compositional information is not linearly accessible in the representations.However, for ResNet-50 and CLIP ViT-Large, the difference between linear and non-linear probe performance is smaller, suggesting that their representations are more linearly separable for the evaluated concepts.D. Dataset details and examplesThis section provides comprehensive information about all datasets used in our experiments, including detailed descriptions and visual examples.
Measuring Compositionality in Representation Learning. J Andreas, 2019</p>
<p>Invariant Risk Minimization. M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, 2020</p>
<p>A Theory for Emergence of Complex Skills in Language Models. S Arora, A Goyal, 2023</p>
<p>A causal view of compositional zero-shot recognition. Y Atzmon, F Kreuk, U Shalit, G Chechik, 2020</p>
<p>PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning. F Bordes, S Shekhar, M Ibrahim, D Bouchacourt, P Vincent, A S Morcos, 2023</p>
<p>Language Models are Few-Shot Learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>Emerging Properties in Self-Supervised Vision Transformers. M Caron, H Touvron, I Misra, H Jégou, J Mairal, P Bojanowski, A Joulin, 2021</p>
<p>Symbols and mental programs: A hypothesis about human singularity. S Dehaene, F Al Roumi, Y Lakretz, S Planton, M Sablé-Meyer, Trends in Cognitive Sciences. 2622022</p>
<p>A Dittadi, F Träuble, F Locatello, M Wüthrich, V Agrawal, O Winther, S Bauer, B Schölkopf, arXiv:2010.14407On the transfer of disentangled representations in realistic settings. 2020arXiv preprint</p>
<p>W Dorrell, K Hsu, L Hollingsworth, J H Lee, J Wu, C Finn, P E Latham, T E Behrens, J C Whittington, arXiv:2410.06232Don't cut corners: Exact conditions for modularity in biologically inspired representations. 2024arXiv preprint</p>
<p>. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, J Uszkoreit, N Houlsby, 2021An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</p>
<p>Compositional Generative Modeling: A Single Model is Not All You Need. Y Du, L Kaelbling, 202413</p>
<p>. N Dziri, X Lu, M Sclar, X L Li, L Jiang, B Y Lin, P West, C Bhagavatula, R L Bras, J D Hwang, S Sanyal, S Welleck, X Ren, A Ettinger, Z Harchaoui, Y Choi, Faith, Fate, 2023Limits of Transformers on Compositionality</p>
<p>E Elmoznino, T Jiralerspong, Y Bengio, G Lajoie, arXiv:2410.14817A complexity-based theory of compositionality. 2024arXiv preprint</p>
<p>The Language of Thought. The Language and Thought Series. J A Fodor, J A Fodor, 1975Crowell1New York, NY</p>
<p>Shortcut Learning in Deep Neural Networks. R Geirhos, J.-H Jacobsen, C Michaelis, R Zemel, W Brendel, M Bethge, F A Wichmann, Nature Machine Intelligence. 262020</p>
<p>On the transfer of inductive bias from simulation to the real world: a new disentanglement dataset. M W Gondal, M Wuthrich, D Miladinovic, F Locatello, M Breidt, V Volchkov, J Akpo, O Bachem, B Schölkopf, S Bauer, Advances in Neural Information Processing Systems. 20193222</p>
<p>Search of Lost Domain Generalization. I Gulrajani, D Lopez-Paz, 2020</p>
<p>Deep Residual Learning for Image Recognition. K He, X Zhang, S Ren, J Sun, 2015</p>
<p>M A Lepori, T Serre, E Pavlick, Break It Down: Evidence for Structural Compositionality in Neural Networks. 2023</p>
<p>When does compositional structure yield compositional generalization? a kernel theory. S Lippl, K Stachenfeld, 2024</p>
<p>Object-Centric Learning with Slot Attention. F Locatello, D Weissenborn, T Unterthiner, A Mahendran, G Heigold, J Uszkoreit, A Dosovitskiy, T Kipf, 2020</p>
<p>When and how CNNs generalize to out-of-distribution categoryviewpoint combinations. S Madan, T Henry, J Dozier, H Ho, N Bhandari, T Sasaki, F Durand, H Pfister, X Boix, 202123</p>
<p>. D Mahajan, M Pezeshki, C Arnal, I Mitliagkas, K Ahuja, Vincent, P. Compositional risk minimization. 22024</p>
<p>Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models. A M K Mamaghan, S Papa, K H Johansson, S Bauer, A Dittadi, 2024</p>
<p>L Matthey, I Higgins, D Hassabis, Lerchner, A. dSprites: Disentanglement testing sprites dataset. 2017</p>
<p>The role of Disentanglement in Generalisation. M L Montero, C J Ludwig, R P Costa, G Malhotra, J Bowers, International Conference on Learning Representations. 2020</p>
<p>Lost in Latent Space: Disentangled Models and the Challenge of Combinatorial Generalisation. M L Montero, J S Bowers, R P Costa, C J H Ludwig, G Malhotra, 2022</p>
<p>Learning to Compose Soft Prompts for Compositional Zero-Shot Learning. N V Nayak, P Yu, S H Bach, 2023</p>
<p>. M Oquab, T Darcet, T Moutakanni, H Vo, M Szafraniec, V Khalidov, P Fernandez, D Haziza, F Massa, A El-Nouby, M Assran, N Ballas, W Galuba, R Howes, P.-Y Huang, S.-W Li, I Misra, M Rabbat, V Sharma, G Synnaeve, H Xu, H Jegou, J Mairal, P Labatut, A Joulin, Bojanowski, 2024DINOv2: Learning Robust Visual Features without Supervision</p>
<p>The Geometry of Categorical and Hierarchical Concepts in Large Language Models. K Park, Y J Choe, Y Jiang, V Veitch, 2024</p>
<p>Learning Transferable Visual Models From Natural Language Supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, G Krueger, I Sutskever, 202115</p>
<p>Vision language models are blind. P Rahmanzadehgervi, L Bolton, M R Taesiri, A T Nguyen, 2024</p>
<p>From causal to concept-based representation learning. G Rajendran, S Buchholz, B Aragam, B Schölkopf, P Ravikumar, The Twelfth International Conference on Learning Representations (ICLR). 2024</p>
<p>Understanding Simplicity Bias towards Compositional Mappings via Learning Dynamics. Y Ren, D J Sutherland, 2024</p>
<p>Compositional Languages Emerge in a Neural Iterated Learning Model. Y Ren, S Guo, M Labeau, S B Cohen, S Kirby, 2020</p>
<p>Improving Compositional Generalization using Iterated Learning and Simplicial Embeddings. Y Ren, S Lavoie, M Galkin, D J Sutherland, A Courville, 2023</p>
<p>Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. S Sagawa, P W Koh, T B Hashimoto, P Liang, 2020</p>
<p>Visual Representation Learning Does Not Generalize Strongly Within the Same Domain. L Schott, J Von Kügelgen, F Träuble, P Gehler, C Russell, M Bethge, B Schölkopf, F Locatello, W Brendel, 2022</p>
<p>C Schuhmann, R Vencu, R Beaumont, R Kaczmarczyk, C Mullis, A Katta, T Coombes, J Jitsev, A Komatsuzaki, Laion-400m, Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs. 2021</p>
<p>On the rankability of visual embeddings. A Sonthalia, A Uselis, S J Oh, 2025</p>
<p>A Stein, A Naik, Y Wu, M Naik, E Wong, Towards Compositionality in Concept Learning. 202435</p>
<p>. A Stone, H Wang, M Stark, Y Liu, D S Phoenix, D George, Teaching Compositionality to CNNs. 22017</p>
<p>Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. S Tong, Z Liu, Y Zhai, Y Ma, Y Lecun, S Xie, 2024</p>
<p>H Touvron, L Martin, K Stone, P Albert, A Almahairi, Y Babaei, N Bashlykov, S Batra, P Bhargava, S Bhosale, D Bikel, L Blecher, C C Ferrer, M Chen, G Cucurull, D Esiobu, J Fernandes, J Fu, W Fu, B Fuller, C Gao, V Goswami, N Goyal, A Hartshorn, S Hosseini, R Hou, H Inan, M Kardas, V Kerkez, M Khabsa, I Kloumann, A Korenev, P S Koura, M.-A Lachaux, T Lavril, J Lee, D Liskovich, Y Lu, Y Mao, X Martinet, T Mihaylov, P Mishra, I Molybog, Y Nie, A Poulton, J Reizenstein, R Rungta, K Saladi, A Schelten, R Silva, E M Smith, R Subramanian, X E Tan, B Tang, R Taylor, A Williams, J X Kuan, P Xu, Z Yan, I Zarov, Y Zhang, A Fan, M Kambadur, S Narang, A Rodriguez, R Stojnic, S Edunov, Scialom, Open Foundation and Fine-Tuned Chat Models. 20232</p>
<p>Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. M Trager, P Perera, L Zancato, A Achille, P Bhatia, S Soatto, 2023415</p>
<p>V Udandarao, A Prabhu, A Ghosh, Y Sharma, P H S Torr, A Bibi, S Albanie, M Bethge, Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance. 2024</p>
<p>Intermediate layer classifiers for ood generalization. A Uselis, S J Oh, 2025</p>
<p>Deep learning generalizes because the parameterfunction map is biased towards simple functions. G Valle-Pérez, C Q Camargo, A A Louis, 2018</p>
<p>SPARO: Selective Attention for Robust and Compositional Transformer Encodings for Vision. A Vani, B Nguyen, S Lavoie, R Krishna, A Courville, 2024</p>
<p>Enhancing Compositional Generalization via Compositional Feature Alignment. H Wang, H Si, H Shao, H Zhao, 2024a</p>
<p>Learning Conditional Attributes for Compositional Zero-Shot Learning. Q Wang, L Liu, C Jing, H Chen, G Liang, P Wang, C Shen, IEEE. 32023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Vancouver, BC, Canada2023</p>
<p>Concept algebra for (score-based) text-controlled generative models. Z Wang, L Gui, J Negrea, V Veitch, 2024b</p>
<p>J C Whittington, W Dorrell, S Ganguli, T E Behrens, arXiv:2210.01768Disentanglement with biological constraints: A theory of functional cell types. 2022arXiv preprint</p>
<p>. T Wiedemer, J Brady, A Panfilov, A Juhos, M Bethge, W Brendel, 2023Provable Compositional Generalization for Object-Centric Learning</p>
<p>Pretraining Frequency Predicts Compositional Generalization of CLIP on Real-World Tasks. T Wiedemer, Y Sharma, A Prabhu, M Bethge, W Brendel, 2025</p>
<p>Zero-Shot Learning -A Comprehensive Evaluation of the Good, the Bad and the Ugly. Y Xian, C H Lampert, B Schiele, Z Akata, 2020</p>
<p>Skill-Mix: A Flexible and Expandable Family of Evaluations for AI models. D Yu, S Kaur, A Gupta, J Brown-Cohen, A Goyal, S Arora, 2023</p>
<p>When and why vision-language models behave like bags-of-words, and what to do about it?. M Yuksekgonul, F Bianchi, P Kalluri, D Jurafsky, J Zou, 202313</p>
<p>Anticipating Future Object Compositions without Forgetting. Y Zahran, G Burghouts, Y B Eisma, 2024</p>
<p>Investigating compositional challenges in visionlanguage models for visual grounding. Y Zeng, Y Huang, J Zhang, Z Jie, Z Chai, L Wang, 2023</p>
<p>A benchmark for compositional visual reasoning. A Zerroug, M Vaishnav, J Colin, S Musslick, T Serre, Advances in Neural Information Processing Systems. 2022</p>
<p>Can Models Learn Skill Composition from Examples?. H Zhao, S Kaur, D Yu, A Goyal, S Arora, 2024</p>
<p>Data factors for better compositional generalization. C Zhou, P Chen, B Liu, X Li, C Zhang, H Huang, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>ArtVLM: Attribute Recognition Through Vision-Based Prefix Language Modeling. W Y Zhu, K Ye, J Ke, J Yu, L Guibas, P Milanfar, F Yang, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
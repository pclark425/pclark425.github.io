<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1501 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1501</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1501</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-dc4b654b1a9688ead7ae67f02afe9d22be4a43d3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dc4b654b1a9688ead7ae67f02afe9d22be4a43d3" target="_blank">Grounded Action Transformation for Robot Learning in Simulation</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> A new algorithm for learning in simulation — Grounded Action Transformation — is proposed and applied to learning of humanoid bipedal locomotion and results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk.</p>
                <p><strong>Paper Abstract:</strong> 
 
 Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. This paper proposes a new algorithm for learning in simulation — Grounded Action Transformation — and applies it to learning of humanoid bipedal locomotion. Our approach results in a 43.27% improvement in forward walk velocity compared to a state-of-the art hand-coded walk.
 
</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1501.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1501.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SimSpark</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SimSpark simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source physics simulator (used here as the lower-fidelity training environment) based on the Open Dynamics Engine that enables fast simulation of the SoftBank NAO robot but models joint actions as achieving commanded angles almost instantaneously.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>SimSpark</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Open-source robotics simulator using the Open Dynamics Engine (ODE) for physics; used to simulate the SoftBank NAO robot with relatively fast execution and simplified/more-reactive joint dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (bipedal locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-fidelity (fast simulation with simplified/reactive joint dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Uses ODE for rigid-body physics; actions in SimSpark achieve desired joint angles nearly instantaneously (more reactive than real hardware); faster runtime enabling many simulated trajectories; simplified contact/action delay modeling relative to real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>NAO walk policy (UNSW walk engine parameters as θ0) and CMA-ES optimized policies; GAT forward/inverse dynamics nets</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement learning / policy-search: deterministic parameterized walk policy (θ) initialized from UNSW walk engine; optimized with CMA-ES in simulation. Additionally, GAT trains two supervised neural nets (forward model f and simulator inverse model f_sim^{-1}) to build an action-transformation g.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn and optimize bipedal walking control policies for the NAO robot (maximize forward walk velocity while remaining stable).</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>When training in SimSpark and transferring to Gazebo (treated as surrogate real-world), GAT found an average maximum improvement of 22.48% (Table 1) compared to baseline; CMA-ES optimization in simulation used ~30,000 simulated trajectories per trial/generation setup. (If interpreted as pure in-sim optimization, the paper reports simulated optimization necessary: 30,000 trajectories per trial.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Higher-fidelity Gazebo simulator (in one experiment) and the physical SoftBank NAO robot (in final experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>SimSpark -> Gazebo: GAT average max improvement 22.48% (vs Noise-Envelope 18.93% and No Ground 11.094%), failures: GAT 1/10 trials, No Ground 7/10, Noise-Envelope 5/10. SimSpark -> physical NAO: after two GAT iterations, walk velocity increased from 19.52 cm/s to 27.97 cm/s (43.27% improvement) using policies optimized in SimSpark and grounded with GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>SimSpark (low-fidelity) was used as the grounded-simulator training substrate and, when combined with GAT, produced better transfer than ungrounded training or simple noise-envelope methods. GAT enabled policies optimized in low-fidelity SimSpark to transfer successfully to both Gazebo and the physical robot, outperforming baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper emphasizes that specific fidelity aspects (action reactivity/delays) matter: SimSpark's near-instantaneous action assumption does not match real hardware and breaks some prior grounding methods (GUIDED GSL). The authors do not state an absolute minimal fidelity but argue that correct modeling of how actions change the robot's configuration (action timing/reactivity, contact dynamics) is important for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Ungrounded SimSpark led to rapid overfitting and frequent failures (No Ground: 7/10 failures). GAT sometimes fails if the action-modification assumption breaks (e.g., contact dynamics where desired transitions cannot be achieved by any simulated action); distribution shift as policies change can reduce the effectiveness of learned forward/inverse models, requiring re-grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Action Transformation for Robot Learning in Simulation', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1501.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1501.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gazebo simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source, higher-fidelity robotics simulator provided by the Open Source Robotics Foundation used as a more realistic environment (and in one experiment as a surrogate for the real world) that models robot joints and physics more accurately than SimSpark at higher computational cost.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Gazebo</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Popular open-source robotics simulator with more realistic physics modeling (relative to SimSpark), used here to simulate the SoftBank NAO; higher computational cost but models joints as less-reactive than SimSpark and closer to real hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics (bipedal locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>higher-fidelity (relative to SimSpark) but still an approximation of the physical robot</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>More realistic joint dynamics than SimSpark (joints less reactive than SimSpark but still more reactive than the physical robot); higher computational cost; uses continuous physics integration and models contacts and delays more faithfully than SimSpark; simulation timestep/episode durations used in experiments (Gazebo trajectories up to 10s).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>NAO walk policy (UNSW walk engine parameters as θ0) and CMA-ES optimized policies; GAT forward/inverse dynamics nets when Gazebo is target or used as E.</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same as for SimSpark: deterministic parameterized walk policy (θ) optimized with CMA-ES in a grounded simulator; GAT trains supervised neural nets to transform actions in the training simulator so policies behave similarly when evaluated in Gazebo/real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Evaluate transfer of policies learned in lower-fidelity simulator (SimSpark) to a higher-fidelity simulator (Gazebo) serving as surrogate real world, and to act as a target for transfer evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td>When Gazebo was used as the target environment (SimSpark -> Gazebo experiments), GAT produced an average maximum improvement of 22.48% in Gazebo vs baselines (Table 1). Specific in-sim training metrics in Gazebo alone are not enumerated beyond transfer results.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Physical SoftBank NAO robot (in other experiments) and evaluation when treated as the surrogate real-world target for SimSpark-trained policies.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>GAT trained with Gazebo as the simulator also improved physical NAO walk velocity by over 30% (paper states 'GAT with SimSpark and GAT with Gazebo both improved walk velocity by over 30%'); exact numeric percent for Gazebo->NAO not explicitly given. When used as transfer target (SimSpark->Gazebo), GAT achieved 22.48% average improvement in Gazebo.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Gazebo (higher-fidelity) as a target results in better-aligned evaluation than SimSpark; GAT provided superior transfer relative to noise-injection and ungrounded approaches across both simulators, showing method generality across fidelity levels. Unmodified simulators overfit quickly; Noise-Envelope was more robust than no grounding but inferior to GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>The paper notes Gazebo models joints more realistically than SimSpark but still more reactive than the real robot; while not prescribing a minimal fidelity, the discussion highlights that correct modeling of action timing/delays and contact dynamics is critical for successful transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Even with higher-fidelity Gazebo, grounding is local to the training policy's state distribution; as policies change, the learned transformations may become less accurate and require re-grounding. The paper reports that noise-injection in Gazebo helps avoid overfitting but does not match the transfer improvements achieved with GAT.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Action Transformation for Robot Learning in Simulation', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1501.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1501.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Open Dynamics Engine (ODE)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Open Dynamics Engine</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A physics engine for simulating rigid-body dynamics used by SimSpark (mentioned as the underlying physics implementation); contributes to the simulator's capabilities and limitations regarding action reactivity and contact modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>Open Dynamics Engine (ODE)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>A general-purpose rigid-body physics engine used by SimSpark to simulate dynamics, collisions and contacts; influences fidelity and computational characteristics of the SimSpark environment.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / physics simulation</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>component physics engine; fidelity depends on how the engine is configured and how the higher-level simulator uses it (in this work, contributes to SimSpark's fast but relatively simplified dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides rigid-body integration and collision/contact modeling; in SimSpark usage here results in near-instantaneous joint responses as configured, and simpler modeling of action delays compared to the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>n/a (physics engine component rather than trained agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>n/a (component used within SimSpark, which was used as training simulator for transfer experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Mentioned as part of the simulation stack; paper attributes some of SimSpark's characteristics (fast, highly reactive joints) to the simulator setup which uses ODE.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Implicitly, usage/configuration of ODE within SimSpark contributes to unrealistic instantaneous-action behavior that contradicted physics on the real robot and broke assumptions of some grounding algorithms (e.g., GUIDED GSL).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Grounded Action Transformation for Robot Learning in Simulation', 'publication_date_yy_mm': '2017-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Humanoid robots learning to walk faster: From the real world to simulation and back <em>(Rating: 2)</em></li>
                <li>Transfer from simulation to real world through learning deep inverse dynamics model <em>(Rating: 2)</em></li>
                <li>Noise and the reality gap: The use of simulation in evolutionary robotics <em>(Rating: 2)</em></li>
                <li>Reinforcement learning with multi-fidelity simulators <em>(Rating: 2)</em></li>
                <li>EPOpt: Learning robust neural network policies using model ensembles <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1501",
    "paper_id": "paper-dc4b654b1a9688ead7ae67f02afe9d22be4a43d3",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "SimSpark",
            "name_full": "SimSpark simulator",
            "brief_description": "An open-source physics simulator (used here as the lower-fidelity training environment) based on the Open Dynamics Engine that enables fast simulation of the SoftBank NAO robot but models joint actions as achieving commanded angles almost instantaneously.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "SimSpark",
            "simulator_description": "Open-source robotics simulator using the Open Dynamics Engine (ODE) for physics; used to simulate the SoftBank NAO robot with relatively fast execution and simplified/more-reactive joint dynamics.",
            "scientific_domain": "mechanics / robotics (bipedal locomotion)",
            "fidelity_level": "low-fidelity (fast simulation with simplified/reactive joint dynamics)",
            "fidelity_characteristics": "Uses ODE for rigid-body physics; actions in SimSpark achieve desired joint angles nearly instantaneously (more reactive than real hardware); faster runtime enabling many simulated trajectories; simplified contact/action delay modeling relative to real robot.",
            "model_or_agent_name": "NAO walk policy (UNSW walk engine parameters as θ0) and CMA-ES optimized policies; GAT forward/inverse dynamics nets",
            "model_description": "Reinforcement learning / policy-search: deterministic parameterized walk policy (θ) initialized from UNSW walk engine; optimized with CMA-ES in simulation. Additionally, GAT trains two supervised neural nets (forward model f and simulator inverse model f_sim^{-1}) to build an action-transformation g.",
            "reasoning_task": "Learn and optimize bipedal walking control policies for the NAO robot (maximize forward walk velocity while remaining stable).",
            "training_performance": "When training in SimSpark and transferring to Gazebo (treated as surrogate real-world), GAT found an average maximum improvement of 22.48% (Table 1) compared to baseline; CMA-ES optimization in simulation used ~30,000 simulated trajectories per trial/generation setup. (If interpreted as pure in-sim optimization, the paper reports simulated optimization necessary: 30,000 trajectories per trial.)",
            "transfer_target": "Higher-fidelity Gazebo simulator (in one experiment) and the physical SoftBank NAO robot (in final experiments).",
            "transfer_performance": "SimSpark -&gt; Gazebo: GAT average max improvement 22.48% (vs Noise-Envelope 18.93% and No Ground 11.094%), failures: GAT 1/10 trials, No Ground 7/10, Noise-Envelope 5/10. SimSpark -&gt; physical NAO: after two GAT iterations, walk velocity increased from 19.52 cm/s to 27.97 cm/s (43.27% improvement) using policies optimized in SimSpark and grounded with GAT.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "SimSpark (low-fidelity) was used as the grounded-simulator training substrate and, when combined with GAT, produced better transfer than ungrounded training or simple noise-envelope methods. GAT enabled policies optimized in low-fidelity SimSpark to transfer successfully to both Gazebo and the physical robot, outperforming baselines.",
            "minimal_fidelity_discussion": "The paper emphasizes that specific fidelity aspects (action reactivity/delays) matter: SimSpark's near-instantaneous action assumption does not match real hardware and breaks some prior grounding methods (GUIDED GSL). The authors do not state an absolute minimal fidelity but argue that correct modeling of how actions change the robot's configuration (action timing/reactivity, contact dynamics) is important for transfer.",
            "failure_cases": "Ungrounded SimSpark led to rapid overfitting and frequent failures (No Ground: 7/10 failures). GAT sometimes fails if the action-modification assumption breaks (e.g., contact dynamics where desired transitions cannot be achieved by any simulated action); distribution shift as policies change can reduce the effectiveness of learned forward/inverse models, requiring re-grounding.",
            "uuid": "e1501.0",
            "source_info": {
                "paper_title": "Grounded Action Transformation for Robot Learning in Simulation",
                "publication_date_yy_mm": "2017-02"
            }
        },
        {
            "name_short": "Gazebo",
            "name_full": "Gazebo simulator",
            "brief_description": "An open-source, higher-fidelity robotics simulator provided by the Open Source Robotics Foundation used as a more realistic environment (and in one experiment as a surrogate for the real world) that models robot joints and physics more accurately than SimSpark at higher computational cost.",
            "citation_title": "",
            "mention_or_use": "use",
            "simulator_name": "Gazebo",
            "simulator_description": "Popular open-source robotics simulator with more realistic physics modeling (relative to SimSpark), used here to simulate the SoftBank NAO; higher computational cost but models joints as less-reactive than SimSpark and closer to real hardware.",
            "scientific_domain": "mechanics / robotics (bipedal locomotion)",
            "fidelity_level": "higher-fidelity (relative to SimSpark) but still an approximation of the physical robot",
            "fidelity_characteristics": "More realistic joint dynamics than SimSpark (joints less reactive than SimSpark but still more reactive than the physical robot); higher computational cost; uses continuous physics integration and models contacts and delays more faithfully than SimSpark; simulation timestep/episode durations used in experiments (Gazebo trajectories up to 10s).",
            "model_or_agent_name": "NAO walk policy (UNSW walk engine parameters as θ0) and CMA-ES optimized policies; GAT forward/inverse dynamics nets when Gazebo is target or used as E.",
            "model_description": "Same as for SimSpark: deterministic parameterized walk policy (θ) optimized with CMA-ES in a grounded simulator; GAT trains supervised neural nets to transform actions in the training simulator so policies behave similarly when evaluated in Gazebo/real robot.",
            "reasoning_task": "Evaluate transfer of policies learned in lower-fidelity simulator (SimSpark) to a higher-fidelity simulator (Gazebo) serving as surrogate real world, and to act as a target for transfer evaluation.",
            "training_performance": "When Gazebo was used as the target environment (SimSpark -&gt; Gazebo experiments), GAT produced an average maximum improvement of 22.48% in Gazebo vs baselines (Table 1). Specific in-sim training metrics in Gazebo alone are not enumerated beyond transfer results.",
            "transfer_target": "Physical SoftBank NAO robot (in other experiments) and evaluation when treated as the surrogate real-world target for SimSpark-trained policies.",
            "transfer_performance": "GAT trained with Gazebo as the simulator also improved physical NAO walk velocity by over 30% (paper states 'GAT with SimSpark and GAT with Gazebo both improved walk velocity by over 30%'); exact numeric percent for Gazebo-&gt;NAO not explicitly given. When used as transfer target (SimSpark-&gt;Gazebo), GAT achieved 22.48% average improvement in Gazebo.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Gazebo (higher-fidelity) as a target results in better-aligned evaluation than SimSpark; GAT provided superior transfer relative to noise-injection and ungrounded approaches across both simulators, showing method generality across fidelity levels. Unmodified simulators overfit quickly; Noise-Envelope was more robust than no grounding but inferior to GAT.",
            "minimal_fidelity_discussion": "The paper notes Gazebo models joints more realistically than SimSpark but still more reactive than the real robot; while not prescribing a minimal fidelity, the discussion highlights that correct modeling of action timing/delays and contact dynamics is critical for successful transfer.",
            "failure_cases": "Even with higher-fidelity Gazebo, grounding is local to the training policy's state distribution; as policies change, the learned transformations may become less accurate and require re-grounding. The paper reports that noise-injection in Gazebo helps avoid overfitting but does not match the transfer improvements achieved with GAT.",
            "uuid": "e1501.1",
            "source_info": {
                "paper_title": "Grounded Action Transformation for Robot Learning in Simulation",
                "publication_date_yy_mm": "2017-02"
            }
        },
        {
            "name_short": "Open Dynamics Engine (ODE)",
            "name_full": "Open Dynamics Engine",
            "brief_description": "A physics engine for simulating rigid-body dynamics used by SimSpark (mentioned as the underlying physics implementation); contributes to the simulator's capabilities and limitations regarding action reactivity and contact modeling.",
            "citation_title": "",
            "mention_or_use": "mention",
            "simulator_name": "Open Dynamics Engine (ODE)",
            "simulator_description": "A general-purpose rigid-body physics engine used by SimSpark to simulate dynamics, collisions and contacts; influences fidelity and computational characteristics of the SimSpark environment.",
            "scientific_domain": "mechanics / physics simulation",
            "fidelity_level": "component physics engine; fidelity depends on how the engine is configured and how the higher-level simulator uses it (in this work, contributes to SimSpark's fast but relatively simplified dynamics)",
            "fidelity_characteristics": "Provides rigid-body integration and collision/contact modeling; in SimSpark usage here results in near-instantaneous joint responses as configured, and simpler modeling of action delays compared to the real robot.",
            "model_or_agent_name": "n/a (physics engine component rather than trained agent)",
            "model_description": "n/a",
            "reasoning_task": "n/a",
            "training_performance": null,
            "transfer_target": "n/a (component used within SimSpark, which was used as training simulator for transfer experiments)",
            "transfer_performance": null,
            "compares_fidelity_levels": null,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Mentioned as part of the simulation stack; paper attributes some of SimSpark's characteristics (fast, highly reactive joints) to the simulator setup which uses ODE.",
            "failure_cases": "Implicitly, usage/configuration of ODE within SimSpark contributes to unrealistic instantaneous-action behavior that contradicted physics on the real robot and broke assumptions of some grounding algorithms (e.g., GUIDED GSL).",
            "uuid": "e1501.2",
            "source_info": {
                "paper_title": "Grounded Action Transformation for Robot Learning in Simulation",
                "publication_date_yy_mm": "2017-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Humanoid robots learning to walk faster: From the real world to simulation and back",
            "rating": 2
        },
        {
            "paper_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "rating": 2
        },
        {
            "paper_title": "Noise and the reality gap: The use of simulation in evolutionary robotics",
            "rating": 2
        },
        {
            "paper_title": "Reinforcement learning with multi-fidelity simulators",
            "rating": 2
        },
        {
            "paper_title": "EPOpt: Learning robust neural network policies using model ensembles",
            "rating": 1
        }
    ],
    "cost": 0.01140925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Grounded Action Transformation for Robot Learning in Simulation</h1>
<p>Josiah P. Hanna, Peter Stone<br>Dept. of Computer Science<br>The University of Texas at Austin<br>Austin, TX 78712 USA<br>{jphanna,pstone}@cs.utexas.edu</p>
<h4>Abstract</h4>
<p>Robot learning in simulation is a promising alternative to the prohibitive sample cost of learning in the physical world. Unfortunately, policies learned in simulation often perform worse than hand-coded policies when applied on the physical robot. Grounded simulation learning (GSL) promises to address this issue by altering the simulator to better match the real world. This paper proposes a new algorithm for GSL - Grounded Action Transformation - and applies it to learning of humanoid bipedal locomotion. Our approach results in a $43.27 \%$ improvement in forward walk velocity compared to a state-of-the art hand-coded walk. We further evaluate our methodology in controlled experiments using a second, higher-fidelity simulator in place of the real world. Our results contribute to a deeper understanding of grounded simulation learning and demonstrate its effectiveness for learning robot control policies.</p>
<h2>Introduction</h2>
<p>Manually designing control policies for every possible situation a robot could encounter is impractical. Reinforcement learning (RL) provides a promising alternative to handcoding skills. Recent applications of RL to high dimensional control tasks have seen impressive successes within simulation (Schulman et al. 2015; Lillicrap et al. 2015). Unfortunately, a large gap exists between what is possible in simulation and the reality of robot learning. State-of-the-art learning methods require thousands of episodes of experience which is impractical for a physical robot. Aside from the time it would take, collecting the required training data may lead to substantial wear on the robot. Furthermore, as the robot explores different policies it may execute unsafe actions which could damage the robot.</p>
<p>An alternative to learning directly on the robot is learning in simulation (Cutler and How 2015; Koos, Mouret, and Doncieux 2010). Simulation is a valuable tool for robotics research as execution of a robotic skill in simulation is comparatively easier than real world execution. However, the value of simulation learning is limited by the inherent inaccuracy of simulators in modeling the dynamics of the physical world (Kober, Bagnell, and Peters 2013). As a result, learning that takes place in a simulator is unlikely to improve real world performance.</p>
<p>Copyright (C) 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.</p>
<p>Grounded Simulation Learning (GSL) is a framework for learning with a simulator in which the simulator is modified with data from the physical robot, learning takes place in simulation, the new policy is evaluated on the robot, and data from the new policy is used to further modify the simulator (Farchy et al. 2013). The paper introducing GSL demonstrates the effectiveness of the method in a single, limited experiment, by increasing the forward walking velocity of a slow, stable bipedal walk by $26.7 \%$.</p>
<p>This paper introduces a new algorithm - Grounded Action Transformation (GAT) - for simulator grounding within the GSL framework. The algorithm is fully implemented and evaluated using a high-fidelity simulator as a surrogate for the real world. The results of this study contribute to a deeper understanding of transfer from simulation methods and the effectiveness of GAT. In contrast to prior work, our real-world evaluation of GAT starts from a state-of-the-art walk engine as the base policy, and nonetheless is able to improve the walk velocity by over $43 \%$, leading to what may be the fastest known walk on the SoftBank NAO robot.</p>
<h2>Background</h2>
<h2>Preliminaries</h2>
<p>We formalize robot skill learning as a reinforcement learning (RL) problem (Sutton and Barto 1998). At discrete time-step $t$ the robot takes action $A_{t} \sim \pi\left(\cdot \mid S_{t}\right)$ according to a policy, $\pi$, which is a distribution over actions, $A_{t} \in \mathcal{A}$, conditioned on the current state, $S_{t} \in \mathcal{S}$. The environment, $E$, responds with $S_{t+1} \sim P\left(\cdot \mid S_{t}, A_{t}\right)$ according to the dynamics, $P$ : $\mathcal{S} \times \mathcal{A} \times \mathcal{S} \rightarrow \mathbb{R}<em 0="0">{\geq 0}$ which is a probability density function over next states conditioned on the current state and action. A trajectory of length $L$ is a sequence of states and actions, $\tau:=S</em>}, A_{0}, \ldots, S_{L}, A_{L}$. We also define a cost function, $c$, which assigns a scalar cost to each $(s, a)$ pair. We denote the value of policy, $\pi$, as $J(\pi):=\mathbb{E<em t="0">{\tau \sim \operatorname{Pr}(\cdot \mid \pi)}\left[\sum</em>(\tau \mid \pi)$ is the probability of $\tau$ when selecting actions according to $\pi$.}^{L} c\left(S_{t}, A_{t}\right)\right]$ where $\operatorname{Pr</p>
<p>We assume $\pi$ is parameterized by a vector $\boldsymbol{\theta}$ and denote the parameterized policy as $\pi_{\boldsymbol{\theta}}$. Since $\boldsymbol{\theta}$ determines the policy distribution we overload notation by referring to $\pi_{\boldsymbol{\theta}}$ as $\boldsymbol{\theta}$. Given initial parameters $\boldsymbol{\theta}<em 0="0">{0}$, the goal of policy improvement is to find $\boldsymbol{\theta}^{\prime}$ such that $J\left(\boldsymbol{\theta}^{\prime}\right)&lt;J\left(\boldsymbol{\theta}</em>$ is a deterministic policy that maps state observations to an action}\right)$. In this work, $\boldsymbol{\theta</p>
<p>vector $\mathbf{a}<em t="t">{t}$. Each component of $\mathbf{a}</em>}, \mathbf{a<em t="t">{t}^{i}$, is the desired joint angle for degree-of-freedom $i$. At each time-step, $t, \boldsymbol{\theta}$ selects $\mathbf{a}</em>}$ according to the robot's configuration in joint space, $\mathbf{x<em t="t">{t}$, high level intention commands (e.g., walk forward at $75 \%$ of maximum velocity), $\boldsymbol{\omega}</em>}$, and a sensor vector, $\boldsymbol{\psi<em t="t">{t}$, of inertial measurement (IMU) and foot sensor readings. The state of the robot can be fully described as $\mathbf{s}</em>}=\left\langle\mathbf{x<em t="t">{t}, \dot{\mathbf{x}}</em>}, \ddot{\mathbf{x}<em t="t">{t}, \boldsymbol{\omega}</em>}, \boldsymbol{\psi<em t="t">{t}\right\rangle$ where $\dot{\mathbf{x}}</em>}$ and $\ddot{\mathbf{x}<em t="t">{t}$ are the time derivatives of $\mathbf{x}</em>}$. Since the robot only observes $\mathbf{x<em t="t">{t}, \boldsymbol{\omega}</em>$, these directional commands are state features.}$, and $\boldsymbol{\psi}_{t}, E$ is partially observable. Note that while the high level intention commands are determined by the robot, from the point-of-view of $\boldsymbol{\theta</p>
<p>In this paper, learning takes place in a simulator which is an environment, $E_{\text {sim }}$, that models $E$. Specifically $E_{\text {sim }}$ has the same state-action space as $E$ but inevitably a different dynamics distribution, $P_{\text {sim }}$. In many robotics settings, $c$ is user defined and thus is identical for $E$ and $E_{\text {sim }}$. However, the different dynamics distribution mean that for any policy, $\boldsymbol{\theta}$, $J(\boldsymbol{\theta}) \neq J_{\text {sim }}(\boldsymbol{\theta})$ since $\boldsymbol{\theta}$ induces a different trajectory distribution in $E$ than in $E_{\text {sim }}$. Thus $\boldsymbol{\theta}^{\prime}$ with $J_{\text {sim }}\left(\boldsymbol{\theta}^{\prime}\right)&lt;J_{\text {sim }}\left(\boldsymbol{\theta}<em 0="0">{0}\right)$ does not imply $J\left(\boldsymbol{\theta}^{\prime}\right)&lt;J\left(\boldsymbol{\theta}</em>}\right)$ - in fact $J\left(\boldsymbol{\theta}^{\prime}\right)$ could even be worse than $J\left(\boldsymbol{\theta<em _sim="{sim" _text="\text">{0}\right)$. In practice and in the literature, learning in a simulator frequently leads to catastrophic degradation of $J$. This paper explores methods for learning in $E</em>$ that result in lower policy cost.}</p>
<h2>Related Work</h2>
<p>This section surveys previous research in simulation-transfer methods. Our work also relates to model-based reinforcement learning, however, we restrict our attention here to methods directly concerned with learning in simulation.</p>
<p>One approach to simulation-transfer is using experience in simulation to reduce learning time on the physical robot. Cutler et al. (2014) use lower fidelity simulators to narrow the action search space for faster learning in higher fidelity simulators or the real world. Cully et al. (2015) use a simulator to estimate fitness values for low-dimensional robot behaviors which gives the robot prior knowledge of how to adapt its behavior if it becomes damaged during real world operation. Cutler et al. (2015) use experience in simulation to estimate a prior for a Gaussian process model to be used with the PILCO learning algorithm (Deisenroth and Rasmussen 2011). Rusu et al. (2016a; 2016b) introduce progressive neural network policies which are initially trained in simulation before a final period of learning in the true environment. In contrast to these methods, our method performs all learning in a grounded simulator and only uses the physical robot for policy evaluation.</p>
<p>Another class of simulation-transfer methods optimize an objective besides $J_{\text {sim }}$ in simulation. Rajeswaran et al. (2016) use a set of different simulators to learn robust policies that can perform well in a variety of environments. Koos et al. (2010) use multi-objective optimization to find policies that trade off between optimizing $J_{\text {sim }}(\pi)$ and transferability. Iocchi et al. (2007) compute a correction function from $J(\pi)$ such that $J_{\text {sim }}(\pi)=J(\pi)$ and then optimize the corrected objective. In contrast, we directly optimize $J_{\text {sim }}$ within a modified simulator.</p>
<p>Christiano et al. (2016) turn simulation policies into real
world policies by transforming policy actions so that they produce the same effect that they did in simulation. This approach is complementary to ours although it also requires a simulator which can be reset to an arbitrary state during policy execution.</p>
<h2>Grounded Simulation Learning</h2>
<p>In this section we introduce the Grounded Simulation Learning (GSL) framework as presented by Farchy et al. (2013). GSL improves robot learning in simulation by using state transition data from the physical system to modify $E_{\text {sim }}$ such that the modified $E_{\text {sim }}$ is a higher fidelity model of $E$. The process of making the simulator more like the real world is referred to as grounding.</p>
<p>The GSL framework assumes the following:</p>
<ol>
<li>There is an imperfect simulator, $E_{\text {sim }}=\left\langle\mathcal{S}, \mathcal{A}, P_{\text {sim }}, c\right\rangle$, that models the environment of interest, $E$. Furthermore, $E_{\text {sim }}$ must be modifiable. A modifiable simulator has parameterized transition probabilities $P_{\phi}(\cdot \mid s, a):=$ $P_{\text {sim }}(\cdot \mid s, a ; \phi)$ where the vector $\phi$ can be changed to produce, in effect, a different simulator.</li>
<li>Additionally, GSL assumes the physical robot can record a data set, $\mathcal{D}$ of trajectories when executing any policy $\boldsymbol{\theta}$.</li>
<li>Finally, GSL requires a policy improvement routine, optimize, that searches for $\boldsymbol{\theta}$ which decreases $J_{\text {sim }}(\boldsymbol{\theta})$. The optimize routine returns a set of candidate policies, $\Pi_{c}$ to evaluate on the physical robot.
Let $d(p, q)$ be a measure of similarity between probabilities $p$ and $q$. GSL grounds $E_{\text {sim }}$ by finding $\phi^{*}$ such that:</li>
</ol>
<p>$$
\phi^{*}=\underset{\phi}{\operatorname{argmin}} \sum_{\tau \in \mathcal{D}} d\left(\operatorname{Pr}(\tau \mid \boldsymbol{\theta}), \operatorname{Pr}_{\text {sim }}(\tau \mid \boldsymbol{\theta}, \boldsymbol{\phi})\right)
$$</p>
<p>where $\operatorname{Pr}(\tau \mid \boldsymbol{\theta})$ is the probability of observing trajectory $\tau$ on the physical robot and $\operatorname{Pr}_{\text {sim }}(\tau \mid \boldsymbol{\theta}, \boldsymbol{\phi})$ is the probability of $\tau$ in the simulator with dynamics parameterized by $\phi$. For instance, if $d(p, q):=\log p-\log q$ for two probabilities $p$ and $q$ then $\phi^{<em>}$ minimizes the Kullback-Leibler divergence between $\operatorname{Pr}(\cdot \mid \boldsymbol{\theta})$ and $\operatorname{Pr}_{\text {sim }}\left(\cdot \mid \boldsymbol{\theta}, \boldsymbol{\phi}^{</em>}\right)$.</p>
<p>Assuming GSL can solve (1), the framework is as follows:</p>
<ol>
<li>Execute policy $\boldsymbol{\theta}_{0}$ on the physical robot to collect a data set of trajectories, $\mathcal{D}$.</li>
<li>Use $\mathcal{D}$ to find $\phi^{*}$ that satisfies Equation 1.</li>
<li>Use optimize with $J_{\text {sim }}$ and $P_{\phi^{*}}$ to learn a set of candidate policies $\Pi_{c}$ in simulation which are expected to perform well on the physical robot.</li>
<li>Evaluate each proposed $\boldsymbol{\theta}<em c="c">{c} \in \Pi</em>}$ on the physical robot and return the policy, $\boldsymbol{\theta<em 1="1">{1}$, with minimal $J$.
GSL can be applied iteratively with $\boldsymbol{\theta}</em>}$ being used to collect more trajectories to ground the simulator again before learning $\boldsymbol{\theta<em 0="0">{2}$. The re-grounding step is necessary since changes to $\boldsymbol{\theta}$ result in changes to the distribution of states that the robot observes. When this happens, a simulator that has been modified with data from the state distribution of $\boldsymbol{\theta}</em>$.}$ may be a poor model under the state distribution of $\boldsymbol{\theta}_{1</li>
</ol>
<p>Farchy et al. proposed a GSL algorithm, which we refer to as GUIDED GSL. GUIDED GSL blends simulator modification with human guided policy optimization. This algorithm demonstrated the efficacy of GSL in optimizing forward walk velocity of a bipedal robot. The robot in that research began learning with a slow but stable walk policy. Four iterations of GUIDED GSL led to an improvement of over $26 \%$ on this baseline walk. Two limitations of GUIDED GSL are that:</p>
<ol>
<li>It makes the assumption that actions in simulation achieve their desired effect instantaneously.</li>
<li>It required a human expert to manually select which components of $\boldsymbol{\theta}$ were allowed to change between iterations of the optimize routine.
In this work, we introduce an enhanced GSL methodology that eliminates both assumptions and optimizes one of the fastest available NAO walk engines.</li>
</ol>
<h2>Robot Platform and Task</h2>
<p>Before presenting GAT, our new GSL algorithm, we describe the robot platform, initial walk policy, and simulators which we use for evaluation. While our method is applicable to other robots, tasks, and simulators we describe these components first in order to ground the algorithm's presentation.</p>
<p>Robot Platform: Our target task is bipedal walking using the SoftBank NAO. ${ }^{1}$ The NAO is a humanoid robot with 25 degrees of freedom (See Figure 2a). For walking, our NAO uses an open source walk engine developed at the University of New South Wales (UNSW) (Ashar et al. 2015; Hall et al. 2016). This walk engine has been used by at least one team in each of the past three RoboCup Standard Platform League (SPL) championship games in which teams of five NAOs compete in soccer matches. To the best of our knowledge, it is one of the fastest open source walks available. ${ }^{2}$ The walk is a zero moment point walk based on an inverted pendulum model. The walk is closed loop, using the inertial measurement (IMU) sensors and a learned ankle control policy to improve stability. The UNSW walk engine has 15 parameters that determine features of the walk (e.g., step height, pendulum model height). The values of the parameters from the open source release constitute $\boldsymbol{\theta}_{0}$. For full information on the UNSW walk see (Hengst 2014).</p>
<p>Simulators: We use two different simulators in this work. The first is the open source SimSpark simulator. ${ }^{3}$ The simulator simulates realistic physics with the Open Dynamics Engine. ${ }^{4}$ This simulator was also used in the work introducing GSL (Farchy et al. 2013). We also use the Gazebo simulator ${ }^{5}$ provided by the Open Source Robotics Foundation. ${ }^{6}$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Gazebo is an open source simulator that is commonly used in robotics research. SimSpark enables fast simulation but is a lower fidelity model of the real world. Gazebo enables high fidelity simulation with an additional computational cost. In one of our experiments we consider the more realistic Gazebo environment as $E$ and SimSpark as $E_{\text {sin }}$. The main difference between these simulators and the physical robot is how actions change the robot's configuration. In SimSpark, actions achieve the desired command angle almost instantaneously. On the physical robot there is a delay. Gazebo also models joints as more reactive than the real world although less reactive than in SimSpark.</p>
<h2>Grounded Action Transformation</h2>
<p>We now introduce our principle algorithmic contribution, GSL with Grounded Action Transformation (GAT) which improves the grounding step (step 2) of the GSL framework. GAT improves grounding by correcting differences in the effects of actions between $E$ and $E_{\text {sin }}$. GSL with GAT is presented in Algorithm 1.</p>
<p>The GSL framework grounds the simulator by finding $\phi^{*}$ that satisfies (1). Since it is often easier to minimize error in the one step dynamics distribution than error in the trajectory distributions, GAT uses:</p>
<p>$$
\phi^{*}=\underset{\phi}{\operatorname{argmin}} \sum_{\tau_{i} \in \mathcal{D}} \sum_{t=0}^{L} d\left(P\left(\mathbf{s}<em t="t">{t+1}^{i} \mid \mathbf{s}</em>}^{i}, \mathbf{a<em _phi="\phi">{t}^{i}\right), P</em>}\left(\mathbf{s<em t="t">{t+1}^{i} \mid \mathbf{s}</em>\right)\right)
$$}^{i}, \mathbf{a}_{t}^{i</p>
<p>as a surrogate loss function which can be minimized with transitions observed in $\mathcal{D}$. GSL with GAT begins by collecting the dataset $\mathcal{D}$ (Algorithm 1, line 4).</p>
<p>Physics-based simulators (such as SimSpark and Gazebo) have a large number of parameters determining the physics of the simulated environment (e.g., friction coefficients). However, using these parameters as $\phi$ is not amenable to numerical optimization of (2). To find $\phi^{*}$ efficiently, GAT uses a parameterized action transformation function which takes the agent's state and action as input and outputs a new action which - when taken in simulation - will result in the robot transitioning to the same next state it would have in $E$. We denote this function, $g_{\phi}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{A}$; the parameters of $g$ serve as $\phi$ under the GSL framework. GAT constructs $g$ with parameterized models of the robot's dynamics and the simulator's inverse dynamics. Assuming the simulated robot can record a dataset $\mathcal{D}_{\text {sin }}$ of experience like the physical robot, GAT reduces (2) to a supervised learning problem.</p>
<p>GAT defines $g:=g_{\phi}$ by a deterministic forward model of the robot's dynamics, $f$ and a deterministic model of the simulator's inverse dynamics, $f_{\text {sin }}^{-1}$. The function, $f$ maps $\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right)$ to the maximum likelihood estimate of $\mathbf{x<em _sin="{sin" _text="\text">{t+1}$ under $P$. The function $f</em>}}^{-1}$ maps $\left(\mathbf{s<em t_1="t+1">{t}, \mathbf{s}</em>}\right)$ to the action that is most likely to produce this transition in simulation. When executing $\boldsymbol{\theta}$ in simulation, the robot selects $\mathbf{a<em _boldsymbol_theta="\boldsymbol{\theta">{t} \sim \pi</em>}}\left(\cdot \mid \mathbf{s<em t_1="t+1">{t}\right)$ and then uses $f$ to predict what the resulting configuration, $\mathbf{x}</em>}$, would be in $E$. Then $\mathbf{a<em t="t">{t}$ is replaced with $\hat{\mathbf{a}}</em>}:=f_{\text {sin }}^{-1}\left(\mathbf{s<em t="t">{t}, f\left(\mathbf{s}</em>}, \mathbf{a<em t_1="t+1">{t}\right)\right)$. The result is that the robot achieves the exact $\mathbf{x}</em>$}$ it would have on the physical robot. ${ }^{7</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Algorithm 1 Grounded Action Transformation (GAT) Pseudo code. Input: An initial policy, $\boldsymbol{\theta}$, the environment, $E$, a simulator, $E_{\text {sim }}$, smoothing parameter $\alpha$, and a policy improvement method, optimize. The function rolloutN $(\boldsymbol{\theta}, \mathrm{N})$ executes $N$ trajectories with $\boldsymbol{\theta}$ and returns the observed state transition data. The functions trainForwardModel and trainInverseModel estimate models of the forward and inverse dynamics respectively.</p>
<div class="codehilite"><pre><span></span><code><span class="k">function</span><span class="w"> </span><span class="nf">GAT</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">robot</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">RolloutN</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">E</span><span class="p">,</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="n">N</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">RolloutN</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">E_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="mi">0</span><span class="p">},</span><span class="w"> </span><span class="n">N</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="n">f</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="o">\</span><span class="p">)</span><span class="w"> </span><span class="n">trainForwardModel</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">robot</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="n">f_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">trainInverseModel</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="o">\</span><span class="n">mathcal</span><span class="p">{</span><span class="n">D</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="nb">alpha</span><span class="w"> </span><span class="n">f_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}}</span><span class="o">^</span><span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">}(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="p">))</span><span class="o">+</span><span class="p">(</span><span class="mi">1</span><span class="o">-\</span><span class="nb">alpha</span><span class="p">)</span><span class="w"> </span><span class="o">\</span><span class="n">cdot</span><span class="w"> </span><span class="n">a_</span><span class="p">{</span><span class="n">t</span><span class="p">}</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">Pi</span><span class="w"> </span><span class="o">\</span><span class="n">leftarrow</span><span class="w"> </span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">optimize</span><span class="p">}</span><span class="o">\</span><span class="n">left</span><span class="p">(</span><span class="n">E_</span><span class="p">{</span><span class="o">\</span><span class="nb">text</span><span class="w"> </span><span class="p">{</span><span class="n">sim</span><span class="w"> </span><span class="p">}},</span><span class="w"> </span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">},</span><span class="w"> </span><span class="n">g</span><span class="o">\</span><span class="n">right</span><span class="p">)</span><span class="o">\</span><span class="p">)</span>
<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="o">\</span><span class="p">(</span><span class="o">\</span><span class="n">operatorname</span><span class="p">{</span><span class="n">argmin</span><span class="p">}</span><span class="n">_</span><span class="p">{</span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">}</span><span class="w"> </span><span class="o">\</span><span class="n">in</span><span class="w"> </span><span class="o">\</span><span class="n">Pi</span><span class="p">}</span><span class="w"> </span><span class="n">J</span><span class="p">(</span><span class="o">\</span><span class="n">boldsymbol</span><span class="p">{</span><span class="o">\</span><span class="n">theta</span><span class="p">})</span><span class="o">\</span><span class="p">)</span>
<span class="k">end</span><span class="w"> </span><span class="k">function</span>
</code></pre></div>

<p>In practice $f$ and $f_{\text {sim }}^{-1}$ are represented with supervised regression models and learned from $\mathcal{D}$ and $\mathcal{D}_{\text {sim }}$ respectively (Algorithm 1 lines 5-6). The approximation of $g$ introduces noise into the robot's motion. To ensure stable motions, GAT uses a smoothing parameter $\alpha$. The action transformation function (Algorithm 1 line 8) is then defined as:</p>
<p>$$
g\left(\mathbf{s}<em t="t">{t}, \mathbf{a}</em>}\right):=\alpha f_{\text {sim }}^{-1}\left(\mathbf{s<em t="t">{t}, f\left(\mathbf{s}</em>}, \mathbf{a<em t="t">{t}\right)\right)+(1-\alpha) \mathbf{a}</em>
$$</p>
<p>In our experiments, we set $\alpha$ as high as possible subject to the walk remaining stable. Figure 1 illustrates the GAT-modified $E_{\text {sim }}$. GAT then proceeds to improve $\boldsymbol{\theta}$ with optimize within the grounded simulator (lines 8-9).</p>
<h2>GAT Implementation</h2>
<p>In this work, GAT uses two neural networks to approximate $f$ and $f_{\text {sim }}^{-1}$. Each function is a three layer network with 200 hidden units in the first layer and 180 hidden units in the second. During simulator modification, the $f$ network receives $\mathbf{s}<em t="t">{t}$ and $\mathbf{a}</em>}$ as input and the $f_{\text {sim }}^{-1}$ network receives $\mathbf{s<em _sim="{sim" _text="\text">{t}$ and the output of $f$ as input. The final output from $f</em>}}^{-1}$ is the replacement action $\tilde{\mathbf{a}<em t="t">{t}$. While $\mathbf{a}</em>}$ is a vector of desired joint angles, the action input to $f$ and action output of $f_{\text {sim }}^{-1}$ is encoded as a desired change in $\mathbf{x<em t="t">{t}$ which was found to improve prediction. We follow Fu et al. (2015) by having $f$ predict the joint acceleration caused by executing $\mathbf{a}</em>}$ in $\mathbf{s<em t_1="t+1">{t}$ instead of directly predicting $\mathbf{s}</em>}$. The accelerations can then be integrated and added to $\mathbf{s<em _sim="{sim" _text="\text">{t+1}$ to produce the resulting next state. Additionally, $f$ and $f</em>$ regress to the sine and cosine of the target angular accelerations and then pass the}}^{-1</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of the modifiable simulator GAT induces. At each time-step the robot takes an action, $\mathbf{a}<em t="t">{t}$, and passes $\mathbf{a}</em>}$ to a modification function, $g$. The modification function uses a deterministic model of the real robot's dynamics, $f$, to predict the effect of executing $\mathbf{a<em t="t">{t}$ on the physical robot. Then, a model of the simulated robot's inverse dynamics uses the prediction, $\tilde{\mathbf{x}}</em>}$, to predict the action $\tilde{\mathbf{a}<em t="t">{t}$ which will achieve $\tilde{\mathbf{x}}</em>}$ in simulation. Finally, $\tilde{\mathbf{a}<em _sim="{sim" _text="\text">{t}$ is passed to the environment, $E</em>$ and the resulting state transition will be similar to the transition that would have occurred in $E$.
outputs through the $\operatorname{arctan}$ function to produce the final angular acceleration. Passing the network outputs through the $\arctan$ function ensures $f$ and $f_{\text {sim }}^{-1}$ produce valid joint angles. The true state, $s_{t}$, is estimated by concatenating joint configurations $\mathbf{x}}<em t-1="t-1">{t}, \mathbf{x}</em>}, . . \mathbf{x<em t-1="t-1">{t-4}$ and past actions $\mathbf{a}</em>}, \ldots, \mathbf{a<em t="t">{t-4}$. The state estimate $\left\langle\mathbf{x}</em>}, \ldots, \mathbf{x<em t-1="t-1">{t-4}, \mathbf{a}</em>}, \ldots, \mathbf{a<em _sim="{sim" _text="\text">{t-4}\right\rangle$ improves the predictions of $f$ and $f</em>}}^{-1}$ because it implicitly captures the unobserved $\tilde{\mathbf{x}<em t="t">{t}$ and $\tilde{\mathbf{x}}</em>$ state variables. The length of the configuration history was chosen to balance predictive accuracy with keeping the number of inputs to the networks small. Both networks are trained with backpropagation.</p>
<h2>Empirical Results</h2>
<h2>Experimental Set-up</h2>
<p>We evaluate GAT on the task of bipedal robot walking. The walk takes a target forward velocity parameter in the range $[0,1]$. We set this parameter to 0.75 which we found led to the fastest walk that was reliably stable. The robot walks forward towards a target at this velocity. If the robot's angle to the target becomes greater than five degrees it turns back towards the target while continuing to walk forward. In all environments, $J(\boldsymbol{\theta})$ is the negative of the average forward walk velocity while executing $\boldsymbol{\theta}$. On the physical robot a trajectory terminates once the robot has walked four meters or falls. A trajectory generated with $\boldsymbol{\theta}<em t="t">{0}$ lasts $\approx 20.5$ seconds on the robot. In simulation a trajectory terminates after a fixed time interval ( 7.5 seconds in SimSpark and 10 seconds in Gazebo) or when the robot falls. Previous work has shown that when using a model estimated from data it is better to use shorter action trajectories to avoid overfitting to an inaccurate model (Jiang et al. 2015). Even for identical $s</em>$. This observation motivates using shorter trajectory lengths as simulator fidelity decreases.}$ and $a_{t}, s_{t+1}$ in $E_{\text {sim }}$ will most likely be different than $s_{t+1}$ in $E$. This error compounds over the course of a trajectory since state error at $s_{t}$ is propagated forward into $s_{t+1</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The three robotic environments used in this work. The Softbank NAO is our target physical robot. The NAO is simulated in the Gazebo and SimSpark simulators.</p>
<p>Optimizing $\boldsymbol{\theta}$ We use the Covariance Matrix AdaptionEvolutionary Strategy (CMA-ES) algorithm (Hansen 2006) as the optimize routine. CMA-ES is a policy search method which samples a population of candidate $\boldsymbol{\theta}$ from a Gaussian distribution over parameters. The top $k$ parameter vectors are used to update the sampling distribution so that the mean is closer to an optimal policy. We modify $J_{\text {sim }}(\boldsymbol{\theta})$ for the optimization by adding a cost of 15 for any trajectory in which the robot falls. The added penalty encourages CMA-ES to strongly favor stable policies over faster, less stable ones. To clarify terminology, a generation refers to a single update of CMA-ES; an iteration refers to a complete cycle of GAT.</p>
<p>SimSpark to Gazebo: Since a large number of trials are difficult to obtain on a physical robot, we present a study of GAT using Gazebo as a surrogate for the real world. In this setting we evaluate the effectiveness of GAT compared to learning with no grounding and grounding $E_{\text {sim }}$ by injecting noise into the robot's actions. Adding an "envelope" of noise has been used before to minimize simulation bias by preventing the policy improvement algorithm from overfitting to the simulator's dynamics (Jakobi, Husbands, and Harvey 1995). We refer to this baseline as Noise-Envelope. Since GAT with function approximation introduces noise into the robot's actions we wish to verify that GAT offers benefits over such methods. Noise-Envelope adds standard Gaussian noise to the robot's actions within the ungrounded simulator. We also attempted to evaluate GUIDED GSL but preliminary experiments showed that the assumption that actions achieve their desired effect instantaneously did not hold for this setting.</p>
<p>We run 10 trials of each method. For GAT we collect 50 trajectories of robot experience to train $f$ and 50 trajectories of simulated experience to train $f_{\text {sim }}^{-1}$. For each method, we optimize $\boldsymbol{\theta}$ for 10 generations of the CMA-ES algorithm. In each generation, 150 policies are sampled and evaluated in simulation. CMA-ES estimates $J_{\text {sim }}$ with 20 trajectories from each policy. Overall, the CMA-ES optimization requires 30,000 simulated trajectories for each trial.</p>
<p>Table 1 gives the average improvement in stable walk policies for each method and the number of trials in which a method failed to produce a stable improvement. This table only considers policies found after the first generation of CMA-ES. The reason for this is that the policies in the first generation were randomly sampled from an initial search
distribution. We consider learning to begin once CMA-ES has used the $J_{\text {sim }}$ values of the first generation to update the search distribution. Using $J_{\text {sim }}$ to identify $\boldsymbol{\theta}$ which improve $J$ in the first generation is a policy evaluation problem while improvement afterwards is a policy improvement problem.</p>
<p>Simulation to NAO Set-up: We evaluate GAT for transferring policies learned in Simspark or Gazebo to the physical NAO. The data set $\mathcal{D}$ consists of 15 trajectories collected with $\boldsymbol{\theta}_{0}$ on the physical NAO. For each iteration, we optimize $\boldsymbol{\theta}$ for 10 generations of the CMA-ES algorithm. We evaluate the best policy from each generation with 5 trajectories on the physical robot. If the robot falls in any of the 5 trajectories the policy is considered unstable. While the number of evaluations is small, in practice stable policies had small variance in walk velocity and unstable policies fell in almost all trajectories.</p>
<p>One challenge in this setting is the simulators receive robot actions at 50 Hz while the physical NAO receives robot actions at 100 Hz . The discrepancy in action frequency poses a problem for using real world data to modify how joints move in simulation. By skipping every other measurement to get an effectively 50 Hz data trace, we are able to model how the physical robot's joints move at the simulator's frequency.</p>
<h2>Experimental Results</h2>
<p>SimSpark to Gazebo Results: Table 1 shows that GAT maximizes improvement in $J$ while minimizing iterations without improvement. NOISE-ENVELOPE improves upon no grounding in both improvement and number of iterations without improvement. Adding noise to the simulator encourages CMA-ES to propose robust policies which are more likely to be stable. However, GAT further improves over NOISEENVELOPE. This result demonstrates that action transformations are grounding the simulator in a more effective way than simply injecting noise.</p>
<p>Table 1 also shows that on average GAT finds an improved policy within the first few policy updates after grounding. When learning with no grounding finds an improvement it is also usually in an early generation of CMA-ES. As policy improvement progresses, the best policies in each generation begin to overfit to the dynamics of $E_{\text {sim }}$. Without grounding overfitting happens almost immediately. NOISE-ENVELOPE is shown to be more robust to overfitting since any policy it</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">\% Improve</th>
<th style="text-align: center;">Failures</th>
<th style="text-align: center;">Best Gen.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">No Ground</td>
<td style="text-align: center;">11.094</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">1.33</td>
</tr>
<tr>
<td style="text-align: center;">Noise-Envelope</td>
<td style="text-align: center;">18.93</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6.6</td>
</tr>
<tr>
<td style="text-align: center;">GAT</td>
<td style="text-align: center;">$\mathbf{2 2 . 4 8}$</td>
<td style="text-align: center;">$\mathbf{1}$</td>
<td style="text-align: center;">2.67</td>
</tr>
</tbody>
</table>
<p>Table 1: This table compares Grounded Action Transformation (GAT) with baseline approaches for transferring learning between SimSpark and Gazebo. The first column displays the average maximum improvement found by each method after the first policy update made by CMA-ES. The second column is the number of times a method failed to find a stable walk. The third column gives the average generation of CMA-ES when the best policy was found. No Ground refers to learning done in the unmodified simulator.
proposes achieved good cost in a noisy $E_{\text {sim }}$. The grounding done by GAT is inherently local to the trajectory distribution of $\boldsymbol{\theta}_{0}$. Thus as $\boldsymbol{\theta}$ changes in policy improvement, the action transformation function fails to produce a more realistic simulator. Noise modification methods can mitigate overfitting by emphasizing robust policies although it is also limited in finding as strong of an improvement as GAT.</p>
<p>Simulator to Physical NAO Results: Our final empirical evaluation applies GAT to learning bipedal walks on a physical NAO. Table 2 gives the walk velocity of policies learned in simulation with GAT. The physical robot walks at a velocity of $19.52 \mathrm{~cm} / \mathrm{s}$ with $\boldsymbol{\theta}<em 0="0">{0}$. Two iterations of GAT with SimSpark increased the walk velocity of the NAO to 27.97 $\mathrm{cm} / \mathrm{s}$ - an improvement of $43.27 \%$ compared to $\boldsymbol{\theta}</em>$ GAT with SimSpark and GAT with Gazebo both improved walk velocity by over $30 \%$. This result demonstrates generality of our approach across different simulators.} .{ }^{8</p>
<p>As in the previous experiment, policy improvement with CMA-ES required 30,000 trajectories per iteration to find the 10 policies that were evaluated on the robot. In contrast the total number of trajectories executed on the physical robot is 65 ( 15 trajectories in $\mathcal{D}$ and 5 evaluations per $\boldsymbol{\theta}<em c="c">{c} \in \Pi</em>$ ). This result demonstrates GAT can use sample-intensive simulation learning to optimize real world skills with a low number of trajectories on the physical robot.</p>
<p>Farchy et al. demonstrated the benefits of re-grounding and further optimizing $\boldsymbol{\theta}$. We reground using the 15 trajectories collected with the best policy found by GAT with SimSpark and optimize for a further 10 generations in simulation. The second iteration results in a walk, $\boldsymbol{\theta}<em 0="0">{2}$, which averages 27.97 $\mathrm{cm} / \mathrm{s}$ for a total improvement of $43.27 \%$ over $\boldsymbol{\theta}</em>$.</p>
<p>Finally, we evaluate $\boldsymbol{\theta}_{0}$ with $100 \%$ of maximum velocity requested (i.e., the forward velocity request parameter set to 1.0). The average velocity of this walk across five stable trajectories is $24.3 \mathrm{~cm} / \mathrm{s}$. This result shows that GAT can learn walk policies that outperform one of the best hand coded walks available.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 2: This table gives the maximum learned velocity and percent improvement for each method starting from $\boldsymbol{\theta}_{0}$ (top row).</p>
<h2>Discussion and Future Work</h2>
<p>Our proposed algorithm, GAT, has some limitations that we discuss here. The decision to learn an action modification function, $g$, makes the assumption that $\forall\left(s_{t}, a_{t}, s_{t+1}\right)$ that could be observed on the physical robot there exists an action $\tilde{a}<em t="t">{t}$ that will produce the same transition when used in place of $a</em>$ can usually be executed with more or less force in simulation to achieve the desired response. However, this assumption is likely to break down under contact dynamics where external forces resist the robot's actions. Other tasks may introduce other forms of simulator bias that GAT is currently limited in handling. An important direction for future work is to characterize the settings where this approach is limited and to identify alternatives.}$. We posit that this assumption is often true since $\tilde{a}_{t</p>
<p>We specify simulator grounding as a supervised learning problem however the distribution of inputs to the learned models, $f$ and $f_{\text {sim }}^{-1}$, during policy execution differ from the training distributions. The distribution change is likely to weaken the quality of action modification under $g$. To see why the change happens consider that $f$ is trained under $\left(s_{t}, a_{t}, s_{t+1}\right)$ sampled from executing $\boldsymbol{\theta}$ on the real robot while $f_{\text {sim }}^{-1}$ is trained on $\left(s_{t}, a_{t}, s_{t+1}\right)$ sampled in simulation. During simulator modification, both models are evaluated on data sampled from the distribution of $\boldsymbol{\theta}$ in the grounded simulator. We have started to explore methods similar to the Dataset Aggregation (DAgger) algorithm (Ross and Bagnell 2012) which collects new data to retrain $f_{\text {sim }}^{-1}$ as the state distribution of $\boldsymbol{\theta}$ in $E_{\text {sim }}$ changes with grounding.</p>
<p>Finally, this paper considers action modification but a complementary approach could consider sensor modification. Sensor modification would add another layer to the modified simulator such that the environment returns a state and the sensor modification function predicts what the robot would observe in that state. The two methods have an interesting interaction since sensor modification changes the action $\boldsymbol{\theta}$ selects while action modification changes the next state which changes the observed sensor values as well.</p>
<h2>Conclusion</h2>
<p>This paper proposed and evaluated the Grounded Action Transformation (GAT) algorithm for grounded simulation learning. Our method led to a $43.27 \%$ improvement in the walk velocity of a state-of-the-art bipedal robot. We conducted an empirical study that analyzed the benefits of GAT compared to a pair of baseline simulation-transfer methods. This experiment demonstrates GAT enhances learning in simulation in comparison to other methods.</p>
<h2>Acknowledgments</h2>
<p>We would like to thank Matthew Hausknecht, Patrick MacAlpine, and Garrett Warnell for insightful discussions and the anonymous reviewers for helpful comments. This work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG research is supported in part by NSF (CNS-1330072, CNS-1305287, IIS-1637736, IIS1651089), ONR (21C184-01), and AFOSR (FA9550-14-10087). Josiah Hanna is supported by an NSF Graduate Research Fellowship. Peter Stone serves on the Board of Directors of, Cogitai, Inc. The terms of this arrangement have been reviewed and approved by the University of Texas at Austin in accordance with its policy on objectivity in research.</p>
<h2>References</h2>
<p>Ashar, J.; Ashmore, J.; Hall, B.; and Harris, S. e. a. 2015. Robocup spl 2014 champion team paper. In RoboCup 2014: Robot World Cup XVIII, volume 8992 of Lecture Notes in Computer Science. Springer International Publishing. 70-81.
Christiano, P.; Shah, Z.; Mordatch, I.; Schneider, J.; Blackwell, T.; Tobin, J.; Abbeel, P.; and Zaremba, W. 2016. Transfer from simulation to real world through learning deep inverse dynamics model. arXiv preprint arXiv:1610.03518.
Cully, A.; Clune, J.; Tarapore, D.; and Mouret, J.-B. 2015. Robots that can adapt like animals. Nature.
Cutler, M., and How, J. P. 2015. Efficient reinforcement learnng for robots using informative simulated priors. In IEEE International Conference on Robotics and Automation, ICRA.
Cutler, M.; Walsh, T. J.; and How, J. P. 2014. Reinforcement learning with multi-fidelity simulators. In IEEE Conference on Robotics and Automation, ICRA.
Deisenroth, M. P., and Rasmussen, C. E. 2011. Pilco: A model-based and data-efficient approach to policy search. In International Conference on Machine Learning, ICML.
Farchy, A.; Barrett, S.; MacAlpine, P.; and Stone, P. 2013. Humanoid robots learning to walk faster: From the real world to simulation and back. In Twelth International Conference on Autonomous Agents and Multiagent Systems, AAMAS.
Fu, J.; Levine, S.; and Abbeel, P. 2015. One-shot learning of manipulation skills with online dynamics adaptation and neural network priors. In IEEE/RSJ International Conference on Intelligent Robots and Systems.
Hall, B.; Harris, S.; Hengst, B.; Liu, R.; Ng, K.; Pagnucco, M.; Pearson, L.; Sammut, C.; and Schmidt, P. 2016. Robocup spl 2015 champion team paper.
Hansen, N. 2006. The CMA evolution strategy: a comparing review. In Lozano, J.; Larranaga, P.; Inza, I.; and Bengoetxea, E., eds., Towards a new evolutionary computation. Advances on estimation of distribution algorithms. Springer. 75-102.
Hengst, B. 2014. runswift walk2014 report robocup standard platform league. Technical report, The University of New South Wales.
Iocchi, L.; Libera, F. D.; and Menegatti, E. 2007. Learning humanoid soccer actions interleaving simulated and real data. In Second Workshop on Humanoid Soccer Robots.</p>
<p>Jakobi, N.; Husbands, P.; and Harvey, I. 1995. Noise and the reality gap: The use of simulation in evolutionary robotics. In European Conference on Artificial Life, 704-720. Springer.
Jiang, N.; Kulesza, A.; Singh, S.; and Lewis, R. 2015. The dependence of effective planning horizon on model accuracy. In International Conference on Autonomous Agents and Multiagent Systems, AAMAS.
Kober, J.; Bagnell, J. A.; and Peters, J. 2013. Reinforcement learning in robotics: A survey. The International Journal of Robotics Research.
Koos, S.; Mouret, J.-B.; and Doncieux, S. 2010. Crossing the reality gap in evolutionary robotics by promoting transferable controllers. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, 119-126. ACM.
Lillicrap, T. P.; Hunt, J. J.; Pritzel, A.; Heess, N.; Erez, T.; Tassa, Y.; Silver, D.; and Wierstra, D. 2015. Continuous control with deep reinforcement learning. CoRR abs/1509.02971.
Rajeswaran, A.; Ghotra, S.; Levine, S.; and Ravindran, B. 2016. Epopt: Learning robust neural network policies using model ensembles. arXiv preprint arXiv:1610.01283.
Ross, S., and Bagnell, J. A. 2012. Agnostic system identification for model-based reinforcement learning. In 29th International Conference on Machine Learning, ICML.
Rusu, A. A.; Rabinowitz, N. C.; Desjardins, G.; Soyer, H.; Kirkpatrick, J.; Kavukcuoglu, K.; Pascanu, R.; and Hadsell, R. 2016a. Progressive neural networks. arXiv preprint arXiv:1606.04671.
Rusu, A. A.; Vecerik, M.; Rothörl, T.; Heess, N.; Pascanu, R.; and Hadsell, R. 2016b. Sim-to-real robot learning from pixels with progressive nets. arXiv preprint arXiv:1610.04286.
Schulman, J.; Moritz, P.; Levine, S.; Jordan, M. I.; and Abbeel, P. 2015. High-dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations.
Sutton, R. S., and Barto, A. G. 1998. Reinforcement Learning: An Introduction. MIT Press.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8} \mathrm{~A}$ video of the learned walk policies is available at https://www.cs.utexas.edu/users/AustinVilla/?p=research/ real_and_sim_walk_learning.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ GAT subsumes GUIDED GSL which makes the additional as-&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
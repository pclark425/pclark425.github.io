<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1088 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1088</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1088</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-286f5db8b950a1cb35cbd951eb1c3ddcdb5d56b8</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/286f5db8b950a1cb35cbd951eb1c3ddcdb5d56b8" target="_blank">Robust Reinforcement Learning via Genetic Curriculum</a></p>
                <p><strong>Paper Venue:</strong> IEEE International Conference on Robotics and Automation</p>
                <p><strong>Paper TL;DR:</strong> This work proposes genetic curriculum, an algorithm that automatically identifies scenarios in which the agent currently fails and generates an associated curriculum to help the agent learn to solve the scenarios and acquire more robust behaviors, using a raw, non-parametric optimizer.</p>
                <p><strong>Paper Abstract:</strong> Achieving robust performance is crucial when applying deep reinforcement learning (RL) in safety critical systems. Some of the state of the art approaches try to address the problem with adversarial agents, but these agents often require expert supervision to fine tune and prevent the adversary from becoming too challenging to the trainee agent. While other approaches involve automatically adjusting environment setups during training, they have been limited to simple environments where low-dimensional encodings can be used. Inspired by these approaches, we propose genetic curriculum, an algorithm that automatically identifies scenarios in which the agent currently fails and generates an associated curriculum to help the agent learn to solve the scenarios and acquire more robust behaviors. As a non-parametric optimizer, our approach uses a raw, non-fixed encoding of scenarios, reducing the need for expert supervision and allowing our algorithm to adapt to the changing performance of the agent. Our empirical studies show improvement in robustness over the existing state of the art algorithms, providing training curricula that result in agents being 2 - 8x times less likely to fail without sacrificing cumulative reward. We include an ablation study and share insights on why our algorithm outperforms prior approaches.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1088.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1088.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GC-BipedalWalkerHardcore</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy trained with Genetic Curriculum on BipedalWalkerHardcore</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated bipedal walking agent (policy) trained with the paper's Genetic Curriculum (GC) method using SAC; evaluated on procedurally generated obstacle courses with actuator/system failure variations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GC-trained policy (SAC) - BipedalWalkerHardcore</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL policy trained with Soft Actor-Critic (SAC) under the Genetic Curriculum (GC) procedure; learning objective is to traverse randomly generated obstacle courses using LIDAR/IMU/joint encoders to control leg servos.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BipedalWalkerHardcore</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated 2D obstacle courses composed of stairs, pitfalls, walls and varied obstacle sequences; partial observability via sensors and occasional simulated actuator/system failures. Complexity arises from long sequences of obstacles and timing of failures; variation arises from procedural RNG producing many distinct course instances.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Sequence length / scenario encoding length L(ψ) (dynamic; reported expansion from ~20D to ~300D), number/sequence of obstacles, presence/timing/severity of actuator/system failures</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (encoding expanded up to ~300 dimensions during training; long sequences of obstacles and failures)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation of scenario sequences (testing set size = 1000 distinct scenarios), curriculum size (300 for this benchmark), and mean genetic distance between curriculum scenarios (reported quantitative values in ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (large procedurally generated scenario space; testing set 1000 instances; curriculum of 300 varied failure scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward and failure rate (%) (failure defined as falling / mission failure before resource budget exhausted)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reward = 304.33 ± 1.65; Failure rate = 3.96% ± 0.37% (GC, testing set)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: GC expands scenario encoding length as agent improves (complexity increases) while keeping curricula composed of similar scenarios (moderate genetic distance) to enable skill transfer. Ablation shows low mean genetic distance (≈10.6) plus sufficient coverage gives low failure; extremely low variation (Single Run, distance=0) yields poor coverage and high failure; high genetic distance (Random Failure, ≈23.34) also yields worse transfer and higher failure. Thus there is a trade-off: maintain similarity between successive training scenarios (low-to-moderate genetic distance) to transfer skills, but ensure sufficient coverage/variation to generalize across the scenario space.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Single Run (very low variation / genetic distance = 0): Reward = 99.45; Failure rate = 33.33% (from ablation study)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Random Failure (high genetic distance ≈23.34): Reward = 251.37; Failure rate = 24.50% (from ablation study) — indicates large variation without structured similarity harms learning</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via Genetic Curriculum (non-parametric genetic algorithm: crossover + mutation to generate failure scenarios), base RL = SAC</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Tested on large held-out procedurally generated testing set (1000 scenarios); GC-trained policy achieved low failure rate (3.96%) and high mean reward, indicating strong generalization across the benchmark's scenario distribution compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Epoch length 1e5 environment steps; number of epochs 350 (per Table I) → ~3.5e7 environment interactions reported as the training regime; GC shows slower start but sustained improvement per environment interaction compared to baselines (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GC-produced curriculum (moderate genetic distance, adaptive encoding length) substantially reduces failure rate (2–8x improvement over base RL) while maintaining or improving reward; similarity in curriculum examples enables transfer while mutation ensures coverage; extreme low-variation or unstructured high-variation curricula both hurt robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning via Genetic Curriculum', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1088.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1088.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GC-BipedalWalkerSystem</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy trained with Genetic Curriculum on BipedalWalkerSystem</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated bipedal walker policy trained with GC (SAC) on a fixed obstacle sequence environment augmented with randomized simulated system/actuator failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GC-trained policy (SAC) - BipedalWalkerSystem</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL policy trained with SAC under Genetic Curriculum; environment uses fixed obstacle sequences with randomly triggered servo power reductions (60–100% of nominal) to simulate internal failures.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>BipedalWalkerSystem</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Fixed sequence of obstacles; variation introduced by simulated random system failures (random time of failure, affected servo power limited to 60–100%); complexity mainly from managing kinetic energy and handling actuator partial failures while traversing obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Presence and timing/severity of actuator failure events (which servo, when triggered, and severity range 60–100%), plus obstacle difficulty in fixed sequence</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-high (fixed sequence reduces combinatorial layout complexity relative to fully random courses, but failures add system-level challenge)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of test instances (testing set size = 2500), range/severity of servo power reductions (60–100%), curriculum size used by GC = 300</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (fixed obstacle sequence but high variation in failure timing and severity; testing uses 2500 instances)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward and failure rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reward = 300.00 ± 1.00; Failure rate = 2.16% ± 0.45% (GC, testing set)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper notes that GC focuses on generating failure scenarios the agent currently cannot solve; for this environment, variation comes mostly from failure timing/severity rather than obstacle layout. GC's curriculum reduces failure by focusing on similar failing sequences; adversarial/no-curriculum approaches are less effective.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via Genetic Curriculum (GC), base RL = SAC</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Tested on 2500 randomly generated failure scenarios; GC-trained policy achieved very low failure rate (2.16%), outperforming baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Epoch steps = 1e5; number of epochs = 30 (Table I) → ~3.0e6 environment interactions in full training schedule; GC shows robustness gains relative to baselines across these interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Focusing training on failure scenarios (GC) yields strong robustness to system/actuator failures in a fixed-sequence environment; curriculum generation using raw non-fixed encodings helps handle system-level variation without heavy expert feature engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning via Genetic Curriculum', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1088.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1088.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GC-LunarLander</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy trained with Genetic Curriculum on LunarLander</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated lander policy trained with GC (SAC) on a 2D lunar landing task with randomized engine/actuator failures and procedural initial conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GC-trained policy (SAC) - LunarLander</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL policy trained with SAC and GC; task is to land a spacecraft on a pad using main engine and side thrusters; environment introduces random failures that limit motor throttle to 60–100% at a random timestep.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>LunarLander (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D landing simulator with position/velocity observations; nominal best policy waits until the end to fire main engine but failures (throttle limited) create scenarios requiring more conservative descent control and robust planning.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Failure timing and severity (throttle limited to 60–100%), continuous state space (position/velocity), need to manage fuel vs descent rate</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium (continuous control with safety-critical trade-offs; complexity rises with inclusion of random failures)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Testing set size = 2500 distinct scenarios; failure severity distribution (60–100% throttle) and randomized timing; curriculum size for GC = 100 for this benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (large testing set and randomized failure parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward and failure rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reward = 272.82 ± 0.30; Failure rate = 0.64% ± 0.02% (GC, testing set)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper emphasizes that GC finds failure cases the agent currently fails on and constructs similar scenarios to promote transfer; for LunarLander this produced policies that sacrifice fuel optimality under nominal conditions to maintain margin to survive random engine throttling—i.e., increased robustness at similar reward.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via Genetic Curriculum (GC), base RL = SAC</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated on 2500 randomized test scenarios; GC achieved extremely low failure rate (0.64%), indicating strong generalization to varied failure timings and severities.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Steps per epoch = 1e4; number of epochs = 80 (Table I) → ~8e5 environment interactions in the scheduled training; GC shows sustained performance improvements per interaction compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GC enables agents to learn more conservative but robust control policies that survive randomized engine failures; focusing training on failure scenarios drastically reduces test-time catastrophic failures while preserving reward.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning via Genetic Curriculum', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1088.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1088.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GC-Walker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy trained with Genetic Curriculum on Walker (legged robot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated legged robot policy trained with GC (TD3) on varied terrains with randomized actuator torque limits and added payloads.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GC-trained policy (TD3) - Walker</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL policy trained with Twin Delayed DDPG (TD3) under Genetic Curriculum; environment includes random actuator torque limits (75–100%) and a mounted simulated payload (size 0.5) increasing robustness demands.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Walker (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous control legged locomotion simulator with added payload and random actuator failures that limit torque; complexity from multi-joint coordination under variable actuator capabilities and payload-induced dynamics changes.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Actuator torque limit range (75–100%), added payload mass (0.5), multi-joint continuous control dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (multi-joint continuous control with payload and stochastic actuator limits)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Testing set size = 2500; randomness in which servo fails, timing and severity; curriculum size for GC = 100</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward and failure rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reward = 2342.61 ± 5.45; Failure rate = 2.35% ± 1.11% (GC, testing set)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>GC generates failure-focused curricula that are similar across examples enabling transfer of locomotion skills despite high-dimension dynamics; paper reports that structured similarity (moderate genetic distance) in curriculum helps robustness more than unstructured high variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via Genetic Curriculum (GC), base RL = TD3</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Tested on 2500 randomized scenarios including actuator failures and payload perturbations; GC achieved low failure rate (2.35%), indicating good generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Steps per epoch = 5e4; number of epochs = 60 → ~3.0e6 environment interactions in training schedule; GC sustains improvements per interaction longer than baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GC-generated curricula focusing on similar failure scenarios improve robustness in high-dimensional locomotion tasks; structured curricula outperform both purely random failure selection and curricula lacking crossover.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning via Genetic Curriculum', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1088.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1088.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GC-Hopper</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Policy trained with Genetic Curriculum on Hopper (legged robot)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated hopper robot policy trained with GC (TD3) to be robust to randomized actuator torque limits and added payload.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GC-trained policy (TD3) - Hopper</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An RL policy trained with TD3 under Genetic Curriculum; environment includes random servo torque limits (75–100%) and a simulated payload (size 0.75); failure defined as payload touching the ground.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Hopper (modified)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Single-legged hopping robot simulation with added payload and random actuator failures; complexity from balancing, payload dynamics, and dealing with partial actuator performance.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Actuator torque limits (75–100%), payload mass (0.75), continuous multi-timestep balancing dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Testing set size = 2500; stochastic failures parameters and initial conditions; curriculum size for GC = 100</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward and failure rate (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reward = 2283.48 ± 2.01; Failure rate = 7.30% ± 2.79% (GC, testing set)</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>GC improves robustness by generating similar failure scenarios enabling skill transfer; paper notes that in Hopper the improvement is present but less pronounced than other benchmarks, indicating task-specific sensitivity to curriculum design and variation coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum learning via Genetic Curriculum (GC), base RL = TD3</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated on 2500 randomized test cases; GC reduced failure rate relative to baselines (to 7.30%) indicating improved robustness under payload and actuator variations.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Steps per epoch = 5e4; number of epochs = 40 → ~2.0e6 environment interactions in scheduled training; GC shows sustained learning improvements per interaction compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GC yields lower failure rates in hopper locomotion despite high dynamic complexity; effectiveness depends on balancing similarity in curricula (for transfer) and coverage (for generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning via Genetic Curriculum', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1088.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1088.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GeneticCurriculum</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Genetic Curriculum (GC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-parametric curriculum generation algorithm using genetic operators (crossover + mutation) on raw, variable-length scenario encodings to generate failure-focused curricula for robust RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GC-trained policies (general)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Genetic Curriculum is a training-time method that (outside the inner training loop) runs a genetic algorithm to find scenarios the current policy fails on, collects a curriculum of similar failure scenarios, and trains the RL policy (SAC/TD3) on this curriculum to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>training procedure for simulated agents</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Various simulated benchmarks (BipedalWalkerHardcore, BipedalWalkerSystem, LunarLander, Walker, Hopper)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Supports environments with raw non-fixed-length scenario encodings (sequences of RNG outputs) and high procedural variation, including obstacle courses, actuator failures, and payload perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Scenario encoding length L(ψ) (raw non-fixed-length sequences), genetic distance between scenarios, number of variables in a scenario sequence; reported encoding expansion from ~20 to ~300 dimensions</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>adaptive (GC allows encoding length to grow up to ~300D when needed), effectively enabling high complexity</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Curriculum size (300 for BipedalWalker, 100 for other benchmarks), population sizes (M_pop ~100), mutation prob p_μ (0.1), testing set sizes (1000–2500), mean genetic distance between curriculum examples</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>adaptive from low-to-high depending on agent needs (GC both preserves similarity and introduces variation via mutation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Mean episodic reward and failure rate (%) on held-out testing sets; improvement vs baselines reported as multiplicative reductions in failure rate (2–8x)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Across benchmarks GC yields e.g., BipedalWalkerHardcore: Reward 304.33 ±1.65, Failure 3.96% ±0.37%; improvements reported as agents being 2–8x less likely to fail compared to baseline SAC/TD3 depending on benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper explicitly analyzes trade-offs: curricula must balance similarity (to enable transfer; lower mean genetic distance ≈10) and coverage/variation (to generalize); GC uses crossover to produce similar offspring and mutation to introduce novel variations — empirical ablation shows both are necessary: no-crossover or random failure sampling increase mean genetic distance and failure rates, while single-run (too low variation) yields poor coverage and poor robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Single Run ablation (single repeated failure example; very low variation): Reward = 99.45; Failure = 33.33% (BipedalWalkerHardcore ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Random Failure ablation (high genetic distance ≈23.34): Reward = 251.37; Failure = 24.50% (BipedalWalkerHardcore ablation) — indicates unstructured high variation hurts transfer</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Non-parametric curriculum learning via genetic algorithm (crossover prioritized for mixing failure subsequences; occasional mutation to introduce novelty); separates GA from policy training (GA runs outside inner optimization loop).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>GC-trained agents evaluated on large randomized testing sets per benchmark; in all reported benchmarks GC reduced failure rates substantially compared to base RL and many baselines, demonstrating improved generalization across procedurally generated scenario spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>GC requires extra evaluation cost for running GA but achieves better marginal utility per environment interaction over time; reported per-benchmark training regimes (e.g., 1e5 steps/epoch × 350 epochs for BipedalWalkerHardcore); GC slower initially but sustains improvement and converges to more robust policies compared to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Non-parametric curriculum generation using raw variable-length encodings allows GC to adapt scenario complexity as agent improves, producing curricula of similar failure examples that transfer skills effectively; balancing similarity and coverage is crucial — crossover (to keep similarity) plus mutation (to ensure exploration) yields best robustness; GC outperforms adversarial methods and parametric curriculum methods on failure-rate metrics while maintaining reward.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Robust Reinforcement Learning via Genetic Curriculum', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Poet: openended coevolution of environments and their optimized solutions <em>(Rating: 2)</em></li>
                <li>Self-paced deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Fingerprint policy optimisation for robust reinforcement learning <em>(Rating: 2)</em></li>
                <li>Robust adversarial reinforcement learning <em>(Rating: 2)</em></li>
                <li>Risk averse robust adversarial reinforcement learning <em>(Rating: 1)</em></li>
                <li>Action robust reinforcement learning and applications in continuous control <em>(Rating: 1)</em></li>
                <li>OpenAI Gym <em>(Rating: 1)</em></li>
                <li>CARLA: An open urban driving simulator <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1088",
    "paper_id": "paper-286f5db8b950a1cb35cbd951eb1c3ddcdb5d56b8",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "GC-BipedalWalkerHardcore",
            "name_full": "Policy trained with Genetic Curriculum on BipedalWalkerHardcore",
            "brief_description": "A simulated bipedal walking agent (policy) trained with the paper's Genetic Curriculum (GC) method using SAC; evaluated on procedurally generated obstacle courses with actuator/system failure variations.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GC-trained policy (SAC) - BipedalWalkerHardcore",
            "agent_description": "An RL policy trained with Soft Actor-Critic (SAC) under the Genetic Curriculum (GC) procedure; learning objective is to traverse randomly generated obstacle courses using LIDAR/IMU/joint encoders to control leg servos.",
            "agent_type": "simulated agent",
            "environment_name": "BipedalWalkerHardcore",
            "environment_description": "Procedurally generated 2D obstacle courses composed of stairs, pitfalls, walls and varied obstacle sequences; partial observability via sensors and occasional simulated actuator/system failures. Complexity arises from long sequences of obstacles and timing of failures; variation arises from procedural RNG producing many distinct course instances.",
            "complexity_measure": "Sequence length / scenario encoding length L(ψ) (dynamic; reported expansion from ~20D to ~300D), number/sequence of obstacles, presence/timing/severity of actuator/system failures",
            "complexity_level": "high (encoding expanded up to ~300 dimensions during training; long sequences of obstacles and failures)",
            "variation_measure": "Procedural generation of scenario sequences (testing set size = 1000 distinct scenarios), curriculum size (300 for this benchmark), and mean genetic distance between curriculum scenarios (reported quantitative values in ablation)",
            "variation_level": "high (large procedurally generated scenario space; testing set 1000 instances; curriculum of 300 varied failure scenarios)",
            "performance_metric": "Mean episodic reward and failure rate (%) (failure defined as falling / mission failure before resource budget exhausted)",
            "performance_value": "Reward = 304.33 ± 1.65; Failure rate = 3.96% ± 0.37% (GC, testing set)",
            "complexity_variation_relationship": "Explicitly discussed: GC expands scenario encoding length as agent improves (complexity increases) while keeping curricula composed of similar scenarios (moderate genetic distance) to enable skill transfer. Ablation shows low mean genetic distance (≈10.6) plus sufficient coverage gives low failure; extremely low variation (Single Run, distance=0) yields poor coverage and high failure; high genetic distance (Random Failure, ≈23.34) also yields worse transfer and higher failure. Thus there is a trade-off: maintain similarity between successive training scenarios (low-to-moderate genetic distance) to transfer skills, but ensure sufficient coverage/variation to generalize across the scenario space.",
            "high_complexity_low_variation_performance": "Single Run (very low variation / genetic distance = 0): Reward = 99.45; Failure rate = 33.33% (from ablation study)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Random Failure (high genetic distance ≈23.34): Reward = 251.37; Failure rate = 24.50% (from ablation study) — indicates large variation without structured similarity harms learning",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via Genetic Curriculum (non-parametric genetic algorithm: crossover + mutation to generate failure scenarios), base RL = SAC",
            "generalization_tested": true,
            "generalization_results": "Tested on large held-out procedurally generated testing set (1000 scenarios); GC-trained policy achieved low failure rate (3.96%) and high mean reward, indicating strong generalization across the benchmark's scenario distribution compared to baselines.",
            "sample_efficiency": "Epoch length 1e5 environment steps; number of epochs 350 (per Table I) → ~3.5e7 environment interactions reported as the training regime; GC shows slower start but sustained improvement per environment interaction compared to baselines (Figure 5).",
            "key_findings": "GC-produced curriculum (moderate genetic distance, adaptive encoding length) substantially reduces failure rate (2–8x improvement over base RL) while maintaining or improving reward; similarity in curriculum examples enables transfer while mutation ensures coverage; extreme low-variation or unstructured high-variation curricula both hurt robustness.",
            "uuid": "e1088.0",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning via Genetic Curriculum",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GC-BipedalWalkerSystem",
            "name_full": "Policy trained with Genetic Curriculum on BipedalWalkerSystem",
            "brief_description": "A simulated bipedal walker policy trained with GC (SAC) on a fixed obstacle sequence environment augmented with randomized simulated system/actuator failures.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GC-trained policy (SAC) - BipedalWalkerSystem",
            "agent_description": "An RL policy trained with SAC under Genetic Curriculum; environment uses fixed obstacle sequences with randomly triggered servo power reductions (60–100% of nominal) to simulate internal failures.",
            "agent_type": "simulated agent",
            "environment_name": "BipedalWalkerSystem",
            "environment_description": "Fixed sequence of obstacles; variation introduced by simulated random system failures (random time of failure, affected servo power limited to 60–100%); complexity mainly from managing kinetic energy and handling actuator partial failures while traversing obstacles.",
            "complexity_measure": "Presence and timing/severity of actuator failure events (which servo, when triggered, and severity range 60–100%), plus obstacle difficulty in fixed sequence",
            "complexity_level": "medium-high (fixed sequence reduces combinatorial layout complexity relative to fully random courses, but failures add system-level challenge)",
            "variation_measure": "Number of test instances (testing set size = 2500), range/severity of servo power reductions (60–100%), curriculum size used by GC = 300",
            "variation_level": "medium (fixed obstacle sequence but high variation in failure timing and severity; testing uses 2500 instances)",
            "performance_metric": "Mean episodic reward and failure rate (%)",
            "performance_value": "Reward = 300.00 ± 1.00; Failure rate = 2.16% ± 0.45% (GC, testing set)",
            "complexity_variation_relationship": "Paper notes that GC focuses on generating failure scenarios the agent currently cannot solve; for this environment, variation comes mostly from failure timing/severity rather than obstacle layout. GC's curriculum reduces failure by focusing on similar failing sequences; adversarial/no-curriculum approaches are less effective.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via Genetic Curriculum (GC), base RL = SAC",
            "generalization_tested": true,
            "generalization_results": "Tested on 2500 randomly generated failure scenarios; GC-trained policy achieved very low failure rate (2.16%), outperforming baselines.",
            "sample_efficiency": "Epoch steps = 1e5; number of epochs = 30 (Table I) → ~3.0e6 environment interactions in full training schedule; GC shows robustness gains relative to baselines across these interactions.",
            "key_findings": "Focusing training on failure scenarios (GC) yields strong robustness to system/actuator failures in a fixed-sequence environment; curriculum generation using raw non-fixed encodings helps handle system-level variation without heavy expert feature engineering.",
            "uuid": "e1088.1",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning via Genetic Curriculum",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GC-LunarLander",
            "name_full": "Policy trained with Genetic Curriculum on LunarLander",
            "brief_description": "A simulated lander policy trained with GC (SAC) on a 2D lunar landing task with randomized engine/actuator failures and procedural initial conditions.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GC-trained policy (SAC) - LunarLander",
            "agent_description": "An RL policy trained with SAC and GC; task is to land a spacecraft on a pad using main engine and side thrusters; environment introduces random failures that limit motor throttle to 60–100% at a random timestep.",
            "agent_type": "simulated agent",
            "environment_name": "LunarLander (modified)",
            "environment_description": "2D landing simulator with position/velocity observations; nominal best policy waits until the end to fire main engine but failures (throttle limited) create scenarios requiring more conservative descent control and robust planning.",
            "complexity_measure": "Failure timing and severity (throttle limited to 60–100%), continuous state space (position/velocity), need to manage fuel vs descent rate",
            "complexity_level": "medium (continuous control with safety-critical trade-offs; complexity rises with inclusion of random failures)",
            "variation_measure": "Testing set size = 2500 distinct scenarios; failure severity distribution (60–100% throttle) and randomized timing; curriculum size for GC = 100 for this benchmark",
            "variation_level": "high (large testing set and randomized failure parameters)",
            "performance_metric": "Mean episodic reward and failure rate (%)",
            "performance_value": "Reward = 272.82 ± 0.30; Failure rate = 0.64% ± 0.02% (GC, testing set)",
            "complexity_variation_relationship": "Paper emphasizes that GC finds failure cases the agent currently fails on and constructs similar scenarios to promote transfer; for LunarLander this produced policies that sacrifice fuel optimality under nominal conditions to maintain margin to survive random engine throttling—i.e., increased robustness at similar reward.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via Genetic Curriculum (GC), base RL = SAC",
            "generalization_tested": true,
            "generalization_results": "Evaluated on 2500 randomized test scenarios; GC achieved extremely low failure rate (0.64%), indicating strong generalization to varied failure timings and severities.",
            "sample_efficiency": "Steps per epoch = 1e4; number of epochs = 80 (Table I) → ~8e5 environment interactions in the scheduled training; GC shows sustained performance improvements per interaction compared to baselines.",
            "key_findings": "GC enables agents to learn more conservative but robust control policies that survive randomized engine failures; focusing training on failure scenarios drastically reduces test-time catastrophic failures while preserving reward.",
            "uuid": "e1088.2",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning via Genetic Curriculum",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GC-Walker",
            "name_full": "Policy trained with Genetic Curriculum on Walker (legged robot)",
            "brief_description": "A simulated legged robot policy trained with GC (TD3) on varied terrains with randomized actuator torque limits and added payloads.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GC-trained policy (TD3) - Walker",
            "agent_description": "An RL policy trained with Twin Delayed DDPG (TD3) under Genetic Curriculum; environment includes random actuator torque limits (75–100%) and a mounted simulated payload (size 0.5) increasing robustness demands.",
            "agent_type": "simulated agent",
            "environment_name": "Walker (modified)",
            "environment_description": "Continuous control legged locomotion simulator with added payload and random actuator failures that limit torque; complexity from multi-joint coordination under variable actuator capabilities and payload-induced dynamics changes.",
            "complexity_measure": "Actuator torque limit range (75–100%), added payload mass (0.5), multi-joint continuous control dynamics",
            "complexity_level": "high (multi-joint continuous control with payload and stochastic actuator limits)",
            "variation_measure": "Testing set size = 2500; randomness in which servo fails, timing and severity; curriculum size for GC = 100",
            "variation_level": "high",
            "performance_metric": "Mean episodic reward and failure rate (%)",
            "performance_value": "Reward = 2342.61 ± 5.45; Failure rate = 2.35% ± 1.11% (GC, testing set)",
            "complexity_variation_relationship": "GC generates failure-focused curricula that are similar across examples enabling transfer of locomotion skills despite high-dimension dynamics; paper reports that structured similarity (moderate genetic distance) in curriculum helps robustness more than unstructured high variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via Genetic Curriculum (GC), base RL = TD3",
            "generalization_tested": true,
            "generalization_results": "Tested on 2500 randomized scenarios including actuator failures and payload perturbations; GC achieved low failure rate (2.35%), indicating good generalization.",
            "sample_efficiency": "Steps per epoch = 5e4; number of epochs = 60 → ~3.0e6 environment interactions in training schedule; GC sustains improvements per interaction longer than baselines.",
            "key_findings": "GC-generated curricula focusing on similar failure scenarios improve robustness in high-dimensional locomotion tasks; structured curricula outperform both purely random failure selection and curricula lacking crossover.",
            "uuid": "e1088.3",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning via Genetic Curriculum",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GC-Hopper",
            "name_full": "Policy trained with Genetic Curriculum on Hopper (legged robot)",
            "brief_description": "A simulated hopper robot policy trained with GC (TD3) to be robust to randomized actuator torque limits and added payload.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "GC-trained policy (TD3) - Hopper",
            "agent_description": "An RL policy trained with TD3 under Genetic Curriculum; environment includes random servo torque limits (75–100%) and a simulated payload (size 0.75); failure defined as payload touching the ground.",
            "agent_type": "simulated agent",
            "environment_name": "Hopper (modified)",
            "environment_description": "Single-legged hopping robot simulation with added payload and random actuator failures; complexity from balancing, payload dynamics, and dealing with partial actuator performance.",
            "complexity_measure": "Actuator torque limits (75–100%), payload mass (0.75), continuous multi-timestep balancing dynamics",
            "complexity_level": "high",
            "variation_measure": "Testing set size = 2500; stochastic failures parameters and initial conditions; curriculum size for GC = 100",
            "variation_level": "high",
            "performance_metric": "Mean episodic reward and failure rate (%)",
            "performance_value": "Reward = 2283.48 ± 2.01; Failure rate = 7.30% ± 2.79% (GC, testing set)",
            "complexity_variation_relationship": "GC improves robustness by generating similar failure scenarios enabling skill transfer; paper notes that in Hopper the improvement is present but less pronounced than other benchmarks, indicating task-specific sensitivity to curriculum design and variation coverage.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Curriculum learning via Genetic Curriculum (GC), base RL = TD3",
            "generalization_tested": true,
            "generalization_results": "Evaluated on 2500 randomized test cases; GC reduced failure rate relative to baselines (to 7.30%) indicating improved robustness under payload and actuator variations.",
            "sample_efficiency": "Steps per epoch = 5e4; number of epochs = 40 → ~2.0e6 environment interactions in scheduled training; GC shows sustained learning improvements per interaction compared to baselines.",
            "key_findings": "GC yields lower failure rates in hopper locomotion despite high dynamic complexity; effectiveness depends on balancing similarity in curricula (for transfer) and coverage (for generalization).",
            "uuid": "e1088.4",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning via Genetic Curriculum",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "GeneticCurriculum",
            "name_full": "Genetic Curriculum (GC)",
            "brief_description": "A non-parametric curriculum generation algorithm using genetic operators (crossover + mutation) on raw, variable-length scenario encodings to generate failure-focused curricula for robust RL.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GC-trained policies (general)",
            "agent_description": "Genetic Curriculum is a training-time method that (outside the inner training loop) runs a genetic algorithm to find scenarios the current policy fails on, collects a curriculum of similar failure scenarios, and trains the RL policy (SAC/TD3) on this curriculum to improve robustness.",
            "agent_type": "training procedure for simulated agents",
            "environment_name": "Various simulated benchmarks (BipedalWalkerHardcore, BipedalWalkerSystem, LunarLander, Walker, Hopper)",
            "environment_description": "Supports environments with raw non-fixed-length scenario encodings (sequences of RNG outputs) and high procedural variation, including obstacle courses, actuator failures, and payload perturbations.",
            "complexity_measure": "Scenario encoding length L(ψ) (raw non-fixed-length sequences), genetic distance between scenarios, number of variables in a scenario sequence; reported encoding expansion from ~20 to ~300 dimensions",
            "complexity_level": "adaptive (GC allows encoding length to grow up to ~300D when needed), effectively enabling high complexity",
            "variation_measure": "Curriculum size (300 for BipedalWalker, 100 for other benchmarks), population sizes (M_pop ~100), mutation prob p_μ (0.1), testing set sizes (1000–2500), mean genetic distance between curriculum examples",
            "variation_level": "adaptive from low-to-high depending on agent needs (GC both preserves similarity and introduces variation via mutation)",
            "performance_metric": "Mean episodic reward and failure rate (%) on held-out testing sets; improvement vs baselines reported as multiplicative reductions in failure rate (2–8x)",
            "performance_value": "Across benchmarks GC yields e.g., BipedalWalkerHardcore: Reward 304.33 ±1.65, Failure 3.96% ±0.37%; improvements reported as agents being 2–8x less likely to fail compared to baseline SAC/TD3 depending on benchmark.",
            "complexity_variation_relationship": "Paper explicitly analyzes trade-offs: curricula must balance similarity (to enable transfer; lower mean genetic distance ≈10) and coverage/variation (to generalize); GC uses crossover to produce similar offspring and mutation to introduce novel variations — empirical ablation shows both are necessary: no-crossover or random failure sampling increase mean genetic distance and failure rates, while single-run (too low variation) yields poor coverage and poor robustness.",
            "high_complexity_low_variation_performance": "Single Run ablation (single repeated failure example; very low variation): Reward = 99.45; Failure = 33.33% (BipedalWalkerHardcore ablation)",
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Random Failure ablation (high genetic distance ≈23.34): Reward = 251.37; Failure = 24.50% (BipedalWalkerHardcore ablation) — indicates unstructured high variation hurts transfer",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Non-parametric curriculum learning via genetic algorithm (crossover prioritized for mixing failure subsequences; occasional mutation to introduce novelty); separates GA from policy training (GA runs outside inner optimization loop).",
            "generalization_tested": true,
            "generalization_results": "GC-trained agents evaluated on large randomized testing sets per benchmark; in all reported benchmarks GC reduced failure rates substantially compared to base RL and many baselines, demonstrating improved generalization across procedurally generated scenario spaces.",
            "sample_efficiency": "GC requires extra evaluation cost for running GA but achieves better marginal utility per environment interaction over time; reported per-benchmark training regimes (e.g., 1e5 steps/epoch × 350 epochs for BipedalWalkerHardcore); GC slower initially but sustains improvement and converges to more robust policies compared to baselines.",
            "key_findings": "Non-parametric curriculum generation using raw variable-length encodings allows GC to adapt scenario complexity as agent improves, producing curricula of similar failure examples that transfer skills effectively; balancing similarity and coverage is crucial — crossover (to keep similarity) plus mutation (to ensure exploration) yields best robustness; GC outperforms adversarial methods and parametric curriculum methods on failure-rate metrics while maintaining reward.",
            "uuid": "e1088.5",
            "source_info": {
                "paper_title": "Robust Reinforcement Learning via Genetic Curriculum",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Poet: openended coevolution of environments and their optimized solutions",
            "rating": 2
        },
        {
            "paper_title": "Self-paced deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Fingerprint policy optimisation for robust reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Robust adversarial reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "Risk averse robust adversarial reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "Action robust reinforcement learning and applications in continuous control",
            "rating": 1
        },
        {
            "paper_title": "OpenAI Gym",
            "rating": 1
        },
        {
            "paper_title": "CARLA: An open urban driving simulator",
            "rating": 1
        }
    ],
    "cost": 0.017877749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Robust Reinforcement Learning via Genetic Curriculum</h1>
<p>Yeeho Song ${ }^{1}$ and Jeff Schneider ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Achieving robust performance is crucial when applying deep reinforcement learning (RL) in safety critical systems. Some of the state of the art approaches try to address the problem with adversarial agents, but these agents often require expert supervision to fine tune and prevent the adversary from becoming too challenging to the trainee agent. While other approaches involve automatically adjusting environment setups during training, they have been limited to simple environments where low-dimensional encodings can be used. Inspired by these approaches, we propose genetic curriculum, an algorithm that automatically identifies scenarios in which the agent currently fails and generates an associated curriculum to help the agent learn to solve the scenarios and acquire more robust behaviors. As a non-parametric optimizer, our approach uses a raw, non-fixed encoding of scenarios, reducing the need for expert supervision and allowing our algorithm to adapt to the changing performance of the agent. Our empirical studies show improvement in robustness over the existing state of the art algorithms, providing training curricula that result in agents being 2 - 8x times less likely to fail without sacrificing cumulative reward. We include an ablation study and share insights on why our algorithm outperforms prior approaches.</p>
<h2>I. INTRODUCTION</h2>
<p>When training an RL agent, learning to solve the remaining $10 \%$ of the scenarios is often significantly more difficult compared to learning to solve the first $90 \%$ of the scenarios. This presents a challenge to using RL in safety critical applications, such as autonomous vehicles, where robustness, the probability of an agent not landing in irreversible and catastrophic states (i.e. collision) plays a crucial role in determining product viability. In a typical RL setup, as an agent's performance improves, it becomes not only rare to encounter and collect data on the scenarios in which the agent does poorly but also difficult to learn new behaviors when approaching a local minimum. This results in an agent converging to a suboptimum with several scenarios left unsolved.</p>
<p>One prominent approach for robust RL is to use adversarial agents to inject adversarial noise to explore challenging situations. However, adversarial agents often converge to the worst case scenario in which the protagonist cannot learn and requires expert supervision to avoid this issue. Some scenarios are not well represented by adversarial noise, such as a particular sequence of tasks or environment setup difficult for the agent. While other approaches involve encoding the environment or generating a curriculum to help learn difficult tasks, they are mostly limited to benchmarks with small scenario space where low-dimensional encodings can be used.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>In this paper, we propose genetic curriculum (GC) which uses a genetic algorithm to generate curricula for training robust RL agents. By running a genetic algorithm, GC will generate training scenarios that the agent cannot solve, helping the agent to explore the scenario space efficiently. As scenarios generated by the genetic algorithm will be similar to each other, a skill learned from one scenario can easily be transferred to another scenario, allowing them to work as a curriculum helping the agent to learn faster and converge to a more optimal policy. As our algorithm is non-parametric, it can use raw scenario encoding of nonfixed length, minimizing expert supervision of designing encoding methods and helping support highly complex scenario description as the agent's performance improves.</p>
<h2>II. Related Works</h2>
<p>In robust RL where an agent should be trained against and verified in a variety of different scenarios, recent advances in sim2real [1]-[4] and high fidelity simulators [5], [6] makes it feasible to collect realistic training data in scenarios too dangerous and difficult to collect in real life. However, even with this setup, an agent would often leave a long tail of unsolved scenarios. As an agent becomes more robust, it becomes less likely to encounter and collect data from situations where the agent fails. Even when data is available, it is often difficult to learn new skills as the agent would often be approaching a local minimum optimized towards more probable scenarios.</p>
<p>Adversarial training is one such method for gathering data in the region where the agent does not do well. Showing success with classical RL [7]-[9] and deep learning architectures [10]-[13], adversarial training in RL pairs a protagonist agent with an adversary agent each playing a zero-sum game of maximizing/minimizing reward in the environment. Robust adversarial RL (RARL) [14] uses an adversary to apply external force to the protagonist. Risk averse robust adversarial RL (RARARL) [15], probabilistic action robust Markov decision process (MDP) and robust action robust MDP (PRMDP / NRMDP) [16] uses adversaries to inject action noise. However, some challenging situations are difficult to represent as noise, such as particularly hard scenarios or environment setups. Also, such a min-max setup often leads to the adversary quickly converging to a worst-case scenario too difficult for the protagonist to learn. Our approach differs by not only encoding scenarios and environment setup instead of adversarial noise but also generating supporting scenarios that help the agent to learn new skills.</p>
<p>Fingerprint policy optimization (FPO) [17] shares insights on encoding environments and scenarios. Building upon</p>
<p>the previous works on classical RL [18] [19], FPO uses Bayesian optimization to select the training setup with the biggest expected performance improvement. However, such approaches have been limited to low-dimensional fixed-length encoding for training environments. Our approach differs by using non-parametric optimizers to use non-fixed length encoding. This allows us to minimize expert supervision by directly using the raw values of a simulator while being more versatile to adapt to the agent’s changing needs with no information loss.</p>
<p>Curricular learning explores generating supporting scenarios to help learn new skills. Organizing training data to gradually introduce more complex concepts, [20], curricular learning has shown success in supervised learning tasks [21]–[25] as well as various RL tasks [26]–[33]. Automatically generating a curriculum of similar yet gradually more complex scenarios is an ongoing question in curriculum learning. Self-paced deep RL (SPDL) [34] is one such approach of exploring how curriculum can automatically be generated based on the agent’s current performance. At each epoch, SPDL locates the distribution of scenarios the agent currently performs well and will select training scenarios as a distribution progressively moving towards the goal distribution. However, SPDL likewise has been limited to low-dimensional fixed length encoding of scenarios. We will explore non-parametric approaches to expand the ideas to non-fixed raw encoding of curriculum.</p>
<p>Fig. 1. Curriculum generation during POET [35]. Unlike our proposed approach, POET runs training inside genetic algorithm, greatly increasing computational load.</p>
<p>Paired open-ended trailblazer (POET) [35] borrows several elements from genetic algorithms to use non-fixed length encodings. As shown in Figure 1, POET starts by adding a pair consisting of a random agent $\left(A_{0}\right)$ and a random training example $\left(E_{0}\right)$. During an epoch, all pairs that haven’t reached a satisfactory performance are trained in parallel. At the end of each epoch, a new pair is generated by adding a small random perturbation, or mutation, to an existing example and pairing it up with a new agent that inherits the parameter weights of the existing agent that performs best on the new example. As POET has training nested inside the genetic algorithm, the algorithm is computationally expensive as resources spent by any agents not contributing to the performance of the best-performing agent are wasted. Our approach is computationally more efficient by separately running the genetic algorithm outside of the loop. At the end of each epoch, the current policy is fixed and the genetic algorithm runs to generate a set of scenarios the agent cannot solve. Furthermore, POET uses mutation to generate new scenarios. This makes the search for new scenarios an inefficient random walk and makes it difficult to advance towards scenarios drastically different from the starting scenario. Our approach on the other hand incorporates crossover, the processing of mixing sequences from two parent sequences to generate two offspring, to help cover a wider variety of scenarios faster.</p>
<h2>III. BACKGROUND</h2>
<p>This paper examines continuous space MDP represented as a tuple: $\left[S,A,P_{\psi},r_{\psi},\gamma\right]$ where $S$ is a set of states, $A$ action space, and $\gamma\in[0,1)$ is the temporal discount factor. Scenario $\psi$ are not fully observable to the agent, such as obstacles situated out of the line of sight or an internal systems failure. $P_{\psi}$ and $r_{\psi}$ represent the state transition dynamics and reward function of the scenario. In the event of a partial engine failure related to fuel pumps, the engine would be burning less fuel per second and will be delivering less thrust, hence the state transition probability and reward on fuel usage will be different from those of a nominal scenario. The agent’s policy, $\pi(a~{}|~s)$ maps states $s \in S$ to $a \in A$. The utility of a policy $\pi$ for scenario $\psi$ is the expected return, $J_{\psi}(\pi)=\mathbb{E}<em t="t">{a</em>\right)$.} \sim \pi} \sum_{t} \gamma^{t} r_{\psi}\left(s_{t}, a_{t</p>
<p>During training, an RL algorithm seeks the optimal policy $\pi^{*}$ by exploring and gathering data about reward and state dynamics. The data gathered will be dependent on the distribution of scenarios the agent experienced during training $p_{\text {train }}(\psi)$;</p>
<p>$$
\pi^{*}=\operatorname{argmax} \sum_{\psi} p_{\text {train }}(\psi) J_{\psi}(\pi)
$$</p>
<h2>IV. Problem Statement</h2>
<p>For a given scenario, $\psi$, we measure success as follows;</p>
<p>$$
G_{\psi}(\pi)= \begin{cases}0, &amp; \text { if } \pi \text { fails for } \psi \ 1, &amp; \text { otherwise }\end{cases}
$$</p>
<p>Failure is defined as failing to achieve a goal before exhausting a resource budget set by the user. This could be a walking robot falling down before reaching a target, a wheeled robot crashing into a stationary object, a manipulator robot reaching certain time steps with a cumulative reward lower than a threshold. A robust algorithm should minimize the probability of failure during testing;</p>
<p>$$
\pi_{\text {robust }}^{*}=\operatorname{argmax} \sum_{\psi} p_{\text {test }}(\psi) G_{\psi}(\pi)
$$</p>
<p>Ideally, the trained agent should satisfy the robustness criteria $\pi^{<em>}=\pi_{\text {robust }}^{</em>}$. As the distribution of scenarios encountered during testing, $p_{\text {test }}(\psi)$, and the definition of failure $G_{\psi}(\pi)$, are problem specific, most papers focus on $J_{\psi}(\pi)$ and $p_{\text {train }}(\psi)$. While reward shaping with $J_{\psi}(\pi)$ is possible, such as giving a high penalty towards failure, this often requires expert supervision and fine-tuning for the training to be stable. We, therefore, take the curricular approach of investigating how $p_{\text {train }}(\psi)$ can be better selected to train a robust agent.</p>
<h2>V. APPROACH</h2>
<p>To train a robust agent, we select training scenarios as scenarios the agent currently fails in. Solving these examples directly addresses Equation (3). We also select the scenarios to be similar to each other. This follows the idea of curricular learning on building a set of similar scenarios with varying types of challenges and levels of difficulty to help transfer skills from one task to another more easily. Also, just as adversarial RL adds perturbations to make an agent robust to a variety of situations, the differences in our scenarios will help an agent learn not only a specific task but also a variety of similar tasks as well. This paper proposes GC, which borrows concepts from genetic algorithms and curriculum learning to achieve these goals.</p>
<p>At each epoch, curriculum generation starts by initializing a population $\Psi_{\text {population }}$ of size $M_{\text {pop }}$ with randomly generated scenarios. We express scenarios as a sequence of none-fixed length $\psi=\left(z_{0}, z_{1}, z_{2}, \ldots\right)$ where each vector $z$ defines the order of values to be used which would otherwise be filled in by a random number generator in the original benchmark. Factors of variation include size and duration of obstacles and bumps on terrain to the time of occurrence, or type and magnitude of an actuator failure of a legged robot depending on the benchmark. At each iteration, $\Psi_{\text {population }}$ is evaluated by current policy $\pi . \psi$ is appended to $\Psi_{\text {training }}$ if $\pi$ is unable to solve $\psi$.</p>
<p>The next $\Psi_{\text {population }}$ is generated by crossover. With $L(\psi)$ as the length of encoding for $\psi$, the probability of a scenario being chosen as a parent for a crossover operation is higher if the scenario's encoding is shorter. This encourages sequences to only retain the sections critical to failure and avoid having offspring diverse in irrelevant ways. A selected parent change a random section of its encoding with a random section of another parent's encoding as shown in Figure 2. The crossover operation repeats until $\left|\Psi_{\text {population }}\right| \geq M_{\text {pop }}$. To introduce new vectors to the gene pool, every $\psi$ in $\Psi_{\text {population }}$ has mutation probability $p_{\mu}$. The mutation is equivalent to conducting crossover with a randomly generated sequence as shown in Figure 2.</p>
<p>Once $\left|\Psi_{\text {population }}\right| \geq M_{\text {pop }}$, GC exits the scenario generation cycle and $\Psi_{\text {training }}$ is used to train $\pi$ for an epoch. Algorithm 1 shows the pseudocode of our proposed approach.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 2. Visualization on crossover and mutation. Our approach use genetic algorithm to use raw, non-fixed length encoding to generate similar scenarios that can act as a curriculum and dynamically change length of encoding to keep up with agent's performance.</p>
<p>$$
p_{\text {parent }}(\psi)= \begin{cases}\frac{1}{(\max (L(i))-L(i)+1)}, &amp; \text { if } \pi \text { fails } \psi \ 0, &amp; \text { otherwise }\end{cases}
$$</p>
<p>Algorithm 1 Genetic Curriculum (GC)
1: Initialize Policy $\pi_{0}$
2: Input training steps, iterations, epochs, $M_{\text {train }}, M_{\text {pop }}, p_{\mu}$
3: for $i$ in epochs do
4: Initialize $\Psi_{\text {population }}$
5: Initialize $\Psi_{\text {train }}={ }$
6: for $k$ in iterations do
7: Fitness $=$ evaluate $\left(\Psi_{\text {population }}, \pi_{i+1}\right)$
8: $\Psi_{\text {train }}=$ collect $\left(\Psi_{\text {train }}, \Psi_{\text {population }}\right.$, Fitness $)$
9: if $\left|\Psi_{\text {train }}\right|&gt;M_{\text {train }}$ then
10: Break
11: end if
12: $\Psi_{\text {population }}=$ crossover $\left(\Psi_{\text {population }}, M_{\text {pop }}\right.$, Fitness $)$
13: $\Psi_{\text {population }}=$ mutate $\left(\Psi_{\text {population }}, p_{\mu}\right)$
14: end for
15: while steps $&lt;$ training steps do
16: $\pi_{i+1}=$ Train Agent $\left(\pi_{i}, \Psi_{\text {train }}\right)$
17: end while
18: end for</p>
<p>Our algorithm has several desirable features. As a nonparametric optimizer, it is easy to adapt to various types of tasks and policies. $\psi$ not only has no fixed length, but as visualized in Figure 2, offspring scenarios can easily get longer or shorter during curriculum generation. This allows encoding length to expand and contract as needed to accommodate changes in the agent's performance over time. During the experiments, the encoding dimension dynamically changed from 20 - 300D, which would have been difficult with Bayesian optimization as used in FPO [17]. Another feature visible in Figure 2 is that scenarios within a curriculum will be similar to each other. With the crossover and mutation operations, all scenarios have part of their sequence shared recurring in another scenario. This similarity makes it easier to transfer skills from one to another.</p>
<h2>VI. EXPERIMENTS</h2>
<h2>A. Benchmarks</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3. Screenshot of Benchmarks used in this paper (from left to right), BipedalWalkerHardcore/System), LunarLander, Hopper, and Walker which tests agent's robustness against a variety of obstacle courses / actuator failure.</p>
<p>BipedalWalkerHardcore [36] involves agent observing the world with LIDAR, IMU, and joint encoder values to control torque on each of the bipedal walker's leg servos. The goal is to traverse through a randomly generated obstacle course filled with stairs, pitfalls, and walls. While individual obstacles are easy, the challenge is to learn a robust policy that can solve a variety of sequences of obstacles without falling.</p>
<p>BipedalWalkerSystem is a modified version of the above where the agent traverses through a fixed sequence of obstacles with simulated random system failures. When a failure is triggered at a random timestep, the affected servo will only be able to deliver $60-100 \%$ of the original power</p>
<p>TABLE I: Training Duration and Testing Resolution</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Steps per</th>
<th>Testing</th>
<th>Number of</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Epoch</td>
<td>Set Size</td>
<td>Epochs</td>
</tr>
<tr>
<td>BipedalWalkerHardcore</td>
<td>1e5</td>
<td>1000</td>
<td>350</td>
</tr>
<tr>
<td>BipedalWalkersystem</td>
<td>1e5</td>
<td>2500</td>
<td>30</td>
</tr>
<tr>
<td>LunarLander</td>
<td>1e4</td>
<td>2500</td>
<td>80</td>
</tr>
<tr>
<td>Walker</td>
<td>5e4</td>
<td>2500</td>
<td>60</td>
</tr>
<tr>
<td>Hopper</td>
<td>5e4</td>
<td>2500</td>
<td>40</td>
</tr>
</tbody>
</table>
<p>depending on the severity. While individual scenarios are easy, the challenge is to learn a robust kinetic energy management skill to go through obstacles even if an actuator fails.</p>
<p>LunarLander is a modified benchmark of the one provided by [36]. Using position and velocity observations, an agent has to safely land on the landing pad using its main engine (ME) and side thrusters. When failure is triggered at a random timestep, the throttle of the affected rocket motor is limited to 60 - 100% of the nominal power. Under nominal scenarios, the best policy is to wait until the last moment and fire the ME at full thrust to minimize fuel used. However, if a system failure occurs, the lander may crash due to ME being unable to provide enough thrust. A robust policy should keep the rate of descent to a manageable level to maximize the possibility of landing even when a failure occurs.</p>
<p>Hopper and Walker are modified benchmarks based on the original versions provided by [37]. When a random system failure occurs, a torque limit of 75 - 100% of the nominal maximum is applied to the affected servo. To make the benchmarks more challenging, we mount a simulated payload of sizes 0.75 and 0.5 on the Hopper and Walker legged robots. A policy is considered to have failed if its simulated payload touches the ground.</p>
<h3>V-B Baseline Algorithms</h3>
<p>Some of the state of the art approaches have been chosen as follows. To compare GC against adversarial RL approaches, we chose RARL [14], RARARL [15], and PRMDP / NRMDP [16]. We chose FPO [17] for comparison against approaches that control the training environment. For comparison against curricular RL, we chose SPDL [34] for parametric curricular approaches and POET [35] for non-parametric approaches.</p>
<p>The algorithms in this paper require a base RL algorithm for updating policies. RL algorithms listed on top of the respective leaderboards for the original version of the benchmarks are used as base RL algorithms. BipedalWalker and LunarLander use soft actor-critic (SAC) [38], while Hopper and Walker use twin delayed DDPG (TD3) [39]. SAC uses $\gamma=0.99$, learning rate of 1e-4, batch size of 100, and replay memory size of 1e6, while TD3 uses $\gamma=0.98$, learning rate of 3e-4, batch size of 100, and replay memory size of 2e5. Both algorithms use fully connected networks consisting of layers sized 400 and 300 updated by ADAM [40] and activated with the ReLU function.</p>
<h3>V-C Evaluation and Hyperparameters</h3>
<p>One of the main challenges for comparing the performance of each algorithm is the vastly different computation requirements of each algorithm during training. FPO, POET, SPDL, and ours require additional steps for evaluating the agent’s performance. While this can be expensive, evaluation can not only run in parallel but is also cheaper than exploration which requires backpropagation. Adversarial RL algorithms, on the other hand, had extra computational costs for training both protagonist and adversarial networks at the same time. As we are concerned about how robust a converged solution is, we report results based on how many epochs have passed. Each epoch consists of the same numbers of policy updates and exploration steps per benchmark. To share insights in cases where the total number of environment interactions is more important, we also include a separate set of experiments on the LunarLander benchmark where values are reported based on how many steps each algorithm interacted with the simulator.</p>
<p>We report performance with mean and standard error on 10 random seeds per algorithm per benchmark, with testing sets consisting of randomly generated scenarios. For BipedalWalker benchmarks, only 3 random seeds were used to balance accuracy and computational cost. This is because it would take 7 - 14 days to approach convergence for such benchmarks. In the case of POET where multiple agents are trained simultaneously, we report the performance of the best agent in terms of reward as the result of the random seed. Table I shows the length of each epoch, testing set size, and the number of epochs for each benchmark.</p>
<p>To offer a fair comparison, a hyperparameter search is conducted for adversarial RL algorithms as shown in Table II. Hyperparameters that performed the best overall throughout the benchmarks were selected.</p>
<p>The size of policy evaluation for FPO, POET, SPDL is the same as the size of policy evaluation used for reporting performance during training. POET also requires manual reward thresholds on what is considered as not too trivial nor difficult before adding an scenario for training. BipedalWalker benchmarks use the same threshold of 50 - 300 as used in the original POET paper. For other benchmarks, we selected the value by checking their training curves and marking when the reward starts to climb and flatten. The threshold is set as 100 - 250 for LunarLander and 1000 - 2000 for Walker and Hopper benchmarks.</p>
<p>The default versions of the benchmarks random use number generators to create scenarios at each run. For FPO and SPDL, we engineered a fixed-length encoder where the range of the numbers coming out of the random number generator was defined. For POET and our method, the string of numbers to be used in the place of the random number generator was stored in a sequence.</p>
<p>For GC, the curriculum size is 300 for BipedalWalker</p>
<p>benchmarks and 100 for the rest, which is a rounded value on how many times the simulators are reset per each epoch. The size of parent and offspring populations is 100 each which is a rounded value on the minimum size required to have at least two or three failure sequences upon random initialization to act as parents for subsequent generations. While hyperparameter tunning was also conducted on $p_{\mu}$, GC didn’t show much sensitivity towards $p_{\mu}$ and a value of 0.1 is used. The GC’s reliance on crossover more than the mutation rate is highlighted in the ablation study.</p>
<h2>D. Ablation Study</h2>
<p>To better understand how our approach help improves the robustness of an agent, an ablation study with BipedalWalkerHardcore is conducted. When generating a curriculum filled with failure scenarios, NoMutation turns off mutation, NoCrossover turns off crossover, and RandomFailure fills a curriculum with examples that are randomly generated and are unsolvable when tested by current policy. To see how a genetic algorithm can generate a curriculum of similar examples and its effect on performance, the mean genetic distance of a curriculum is also reported. Every time a new example is loaded during training, genetic distance is calculated by counting the minimum number of variables that have to be changed, added, and deleted to convert the previous example to the new example. Single Run provides additional data on the effect of genetic distance on robustness by generating a curriculum consisting of one failure scenario, making the mean genetic distance of the curriculum to be zero.</p>
<h2>VII. RESULTS</h2>
<h2>A. Comparison with Baseline Algorithms</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4. Training Curve for BipedalWalkerHardcore. Lower the better
As shown in Figure 4 and Table III, our proposed GC consistently improves over the state of the art algorithms especially in terms of robustness. Agents trained by GC are 2 - 8x times less likely to fail compared to those trained on baseline SAC / TD3.</p>
<p>One interesting observation from Table III is that even when agents show quite a difference in performance in terms of robustness, such differences are less obvious when looking at rewards only. When trained, big reward coming from the majority of the cases tends to dominate over bad
rewards coming from the minority of cases. During training, similar issues are observed where the failure rate continues to converge when the reward does not show significant progress. This highlights the challenge of capturing robustness alone by reward and using it to optimize the policy.</p>
<p>The results from FPO and SPDL highlight the GC's benefit of using raw scenario encoding of non-fixed length. As a nonparametric optimizer, GC can adapt the length of its encoding to generate more complex scenarios as the protagonist agent's performance improves. The effect is most well observed in the case of BipedalWalkerHardcore where GC expanded the encoding size by 10 - 15 times during training to precisely describe the what training scenarios should be.</p>
<p>Also, the results from adversarial RL show selecting a supporting curriculum is as important as generating challenging scenarios during training. While adversarial agents can generate challenging situations during training, they do not present in a way that the trainee can easily learn new skills. The challenges of injecting difficult cases without supporting a curriculum are further highlighted in the following section on the ablation study.</p>
<p>The effect of not nesting training within the genetic algorithm can be observed from POET's results. The original POET paper reported a reward of around 250 when trained and tested on scenarios based on the BipedalWalkerHardcore. POET showed a similar performance when evaluated on the training set during our experiments. However, when evaluated against the testing set which includes the entire scenario space as designed by the benchmark, POET performed poorly agent's skills were not generalizable across a wide variety of scenarios. As POET is computationally expensive and relies only on mutation for curriculum generation, each trained agent could only experience less training data from a less diverse set of scenarios compared to those from GC.</p>
<p>An interesting observation from the trained policies is while there are some scenarios where an agent had more difficulty solving than the others, there was no clear trend describing which scenario is objectively more difficult than others or a priori difficult scenarios where solving one case means being able to solve all the easier cases. When a scenario that a trained agent consistently fails are used as a training scenario to a randomly initialized agent, the agent would learn how to solve the scenario. However, regardless of the random seed used, finding a general policy that solves all the scenarios was difficult. This shares insight that a robust training scheme should not only focus on performance per each task but also on learning a general skillset applicable across the tasks.</p>
<h2>B. Comparison with Respect to Environment Interactions</h2>
<p>One of the important criteria in RL is how efficiently it can learn per the number of environment interactions. Figure 5 shows that while GC is a bit slow at the start due to the extra cost of running genetic algorithms, the cost is offset by having better training examples. Unlike the baseline RL (SAC) and adversarial RL methods where marginal utility per environment interaction quickly diminishes, GC can sustain</p>
<p>TABLE III: Reward and Mean Failure Rate of Trained Agents(%)</p>
<table>
<thead>
<tr>
<th>Reward</th>
<th>BipedalWalkerHardcore</th>
<th>BipedalWalkerSystem</th>
<th>LunarLander</th>
<th>Walker</th>
<th>Hopper</th>
</tr>
</thead>
<tbody>
<tr>
<td>Algorithm</td>
<td>291.76 ± 17.41</td>
<td>300.94 ± 1.85</td>
<td>265.30 ± 1.92</td>
<td>2300.81 ± 29.30</td>
<td>2266.64 ± 3.05</td>
</tr>
<tr>
<td>Base RL (SAC / TD3)</td>
<td>7.67 ± 13.49</td>
<td>289.25 ± 5.08</td>
<td>28.00 ± 12.24</td>
<td>122.60 ± 4.58</td>
<td>203.12 ± 4.53</td>
</tr>
<tr>
<td>RARL</td>
<td>230.14 ± 19.52</td>
<td>270.89 ± 13.59</td>
<td>272.29 ± 0.80</td>
<td>2156.48 ± 10.31</td>
<td>2199.82 ± 3.18</td>
</tr>
<tr>
<td>RARARL</td>
<td>285.30 ± 25.66</td>
<td>298.42 ± 0.23</td>
<td>260.73 ± 2.28</td>
<td>2165.19 ± 11.36</td>
<td>2275.72 ± 2.04</td>
</tr>
<tr>
<td>PRMDP</td>
<td>289.82 ± 19.25</td>
<td>291.42 ± 5.05</td>
<td>254.99 ± 1.27</td>
<td>2147.51 ± 1.30</td>
<td>2092.10 ± 3.85</td>
</tr>
<tr>
<td>NRMDP</td>
<td>118.60 ± 1.21</td>
<td>286.47 ± 10.84</td>
<td>256.31 ± 4.61</td>
<td>2134.83 ± 5.09</td>
<td>2044.73 ± 3.21</td>
</tr>
<tr>
<td>FPO</td>
<td>24.60 ± 18.61</td>
<td>-62.58 ± 14.14</td>
<td>213.30 ± 3.94</td>
<td>2068.50 ± 31.01</td>
<td>2129.93 ± 1.81</td>
</tr>
<tr>
<td>SPDL</td>
<td>305.90 ± 0.45</td>
<td>289.13 ± 5.19</td>
<td>221.31 ± 10.71</td>
<td>589.78 ± 74.04</td>
<td>2274.70 ± 13.06</td>
</tr>
<tr>
<td>GC (Proposed)</td>
<td>304.33 ± 1.65</td>
<td>300.00 ± 1.00</td>
<td>272.82 ± 0.30</td>
<td>2342.61 ± 5.45</td>
<td>2283.48 ± 2.01</td>
</tr>
<tr>
<td>Failure Rate(%)</td>
<td>BipedalWalkerHardcore</td>
<td>BipedalWalkerSystem</td>
<td>LunarLander</td>
<td>Walker</td>
<td>Hopper</td>
</tr>
<tr>
<td>Algorithm</td>
<td>10.20 ± 0.71</td>
<td>3.62 ± 0.58</td>
<td>5.19 ± 1.20</td>
<td>4.31 ± 1.00</td>
<td>12.99 ± 3.52</td>
</tr>
<tr>
<td>Base RL (SAC / TD3)</td>
<td>91.27 ± 3.28</td>
<td>5.97 ± 2.87</td>
<td>73.56 ± 13.52</td>
<td>79.01 ± 16.82</td>
<td>85.61 ± 9.77</td>
</tr>
<tr>
<td>RARL</td>
<td>27.69 ± 3.14</td>
<td>15.29 ± 5.27</td>
<td>4.33 ± 0.88</td>
<td>3.85 ± 0.41</td>
<td>14.36 ± 5.27</td>
</tr>
<tr>
<td>RARARL</td>
<td>11.23 ± 0.36</td>
<td>2.85 ± 0.40</td>
<td>2.24 ± 0.90</td>
<td>3.88 ± 1.75</td>
<td>12.46 ± 4.70</td>
</tr>
<tr>
<td>NRMDP</td>
<td>11.00 ± 1.27</td>
<td>5.02 ± 1.38</td>
<td>6.41 ± 1.15</td>
<td>6.96 ± 2.09</td>
<td>28.32 ± 5.91</td>
</tr>
<tr>
<td>FPO</td>
<td>67.60 ± 19.05</td>
<td>8.30 ± 4.55</td>
<td>11.73 ± 2.71</td>
<td>12.12 ± 4.83</td>
<td>38.33 ± 5.11</td>
</tr>
<tr>
<td>POET</td>
<td>84.96 ± 9.45</td>
<td>100 ± 0.00</td>
<td>29.53 ± 2.36</td>
<td>12.31 ± 7.40</td>
<td>24.98 ± 7.26</td>
</tr>
<tr>
<td>SPDL</td>
<td>20.87 ± 7.40</td>
<td>12.57 ± 2.41</td>
<td>27.47 ± 4.13</td>
<td>21.18 ± 6.56</td>
<td>8.69 ± 6.57</td>
</tr>
<tr>
<td>GC (Proposed)</td>
<td>3.96 ± 0.37</td>
<td>2.16 ± 0.45</td>
<td>0.64 ± 0.02</td>
<td>2.35 ± 1.11</td>
<td>7.30 ± 2.79</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 5. Characteristic Training Curve from LunarLander Benchmark with Respect to Environment Interaction Steps</p>
<p>TABLE IV: Reward, Failure Rate, and Mean Genetic Distance between Training Examples during Ablation Study</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Reward</th>
<th>Failure Rate(%)</th>
<th>Genetic Distance</th>
</tr>
</thead>
<tbody>
<tr>
<td>Base RL (SAC)</td>
<td>291.76</td>
<td>10.2</td>
<td>22.65</td>
</tr>
<tr>
<td>GC (Ours)</td>
<td>304.33</td>
<td>3.96</td>
<td>10.60</td>
</tr>
<tr>
<td>No Mutate</td>
<td>294.17</td>
<td>8.51</td>
<td>10.44</td>
</tr>
<tr>
<td>No Crossover</td>
<td>271.72</td>
<td>17.63</td>
<td>20.92</td>
</tr>
<tr>
<td>Random Failure</td>
<td>251.37</td>
<td>24.50</td>
<td>23.34</td>
</tr>
<tr>
<td>Single Run</td>
<td>99.45</td>
<td>33.33</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>the rate of performance improvement longer and converges to a better solution.</p>
<p>C. Ablation Study</p>
<p>One of the insights from Table IV is, except for Single Run, curricula with similar scenarios, i.e. shorter mean genetic distance, perform better. While Random Failure builds a curriculum with failed scenarios, the similarity between the scenarios is not ensured. In the case of No Crossover, genetic similarity between scenarios is low as unlike crossover which mixes sequences from two parents to generate two offsprings, mutation only creates one offspring from one parent. The</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 6. tSNE analysis on genetic distance of generated scenarios</p>
<p>difficulty in transferring skills between more distant scenarios seems to result in No Crossover and Random Failure performs poorly.</p>
<p>Figure 6 on the other hand highlights how a coverage over scenario space affects performance for curriculums with short mean genetic distance between scenarios. As an agent is trained based on scenarios it experiences, having curriculum scenarios more spread out in the scenario space can help the agent generalize across diverse scenarios. While Single Run keeps genetic distance between scenarios to a minimum, it offers a poor coverage of the scenario space as in Figure 6. Genetic Distance between scenarios generated by No Mutate is similar to those in GC, but the former offers narrower coverage of the scenario space. While No Mutate can only reorganize genetic sequences it had upon initialization, GC can introduce new sequences through mutation, allowing it to explore a wider scenario space and train a more robust agent.</p>
<p>VIII. CONCLUSION AND FUTURE WORKS</p>
<p>This paper proposes genetic curriculum, an RL algorithm that uses a genetic algorithm to generate a curriculum of scenario for training RL agents. Through empirical study, our algorithms show improvement over existing state-of-the-art approaches concerning robustness. Future works will focus on decreasing the computational load of our algorithms, improving rate of convergence, as well as implementing our method in real and more complex benchmarks.</p>
<h2>REFERENCES</h2>
<p>[1] I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, et al., "Solving rubik's cube with a robot hand," arXiv preprint arXiv:1910.07113, 2019.
[2] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox, "Closing the sim-to-real loop: Adapting simulation randomization with real world experience," in 2019 International Conference on Robotics and Automation (ICRA), pp. 8973-8979, IEEE, 2019.
[3] R. Kaushik, T. Anne, and J.-B. Mouret, "Fast online adaptation in robotics through meta-learning embeddings of simulated priors," in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 5269-5276, IEEE, 2020.
[4] N. Hansen, R. Jangir, Y. Sun, G. Alenyà, P. Abbeel, A. A. Efros, L. Pinto, and X. Wang, "Self-supervised policy adaptation during deployment," in International Conference on Learning Representations, 2020.
[5] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, "CARLA: An open urban driving simulator," in Proceedings of the 1st Annual Conference on Robot Learning, pp. 1-16, 2017.
[6] L. Research, "X-plane 11." https://www.x-plane.com, 2017.
[7] G. N. Iyengar, "Robust dynamic programming," Mathematics of Operations Research, vol. 30, no. 2, pp. 257-280, 2005.
[8] A. Nilim and L. El Ghaoui, "Robust control of markov decision processes with uncertain transition matrices," Operations Research, vol. 53, no. 5, pp. 780-798, 2005.
[9] S. Mannor, O. Mebel, and H. Xu, "Lightning does not strike twice: robust mdps with coupled uncertainty," in Proceedings of the 29th International Coference on International Conference on Machine Learning, pp. 451-458, 2012.
[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, "Generative adversarial nets," in Advances in neural information processing systems, pp. 2672-2680, 2014.
[11] A. Kurakin, I. Goodfellow, S. Bengio, Y. Dong, F. Liao, M. Liang, T. Pang, J. Zhu, X. Hu, C. Xie, et al., "Adversarial attacks and defences competition," in The NIPS'17 Competition: Building Intelligent Systems, pp. 195-231, Springer, 2018.
[12] P. Samangouei, M. Kabkab, and R. Chellappa, "Defense-gan: Protecting classifiers against adversarial attacks using generative models," in International Conference on Learning Representations, 2018.
[13] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song, "Generating adversarial examples with adversarial networks," in Proceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 39053911, 2018.
[14] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, "Robust adversarial reinforcement learning," in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 2817-2826, JMLR. org, 2017.
[15] X. Pan, D. Seita, Y. Gao, and J. Canny, "Risk averse robust adversarial reinforcement learning," in 2019 International Conference on Robotics and Automation (ICRA), pp. 8522-8528, IEEE, 2019.
[16] C. Tessler, Y. Efroni, and S. Mannor, "Action robust reinforcement learning and applications in continuous control," in International Conference on Machine Learning, pp. 6215-6224, PMLR, 2019.
[17] S. Paul, M. A. Osborne, and S. Whiteson, "Fingerprint policy optimisation for robust reinforcement learning," in International Conference on Machine Learning, pp. 5082-5091, PMLR, 2019.
[18] K. A. Ciosek and S. Whiteson, "Offer: Off-environment reinforcement learning," in Thirty-first AAAI conference on artificial intelligence, 2017.
[19] S. Paul, K. Chatzilygeroudis, K. Ciosek, J.-B. Mouret, M. A. Osborne, and S. Whiteson, "Alternating optimisation and quadrature for robust control," in Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
[20] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, "Curriculum learning," in Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.
[21] W. Zaremba and I. Sutskever, "Learning to execute," 2015.
[22] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, "Scheduled sampling for sequence prediction with recurrent neural networks," in Advances in Neural Information Processing Systems, pp. 1171-1179, 2015.
[23] A. Graves, M. G. Bellemare, J. Menick, R. Munos, and K. Kavukcuoglu, "Automated curriculum learning for neural networks," in Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1311-1320, JMLR. org, 2017.
[24] M. P. Kumar, B. Packer, and D. Koller, "Self-paced learning for latent variable models," in Advances in Neural Information Processing Systems, pp. 1189-1197, 2010.
[25] L. Jiang, D. Meng, Q. Zhao, S. Shan, and A. G. Hauptmann, "Self-paced curriculum learning," in Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015.
[26] M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda, "Purposive behavior acquisition for a real robot by vision-based reinforcement learning," Machine learning, vol. 23, no. 2-3, pp. 279-303, 1996.
[27] A. Karpathy and M. Van De Panne, "Curriculum learning for motor skills," in Canadian Conference on Artificial Intelligence, pp. 325-330, Springer, 2012.
[28] D. Held, X. Geng, C. Florensa, and P. Abbeel, "Automatic goal generation for reinforcement learning agents," 2018.
[29] B. Ivanovic, J. Harrison, A. Sharma, M. Chen, and M. Pavone, "Barc: Backward reachability curriculum for robotic reinforcement learning," in 2019 International Conference on Robotics and Automation (ICRA), pp. 15-21, IEEE, 2019.
[30] C. Florensa, D. Held, M. Wulfmeier, M. Zhang, and P. Abbeel, "Reverse curriculum generation for reinforcement learning," in Conference on robot learning, pp. 482-495, PMLR, 2017.
[31] C. Florensa, D. Held, X. Geng, and P. Abbeel, "Automatic goal generation for reinforcement learning agents," in International conference on machine learning, pp. 1515-1528, PMLR, 2018.
[32] S. Narvekar and P. Stone, "Learning curriculum policies for reinforcement learning," in Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 25-33, International Foundation for Autonomous Agents and Multiagent Systems, 2019.
[33] P. Fournier, M. Chetouani, P.-Y. Oudeyer, and O. Sigaud, "Accuracybased curriculum learning in deep reinforcement learning,"
[34] P. Klink, C. D’Eramo, J. Peters, and J. Pajarinen, "Self-paced deep reinforcement learning," in NeurIPS, 2020.
[35] R. Wang, J. Lehman, J. Clune, and K. O. Stanley, "Poet: openended coevolution of environments and their optimized solutions," in Proceedings of the Genetic and Evolutionary Computation Conference, pp. 142-151, 2019.
[36] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba, "Openai gym," arXiv preprint arXiv:1606.01540, 2016.
[37] B. Ellenberger, "Pybullet gymperium." https://github.com/ benelot/pybullet-gym, 2018-2019.
[38] createmind, "Deep reinforcement learning." https://github. com/createamind/DRL/tree/master/spinup/envs/ BipedalWalkerHardcore, 2019.
[39] A. Raffin, A. Hill, M. Ernestus, A. Gleave, A. Kanervisto, and N. Dormann, "Stable baselines3." https://github.com/ DLR-WM/stable-baselines3, 2019.
[40] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," arXiv preprint arXiv:1412.6980, 2014.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, USA. {yeehos, jeff4, ]@andrew.cmu.edu
This work was partly funded by The Boeing Company,&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
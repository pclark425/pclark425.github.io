<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4299 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4299</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4299</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-99.html">extraction-schema-99</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <p><strong>Paper ID:</strong> paper-271516269</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.18827v1.pdf" target="_blank">Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models</a></p>
                <p><strong>Paper Abstract:</strong> Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. It requires substantial effort and time to extract scientific information from these works. AM domain experts have contributed over two dozen review papers to summarize these works. However, information specific to AM and AI contexts still requires manual effort to extract. The recent success of foundation models such as BERT (Bidirectional Encoder Representations for Transformers) or GPT (Generative Pre-trained Transformers) on textual data has opened the possibility of expediting scientific information extraction. We propose a framework that enables collaboration between AM and AI experts to continuously extract scientific information from data-driven AM literature. A demonstration tool is implemented based on the proposed framework and a case study is conducted to extract information relevant to the datasets, modeling, sensing, and AM system categories. We show the ability of LLMs (Large Language Models) to expedite the extraction of relevant information from data-driven AM literature. In the future, the framework can be used to extract information from the broader design and manufacturing literature in the engineering discipline.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4299.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4299.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-AI Teaming IE Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-Artificial Intelligence Teaming for Scientific Information Extraction Framework (prototype tool)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-centered information extraction pipeline that combines PDF parsing (GROBID), paragraph-level embeddings, customizable semantic retrieval, LLM-based question answering, and paragraph classification to extract and distill paper-level information in additive manufacturing + ML literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Human-AI Teaming IE Framework (retrieval + LLM + classifier)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Pipeline: (1) Parse PDFs into paragraphs/equations using GROBID. (2) Compute paragraph embeddings with OpenAI text-embedding-ada-002 and use cosine similarity to retrieve top candidate paragraphs. (3) Allow human users to create/customize retrievals via ensembles of positive/negative queries and paragraphs; compute a retrieval embedding R as a weighted sum of averaged embeddings (Equation 1) to refine paragraph ranking. (4) Feed retrieved paragraphs plus a user query to an OpenAI chat LLM in a sequence-to-sequence formulation (prompt structure: Q + SEP + Passage1 + SEP + Passage2 ...), instructing the model to extract/distill the requested information or reply 'I cannot answer that' when absent. (5) Accumulate labeled paragraphs and train an offline multi-label paragraph classifier (they used OpenAI embeddings as features and a RandomForest MultiOutputClassifier) to accelerate future retrieval. Human-in-the-loop review is integrated at parsing, retrieval customization, and verification of LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>OpenAI chat API (unspecified chat model); OpenAI text-embedding-ada-002 for embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>Additive manufacturing (3D printing) research intersecting with machine learning / data-driven AM</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>100 papers</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td>None explicitly extracted; system extracts experimental metadata, data characteristics, modeling details, sensing and system descriptors rather than explicit quantitative laws (scaling laws/equations).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Textual answers to queries (natural-language distilled information), labeled paragraphs (multi-label categories), paragraph-level embeddings, and classifier labels; not mathematical equations or symbolic laws.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Human expert iterative labeling and cross-referencing of retrieved paragraphs; classifier validated on held-out test split of the labeled paragraph dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Paragraph-classifier test results (Random Forest MultiOutputClassifier on paragraph embeddings): per-class precision/recall/F1 — class 0 (data): precision 0.85, recall 0.79, F1 0.82 (support 121); class 1 (sensing): precision 0.84, recall 0.89, F1 0.87 (support 110); class 2 (model): precision 0.88, recall 0.90, F1 0.89 (support 103); class 3 (system): precision 0.86, recall 0.90, F1 0.88 (support 49). Micro avg precision/recall/F1 = 0.86; Macro avg F1 = 0.863; Samples avg = 0.833 (from Table 3). No numeric retrieval-ranking deltas reported, only qualitative improvement and example similarity-rank improvements noted.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Default (initial) retrievals were compared qualitatively to customized retrievals created by AM experts; authors report improved ranking of relevant paragraphs (Table 2) but do not provide explicit numeric baseline-vs-method metrics. Classifier was a simple Random Forest trained on generated embeddings; no stronger baseline models (e.g., fine-tuned transformer classifiers) were compared.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Parsing errors from GROBID (missing/duplicate paragraphs, illegible equations); LLM hallucination risk due to domain specificity, limited context, ambiguity, outdated knowledge; lack of a defined similarity threshold to decide 'good enough' retrieved paragraphs; small labeled dataset for high-capacity models; current outputs are textual/distilled not formalized mathematical laws; computational/maintainability constraints led to use of hosted OpenAI APIs rather than local models.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4299.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4299.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq LLM IE (Dunn et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach that formulates scientific information extraction as a sequence-to-sequence task solved by fine-tuned large language models to output structured information from complex scientific text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Structured information extraction from complex scientific text with fine-tuned large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Sequence-to-sequence LLM-based Structured IE (as referenced)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced method frames IE as a seq2seq problem where an LLM is fine-tuned to map input passages to structured outputs (e.g., JSON or tabular fields). The present paper notes similarity to Dunn et al.'s seq2seq formulation but elects to use prompting to synthesize retrieved paragraphs instead of enforcing rigid structured outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General scientific literature / materials science examples in referenced work (as per citation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Structured outputs (as per referenced method), e.g., structured information fields — exact formats not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>The authors of this paper note that rigidly structured LLM outputs (as in fine-tuned seq2seq models) did not suit their needs across heterogeneous categories, motivating a more flexible prompting/synthesis approach. No further limitations from Dunn et al. are described here.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4299.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4299.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive NLP tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A general approach that augments generative LLMs with an external retrieval step, providing relevant passages as context to improve factual grounding of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Referenced as conceptual background: first retrieve relevant documents/passages (via embeddings + similarity search) then condition the LLM generation on these retrieved contexts to answer knowledge-intensive queries; the paper implements a practical variant by retrieving paragraphs and feeding them to an OpenAI chat LLM with an instruction prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General knowledge-intensive NLP tasks; applied in this paper to AM+ML literature</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Context-conditioned natural-language answers or structured fields depending on implementation; in this work, textual answers and paragraph labels.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>RAG reduces but does not eliminate hallucinations; effectiveness depends on retrieval quality and the ability of human reviewers to verify outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4299.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4299.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or approaches that use LLMs (or other AI models) to extract, distill, or discover quantitative laws, patterns, relationships, or principles from scientific papers or scholarly literature.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zero-shot ChatGPT IE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zero-shot information extraction via chatting with ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach describing zero-shot information extraction using a conversational LLM (ChatGPT) prompted directly without task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zero-shot information extraction via chatting with chatgpt</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Zero-shot ChatGPT-based IE</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Mentioned in references as an example of using large conversational LLMs in a zero-shot manner to extract information from text by crafting prompts; the present work notes conversational/chat APIs are used but relies on augmented retrieval and human-in-the-loop verification.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (as per referenced work) — not used directly in experiments of this paper</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>General NLP / information extraction tasks</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>type_of_quantitative_law</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_output_format</strong></td>
                            <td>Textual answers produced by chat model prompted for IE</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Zero-shot chat-based IE can be convenient but is susceptible to hallucination and domain gaps; the paper uses human review and retrieval to mitigate such issues.</td>
                        </tr>
                        <tr>
                            <td><strong>requires_human_in_loop</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fully_automated</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Structured information extraction from complex scientific text with fine-tuned large language models <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Zero-shot information extraction via chatting with chatgpt <em>(Rating: 2)</em></li>
                <li>S2ORC: The semantic scholar open research corpus <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4299",
    "paper_id": "paper-271516269",
    "extraction_schema_id": "extraction-schema-99",
    "extracted_data": [
        {
            "name_short": "Human-AI Teaming IE Framework",
            "name_full": "Human-Artificial Intelligence Teaming for Scientific Information Extraction Framework (prototype tool)",
            "brief_description": "A human-centered information extraction pipeline that combines PDF parsing (GROBID), paragraph-level embeddings, customizable semantic retrieval, LLM-based question answering, and paragraph classification to extract and distill paper-level information in additive manufacturing + ML literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Human-AI Teaming IE Framework (retrieval + LLM + classifier)",
            "method_description": "Pipeline: (1) Parse PDFs into paragraphs/equations using GROBID. (2) Compute paragraph embeddings with OpenAI text-embedding-ada-002 and use cosine similarity to retrieve top candidate paragraphs. (3) Allow human users to create/customize retrievals via ensembles of positive/negative queries and paragraphs; compute a retrieval embedding R as a weighted sum of averaged embeddings (Equation 1) to refine paragraph ranking. (4) Feed retrieved paragraphs plus a user query to an OpenAI chat LLM in a sequence-to-sequence formulation (prompt structure: Q + SEP + Passage1 + SEP + Passage2 ...), instructing the model to extract/distill the requested information or reply 'I cannot answer that' when absent. (5) Accumulate labeled paragraphs and train an offline multi-label paragraph classifier (they used OpenAI embeddings as features and a RandomForest MultiOutputClassifier) to accelerate future retrieval. Human-in-the-loop review is integrated at parsing, retrieval customization, and verification of LLM outputs.",
            "llm_model_used": "OpenAI chat API (unspecified chat model); OpenAI text-embedding-ada-002 for embeddings",
            "scientific_domain": "Additive manufacturing (3D printing) research intersecting with machine learning / data-driven AM",
            "number_of_papers": "100 papers",
            "type_of_quantitative_law": "None explicitly extracted; system extracts experimental metadata, data characteristics, modeling details, sensing and system descriptors rather than explicit quantitative laws (scaling laws/equations).",
            "extraction_output_format": "Textual answers to queries (natural-language distilled information), labeled paragraphs (multi-label categories), paragraph-level embeddings, and classifier labels; not mathematical equations or symbolic laws.",
            "validation_method": "Human expert iterative labeling and cross-referencing of retrieved paragraphs; classifier validated on held-out test split of the labeled paragraph dataset.",
            "performance_metrics": "Paragraph-classifier test results (Random Forest MultiOutputClassifier on paragraph embeddings): per-class precision/recall/F1 — class 0 (data): precision 0.85, recall 0.79, F1 0.82 (support 121); class 1 (sensing): precision 0.84, recall 0.89, F1 0.87 (support 110); class 2 (model): precision 0.88, recall 0.90, F1 0.89 (support 103); class 3 (system): precision 0.86, recall 0.90, F1 0.88 (support 49). Micro avg precision/recall/F1 = 0.86; Macro avg F1 = 0.863; Samples avg = 0.833 (from Table 3). No numeric retrieval-ranking deltas reported, only qualitative improvement and example similarity-rank improvements noted.",
            "baseline_comparison": "Default (initial) retrievals were compared qualitatively to customized retrievals created by AM experts; authors report improved ranking of relevant paragraphs (Table 2) but do not provide explicit numeric baseline-vs-method metrics. Classifier was a simple Random Forest trained on generated embeddings; no stronger baseline models (e.g., fine-tuned transformer classifiers) were compared.",
            "challenges_limitations": "Parsing errors from GROBID (missing/duplicate paragraphs, illegible equations); LLM hallucination risk due to domain specificity, limited context, ambiguity, outdated knowledge; lack of a defined similarity threshold to decide 'good enough' retrieved paragraphs; small labeled dataset for high-capacity models; current outputs are textual/distilled not formalized mathematical laws; computational/maintainability constraints led to use of hosted OpenAI APIs rather than local models.",
            "requires_human_in_loop": true,
            "fully_automated": false,
            "uuid": "e4299.0",
            "source_info": {
                "paper_title": "Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Seq2Seq LLM IE (Dunn et al.)",
            "name_full": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "brief_description": "A referenced approach that formulates scientific information extraction as a sequence-to-sequence task solved by fine-tuned large language models to output structured information from complex scientific text.",
            "citation_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "mention_or_use": "mention",
            "method_name": "Sequence-to-sequence LLM-based Structured IE (as referenced)",
            "method_description": "Referenced method frames IE as a seq2seq problem where an LLM is fine-tuned to map input passages to structured outputs (e.g., JSON or tabular fields). The present paper notes similarity to Dunn et al.'s seq2seq formulation but elects to use prompting to synthesize retrieved paragraphs instead of enforcing rigid structured outputs.",
            "llm_model_used": null,
            "scientific_domain": "General scientific literature / materials science examples in referenced work (as per citation)",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": "Structured outputs (as per referenced method), e.g., structured information fields — exact formats not detailed in this paper.",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "The authors of this paper note that rigidly structured LLM outputs (as in fine-tuned seq2seq models) did not suit their needs across heterogeneous categories, motivating a more flexible prompting/synthesis approach. No further limitations from Dunn et al. are described here.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4299.1",
            "source_info": {
                "paper_title": "Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Retrieval-Augmented Generation (RAG)",
            "name_full": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "brief_description": "A general approach that augments generative LLMs with an external retrieval step, providing relevant passages as context to improve factual grounding of model outputs.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "mention",
            "method_name": "Retrieval-Augmented Generation (RAG)",
            "method_description": "Referenced as conceptual background: first retrieve relevant documents/passages (via embeddings + similarity search) then condition the LLM generation on these retrieved contexts to answer knowledge-intensive queries; the paper implements a practical variant by retrieving paragraphs and feeding them to an OpenAI chat LLM with an instruction prompt.",
            "llm_model_used": null,
            "scientific_domain": "General knowledge-intensive NLP tasks; applied in this paper to AM+ML literature",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": "Context-conditioned natural-language answers or structured fields depending on implementation; in this work, textual answers and paragraph labels.",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "RAG reduces but does not eliminate hallucinations; effectiveness depends on retrieval quality and the ability of human reviewers to verify outputs.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4299.2",
            "source_info": {
                "paper_title": "Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Zero-shot ChatGPT IE",
            "name_full": "Zero-shot information extraction via chatting with ChatGPT",
            "brief_description": "A referenced approach describing zero-shot information extraction using a conversational LLM (ChatGPT) prompted directly without task-specific fine-tuning.",
            "citation_title": "Zero-shot information extraction via chatting with chatgpt",
            "mention_or_use": "mention",
            "method_name": "Zero-shot ChatGPT-based IE",
            "method_description": "Mentioned in references as an example of using large conversational LLMs in a zero-shot manner to extract information from text by crafting prompts; the present work notes conversational/chat APIs are used but relies on augmented retrieval and human-in-the-loop verification.",
            "llm_model_used": "ChatGPT (as per referenced work) — not used directly in experiments of this paper",
            "scientific_domain": "General NLP / information extraction tasks",
            "number_of_papers": null,
            "type_of_quantitative_law": null,
            "extraction_output_format": "Textual answers produced by chat model prompted for IE",
            "validation_method": null,
            "performance_metrics": null,
            "baseline_comparison": null,
            "challenges_limitations": "Zero-shot chat-based IE can be convenient but is susceptible to hallucination and domain gaps; the paper uses human review and retrieval to mitigate such issues.",
            "requires_human_in_loop": null,
            "fully_automated": null,
            "uuid": "e4299.3",
            "source_info": {
                "paper_title": "Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Structured information extraction from complex scientific text with fine-tuned large language models",
            "rating": 2,
            "sanitized_title": "structured_information_extraction_from_complex_scientific_text_with_finetuned_large_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Zero-shot information extraction via chatting with chatgpt",
            "rating": 2,
            "sanitized_title": "zeroshot_information_extraction_via_chatting_with_chatgpt"
        },
        {
            "paper_title": "S2ORC: The semantic scholar open research corpus",
            "rating": 1,
            "sanitized_title": "s2orc_the_semantic_scholar_open_research_corpus"
        }
    ],
    "cost": 0.009823249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HUMAN-ARTIFICIAL INTELLIGENCE TEAMING FOR SCIENTIFIC INFORMATION EXTRACTION FROM DATA-DRIVEN ADDITIVE MANUFACTURING RESEARCH USING LARGE LANGUAGE MODELS</p>
<p>Mutahar Safdar 
Jiarui Xie 
Andrei Mircea 
Yaoyao Fiona Zhao </p>
<p>McGill University Montreal
QCCanada</p>
<p>McGill University Montreal
QCCanada</p>
<p>University of Montreal &amp; Mila Montreal
QCCanada</p>
<p>McGill University Montreal
QCCanada</p>
<p>HUMAN-ARTIFICIAL INTELLIGENCE TEAMING FOR SCIENTIFIC INFORMATION EXTRACTION FROM DATA-DRIVEN ADDITIVE MANUFACTURING RESEARCH USING LARGE LANGUAGE MODELS
6747D454AA41E3E7C4D8539B7F9FB880Scientific Information ExtractionDesignManufacturingLarge Language ModelsHuman-AI Teaming
Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years.This has led to a plethora of scientific literature to emerge.The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that haven't been mined and formalized in an integrated way.It requires substantial effort and time to extract scientific information from these works.AM domain experts have contributed over two dozen review papers to summarize these works.However, information specific to AM and AI contexts still requires manual effort to extract.The recent success of foundation models such as BERT (Bidirectional Encoder Representations for Transformers) or GPT (Generative Pretrained Transformers) on textual data has opened the possibility of expediting scientific information extraction.We propose a framework that enables collaboration between AM and AI experts to continuously extract scientific information from datadriven AM literature.A demonstration tool is implemented based on the proposed framework and a case study is conducted to extract information relevant to the datasets, modeling, sensing, and AM system categories.We show the ability of LLMs (Large Language Models) to expedite the extraction of relevant information from data-driven AM literature.In the future, the framework can be used to extract information from the broader design and manufacturing literature in the engineering discipline.</p>
<p>INTRODUCTION</p>
<p>Additive manufacturing (AM), commonly known as 3D printing, fabricates parts layer-by-layer [1].Offering unique benefits, the process can rival conventional manufacturing techniques.This has inspired significant research efforts into the technology aimed at enhancing its maturity for industrial adoption.A major portion of the research from recent years has relied on machine learning (ML) or deep learning (DL)-based approaches following the success of advanced data analytics techniques [2,3].The scientific works at the intersection of two growing disciplines are extremely information-rich.It is critical to extract relevant information from the incoming literature flux in order to reproduce and adapt these solutions for real-world applications.</p>
<p>The plethora of emerging literature has led to several stateof-the-art reviews to summarize the development and highlight the future of technology [1][2][3].These reviews are divided across process technologies, applications, and solution types with varying scopes.This leads to subjective and time-irrelevant information being captured.The efforts to summarize datadriven AM research are limited in scope due to several reasons and fail to provide an all-encompassing reusable approach to information retrieval.Solutions capable of extracting the most relevant information from data-driven AM research are needed that can be re-used across a range of topics (e.g., technologies, applications, and data analytics solutions) in the field.</p>
<p>The challenge of extracting relevant information from science and engineering publications is not new and dates back to the 1960s [4].Many scientific disciplines are faced with the high flux of newly published literature limiting access to relevant information [5].As a result, Scientific Information Extraction, or SciIE is an established field though its maturity varies across disciplines.In general, Information Extraction or IE refers to a set of techniques in Natural Language Processing (NLP) that enable automated retrieval of structured information from text [6].The solution to extract information can take many forms once the raw data is processed and cleaned.In their review on SciIE, Hong et al. identified vocabulary generation, text classification, named entity recognition, and relationship extraction as some of the steps in the information retrieval pipeline [7].In the past, structured information extraction in SciIE has relied on manual curation and data mining methods.This required domain experts to manually annotate and index research papers, which took a lot of time and resources [7].However, recent advancements in NLP and ML have revolutionized the field.Modern approaches use DL models like transformers to automatically extract useful information from unstructured text [8].These models can identify key entities, relationships, and contextual information from scientific documents, enabling rapid and scalable IE.Additionally, modern methods take advantage of domain expertise to improve the precision and relevance of information retrieved.This combination of traditional curation and cutting-edge technology has the potential to significantly accelerate scientific discovery and information retrieval in research domains such as data-driven AM.</p>
<p>Large language models or LLMs are based on transformer architectures (models designed for sequence-to-sequence tasks) and employ self-attention which enables the model to weigh the importance of different segments within the text [9].Their introduction has been a breakthrough in AI and their capabilities make them well-suited for key NLP tasks including IE.While LLMs can support IE in various ways, their direct application in scientific disciplines may lead to lower performance.There are several reasons that can result in model hallucination such as domain specificity, limited context, ambiguity, outdated knowledge, low signal-to-noise ratio, and data privacy.There is a need to overcome this gap when leveraging powerful LLMs for SciIE.</p>
<p>To utilize LLMs for effective IE in data-driven AM, this work proposes a human (domain expert) centered approach integrating text retrieval, classification, and generation models.We particularly focus on making the extraction process transparent by having human review and feedback incorporated at each step.The remainder of the paper is as follows.Section 2 introduces the framework.Section 3 demonstrates the humancentered features.Section 4 details the case study and its steps.Section 5 presents the results and discussions.We conclude this article with concluding remarks and future works in Section 6.</p>
<p>INFORMATION EXTRACTION FRAMEWORK</p>
<p>The proposed IE framework is divided into three major components namely the base IE system, paragraph classification tier, and the query tier.The base IE system acts as the engine of the IE framework and allows AM and AI researchers to interact through a GUI to iteratively improve the answers coming from the query tier.Figure 1 presents an overview of the framework and outlines the key steps of each component.These components are explained in detail below.</p>
<p>Base IE System</p>
<p>The base IE system acts as the engine of our IE framework allowing users to upload scientific articles as PDF files, parsing the PDF files into paragraphs and equations for downstream retrieval, allowing AM experts to search relevant paragraphs through regular or semantic search, and customizing retrievals to effectively shortlist relevant paragraphs.</p>
<p>Based on the methodology of Lo et al., the base IE system processes user-provided PDFs of scientific AM articles with GROBID [10].Metadata such as publication title, abstract, authors, date and DOI are first extracted.Then GROBID parses a paper's paragraphs and equations organized by section; as well as figures, tables and references (with corresponding links in the text).Lastly, the different parsing outputs are saved to the user's library for use in the IE system, along with the original PDFs for user reference.Despite its widespread use, GROBID still struggles with certain PDFs, displaying parsing errors like missing or duplicate paragraphs, invalid organization of sections, incorrect or missing metadata, and illegible equations.In alignment with our human-centered considerations, we ensure transparency and iterative improvement by letting users view and correct parsed results with the original PDFs for reference.</p>
<p>IE systems typically involve an information retrieval (IR) step, where relevant documents are first retrieved [11].Because our use case involves extracting information from a given paper, we retrieve relevant paragraphs instead.Specifically, we generate embeddings for paragraphs and for queries associated with four categories of interest defined by the AM experts (e.g., data, modeling, sensing, and AM systems) We use cosine similarity to retrieve the paragraphs that are most similar to a given query.Because of computational constraints and maintainability concerns associated with hosting the IE system, we decided to use the OpenAI embeddings API instead of our own models.</p>
<p>While we could perform IE on the full text of a paper, this has significant drawbacks that motivate paragraph retrieval.First, for a given information item, typically only a small portion of the full text contains the desired information.Conversely, feeding full-texts into our LLM-based IE model is infeasible or too costly to be viable.Second, limiting our LLM-based IE model to retrieved paragraphs -which are typically quickly readable and self-coherent-enables rapid and easy userverification of our IE system's output by cross-referencing with retrieved passages.As errors are inevitable in any IE system, and pernicious with LLMs, this human-centered consideration helps build trust through transparency (Explained in Section 3.2).</p>
<p>Similar to Dunn et al., we use LLMs to perform IE on scientific texts with a sequence-to-sequence formulation [12].However, in the process of participatory design (Explained in Section 3.4), we found that rigidly structured outputs did not lend themselves well to extracting specific information items across multiple categories.Instead, we prompt the LLM to synthesize the retrieved passages, extracting the information that is relevant to the query of a given information item, or clearly indicating cases where no such information is present.As mentioned earlier, we use the OpenAI chat API instead of our own models due to constraints on computational resources and maintainability.</p>
<p>As seen in Figure 2, the query used to prompt the LLMbased IE model may or may not be appropriate for retrieving relevant paragraphs.Throughout development, this became rapidly apparent, and users began experimenting with different retrieval queries and building intuitions for common failure mechanisms and working mitigations.However, sharing queries between the IR and IE systems limits the flexibility afforded to users in designing a query, as it is constrained to being a valid instruction interpretable by the LLM-based IE system.</p>
<p>To address this limitation and enable iterative improvement (Explained in Section 3.2), we create an interface that allows users to create and update custom retrievals with ensembles of positive and negative queries Q+ and Q−, as well as positive and negative paragraphs P+ and P−.Positive queries are meant to retrieve similar paragraphs, while negative queries are meant to prevent retrieving similar paragraphs.Conversely, when retrieving paragraphs, users can annotate these as positive or negative to obtain a similar effect.The interfaces for these functionalities are shown in Figure 2 (B-E).Concretely, we compute a retrieval embedding R as a sum of the averages of these embeddings, weighted by a, b, c, d:
𝑅 = ∑ | | + ∑ | | − ∑ | | − ∑ | |(1)
While simple, this human-in-the-loop approach enables users to improve retrieval in a way that is intuitive and enables fine-grained control or experimentation; all the while not interfering with or being constrained by the IE system query.</p>
<p>Paragraph Classification Tier</p>
<p>We expect to find the relevant information in specific paragraphs and hence there is value in training paragraph classifiers to further expedite the IE process.In the long term, classifiers specific to a certain domain can quickly filter the relevant paragraph from the whole article.These relevant paragraphs can then be used in the query tier to provide specific answers to the readers.It is also important to mention that currently the paragraph classification is done at an abstract level (e.g., data as compared to specific data characteristics or modeling as compared to specific modeling details) to use shallow and light-weight classifiers that simplify the training process.</p>
<p>One of the key goals at this stage is to prepare global classifier(s) for each field to quickly filter the relevant paragraph from irrelevant.The accuracy of the classifiers is expected to grow gradually as the domain experts read through the papers and label the paragraphs in the base IE system.As a result, this will incrementally decrease the effort of manually going through each article in the base IE system.Nonetheless, the long-term effectiveness of these classification models will depend on regular fine-tuning with newly labeled paragraphs.This could be inspired by the poor performance of the trained classifiers on certain components of scientific information.</p>
<p>𝑙 = 𝜎(𝑤 𝑝 + 𝑏 )</p>
<p>(2)</p>
<p>In the proposed framework, multilabel l(i,j) paragraph (Pi) classification models (e.g., Equation 2) are being built offline on top of the data labeled in the base IE system.This is to provide flexibility in the choice of a model (shallow vs deep, binary, multi-class or multi-label) that works best for each scientific domain.The functionality to deploy trained models online by integrating them into the base IE system can be implemented in the future.</p>
<p>Query Tier</p>
<p>The question or query tier represents the last component of the proposed IE framework.It simply allows the users to ask a specific question and expect a well-formulated answer.For this purpose, the filtered paragraphs are fed into a GPT model along with the user query to output the answer.Currently, a query function is available in the base IE system to directly filter the information.In the future, we expect query functions to also interact with the outputs of classification models once these are integrated into the base IE system.Equation 3 represents the structure of the prompt where both query (Q) and paragraphs (Pi) are fed to the model while being kept apart through a separator (SEP).</p>
<p>𝐺𝑃𝑇 = 𝑄 + |𝑆𝐸𝑃| + 𝑃 … + |𝑆𝐸𝑃| + 𝑃</p>
<p>(3)</p>
<p>HUMAN-AI TEAMING FEATURES</p>
<p>The Human-AI teaming is central to the proposed IE framework to keep both AM and AI researchers in the loop as the information retrieval pipelines specific to a certain domain are optimized.These are oftentimes referred to as humancentered considerations in the broader NLP literature.In the context of data discovery, a problem similar to IE, Gregory and Koesten refine the notion of "human-centered" as thinking from the perspective of the person(s) engaging in the activity; with a focus on the interaction process and the "user" experience, taking into account different contexts and needs [13].In a different vein, Egan et al. present "user-centered" NLP systems as a humancomputer collaboration where computers do what they do well (process large amounts of information, filter, sort and prioritize) and humans do what they do well (assess, select, and refine with domain expertise) [14].And lastly, on a more abstract level, Kotnis et al. define "human-centric" NLP research as a process where human stakeholders actively participate in the research [15].In this section, we discuss several human-centered considerations which relate to these formulations and have influenced the development of our IE system.</p>
<p>User Interface</p>
<p>A prototype tool is implemented that reflects the base IE system explained in the previous section.Figure 2 shows an overview of the tool.The tool provides several functionalities to the users including the option to create libraries to group PDF files from similar domains.Each library provides a list of papers and highlights the author and publication data.The uploaded PDF files can be viewed as-is.In addition, a simple text search or semantic search can be performed on the parsed PDF files.</p>
<p>The Query tab represents the option to write specific queries.The Retrieval tab highlights the functionality to create retrievals and iteratively update them by labeling the paragraphs as positive or negative.The user interface enables several human-centric features that are explained in the following subsections.</p>
<p>Transparency and Trust</p>
<p>IE systems ideally improve the efficiency of their users, however, Schleith et al. suggest that a lack of transparency can lead to a lack of trust [16].This lack of trust can in turn undo efficiency gains as users spend more time carefully reviewing system outputs they do not trust.Similarly, but in the context of LLM-generated summaries, Cheng et al. present appropriate trust as enabling users to decide whether or not to rely on a given system output; which in turn requires transparency [17].These challenges are all-the-more important for LLMs, which are known to generate convincing but erroneous confabulations.</p>
<p>We build our IE system around this human-centered consideration of trust by transparency in a variety of ways.First, as noted in the base IE system, errors are introduced as early as the data parsing stage.By overlaying interactions with the IE system on top of this raw data, we enable users to more easily catch and correct errors related to parsing, building appropriate trust.Additionally, transparency on the data level is essential for mitigating potential issues of data cascades [18].</p>
<p>As mentioned in Section 2.1, we also introduce an intermediate retrieval step before performing LLM-based IE.While this can improve factual consistency, it does not completely prevent confabulation [19].However, adapting the interface to enable streamlined cross-referencing of system outputs with retrieved passages (specifically paragraphs, to facilitate quick verification) builds appropriate trust where eliminating errors is otherwise infeasible.</p>
<p>Iterative Improvement</p>
<p>While transparency builds appropriate trust by supporting users in deciding whether to rely on system outputs or not, iterative improvements in IE systems can minimize the rate at which users should decide not to rely on a given output, improving their experience.More specifically, human feedback with human-in-the-loop approaches can be leveraged to improve the reliability of IE system outputs [20].We adapt this humancentered consideration by enabling users to create custom retrievals where they can provide feedback on retrieved passages and iteratively improve the reliability of the custom retrieval.More generally, Rahman and Kandogan find that human-in-theloop IE workflows are typically iterative in nature, characterized by information foraging and sensemaking loops as users iteratively improve their understanding of the task and the data [21].We try to support this consideration and give users the flexibility required for this kind of iteration.Notably, we give users fine-grained control over the underlying retrieval and extraction systems so they can experiment with different approaches.</p>
<p>Participatory Design</p>
<p>An important consideration throughout this work has been participatory design: ensuring a relationship of meaningful cocreation and mutual learning between users and researchers [22,23].This collaborative approach between developers (machine learning researchers) and users (mechanical engineering researchers) enabled iterative refinements throughout the development of the IE system prototype; from initial brainstorming and problem formulation to interface adaptations that address user-identified limitations of the underlying machine learning models.Crucially, our approach is fundamentally human-in-the-loop rather than human-on-theloop.In other words, users and researchers actively participate in a task rather than passively supervising or validating its automated completion.We found this dynamic played a significant role in fostering participatory design throughout development.</p>
<p>CASE STUDY</p>
<p>Inspired by the increasing scientific publications as shown in Figure 3, a case study was conducted using literature at the intersection of AM and ML.It is particularly challenging to find key components of information quickly and effectively from literature at the intersection of two growing fields.The tool enables an interactive way to query the information required and hence provides an opportunity to go through the literature quickly as compared to relying on existing reviews.The reviews become outdated with time and are limited in the way information can be represented.In addition to providing a faster and effective way to retrieve key information components, the tool can be used for other domains and applications so as to provide a reproducible pipeline for SciIE.</p>
<p>Defining Relevant AM+AI Information</p>
<p>We categorized the information contained within the datadriven AM literature into four categories which jointly represent most of the key information required to understand and evaluate the presented research.These categories are listed below:</p>
<p>-Data Relevant: Information representing data for ML applications such as data characteristics, experimental settings, data preparation, data processing, and data availability [24,25].</p>
<p>-Model Relevant: Information related to ML-based modeling such as the algorithm, training process, the compute hardware &amp; software, and model availability.</p>
<p>-Sensing Relevant: Information relevant to sensing technique and equipment such as the physical phenomenon, sensor type, sensor specifications, sensor settings, and sensor deployment.</p>
<p>-System Relevant: Information representing manufacturing technology, hardware, and materials used</p>
<p>Collecting Research Articles</p>
<p>In order to conduct the case study, we retrieved 100 research articles representing ML-based research on in-process monitoring and quality prediction challenges in AM.The latest publication year among the articles is 2023 whereas no limit was set on the starting year.These papers represent a diverse and comprehensive body of research in ML-driven AM research.All articles were collected from Scopus.The decision to use 100 research articles to validate the IE pipeline was made to get a representative dataset spanning various subdomains in AM and ML.The PDF files of all articles were downloaded and grouped into an "IE Validation" library inside the prototype tool.As soon as the PDF of an article is uploaded, it is parsed at the backend to support subsequent search, labeling, and retrieval.</p>
<p>Searching, Labeling and Retrieval Customization</p>
<p>Once the PDF files were added in the prototype tool, these were parsed to act as the input of the embedding model.The current version of the prototype tool used the OpenAI textembedding-ada-002 model to provide a paragraph-level vector representation of the parsed PDF files.This set the stage to find relevant information through semantic search where input query was also featurized using the same model and the two were compared using cosine similarity.However, in the case study, we relied primarily on the retrieval functionality (Figure 4) and iteratively updated it by selecting positive and negative paragraphs to compute the retrieval embedding of Equation 1.As will be shown in the results, the ranking results and score gradually improved as we went through the papers labeling the paragraphs.This reflected the effectiveness of customizing the retrievals specific to each relevant information category.</p>
<p>Where specific information was not found in the top highlighted paragraphs from customized retrievals, we used search functionality to find it.If the relevant paragraphs were found through the search functionality, they were labeled as positive to include them in the retrieval embedding.Similarly, where irrelevant paragraphs were found in the top results of a specific retrieval, these were marked as negative to be excluded from future results.Figure 5 shows the relevant and irrelevant paragraphs highlighted as positive and negative to account their respective embeddings in the overall retrieval embedding.The labeling process led to a multi-label text dataset for ML-driven AM literature to be used in the next step.To the best of the author's knowledge, this is the first NLP dataset in AM [24,26].</p>
<p>Classifying Paragraphs</p>
<p>The resulting multi-label paragraph dataset was downloaded from the prototype tool and used to develop paragraph classifiers as global domain models to rapidly filter relevant paragraphs for downstream IE.The raw dataset represented 5039 paragraphs labeled into four relevant categories namely data, model, sensing, and system.We introduced a fifth category for paragraphs that didn't belong to any of the above-mentioned categories as "irrelevant."This was done to evaluate the effect of including these paragraphs in the learning process.However, their inclusion introduced data imbalance, and these were subsequently removed as a redundant category label.</p>
<p>The dataset was processed using the OpenAI embedding model to generate feature vectors for each paragraph.The augmented dataset was used to train a Random Forest classifier from the Scikit-learn library.Since the classifier doesn't natively support multi-target classification, we used the built-in MultiOutputClassifier strategy.We trained a simple multi-label model as a global classifier for ML-based AM literature.The classifier can categorize the paragraph into four categories of relevant information.However, irrelevant paragraphs will require to be filtered and out-of-balanced classes should be down-sampled for future training of the model.The results are presented in the next Section.</p>
<p>Query and Response</p>
<p>During the case study, we used a query function that prompts an LLM to extract the relevant information by providing both a user query and the relevant paragraph(s) to generate the answer.This functionality can be used both during the labeling process to find missing information as well as after the classifiers have been trained to filter the relevant paragraphs.Table 1 shows the function used to prompt.retrieved = [f"-Passage {i}: {x}" for i, x in enumerate(retrieved)] retrieved = "\n".join(retrieved)prompt = f""" You are an assistant for a researcher working at the intersection of additive manufacturing and machine learning.Your goal is to help the researcher find and distill significant information in a scientific paper.To this end, answer the following triple-backtick delimited query from 404 the researcher: <code>{query}</code> To answer the question, use the following passages from the paper.If there is no information in the passages that answers the question, write "I cannot answer that."{retrieved} """ return prompt</p>
<p>RESULTS AND DISCUSSIONS</p>
<p>The results from the case study are divided into two categories.Table 2 shows the improvement in the ranking of System Relevant similarity across the four information categories.The passages are selected from one random paper out of the 100 and its most relevant paragraphs for four categories are shown against starting/default retrievals as well as the customized retrievals after the AM researchers went through the 100 selected papers.The improvement in the similarity ensures that top matches to a selected retrieval contain relevant paragraphs with the required information.Appendix A shows the default retrievals associated with four information categories representing AM+ML literature that were used.</p>
<p>The second set of results represents the performance of the downstream global classifier on the multi-label paragraph dataset.The classifier was trained on a diverse set of ML-driven AM literature and its ability to classify represents initial success in building the global classifier.The similarity percentage fluctuates across paragraphs and is sensitive to their length.Moreover, the current approach lacks a threshold to define "good enough" similarity for relevant paragraphs.Similarly, the developed classifier is relatively simple since the post-process dataset is small as compared to those used in deep learning models of textual data.A highercapacity global classifier requires more labeled data.For the next 100 papers, authors expect the effort to be significantly lower due to already customized retrievals.The options to generate synthetic data can be considered as well.In addition to increasing the data quality and quantity, irrelevant paragraphs need to be considered to further refine classifier boundaries.</p>
<p>CONCLUSIONS AND FUTURE WORKS</p>
<p>Inspired by the increasing frequency of research into datadriven solutions of AM challenges, we propose an information extraction framework powered by LLMs and built around human-centered considerations.The framework is divided into three components namely base IE system, classification tier, and the query tier whereas the query functionality is also integrated into the base system.The tool enables continuous update of the database representing a specific scientific domain while allowing domain experts to iteratively customize retrieval for LLM-based IE.The tool is deployed on the web and has restricted development access at the moment.We carried out a case study by building a library of 100 ML-based AM research articles and going through them to manually label and validate paragraph containing key information belonging to four categories namely data, model, sensing and system.We confirm the gradual effectiveness of customizing retrievals as we progress through the database.Moreover, the relevant paragraphs labeled as a result of retrieval customization were downloaded and used to train a shallow multi-label classifier.The results of classification on the test set indicate that it is possible to develop a global classifier for a given domain thereby significantly expediting the information filtering step.</p>
<p>The future works include: -Validating the prototype tool and proposed framework in another design and manufacturing subdomain.-Defining methods and metrics to benchmark IE efficiency and effectiveness as compared to existing tools and approaches.-Introducing a notion of similarity threshold for relevant paragraphs in design and manufacturing scientific literature.-Opening the tool to the broader design and manufacturing community to gather feedback.</p>
<p>FIGURE 1 :
1
FIGURE 1: INFORMATION EXTRACTION FRAMEWORK WITH BASE IE SYSTEM (Left), PARAGRAPH CLASSIFICATION TIER (Middle), AND QUERY TIER (Right).THE SUBCOMPONENTS ARE EXPLAINED IN THE FRAMEWORK AND CASE STUDY SECTIONS SciIE techniques can be broadly classified into traditional methods or more recent approaches based on ML or DL.In the past, structured information extraction in SciIE has relied on manual curation and data mining methods.This required domain experts to manually annotate and index research papers, which took a lot of time and resources[7].However, recent advancements in NLP and ML have revolutionized the field.Modern approaches use DL models like transformers to automatically extract useful information from unstructured text[8].These models can identify key entities, relationships, and contextual information from scientific documents, enabling rapid and scalable IE.Additionally, modern methods take advantage of domain expertise to improve the precision and relevance of information retrieved.This combination of traditional curation and cutting-edge technology has the potential to significantly accelerate scientific discovery and information retrieval in research domains such as data-driven AM.Large language models or LLMs are based on transformer architectures (models designed for sequence-to-sequence tasks) and employ self-attention which enables the model to weigh the importance of different segments within the text[9].Their introduction has been a breakthrough in AI and their capabilities make them well-suited for key NLP tasks including IE.While LLMs can support IE in various ways, their direct application in scientific disciplines may lead to lower performance.There are several reasons that can result in model hallucination such as domain specificity, limited context, ambiguity, outdated</p>
<p>FIGURE 2 :
2
FIGURE 2: OVERVIEW OF THE PROTOTYPE TOOL IMPLEMENTED.(A) REPRESENTS THE MAIN VIEW OF THE TOOL IN A WEB BROWSER ONCE THE IE VALIDATION CHECKLIST OF 100 PAPERS IS SELECTED.THE PAPERS ARE LISTED ON THE LEFT SIDE WHEREAS THE PDF OF THE SELECTED PAPER IS SHOWN ON THE RIGHT.(B) REPRESENTS THE STRING-BASED TEXT SEARCH FUNCTION.(C) REPRESENTS THE SEMANTIC SEARCH WHICH VECTORIZES USER QUERY FOR COSINE SIMILARITY.THE RESULTING TOP FIVE PARAGRAPHS ARE HIGHLIGHTED.THE ARROWS NEXT TO THE SEARCH BUTTON ENABLE USER VERIFICATION OF THE OUTPUT WITH EASY NAVIGATION BETWEEN RETRIEVED PARAGRAPHS USED BY THE IE SYSTEM.THE HUE OF THE HIGHLIGHT INDICATES THE STRENGTH OF THE COSINE SIMILARITY BETWEEN THE QUERY AND THE PASSAGE.(D) REPRENTS THE QUERY FUNCTION AND THE RESULTING ANSWER GENERATED FROM TOP FIVE RELEVANT PARAGRAPHS.(E) REPRESENTS THE FUNCTIONALITY TO CREATE, SELECT AND UPDATE A RETRIEVAL</p>
<p>FIGURE 3 :
3
FIGURE 3: THE PLOT HIGHLIGHTS THE INCREASING LITERATURE IN ADDITIVE MANUFACTURING INSPIRING THE CREATION OF AN AI TOOL TO QUICKLY AND EFFECTIVELY FILTER KEY SCIENTIFIC INFORMATION.</p>
<p>FIGURE 4 :
4
FIGURE 4: RETRIEVAL CREATION WINDOW.A RETRIEVAL CAN BE NAMED AND POPULATED WITH POSITIVE (RELEVANT E.G., DATA, MODEL) AND NEGATIVE QUERIES (IRRELEVANT E.G., RELATED WORKS SECTION).WE CREATED FOUR RETRIEVALS AS SHOWN IN THE APPENDIX.</p>
<p>FIGURE 5 :
5
FIGURE 5: A POPULATED RETRIEVAL (DATA RELEVANT IN THIS CASE) CAN BE CUSTOMIZED BY SELECTING THE RELEVANT AND IRRELEVANT PARAGRAPHS.BY CLICKING ON A PLUS SIGN BUTTON, PARAGRAPHS CAN BE ADDED TO P+ TO INCREASE THE CHANCE OF RETRIEVING SIMILAR PARAGRAPHS FOR A GIVEN CUSTOMIZED RETRIEVAL.WE CAN ALSO ADD PARAGRAPHS TO P− TO REDUCE THE CHANCE OF RETRIEVING SIMILAR PARAGRAPHS FOR THE SAME CUSTOMIZED RETRIEVAL.</p>
<p>TABLE 1 :
1
TO PROMPT LLM AND SUPPORT
SPECIFIC IE RETRIEVALFunction: create_promptDescription: return the required information from therelevant passagesdef create_prompt(query: str, retrieved: list[str]):</p>
<p>TABLE 2 :
2
IMPROVEMENT IN SIMILARITY RANKING FROM FIRST TO LAST PAPER IN THE CASE STUDY.THE COMPARISON IS DONE BY SELECTING A RANDOM PAPER AND APPLYING DEFAULT RETRIEVALS.THE RESULTS FROM DEFAULT RETRIEVALS AND FINAL CUSTOMIZED RETRIEVALS ARE COMPARED.AS EVIDENT FROM THE RESULTS, THE INITIAL RANKING LEADS TO IRRELEVANT PARAGRAPHS WITH VARYING SCORES WHERAS THE CUSTOMIZED RETRIEVALS LEAD TO TOP RELEVANT PARAGRAPGH WITH REQUIRED INFORMATION
IEInitialCategoryRanking</p>
<p>Table 3
3
represents the test performance of the Random Forest classifier in terms of precision, recall, and F1 score.</p>
<p>TABLE 3 :
3
CLASSIFICATION REPORT OF THE RANDOM FOREST MODEL ON TEST SPLIT OF PROCESSED AND RELEVANT PARAGRAPH DATASET.0, 1,2,3 CORRESPOND TO DATA, SENSING, MODEL, AND SYSTEM CLASSES
ClassPrecisionRecallF1-Score Support00.850.790.8212110.840.890.8711020.880.900.8910130.860.900.8849Micro avg0.860.860.86381Macro avg0.860.870.86381Weighted avg0.860.860.86381Samples avg0.840.860.83381
ACKNOWLEDGEMENTSMcGill Engineering Doctoral Award (MEDA) fellowship for Mutahar Safdar is acknowledged with gratitude.Mutahar Safdar also received financial support from National Research Council of Canada (Grant# NRC INT-015-1).McGill Graduate Excellence Award (Grant# 00157), Mitacs Accelerate Program (Grant# IT13369), and MEDA fellowship for Jiarui Xie are acknowledged with gratitude.The authors are grateful to Digital Research Alliance of Canada (RRG# 4294) for providing computational resources to support this research.DEMO OF PROTOTYPE TOOL https://www.youtube.com/watch?v=gM7rFLmJEH0
A review on machine learning in 3D printing: applications, potential, and challenges. Guo Goh, Dong, Swee Sing, Yeong Leong, Wai Yee, Artificial Intelligence Review. 5412021</p>
<p>Invited review: Machine learning for materials developments in metals additive manufacturing. N S Johnson, Vulimiri, Ps, To, Ac, X Zhang, C A Brice, B B Kappes, A P Stebner, Additive Manufacturing. 361016412020</p>
<p>A systematic literature review on recent trends of machine learning applications in additive manufacturing. Md Xames, Doulotuzzaman, Fariha Torsha, Kabir, Ferdous Sarwar, Journal of Intelligent Manufacturing. 2022</p>
<p>The development of the medical literature analysis and retrieval system (MEDLARS). Cheryl Dee, Rae, Journal of the Medical Library Association: JMLA. </p>
<p>Scientific literature: Information overload. Esther Landhuis, Nature. 5352016</p>
<p>Structure, effectiveness and benefits of LEXtractor, an operational computer program for automatic extraction of case summaries and dispositions from court decisions. Casimir Borkowski, Sperling Martin, J , Journal of the American Society for Information Science. 2621975</p>
<p>Challenges and advances in information extraction from scientific literature: a review. Zhi Hong, Ward, Logan, Chard, Kyle, Ben Blaiszik, Ian Foster, JOM. 73112021</p>
<p>Zero-shot information extraction via chatting with chatgpt. Xiang Wei, Cui, Xingyu, Cheng, Ning, Wang, Xiaobin, Zhang, Xin, Huang, Shen, Xie, Pengjun, Xu, Chen Jinan, Yufeng Zhang, Meishan , arXiv:2302.102052023arXiv preprint</p>
<p>Attention is all you need. Ashish Vaswani, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 201730</p>
<p>S2ORC: The semantic scholar open research corpus. Kyle Lo, Wang , Lucy Lu, Neumann, Mark, Rodney Kinney, Dan S Weld, arXiv:1911.027822019arXiv preprint</p>
<p>Information extraction. Jim Cowie, Wendy Lehnert, Communications of the ACM. 3911996</p>
<p>Structured information extraction from complex scientific text with fine-tuned large language models. Alexander Dunn, Dagdelen, John, Walker, Nicholas, Lee, Sanghoon, Andrew S Rosen, Ceder, Gerbrand, Kristin Persson, Anubhav Jain, arXiv:2212.052382022arXiv preprint</p>
<p>Humancentered data discovery. Kathleen Gregory, Laura Koesten, 2023Springer Nature</p>
<p>User-centered MT Development and Implementation. Kathleen Egan, Francis Kubala, Allen Sears, Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT. the 8th Conference of the Association for Machine Translation in the Americas: Government and Commercial Uses of MT2008</p>
<p>Human-centric research for nlp: Towards a definition and guiding questions. Bhushan Kotnis, Gashteovski, Kiril, Gastinger, Julia, Serra, Giuseppe, Alesiani, Francesco, Sztyler, Timo, Shaker, Ammar, Gong, Na, Carolin Lawrence, Zhao Xu, arXiv:2207.044472022arXiv preprint</p>
<p>Human in the loop information extraction increases efficiency and trust. Johannes Schleith, Hoffmann, Hella, Milda Norkute, Brian Cechmanek, 2022</p>
<p>Mapping the design space of human-ai interaction in text summarization. Ruijia Cheng, Alison Smith-Renner, Zhang, Ke, Tetreault, R Joel, Alejandro Jaimes, arXiv:2206.148632022arXiv preprint</p>
<p>Everyone wants to do the model work, not the data work": Data Cascades in High-Stakes AI. Nithya Sambasivan, Kapania, Shivani, Highfill, Hannah, Akrong, Diana, Praveen Paritosh, Lora M Aroyo, proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. the 2021 CHI Conference on Human Factors in Computing Systems2021</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Piktus, Aleksandra, Petroni, Fabio, Karpukhin, Vladimir, Goyal, Naman, Küttler, Heinrich, Lewis, Mike, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Information Extraction with Humans in the Loop. Anna Gentile, Lisa, Companion Proceedings of The 2019 World Wide Web Conference. 2019</p>
<p>Characterizing practices, limitations, and opportunities related to text information extraction workflows: a human-in-the-loop perspective. Sajjadur Rahman, Eser Kandogan, Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. the 2022 CHI Conference on Human Factors in Computing Systems2022</p>
<p>Guiding principles for participatory design-inspired natural language processing. Tommaso Caselli, Cibin, Roberto, Conforti, Costanza, Enrique Encinas, Maurizio Teli, Proceedings of the 1st Workshop on NLP for Positive Impact. the 1st Workshop on NLP for Positive Impact2021</p>
<p>Power to the people? Opportunities and challenges for participatory AI. Abeba Birhane, Isaac, William, Prabhakaran, Vinodkumar, Diaz, Mark, Madeleine Elish, Clare, Iason Gabriel, Mohamed, Shakir, Proceedings of the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization. the 2nd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization2022</p>
<p>A systematic review on data of additive manufacturing for machine learning applications: the data quality, type, preprocessing, and management. Ying Zhang, Safdar, Mutahar, Xie, Jiarui, Li, Jinghao, Sage, Manuel, Yaoyao Zhao, Fiona, Journal of Intelligent Manufacturing. 2022</p>
<p>Engineering of Additive Manufacturing Features for Data-Driven Solutions: Sources, Techniques, Pipelines, and Applications. Mutahar Safdar, Lamouche, Guy, Padma Paul, Polash, Wood, Gentry, Yaoyao Zhao, Fiona, 2023Springer Nature</p>
<p>Engineering of Additive Manufacturing Features for Data-Driven Solutions: Sources, Techniques, Pipelines, and Applications. Mutahar Safdar, Lamouche, Guy, Padma Paul, Polash, Wood, Gentry, Yaoyao Zhao, Fiona, 2023SpringerFeature engineering in additive manufacturing</p>            </div>
        </div>

    </div>
</body>
</html>
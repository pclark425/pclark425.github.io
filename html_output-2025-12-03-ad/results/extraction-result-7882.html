<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7882 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7882</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7882</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-144.html">extraction-schema-144</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods that use large language models to synthesize, distill, or generate scientific theories, hypotheses, or structured knowledge from collections of scholarly papers.</div>
                <p><strong>Paper ID:</strong> paper-9e557a199972b963d8ac064ac6e625c115c03cde</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/9e557a199972b963d8ac064ac6e625c115c03cde" target="_blank">Causal Structure Learning Supervised by Large Language Model</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work innovatively integrates LLM-based causal inference with CSL in an iterative process, refining the causal DAG using feedback from LLMs, and generates more robust and high-quality structural constraints compared to previous methodologies.</p>
                <p><strong>Paper Abstract:</strong> Causal discovery from observational data is pivotal for deciphering complex relationships. Causal Structure Learning (CSL), which focuses on deriving causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast DAG spaces and data sparsity. The integration of Large Language Models (LLMs), recognized for their causal reasoning capabilities, offers a promising direction to enhance CSL by infusing it with knowledge-based causal inferences. However, existing approaches utilizing LLMs for CSL have encountered issues, including unreliable constraints from imperfect LLM inferences and the computational intensity of full pairwise variable analyses. In response, we introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL innovatively integrates LLM-based causal inference with CSL in an iterative process, refining the causal DAG using feedback from LLMs. This method not only utilizes LLM resources more efficiently but also generates more robust and high-quality structural constraints compared to previous methodologies. Our comprehensive evaluation across eight real-world datasets demonstrates ILS-CSL's superior performance, setting a new standard in CSL efficacy and showcasing its potential to significantly advance the field of causal discovery. The codes are available at \url{https://github.com/tyMadara/ILS-CSL}.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7882",
    "paper_id": "paper-9e557a199972b963d8ac064ac6e625c115c03cde",
    "extraction_schema_id": "extraction-schema-144",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0068544999999999995,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Causal Structure Learning Supervised by Large Language Model</h1>
<p>Taiyu Ban Lyuzhou Chen Derui Lyu Xiangyu Wang<em> Huanhuan Chen</em><br>School of Computer Science and Technology, University of Science and Technology of China<br>{banty, clz31415, drlv}@mail.ustc.edu.cn {sa312, hchen}@ustc.edu.cn</p>
<h4>Abstract</h4>
<p>Causal discovery from observational data is pivotal for deciphering complex relationships. Causal Structure Learning (CSL), which focuses on deriving causal Directed Acyclic Graphs (DAGs) from data, faces challenges due to vast DAG spaces and data sparsity. The integration of Large Language Models (LLMs), recognized for their causal reasoning capabilities, offers a promising direction to enhance CSL by infusing it with knowledgebased causal inferences. However, existing approaches utilizing LLMs for CSL have encountered issues, including unreliable constraints from imperfect LLM inferences and the computational intensity of full pairwise variable analyses. In response, we introduce the Iterative LLM Supervised CSL (ILS-CSL) framework. ILS-CSL innovatively integrates LLM-based causal inference with CSL in an iterative process, refining the causal DAG using feedback from LLMs. This method not only utilizes LLM resources more efficiently but also generates more robust and high-quality structural constraints compared to previous methodologies. Our comprehensive evaluation across eight realworld datasets demonstrates ILS-CSL's superior performance, setting a new standard in CSL efficacy and showcasing its potential to significantly advance the field of causal discovery. The codes are available at https://github.com/tyMadara/ILS-CSL.</p>
<h2>I. INTRODUCTION</h2>
<p>Causal discovery from the observed data is pivotal in understanding intricate relationships across various domains. Central to this endeavor is Causal Structure Learning (CSL), aiming to construct a causal Directed Acyclic Graph (DAG) ${ }^{1}$ from observed data [1]. We adopt causal Bayesian Networks (BNs) as the causal graphical model, renowned for effectively modeling intricate real-world variable relationships [2].</p>
<p>The recovery of high-quality causal BNs faces significant challenges. Firstly, there is the issue of the super-exponential increase in the DAG space as the number of variables grows [3], [4]. Additionally, real-world data is typically sparse and insufficient for accurately representing the true probability distributions [5]. Furthermore, the orientation of edges in a BN cannot be fully deduced from the observed data alone due to the presence of equivalent DAGs [6]. In summary, CSL, when reliant solely on observed data, encounters both practical and theoretical limitations.</p>
<p>Given these inherent limitations, the integration of prior knowledge to constrain specific structures becomes important</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>for reliable causal discovery [7], [8]. While promising, this approach has been limited by the high costs and time associated with expert input [9]. However, the advent of Large Language Models (LLMs) has ushered in a new frontier. Recent studies have underscored the capabilities of LLMs in causal reasoning, positioning them as a valuable and readily accessible resource for knowledge-based causal inference [10], [11], [12].</p>
<p>Kıcıman et al. have shown that Large Language Models (LLMs) are effective in determining causality direction between pairs of variables, outperforming even human analysis in this respect [10]. However, other studies highlight LLMs' limitations in constructing causal DAGs from sets of variables, not satisfying even in small-scale contexts [13], [14]. This difficulty mainly stems from the inherent complexity in inferring detailed causal mechanisms, such as establishing the relative directness of causes for an effect, a task that often exceeds simple knowledge-based inference.</p>
<p>In response to these challenges, recent studies have begun integrating LLM-derived causal knowledge with data analysis to enhance causal discovery. For example, Ban et al. [15] utilize LLMs to discern the presence of causal links among variables, subsequently applying ancestral constraints to structure learning [7]. This approach yields improvements in learning causal structures from data for smaller-scale problems, but it encounters difficulties with larger datasets due to inaccuracies in the LLM-derived constraints, as evidenced in Table I. As an alternative, Vashishtha et al. [16] employ a detailed, pairbased prompting strategy with a voting system to determine reliable prior knowledge. Regretably, the authors fail to show the effectiveness on the larger-scale datasets, likely limited by the complexity and computational demands of the prompt process, which requires $\binom{N}{2}$ LLM inferences with $N$ denoting the variable count.</p>
<p>In response to the challenges, we introduce a simple but effective strategy, named iterative LLM supervised CSL framework (ILS-CSL). Contrasting with prior methodologies that deploy LLMs and CSL separately, ILS-CSL uniquely focuses LLMs on verifying direct causal relationships already suggested by the data. Specifically, ILS-CSL employs LLMs to validate the accuracy of edges in the learned causal DAG, with an iterative process fine-tuning CSL based on LLM feedback. The iteration concludes when the LLM-based inferences and data-driven CSL align within the established causal structure. This innovative integration of LLMs into the CSL process</p>
<p>TABLE I: SHD$\downarrow$ and constraint quality of the ancestral constraint-based CSL driven by GPT-4, reported in the work [15].</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Cancer 5 nodes</th>
<th></th>
<th>Asia 8 nodes</th>
<th></th>
<th>Child 20 nodes</th>
<th></th>
<th>Insurance 27 nodes</th>
<th></th>
<th>Alarm 37 nodes</th>
<th></th>
<th>Mildew 35 nodes</th>
<th></th>
<th>Water 32 nodes</th>
<th></th>
<th>Barley 48 nodes</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Data size</td>
<td>250</td>
<td>1000</td>
<td>250</td>
<td>1000</td>
<td>500</td>
<td>2000</td>
<td>500</td>
<td>2000</td>
<td>1000</td>
<td>4000</td>
<td>8000</td>
<td>32000</td>
<td>1000</td>
<td>4000</td>
<td>2000</td>
<td>8000</td>
</tr>
<tr>
<td>MINOB5x</td>
<td>3.0</td>
<td>1.8</td>
<td>4.2</td>
<td>2.5</td>
<td>9.5</td>
<td>5.3</td>
<td>25.7</td>
<td>15.0</td>
<td>9.5</td>
<td>6.5</td>
<td>22.8</td>
<td>21.0</td>
<td>62.3</td>
<td>53.7</td>
<td>47.0</td>
<td>33.7</td>
</tr>
<tr>
<td>+GPT-4</td>
<td>0.5</td>
<td>0.0</td>
<td>2.2</td>
<td>0.3</td>
<td>10.5</td>
<td>7.8</td>
<td>24.5</td>
<td>16.2</td>
<td>12.3</td>
<td>8.8</td>
<td>40.5</td>
<td>21.5</td>
<td>66.7</td>
<td>55.7</td>
<td>52.0</td>
<td>54.5</td>
</tr>
<tr>
<td>CaMML</td>
<td>2.0</td>
<td>2.5</td>
<td>3.5</td>
<td>2.2</td>
<td>6.0</td>
<td>1.0</td>
<td>34.3</td>
<td>31.7</td>
<td>11.0</td>
<td>8.2</td>
<td>48.2</td>
<td>62.2</td>
<td>59.0</td>
<td>53.2</td>
<td>81.5</td>
<td>81.2</td>
</tr>
<tr>
<td>+GPT-4</td>
<td>2.0</td>
<td>1.3</td>
<td>0.2</td>
<td>0.0</td>
<td>4.7</td>
<td>1.0</td>
<td>27.0</td>
<td>22.2</td>
<td>6.0</td>
<td>3.0</td>
<td>49.2</td>
<td>60.0</td>
<td>58.7</td>
<td>48.3</td>
<td>82.2</td>
<td>82.3</td>
</tr>
<tr>
<td>T / F</td>
<td>5 / 0</td>
<td></td>
<td>9 / 0</td>
<td></td>
<td>8 / 2</td>
<td></td>
<td>10 / 0</td>
<td></td>
<td>20 / 1</td>
<td></td>
<td>9 / 6</td>
<td></td>
<td>5 / 3</td>
<td></td>
<td>17 / 7</td>
<td></td>
</tr>
</tbody>
</table>
<p>The bold SHD is the best performance in each dataset. The cell highlighted in gray indicates a degraded performance by integrating LLM-derived causal knowledge. The row ‘T / F’ represents the number of correct LLM-derived structural constraints (T) and that of erroneous ones (F).
offers significant enhancements to the task, as outlined below.</p>
<p>1) Powerful Structural Constraints: ILS-CSL transforms the causal inferences made by LLMs into structural constraints explicitly indicating the edge existence or absence. The edge-level constraint is more powerful than its path-level counterpart (ancestral constraint) in improving CSL ${ }^{2}$, with less risk ${ }^{3}$. Please see Section III-C for further discussions.
2) Mitigation of Prior Errors: ILS-CSL markedly diminishes the count of erroneous constraints, all while harnessing identical LLM resources. The reduction is theoretically by a factor of $O(N)$, estimated as $1.8(N-$ 1), compared to the full inference on pairwise variables. Please refer to Section V-B for detailed estimation.
3) Efficient Causal Inference with LLM: ILS-CSL decreases the number of pairwise variable inferences from $\binom{N}{2}$ to about $O(N)$, as the LLM inference is restricted in the edges of of causal DAG ${ }^{4}$. Such reduction makes the process more manageable and enhances the scalability of the framework.</p>
<p>ILS-CSL has shown consistent improvement in data-driven CSL across all scales of the dataset used in the previous study [15]. It effectively leverages various backbone causal discovery algorithms and demonstrates superior performance, especially as the number of variables increases. These results underscore ILS-CSL's significant potential for facilitating complex causal discovery tasks in real-world scenarios.</p>
<h2>II. Related Work</h2>
<p>This section discusses the emerging interest in the use of Large Language Models' (LLMs) common sense for understanding causal knowledge. It particularly focuses on the ways this knowledge is being harnessed in causal discovery.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>A. LLM-based Causal Discovery</h2>
<p>Recent advancements in LLM-based causal discovery primarily focus on assessing the inherent capabilities of LLMs [17], [18]. Long et al. [14] have tested LLMs' ability to generate simple causal structures, typically with sets of 3-4 variables. In a specialized domain, a study [13] investigates LLMs' effectiveness in discerning causal relationships within medical pain diagnosis, though the findings were somewhat inconclusive.</p>
<p>Kıcıman et al. [10] have made strides in optimizing LLM performance for causal analysis by developing more refined prompting techniques. Their work assesses LLMs across a range of causal tasks, revealing notable performance in pairwise causal discovery [19] and counterfactual inference [20], even outperforming human analysis in certain aspects. Additionally, they have enhanced LLMs' capacity to identify causal structures in datasets concerning medical pain diagnosis. However, despite these advancements, a significant gap persists between the quality of causal DAGs generated by LLMs and those derived from data-based algorithms. These findings highlight the potential of LLM-based causal knowledge, yet they also underscore the importance of integrating data in uncovering genuine causal mechanisms.</p>
<h2>B. Integration of LLM in Data-based Causal Discovery</h2>
<p>A recent work first introduces LLM in causal discovery from data [15]. Recognizing LLMs' limitations in differentiating indirect from direct causality, they applied ancestral constraints based on LLM-generated statements about the existence of causal relationships between variable pairs. The authors prompted the LLM with a complete set of variables, seeking the most confident causal assertions. However, when presented with numerous variables, LLM struggles to provide results that align with causal structures. This complexity leads to a decrease in the accuracy of causal statements as the number of variables increases, as demonstrated in Table I. Moreover, we observe that the LLM also fails to make comprehensive causal analyses in larger scale datasets as would be possible with individual prompts for each pair of variables.</p>
<p>Motivated by this work, Vashishtha et al. [16] adopted a more targeted method. They individually prompted the LLM</p>
<p>for causal relationships between each variable pair and implemented a voting strategy to deduce ordering constraints. These constraints, although weaker than ancestral constraints (see Section III-C for illustrations), offer more precise structural guidance for causal discovery. Their methodology demonstrates notable improvements across seven real-world datasets. However, the largest dataset examined containes only 23 nodes, leaving the approach’s effectiveness in more complex scenarios untested.</p>
<h2>III Preliminaries</h2>
<p>We begin by introducing the task of causal structure learning (CSL) on causal Bayesian Networks (BNs) and subsequently discuss the integration of structural constraints.</p>
<h3>III-A Causal Bayesian Network</h3>
<p>A Bayesian Network (BN) is a probabilistic graphical model that uses a Directed Acyclic Graph (DAG) to represent conditional dependencies among a set of variables, thus defining their joint probability distribution. For a set of variables $X=\left{X_{1},X_{2},...,X_{n}\right}$ in a BN $\mathcal{G}$, the joint probability distribution is given by:</p>
<p>$P(X_{1},X_{2},...,X_{n})=\prod_{i=1}^{n}{P(X_{i}\mid\mathbf{Pa}_{i}^{\mathcal{G}})}$</p>
<p>$\mathbf{Pa}<em i="i">{i}^{\mathcal{G}}$ denotes the parent nodes of $X</em>$ in the DAG. It’s important to note that an edge in a BN does not inherently imply a causal relationship [1]. A BN representing a joint probability distribution can be constructed using any variable ordering. However, the causal order of variables, indicating cause and effect, cannot be arbitrarily reversed.</p>
<p>A causal BN, in contrast, not only models the data distribution but also conforms to the principles of causality [21]. In the context of cause-effect relationship, intervening on the causes should render the effect independent of other factors. This introduces additional requirements for representing causality in a BN. In a causal BN, intervening on any subset of variables $X_{I}\subseteq X$, denoted as $d{o}(X_{I}=x)$, results in a modified probability distribution $P_{I}(X)$. This is computed by severing the edges from each variable in $X_{I}$ to its parents and fixing their values as per the intervention:</p>
<p>$P_{I}(X)=\prod_{X_{i}\notin X_{I}}P(X_{i}\mid\mathbf{Pa}_{i}^{\mathcal{G}})\quad\text{for all }X\text{ consistent with }x$</p>
<p>This aspect of causal BNs allows for the modeling of interventions and causal inferences, distinguishing them from standard BNs. It is important to note that in real-world scenarios, direct intervention data is often not available. As a result, observed data is typically employed to infer intervention characteristics and understand causal relationships.</p>
<h3>III-B Learning Causal BNs</h3>
<p>This part introduces the task of two mainstream solutions of learning causal BNs, constraint- and score-based methods. Formally, let $\mathbf{D}\in\mathbb{N}^{m\times n}$ represent the observational data, where $m$ denotes the number of observed samples and $n$ represents the number of observed variables, denoted as $X=\left{X_{1},X_{2},...,X_{n}\right}$. Each $X_{i}$ in $\mathbf{D}$ takes discrete integer values in the range $[0,C_{i})$. Given $\mathbf{D}$, the goal is to determine the causal DAG $\mathcal{G}=(X,E(\mathcal{G}))$, where $E(\mathcal{G})$ denotes the set of directed causal edges among the variables in $X$. The formal definitions are present as follows:</p>
<p>$E(\mathcal{G})\leftarrow\left{X_{i}-X_{j}\left|X_{i}\not\perp X_{j}\right|Y,\ \forall Y \subseteq X \backslash\left{X_{i},X_{j}\right}\right}$ (1)
$\max <em i="1">{\mathcal{G}} \sigma(\mathcal{G} ; \mathbf{D})=\sum</em>}^{n} \mathcal{L<em i="i">{\sigma}\left(X</em>$ (2)} \mid \mathbf{P a}_{i}^{\mathcal{G}} ; \mathbf{D}\right)$ s.t. $\mathcal{G} \in \mathrm{DAG</p>
<p>Equations (1) and (2) define the CSL task of constraint- and score-based methods, repectively. Constraint-based methods first determine the skeleton of the graph using undirected edges, $X_{i}-X_{j}$, based on conditional independence tests. Subsequently, they orient some of these edges based on Vstructure detection and DAG constraints [22], [23]. Scorebased methods employ a scoring function, $\sigma$, to evaluate how well a given causal DAG $\mathcal{G}$ represents the observed data $\mathbf{D}$. Typically, $\sigma$ can be decomposed into scores of local structures, $\mathcal{L}<em i="i">{\sigma}\left(X</em>\right)$, which simplifies the search process [24], [25]. The objective is to optimize these local scores by assigning appropriate parent nodes to each node, ensuring the resulting graph is a DAG. An alternative approach to searching the DAG space is the ordering-based search, which optimizes Equation (2) under a given ordering $O$, inherently satisfying the DAG constraint [26], [27]. The best-scored DAG of the searched orderings is then selected as the output.} \mid \mathbf{P a}_{i}^{\mathcal{G}} ; \mathbf{D</p>
<p>The design of scoring functions is based on the posterior probability of the DAG given the data, which includes a component representing the prior probability of DAG structures. Due to this adaptability in accommodating the prior constraints on structures, the score-based method is chosen as the backbone CSL algorithm in our ILS-CSL framework.</p>
<h3>III-C Prior Constraints on Structures</h3>
<p>Prior structural constraints play a pivotal role in improving the discovery of causal structures. The most prevalent among these constraints include [28]:</p>
<ul>
<li>Edge Existence: Denoted as $X_{i} \rightarrow X_{j}$ or, when forbidden, $X_{i} \nrightarrow X_{j}$. This constraint dictates that the DAG should (or should not) contain the edge $X_{i} \rightarrow X_{j}$.</li>
<li>Ordering Constraint: Represented as $X_{i} \prec X_{j}$, it mandates that $X_{i}$ should precede $X_{j}$ in the variable ordering.</li>
<li>Path Existence (Ancestral Constraint): Symbolized as $X_{i} \rightsquigarrow X_{j}$, it requires the DAG to encompass the path $X_{i} \rightsquigarrow X_{j}$.
Given the implication chain $X_{i} \rightarrow X_{j} \Rightarrow X_{i} \rightsquigarrow X_{j} \Rightarrow$ $X_{i} \prec X_{j}$, it is clear that the existence of an edge (direct causality) represents the most stringent structural constraint. Correspondingly, its derivation necessitates a thorough examination of potential combinations of causality. Regrettably, as evidenced by the studies [10], [15], [13], LLMs lack the ability</li>
</ul>
<p>Algorithm 1 LLM supervised CSL
0: Observed data, D; Textual descriptions, T
0: Causal DAG, $\mathcal{G}$
1: Initialize the set of structural constraints, $\lambda \leftarrow{}$
2: repeat
3: $\quad \mathcal{G} \leftarrow \underset{\mathcal{G}}{\arg \max } \sigma(\mathcal{G} ; \mathbf{D})$, s.t. $\mathcal{G} \in \operatorname{DAG}, \mathcal{G}=\lambda$
4: for $X_{i} \xrightarrow{\mathcal{G}} X_{j} \in E(\mathcal{G})$ do
5: $\quad c \leftarrow$ LLM infers causality between $X_{i}$ and $X_{j}$ based on T
6: if $c$ is $X_{i} \leftarrow X_{j}$ then
7: $\quad \lambda \leftarrow \lambda \cup\left{X_{j} \rightarrow X_{i}\right}$
8: end if
9: if $c$ is $X_{i} \rightleftharpoons X_{j}$ then
10: $\quad \lambda \leftarrow \lambda \cup\left{X_{i} \rightleftharpoons X_{j}, X_{j} \rightleftharpoons X_{i}}$
11: end if
12: end for
13: until no new constraints are added
14: return $\mathcal{G}$
to accurately specify direct causality, often confusing it with indirect causality or non-causal correlations. Please refer to Appendix VII-F for empirical estimation.</p>
<p>Regarding the application of these prior constraints, there are two predominant methodologies: hard and soft approaches. The hard approach prioritizes adherence to prior constraints, followed by score optimization [29]. Conversely, the soft approach strikes a balance between honoring prior constraints and the associated score costs [8]. This often involves adjusting the scoring function to $\sigma(\mathcal{G} ; \mathbf{D})+b(\mathcal{G} ; \lambda)$, where a prior probability $P_{\lambda}$ is assigned to structural constraints $\lambda$. A constraint is only accepted if the bonus score, $b$, compensates for the penalty in the DAG-data consistency score, $\sigma$.</p>
<p>We implement both hard and soft approaches to incorporate structural constraints in this paper.</p>
<h2>IV. Iterative LLM Supervised Causal Structure LEARNING</h2>
<p>Given the observed data, $\mathbf{D}$, and the descriptive texts on the investigated field and variables, $\mathbf{T}$, the LLM supervised causal structure learning is presented in Algorithm 1.</p>
<p>Initially, a causal DAG $\mathcal{G}$ is learned from $\mathbf{D}$ with modular scoring function $\sigma, \mathcal{L}_{\sigma}$ (see Equation (2) for definition), and search method $\mathcal{M}$. Subsequently, we explicate the details on LLM supervision and how to constrain CSL accordingly.</p>
<h2>A. LLM Supervision</h2>
<p>For each directed edge $X_{i} \rightarrow X_{j} \in E(\mathcal{G})$, we prompt the used LLM to verify the causal statement that $X_{i}$ causes $X_{j}$ (Line 5 in Algorithm 1). The prompt design for causal inference is inspired by the work [10], which employs choicebased queries to determine the orientation of pairwise variables with known causal relationships. On this basis, we incorporate field-specific descriptions to provide context and introduce additional choices to accommodate uncertainties in causal existence and intricate causal mechanisms. For a given edge
$X_{i} \rightarrow X_{j}$ and associated textual descriptions $\mathbf{T}=\left{t_{f}, t_{i}, t_{j}\right}$, the LLM is prompted as:</p>
<div class="codehilite"><pre><span></span><code>You<span class="w"> </span>are<span class="w"> </span>an<span class="w"> </span>expert<span class="w"> </span>on<span class="w"> </span>\(t_{f}\).<span class="w"> </span>There<span class="w"> </span>are<span class="w"> </span>two
factors:<span class="w"> </span>\(X_{i}:<span class="w"> </span>t_{i},<span class="w"> </span>X_{j}:<span class="w"> </span>t_{j}\).
Which<span class="w"> </span>cause-and-effect<span class="w"> </span>relationship<span class="w"> </span>is<span class="w"> </span>more<span class="w"> </span>likely
for<span class="w"> </span>following<span class="w"> </span>causal<span class="w"> </span>statements<span class="w"> </span>for<span class="w"> </span>V1<span class="w"> </span>and<span class="w"> </span>V2?
A.changing<span class="w"> </span>V1<span class="w"> </span>causes<span class="w"> </span>a<span class="w"> </span>change<span class="w"> </span>in<span class="w"> </span>V2.
B.changing<span class="w"> </span>V2<span class="w"> </span>causes<span class="w"> </span>a<span class="w"> </span>change<span class="w"> </span>in<span class="w"> </span>V1.
C.changes<span class="w"> </span>in<span class="w"> </span>V1<span class="w"> </span>and<span class="w"> </span>in<span class="w"> </span>V2<span class="w"> </span>are<span class="w"> </span>not<span class="w"> </span>correlated.
D.uncertain.
Provide<span class="w"> </span>your<span class="w"> </span>final<span class="w"> </span>answer<span class="w"> </span>within<span class="w"> </span>the<span class="w"> </span>tags
<span class="nt">&lt;Answer&gt;</span>A/B/C/D<span class="nt">&lt;/Answer&gt;</span>.
Analyze<span class="w"> </span>the<span class="w"> </span>statement:<span class="w"> </span>\(X_{i}<span class="w"> </span>X_{j}\).
</code></pre></div>

<p>$t_{f}$ describes the investigated field, and $t_{i}, t_{j}$ describes $X_{i}, X_{j}$, respectively. From the LLM's response to this prompt, we can obtain one of the answers: A, B, C, or D.</p>
<p>To specify constraints $\lambda$ (Lines 6-11 in Algorithm 1), if the answer is B (reversed), we specify the existence of $X_{j} \rightarrow X_{i}$. If C (no causality), then we specify $X_{i} \rightleftharpoons X_{j}$ to forbid the existence of edge. If D (uncertain) or A (correct), we do not specify constraints. This is because specifying the existence of an edge already discovered from data does not often enhance the CSL and can inadvertently lead to errors. For example, if the true structure is $X_{i} \rightleftharpoons X_{j}$ but not directly, $X_{i} \rightleftharpoons X_{j}$, LLM easily infers that $X_{i}$ causes $X_{j}$ due to its shortness in distinguishing indirect causality for the direct. If we specify $X_{i} \rightarrow X_{j}$, an erroneous edge is introduced.</p>
<h2>B. Prior constraint-based CSL</h2>
<p>With the structural constraints $\lambda$ obtained from LLM supervision, we integrate them into the next iteration of CSL process (Line 3 in Algorithm 1), with either hard or soft approach. The process terminates if no new constraint is specified.
a) Hard approach: Firstly, the edge existence and forbidden constraints are used to specify the set of legal candidate parents, $C(i)$, and the set of variables always included in the parents, $K(i)$, of each variable $X_{i}$.</p>
<p>$$
\begin{aligned}
&amp; C(i)=X \backslash\left{X_{j} \mid X_{j} \rightleftharpoons X_{i} \in \lambda\right} \backslash\left{X_{i}\right} \
&amp; K(i)=\left{X_{j} \mid X_{j} \rightarrow X_{i} \in \lambda\right}
\end{aligned}
$$</p>
<p>With $K(i), C(i)$, we prune the space of local structures.</p>
<p>$$
L\left(X_{i} ; \lambda\right)={P \mid K(i) \subseteq P \subseteq C(i)}
$$</p>
<p>The pruned space of local structures, $L(\cdot)$, is taken as input for the search method $\mathcal{M}$ :</p>
<p>$$
\begin{aligned}
&amp; \mathcal{M}: \max <em i="i">{\mathbf{P a}</em>}^{\mathcal{G}}} \sum_{i}^{n} \mathcal{L<em i="i">{\sigma}\left(X</em>} \mid \mathbf{P a<em i="i">{i}^{\mathcal{G}} ; \mathbf{D}\right) \
&amp; \text { s.t. } \mathcal{G} \in \mathrm{DAG}, \mathbf{P a}</em> ; \lambda\right)
\end{aligned}
$$}^{\mathcal{G}} \in L\left(X_{i</p>
<p>In comparison to the problem form without prior constraints, as presented in Equation (2), the restriction of the candidate parent sets of each node, $\mathbf{P a}<em i="i">{i}^{\mathcal{G}} \in L\left(X</em> \models \lambda$.} ; \lambda\right)$, ensures that the output DAG absolutely satisfies every edge constraint, $\mathcal{G</p>
<p>b) Soft approach: We adapt the scoring function to model the edge constraints as follows:</p>
<p>$\sigma^{\prime}(\mathcal{G} ;\mathcal{D},\lambda)=\sum_{i}^{n}\mathcal{L}<em i="i">{\sigma}\left(X</em>}\mid\mathbf{Pa<em b="b">{i}^{\mathcal{G}};\mathbf{D}\right)+\mathcal{L}</em>}\left(X_{i},\mathbf{Pa<em b="b">{i}^{\mathcal{G}};{\lambda}\right)$ (6)
$\mathcal{L}</em>}\left(X_{i},\mathbf{Pa<em X__j="X_{j">{i}^{\mathcal{G}};{\lambda}\right)=$
$\sum</em>}\rightarrow X_{i}\in\lambda}\left(\mathbb{I<em j="j">{X</em>}\in\mathbf{Pa<em _lambda="\lambda">{i}^{\mathcal{G}}}\log P</em>}+\mathbb{I<em j="j">{X</em>}\notin\mathbf{Pa<em _lambda="\lambda">{i}^{\mathcal{G}}}\log\left(1-P</em>\right)\right)$ (7)
$+\sum_{X_{j}\rightsquigarrow X_{i}\in\lambda}\left(\mathbb{I}<em j="j">{X</em>}\in\mathbf{Pa<em _lambda="\lambda">{i}^{\mathcal{G}}}\log\left(1-P</em>}\right)+\mathbb{I<em j="j">{X</em>}\notin\mathbf{Pa<em _lambda="\lambda">{i}^{\mathcal{G}}}\log P</em>\right)$</p>
<p>This formulation is grounded in the decomposability of edge constraints. A detailed derivation can be found in Section VI-A. $\mathbb{I}<em _lambda="\lambda">{\text {condition }}$ is the indicator function, which takes the value 1 if the condition is true and 0 otherwise. $P</em>$ is the prior confidence, a hyper-parameter. Then search method $M$ optimizes the modified score:
$\mathcal{M}: \max <em i="i">{\mathcal{G}} \sum</em>}^{n} \mathcal{L<em i="i">{\sigma}\left(X</em>} \mid \mathbf{P a<em b="b">{i}^{\mathcal{G}} ; \mathbf{D}\right)+\mathcal{L}</em>}\left(X_{i}, \mathbf{P a<em b="b">{i}^{\mathcal{G}} ;{\lambda}\right)$, s.t. $\mathcal{G} \in \mathrm{DAG}$
The bonus score, $\mathcal{L}</em>$.}$, favors DAGs that align more closely with the structural constraints. Note that a constraint will not be satisfied if it excessively penalizes the score $\mathcal{L}_{\sigma</p>
<p>To sum up, while the hard approach derives greater benefits from accurate constraints (at the risk of being more sensitive to errors), the soft approach might not always adhere to all correct constraints but offers a degree of resilience against potential inaccuracies.</p>
<h2>V. ANALYSIS OF KEY CONCERNS</h2>
<p>Theoretically quantifying the impact of prior knowledge on learned causal structures is difficult, mainly due to the complex and unpredictable nature of data insufficiency and noise. Analyzing the disparity between data-implied causal structures and actual causal truths is intricate. Making strict assumptions for analytical purposes might not reflect realworld scenarios, potentially leading to theoretical conclusions with limited practical applicability.</p>
<p>Nevertheless, we can examine two primary aspects of prior knowledge under simple and general assumptions: 1) the ability of the applied prior knowledge to correct causal structures, and 2) the alignment of the quality of this derived prior knowledge with the actual causal structures. These aspects provide a more tangible and realistic assessment of the effectiveness of prior knowledge in causal discovery.</p>
<h2>A. Correction of Prior Independent Structures</h2>
<p>Causal discovery fundamentally seeks to uncover unknown causal mechanisms. The role of prior knowledge, representing known causality, extends beyond merely adjusting the final output; it should ideally enhance the accuracy of the reconstructed causal structures. A key question is whether a prior constraint can indirectly influence and correct edges that are not directly governed by this knowledge.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: An example of recovering missing edges by reversing existing edges.</p>
<p>In the context of ILS-CSL, this question becomes particularly relevant when examining the orientation and prohibition of learned edges: Do these constraints contribute to identifying missing edges? We explore this aspect, offering an illustrative example in Figure 1. Due to limitations in real-world observational data, the probability distribution suggested by the data corresponds to a DAG with two errors: one reversed edge and one missing edge.</p>
<p>ILS-CSL supervises the existing edges and corrects the reversed edge $X_{3} \rightarrow X_{2}$. According to Bayesian Network principles, we have $P\left(X_{3} \mid \mathbf{P a}<em 3="3">{3}^{\mathcal{G}}\right)=P\left(X</em>$, thus refining the DAG to better align with the underlying data distribution.} \mid X_{1}, X_{2}\right)$. However, the observed data indicate that $X_{3}$ and $X_{1}$ are not independent when conditioned on $X_{2}$, as per the current DAG structure. This inconsistency implies that the BN cannot accurately model the data distribution if $X_{2}$ is the only parent of $X_{3}$. Consequently, ILS-CSL identifies and reinstates the missing edge $X_{1} \rightarrow X_{3</p>
<p>Viewing this from the lens of knowledge-based causality, constraints derived from known causal relations can enhance the discovery of unknown causal mechanisms within data. This highlights the invaluable role of prior knowledge in advancing causal discovery in uncharted fields.</p>
<h2>B. Estimation of Prior Error Counts</h2>
<p>This section estimates and compares the number of erroneous constraints ILS-CSL against that stemming from a full inference on all pairwise variables, an intuitive strategy in the existing methods [15], [16].</p>
<p>We commence by defining five cases during LLM-based causality inference, along with their respective probabilities:</p>
<p>1) Extra Causality $\left(p_{r}\right)$ : Given a causal statement $\left(X_{1}, X_{2}\right)$, if the true causal DAG neither contains the path $X_{1} \rightsquigarrow$ $X_{2}$ nor $X_{2} \rightsquigarrow X_{1}$, it's an instance of extra causality.
2) Reversed Causality $\left(p_{r}\right)$ : Given a causal statement $\left(X_{1}, X_{2}\right)$, if the true causal DAG contains the path $X_{2} \rightsquigarrow X_{1}$, it's an instance of reversed causality.
3) Reversed Direct Causality $\left(p_{r}^{\mathrm{d}}\right)$ : Given a causal statement $\left(X_{1}, X_{2}\right)$, if the true causal DAG has an edge $X_{2} \rightarrow X_{1}$, it's an instance of extra causality.
4) Missing Direct Causality $\left(p_{m}^{\mathrm{d}}\right)$ : If an edge $X_{1} \rightarrow X_{2}$ or $X_{2} \rightarrow X_{1}$ exist in the true causal DAG, but $X_{1}$ and $X_{2}$ are inferred to have no causal relationship, it's a instance of missing direct causality.</p>
<p>5) Correct Existing Causality ( $p_{c}$ ): Given a causal statement $\left(X_{1}, X_{2}\right)$, if the path $X_{1} \rightsquigarrow X_{2}$ exists in the true causal DAG, it's a instance of correct existing causality.</p>
<p>We assume that 1) the probability of these cases is identical when satisfying the corresponding structures, and 2) the truth DAG and learned DAG are both sparse.</p>
<p>Consider a causal DAG consisting of $N$ nodes. Based on the sparsity assumption, the number of node pairs without connecting paths in the truth DAG is represented as $\gamma_{1}\binom{N}{2}$. In the learned causal DAG, there are $\gamma_{2} N$ edges. Of these edges, the proportion of correctly identified edges is denoted as $z_{1}$, the proportion of reversed edges as $z_{2}$, and the proportion of extra edges that do not exist in the true DAG as $z_{3}$.</p>
<p>The number of prior errors derived from full inference consists of two parts: the extra causality, $p_{e} \gamma_{1}\binom{N}{2}$, and the reversed causality, $p_{r}\left(1-\gamma_{1}\right)\binom{N}{2}$. Note that the missing causality will not harm the CSL since it does not produce any structural constraints in this context. Then the total number of erroneous constraints is estimated as:</p>
<p>$$
E_{\text {full }}=\left(p_{e} \gamma_{1}+p_{r}\left(1-\gamma_{1}\right)\right)\binom{N}{2}
$$</p>
<p>As for the prior errors within our framework, we consider the output DAG of CSL algorithms. The erroneous constraints on the correctly discovered edges consist of the reversed and missing direct causality: $\left(p_{r}^{d}+p_{m}^{d}\right) z_{1} \gamma_{2} N$; The erroneous constraints derived from inferring causality on erroneous edges consist of 1) missing direct causality on reversed edges, $p_{m}^{d} z_{2} \gamma_{2} N$, and 2) extra inferred direct causality on extra edges no more than $\left(p_{r}+p_{c} P_{R \mid E}\right) z_{3} \gamma_{2} N$, where $P_{R \mid E}$ is the probability where for an extra edge $X_{1} \rightarrow X_{2}$ in the learned DAG, a reversed path $X_{2} \rightsquigarrow X_{1}$ exists in the ground truth. Gathering all these, we derive the number prior errors:</p>
<p>$$
E_{\text {ours }} \leq\left(\left(p_{r}^{d}+p_{m}^{d}\right) z_{1}+p_{m}^{d} z_{2}+\left(p_{r}+p_{c} P_{R \mid E}\right) z_{3}\right) \gamma_{2} N
$$</p>
<p>We utilize eight real-world datasets, and GPT-4 as LLM to estimate $p$, and MIONBSx algorithm to estimate $\lambda, r, P_{R \mid E}$, see Section VI-B for details. The results are:</p>
<p>$$
\begin{aligned}
&amp; p_{e} \approx 0.56, p_{r} \approx 0.15, p_{r}^{d} \approx 0.03, p_{m}^{d} \approx 0.05 \
&amp; p_{c} \approx 0.75, \gamma_{1} \approx 0.51, \gamma_{2} \approx 1.09, z_{1} \approx 0.88 \
&amp; z_{2} \approx 0.05, z_{3} \approx 0.07, P_{R \mid E} \approx 0.05
\end{aligned}
$$</p>
<p>And then we have:</p>
<p>$$
E_{\text {ours }} \approx 0.10 N, E_{\text {full }} \approx 0.36\binom{N}{2}, \frac{E_{\text {ours }}}{E_{\text {full }}} \approx \frac{1}{1.8(N-1)}
$$</p>
<p>This indicates that, relative to full pairwise variable inference, ILS-CSL significantly reduces the number of erroneous constraints resulting from imperfect LLM inferences by approximately a factor of $1.8(N-1)$. This reduction is particularly impactful when dealing with larger sets of variables.</p>
<h2>VI. SUPPLEMENTARY ILLUSTRATIONS</h2>
<h2>A. Derivation of Prior-based Scoring</h2>
<p>In this section, we derive the prior-based scoring function, as presented in Equations (6) and (7), for the DAG $\mathcal{G}(X, E(\mathcal{G}))$. The prior constraints are denoted as $\lambda:&lt;\mathbf{R}, \boldsymbol{\Pi}&gt;$. The set $\mathbf{R}=\left{r_{1}, r_{2}, \cdots, r_{m}\right}$ comprises edge variables on $m$ pairwise variables, where $r_{i} \in{\rightarrow, \nrightarrow}$. $\boldsymbol{\Pi}=\Pi_{i=1}^{m} P\left(r_{i}\right)$ is the associated probability distribution.</p>
<p>Beginning with the derivation of the scoring function without prior constraints, let $\mathbf{D}$ be a complete multinomial observed data over variables $X$. Utilizing the Bayesian Theorem, the probability of a network $\mathcal{G}$ over $X$ is expressed as:</p>
<p>$$
P(\mathcal{G} \mid \mathbf{D}) \propto P(\mathbf{D} \mid \mathcal{G}) \cdot P(\mathcal{G})
$$</p>
<p>Given that $P(\mathbf{D})$ remains consistent across all DAGs, the score of a network is typically the logarithm of $P(\mathcal{G} \mid \mathbf{D})$, resulting in $S c(\mathcal{G} \mid \mathbf{D})=S c(\mathbf{D} \mid \mathcal{G})+S c(\mathcal{G})$. Bayesian scoring methods, such as K2 [30] and BDe, BDeu [24], aim to approximate the log-likelihood based on various assumptions. When priors are uniform, $S c(\mathcal{G})$ can be disregarded during maximization. However, with the introduction of prior structural constraints, denoted as $\lambda$, this term gains significance.</p>
<p>Let's define $C$ as a configuration, representing a joint instantiation of values to edge variables $\mathbf{R}=\left{r_{1}, r_{2}, \ldots, r_{m}\right}$. The probability for this configuration is $J_{C}=P(\mathbf{R}=C \mid \boldsymbol{\Pi})$. For a specific DAG $\mathcal{G}$, its configuration is represented as $C_{\mathcal{G}}$. Thus, we can express:</p>
<p>$$
P(\mathcal{G} \mid \mathbf{D}, \lambda)=\frac{P(\mathbf{D} \mid \mathcal{G}) \cdot P(\mathcal{G} \mid J)}{P(\mathbf{D} \mid J)}
$$</p>
<p>The above equation is derived from the understanding that, given the graph $\mathcal{G}$, the data $\mathbf{D}$ is independent of $J$. This is because $J$ offers no supplementary information about the data once the graph structure is known. The term $P(\mathbf{D} \mid J)$ serves as a normalizing constant, consistent across all DAGs. The term $P(\mathbf{D} \mid \mathcal{G})$ corresponds to the scoring function $S c(\mathbf{D} \mid \mathcal{G})$ in the absence of prior constraints. The scoring function can be expressed as:</p>
<p>$$
S c(\mathcal{G} \mid \mathbf{D}, \lambda)=S c(\mathbf{D} \mid \mathcal{G})+S c(\mathcal{G} \mid J)
$$</p>
<p>Here, $S c(\mathbf{D} \mid \mathcal{G})$ represents the scoring function without prior constraints, denoted as $\sigma(\mathcal{G} \mid \mathbf{D})$. Meanwhile, $S c(\mathcal{G} \mid J)$ pertains to the bonus score associated with prior constraints. Shifting our focus to the prior factor $P(\mathcal{G} \mid J)$, we have:</p>
<p>$$
\begin{aligned}
P(\mathcal{G} \mid J) &amp; =P\left(\mathcal{G}, C_{\mathcal{G}} \mid J\right)=P\left(\mathcal{G} \mid J, C_{\mathcal{G}}\right) \cdot P\left(C_{\mathcal{G}} \mid J\right) \
&amp; =P\left(\mathcal{G} \mid C_{\mathcal{G}}\right) \cdot J_{C_{\mathcal{G}}}
\end{aligned}
$$</p>
<p>The first equation holds since $C_{\mathcal{G}}$ is inherently a function of $\mathcal{G}$. The term $P\left(\mathcal{G} \mid C_{\mathcal{G}}\right)$ denotes the likelihood of graph $\mathcal{G}$ when a specific configuration is present. In the absence of any other prior constraints, we assign an identical prior to all graphs sharing the same configuration. Let $N_{C}$ represent the count</p>
<p>TABLE II: Accuracy and reversed ratio of the sampled pairwise variables on eight datasets.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Alarm</th>
<th>Asia</th>
<th>Insurance</th>
<th>Mildew</th>
<th>Child</th>
<th>Cancer</th>
<th>Water</th>
<th>Barley</th>
</tr>
</thead>
<tbody>
<tr>
<td>Direct causality (Acc_{1} /Rev_{1})</td>
<td>1.00 / 0.00</td>
<td>1.00 / 0.00</td>
<td>0.85 / 0.05</td>
<td>0.95 / 0.05</td>
<td>1.00 / 0.00</td>
<td>1.00 / 0.00</td>
<td>0.95 / 0.05</td>
<td>0.70 / 0.05</td>
</tr>
<tr>
<td>Indirect causality (Acc_{2} /Rev_{2})</td>
<td>0.65 / 0.15</td>
<td>1.00 / 0.00</td>
<td>0.95 / 0.05</td>
<td>1.00 / 0.00</td>
<td>0.50 / 0.40</td>
<td>1.00 / 0.00</td>
<td>0.50 / 0.50</td>
<td>0.30 / 0.30</td>
</tr>
<tr>
<td>No causality (Acc_{3})</td>
<td>0.60</td>
<td>0.80</td>
<td>0.35</td>
<td>0.10</td>
<td>0.50</td>
<td>0.00</td>
<td>0.45</td>
<td>0.50</td>
</tr>
<tr>
<td>Qualitative causality(Acc_{4} / Rev_{4})</td>
<td>0.72 / 0.12</td>
<td>1.00 / 0.00</td>
<td>0.92 / 0.05</td>
<td>0.99 / 0.01</td>
<td>0.70 / 0.24</td>
<td>1.00 / 0.00</td>
<td>0.67 / 0.33</td>
<td>0.36 / 0.26</td>
</tr>
</tbody>
</table>
<p>of DAGs over nodes $\mathcal{V}$ that have the configuration $C$. Thus, $P\left(\mathcal{G} \mid C_{\mathcal{G}}\right)=1 / N_{C_{\mathcal{G}}}$, leading to:</p>
<p>$$
P(\mathcal{G} \mid J)=\frac{J_{C_{\mathcal{G}}}}{N_{C_{\mathcal{G}}}} \quad \text { and } \quad S c(\mathcal{G} \mid J)=\log \left(\frac{J_{C_{\mathcal{G}}}}{N_{C_{\mathcal{G}}}}\right)
$$</p>
<p>Given that the count of edge variables (or edge constraints) remains consistent across all DAGs, $N_{C_{\mathcal{G}}}$ is also consistent for all DAGs. Therefore:
$S c(\mathcal{G} \mid J)=\log J_{C_{\mathcal{G}}}=\log P\left(\mathbf{R}=C_{\mathcal{G}} \mid \boldsymbol{\Pi}\right)=\sum_{r_{i} \in \mathbf{R}} \log P\left(r_{i}\right)$
Assuming $P\left(r_{i}\right)=P_{\lambda}$ when $\lambda$ indicates the presence of the corresponding edge, and $P\left(r_{i}\right)=1-P_{\lambda}$ when the edge's existence is negated, we deduce:
$S c(\mathcal{G} \mid J)=$</p>
<p>$$
\begin{aligned}
&amp; \sum_{X_{j} \rightarrow X_{i} \in \lambda} \mathbb{I}<em j="j">{X</em>} \rightarrow X_{i} \in E(\mathcal{G})} \log P_{\lambda}+\mathbb{I<em j="j">{X</em>\right)+ \
&amp; \sum_{X_{j} \rightsquigarrow X_{i} \in \lambda} \mathbb{I}} \rightarrow X_{i} \notin E(\mathcal{G})} \log \left(1-P_{\lambda<em j="j">{X</em>} \rightarrow X_{i} \in E(\mathcal{G})} \log \left(1-P_{\lambda}\right)+\mathbb{I<em j="j">{X</em>
\end{aligned}
$$} \rightarrow X_{i} \notin E(\mathcal{G})} \log P_{\lambda</p>
<p>By integrating Equations (14), (18), and (2), we derive the form of the local prior constraint-based scoring function, as depicted in Equations (6) and (7).</p>
<h2>B. Parameter Estimation in Section V-B</h2>
<p>This section presents the details on the estimation of parameters related to the quality of LLM based causal inference, $p_{e}, p_{r}, p_{e}^{d}, p_{m}^{d}, p_{c}$, structures of the true causal DAGs, $\gamma_{1}$, and structures of the learned causal DAGs, $\gamma_{2}, z_{1}, z_{2}, z_{3}, P_{R \mid E}$.
a) Quality of LLM causal inference: We randomly sample three kinds of pairwise variables from the employed eight datasets in experiments:</p>
<p>1) Direct edges: Sampling pairwise variables with direct edge $X_{i} \rightarrow X_{j}$ in the ground truth.
2) Indirect path: Sampling pairwise variables without direct edge but with a directed path, $X_{i} \rightsquigarrow X_{j}, X_{i} \rightsquigarrow X_{j}$.
3) Not connected: Sampling pairwise variables without any path, $X_{i} \nrightarrow X_{j}, X_{j} \nrightarrow X_{i}$.
For each type, we sample 20 pairwise variables form each dataset, if more than 20 pairwise variables satisfying the condition exist in the causal DAG. Or we use all the pairwise variables as samples.</p>
<p>Subsequently, we query GPT-4 the causality between each pairwise variables through the prompt in Section IV. The true answer of Types 1 and 2 is A, and that of Type 3 is C. The accuracy of GPT-4 on different datasets on these samples
together with the ratio of reversed inference (B for Types 1 and 2) are reported in Table II.</p>
<p>Direct causality corresponds to direct edges, indirect causality to indirect paths, and no causality corresponds to not connected variables. The accuracy and reversed ratio of LLM inference on them is obtained by experiments. The qualitative causality corresponds the paths (including edges), whose accuracy is estimated by $\operatorname{Acc}<em 1="1">{4}=\left(\operatorname{Acc}</em> \times|P|\right) /(|E|+$ $|P|)$, where $|E|$ and $|P|$ represents the number of edges and indirect paths in the true causal DAG.} \times|E|+\operatorname{Acc}_{2</p>
<p>By weighted sum of the accuracy and reversed ratio, we obtain the estimation of them. Then the probability of the five introduced error that GPT-4 makes are presented as follows:</p>
<p>1) Extra causality: $p_{e}=1-\operatorname{Acc}<em r="r">{3}=0.56$
2) Reversed causality: $p</em>}=\operatorname{Rev<em r="r">{4}=0.15$
3) Reversed direct causality: $p</em>}^{d}=\operatorname{Rev<em m="m">{1}=0.03$
4) Missing direct causality: $p</em>}^{d}=1-\operatorname{Acc<em 1="1">{1}-\operatorname{Rev}</em>=0.05$
5) Correct existing causality: $p_{c}=\operatorname{Acc}_{4}=0.75$</p>
<p>We see that the major errors of GPT-4 inference is sourced from the extra causality, which is because some intuitively correlated concepts may not generate real causal relations in an experiment with specific conditions. And that is why we should refer to data for causal analysis. However, GPT4 is prone to infer correct causality on pairwise variables with direct causality, which is the base of our framework to efficiently improves the quality of learned causal DAGs.
b) Structural parameters: The structural parameters is estimated by the average value of them on the eight datasets. The ones related to the causal structure learning of each dataset is estimated by the average value of them on twelve segments of observed data, using MINOBSx search and BDeu score. See the detailed results in Table III.</p>
<p>TABLE III: The estimated structural paramters on eight datasets.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Alarm</th>
<th style="text-align: center;">Asia</th>
<th style="text-align: center;">Insurance</th>
<th style="text-align: center;">Mildew</th>
<th style="text-align: center;">Child</th>
<th style="text-align: center;">Cancer</th>
<th style="text-align: center;">Water</th>
<th style="text-align: center;">Barley</th>
<th style="text-align: center;">Avg.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\gamma_{1}$</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.51</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma_{2}$</td>
<td style="text-align: center;">1.22</td>
<td style="text-align: center;">1.01</td>
<td style="text-align: center;">1.44</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">1.09</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">1.34</td>
<td style="text-align: center;">1.27</td>
<td style="text-align: center;">1.09</td>
</tr>
<tr>
<td style="text-align: center;">$z_{1}$</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">0.88</td>
</tr>
<tr>
<td style="text-align: center;">$z_{2}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.05</td>
</tr>
<tr>
<td style="text-align: center;">$z_{3}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.07</td>
</tr>
<tr>
<td style="text-align: center;">$P_{R \mid E}$</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.00</td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.12</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">0.05</td>
</tr>
</tbody>
</table>
<h2>VII. EXPERIMENTS</h2>
<p>We conduct experiments to address the research questions: RQ1: Can ILS-CSL enhance data-based CSL baselines and outperform the existing LLM-driven CSL method?</p>
<p>TABLE IV: The used datasets of causal DAGs.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Cancer</th>
<th>Asia</th>
<th>Child</th>
<th>Alarm</th>
<th>Insurance</th>
<th>Water</th>
<th>Mildew</th>
<th>Barley</th>
</tr>
</thead>
<tbody>
<tr>
<td>Variables</td>
<td>5</td>
<td>8</td>
<td>20</td>
<td>37</td>
<td>27</td>
<td>32</td>
<td>35</td>
<td>48</td>
</tr>
<tr>
<td>Edges</td>
<td>4</td>
<td>8</td>
<td>25</td>
<td>46</td>
<td>52</td>
<td>66</td>
<td>46</td>
<td>84</td>
</tr>
<tr>
<td>Parameters</td>
<td>10</td>
<td>18</td>
<td>230</td>
<td>509</td>
<td>1008</td>
<td>10083</td>
<td>540150</td>
<td>114005</td>
</tr>
<tr>
<td>Data size</td>
<td>250 / 1000</td>
<td>250 / 1000</td>
<td>500 / 2000</td>
<td>1000 / 4000</td>
<td>500 / 2000</td>
<td>1000 / 4000</td>
<td>8000 / 32000</td>
<td>2000 / 8000</td>
</tr>
</tbody>
</table>
<p>TABLE V: Scaled SHD $\downarrow$ comparison to data-based and LLM-driven CSL.</p>
<p>| Dataset | Cancer | | | Asia | | | Child | | | Insurance | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>TABLE VII: Scaled SHD$\downarrow$ enhancement on data-based CSL with different scores, search algorithms and approaches to apply prior constraints, by the proposed framework.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Cancer</th>
<th></th>
<th>Asia</th>
<th></th>
<th>Child</th>
<th></th>
<th>Insurance</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>N</td>
<td>250</td>
<td>1000</td>
<td>250</td>
<td>1000</td>
<td>500</td>
<td>2000</td>
<td>500</td>
<td>2000</td>
</tr>
<tr>
<td>HC-BDeu</td>
<td>0.58±0.13</td>
<td>0.33±0.26</td>
<td>0.56±0.27</td>
<td>0.23±0.17</td>
<td>0.57±0.12</td>
<td>0.49±0.18</td>
<td>0.69±0.06</td>
<td>0.68±0.09</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.50±0.22-14\%</td>
<td>0.29±0.29-12\%</td>
<td>0.46±0.33-18\%</td>
<td>0.15±0.15-35\%</td>
<td>0.24±0.07-58\%</td>
<td>0.10±0.02-80\%</td>
<td>0.45±0.06-35\%</td>
<td>0.34±0.04-50\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.50±0.22-14\%</td>
<td>0.29±0.29-12\%</td>
<td>0.44±0.30-21\%</td>
<td>0.15±0.15-35\%</td>
<td>0.26±0.06-54\%</td>
<td>0.11±0.03-78\%</td>
<td>0.50±0.08-28\%</td>
<td>0.35±0.04-49\%</td>
</tr>
<tr>
<td>MINOBSx-BDeu</td>
<td>0.75±0.22</td>
<td>0.46±0.29</td>
<td>0.52±0.32</td>
<td>0.31±0.07</td>
<td>0.38±0.08</td>
<td>0.21±0.04</td>
<td>0.46±0.05</td>
<td>0.29±0.02</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.50±0.22-33\%</td>
<td>0.29±0.29-37\%</td>
<td>0.42±0.37-19\%</td>
<td>0.15±0.15-52\%</td>
<td>0.25±0.06-34\%</td>
<td>0.07±0.03-67\%</td>
<td>0.42±0.03-9\%</td>
<td>0.28±0.06-3\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.50±0.22-33\%</td>
<td>0.29±0.29-37\%</td>
<td>0.42±0.37-19\%</td>
<td>0.15±0.15-52\%</td>
<td>0.25±0.04-34\%</td>
<td>0.08±0.04-62\%</td>
<td>0.41±0.03-11\%</td>
<td>0.26±0.04-10\%</td>
</tr>
<tr>
<td>HC-BIC</td>
<td>0.92±0.29</td>
<td>0.62±0.34</td>
<td>0.48±0.36</td>
<td>0.31±0.29</td>
<td>0.53±0.07</td>
<td>0.38±0.16</td>
<td>0.76±0.05</td>
<td>0.72±0.06</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.92±0.29+0\%</td>
<td>0.42±0.34-32\%</td>
<td>0.33±0.25-31\%</td>
<td>0.19±0.17-39\%</td>
<td>0.26±0.07-51\%</td>
<td>0.07±0.03-82\%</td>
<td>0.60±0.03-21\%</td>
<td>0.41±0.03-43\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.92±0.29+0\%</td>
<td>0.42±0.34-32\%</td>
<td>0.35±0.26-27\%</td>
<td>0.21±0.19-32\%</td>
<td>0.27±0.08-49\%</td>
<td>0.07±0.05-82\%</td>
<td>0.62±0.06-18\%</td>
<td>0.42±0.03-42\%</td>
</tr>
<tr>
<td>MINOBSx-BIC</td>
<td>1.00±0.25</td>
<td>0.62±0.21</td>
<td>0.46±0.23</td>
<td>0.27±0.05</td>
<td>0.34±0.06</td>
<td>0.18±0.04</td>
<td>0.62±0.05</td>
<td>0.55±0.05</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.92±0.29-8\%</td>
<td>0.38±0.26-39\%</td>
<td>0.42±0.40-9\%</td>
<td>0.12±0.08-56\%</td>
<td>0.24±0.08-29\%</td>
<td>0.06±0.02-67\%</td>
<td>0.55±0.03-11\%</td>
<td>0.39±0.08-29\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.92±0.29-8\%</td>
<td>0.38±0.26-39\%</td>
<td>0.35±0.26-24\%</td>
<td>0.15±0.12-44\%</td>
<td>0.25±0.05-26\%</td>
<td>0.06±0.02-67\%</td>
<td>0.55±0.03-11\%</td>
<td>0.41±0.09-25\%</td>
</tr>
<tr>
<td>Dataset</td>
<td>Alarm</td>
<td></td>
<td>Mildew</td>
<td></td>
<td>Water</td>
<td></td>
<td>Barley</td>
<td></td>
</tr>
<tr>
<td>N</td>
<td>1000</td>
<td>4000</td>
<td>8000</td>
<td>32000</td>
<td>1000</td>
<td>4000</td>
<td>2000</td>
<td>8000</td>
</tr>
<tr>
<td>HC-BDeu</td>
<td>0.65±0.12</td>
<td>0.64±0.09</td>
<td>0.79±0.11</td>
<td>0.99±0.07</td>
<td>0.76±0.07</td>
<td>0.64±0.08</td>
<td>0.80±0.06</td>
<td>0.65±0.06</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.12±0.02-82\%</td>
<td>0.08±0.01-88\%</td>
<td>0.46±0.01-42\%</td>
<td>0.22±0.02-78\%</td>
<td>0.64±0.02-16\%</td>
<td>0.55±0.03-14\%</td>
<td>0.69±0.06-14\%</td>
<td>0.57±0.06-12\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.30±0.05-54\%</td>
<td>0.25±0.06-61\%</td>
<td>0.43±0.00-46\%</td>
<td>0.47±0.04-53\%</td>
<td>0.64±0.01-16\%</td>
<td>0.56±0.03-12\%</td>
<td>0.76±0.04-5\%</td>
<td>0.62±0.03-5\%</td>
</tr>
<tr>
<td>MINOBSx-BDeu</td>
<td>0.21±0.06</td>
<td>0.14±0.04</td>
<td>0.50±0.02</td>
<td>0.46±0.05</td>
<td>0.77±0.07</td>
<td>0.61±0.04</td>
<td>0.56±0.04</td>
<td>0.40±0.03</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.09±0.03-57\%</td>
<td>0.08±0.02-83\%</td>
<td>0.43±0.00-14\%</td>
<td>0.33±0.18-28\%</td>
<td>0.68±0.05-12\%</td>
<td>0.56±0.02-8\%</td>
<td>0.54±0.02-4\%</td>
<td>0.38±0.02-5\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.09±0.02-57\%</td>
<td>0.07±0.01-50\%</td>
<td>0.47±0.01-6\%</td>
<td>0.37±0.02-20\%</td>
<td>0.68±0.04-12\%</td>
<td>0.56±0.02-8\%</td>
<td>0.55±0.03-2\%</td>
<td>0.38±0.02-5\%</td>
</tr>
<tr>
<td>HC-BIC</td>
<td>0.68±0.05</td>
<td>0.59±0.10</td>
<td>0.90±0.06</td>
<td>0.91±0.13</td>
<td>0.76±0.04</td>
<td>0.70±0.03</td>
<td>0.87±0.05</td>
<td>0.80±0.08</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.22±0.04-68\%</td>
<td>0.12±0.04-80\%</td>
<td>0.58±0.01-36\%</td>
<td>0.46±0.04-49\%</td>
<td>0.69±0.02-9\%</td>
<td>0.61±0.03-13\%</td>
<td>0.76±0.02-13\%</td>
<td>0.69±0.06-14\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.41±0.04-40\%</td>
<td>0.35±0.11-41\%</td>
<td>0.71±0.01-21\%</td>
<td>0.57±0.02-37\%</td>
<td>0.69±0.02-9\%</td>
<td>0.61±0.03-13\%</td>
<td>0.82±0.04-6\%</td>
<td>0.74±0.09-8\%</td>
</tr>
<tr>
<td>MINOBSx-BIC</td>
<td>0.32±0.08</td>
<td>0.15±0.04</td>
<td>0.74±0.01</td>
<td>0.73±0.09</td>
<td>0.82±0.03</td>
<td>0.77±0.03</td>
<td>0.79±0.04</td>
<td>0.58±0.03</td>
</tr>
<tr>
<td>+ILS-CSL-hard</td>
<td>0.16±0.07-50\%</td>
<td>0.09±0.03-40\%</td>
<td>0.58±0.01-22\%</td>
<td>0.45±0.03-38\%</td>
<td>0.69±0.03-16\%</td>
<td>0.62±0.01-19\%</td>
<td>0.73±0.03-8\%</td>
<td>0.55±0.03-5\%</td>
</tr>
<tr>
<td>+ILS-CSL-soft</td>
<td>0.19±0.06-41\%</td>
<td>0.10±0.01-33\%</td>
<td>0.73±0.01-1\%</td>
<td>0.64±0.04-12\%</td>
<td>0.70±0.02-15\%</td>
<td>0.64±0.02-17\%</td>
<td>0.76±0.02-4\%</td>
<td>0.56±0.03-3\%</td>
</tr>
</tbody>
</table>
<p>TABLE VIII: Ranking of methods in Table VII.</p>
<p>| BDeu | | | | | BIC | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | |</p>
<p>TABLE IX: The precision along with ratio of different structures of different answers by GPT-4.</p>
<table>
<thead>
<tr>
<th>Answer</th>
<th>Dataset</th>
<th>Direct edges</th>
<th>Reversed edges</th>
<th>Precision</th>
<th>Indirect paths</th>
<th>Reversed indirect paths</th>
<th>Not reachable</th>
<th>Overall</th>
<th>Precision</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>Qualitative</td>
<td>Structural</td>
</tr>
<tr>
<td>A</td>
<td>Alarm</td>
<td>0.33</td>
<td>0.02</td>
<td>0.94</td>
<td>0.28</td>
<td>0.00</td>
<td>0.37</td>
<td>0.61</td>
<td>0.33</td>
</tr>
<tr>
<td></td>
<td>Asia</td>
<td>0.44</td>
<td>0.00</td>
<td>1.00</td>
<td>0.50</td>
<td>0.00</td>
<td>0.06</td>
<td>0.94</td>
<td>0.44</td>
</tr>
<tr>
<td></td>
<td>Barley</td>
<td>0.22</td>
<td>0.12</td>
<td>0.65</td>
<td>0.23</td>
<td>0.12</td>
<td>0.31</td>
<td>0.45</td>
<td>0.22</td>
</tr>
<tr>
<td></td>
<td>Cancer</td>
<td>0.36</td>
<td>0.09</td>
<td>0.80</td>
<td>0.36</td>
<td>0.09</td>
<td>0.09</td>
<td>0.73</td>
<td>0.36</td>
</tr>
<tr>
<td></td>
<td>Child</td>
<td>0.46</td>
<td>0.02</td>
<td>0.96</td>
<td>0.26</td>
<td>0.04</td>
<td>0.22</td>
<td>0.72</td>
<td>0.46</td>
</tr>
<tr>
<td></td>
<td>Insurance</td>
<td>0.41</td>
<td>0.05</td>
<td>0.89</td>
<td>0.32</td>
<td>0.06</td>
<td>0.15</td>
<td>0.74</td>
<td>0.41</td>
</tr>
<tr>
<td></td>
<td>Mildew</td>
<td>0.45</td>
<td>0.04</td>
<td>0.92</td>
<td>0.36</td>
<td>0.03</td>
<td>0.11</td>
<td>0.82</td>
<td>0.45</td>
</tr>
<tr>
<td></td>
<td>Water</td>
<td>0.47</td>
<td>0.13</td>
<td>0.78</td>
<td>0.11</td>
<td>0.01</td>
<td>0.28</td>
<td>0.58</td>
<td>0.47</td>
</tr>
<tr>
<td>B</td>
<td>Alarm</td>
<td>0.02</td>
<td>0.36</td>
<td>0.95</td>
<td>0.10</td>
<td>0.18</td>
<td>0.34</td>
<td>0.54</td>
<td>0.36</td>
</tr>
<tr>
<td></td>
<td>Asia</td>
<td>0.00</td>
<td>0.50</td>
<td>1.00</td>
<td>0.00</td>
<td>0.36</td>
<td>0.14</td>
<td>0.86</td>
<td>0.50</td>
</tr>
<tr>
<td></td>
<td>Barley</td>
<td>0.02</td>
<td>0.21</td>
<td>0.91</td>
<td>0.08</td>
<td>0.43</td>
<td>0.25</td>
<td>0.64</td>
<td>0.21</td>
</tr>
<tr>
<td></td>
<td>Cancer</td>
<td>0.00</td>
<td>0.60</td>
<td>1.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.40</td>
<td>0.60</td>
<td>0.60</td>
</tr>
<tr>
<td></td>
<td>Child</td>
<td>0.00</td>
<td>0.45</td>
<td>1.00</td>
<td>0.24</td>
<td>0.12</td>
<td>0.18</td>
<td>0.58</td>
<td>0.45</td>
</tr>
<tr>
<td></td>
<td>Insurance</td>
<td>0.02</td>
<td>0.59</td>
<td>0.97</td>
<td>0.02</td>
<td>0.10</td>
<td>0.27</td>
<td>0.68</td>
<td>0.59</td>
</tr>
<tr>
<td></td>
<td>Mildew</td>
<td>0.01</td>
<td>0.49</td>
<td>0.98</td>
<td>0.00</td>
<td>0.14</td>
<td>0.35</td>
<td>0.64</td>
<td>0.49</td>
</tr>
<tr>
<td></td>
<td>Water</td>
<td>0.03</td>
<td>0.51</td>
<td>0.94</td>
<td>0.29</td>
<td>0.03</td>
<td>0.14</td>
<td>0.54</td>
<td>0.51</td>
</tr>
<tr>
<td>C</td>
<td>Alarm</td>
<td>0.00</td>
<td>0.00</td>
<td>-</td>
<td>0.00</td>
<td>0.03</td>
<td>0.97</td>
<td>0.97</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Asia</td>
<td>0.00</td>
<td>0.00</td>
<td>-</td>
<td>0.00</td>
<td>0.00</td>
<td>1.00</td>
<td>1.00</td>
<td>1.00</td>
</tr>
<tr>
<td></td>
<td>Barley</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Cancer</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>Child</td>
<td>0.00</td>
<td>0.11</td>
<td>-</td>
<td>0.00</td>
<td>0.11</td>
<td>0.79</td>
<td>0.79</td>
<td>0.89</td>
</tr>
<tr>
<td></td>
<td>Insurance</td>
<td>0.03</td>
<td>0.05</td>
<td>-</td>
<td>0.00</td>
<td>0.10</td>
<td>0.83</td>
<td>0.83</td>
<td>0.93</td>
</tr>
<tr>
<td></td>
<td>Mildew</td>
<td>0.00</td>
<td>0.01</td>
<td>-</td>
<td>0.32</td>
<td>0.36</td>
<td>0.32</td>
<td>0.32</td>
<td>0.99</td>
</tr>
<tr>
<td></td>
<td>Water</td>
<td>0.00</td>
<td>0.04</td>
<td>-</td>
<td>0.30</td>
<td>0.19</td>
<td>0.47</td>
<td>0.47</td>
<td>0.96</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: Erroneous LLM inference and erroneous specified edge constraints of MINOBSx-BDeu+ILS-CSL-hard.</p>
<p>ing ILS-CSL's broad applicability and effectiveness in improving causal discovery outcomes.</p>
<ol>
<li>The hard approach outperforms the soft approach, attributed to the high quality of specified constraints within ILS-CSL. This stands in stark contrast to the findings by [15], where the soft approach fared better due to the lower quality of prior constraints.</li>
</ol>
<h1><em>E. Errors in LLM Inference and Prior Constraints (RQ3)</em></h1>
<p>This section is dedicated to the evaluation of ILS-CSL's robustness against the inaccuracies in LLM inference. We scrutinize the erroneous causal relationships inferred by LLM on the edges of the learned DAG, along with the incorrect prior constraints that stem from them. The results pertaining to each dataset, which includes two unique sizes of observed data related to MINOBSx-BDeu with the hard approach, are illustrated in Figure 2. For a more comprehensive set of results, refer to the external repository.</p>
<p>Our observations highlight a substantial reduction in the errors of specified edge constraints compared to erroneous LLM inference. This reduction stems from the strategy of only imposing constraints on causality that is inconsistent with what has been learned. A more detailed analysis on the superior aspect of ILS-CSL to reduce erroneous constraints is made in the following experiment.</p>
<h1><em>F. Why Resistant to Imperfect LLM Inference (RQ3)</em></h1>
<p>This section elucidates the ability of ILS-CSL to minimize prior errors by limiting LLM supervision to edges. We present the ratio of various real structures corresponding to all pairwise variables inferred by GPT-4. Table IX displays the results for all datasets, highlighting the precision related to ILS-CSL (light red cells) and full inference (light blue cells). It distinguishes between qualitative precision (correct paths) and structural precision (correct edges only).</p>
<p>In the context of the analysis, the outcomes A, B, and C from GPT-4 have specific meanings related to inferred causal relationships between two variables <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub>:</p>
<ul>
<li><strong>Outcome A:</strong> GPT-4 infers that <em>X</em><sub>1</sub> causes <em>X</em><sub>2</sub> (<em>X</em><sub>1</sub> → <em>X</em><sub>2</sub>).</li>
<li><strong>Outcome B:</strong> GPT-4 infers that <em>X</em><sub>2</sub> causes <em>X</em><sub>1</sub> (<em>X</em><sub>2</sub> → <em>X</em><sub>1</sub>).</li>
<li><strong>Outcome C:</strong> GPT-4 infers that <em>X</em><sub>1</sub> and <em>X</em><sub>2</sub> are not causally related (<em>X</em><sub>1</sub> → <em>X</em><sub>2</sub>).</li>
</ul>
<p>In the table, various columns represent the ratio of different corresponding structures in the ground truth:</p>
<ul>
<li><strong>Direct Edges:</strong> The edge (<em>X</em><sub>1</sub> → <em>X</em><sub>2</sub>) exists in truth.</li>
<li><strong>Reversed Edges:</strong> An reversed edge (<em>X</em><sub>2</sub> → <em>X</em><sub>1</sub>) exists in truth.</li>
<li><strong>Indirect Paths:</strong> A path (<em>X</em><sub>1</sub> → <em>X</em><sub>2</sub>) exists, but (<em>X</em><sub>1</sub> → <em>X</em><sub>2</sub>).</li>
</ul>
<p>Reversed Indirect Paths: $\left(X_{2} \rightsquigarrow X_{1}\right)$, but $\left(X_{2} \rightsquigarrow X_{1}\right)$. Not Reachable: $\left(X_{1} \nrightarrow X_{2}, X_{2} \nrightarrow X_{1}\right)$.</p>
<p>The precision of LLM on variables that have edges (light red cells of answers A and B) is notably high, significantly exceeding the precision on variables that may not. Analyzing prior errors in ILS-CSL reveals:</p>
<p>1) For GPT-4 outcome C, the corresponding edge forbidden constraints exhibit high precision, generating few erroneous structural constraints. This is attributed to the high confidence in the absence of causal relations inferred based on knowledge, leading to excellent precision on pairwise variables without structural edges, albeit with a lower recall.
2) For GPT-4 outcomes A or B, high precision is observed on learned edges belonging to the true skeleton, producing few erroneous structural constraints. Given known direct causality between pairwise variables, LLM can easily infer the correct causal direction, stemming from the counterintuitive nature of reversed causal statements.
3) Major LLM inference errors stem from outcomes A and B on learned edges outside the true skeleton. However, the impact of these errors on generating incorrect structural constraints is mitigated by the low probability of extra edges occurring in a learned structure ( $z_{3} \approx 0.07$, see Table III) and the strategy of specifying a prior constraint only when inconsistent.</p>
<p>In essence, the primary limitation of LLM in causal inference is the confusion between direct causal relationships, indirect causality, and correlations, evidenced by the low overall qualitative and structural precision. This limitation hampers the performance of using LLM-derived existence on causality as ancestral (qualitative precision) or edge constraints (structural precision) seperately.</p>
<p>Contrarily, ILS-CSL effectively minimizes prior errors by leveraging the inherent precision of LLM in inferring noncausal relations and determining causal direction on pairwise variables with direct causality. It smartly circumvents LLM's limitation in discerning the existence of direct causal relationships, which are easily confused with indirect causality or correlations, by restricting the LLM inference into the range of learned structures from data, as analyzed in point 3.</p>
<h2>G. Trend of DAG Quality over Iterations (RQ4)</h2>
<p>This section outlines the iterative trends of scaled SHD (aiming for a decrease, denoted as $\mathrm{SHD} \downarrow$ ) and True Positive Rate (aiming for an increase, denoted as TPR $\uparrow$ ) for various backbone algorithms across eight datasets. Each dataset spans two distinct data sizes, resulting in 12 segments of observed data. It's crucial to note the potential for significant derivation due to performance differences across varying data sizes, particularly for smaller-scale datasets like Cancer and Asia. The results of HC+BIC+ILS-CSL-hard on various datasets are reported in Figure 3, with comprehensive results available in the external repository. Key observations from the iterative trends include:
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Trend of TPR $\uparrow$ (green line) and scaled $\mathrm{SHD} \downarrow$ (purple line) of HC+BIC+ILS-CSL-hard on various datasets.</p>
<ul>
<li>Limited Iteration Numbers: Most cases require a limited number of iterations. The area near the maximum iteration in each figure is small when exceeding 4, indicating that few out of the 12 cases reach this point. Some cases even have a derivation of zero at the maximum iteration, signifying that only one case attains this maximum value.</li>
<li>Quality Improvement Trend: Generally, as the iteration number increases, the scaled SHD decreases, and the TPR increases. This trend underscores the enhancement in the quality of the learned causal structures as ILS-CSL progresses.</li>
<li>Significant Initial Improvement: The most substantial improvement in the quality of learned causal DAGs occurs in the first round of LLM supervision (from Iteration 1 to 2). Subsequent iterations offer diminished enhancements. This pattern is attributed to the initial presentation of most inconsistent edges with LLM inference in the first iteration. Post the integration of prior constraints, the new structures learned by CSL exhibit far fewer inconsistencies with LLM inference.</li>
<li>Potential Quality Degradation: In certain instances, the quality of the causal DAG diminishes across specific iterations. This decline could stem from the introduction of new erroneous prior constraints in a given iteration or a statistical artifact. The latter scenario arises when two consecutive iterations do not employ the same set of observed data, as some cases conclude in the preceding iteration.
These observations provide a comprehensive insight into the iterative behavior of ILS-CSL, highlighting its effectiveness and areas of caution to ensure consistent enhancement in learned causal structures.</li>
</ul>
<h2>H. Illustrative Example of DAG Evolution (RQ4)</h2>
<p>We visualize the learned causal structures in iterations to unfold the details of ILS-CSL. An illustrative example by HC</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: Visualized process of HC-BDeu+ILS-CSL-hard on a set of observed data of Child, 2000 samples. The SHD of iterations are: 12 for Iteration 0, 3 for Iterations 1 and 2.</p>
<p>(BDeu) algorithm on Child dataset, 2000 samples, with hard constraining approach in ILS-CSL, is reported in Figure 4.</p>
<p>Initially, HC (BDue) learns a causal DAG from pure observed data (Iteration 0), whose edges are supervised by LLM, leading to edge constraints (colored arrows) on inconsistent inferred edge by LLM. The constraints could refine local structures (red arrows) or bring harm due to the erroneous inference (blue arrows). The erroneous edges (dotted arrows) are reduced as the iteration goes. Details of further observations are presented as follows:</p>
<ul>
<li>The SHD of the learned causal DAG is greatly reduced from 12 to 3 by employing the ILS-CSL framework, showcasing the significant capability of our framework to enhance the quality of learned causality.</li>
<li>The first round of LLM-based supervision refines the learned DAG to a much greater extent than the following rounds. This addresses the acceptable efficiency loss of ILS-CSL, which usually does not require many iterations.</li>
<li>There are 7 correct constraints (red arrow) and 2 erroneous ones (blue arrow) in total. The number of directly corrected edges by these priors is 7 − 2 = 5, while the reduced SHD is 8, meaning that 3 edges that are distinct from those in constraints are corrected without any prior knowledge on them. It underscores the capability of discovering structures unrelated to prior constraints by integrating them. This phenomenon could be interpreted as the capability of aiding discovery of unknown causal mechanisms by the known causal knowledge.</li>
</ul>
<h2>VIII. CONCLUSIONS</h2>
<p>This paper presents ILS-CSL, a framework that enhances causal discovery from data using Large Language Models (LLMs). ILS-CSL seamlessly incorporates LLM inference on the edges of the learned causal Directed Acyclic Graph (DAG), converting qualitative causal statements into precise edge-level prior constraints while effectively mitigating constraint errors stemming from imperfect prior knowledge. Comprehensive experiments across eight real-world datasets demonstrate the substantial and consistent improvement ILS-CSL brings to the quality of causal structure learning (CSL) outputs. Notably, ILS-CSL surpasses the existing separate way to guide CSL by applying LLM inferred causality as ancestral constraints, with a marked performance increase as the number of variables grows. This advancement underscores the promising application of the ILS-CSL framework in assistance of complex, real-world causal discovery tasks.</p>
<h2>REFERENCES</h2>
<p>[1] J. Pearl, Causality. Cambridge university press, 2009.
[2] B. Ellis and W. H. Wong, "Learning causal bayesian network structures from experimental data," Journal of the American Statistical Association, vol. 103, no. 482, pp. 778-789, 2008.
[3] D. M. Chickering, "Learning bayesian networks is np-complete," Learning from data: Artificial intelligence and statistics V, pp. 121-130, 1996.
[4] N. K. Kitson, A. C. Constantinou, Z. Guo, Y. Liu, and K. Chobtham, "A survey of bayesian network structure learning," Artificial Intelligence Review, pp. 1-94, 2023.
[5] S. L. Morgan and C. Winship, Counterfactuals and causal inference. Cambridge University Press, 2015.
[6] D. M. Chickering, "Optimal structure identification with greedy search," Journal of machine learning research, vol. 3, no. Nov, pp. 507-554, 2002.
[7] E. Y.-J. Chen, Y. Shen, A. Choi, and A. Darwiche, "Learning bayesian networks with ancestral constraints," Advances in Neural Information Processing Systems, vol. 29, 2016.
[8] H. Amirkhani, M. Rahmati, P. J. Lucas, and A. Hommersom, "Exploiting experts' knowledge for structure learning of bayesian networks," IEEE transactions on pattern analysis and machine intelligence, vol. 39, no. 11, pp. 2154-2170, 2016.
[9] A. C. Constantinou, Z. Guo, and N. K. Kitson, "The impact of prior knowledge on causal structure learning," Knowledge and Information Systems, pp. 1-50, 2023.
[10] E. Kıcıman, R. Ness, A. Sharma, and C. Tan, "Causal reasoning and large language models: Opening a new frontier for causality," arXiv preprint arXiv:2305.00050, 2023.
[11] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, "Capabilities of gpt-4 on medical challenge problems," arXiv preprint arXiv:2303.13375, 2023.
[12] L. Chen, T. Ban, X. Wang, D. Lyu, and H. Chen, "Mitigating prior errors in causal structure learning: Towards llm driven prior knowledge," arXiv preprint arXiv:2306.07032, 2023.
[13] R. Tu, C. Ma, and C. Zhang, "Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis," arXiv preprint arXiv:2301.13819, 2023.
[14] S. Long, T. Schuster, A. Piché, S. Research et al., "Can large language models build causal graphs?" arXiv preprint arXiv:2303.05279, 2023.
[15] T. Ban, L. Chen, X. Wang, and H. Chen, "From query tools to causal architects: Harnessing large language models for advanced causal discovery from data," arXiv preprint arXiv:2306.16902, 2023.
[16] A. Vashishtha, A. G. Reddy, A. Kumar, S. Bachu, V. N. Balasubramanian, and A. Sharma, "Causal inference using llm-guided discovery," arXiv preprint arXiv:2310.15117, 2023.
[17] M. Willig, M. Zečević, D. S. Dhami, and K. Kersting, "Can foundation models talk causality?" arXiv preprint arXiv:2206.10591, 2022.
[18] H. Liu, R. Ning, Z. Teng, J. Liu, Q. Zhou, and Y. Zhang, "Evaluating the logical reasoning ability of chatgpt and gpt-4," arXiv preprint arXiv:2304.03439, 2023.
[19] P. Hoyer, D. Janzing, J. M. Mooij, J. Peters, and B. Schölkopf, "Nonlinear causal discovery with additive noise models," Advances in neural information processing systems, vol. 21, 2008.
[20] J. Frohberg and F. Binder, "Crass: A novel data set and benchmark to test counterfactual reasoning of large language models," arXiv preprint arXiv:2112.11941, 2021.
[21] D. Heckerman, "A bayesian approach to learning causal networks," arXiv preprint arXiv:1302.4958, 2013.
[22] P. Spirtes and C. Glymour, "An algorithm for fast recovery of sparse causal graphs," Social science computer review, vol. 9, no. 1, pp. 6272, 1991.
[23] E. V. Strobl, S. Visweswaran, and P. L. Spirtes, "Fast causal inference with non-random missingness by test-wise deletion," International journal of data science and analytics, vol. 6, pp. 47-62, 2018.
[24] D. Heckerman and D. Geiger, "Learning bayesian networks: a unification for discrete and gaussian domains," in Proceedings of the 11th Conference on Uncertainty in Artificial Intelligence, 1995, pp. 274-284.
[25] A. A. Neath and J. E. Cavanaugh, "The bayesian information criterion: background, derivation, and applications," Wiley Interdisciplinary Reviews: Computational Statistics, vol. 4, no. 2, pp. 199-203, 2012.
[26] C. Yuan, B. Malone, and X. Wu, "Learning optimal bayesian networks using a* search," in Twenty-second international joint conference on artificial intelligence, 2011.
[27] F. Trösser, S. de Givry, and G. Katsirelos, "Improved acyclicity reasoning for bayesian network structure learning with constraint programming," arXiv preprint arXiv:2106.12269, 2021.
[28] A. Li and P. Beek, "Bayesian network structure learning with side constraints," in International Conference on Probabilistic Graphical Models. PMLR, 2018, pp. 225-236.
[29] L. M. de Campos and J. G. Castellano, "Bayesian network learning algorithms using structural restrictions," International Journal of Approximate Reasoning, vol. 45, no. 2, pp. 233-254, 2007.
[30] G. F. Cooper and E. Herskovits, "A bayesian method for the induction of probabilistic networks from data," Machine learning, vol. 9, pp. 309347, 1992.
[31] R. T. O’Donnell, A. E. Nicholson, B. Han, K. B. Korb, M. J. Alam, and L. R. Hope, "Causal discovery with prior information," in AI 2006: Advances in Artificial Intelligence: 19th Australian Joint Conference on Artificial Intelligence, Hobart, Australia, December 4-8, 2006. Proceedings 19. Springer, 2006, pp. 1162-1167.
[32] J. A. Gámez, J. L. Mateo, and J. M. Puerta, "Learning bayesian networks by hill climbing: efficient methods based on progressive restriction of the neighborhood," Data Mining and Knowledge Discovery, vol. 22, pp. 106-148, 2011.
[33] C. Lee and P. van Beek, "Metaheuristics for score-and-search bayesian network structure learning," in Advances in Artificial Intelligence: 30th Canadian Conference on Artificial Intelligence, Canadian AI 2017, Edmonton, AB, Canada, May 16-19, 2017, Proceedings 30. Springer, 2017, pp. 129-141.
[34] M. Scutari, C. E. Graafland, and J. M. Gutiérrez, "Who learns better bayesian network structures: Accuracy and speed of structure learning algorithms," International Journal of Approximate Reasoning, vol. 115, pp. 235-253, 2019.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ When an ancestral constraint is correctly identified, CSL might still recover a path that includes erroneous edges. In contrast, specifying the existence of an edge directly ensures accuracy, as it cannot be misinterpreted.
${ }^{3}$ An incorrect ancestral constraint inevitably introduces at least one erroneous edge.
${ }^{4}$ Given that the causal DAG is usually sparse, the number of edges $|E|$ is typically estimated as $O(N)$.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4630 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4630</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4630</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-2a7ae3e98357569c41424dacd60c62d3df78a0db</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2a7ae3e98357569c41424dacd60c62d3df78a0db" target="_blank">Limitations of Language Models in Arithmetic and Symbolic Induction</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Surprisingly, large pretrained Language Models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition, when the total number of symbols or repeating symbols increases, the model performance drops quickly.</p>
                <p><strong>Paper Abstract:</strong> Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models. However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver 100% accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4630.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4630.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 (text-davinci-002 used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive large pretrained Transformer used in few-shot prompting experiments; evaluated on copying, reversing, and multi-digit addition with and without intermediate steps, callable programs, and tutor-style demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 (text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Autoregressive Transformer (GPT-3 family); used via few-shot prompting (prompt examples of 4 for addition, 4 for copy/reverse); temperature=0, top_p=1 in experiments. Model size not specified in paper but refers to GPT-3 as in Brown et al. (2020).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Copying, reversing lists, multi-digit addition (1–30+ digits), multi-step addition with carry (per-digit addition with carries).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Performs arithmetic largely via learned pattern translation/mapping from input sequence to output, improved by chain-of-thought / scratchpad style intermediate steps; struggles when the model must reliably 'locate' which input token to copy (repeating symbols) suggesting pattern-matching rather than explicit index-tracking. Few-shot chain-of-thought gives more computation time and decomposes tasks enabling better induction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Prompted GPT-3's performance improves substantially when provided fine-grained intermediate steps (scratchpad) and positional markers; callable-program prompts (where primitive ops are invoked) boost OOD generalization; explicit tutor-style step-by-step action sequences give 100% accuracy, implying that decomposing tasks into explicit low-level actions simplifies the hypothesis space and yields correct execution.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Failures on repeating-digit inputs, long-digit OOD examples, and reversing tasks (poor locating when distances increase) show that the model is not reliably implementing an index-aware algorithm; callable-program experiments show correct primitive outputs but GPT-3 still sometimes chooses wrong tokens to call (appendix error), demonstrating remaining locating/copying errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Copying: 100% in-distribution (1–5 digits) for few-shot GPT-3; better OOD than finetuned T5 but generalization degrades beyond ~30 digits (does not generalize well beyond 30 digits for some tasks); GPT-3 can generalize to 80 digits on copying random numbers in some settings (Figure 2) but fails on repeated-symbol OOD; Addition: few-shot GPT-3 accuracy increases with fine-grained steps and positional markers; GPT-3 + callable programs improves OOD but still degrades as digits increase; GPT-3 + tutor achieves 100% accuracy across the experimented ranges. Exact numeric curves given in paper figures; specific examples: GPT-3 achieves 100% in-distribution copy (1–5 digits) and 100% with tutor across OOD tests.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions tried: (1) add explicit positional markers (ordered or random) in prompts — improved locating and OOD in some GPT-3 settings; (2) provide fine-grained scratchpad steps — improved both in-distribution and OOD (notably for GPT-3); (3) callable-program prompts — primitive operations executed externally returned correct results but model still mis-selects inputs to call in some examples; (4) tutor-style action sequences — yields perfect generalization. No neuron-level probes reported.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Fails on inputs with repeating symbols (difficulty differentiating identical tokens), length OOD (overfitting to training-length patterns), and reversing (increasing source-target distances). Callable-program setting failure mode: incorrect selection of which tokens to feed into correct primitives. Fine-tuning and scratchpad help but do not eliminate OOD failures unless tutor-style action sequences are used.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>GPT-3 generally outperforms finetuned T5 on OOD for some settings; GPT-3 benefits more from fine-grained steps and callable-program prompting than T5 fine-tuning; GPT-3 + tutor achieves similar perfect generalization as T5+Tutor and outperforms DeBERTa and T5 baselines on OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of Language Models in Arithmetic and Symbolic Induction', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4630.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4630.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5-base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5 (base) fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Text-to-text Transformer (T5-base) fine-tuned on synthetic datasets for copy, reverse, and addition tasks; experiments examine effects of explicit positional markers, fine-grained steps, callable programs, and tutor demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>T5-base (220M) fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer (T5-base ~220M parameters) initialized from pretrained weights and fine-tuned on task-specific synthetic datasets (copy: 2,000 examples up to 5 digits; addition: 1,000 examples 1–5 digits; training hyperparams reported in appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Copying, reversing lists, multi-digit addition (1–30 digits tested), with and without intermediate steps and positional markers.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Learns mapping heuristics from training distribution; explicit positional markers help by breaking repeating-symbol symmetry, enabling token-specific copying; fine-grained steps can help but fine-tuning tends to overfit to training-length patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>T5 with explicit ordered/random positional markers achieves 100% in-distribution accuracy on copy and improves OOD generalization relative to vanilla T5; fine-grained scratchpad steps improved in-distribution but less OOD gain compared to GPT-3 prompting. T5+Tutor also achieves 100% generalization when trained with tutor action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>T5 without positional markers achieves poor accuracy on repeating-symbol in-distribution examples (reported 78% on 5-digit repeating numbers) and fails on OOD lengths beyond training; fine-grained steps via fine-tuning can overfit and not provide robust OOD improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Copying: fine-tuned T5 baseline ~78% on 5-digit repeating numbers (in-distribution). T5 + explicit positional markers: 100% in-distribution. OOD: T5+markers generalize significantly better than baseline but still worse than tutor; addition: T5 fine-tuned often achieves high in-distribution but poor OOD (figures show steep accuracy drop past training length); T5+Tutor achieves 100% across tested lengths (per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions: explicit positional markers (ordered/random) added to inputs — large in-distribution gains and improved OOD; fine-grained steps (scratchpad) included in targets — improved in-distribution but smaller OOD gains; callable-program style not reported to fully solve locating/copying errors for T5. Training behavior: T5 fine-tuning prone to overfitting (worse OOD) relative to prompting GPT-3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Overfitting to training lengths (fails to generalize to longer numbers), difficulty with repeating symbols unless explicit markers are used, limited OOD generalization even with large training sets, limited improvements from scratchpad when fine-tuning (compared with prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>T5 lags behind GPT-3 in OOD generalization when both are given fine-grained steps; T5 with explicit positional markers can match in-distribution performance of DeBERTa but DeBERTa exhibits stronger overfitting characteristics (near perfect in-distribution but 0% OOD in some tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of Language Models in Arithmetic and Symbolic Induction', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4630.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4630.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeBERTa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeBERTa (disentangled attention) base</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Transformer model using disentangled relative position embeddings (implicit positional markers); evaluated on symbolic tasks to probe whether implicit position representations improve locating and induction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeBERTa-base (140M) fine-tuned</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-style Transformer with disentangled content and position attention (relative position embeddings). Fine-tuned on synthetic copy/add/reverse data (2,000 examples up to 5 tokens for some experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Copying, reversing lists, multi-digit addition (1–30 digits tested) with/without fine-grained steps.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Disentangled relative position embeddings act as implicit positional markers, enabling the attention mechanism to incorporate relative position signals into content attention which should help locating tokens among repeating symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>DeBERTa achieves near 100% in-distribution accuracy on copying and addition tasks (improved locating) indicating the relative position embeddings aid in in-domain locating.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>DeBERTa shows severe overfitting and fails catastrophically on OOD: near 100% in-distribution but ~0% OOD accuracy for some tasks (addition, copying), indicating implicit positional embeddings as implemented do not generalize to longer sequences or OOD repetition patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Near 100% in-distribution accuracy on copy/add tasks (1–5 digits) but ~0% OOD accuracy on longer-digit tests (e.g., addition OOD results showed 0% accuracy in paper figures and text).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Tried fine-grained steps augmentations; DeBERTa + fine-grained steps still overfit quickly (e.g., achieves 100% in-distribution within few thousand steps but OOD fails). Authors diagnose that DeBERTa's implicit position capacity increases in-distribution performance but harms OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Severe overfitting to training distribution (lengths) despite improved in-distribution locating; poor OOD generalization (0% OOD in some settings). Implicit positional encodings without pretraining on targeted symbolic tasks may be insufficient for robust induction.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>DeBERTa outperforms vanilla T5 on in-distribution locating tasks but underperforms on OOD generalization; explicit positional markers on T5 sometimes yield better OOD generalization than DeBERTa.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of Language Models in Arithmetic and Symbolic Induction', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4630.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4630.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Positional markers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Positional markers (explicit ordered/random and implicit via disentangled attention)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to disambiguate repeating tokens by tagging tokens with explicit markers (ordered or random) or relying on implicit relative position embeddings (DeBERTa) so the model can locate specific instances of identical symbols.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based LMs (T5, DeBERTa, GPT-3) with positional marker interventions</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Explicit ordered markers: inject tokens like A, B, C between digits (A 2 B 2 C 2); explicit random markers: inject arbitrary marker tokens; implicit markers: relative/disentangled position embeddings (DeBERTa). Used as input augmentations in fine-tuning or prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Copying, reversing, and addition of repeated-symbol and long sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Tagging identical symbols with unique markers breaks symmetry so the model can learn unique mappings for each token instance, effectively converting a repetition problem into a non-repeating sequence where standard attention can track identity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>T5 + explicit markers achieves 100% in-distribution copying and improves OOD generalization relative to unmarked T5; DeBERTa's implicit disentangled position embeddings produce similar in-distribution gains (near 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>DeBERTa's implicit markers still fail OOD (0% accuracy); explicit positional markers improve but do not universally solve OOD generalization — models can still fail on longer-than-trained sequences or pattern types not seen during training unless tutor-style action sequences are used.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>T5 + ordered/random marker: 100% in-distribution copying; improved OOD relative to baseline (figures show much higher OOD curves than unmarked T5). DeBERTa (implicit): near 100% in-distribution but poor OOD. Exact numeric curves in paper figures.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Intervention = add markers to training/prompt. Results show markers reduce repeating-symbol errors and improve locating, but interaction with model pretraining and fine-tuning dynamics matters (DeBERTa overfits).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Markers require choice/design and may not generalize to unseen lengths/patterns; ordering/randomization matters; markers do not address algorithmic carry/iteration induction and do not alone guarantee OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Explicit markers on T5 often yield better OOD than DeBERTa's implicit markers; GPT-3 benefits from markers in prompting but still has length limits without tutor.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of Language Models in Arithmetic and Symbolic Induction', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4630.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4630.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fine-grained steps (Scratchpad)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fine-grained computation steps / Scratchpad</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Provide intermediate, per-step computations (chain-of-thought / scratchpad-style) as supervised targets or few-shot exemplars to decompose multi-digit arithmetic into sequences of elementary actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Applied to GPT-3 prompting and T5/DeBERTa fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Training or prompting examples explicitly include step-by-step subtasks (convert digits, per-digit add with carry, combine results). Variants tested: coarse-grained vs fine-grained (more granular steps).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition decomposed into per-digit additions with carry; used also for reverse and copy by giving intermediate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Decomposing tasks into smaller steps reduces per-step hypothesis complexity so the model can learn local transition rules (e.g., add single digits + carry) which helps induction and length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GPT-3 shows substantial accuracy improvements (both in-distribution and OOD) when given finer-grained steps versus coarse steps; T5 shows in-distribution gains but less OOD improvement, likely due to fine-tuning overfitting.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Even extensive scratchpad fine-tuning (100k examples reported in literature) might not produce 100% OOD performance; T5 fine-tuning with scratchpad still can fail OOD; scratchpad alone does not solve repeating-symbol locating issues.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Finer-grained steps yield larger improvements: GPT-3 with fine-grained steps outperforms coarse-grained examples (figures show monotonic gains), but numeric values vary across figures. Scratchpad-based fine-tuning on 1B models in prior work failed to reach 100% OOD; in this paper GPT-3 + fine-grained steps helps but does not match full tutor performance.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions: adding more granular intermediate steps to the exemplar/prompt or fine-tuning targets. For GPT-3, granularity correlates with improved OOD; for T5, granularity helps in-distribution but fine-tuning can overfit.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Doesn't by itself fix locating/copying of repeated symbols; susceptible to overfitting when used in fine-tuning; may require many examples to approach robust induction; still falls short of perfect generalization without tutor-style action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>GPT-3 benefits more from scratchpad prompting than T5 does from scratchpad fine-tuning (GPT-3 prompting generalizes better), and both are outperformed by tutor-style methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of Language Models in Arithmetic and Symbolic Induction', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4630.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4630.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Callable programs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM with callable programs (external primitive execution)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hybrid approach where the LM generates high-level actions or function-call tokens (e.g., add(1,5)), and an external deterministic function is invoked to compute primitive operations and return exact results back to the LM's context.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based LMs (GPT-3, T5) orchestrating callable primitives</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompting/fine-tuning format instructs the LM to generate calls to functions (convert, add, combine). The function outputs are deterministically returned and appended to the generated solution stream for subsequent steps.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Multi-digit addition with per-digit add primitives, copy/split/combine primitives.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Delegating primitive symbolic operations to exact callable programs removes the model's need to implement low-level arithmetic correctly; the LM's role becomes chaining/composing primitives — i.e., learning an algorithm sketch for when to call which primitive.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>GPT-3 + callable programs shows substantially better OOD generalization than pure LM-only prompting/fine-tuning because primitive results are exact. The paper shows callable programs improve robustness on addition tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>LM still must decide which tokens/arguments to pass into primitives (locating); authors provide an error example where the LM calls add with the wrong token (appendix A.1), causing incorrect end-to-end output despite correct primitive implementation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Callable-program prompting for GPT-3 improved OOD addition accuracy markedly compared to non-callable baselines, but accuracy still degrades as digit count increases (figures). Not perfect; numeric degradation shown in paper plots. Exact numbers vary by digit-length.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Intervention: replace primitive ops (add, convert, combine) with external deterministic functions during generation. Observed that primitive correctness is guaranteed but errors remain due to incorrect argument selection by the LM.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Primary failure mode is incorrect argument selection / locating which tokens to copy into function calls. Callable programs do not fix the LM's inability to generalize index-selection rules or to robustly track positions across longer sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>GPT-3 with callable programs outperforms pure GPT-3 prompting and fine-tuned T5 on OOD addition, but still falls short of tutor-based perfect generalization. T5 with callable programs not shown to fully resolve overseeing copying errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of Language Models in Arithmetic and Symbolic Induction', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4630.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4630.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LM with Tutor</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LM with Tutor (action-sequence tutoring / Turing-machine-style actions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Method introduced in this paper where the LM is trained or prompted to generate explicit, finest-grained action sequences (move cursor, copy, add, check end) that mimic a human tutor or multi-tape Turing machine; primitives may be called for single-digit ops, and the action trace pinpoints origin of each output digit.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer-based LMs (GPT-3, T5) trained/prompted with tutor-style action sequences</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Supervision or exemplars include low-level actions (rmov, lmov, cpy, add, end=F/T) and explicit references to where each output token originates, effectively training the model to produce a state-transition trace that can be executed deterministically.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Copying, reversing, and multi-digit addition (including repeating symbols and OOD lengths); evaluated up to 80 digits for copy, up to 30 digits for addition/reverse in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>By forcing the model to produce atomic state transitions and pointer movements (a small hypothesis space between steps), the LM learns a true algorithmic procedure (akin to programming a finite-state or multi-tape Turing machine) rather than pattern-matching, enabling length generalization and correct locating.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical results: both T5+Tutor and GPT-3+Tutor achieve 100% accuracy on in-distribution and out-of-distribution tests reported (copying up to 80 digits, addition and reverse up to tested lengths). The tutoring demonstrations remove ambiguity about token origin and the sequence of state transitions, enabling perfect generalization in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No counterexamples presented within the tested ranges; the approach presumes availability of tutor-style traces or an algorithm-sketch generator, which may not always be obtainable. The paper does not present large-scale limits beyond tested lengths.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Achieves 100% accuracy across all experimented OOD and repeated-symbol conditions reported (copy up to 80 digits, addition and reverse up to tested ranges), while other methods degrade.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Intervention: train/prompt the LM to output explicit action sequences (imitation learning style). Result: model reliably reproduces algorithmic behavior and generalizes to longer/replicated inputs. No neuron-level probes shown; success is empirical.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Requires tutor traces or an algorithm-sketch generator — supervision-intensive relative to simple prompting; the feasibility of scaling tutor traces to arbitrarily complex tasks is an open question. Paper does not explore limits beyond tested sizes or whether tutor-style training generalizes to wide range of other algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>LM+Tutor outperforms all other mitigation methods (positional markers, scratchpad, callable programs) in experiments, delivering perfect generalization while others fail on some OOD / repeating-symbol scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Limitations of Language Models in Arithmetic and Symbolic Induction', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Show your work: Scratchpads for intermediate computation with language models <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Investigating the limitations of the transformers with simple arithmetic tasks <em>(Rating: 2)</em></li>
                <li>Neural turing machines <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4630",
    "paper_id": "paper-2a7ae3e98357569c41424dacd60c62d3df78a0db",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "GPT-3",
            "name_full": "Generative Pre-trained Transformer 3 (text-davinci-002 used in experiments)",
            "brief_description": "An autoregressive large pretrained Transformer used in few-shot prompting experiments; evaluated on copying, reversing, and multi-digit addition with and without intermediate steps, callable programs, and tutor-style demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3 (text-davinci-002)",
            "model_description": "Autoregressive Transformer (GPT-3 family); used via few-shot prompting (prompt examples of 4 for addition, 4 for copy/reverse); temperature=0, top_p=1 in experiments. Model size not specified in paper but refers to GPT-3 as in Brown et al. (2020).",
            "arithmetic_task_type": "Copying, reversing lists, multi-digit addition (1–30+ digits), multi-step addition with carry (per-digit addition with carries).",
            "mechanism_hypothesis": "Performs arithmetic largely via learned pattern translation/mapping from input sequence to output, improved by chain-of-thought / scratchpad style intermediate steps; struggles when the model must reliably 'locate' which input token to copy (repeating symbols) suggesting pattern-matching rather than explicit index-tracking. Few-shot chain-of-thought gives more computation time and decomposes tasks enabling better induction.",
            "evidence_for_mechanism": "Prompted GPT-3's performance improves substantially when provided fine-grained intermediate steps (scratchpad) and positional markers; callable-program prompts (where primitive ops are invoked) boost OOD generalization; explicit tutor-style step-by-step action sequences give 100% accuracy, implying that decomposing tasks into explicit low-level actions simplifies the hypothesis space and yields correct execution.",
            "evidence_against_mechanism": "Failures on repeating-digit inputs, long-digit OOD examples, and reversing tasks (poor locating when distances increase) show that the model is not reliably implementing an index-aware algorithm; callable-program experiments show correct primitive outputs but GPT-3 still sometimes chooses wrong tokens to call (appendix error), demonstrating remaining locating/copying errors.",
            "performance_metrics": "Copying: 100% in-distribution (1–5 digits) for few-shot GPT-3; better OOD than finetuned T5 but generalization degrades beyond ~30 digits (does not generalize well beyond 30 digits for some tasks); GPT-3 can generalize to 80 digits on copying random numbers in some settings (Figure 2) but fails on repeated-symbol OOD; Addition: few-shot GPT-3 accuracy increases with fine-grained steps and positional markers; GPT-3 + callable programs improves OOD but still degrades as digits increase; GPT-3 + tutor achieves 100% accuracy across the experimented ranges. Exact numeric curves given in paper figures; specific examples: GPT-3 achieves 100% in-distribution copy (1–5 digits) and 100% with tutor across OOD tests.",
            "probing_or_intervention_results": "Interventions tried: (1) add explicit positional markers (ordered or random) in prompts — improved locating and OOD in some GPT-3 settings; (2) provide fine-grained scratchpad steps — improved both in-distribution and OOD (notably for GPT-3); (3) callable-program prompts — primitive operations executed externally returned correct results but model still mis-selects inputs to call in some examples; (4) tutor-style action sequences — yields perfect generalization. No neuron-level probes reported.",
            "limitations_and_failure_modes": "Fails on inputs with repeating symbols (difficulty differentiating identical tokens), length OOD (overfitting to training-length patterns), and reversing (increasing source-target distances). Callable-program setting failure mode: incorrect selection of which tokens to feed into correct primitives. Fine-tuning and scratchpad help but do not eliminate OOD failures unless tutor-style action sequences are used.",
            "comparison_to_other_models": "GPT-3 generally outperforms finetuned T5 on OOD for some settings; GPT-3 benefits more from fine-grained steps and callable-program prompting than T5 fine-tuning; GPT-3 + tutor achieves similar perfect generalization as T5+Tutor and outperforms DeBERTa and T5 baselines on OOD.",
            "uuid": "e4630.0",
            "source_info": {
                "paper_title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "T5-base",
            "name_full": "T5 (base) fine-tuned",
            "brief_description": "Text-to-text Transformer (T5-base) fine-tuned on synthetic datasets for copy, reverse, and addition tasks; experiments examine effects of explicit positional markers, fine-grained steps, callable programs, and tutor demonstrations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "T5-base (220M) fine-tuned",
            "model_description": "Encoder-decoder Transformer (T5-base ~220M parameters) initialized from pretrained weights and fine-tuned on task-specific synthetic datasets (copy: 2,000 examples up to 5 digits; addition: 1,000 examples 1–5 digits; training hyperparams reported in appendix).",
            "arithmetic_task_type": "Copying, reversing lists, multi-digit addition (1–30 digits tested), with and without intermediate steps and positional markers.",
            "mechanism_hypothesis": "Learns mapping heuristics from training distribution; explicit positional markers help by breaking repeating-symbol symmetry, enabling token-specific copying; fine-grained steps can help but fine-tuning tends to overfit to training-length patterns.",
            "evidence_for_mechanism": "T5 with explicit ordered/random positional markers achieves 100% in-distribution accuracy on copy and improves OOD generalization relative to vanilla T5; fine-grained scratchpad steps improved in-distribution but less OOD gain compared to GPT-3 prompting. T5+Tutor also achieves 100% generalization when trained with tutor action sequences.",
            "evidence_against_mechanism": "T5 without positional markers achieves poor accuracy on repeating-symbol in-distribution examples (reported 78% on 5-digit repeating numbers) and fails on OOD lengths beyond training; fine-grained steps via fine-tuning can overfit and not provide robust OOD improvement.",
            "performance_metrics": "Copying: fine-tuned T5 baseline ~78% on 5-digit repeating numbers (in-distribution). T5 + explicit positional markers: 100% in-distribution. OOD: T5+markers generalize significantly better than baseline but still worse than tutor; addition: T5 fine-tuned often achieves high in-distribution but poor OOD (figures show steep accuracy drop past training length); T5+Tutor achieves 100% across tested lengths (per paper).",
            "probing_or_intervention_results": "Interventions: explicit positional markers (ordered/random) added to inputs — large in-distribution gains and improved OOD; fine-grained steps (scratchpad) included in targets — improved in-distribution but smaller OOD gains; callable-program style not reported to fully solve locating/copying errors for T5. Training behavior: T5 fine-tuning prone to overfitting (worse OOD) relative to prompting GPT-3.",
            "limitations_and_failure_modes": "Overfitting to training lengths (fails to generalize to longer numbers), difficulty with repeating symbols unless explicit markers are used, limited OOD generalization even with large training sets, limited improvements from scratchpad when fine-tuning (compared with prompting).",
            "comparison_to_other_models": "T5 lags behind GPT-3 in OOD generalization when both are given fine-grained steps; T5 with explicit positional markers can match in-distribution performance of DeBERTa but DeBERTa exhibits stronger overfitting characteristics (near perfect in-distribution but 0% OOD in some tasks).",
            "uuid": "e4630.1",
            "source_info": {
                "paper_title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "DeBERTa",
            "name_full": "DeBERTa (disentangled attention) base",
            "brief_description": "Transformer model using disentangled relative position embeddings (implicit positional markers); evaluated on symbolic tasks to probe whether implicit position representations improve locating and induction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "DeBERTa-base (140M) fine-tuned",
            "model_description": "Encoder-style Transformer with disentangled content and position attention (relative position embeddings). Fine-tuned on synthetic copy/add/reverse data (2,000 examples up to 5 tokens for some experiments).",
            "arithmetic_task_type": "Copying, reversing lists, multi-digit addition (1–30 digits tested) with/without fine-grained steps.",
            "mechanism_hypothesis": "Disentangled relative position embeddings act as implicit positional markers, enabling the attention mechanism to incorporate relative position signals into content attention which should help locating tokens among repeating symbols.",
            "evidence_for_mechanism": "DeBERTa achieves near 100% in-distribution accuracy on copying and addition tasks (improved locating) indicating the relative position embeddings aid in in-domain locating.",
            "evidence_against_mechanism": "DeBERTa shows severe overfitting and fails catastrophically on OOD: near 100% in-distribution but ~0% OOD accuracy for some tasks (addition, copying), indicating implicit positional embeddings as implemented do not generalize to longer sequences or OOD repetition patterns.",
            "performance_metrics": "Near 100% in-distribution accuracy on copy/add tasks (1–5 digits) but ~0% OOD accuracy on longer-digit tests (e.g., addition OOD results showed 0% accuracy in paper figures and text).",
            "probing_or_intervention_results": "Tried fine-grained steps augmentations; DeBERTa + fine-grained steps still overfit quickly (e.g., achieves 100% in-distribution within few thousand steps but OOD fails). Authors diagnose that DeBERTa's implicit position capacity increases in-distribution performance but harms OOD generalization.",
            "limitations_and_failure_modes": "Severe overfitting to training distribution (lengths) despite improved in-distribution locating; poor OOD generalization (0% OOD in some settings). Implicit positional encodings without pretraining on targeted symbolic tasks may be insufficient for robust induction.",
            "comparison_to_other_models": "DeBERTa outperforms vanilla T5 on in-distribution locating tasks but underperforms on OOD generalization; explicit positional markers on T5 sometimes yield better OOD generalization than DeBERTa.",
            "uuid": "e4630.2",
            "source_info": {
                "paper_title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Positional markers",
            "name_full": "Positional markers (explicit ordered/random and implicit via disentangled attention)",
            "brief_description": "Methods to disambiguate repeating tokens by tagging tokens with explicit markers (ordered or random) or relying on implicit relative position embeddings (DeBERTa) so the model can locate specific instances of identical symbols.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-based LMs (T5, DeBERTa, GPT-3) with positional marker interventions",
            "model_description": "Explicit ordered markers: inject tokens like A, B, C between digits (A 2 B 2 C 2); explicit random markers: inject arbitrary marker tokens; implicit markers: relative/disentangled position embeddings (DeBERTa). Used as input augmentations in fine-tuning or prompting.",
            "arithmetic_task_type": "Copying, reversing, and addition of repeated-symbol and long sequences.",
            "mechanism_hypothesis": "Tagging identical symbols with unique markers breaks symmetry so the model can learn unique mappings for each token instance, effectively converting a repetition problem into a non-repeating sequence where standard attention can track identity.",
            "evidence_for_mechanism": "T5 + explicit markers achieves 100% in-distribution copying and improves OOD generalization relative to unmarked T5; DeBERTa's implicit disentangled position embeddings produce similar in-distribution gains (near 100%).",
            "evidence_against_mechanism": "DeBERTa's implicit markers still fail OOD (0% accuracy); explicit positional markers improve but do not universally solve OOD generalization — models can still fail on longer-than-trained sequences or pattern types not seen during training unless tutor-style action sequences are used.",
            "performance_metrics": "T5 + ordered/random marker: 100% in-distribution copying; improved OOD relative to baseline (figures show much higher OOD curves than unmarked T5). DeBERTa (implicit): near 100% in-distribution but poor OOD. Exact numeric curves in paper figures.",
            "probing_or_intervention_results": "Intervention = add markers to training/prompt. Results show markers reduce repeating-symbol errors and improve locating, but interaction with model pretraining and fine-tuning dynamics matters (DeBERTa overfits).",
            "limitations_and_failure_modes": "Markers require choice/design and may not generalize to unseen lengths/patterns; ordering/randomization matters; markers do not address algorithmic carry/iteration induction and do not alone guarantee OOD generalization.",
            "comparison_to_other_models": "Explicit markers on T5 often yield better OOD than DeBERTa's implicit markers; GPT-3 benefits from markers in prompting but still has length limits without tutor.",
            "uuid": "e4630.3",
            "source_info": {
                "paper_title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Fine-grained steps (Scratchpad)",
            "name_full": "Fine-grained computation steps / Scratchpad",
            "brief_description": "Provide intermediate, per-step computations (chain-of-thought / scratchpad-style) as supervised targets or few-shot exemplars to decompose multi-digit arithmetic into sequences of elementary actions.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Applied to GPT-3 prompting and T5/DeBERTa fine-tuning",
            "model_description": "Training or prompting examples explicitly include step-by-step subtasks (convert digits, per-digit add with carry, combine results). Variants tested: coarse-grained vs fine-grained (more granular steps).",
            "arithmetic_task_type": "Multi-digit addition decomposed into per-digit additions with carry; used also for reverse and copy by giving intermediate actions.",
            "mechanism_hypothesis": "Decomposing tasks into smaller steps reduces per-step hypothesis complexity so the model can learn local transition rules (e.g., add single digits + carry) which helps induction and length generalization.",
            "evidence_for_mechanism": "GPT-3 shows substantial accuracy improvements (both in-distribution and OOD) when given finer-grained steps versus coarse steps; T5 shows in-distribution gains but less OOD improvement, likely due to fine-tuning overfitting.",
            "evidence_against_mechanism": "Even extensive scratchpad fine-tuning (100k examples reported in literature) might not produce 100% OOD performance; T5 fine-tuning with scratchpad still can fail OOD; scratchpad alone does not solve repeating-symbol locating issues.",
            "performance_metrics": "Finer-grained steps yield larger improvements: GPT-3 with fine-grained steps outperforms coarse-grained examples (figures show monotonic gains), but numeric values vary across figures. Scratchpad-based fine-tuning on 1B models in prior work failed to reach 100% OOD; in this paper GPT-3 + fine-grained steps helps but does not match full tutor performance.",
            "probing_or_intervention_results": "Interventions: adding more granular intermediate steps to the exemplar/prompt or fine-tuning targets. For GPT-3, granularity correlates with improved OOD; for T5, granularity helps in-distribution but fine-tuning can overfit.",
            "limitations_and_failure_modes": "Doesn't by itself fix locating/copying of repeated symbols; susceptible to overfitting when used in fine-tuning; may require many examples to approach robust induction; still falls short of perfect generalization without tutor-style action sequences.",
            "comparison_to_other_models": "GPT-3 benefits more from scratchpad prompting than T5 does from scratchpad fine-tuning (GPT-3 prompting generalizes better), and both are outperformed by tutor-style methods.",
            "uuid": "e4630.4",
            "source_info": {
                "paper_title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Callable programs",
            "name_full": "LM with callable programs (external primitive execution)",
            "brief_description": "Hybrid approach where the LM generates high-level actions or function-call tokens (e.g., add(1,5)), and an external deterministic function is invoked to compute primitive operations and return exact results back to the LM's context.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-based LMs (GPT-3, T5) orchestrating callable primitives",
            "model_description": "Prompting/fine-tuning format instructs the LM to generate calls to functions (convert, add, combine). The function outputs are deterministically returned and appended to the generated solution stream for subsequent steps.",
            "arithmetic_task_type": "Multi-digit addition with per-digit add primitives, copy/split/combine primitives.",
            "mechanism_hypothesis": "Delegating primitive symbolic operations to exact callable programs removes the model's need to implement low-level arithmetic correctly; the LM's role becomes chaining/composing primitives — i.e., learning an algorithm sketch for when to call which primitive.",
            "evidence_for_mechanism": "GPT-3 + callable programs shows substantially better OOD generalization than pure LM-only prompting/fine-tuning because primitive results are exact. The paper shows callable programs improve robustness on addition tasks.",
            "evidence_against_mechanism": "LM still must decide which tokens/arguments to pass into primitives (locating); authors provide an error example where the LM calls add with the wrong token (appendix A.1), causing incorrect end-to-end output despite correct primitive implementation.",
            "performance_metrics": "Callable-program prompting for GPT-3 improved OOD addition accuracy markedly compared to non-callable baselines, but accuracy still degrades as digit count increases (figures). Not perfect; numeric degradation shown in paper plots. Exact numbers vary by digit-length.",
            "probing_or_intervention_results": "Intervention: replace primitive ops (add, convert, combine) with external deterministic functions during generation. Observed that primitive correctness is guaranteed but errors remain due to incorrect argument selection by the LM.",
            "limitations_and_failure_modes": "Primary failure mode is incorrect argument selection / locating which tokens to copy into function calls. Callable programs do not fix the LM's inability to generalize index-selection rules or to robustly track positions across longer sequences.",
            "comparison_to_other_models": "GPT-3 with callable programs outperforms pure GPT-3 prompting and fine-tuned T5 on OOD addition, but still falls short of tutor-based perfect generalization. T5 with callable programs not shown to fully resolve overseeing copying errors.",
            "uuid": "e4630.5",
            "source_info": {
                "paper_title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "LM with Tutor",
            "name_full": "LM with Tutor (action-sequence tutoring / Turing-machine-style actions)",
            "brief_description": "Method introduced in this paper where the LM is trained or prompted to generate explicit, finest-grained action sequences (move cursor, copy, add, check end) that mimic a human tutor or multi-tape Turing machine; primitives may be called for single-digit ops, and the action trace pinpoints origin of each output digit.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer-based LMs (GPT-3, T5) trained/prompted with tutor-style action sequences",
            "model_description": "Supervision or exemplars include low-level actions (rmov, lmov, cpy, add, end=F/T) and explicit references to where each output token originates, effectively training the model to produce a state-transition trace that can be executed deterministically.",
            "arithmetic_task_type": "Copying, reversing, and multi-digit addition (including repeating symbols and OOD lengths); evaluated up to 80 digits for copy, up to 30 digits for addition/reverse in experiments.",
            "mechanism_hypothesis": "By forcing the model to produce atomic state transitions and pointer movements (a small hypothesis space between steps), the LM learns a true algorithmic procedure (akin to programming a finite-state or multi-tape Turing machine) rather than pattern-matching, enabling length generalization and correct locating.",
            "evidence_for_mechanism": "Empirical results: both T5+Tutor and GPT-3+Tutor achieve 100% accuracy on in-distribution and out-of-distribution tests reported (copying up to 80 digits, addition and reverse up to tested lengths). The tutoring demonstrations remove ambiguity about token origin and the sequence of state transitions, enabling perfect generalization in experiments.",
            "evidence_against_mechanism": "No counterexamples presented within the tested ranges; the approach presumes availability of tutor-style traces or an algorithm-sketch generator, which may not always be obtainable. The paper does not present large-scale limits beyond tested lengths.",
            "performance_metrics": "Achieves 100% accuracy across all experimented OOD and repeated-symbol conditions reported (copy up to 80 digits, addition and reverse up to tested ranges), while other methods degrade.",
            "probing_or_intervention_results": "Intervention: train/prompt the LM to output explicit action sequences (imitation learning style). Result: model reliably reproduces algorithmic behavior and generalizes to longer/replicated inputs. No neuron-level probes shown; success is empirical.",
            "limitations_and_failure_modes": "Requires tutor traces or an algorithm-sketch generator — supervision-intensive relative to simple prompting; the feasibility of scaling tutor traces to arbitrarily complex tasks is an open question. Paper does not explore limits beyond tested sizes or whether tutor-style training generalizes to wide range of other algorithms.",
            "comparison_to_other_models": "LM+Tutor outperforms all other mitigation methods (positional markers, scratchpad, callable programs) in experiments, delivering perfect generalization while others fail on some OOD / repeating-symbol scenarios.",
            "uuid": "e4630.6",
            "source_info": {
                "paper_title": "Limitations of Language Models in Arithmetic and Symbolic Induction",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models",
            "rating": 2,
            "sanitized_title": "show_your_work_scratchpads_for_intermediate_computation_with_language_models"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Investigating the limitations of the transformers with simple arithmetic tasks",
            "rating": 2,
            "sanitized_title": "investigating_the_limitations_of_the_transformers_with_simple_arithmetic_tasks"
        },
        {
            "paper_title": "Neural turing machines",
            "rating": 2,
            "sanitized_title": "neural_turing_machines"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.01659525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Limitations of Language Models in Arithmetic and Symbolic Induction</h1>
<p>Jing Qian<em>, Hong Wang</em>, Zekun Li, Shiyang Li, Xifeng Yan<br>University of California, Santa Barbara<br>{jing_qian, hongwang600, zekunli, shiyangli, xyan}@cs.ucsb.edu</p>
<h4>Abstract</h4>
<p>Recent work has shown that large pretrained Language Models (LMs) can not only perform remarkably well on a range of Natural Language Processing (NLP) tasks but also start improving on reasoning tasks such as arithmetic induction, symbolic manipulation, and commonsense reasoning with increasing size of models (Wei et al., 2022; Chowdhery et al., 2022). However, it is still unclear what the underlying capabilities of these LMs are. Surprisingly, we find that these models have limitations on certain basic symbolic manipulation tasks such as copy, reverse, and addition. When the total number of symbols or repeating symbols increases, the model performance drops quickly. We investigate the potential causes behind this phenomenon and examine a set of possible methods, including explicit positional markers, fine-grained computation steps, and LMs with callable programs. Experimental results show that none of these techniques can solve the simplest addition induction problem completely. In the end, we introduce LMs with tutor, which demonstrates every single step of teaching. LMs with tutor is able to deliver $100 \%$ accuracy in situations of OOD and repeating symbols, shedding new insights on the boundary of large LMs in induction.</p>
<h2>1 Introduction</h2>
<p>Transformer-based large pretrained Language Models, such as GPT3 and T5 (Vaswani et al., 2017; Brown et al., 2020; Raffel et al., 2020), have been widely used as few-shot learners in many NLP tasks. Recent work even finds these models can achieve state-of-the-art performance in arithmetic and symbolic reasoning (Nye et al., 2021; Wei et al., 2022). Although these models exhibit surprisingly impressive capabilities in complex arithmetic reasoning tasks, such as MultiArith (Roy and Roth, 2015) and GSM8k (Cobbe et al., 2021), it has also</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Examples of addition: the baseline setting (top) and Scratchpad (Nye et al., 2021) with intermediate steps (bottom). A similar method with more detailed demonstration is introduced in (Recchia, 2021).
been pointed out that they tend to make certain calculation errors and perform significantly worse when the number of math operations increases in equations (Wei et al., 2022). Brown et al. (2020) find that GPT3 displays strong proficiency in 2digit arithmetic addition, but struggles in arithmetic addition on numbers with more than three digits. Nogueira et al. (2021) also observe that the finetuned T5 model can not correctly add or subtract arbitrarily long numbers. Larger models might perform better on the testing data, but worse on numbers that are longer than the training data (out-of-distribution, OOD) (Nogueira et al., 2021).</p>
<p>Figure 1 shows two possible addition exemplars for LMs on addition problem. The scratchpad version gives more details on how humans do basic arithmetic. Nye et al. (2021) show that with more fine-grained demonstrations, the accuracy of addition can be improved dramatically with fine-tuning. Yet, it still can not achieve $100 \%$ on OOD data, even with thousands of training data points. Figure 2 shows the performance of GPT-3 and T5 on addition using the scratchpad version of training data. The problem becomes more severe when there are</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The horizontal axis is the number of digits and the vertical axis is the accuracy. The prompts for GPT3 consist of 4 examples. The T5 models are trained on 1-5 digits of up to 2,000 examples and each training example consists of random numbers in the format of 2 4 1. In-dist: in-distribution. Out-of-dist.: out-of-distribution (OOD). In-distribution refers to training on up to k-digit numbers and testing on up to k-digit numbers while out-of-distribution refers to training on up to k-digit numbers and testing on numbers with more digits. $\alpha$ indicates the repetition level of the examples. An example $x_{1} \cdots x_{n}$ with $n$ digits are sampled with the next digit probability $p\left(x_{i+1} \mid x_{i}\right)=\alpha$, when $x_{i+1}=x_{i}$; otherwise, $(1-\alpha) / 9$. Larger $\alpha$ indicates a higher repetition level.
repeating digits in the addition operands.
As the performance drops with repeating digits, we suspect that LMs might not handle the repeating symbols well. Figure 2 illustrates the performance of GPT-3 and T5 on the copy task, one of the simplest symbolic manipulation operations. GPT-3 and T5 still can not perform well on OOD. We further do a preliminary experiment where a T5 model is fine-tuned using the data containing repeating numbers of up to 80 digits, T5 still can not achieve $100 \%$ in-distribution accuracy on long repeating digits. The results indicate that there are two problems intervening: Transformers are not good at handling repeating symbols and OOD generalization. The repeating symbols can also be a problem even for in-distribution data. We believe that overcoming the aforementioned limitations is of critical importance for the future application of Transformer-based LMs to reasoning-intensive tasks such as data format conversion and robotic process automation.</p>
<p>In this paper, we investigate the potential causes behind this phenomenon and examine a set of possible mitigation solutions including fine-grained computation steps, positional markers, and LMs with callable programs. Since incorporating computation steps improves the OOD generalization in arithmetic addition (Nye et al., 2021), one possible direction is to provide more fine-grained computation steps in the fine-tuning data or the few-shot prompt. However, it may not be sufficient to alleviate the problem of repeating numbers. When a human does addition, the position of each digit is used to differentiate the repeating digits. However, the self-attention mechanism in the Transformer may not tell which " 1 " is referred to in the input.</p>
<p>This prompts us to explore using positional markers to differentiate the important tokens. Using these two methods to augment the reasoning process, we find that the performance of pretrained LMs still can not reach satisfying results. Then we resort to a method where the copy operation is implemented as a primitive function and explore whether the LM can further boost its performance.</p>
<p>We experiment with three symbolic manipulation tasks: copying, reversing, and addition. Experimental results show that although generalization in these symbolic manipulation tasks is straightforward for humans, it is still challenging for LMs, and none of these mitigation methods fully solves the problems. In the end, we introduce LMs with tutor which demonstrates every single step of teaching, pinpointing where these digits come from. LMs with tutor is able to deliver $100 \%$ accuracy in situations of OOD and repeated symbols. In this design, LMs are used to generate actions that mimic operations in multiple tape Turing machines, rather than the intermediate results. These actions generate the intermediate results on tapes. We hope this could shed light on the capability of Transformer-based LMs in addition to providing large training datasets or scaling up the size of these models.</p>
<p>To conclude, our main contributions are:</p>
<ul>
<li>We identify a set of simple symbolic manipulation tasks and uncover the limitations of the LMs in arithmetic and symbolic induction.</li>
<li>We examine a set of potential techniques including positional markers, fine-grained computation steps, and LMs with callable programs. Though they could mitigate the limitations of the LMs, none of them can completely</li>
</ul>
<p>solve the generalization problem.</p>
<ul>
<li>Finally, we demonstrate that LMs with tutor is able to deliver $100 \%$ accuracy in situations of OOD and repeated symbols. Our analysis could inspire new thoughts to overcome the limitation of LMs in symbolic manipulation.</li>
</ul>
<h2>2 Related Work</h2>
<p>Large Pretrained Language Models: Brown et al. (2020) show that GPT3 exhibits strong proficiency on 2-digit addition and subtraction using simply few-shot prompting, without any task-specific training. Furthermore, the larger the LM, the better the performance. Following GPT3, Chowdhery et al. (2022) further scale the Transformer-based LMs to a 540-billion parameter model, called Pathways Language Model (PaLM). Same as Brown et al. (2020), Chowdhery et al. (2022) find that scaling the LMs consistently results in better arithmetic reasoning ability with few-shot prompting. However, the reasoning ability of the large LMs is still limited. GPT3 struggles with 3-digit arithmetic and with direct prompting, even 540B PaLM can not achieve high performance on complex tasks requiring multi-step reasoning. Therefore Wei et al. (2022) propose the following prompting method for large pretrained LMs.</p>
<p>Chain-of-Thought Prompting: This prompting method provides a few chain-of-thought demonstrations, which is a series of intermediate reasoning steps, as exemplars in the prompting. Therefore, given a complex reasoning task, the model is allowed to calculate the intermediate results step-by-step before generating the final answer. With chain-of-thought prompting, a complex reasoning task is decomposed into a list of simple operations and LMs can derive these operations one by one. Kim et al. (2022) adopt faithful explanations that accurately represent the reasoning process behind solving a math word problem. Wei et al. (2022) show that combining chain-of-thought prompting and a sufficiently large LM, 540B PaLM, can significantly improve the LMs' reasoning ability on complex tasks, such as math word problems.</p>
<p>Fine-tuning with Large Training Datasets: Instead of few-shot prompting, another direction is to fine-tune large LMs with a sufficient amount of training data. Nogueira et al. (2021) fine-tune T5 with different ways of representing numbers, but even with the best-performing representation, the fine-tuned model can not achieve as good ac-
curacy on out-of-distribution testing examples as in-distribution testing examples. Nye et al. (2021) propose to use Scratchpad to improve the out-ofdistribution accuracy. Scratchpad combines step-by-step reasoning with fine-tuning. The training examples include the intermediate steps of an algorithm in target, so the model is trained to generate not only the final answer, but also the intermediate steps, which is similar to chain-of-thought, but requires more training data. Nye et al. (2021) show that using the training data augmented with intermediate steps significantly improves the model performance, but even with 100k augmented training examples for the addition task, the fine-tuned 1B LM still does not perform well on out-of-distribution addition. Our work is also related to Graves et al. (2014), which extends the capabilities of Recurrent Neural Networks to two simple symbolic manipulation tasks, copy and sort, by augmenting the model with external memory resources.</p>
<h2>3 Mitigation Methods</h2>
<h3>3.1 Positional Markers</h3>
<p>We first explore possible methods to mitigate the problem of repeating numbers. We introduce two types of positional markers: implicit positional markers and explicit ones.</p>
<p>Most Transformer-based LMs encode the positional information into positional vectors and add each of them to the corresponding word vector. Although large LMs have already incorporated positional encoding in the model architecture (Figure 3), results in Figure 2 indicate that the positional encoding commonly used in large LMs may not be sufficient to locate each repeating digit effectively. Instead of representing each token by the sum of its contextual token embedding and the position embedding, DeBERTa (He et al., 2021) represents each token with a token embedding and a position embedding, respectively, and the attention weights are computed using disentangled matrices based on both embeddings, respectively (Figure 3). In other words, the self-attention in DeBERTa is disentangled. With the disentangled relative position embeddings, the attention scores between tokens depend not only on the content but also on the relative position between the tokens, so the disentangled relative position embeddings act as implicit position markers within DeBERTa, which might make it easier for the model to learn the latent position relationship in the training data of the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustration of standard Transformer attention (left) and DeBERTa disentangled attention (right).
symbolic manipulation tasks.
Although DeBERTa uses disentangled attention mechanism, it was not originally introduced to enhance the locating capability of LMs, so no pretraining task was specifically proposed for training the position embeddings in DeBERTa. This may potentially lead to its limited generalization ability on the induction tasks requiring accurate locating. Rather than relying on implicit positional markers, another, more straightforward approach is to add explicit positional markers in the model input. For example, the input string 222 is augmented with positional markers $A, B, C, \cdots$. We explore two methods of adding explicit positional markers:
Ordered marker: The markers are inserted into the input in order. $222 \rightarrow$ A 2 B 2 C 2
Random marker: The markers are inserted into the input in random order. $222 \rightarrow$ E 2 X 2 J 2</p>
<p>With the explicit positional markers, each repeating 2 becomes different for the model. When doing symbolic manipulation, the Transformer-based LMs can easily locate the digit by recognizing the explicit positional markers. Essentially, adding explicit positional markers breaks the repeating numbers into a non-repeating input sequence. This method is also related to pointer networks (Vinyals et al., 2015), which uses attention as a pointer to select the position indexes of the input tokens as the output. A hybrid pointer-generator network can also be leveraged to copy number from the source text, while retaining the ability to produce new numbers through the generator (See et al., 2017).</p>
<h3>3.2 Fine-grained Computation Steps</h3>
<p>We then explore possible methods to alleviate the OOD generalization problem. One observation is that the complexity of addition with long digits
is larger than that of the 1-digit addition. Thus, the model should be given more computation time on the task when the numbers are large. The finetuned T5 and prompted GPT3 mentioned above, however, is required to generate the answer with a fixed amount of computation, so one possible direction to mitigate this limitation is to allow the model to operate step-by-step instead of generating the answer in one forward pass. For example, in kdigit addition, the model is allowed to break it down into k simple 1-digit addition and the model is allowed to generate k intermediate addition results to get the final answer.</p>
<p>Generating fine-grained computation steps can potentially alleviate the generalization problem, but may not contribute to the locating capability of the Transformer-based LMs. To mitigate the locating problem, we add positional markers to scratchpad (Nye et al., 2021) (Figure 4).</p>
<div class="codehilite"><pre><span></span><code><span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">5</span>
<span class="n">solution</span><span class="o">:</span>
<span class="n">convert</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="o">}\</span><span class="n">mp</span><span class="err">@</span><span class="n">subsup</span><span class="o">{\</span><span class="n">omega</span><span class="o">}{}{\</span><span class="n">circ</span><span class="o">}</span><span class="mi">1</span><span class="o">,\</span><span class="n">mp</span><span class="o">\</span><span class="n">mp</span><span class="err">@</span><span class="n">subsup</span><span class="o">{\</span><span class="n">omega</span><span class="o">}{}{</span><span class="mi">1</span><span class="o">.</span>
<span class="n">convert</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="o">}\</span><span class="n">mp</span><span class="err">@</span><span class="n">subsup</span><span class="o">{\</span><span class="n">omega</span><span class="o">}{}{\</span><span class="n">circ</span><span class="o">}</span><span class="mi">2</span><span class="o">,\</span><span class="n">mp</span><span class="o">\</span><span class="n">mp</span><span class="err">@</span><span class="n">subsup</span><span class="o">{\</span><span class="n">omega</span><span class="o">}{}{</span><span class="mi">5</span><span class="o">.</span>
<span class="o">-</span><span class="mi">1</span><span class="w"> </span><span class="mi">5</span><span class="o">,</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="mi">1</span><span class="o">+</span><span class="mi">5</span><span class="o">+</span><span class="mi">0</span><span class="o">=</span><span class="mi">6</span><span class="o">.</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
<span class="n">combine</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">result</span><span class="o">,</span><span class="w"> </span><span class="kd">get</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
<span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">2</span><span class="o">,</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="n">so</span><span class="w"> </span><span class="mi">1</span><span class="o">+</span><span class="mi">2</span><span class="o">+</span><span class="mi">0</span><span class="o">=</span><span class="mi">3</span><span class="o">.</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="n">step</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">3</span><span class="o">.</span>
<span class="n">combine</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">6</span><span class="o">,</span><span class="w"> </span><span class="kd">get</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
<span class="n">carry</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="n">combine</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">and</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span><span class="o">,</span><span class="w"> </span><span class="kd">final</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
</code></pre></div>

<p>Figure 4: The prompt for GPT3 on the addition task. We use $\boldsymbol{\sigma}$ and $\boldsymbol{\sigma}$ to denote optional different markers as described in Section 3.1 if they are applied.</p>
<p>We also experiment a more comprehensive scheme where we directly copy the number associated with the explicit positional marker to its later appearance. For example, for the explicit marker $\mathrm{S}[\mathrm{B}]$, we copy its value 1 to the later appearance in the fourth line as shown in Figure 5. More detail and experimental results are put in appendix A.4.</p>
<div class="codehilite"><pre><span></span><code><span class="nl">question</span><span class="p">:</span><span class="w"> </span><span class="nl">question</span><span class="p">:</span><span class="w"> </span><span class="n">S</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">S</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">T</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="n">T</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="mi">5</span>
<span class="nl">solution</span><span class="p">:</span>
<span class="n">S</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">T</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Z</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">R</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="mi">6</span><span class="p">,</span><span class="w"> </span><span class="n">Z</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">0</span>
<span class="n">S</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">T</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Z</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">R</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">Z</span><span class="o">[</span><span class="n">C</span><span class="o">]</span><span class="w"> </span><span class="mi">0</span>
<span class="k">result</span><span class="err">:</span><span class="w"> </span><span class="n">Z</span><span class="o">[</span><span class="n">C</span><span class="o">]</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="n">R</span><span class="o">[</span><span class="n">B</span><span class="o">]</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">R</span><span class="o">[</span><span class="n">A</span><span class="o">]</span><span class="w"> </span><span class="mi">6</span>
</code></pre></div>

<p>Figure 5: The demonstration of comprehensive scheme for addition problem. Position markers are marked in red and reference markers are marked in green.</p>
<h3>3.3 LM with Callable Programs</h3>
<p>Since callable programs do not have the generalization problem, we combine LMs with callable programs to replace the basic symbolic operations when possible. For example, when combined with the fine-grained computation steps in the addition task, the convert, add, or combine operations can be considered callable programs. When the LM generates the text sequence add $(1,5)$, the callable function add will be invoked and return the result in text: carry C: 0 , result 6.</p>
<p>Following the example in Section 3.2, with callable functions, the prompt format is as follows:</p>
<div class="codehilite"><pre><span></span><code><span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">5</span>
<span class="n">solution</span><span class="o">:</span>
<span class="n">call</span><span class="w"> </span><span class="n">convert</span><span class="w"> </span><span class="o">(</span><span class="mi">1</span><span class="w"> </span><span class="mi">1</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">5</span><span class="o">),</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">(</span><span class="mi">1</span><span class="w"> </span><span class="mi">2</span><span class="o">),</span><span class="w"> </span><span class="o">(</span><span class="mi">1</span><span class="w"> </span><span class="mi">5</span><span class="o">).</span>
<span class="o">(</span><span class="mi">1</span><span class="w"> </span><span class="mi">5</span><span class="o">),</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="n">add</span><span class="w"> </span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="w"> </span><span class="mi">5</span><span class="o">),</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="n">C</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
<span class="n">call</span><span class="w"> </span><span class="n">combine</span><span class="w"> </span><span class="o">(</span><span class="mi">6</span><span class="o">,</span><span class="w"> </span><span class="o">),</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
<span class="o">(</span><span class="mi">1</span><span class="w"> </span><span class="mi">2</span><span class="o">),</span><span class="w"> </span><span class="n">call</span><span class="w"> </span><span class="n">add</span><span class="w"> </span><span class="o">(</span><span class="n">C</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="mi">1</span><span class="o">,</span><span class="w"> </span><span class="mi">2</span><span class="o">),</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="n">C</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">3</span><span class="o">.</span>
<span class="n">call</span><span class="w"> </span><span class="n">combine</span><span class="w"> </span><span class="o">(</span><span class="mi">3</span><span class="o">,</span><span class="w"> </span><span class="mi">6</span><span class="o">),</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
<span class="n">call</span><span class="w"> </span><span class="n">combine</span><span class="w"> </span><span class="o">(</span><span class="n">C</span><span class="o">:</span><span class="w"> </span><span class="mi">0</span><span class="o">,</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span><span class="o">),</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span><span class="o">,</span><span class="w"> </span><span class="kd">final</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span><span class="o">.</span>
</code></pre></div>

<p>Figure 6: The prompt for GPT3 on the addition task with callable programs. and are positional markers. Different callable programs (convert, add and combine) are marked in different colors, and the results they returned are underlined with the corresponding color.</p>
<p>Given a testing example, the prompted GPT3 first generates the solution step by step. During the process, the results of the function calls will be appended to the generated result to be used in the following steps. Callable programs can be viewed as decomposing a complex task to smaller, simpler jobs. The remaing issue is to learn chaining these smaller jobs together to complete the task.</p>
<p>Callable programs can guarantee the correctness of output given correct input for a given job. However, LMs may still suffer from the locating problem since the callable programs rely on LMs to decide which token to copy (Figure 11 in the appendix). Unfortunately, LMs cannot guarantee the correctness of this copy action.</p>
<h3>3.4 LM with Tutor</h3>
<p>Scratchpad (Nye et al., 2021) ignores the visual process when an elementary school tutor visually illustrates how to perform addition step by step: pinpointing where each digit in the output sequence comes from, adding single digits together and iterating. It turns out that these details and abstractions
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: An illustration of doing copy with pattern matching.
are important in order to simplify the learning process and help kids learn addition in a few shots.</p>
<p>A tutor shows every single step visually and sometimes calls an already learned sub-module to complete a task. In this way, the hypothesis space between two consecutive steps can be dramatically simplified; hence the chance of learning a correct model can be improved.</p>
<p>Take copy as an example. Instead of providing a training example: copy: 111222 result: 1111222, we need to demonstrate where the first 1 , the second 1 , and the third 1 in the output sequence come from, which exactly imitates the finest action a human could do to perform such an operation. Suppose there is a cursor placed at the beginning of the input sequence, a "rmov" operation moves the cursor one token to the right. A "cpy" operation copies a single digit to the output sequence. An "end" operation checks if the marker reaches the end of the sequence. " $T$ " and " $F$ " represent true and false respectively. We assume all these actions have been learned. Then a possible action sequence to complete the copy operation is as follows:
rmov, end=F, cpy, rmov, end=F, cpy, . . . , rmov, end=T.
This fine-grained action sequence accurately describes the whole copy operation. Certainly, there are other ways to perform copying. For example, instead of using a cursor, one can use a pattern match to perform the copy operation (Figure 7). We suspect that the copy operation learned from Transformer is following this pattern-matching approach, which is error-prone when the pattern has repeating symbols and when the long pattern is out-of-distribution. Positional markers do not help either as they seem unable to handle the OOD generalization problem.</p>
<p>If we take the action sequence "rmov, end=F, ..." to train a Transformer for copying, the hypothesis space is simplified, thus making it possible to find the simplest model that can simulate the whole action sequence. This setting involves train-</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 8: An illustration of the LM with Tutor method. With the tutor (right), the LM or just a transformer (left) generates an action sequence that simulates how humans do arithmetic addition.
ing a learner to predict the next action based on the input and the actions demonstrated by experts, which is similar to the setting of imitation learning (Pomerleau, 1988; Ross et al., 2011). Although there is no guarantee that Transformer can definitely find the correct model, the chance is much higher. One can also relate the setting with a multiple tape Turing machine where the state transition is conducted among the positions of tape heads and read/write operations. The Transformer is trained to learn such state transitions, thus completing the programming of a Turing machine.</p>
<p>As for the addition operation, a similar action sequence can be obtained to simulate how humans tutor kids do addition at an early age (Figure 8). Let "lmov" denote moving the cursor one token to the left. The "add" operation adds three single digits together, one from each of the two operands and the third one from the carry digit, appends the result to the output, and updates the carry digit. Assume "add" is a callable program as kids have learned how to do single digits addition. Suppose the cursor starts from the end of the operands. The entire action sequence looks like the following.
lmov, end=F, add, lmov, end=F, add, . . . , lmov, end=T.</p>
<p>The main difference between the tutor and the Scratchpad method (Nye et al., 2021) is the abstract callable function and detailed action sequence. The action sequence includes all the state transitions needed to complete the task. It perfectly overcomes the OOD issue and does not require many training examples in order to achieve $100 \%$ accuracy.</p>
<p>While there is a great effort to enlarge Transformer-based LMs such as PALM (Chowdhery et al., 2022) and Minerva (Lewkowycz et al.,
2022), to improve the performance in symbolic and logical reasoning, our result reveals that it might be necessary to demonstrate the action sequence with reasonable abstraction to the Transformer to leverage its full strength.</p>
<p>In cases where action sequences are not available, e.g., only a problem specification is given, it might be more appropriate to develop an LLM (algorithm generator) to generate an algorithm sketch and then run another LLM to execute the sketch to get the answer. The sketch need not to be in the form of program codes. A human understand-able step-by-step instruction is good enough. The sketch can be viewed as an intermediate model whose complexity is much smaller than the LLM itself. Hence it has a better chance of solving the generalization/OOD issue.</p>
<h2>4 Experiments</h2>
<p>In this section, we conduct experiments on three different problems including copying, addition, and another basic symbolic manipulation operation, reverse. We illustrate the limitation of LMs in symbolic and arithmetic induction and the improvement that could be achieved by the mitigation methods.</p>
<h3>4.1 Copy Operation</h3>
<p>Copying is the most basic operation. We experiment with the following methods and make sure each digit is tokenized into a single token by separating the digits with blanks:
GPT3: We prompt GPT3 to output the same tokens as the given input. Full prompt can be found in appendix (Figure 12).
DeBERTa / T5: The training example is as follows: copy: 1234 result: 1234
T5 + ordered marker: The training data is augmented with explicit positional markers. copy: A 1 B 2 C 3 result: A 1 B 2 C 3
T5 + random marker: Same as above, but the augmented positional markers are in random order. copy: E 1 A 2 F 3 result: E 1 A 2 F 3
T5 / GPT3 + tutor: The training and testing examples are as described in Section 3.4.</p>
<p>We experiment with the T5-base (220M) model, DeBERTa-base (140M) model, and GPT3 text-davinci-002. The models are initiated with the pretrained parameters and further fine-tuned on the training data. For GPT3 or T5 with tutor, the training data consists of 15 examples of up to 5 digits. For all the other T5 models and DeBERTa, the</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 9: Experimental results. (a): results of copying repeating numbers. (b)(c): results of reversing the list. (d)(e)(f): results on arithmetic addition. The x-axis is the number of digits or number of items.
training data consists of 2,000 random numbers of up to 5 digits. We evaluate all the models on copying repeating numbers of up to 80 digits. The results are illustrated in Figure 9(a).</p>
<p>As shown in Figure 9(a), GPT3 achieves 100\% accuracy on the in-distribution testing data (1-5 digits) but the fine-tuned T5 achieves $78 \%$ accuracy on the 5-digit repeating numbers although they are indistribution. Augmented with random or ordered positional markers, the T5 models achieve 100\% in-distribution accuracy, and so does using implicit positional markers (DeBERTa). This suggests that both implicit positional markers and explicit positional markers may help with the locating capability of LMs. However, using explicit positional markers, either ordered or random, the model exhibits significantly better generalization to OOD testing data whereas DeBERTa fails on OOD data. GPT3 exhibits better OOD generalization than T5 with positional markers but it does not generalize well beyond 30 digits. Both T5 + tutor and GPT3 + tutor keeps $100 \%$ accuracy on OOD testing data.</p>
<h3>4.2 Addition</h3>
<p>For arithmetic addition, we experiment with the following methods:
GPT3: We prompt GPT3 to directly output the
sum for given addition equation. Full prompt can be found in appendix (Figure 13).
GPT3 + coarse-grained steps: The exemplar is similar to that in Figure 4, but the instructions for the result combination and the computation of the carry digit and step result are omitted.
GPT3 + fine-grained steps (+ ordered marker): The exemplar we use is as shown in Figure 4.
GPT3 + callable programs: The exemplar is shown in Figure 6.
DeBERTa / T5: The training data follows the format of the exemplar for GPT3.
DeBERTa / T5 + fine-grained steps: The training data used in this setting follow the format as the exemplar in GPT3 + fine-grained steps.
T5 + ordered / random marker: The training example is augmented with ordered or random markers. For example, question: G 1 C 1 + G 2 C 5 result: G 3 C 6. For the ordered marker, we apply it to the digits as the following: C 2 B 2 A 2. T5 + fine-grained steps + ordered / random marker: The training data in this setting follow a similar format as the exemplar in GPT3 + finegrained steps + ordered marker, but the positional markers can be in random order.
T5 / GPT3 + tutor: The training and testing examples are as described in Section 3.4.</p>
<p>The model settings are the same as in the above copy experiments. For LMs with tutor, the training data or prompt consists of 15 examples of up to 5 digits. In other settings, the training data consists of 1,000 examples of 1-5 digit addition and for GPT3, the prompt includes 4 examples. We evaluate all the models on the addition of up to 30 digits. The results are shown in Figure 9(d)(e)(f).</p>
<p>As shown in Figure 9(d), both coarse-grained and fine-grained computation steps contribute to the in-distribution performance of GPT3, and using finer-grained steps achieves larger performance gains on both in-distribution data and OOD data. The performance is further boosted with explicit positional markers. Experiments on T5 (Figure 9(e)(f)) also show the effectiveness of using explicit positional markers, with or without fine-grained computation steps, indicating that the explicit positional markers might make it easier for LMs to learn the induction in the arithmetic reasoning tasks. Similar to the results on the copying task, both DeBERTa and DeBERTa + fine-grained steps achieve near $100 \%$ in-distribution accuracy but $0 \%$ OOD accuracy, suggesting that the relative position embedding of DeBERTa might have limited OOD generalization ability. On T5, incorporating finegrained computation steps does not improve the OOD performance as significantly as on GPT3 (Figure 9(f)). The reason might be that fine-tuning T5 tends to overfit more easily than prompting GPT3. Unsurprisingly, GPT3 + callable programs achieves much better OOD generalization. However, its OOD performance still degrades as the number of digits increases. Same as in the copy experiments, $L M s+$ tutor keeps $100 \%$ accuracy on all the experimented numbers of digits.</p>
<h3>4.3 Reverse List</h3>
<p>Besides copying and addition, we also experiment with reversing. Reversing is similar to copying. Both require replicating the items in the input, but reversing might be more challenging than copying in the terms of locating. In copying, the distance between each source digit and the replicated digit is the same for each digit in the number. However, when reversing, the distance between the source item and the replicated item keeps increasing during the generation. For this problem, we experiment with the following methods:
GPT3: We prompt GPT3 to directly output the reversed list of items without intermediate steps.</p>
<p>Full prompt can be found in appendix (Figure 14). DeBERTa / T5: reverse the list: bike, apple, book result: bike, cat, pen
GPT3 / DeBERTa / T5 + fine-grained steps: The training example for T5 and the exemplar for GPT3 are shown in Figure 10.</p>
<div class="codehilite"><pre><span></span><code>reverse the list: bike, cat, pen
solution:
A is bike. B is cat. C is pen.
Now to reverse, change the order to:
C is pen. B is cat. A is bike.
Result: pen, cat, bike
</code></pre></div>

<p>Figure 10: The prompt for GPT3 on the reverse task with fine-grained steps.</p>
<p>T5 + ordered marker: The list items are augmented with the ordered positional markers in the input. reverse the list: A bike, B cat, C pen result: pen, cat, bike.
T5 / GPT3 + tutor: The training and testing examples are very similar to that for the copy task. The only difference is the direction for move operation. "rmov" in the copy task is replaced by "lmov" here.</p>
<p>The model settings are the same as in the above experiments and the training data consists of examples of 1-5 items, which are randomly sampled from a predefined list of single-token nouns. For LMs with tutor, the training data or prompt consists of 15 examples of up to 5 items. For T5, the training data consists of 1,000 examples. For GPT3, each prompt includes 4 examples. We evaluate all the models on reversing the list of up to 30 items. The results are shown in Figure 9(b)(c).</p>
<p>Although GPT3 can generalize to 80 digits on copying random numbers (Figure 2), it does not generalize well beyond 20 items on reversing, which suggests that reversing might require stronger locating capability than copying. This problem also occurs on DeBERTa and T5. When tested on the OOD data, the models tends to generate only a sublist of the input. Using fine-grained steps (Figure 9(b)) or positional markers, whether implicit or explicit (Figure 9(c)), does not significantly improve the generalization of the experimented models. The reason might be the increasing distance between the source item and the replicated item as stated above. Again, $L M s+$ tutor maintains $100 \%$ accuracy throughout the experiments. We put more discussion about the results in appendix A. 5 due to the page limit.</p>
<h2>5 Conclusion</h2>
<p>In this work, we explore the limitations of pretrained LMs on arithmetic reasoning and symbolic manipulation. We experiment with three simple symbolic manipulation tasks and show that improving the locating and induction capability of LMs can be important for further improving their performance. Our method that combines abstraction and finest-grained step-by-step tutoring demonstrates its potential to generalize correctly, shedding light on possible directions orthogonal to scaling up LMs for future work in this area.</p>
<h2>6 Limitations</h2>
<p>In this work, we experiment with GPT3, T5, and DeBERTa. Other large pretrained LMs, such as PaLM (Chowdhery et al., 2022), is not covered in this work. We do not experiment with methods such as fine-tuning GPT3 due to the computation cost. The main purpose of this work is to uncover and analyze the fundamental limitations of LMs on symbolic and arithmetic induction instead of improving their performance of reasoning tasks, so we do not directly compare the mitigation methods with the previous work such as scratchpad (Nye et al., 2021) and (Wei et al., 2022) in our experiments. We leave more advanced methods for future work.</p>
<h2>References</h2>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob</p>
<p>Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168.</p>
<p>Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. CoRR, abs/1410.5401.</p>
<p>Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.</p>
<p>Bugeun Kim, Kyung Seo Ki, Sangkyu Rhim, and Gahgene Gweon. 2022. EPT-X: An expression-pointer transformer model that generates eXplanations for numbers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4442-4458.</p>
<p>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. CoRR, abs/2206.14858.</p>
<p>Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2021. Investigating the limitations of the transformers with simple arithmetic tasks. CoRR, abs/2102.13019.</p>
<p>Maxwell I. Nye, Anders Johan Andreassen, Guy GurAri, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show your work: Scratchpads for intermediate computation with language models. CoRR, abs/2112.00114.</p>
<p>Dean Pomerleau. 1988. ALVINN: an autonomous land vehicle in a neural network. In Advances in Neural Information Processing Systems 1, [NIPS Conference, Denver, Colorado, USA, 1988], pages 305-313. Morgan Kaufmann.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1-140:67.</p>
<p>Gabriel Recchia. 2021. Teaching autoregressive language models complex tasks by demonstration. Computing Research Repository, abs/2109.02102. Version 3.</p>
<p>Stéphane Ross, Geoffrey J. Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2011, Fort Lauderdale, USA, April 11-13, 2011, volume 15 of JMLR Proceedings, pages 627-635. JMLR.org.</p>
<p>Subhro Roy and Dan Roth. 2015. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1743-1752. The Association for Computational Linguistics.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 5998-6008.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2692-2700.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38-45.</p>
<h2>A Appendix</h2>
<h2>A. 1 Error case for LM with callable program</h2>
<p>Here we show one error case for LM with callable program in Figure 11.</p>
<div class="codehilite"><pre><span></span><code>question: ... 6 18...+...6 19...
prediction:
call convert (...618...,...619...) return...
*.
(8 9), call add (C:0, 8, 9), return carry C: 1, result 7.
call combine ...
(6 1), call add (C: 1, 6, 1), return carry C: 0, result 8.
<span class="gh">#</span> ERROR! Should be (1 1), call add (C:1, 1, 1)...
...
</code></pre></div>

<p>Figure 11: An error example of GPT3 with callable functions. The error is highlighted.</p>
<h2>A. 2 GPT3 prompts</h2>
<p>Here we show the prompts of GPT3 used for copy, addition and reverse tasks in Figure 12, 13 and 14.</p>
<div class="codehilite"><pre><span></span><code><span class="n">copy</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">2</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">2</span>
<span class="n">copy</span><span class="o">:</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">7</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">7</span>
<span class="n">copy</span><span class="o">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">2</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">2</span>
<span class="n">copy</span><span class="o">:</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span>
</code></pre></div>

<p>Figure 12: The prompt for GPT3 on the copy task.</p>
<div class="codehilite"><pre><span></span><code><span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">5</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">6</span>
<span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="mi">3</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="mi">5</span>
<span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">8</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="mi">5</span>
<span class="n">question</span><span class="o">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">1</span>
<span class="n">result</span><span class="o">:</span><span class="w"> </span><span class="mi">7</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span><span class="mi">6</span><span class="w"> </span><span class="mi">6</span>
</code></pre></div>

<p>Figure 13: The prompt for GPT3 on the addition task without intermediate steps.</p>
<div class="codehilite"><pre><span></span><code>reverse the list: bike, cat, pen
result: pen, cat, bike
reverse the list: chair, bike, apple, book
result: book, apple, bike, chair
reverse the list: book, phone, fish, orange, fish
result: fish, orange, fish, phone, book
</code></pre></div>

<p>Figure 14: The prompt for GPT3 on the reverse task without intermediate steps.</p>
<p>question: S[F] 5 S[E] 2 S[D] 8 S[C] 1 S[B] 7 S[A] 1 + T[F] 6 T[E] 5 T[D] 0 T[C] 2 T[B] 4 T[A] 5
solution:
S[A] $1+T[A] 5+Z[A] 0=R[A] 6, Z[B] 0$.
$S[B] 7+T[B] 4+Z[B] 0=R[B] 1, Z[C] 1$.
$S[C] 1+T[C] 2+Z[C] 1=R[C] 4, Z[D] 0$.
$S[D] 8+T[D] 0+Z[D] 0=R[D] 8, Z[E] 0$.
S[E] $2+T[E] 5+Z[E] 0=R[E] 7, Z[F] 0$.
result: Z[F] 0 R[E] 7 R[D] 8 R[C] 4 R[B] 1 R[A] 6</p>
<p>Figure 15: Error case for T5 model with positional and reference marker on addition problem.</p>
<h2>A. 3 Experiment configuration</h2>
<p>For fine-tuning the T5-base and DeBERTa model, we use the learning rate $5 \mathrm{e}-5$, batch size 16 , training epochs 200. The maximum generation length is set to 512 . The checkpoints are evaluated every 1000 optimization steps. The random seed is fixed to 42. We use the implementation for HuggingFace (Wolf et al., 2020). For GPT3, we set temperature $=0$, top_p=1, frequency_penalty $=0$, and presence_penalty $=0$. All the experiments are conducted on NVIDIA RTX A6000 GPUs.</p>
<h2>A. 4 Reference marker</h2>
<p>As shown in Figure 5, we apply two different markers in the demonstration. The positional marker is used to define the value stored in the marker, while reference marker is used to explicitly copy the value from the positional marker with the same name. Each number in this demonstration is uniquely marked with positional or reference marker. For the positional marker, the model needs to generate both the marker and its value. For the reference marker, the model only needs to generate the marker and the value will be explicitly copied from its corresponding positional marker.</p>
<p>Similar to previous experiments on the addition problem, we train the model on 1-5 digits and test its performance on both in-domain (1-5 digits) and out-of-domain (6-10 digits) settings. The experimental results show that the model is able to achieve $100 \%$ accuracy on in-domain data, but get $0 \%$ accuracy on out-of-domain data. We also tried to extend the in-domain to 10 digits and get the same results that the model can solve in-domain problems, but fail to generalize to out-of-domain.</p>
<p>We show one error case of this model in Figure 15, where the error step is highlighted in yellow. On this 6-digit addition problem, the model skipped the last digit and directly jump to the result, which
causes the error. The problem is the model doesn't learn to how to generalize from 1-5 digits to 6 digits. Instead, it is overfitting to the training data, which makes it directly output the results after adding 5 digits. How to reduce the hypothesis space and force the model to learn to generalize to out-ofdomain data would be one future research direction to solve this problem.</p>
<h2>A. 5 Discussion</h2>
<p>From the experimental results, we observe that finegrained computation steps may improve the LM's induction ability on the arithmetic reasoning tasks and the granularity of the steps has an impact on the performance improvement. Finer-grained computation steps may contribute to larger performance improvement.</p>
<p>Positional markers, whether implicit or explicit, improves LMs' in-distribution performance on all the symbolic manipulation tasks in our experiments. However, We find that augmented with the relative position embeddings, DeBERTa tends to face more severe over-fitting than T5 during fine-tuning. In the reversing experiment, using the T5 model without pretrained parameters, the finetuned model can not achieve a good in-distribution performance after 200k optimization steps. However, the DeBERTa model without pretrained parameters achieves $100 \%$ in-distribution accuracy within only 2 k optimization steps while the OOD accuracy drops, indicating that it has overfitted within 2 k optimization steps. In other words, the relative position embeddings in DeBERTa significantly improve the model's capacity of positions, which improves in-distribution performance on simple symbolic manipulation tasks, but may not generalize well on OOD data. Compared with the implicit positional markers (relative position embeddings in DeBERTa), explicit positional markers might have better OOD generalization ability. However, incorporating symbolic manipulation tasks in the LM pretraining stage might alleviate this problem, so incorporating implicit positional markers can still be a possible direction of improving the LM's performance on reasoning tasks requiring locating ability.</p>
<p>Using LM with callable programs exhibits strong OOD performance on addition, suggesting that the LMs' ability to perform simple symbolic operations, such as copying, splitting, and combining, can be critical for improving their performance on</p>
<p>reasoning tasks. How to further improve the LMs' performance on more complex reasoning tasks in this direction is left for future work.</p>
<h1>A A1. Did you describe the limitations of your work?</h1>
<p>6
\&amp; A2. Did you discuss any potential risks of your work?
We don't think our work has any potential risks.
A3. Do the abstract and introduction summarize the paper's main claims?
1
\&amp; A4. Have you used AI writing assistants when working on this paper?
Left blank.</p>
<h2>B $\mathscr{E}$ Did you use or create scientific artifacts?</h2>
<p>Left blank.
$\square$ B1. Did you cite the creators of artifacts you used?
No response.
$\square$ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
No response.
$\square$ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?
No response.
$\square$ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?
No response.
$\square$ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?
No response.
$\square$ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.
No response.</p>
<h2>C $\square$ Did you run computational experiments?</h2>
<p>4
\&amp; C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?</p>
<p>Left blank.
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p>
<p>C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?
A. 3
$\mathscr{K}$ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?
I reported the results from a single run
( C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?
No used.
D Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.</p>
<p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?
No response.
D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?
No response.
D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?
No response.
D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p>
<p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?
No response.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>The first two authors (Jing and Hong) contributed equally to this work.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
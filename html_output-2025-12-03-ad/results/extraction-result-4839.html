<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4839 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4839</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4839</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264451603</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.16731v1.pdf" target="_blank">Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Spatial reasoning over text is challenging as the models not only need to extract the direct spatial information from the text but also reason over those and infer implicit spatial relations. Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text. In this paper, we explore the potential benefits of disentangling the processes of information extraction and reasoning in models to address this challenge. To explore this, we design various models that disentangle extraction and reasoning(either symbolic or neural) and compare them with state-of-the-art(SOTA) baselines with no explicit design for these parts. Our experimental results consistently demonstrate the efficacy of disentangling, showcasing its ability to enhance models' generalizability within realistic data domains.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4839.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4839.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PISTAQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pipeline for Spatial Question Answering (PISTAQ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline architecture that disentangles extraction and reasoning: BERT-based spatial role labeling and coreference modules extract triplets which are fed to a Prolog-based symbolic spatial reasoner that applies explicit spatial rules to answer multi-hop spatial questions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PISTAQ (pipeline: SPRL + Prolog reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline combining BERT-based Spatial Role Labeling (SPRL) modules (BERT-base token encoder with BIO tagging and triplet classifiers; BERT-base described with 768 hidden dims in paper) for entity/role/relation extraction, a learned coreference module, and a Prolog logic-based spatial reasoner implementing rules (inverse, symmetry, transitivity, combination). Reasoner implemented in Prolog (symbolic).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial Question Answering benchmarks (SPARTUN, SPARTQA-AUTO, SPARTQA-HUMAN, RESQ)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Textual multi-hop spatial reasoning tasks where stories describe spatial relations among entities and questions ask about implicit relations (Yes/No and Find-Relation) requiring combining explicit relations and applying spatial rules (directional, topological, distance, RCC8-like relations and quantifiers).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Explicit information extraction (SPRL producing triplets), coreference linking of entity mentions, then symbolic logical inference (Prolog rules) to derive implicit relations and answer queries; optionally use LLMs (GPT-3.5-Turbo) as the extraction module in place of the trained SPRL.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>High accuracy on controlled synthetic benchmarks (near-deterministic logical chains) demonstrates rule-like multi-hop reasoning: PISTAQ with symbolic reasoner produces correct multi-step derivations (examples and chains shown in paper, e.g., using FRONT and BEHIND rules). Ablation-like upper bound GT-PISTAQ (ground-truth extraction) measures reasoning module capability separate from extraction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SPARTUN (auto synthetic): YN acc 96.37, FR acc 94.52; SPARTQA-AUTO: YN 97.56, FR 98.02 (Table 5). SPARTQA-HUMAN: YN accuracy 41.96 (trained on MSPRL) and 47.21 (with SPARTUN+MSPRL pretraining) (Table 6). RESQ (real-world): YN acc 75.52, Precision 72.11, Recall 35.93, F1 46.80 (Table 7). GT-PISTAQ (perfect extraction) gives upper bounds: e.g., GT-PISTAQ on RESQ: YN acc 79.72, P 96.38, R 66.04, F1 75.16 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Highly sensitive to extraction errors (SPRL and coreference); limited coverage of commonsense spatial knowledge and of some spatial expression variants in the symbolic rule set; in real-world datasets many questions require spatial commonsense not encoded in the reasoner, causing large performance drops; requires high-quality SPRL/coref annotations or robust extraction (error propagation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperforms end-to-end PLM baselines (BERT, BERT-EQ) and SREQA on controlled synthetic benchmarks (deterministic multi-hop logic). In real-world/resourced-limited settings, end-to-end SREQA* can outperform PISTAQ because it better tolerates extraction noise and can learn implicit commonsense from data. GT-PISTAQ shows reasoning module itself is strong when extraction is perfect.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4839.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4839.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SREQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spatial Relation Extraction QA (SREQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end neural model built on top of PLM outputs that explicitly separates extraction and reasoning inside a single network: builds entity and pair representations from BERT and applies per-relation binary classifiers to predict all relations between entity pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SREQA (BERT-based spatial relation extraction + neural reasoner)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-base encoder to produce token representations; entity mention pooling to form entity vectors; pairwise concatenation with [CLS] passed through MLPs to produce pair representations; for each relation type a 2-layer binary MLP classifier predicts relation probabilities; inference resolves inconsistent pairs (e.g., LEFT vs RIGHT). Trained in two phases: (1) relation extraction from stories using SPRL + coref-derived labels; (2) fine-tune on question relations using QA supervision. Focal loss (per-relation) used.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial Question Answering benchmarks (SPARTUN, SPARTQA-AUTO, SPARTQA-HUMAN, RESQ)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same SQA tasks requiring inference of implicit spatial relations from extracted direct relations and coreference, with multi-hop reasoning demands and coverage of many spatial relation types.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Explicit neural extraction layers: entity mention pooling and pairwise encodings to produce structured pair representations, followed by multiple binary relation classifiers (neural reasoner). Trained end-to-end across extraction and reasoning layers to learn to predict implicit relations without an external symbolic engine.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Qualitative and quantitative results: SREQA learns to predict indirect relations via neural classifiers and improves generalization in realistic domains; SREQA* (trained with SPARTUN in phase 1) improves performance on RESQ compared to SREQA trained with MSPRL, indicating learned generalization of spatial rules and some commonsense from data.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SPARTUN / SPARTQA-AUTO (synthetic): SREQA achieves lower than PISTAQ (Table 5): e.g., SREQA on SPARTUN YN 88.21, FR 83.31; SPARTQA-AUTO YN 85.11, FR 86.88. SPARTQA-HUMAN: SREQA around 53.15 acc; SREQA* Zero_shot 53.32; SREQA* (with SPARTUN pretraining) achieves 69.50 on SPARTQA-HUMAN (Table 6). RESQ: SREQA row shows YN acc 53.23, P 15.68, R 13.85, F1 13.70; SREQA* improves to YN acc 46.96, P 18.70, R 25.79, F1 24.61 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Requires SPRL and coreference annotations for phase-1 training to get good story relation supervision; performance drops when annotations are missing or noisy (MSPRL annotation issues reduce SREQA performance); neural reasoner less precise than symbolic reasoner on deterministic logical chains in synthetic controlled settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Performs worse than PISTAQ on controlled deterministic synthetic benchmarks (where symbolic rules excel), but can outperform plain PLM QA models on real-world datasets when trained with appropriate pretraining (SREQA*). SREQA* surpasses BERT and BERT-EQ on RESQ in the paper's experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4839.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4839.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT-base (end-to-end QA baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard pretrained BERT-base model fine-tuned end-to-end for question answering (input: story+question with [CLS] classification) that entangles extraction and reasoning within a single network.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base (end-to-end fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-base transformer encoder (paper cites huggingface BERT base with 768 hidden dimensions) used as a monolithic QA model: concatenated 'question+story' input passed to BERT and [CLS] token projected to answer classification layer; no explicit extraction or symbolic reasoning modules.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial Question Answering benchmarks (SPARTUN, SPARTQA-AUTO, SPARTQA-HUMAN, RESQ)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same SQA tasks requiring multi-hop spatial inference from textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>End-to-end fine-tuning on QA labels (entangles extraction and reasoning internally in BERT weights).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>BERT achieves reasonable performance on synthetic datasets indicating it can learn some spatial patterns from large synthetic supervision, but performs substantially worse than pipeline symbolic reasoning in deterministic settings and worse than humans on human-generated data. Paper presents comparisons and ablations showing BERT's weaker handling of quantifiers and multi-hop logical chains.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SPARTUN/SPARTQA-AUTO: BERT on Table 5: SPARTUN YN 91.80, FR 91.80; SPARTQA-AUTO YN 84.88, FR 94.17 (Table 5). SPARTQA-HUMAN: BERT accuracy 57.37 (Table 6). RESQ: BERT YN acc 51.74, P 30.74, R 30.13, F1 28.17 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Black-box nature makes it unclear if BERT learns explicit spatial rules; struggles with quantifiers and precise multi-hop logic; lower generalization on human-generated real-world data; cannot easily incorporate explicit commonsense rules.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Outperformed by PISTAQ on controlled synthetic benchmarks; sometimes outperformed by SREQA* on RESQ; far below human performance on SPARTQA-HUMAN.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4839.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4839.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT-EQ</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT with SPRL annotations converted to extra QA supervision (BERT-EQ)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A comparison variant where SPRL annotations are converted into extra Yes/No questions and used as additional supervision when fine-tuning BERT end-to-end, to examine if explicit extraction supervision helps without a separate extraction module.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-EQ (BERT with additional QA supervision from SPRL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>BERT-base fine-tuned on original QA examples plus additional YN questions generated from SPRL annotations (balanced positives/negatives by reversing relations in half of examples); same BERT QA classification head as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial QA benchmarks (SPARTQA variants, RESQ)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Same tasks; purpose is to compare using SPRL information as QA supervision versus explicit extraction modules.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>End-to-end QA fine-tuning with extra synthetic YN questions derived from SPRL annotations (no separate SPRL module during inference).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Used to test whether providing the same extraction supervision as QA examples helps BERT internalize spatial relations; results show BERT-EQ is less effective than explicit extraction modules in PISTAQ for leveraging SPRL supervision in controlled settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SPARTUN/SPARTQA-AUTO: BERT-EQ on Table 5 shows 90.71 (SPARTUN YN) and 85.60 (SPARTQA-AUTO FR reported as N/A in table for some columns). SPARTQA-HUMAN: BERT-EQ accuracy 56.55 (Table 6). RESQ: BERT-EQ performance reported in Table 7 (rows 8-9) shows degraded performance compared to some other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Less effective at leveraging SPRL supervision compared to an explicit extraction module (PISTAQ) because QA-format supervision does not disentangle extraction and reasoning; suffers similarly to BERT baseline in handling complex multi-hop deterministic reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>PISTAQ (explicit extraction + symbolic reasoning) outperforms BERT-EQ on controlled deterministic tasks; BERT-EQ sometimes close to BERT baseline but generally inferior to models with disentangled extraction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4839.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4839.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (davinci-003 / turbo variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large generative language model variants (davinci-003, and GPT-3.5-Turbo / ChatGPT) used in zero-shot, few-shot, and few-shot+Chain-of-Thought (CoT) prompting for SQA tasks; also employed as a few-shot information extractor feeding the PISTAQ reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 (davinci-003 and GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Generative LLM variants (OpenAI davinci-003 used for reasoning experiments and GPT-3.5-Turbo used for cheap few-shot extraction prompts); not fine-tuned due to cost, used in zero_shot, few_shot (in-context learning), and few_shot+CoT (manually written CoTs) modes. Exact parameter counts not provided in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Spatial Question Answering benchmarks (SPARTQA-HUMAN, RESQ) and used as extractor for PISTAQ on selected human-generated examples</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Human-generated SQA examples where spatial language is ambiguous and often requires commonsense; tasks include Yes/No and Find-Relation multi-hop queries.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Zero-shot and few-shot prompting (in-context examples from training data) and Chain-of-Thought prompting (manual reasoning steps) for direct answering; few-shot prompting for structured information extraction (entities, relations, coreference) to supply PISTAQ's symbolic reasoner.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Quantitative experiments show limited spatial reasoning ability in zero-shot/few-shot direct QA: GPT-3.5 davinci zero_shot underperforms on SPARTQA-HUMAN and RESQ; CoT prompting improves performance but remains below specialized models and human performance. When used as an extractor, GPT-3.5-Turbo produced higher-quality triplet extractions on sampled RESQ examples enabling PISTAQ to answer questions those LLMs alone could not.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>SPARTQA-HUMAN: GPT-3.5 zero_shot reported accuracy ~60.32? (Table 6 row 5 shows 60.32 for GPT3.5 Zero_shot) and few_shot 65.90, few_shot+CoT 67.05 (Table 6). RESQ: GPT3.5 Zero_shot 45.45 (YN acc), few_shot 60.13, few_shot+CoT 62.93; other metrics in Table 7 show mixed Precision/Recall/F1 (e.g., few_shot P 45.20 etc.). As extractor: GPT-3.5-Turbo few_shot enabled PISTAQ to correctly answer all 7 of 25 manually-sampled RESQ questions that did not require commonsense (paper reports qualitative success in those 7 cases).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Direct LLM reasoning (GPT-3.5) struggles with multi-hop deterministic spatial logic and commonsense; CoT helps but does not fully close the gap; zero/few-shot LLMs often produce 'Don't Know' or incorrect answers on ambiguous human-generated cases. Using LLMs as reasoners in a pipeline with written CoTs (LLM-based reasoner on extracted facts) performed worse than LLMs given the raw story+CoT (reported drop in one experiment: 62.62% vs 67.05%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-3.5 few_shot+CoT improves over GPT-3.5 zero_shot and often outperforms plain BERT on some human-generated datasets but typically underperforms specialized models with explicit extraction+reasoning (PISTAQ in controlled settings) and does not match human performance; using GPT-3.5 as an extractor combined with symbolic reasoning can outperform GPT-3.5 direct answering on some examples.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4839.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4839.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art large language model evaluated on a small number of SQA examples in the paper; shows mixed ability on spatial multi-hop questions and sometimes outputs 'Don't Know' or ambiguous answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large generative LLM (OpenAI GPT-4); evaluated only on a few SQA examples by the authors (limited access) using zero-shot prompts; exact model configuration and fine-tuning not performed.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Selected examples from SPARTQA-HUMAN and RESQ (human-generated SQA examples)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Natural-language descriptions of spatial scenes requiring multi-hop inference and commonsense (e.g., relative positions within blocks, touching/above/below relations).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Zero-shot evaluation (story+question prompt) and analysis of outputs; examples also used to compare behavior of different LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Examples in paper (Figure 11) show GPT-4 sometimes declines ('Don't Know') or gives inconsistent judgments on vertical/horizontal relations when the description is ambiguous; authors note GPT-4 and PaLM2 'excel in multihop reasoning' in some cases but still struggle with spatial QA overall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not systematically benchmarked in tables due to limited access; qualitative examples in Figure 11 show GPT-4 producing mixed answers (some correct, some 'Don't Know' or incorrect) on SPARTQA-HUMAN examples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited evaluation; on presented examples GPT-4 can be over-cautious or incorrect when spatial relations are ambiguous or require explicit rule application/commonsense not directly in text.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>GPT-4 sometimes performs better than GPT-3.5 on select reasoning examples but still fails to reliably solve the SQA tasks compared to specialized pipelines (PISTAQ) or human performance on the datasets evaluated.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4839.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4839.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM 2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's PaLM2 LLM evaluated on a few SQA examples alongside GPT-4 and GPT-3.5; shows some multi-hop reasoning competence but still struggles with robust spatial question answering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>PaLM2 (Google) large language model; evaluated by the authors on a small subset of SQA examples (limited access) using zero-shot prompts; no fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Selected SPARTQA-HUMAN examples (human-generated SQA)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Natural-language spatial descriptions requiring multi-hop inference.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Zero-shot prompting and qualitative comparison with GPT-4/GPT-3.5 on example stories and questions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Paper shows example outputs indicating PaLM2 can perform multi-hop reasoning in some cases but fails other spatial QA examples; overall, spatial reasoning capabilities 'do not meet the expectation' per authors' summary.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Only qualitative, few-example evaluation reported (no tabled numeric benchmarks due to limited access).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Limited evaluation; PaLM2 shows inconsistency on spatial QA examples and is not robust on the datasets used in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared qualitatively with GPT-4 and GPT-3.5 on a few examples; authors observe both GPT-4 and PaLM2 excel on some multihop tasks but remain challenged by spatial QA.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SPARTQA: A textual question answering benchmark for spatial reasoning <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination. <em>(Rating: 2)</em></li>
                <li>Thinksum: Probabilistic reasoning over sets using large language models <em>(Rating: 1)</em></li>
                <li>Large language models are strong zero-shot retriever <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4839",
    "paper_id": "paper-264451603",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "PISTAQ",
            "name_full": "Pipeline for Spatial Question Answering (PISTAQ)",
            "brief_description": "A pipeline architecture that disentangles extraction and reasoning: BERT-based spatial role labeling and coreference modules extract triplets which are fed to a Prolog-based symbolic spatial reasoner that applies explicit spatial rules to answer multi-hop spatial questions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PISTAQ (pipeline: SPRL + Prolog reasoner)",
            "model_description": "Pipeline combining BERT-based Spatial Role Labeling (SPRL) modules (BERT-base token encoder with BIO tagging and triplet classifiers; BERT-base described with 768 hidden dims in paper) for entity/role/relation extraction, a learned coreference module, and a Prolog logic-based spatial reasoner implementing rules (inverse, symmetry, transitivity, combination). Reasoner implemented in Prolog (symbolic).",
            "puzzle_name": "Spatial Question Answering benchmarks (SPARTUN, SPARTQA-AUTO, SPARTQA-HUMAN, RESQ)",
            "puzzle_description": "Textual multi-hop spatial reasoning tasks where stories describe spatial relations among entities and questions ask about implicit relations (Yes/No and Find-Relation) requiring combining explicit relations and applying spatial rules (directional, topological, distance, RCC8-like relations and quantifiers).",
            "mechanism_or_strategy": "Explicit information extraction (SPRL producing triplets), coreference linking of entity mentions, then symbolic logical inference (Prolog rules) to derive implicit relations and answer queries; optionally use LLMs (GPT-3.5-Turbo) as the extraction module in place of the trained SPRL.",
            "evidence_of_spatial_reasoning": "High accuracy on controlled synthetic benchmarks (near-deterministic logical chains) demonstrates rule-like multi-hop reasoning: PISTAQ with symbolic reasoner produces correct multi-step derivations (examples and chains shown in paper, e.g., using FRONT and BEHIND rules). Ablation-like upper bound GT-PISTAQ (ground-truth extraction) measures reasoning module capability separate from extraction errors.",
            "performance_metrics": "SPARTUN (auto synthetic): YN acc 96.37, FR acc 94.52; SPARTQA-AUTO: YN 97.56, FR 98.02 (Table 5). SPARTQA-HUMAN: YN accuracy 41.96 (trained on MSPRL) and 47.21 (with SPARTUN+MSPRL pretraining) (Table 6). RESQ (real-world): YN acc 75.52, Precision 72.11, Recall 35.93, F1 46.80 (Table 7). GT-PISTAQ (perfect extraction) gives upper bounds: e.g., GT-PISTAQ on RESQ: YN acc 79.72, P 96.38, R 66.04, F1 75.16 (Table 7).",
            "limitations_or_failure_cases": "Highly sensitive to extraction errors (SPRL and coreference); limited coverage of commonsense spatial knowledge and of some spatial expression variants in the symbolic rule set; in real-world datasets many questions require spatial commonsense not encoded in the reasoner, causing large performance drops; requires high-quality SPRL/coref annotations or robust extraction (error propagation).",
            "comparison_baseline": "Outperforms end-to-end PLM baselines (BERT, BERT-EQ) and SREQA on controlled synthetic benchmarks (deterministic multi-hop logic). In real-world/resourced-limited settings, end-to-end SREQA* can outperform PISTAQ because it better tolerates extraction noise and can learn implicit commonsense from data. GT-PISTAQ shows reasoning module itself is strong when extraction is perfect.",
            "uuid": "e4839.0"
        },
        {
            "name_short": "SREQA",
            "name_full": "Spatial Relation Extraction QA (SREQA)",
            "brief_description": "An end-to-end neural model built on top of PLM outputs that explicitly separates extraction and reasoning inside a single network: builds entity and pair representations from BERT and applies per-relation binary classifiers to predict all relations between entity pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SREQA (BERT-based spatial relation extraction + neural reasoner)",
            "model_description": "BERT-base encoder to produce token representations; entity mention pooling to form entity vectors; pairwise concatenation with [CLS] passed through MLPs to produce pair representations; for each relation type a 2-layer binary MLP classifier predicts relation probabilities; inference resolves inconsistent pairs (e.g., LEFT vs RIGHT). Trained in two phases: (1) relation extraction from stories using SPRL + coref-derived labels; (2) fine-tune on question relations using QA supervision. Focal loss (per-relation) used.",
            "puzzle_name": "Spatial Question Answering benchmarks (SPARTUN, SPARTQA-AUTO, SPARTQA-HUMAN, RESQ)",
            "puzzle_description": "Same SQA tasks requiring inference of implicit spatial relations from extracted direct relations and coreference, with multi-hop reasoning demands and coverage of many spatial relation types.",
            "mechanism_or_strategy": "Explicit neural extraction layers: entity mention pooling and pairwise encodings to produce structured pair representations, followed by multiple binary relation classifiers (neural reasoner). Trained end-to-end across extraction and reasoning layers to learn to predict implicit relations without an external symbolic engine.",
            "evidence_of_spatial_reasoning": "Qualitative and quantitative results: SREQA learns to predict indirect relations via neural classifiers and improves generalization in realistic domains; SREQA* (trained with SPARTUN in phase 1) improves performance on RESQ compared to SREQA trained with MSPRL, indicating learned generalization of spatial rules and some commonsense from data.",
            "performance_metrics": "SPARTUN / SPARTQA-AUTO (synthetic): SREQA achieves lower than PISTAQ (Table 5): e.g., SREQA on SPARTUN YN 88.21, FR 83.31; SPARTQA-AUTO YN 85.11, FR 86.88. SPARTQA-HUMAN: SREQA around 53.15 acc; SREQA* Zero_shot 53.32; SREQA* (with SPARTUN pretraining) achieves 69.50 on SPARTQA-HUMAN (Table 6). RESQ: SREQA row shows YN acc 53.23, P 15.68, R 13.85, F1 13.70; SREQA* improves to YN acc 46.96, P 18.70, R 25.79, F1 24.61 (Table 7).",
            "limitations_or_failure_cases": "Requires SPRL and coreference annotations for phase-1 training to get good story relation supervision; performance drops when annotations are missing or noisy (MSPRL annotation issues reduce SREQA performance); neural reasoner less precise than symbolic reasoner on deterministic logical chains in synthetic controlled settings.",
            "comparison_baseline": "Performs worse than PISTAQ on controlled deterministic synthetic benchmarks (where symbolic rules excel), but can outperform plain PLM QA models on real-world datasets when trained with appropriate pretraining (SREQA*). SREQA* surpasses BERT and BERT-EQ on RESQ in the paper's experiments.",
            "uuid": "e4839.1"
        },
        {
            "name_short": "BERT (baseline)",
            "name_full": "BERT-base (end-to-end QA baseline)",
            "brief_description": "Standard pretrained BERT-base model fine-tuned end-to-end for question answering (input: story+question with [CLS] classification) that entangles extraction and reasoning within a single network.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "BERT-base (end-to-end fine-tuned)",
            "model_description": "BERT-base transformer encoder (paper cites huggingface BERT base with 768 hidden dimensions) used as a monolithic QA model: concatenated 'question+story' input passed to BERT and [CLS] token projected to answer classification layer; no explicit extraction or symbolic reasoning modules.",
            "puzzle_name": "Spatial Question Answering benchmarks (SPARTUN, SPARTQA-AUTO, SPARTQA-HUMAN, RESQ)",
            "puzzle_description": "Same SQA tasks requiring multi-hop spatial inference from textual descriptions.",
            "mechanism_or_strategy": "End-to-end fine-tuning on QA labels (entangles extraction and reasoning internally in BERT weights).",
            "evidence_of_spatial_reasoning": "BERT achieves reasonable performance on synthetic datasets indicating it can learn some spatial patterns from large synthetic supervision, but performs substantially worse than pipeline symbolic reasoning in deterministic settings and worse than humans on human-generated data. Paper presents comparisons and ablations showing BERT's weaker handling of quantifiers and multi-hop logical chains.",
            "performance_metrics": "SPARTUN/SPARTQA-AUTO: BERT on Table 5: SPARTUN YN 91.80, FR 91.80; SPARTQA-AUTO YN 84.88, FR 94.17 (Table 5). SPARTQA-HUMAN: BERT accuracy 57.37 (Table 6). RESQ: BERT YN acc 51.74, P 30.74, R 30.13, F1 28.17 (Table 7).",
            "limitations_or_failure_cases": "Black-box nature makes it unclear if BERT learns explicit spatial rules; struggles with quantifiers and precise multi-hop logic; lower generalization on human-generated real-world data; cannot easily incorporate explicit commonsense rules.",
            "comparison_baseline": "Outperformed by PISTAQ on controlled synthetic benchmarks; sometimes outperformed by SREQA* on RESQ; far below human performance on SPARTQA-HUMAN.",
            "uuid": "e4839.2"
        },
        {
            "name_short": "BERT-EQ",
            "name_full": "BERT with SPRL annotations converted to extra QA supervision (BERT-EQ)",
            "brief_description": "A comparison variant where SPRL annotations are converted into extra Yes/No questions and used as additional supervision when fine-tuning BERT end-to-end, to examine if explicit extraction supervision helps without a separate extraction module.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-EQ (BERT with additional QA supervision from SPRL)",
            "model_description": "BERT-base fine-tuned on original QA examples plus additional YN questions generated from SPRL annotations (balanced positives/negatives by reversing relations in half of examples); same BERT QA classification head as baseline.",
            "puzzle_name": "Spatial QA benchmarks (SPARTQA variants, RESQ)",
            "puzzle_description": "Same tasks; purpose is to compare using SPRL information as QA supervision versus explicit extraction modules.",
            "mechanism_or_strategy": "End-to-end QA fine-tuning with extra synthetic YN questions derived from SPRL annotations (no separate SPRL module during inference).",
            "evidence_of_spatial_reasoning": "Used to test whether providing the same extraction supervision as QA examples helps BERT internalize spatial relations; results show BERT-EQ is less effective than explicit extraction modules in PISTAQ for leveraging SPRL supervision in controlled settings.",
            "performance_metrics": "SPARTUN/SPARTQA-AUTO: BERT-EQ on Table 5 shows 90.71 (SPARTUN YN) and 85.60 (SPARTQA-AUTO FR reported as N/A in table for some columns). SPARTQA-HUMAN: BERT-EQ accuracy 56.55 (Table 6). RESQ: BERT-EQ performance reported in Table 7 (rows 8-9) shows degraded performance compared to some other methods.",
            "limitations_or_failure_cases": "Less effective at leveraging SPRL supervision compared to an explicit extraction module (PISTAQ) because QA-format supervision does not disentangle extraction and reasoning; suffers similarly to BERT baseline in handling complex multi-hop deterministic reasoning.",
            "comparison_baseline": "PISTAQ (explicit extraction + symbolic reasoning) outperforms BERT-EQ on controlled deterministic tasks; BERT-EQ sometimes close to BERT baseline but generally inferior to models with disentangled extraction.",
            "uuid": "e4839.3"
        },
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (davinci-003 / turbo variants)",
            "brief_description": "Large generative language model variants (davinci-003, and GPT-3.5-Turbo / ChatGPT) used in zero-shot, few-shot, and few-shot+Chain-of-Thought (CoT) prompting for SQA tasks; also employed as a few-shot information extractor feeding the PISTAQ reasoner.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5 (davinci-003 and GPT-3.5-Turbo)",
            "model_description": "Generative LLM variants (OpenAI davinci-003 used for reasoning experiments and GPT-3.5-Turbo used for cheap few-shot extraction prompts); not fine-tuned due to cost, used in zero_shot, few_shot (in-context learning), and few_shot+CoT (manually written CoTs) modes. Exact parameter counts not provided in paper.",
            "puzzle_name": "Spatial Question Answering benchmarks (SPARTQA-HUMAN, RESQ) and used as extractor for PISTAQ on selected human-generated examples",
            "puzzle_description": "Human-generated SQA examples where spatial language is ambiguous and often requires commonsense; tasks include Yes/No and Find-Relation multi-hop queries.",
            "mechanism_or_strategy": "Zero-shot and few-shot prompting (in-context examples from training data) and Chain-of-Thought prompting (manual reasoning steps) for direct answering; few-shot prompting for structured information extraction (entities, relations, coreference) to supply PISTAQ's symbolic reasoner.",
            "evidence_of_spatial_reasoning": "Quantitative experiments show limited spatial reasoning ability in zero-shot/few-shot direct QA: GPT-3.5 davinci zero_shot underperforms on SPARTQA-HUMAN and RESQ; CoT prompting improves performance but remains below specialized models and human performance. When used as an extractor, GPT-3.5-Turbo produced higher-quality triplet extractions on sampled RESQ examples enabling PISTAQ to answer questions those LLMs alone could not.",
            "performance_metrics": "SPARTQA-HUMAN: GPT-3.5 zero_shot reported accuracy ~60.32? (Table 6 row 5 shows 60.32 for GPT3.5 Zero_shot) and few_shot 65.90, few_shot+CoT 67.05 (Table 6). RESQ: GPT3.5 Zero_shot 45.45 (YN acc), few_shot 60.13, few_shot+CoT 62.93; other metrics in Table 7 show mixed Precision/Recall/F1 (e.g., few_shot P 45.20 etc.). As extractor: GPT-3.5-Turbo few_shot enabled PISTAQ to correctly answer all 7 of 25 manually-sampled RESQ questions that did not require commonsense (paper reports qualitative success in those 7 cases).",
            "limitations_or_failure_cases": "Direct LLM reasoning (GPT-3.5) struggles with multi-hop deterministic spatial logic and commonsense; CoT helps but does not fully close the gap; zero/few-shot LLMs often produce 'Don't Know' or incorrect answers on ambiguous human-generated cases. Using LLMs as reasoners in a pipeline with written CoTs (LLM-based reasoner on extracted facts) performed worse than LLMs given the raw story+CoT (reported drop in one experiment: 62.62% vs 67.05%).",
            "comparison_baseline": "GPT-3.5 few_shot+CoT improves over GPT-3.5 zero_shot and often outperforms plain BERT on some human-generated datasets but typically underperforms specialized models with explicit extraction+reasoning (PISTAQ in controlled settings) and does not match human performance; using GPT-3.5 as an extractor combined with symbolic reasoning can outperform GPT-3.5 direct answering on some examples.",
            "uuid": "e4839.4"
        },
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "State-of-the-art large language model evaluated on a small number of SQA examples in the paper; shows mixed ability on spatial multi-hop questions and sometimes outputs 'Don't Know' or ambiguous answers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Large generative LLM (OpenAI GPT-4); evaluated only on a few SQA examples by the authors (limited access) using zero-shot prompts; exact model configuration and fine-tuning not performed.",
            "puzzle_name": "Selected examples from SPARTQA-HUMAN and RESQ (human-generated SQA examples)",
            "puzzle_description": "Natural-language descriptions of spatial scenes requiring multi-hop inference and commonsense (e.g., relative positions within blocks, touching/above/below relations).",
            "mechanism_or_strategy": "Zero-shot evaluation (story+question prompt) and analysis of outputs; examples also used to compare behavior of different LLMs.",
            "evidence_of_spatial_reasoning": "Examples in paper (Figure 11) show GPT-4 sometimes declines ('Don't Know') or gives inconsistent judgments on vertical/horizontal relations when the description is ambiguous; authors note GPT-4 and PaLM2 'excel in multihop reasoning' in some cases but still struggle with spatial QA overall.",
            "performance_metrics": "Not systematically benchmarked in tables due to limited access; qualitative examples in Figure 11 show GPT-4 producing mixed answers (some correct, some 'Don't Know' or incorrect) on SPARTQA-HUMAN examples.",
            "limitations_or_failure_cases": "Limited evaluation; on presented examples GPT-4 can be over-cautious or incorrect when spatial relations are ambiguous or require explicit rule application/commonsense not directly in text.",
            "comparison_baseline": "GPT-4 sometimes performs better than GPT-3.5 on select reasoning examples but still fails to reliably solve the SQA tasks compared to specialized pipelines (PISTAQ) or human performance on the datasets evaluated.",
            "uuid": "e4839.5"
        },
        {
            "name_short": "PaLM2",
            "name_full": "PaLM 2",
            "brief_description": "Google's PaLM2 LLM evaluated on a few SQA examples alongside GPT-4 and GPT-3.5; shows some multi-hop reasoning competence but still struggles with robust spatial question answering.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "PaLM2",
            "model_description": "PaLM2 (Google) large language model; evaluated by the authors on a small subset of SQA examples (limited access) using zero-shot prompts; no fine-tuning.",
            "puzzle_name": "Selected SPARTQA-HUMAN examples (human-generated SQA)",
            "puzzle_description": "Natural-language spatial descriptions requiring multi-hop inference.",
            "mechanism_or_strategy": "Zero-shot prompting and qualitative comparison with GPT-4/GPT-3.5 on example stories and questions.",
            "evidence_of_spatial_reasoning": "Paper shows example outputs indicating PaLM2 can perform multi-hop reasoning in some cases but fails other spatial QA examples; overall, spatial reasoning capabilities 'do not meet the expectation' per authors' summary.",
            "performance_metrics": "Only qualitative, few-example evaluation reported (no tabled numeric benchmarks due to limited access).",
            "limitations_or_failure_cases": "Limited evaluation; PaLM2 shows inconsistency on spatial QA examples and is not robust on the datasets used in this paper.",
            "comparison_baseline": "Compared qualitatively with GPT-4 and GPT-3.5 on a few examples; authors observe both GPT-4 and PaLM2 excel on some multihop tasks but remain challenged by spatial QA.",
            "uuid": "e4839.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SPARTQA: A textual question answering benchmark for spatial reasoning",
            "rating": 2,
            "sanitized_title": "spartqa_a_textual_question_answering_benchmark_for_spatial_reasoning"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination.",
            "rating": 2,
            "sanitized_title": "a_multitask_multilingual_multimodal_evaluation_of_chatgpt_on_reasoning_hallucination"
        },
        {
            "paper_title": "Thinksum: Probabilistic reasoning over sets using large language models",
            "rating": 1,
            "sanitized_title": "thinksum_probabilistic_reasoning_over_sets_using_large_language_models"
        },
        {
            "paper_title": "Large language models are strong zero-shot retriever",
            "rating": 1,
            "sanitized_title": "large_language_models_are_strong_zeroshot_retriever"
        }
    ],
    "cost": 0.018149,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning
25 Oct 2023</p>
<p>Roshanak Mirzaee mirzaeem@msu.edu 
Michigan State university</p>
<p>Parisa Kordjamshidi 
Michigan State university</p>
<p>Disentangling Extraction and Reasoning in Multi-hop Spatial Reasoning
25 Oct 20230828015732604F31F481F2E5532EEB0FarXiv:2310.16731v1[cs.CL]CLStrajSEPindicSEPlandSEPsentenceSEP]
Spatial reasoning over text is challenging as the models not only need to extract the direct spatial information from the text but also reason over those and infer implicit spatial relations.Recent studies highlight the struggles even large language models encounter when it comes to performing spatial reasoning over text.In this paper, we explore the potential benefits of disentangling the processes of information extraction and reasoning in models to address this challenge.To explore this, we design various models that disentangle extraction and reasoning (either symbolic or neural) and compare them with state-of-the-art (SOTA) baselines with no explicit design for these parts.Our experimental results consistently demonstrate the efficacy of disentangling, showcasing its ability to enhance models' generalizability within realistic data domains.</p>
<p>Introduction</p>
<p>Despite the high performance of recent pretrained language models on question-answering (QA) tasks, solving questions that require multi-hop reasoning is still challenging (Mavi et al., 2022).In this paper, we focus on spatial reasoning over text which can be described as inferring the implicit 1 spatial relations from explicit relations 2 described in the text.Spatial reasoning plays a crucial role in diverse domains, including language grounding (Liu et al., 2022), navigation (Zhang et al., 2021), and human-robot interaction (Venkatesh et al., 2021).By studying this task, we can analyze both the reading comprehension and logical reasoning capabilities of models.</p>
<p>Previous work has investigated the use of general end-to-end deep neural models such as pretrained language models (PLM) (Mirzaee et al., 2021) in 1 By implicit, we mean indirect relations, not metaphoric usages or implicit meaning for the relations.</p>
<p>2 relationships between objects and entities in the environment, such as location, distance, and relative position.(1,2).Next, we apply rules such as INVERSE to deduce implicit triplets (3,4,5).Then, utilizing triplets 5 and 6 we determine the final answer, 'Yes'.NTPP: Non-Tangential Proper Part (Table 1).spatial question answering (SQA).PLMs show reasonable performance on the SQA problem and can implicitly learn spatial rules from a large set of training examples.However, the black-box nature of PLMs makes it unclear whether these models are making the abstractions necessary for spatial reasoning or their decisions are based solely on patterns observed in the data.</p>
<p>As a solution for better multi-hop reasoning, recent research has investigated the impact of using fine-grained information extraction modules such as Named Entity Recognition (NER) (Moll et al., 2006;Mendes et al., 2010), gated Entity/Relation (Zheng and Kordjamshidi, 2021) or semantic role labels (SRL) (Shen and Lapata, 2007;Faghihi et al., 2023) on the performance of models.On a different thread, cognitive studies (Stenning and Van Lambalgen, 2012;Dietz et al., 2015) show when the given information is shorter, humans also find spatial abstraction and use spatial rules to infer implicit information.Figure 1 shows an example of such extractions.Building upon these findings, we aim to address the limitations of end-to-end models and capitalize on the advantages of fine-grained information extraction in solving SQA.Thus, we propose models which disentangle the language understanding and spatial reasoning computations as two separate components.Specifically, we first design a pipeline model that includes trained neural modules for extracting direct fine-grained spatial information from the text and performing symbolic spatial reasoning over them.</p>
<p>The second model is simply an end-to-end PLM that uses annotations used in extraction modules of pipeline model in the format of extra QA supervision.This model aims to demonstrate the advantages of using separate extraction modules compared to a QA-based approach while utilizing the same amount of supervision.Ultimately, the third model is an end-to-end PLM-based model on relation extraction tasks that has explicit latent layers to disentangle the extraction and reasoning inside the model.This model incorporates a neural spatial reasoner, which is trained to identify all spatial relations between each pair of entities.</p>
<p>We evaluate the proposed models on multiple SQA datasets, demonstrating the effectiveness of the disentangling extraction and reasoning approach in controlled and realistic environments.Our pipeline outperforms existing SOTA models by a significant margin on benchmarks with a controlled environment (toy tasks) while utilizing the same or fewer training data.However, in realworld scenarios with higher ambiguity of natural language for extraction and more rules to cover, our end-to-end model with explicit layers for extraction and reasoning performs better.</p>
<p>These results show that disentangling extraction and reasoning benefits deterministic spatial reasoning and improves generalization in realistic domains despite the coverage limitations and sensitivity to noises in symbolic reasoning.These findings highlight the potential of leveraging language models for information extraction tasks and emphasize the importance of explicit reasoning modules rather than solely depending on black-box neural models for reasoning.</p>
<p>Related Research</p>
<p>End-to-end model on SQA: To solve SQA tasks, recent research evaluates the performance of different deep neural models such as Memory networks (Shi et al., 2022;Sukhbaatar et al., 2015), Self-attentive Associative Memory (Le et al., 2020), subsymbolic fully connected neural network (Zhu et al., 2022), and Recurrent Relational Network (RRN) (Palm et al., 2017).Mirzaee and Kordjamshidi; Mirzaee et al. use transfer learning and provide large synthetic supervision that enhances the performance of PLMs on spatial question answering.However, the results show a large gap between models and human performance on humangenerated data.Besides, none of these models use explicit spatial semantics to solve the task.The only attempt towards integrating spatial semantics into spatial QA task is a baseline model introduced in (Mirzaee et al., 2021), which uses rulebased spatial semantics extraction for reasoning on bAbI (task 17) which achieves 100% accuracy without using any training data.</p>
<p>Extraction and Reasoning: While prior research has extensively explored the use of end-to-end models for learning the reasoning rules (Minervini et al., 2020;Qu et al., 2021), there is limited discussion on separating the extraction and reasoning tasks.Nye et al. utilizes LMs to generate new sentences and extract facts while using some symbolic rules to ensure consistency between generated sentences.Similarly, ThinkSum (Ozturkler et al., 2022) uses LMs for knowledge extraction (Think) and separate probabilistic reasoning (Sum), which sums the probabilities of the extracted information.However, none of these works are on multi-step or spatial Reasoning.</p>
<p>Proposed Models</p>
<p>To understand the effectiveness of disentangling the extraction and reasoning modules, we provide three groups of models.The first model is a pipeline of extraction and symbolic reasoning ( 3.1), the second model is an end-to-end PLM that uses the same spatial information supervision but in a QA format ( 3.2), and the third model is an end-toend neural model with explicit layers of extraction and reasoning ( 3.3).We elaborate each of these models in the subsequent sections.</p>
<p>Task The target task is spatial question answering (SQA), which assesses models' ability to comprehend spatial language and reason over it.Each example includes a textual story describing entities and their spatial relations, along with questions asking an implicit relation between entities (e.g., Figure 1).SQA benchmarks provide two types of questions: YN (Yes/No) queries about the existence of a relation between two groups of entities, and FR (Find Relation) seeks to identify all possible (direct/indirect) relations between them.The answer to these questions is chosen from a provided candidate list.For instance, the candidate list for FR questions can be a sublist of all relation types in Table 1.</p>
<p>Pipeline of Extraction and Reasoning</p>
<p>Here, we describe our suggested pipeline model designed for spatial question answering task, referred to as PISTAQ 3 .As shown in the extraction part of Figure 2, the spatial information is extracted first and forms a set of triplets for a story (Facts) and a question (Query).Then a coreference resolution module is used to connect these triplets to each other.Given the facts and queries, the spatial reasoner infers all implicit relations.The answer generator next conducts the final answer.Below we describe each module in more detail.Spatial Role Labeling (SPRL) is the task of identifying and classifying the spatial roles of phrases within a text (including the Trajector, Landmark, and Spatial Indicator) and formalizing their relations (Kordjamshidi et al., 2010).Here, we use the same SPRL modules as in (Mirzaee and Kordjamshidi, 2022).This model first computes the token representation of a story and its question using a BERT model.Then a BIO tagging layer is applied on the tokens representations using (O, B-  2) and spatial indicators (e.g., 'in front of' in Figure 2).Given the output of the spatial role extraction module, for each combination of (Trajector, Spatial indicator, Landmark) in each sentence, we create a textual input 5 and pass it to a BERT model.To indicate the position of each spatial role in the sentence, we use segment embeddings and add 1 if it is a role position and 0 otherwise.The [CLS] output of BERT will be passed to a one-layer MLP that provides the probability for each triplet.To apply the logical rules on the triplets, we need to assign a relation type to each triplet.To this aim, we use another multiclassification layer on the same [CLS] token to identify the spatial types of the triplet.The classes are relation types in Table 1 alongside a class NaN for triplet with no spatial meaning.For instance, in Figure 2, (grey car, in front of, grey house) is a triplet with F RON T as its relation type while (grey house, in front of, grey car) is not a triplet and its relation type is N aN .We use a joint loss function for triplet and relation type classification to train the model.Coreference Resolution Linking the extracted triplets from the stories is another important step required in this task, as different phrases or pronouns may refer to same entity.To make such connections, we implement a coreference resolu-   (Mirzaee and Kordjamshidi, 2022).This reasoner is implemented in Prolog and utilizes a set of rules on various relation types, as illustrated in Table 2. Given the facts and queries in Prolog format, the spatial reasoner can carry out the reasoning process and provide an answer to any given query.The reasoner matches variables in the program with concrete values and a backtracking search to explore different possibilities for each rule until a solution is found.As shown in Figure 2, the reasoner uses a FRONT and a BEHIND rules over the facts and generates the True response for the query.
Not (X, Y )  Entities R  {Dir  P P } IF R(X, Y )  NOT(R_reverse(X, Y )) Inverse (X, Y )  Entities R  {Dir  P P } IF R(Y, X)  R_reverse(X, Y ) Symmetry (X, Y )  Entities R  {Dis  (RCC  P P )} IF R(Y, X)  R(X, Y ) Transitivity (X, Y, Z)  Entities R  {Dir  P P } IF R(X, Z), R(Z, Y )  R(X, Y ) Combination (X, Y, Z, H)  Entities R </p>
<p>PLMs Using SPRL Annotations</p>
<p>To have a fair comparison between the QA baselines and models trained on SPRL supervision, we design BERT-EQ 6 .We convert the SPRL annotation into extra YN questions 7 asking about explicit relations between a pair of entities triplet relation type.To have equal positive and negative questions, we reverse the relation in half of the questions.We train BERT-EQ using both original and extra questions by passing the "question+story" into a BERT with answers classification layers.</p>
<p>PLMs with Explicit Extractions</p>
<p>As another approach, we aim to explore a model that disentangles the extraction and reasoning parts inside a neural model.Here, rather than directly predicting the answer from the output of PLMs (as typically done in the QA task), we introduce explicit layers on top of PLM outputs.These layers are designed to generate representations for entities and pairs of entities, which are then passed to neural layers to identify all relations.We call this model SREQA 8 , which is an end-to-end spatial relation extraction model designed for QA. Figure 3 illustrates the structure of this model.</p>
<p>In this model, we first select the entity mentions (M j (E 1 )) from the BERT tokens representation and pass it to the extraction part shown in Figure 3a.Next, the model computes entity representation (M (E 1 )) by summing the BERT token representations of all entity's mentions and passing it to an MLP layer.Then for each pair of entities, a triplet is created by concatenating the pair's entities representations and the BERT [CLS] token representation.This triplet is passed through an MLP layer to compute the final pair representations.Next, in the reasoning part in Figure 3a, for each relation type in Table 1, we use a binary 2-layer MLP classifier to predict the probability of each relation between the pairs.We remove the inconsistent relations by selecting one with a higher probability at inference time, e.g., LEFT and RIGHT cannot be true at the same time.The final output is a list of all possible relations for each pair.This model is trained using the summation of Focal loss (Lin et al., 2017) of all relation classifiers....  We train SREQA in two separate steps.In the first step, the model is trained on a relation extraction task which extracts all direct and indirect relations between each pair of entities only from stories.The top row of Figure 3b shows the annotation and modules employed in this step to gather the necessary supervision.We use the entity and coreference annotation to select the entity mentions from the BERT output.To compute the relations supervision for each pair of entities, we employ the spatial reasoner from PISTAQ and apply it to the direct relations (triplets) from the SPRL annotation, which are connected to each other by coreference annotations.This step of training is only feasible for datasets with available SPRL and coreference annotations.
M n (E 1 ) M 1 (E 2 ) M n (E 2 ) M-E 1 M-E 2 [CLS]</p>
<p>RIGHT</p>
<p>In the next step, we further train SREQA on extracting questions relation using QA supervision.As shown in the bottom row of Figure 3b, we employ the trained spatial role extraction model used in PISTAQ to identify the entities in the question and use R-Coref to find the mentions of these entities in the story.To obtain the relation supervision, we convert the question answers to relation labels.In FR, the label is similar to the actual answer, which is a list of all relations.In YN, the question relation is converted to a label based on the Yes/No answer.For example, in Figure 3a, the question relation is 'BEHIND,' and the answer is Yes, so the label for the BEHIND classifier is 1.</p>
<p>We evaluate the SREQA model's performance in predicting the accurate answers of the test set's questions same as training step 2.</p>
<p>Experiments</p>
<p>Datasets</p>
<p>SPARTQA is an SQA dataset in which examples contain a story and multiple YN9 and FR questions that require multi-hop spatial reasoning to be answered.The stories in this dataset describe relations between entities in a controlled (toy task) environment.This dataset contains a large synthesized part, SPARTQA-AUTO, and a more complex small human-generated subset, SPARTQA-HUMAN.All stories and questions in this dataset also contain SPRL annotations.SPARTUN is an extension of SPARTQA with YN and FR questions containing SPRL annotations.Compared to SPARTQA, the vocabulary and relation types in this dataset are extended, and it covers more relation types, rules, and spatial expressions to describe relations between entities.RESQ is an SQA dataset with Yes/No questions over the human-generated text describing spatial relations in real-world settings.The texts of this dataset are collected from MSPRL dataset (Kordjamshidi et al., 2017) (See Figure 1), which describe some spatial relations in pictures of Im-ageCLEF (Grubinger et al., 2006).Also, the MSPRL dataset already contains some SPRL annotations.To answer some of the questions in this dataset, extra spatial-commonsense information is needed (e.g., a roof is on the top of buildings).</p>
<p>Model Configurations &amp; Baselines</p>
<p>We compare the models described in section 3 with the following baselines.Majority Baseline: This baseline selects the most frequent answer(s) in each dataset.GT-PISTAQ: This model uses ground truth (GT) values of all involved modules in PISTAQ to eliminate the effect of error propagation in the pipeline.This baseline is used to evaluate the alignments between the questions and story entities and the reasoning module in solving the QA task.It also gives an upper bound for the performance of the pipeline model, as the extraction part is perfect.
Model Supervisions Rule-based Modules BERT QA - GPT3.5 zero_shot - - GPT3.5 f ew_shot QA(8 ex) - GPT3.5 f ew_shot +CoT QA(8 ex) + CoT - BERT-EQ QA +SpRL(S) - SREQA QA +SpRL(all)+Coref Reasoner, R-Coref SREQA* QA + SpRL(Q) R-Coref PISTAQ SpRL(all) + Coref Reasoner, R-Coref PISTAQ zero_shot - Reasoner, R-Coref
BERT: We select BERT as a candidate PLM that entangles the extraction and reasoning steps.In this model, the input of the "question+story" is passed to the BERT, and the [CLS] representation is used to do the answer classification.GPT3.5:GPT3.5 (Brown et al., 2020) baselines (GPT3.5 text-davinci-003) is selected as a candidate of generative larger language models which already passes many SOTAs in reasoning tasks (Bang et al., 2023;Kojima et al., 2022).We use Zero_shot and F ew_shot (In-context learning with few examples) settings to evaluate this model on the human-generated benchmarks.We also evaluate the Chain-of-Thoughts (CoT) prompting method (Wei et al., 2022) to extend the prompts with manually-written reasoning steps.The format of the input and some prompt examples are presented in Appendix E.</p>
<p>Transfer learning has already demonstrated significant enhancements in numerous deep learning tasks (Soroushmojdehi et al., 2022;Rajaby Faghihi and Kordjamshidi, 2021).Thus, when applicable, we further train models on SPARTUN synthetic data shown by "*".The datasets' examples and statistics and more details of the experimental setups and configurations are provided in Appendix A and B. All codes are publicly available at https://github.com/RshNk73/PistaQ-SREQA.</p>
<p>Results and Discussion</p>
<p>Here, we discuss the influence of disentangling extraction and reasoning manifested in PISTAQ and SREQA models compared to various end-to-end models with no explicit design for these modules, such as BERT, BERT-EQ, and GPT3.5.Table 3 shows the list of these models with the sources of their supervision as well as extra off-the-shelf or rule-based modules employed in them.</p>
<p>Since the performance of extraction modules, Spatial Role Labeling (SPRL) and Coreference Resolution (Coref), directly contribute to the final accuracy of the designed models, we have evaluated these modules and reported the results in Table 4.We choose the best modules on each dataset for experiments.For a detailed discussion on the performance of these modules, see Appendix C.</p>
<p>Result on Controlled Environment</p>
<p>Table 5 shows the performance of models on two auto-generated benchmarks, SPARTUN and SPARTQA-AUTO.We can observe that PISTAQ outperforms all PLM baselines and SREQA.This outcome first highlights the effectiveness of the extraction and symbolic reasoning pipeline compared to PLMs in addressing deterministic reasoning within a controlled environment.Second, it shows that disentangling extraction and reasoning as a pipeline works better than explicit neural layers in SQA with a controlled environment.The complexity of these environments is more related to conducting several reasoning steps and demands accurate logical computations where a rule-based reasoner excels.Thus, the result of PISTAQ with a rule-based reasoner module is also higher than SREQA with a neural reasoner.</p>
<p>The superior performance of PISTAQ over BERT suggests that SPRL annotations are more effective in the PISTAQ pipeline than when utilized in BERT-EQ in the form of QA supervision.Note that the extraction modules of PISTAQ achieve perfect results on auto-generated benchmarks while trained only on a quarter of the SPRL annotations as shown in Table 5.However, BERT-EQ uses all the original dataset questions and extra questions created from the full SPRL annotations.</p>
<p>Table 6 demonstrates the results of models on SPARTQA-HUMAN with a controlled environment setting.As can be seen, our proposed pipeline, PISTAQ, outperforms the PLMs by a margin of 15% on YN questions, even though the extraction modules, shown in Table 4, perform low.This low performance is due to the ambiguity of human language and smaller training data.We also evaluate PISTAQ on SPARTQA-HUMAN FR questions using Macro_f1 score on all relation types.PISTAQ outperforms all other baselines on FR questions, except for BERT*.</p>
<p>There are two main reasons behind the inconsistency in performance between YN and FR question types.The first reason is the complexity of the YN questions, which goes beyond the basics of spatial reasoning and is due to using quantifiers (e.g., all circles, any object).While previous studies have demonstrated that PLMs struggle with quantifiers (Mirzaee et al., 2021), the reasoning module in PISTAQ can adeptly handle them without any performance loss.Second, further analysis indicates that PISTAQ predicts 'No' when a relationship is not extracted, which can be correct when the answer is 'No'.However, in FR, a missed extraction causes a false negative which decreases F1 score.</p>
<p>Results on Real-world Setting</p>
<p>We select RESQ as an SQA dataset with realistic settings and present the result of models on this dataset in Table 7.</p>
<p>To evaluate PISTAQ on RESQ, we begin by adapting its extraction modules through training on the corresponding dataset.We train the SPRL modules on both MSPRL and SPARTUN, and the performance of these models is presented in Table 4.As the MSPRL dataset lacks coreference an- notations, we employ the model trained on SPAR-TUN for this purpose.Rows 14 and 15 in Table 7 show the performance of the PISTAQ on RESQ is inferior compared to other baselines.To find the reason, we analyze the first 25 questions from the RESQ test set.We find that 18 out of 25 questions required spatial commonsense information and cannot be answered solely based on the given relations in the stories.From the remaining 7 questions, only 2 can be answered using the SPRL annotations provided in the MSPRL dataset.Some examples of this analysis are provided in Appendix D. Hence, the low performance of PISTAQ is attributed to first the absence of integrating commonsense information in this model and, second, the errors in the</p>
<p>SpaRTQA-Human</p>
<p>Story: There is a block called A. Inside there's a red oval that is above a blue triangle ...There is a block called B which is to the right and touching A. Inside B there are 5 triangles, a red one, a green one, a large yellow one, a brown one, and an orange one .. Q2: There are five triangles mentioned in block B, a red one, a green one, a large yellow one, a brown one, and an orange one.All of these triangles are in block B, so the answer is Yes.extraction modules, which are propagated to the reasoning modules.</p>
<p>As shown in Table 7, the best result on RESQ is achieved by SREQA<em> model.Compared to SREQA, SREQA</em> is trained on SPARTUN instead of MSPRL10 in the first step of the training.MSPRL lacks some SPRL and coreference annotations to answer RESQ questions.In the absence of this information, collecting the supervision for the first phase of training results in a significant number of missed relations.Therefore, as shown in row 11 of Table 7, employing MSPRL in the first training phase decreases the performance while replacing it with SPARTUN in SREQA* significantly enhances the results.</p>
<p>SREQA* surpasses the PLMs trained on QA and QA+SPRL annotation, showcasing the advantage of the design of this model in utilizing QA and SPRL data within explicit extraction layers and the data preprocessing.Also, the better performance of this model compared to PISTAQ demonstrates how the end-to-end structure of SREQA can handle the errors from the extraction part and also can capture some rules and commonsense knowledge from RESQ training data that are not explicitly supported in the symbolic reasoner.</p>
<p>In conclusion, compared to PLMs, disentangling extraction and reasoning as a pipeline indicates su-perior performance in deterministic spatial reasoning within controlled settings.Moreover, explicitly training the extraction module proves advantageous in leveraging SPRL annotation more effectively compared to using this annotation in QA format in the end-to-end training.Comparison between disentangling extraction and reasoning as a pipeline and incorporating them within an end-to-end model demonstrates that the end-to-end model performs better in realistic domains even better than PLMs.The end-to-end architecture of this model effectively enhances the generalization in the real-world setting and addresses some of the limitations of rule coverage and commonsense knowledge.</p>
<p>LLMs on Spatial Reasoning</p>
<p>Recent research shows the high performance of LLMs with zero/f ew_shot setting on many tasks (Chowdhery et al., 2022;Brown et al., 2020).However, (Bang et al., 2023) shows that Chat-GPT (GPT3.5-Turbo)with zero_shot evaluation cannot perform well on SQA task using SPARTQA-HUMAN test cases.Similarly, our experiments, as shown in Tables 6 and 7, show the lower performance of GPT3.5 (davinci) with zero/f ew_shot settings compared to human and our models PIS-TAQ and SREQA.a photo of a room with white walls , two single beds with a night table in between and a picture on the wall above the beds .including zero_shot examples is presented in Appendix E).Although Wei et al. shows that using CoT prompting improves the performance of PaLM on multi-step reasoning task, its spatial reasoning capabilities still does not meet the expectation.</p>
<p>LLMs as Extraction Module in PISTAQ</p>
<p>A recent study (Shen et al., 2023) shows that LLMs have a promising performance in information retrieval.Building upon this, we employ LLM, GPT3.5-Turbo with f ew_shot prompting to extract information from a set of SPARTQA-HUMAN and RESQ examples that do not necessitate commonsense reasoning for answering.The extracted information is subsequently utilized within the framework of PISTAQ.</p>
<p>The results, illustrated in the last row of Figure 4, highlight how the combination of LLM extraction and symbolic reasoning enables answering questions that LLMs struggle to address.Furthermore, Figure 5 provides a comparison between the trained BERT-based SPRL extraction modules and GPT3.5 with f ew_shot prompting in PISTAQ.It is evident that GPT3.5 extracts more accurate information, leading to correct answers.As we mentioned before, out of 25 sampled questions from RESQ, only 7 can be solved without relying on spatial commonsense information.Our experimental result shows that PISTAQ using LLM as extraction modules can solve all of these 7 questions.</p>
<p>Based on these findings, leveraging LLMs in PISTAQ to mitigate errors stemming from the SPRL extraction modules rather than relying solely on LLMs for reasoning can be an interesting future research direction.This insight emphasizes the importance of considering new approaches for incorporating explicit reasoning modules whenever possible instead of counting solely on black-box SQA tasks.neural models for reasoning tasks.</p>
<p>Conclusion and Future Works</p>
<p>We investigate the benefits of disentangling the processes of extracting spatial information and reasoning over them.To this end, we devised a series of experiments utilizing PLMs for spatial information extraction coupled with a symbolic reasoner for inferring indirect relations.The outcomes of our experiments provide noteworthy insights: (1) Our observations in controlled experimental conditions demonstrate that disentangling extraction and symbolic reasoning, compared to PLMs, enhances the models' reasoning capabilities, even with comparable or reduced supervision.(2) Despite the acknowledged fragility of symbolic reasoning in real-world domains, our experiments highlight that employing explicit extraction layers and utilizing the same symbolic reasoner in data preprocessing enhances the reasoning capabilities of models.(3) Despite the limitations of LLMs in spatial reasoning, harnessing their potential for information extraction within a disentangled structure of Extraction and Reasoning can yield significant benefits.All of these results emphasize the advantage of disentangling the extraction and reasoning in spatial language understanding.</p>
<p>In future research, an intriguing direction is incorporating spatial commonsense knowledge using LLMs as an extraction module in the pipeline of extraction and reasoning.Additionally, the model's applicability extends beyond spatial reasoning, making it suitable for various reasoning tasks involving logical rules, such as temporal or arithmetic reasoning.This project is partially supported by the National Science Foundation (NSF) CAREER award 202826.Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.We thank all reviewers for their helpful comments and suggestions.We would like to express our gratitude to Hossein Rajaby Faghihi for his valuable discussions and input to this paper.</p>
<p>Limitations</p>
<p>Our model is evaluated on a Spatial Reasoning task using specifically designed spatial logical rules.</p>
<p>However, this methodology can be readily extended to other reasoning tasks that involve a limited set of logical rules, which can be implemented using logic programming techniques.The extraction modules provided in this paper are task-specific and do not perform well on other domains, but they can be fine-tuned on other tasks easily.Using LLM in the extraction phase can also deal with this issue.Also, using MSPRL annotation on RESQ(which this data is provided on) decreases the performance of our models.This annotation does not contain the whole existing relations in the context.The evaluation of the reasoning module is based on the existing datasets.However, we cannot guarantee that they cover all possible combinations between spatial rules and relation types.Many questions in RESQ need spatial commonsense to be answered.As a result, due to the limitation of our symbolic spatial reasoner, the performance of the pipeline model is much lower than what we expected.Due to the high cost of training GPT3.5 on large synthetic data, we cannot fine-tune the whole GPT3.5 and only provide the GPT3.5 with F ew_shot learning on small human-generated benchmarks.Also, due to the limited access, we can only test PaLM2 and GPT4 on a few examples.</p>
<p>A Statistic Information</p>
<p>This section presents statistical information regarding dataset sizes and additional analyses conducted on the evaluation sets of human-generated datasets.</p>
<p>Table 8 provides the number of questions in the training and evaluation sets of the SQA benchmarks.Tables 9 and 10 present the sentence and number of relation triplets within the SPRL annotation for each dataset, respectively.</p>
<p>A.2 SPRL Annotations in MSPRL</p>
<p>RESQ is built on the human-written context of MSPRL dataset which includes SPRL annotations.However, using this annotation in BERT-EQ and SREQA models causes lower performance (Check the result on Tablel 14).Our analysis shows that the Triplet: Behind (a bench (id: t2), behind, it (id: l1))</p>
<p>Triplet: EC (one person (id: t3), on, it (id: l2)) Figure 7: An example of the limitation of MSPRL and coreference annotation to answer RESQ question.The answer to the questions was predicted wrongly due to two main reasons.First, the missed commonsense knowledge in question 2, and second, the limited coverage of ground truth annotation in MSPRL in question 2.</p>
<p>SPRL annotations of MSPRL are not fully practical in our work due to two main reasons:</p>
<ol>
<li>Missed annotations: As shown in Figure 7, there are many missed annotations for each text, e.g., NTPP(bar, with, chair).</li>
</ol>
<p>No coreference :</p>
<p>The coreference annotation is not supported in this dataset, e.g., "L2: it" and "T2: a bench" are the same entity with different mentions, but they are mentioned with different ids.These missed coreferences result in fewer connections between entities and fewer inferred relations.</p>
<p>B Models and modules configuration</p>
<p>We use huggingFace12 implementation of pretrained BERT base models, which have 768 hidden dimensions.All models are trained on the training set, evaluated on the dev set, and reported the result on the test set.For training, we train the model until no changes happen on the dev set and then store and use the best model on the dev set.We use AdamW ( (Loshchilov and Hutter, 2017)), and learning rates from 2  10 6 , 2  10 5 (depends on the task and datasets) on all models and modules.For the extraction modules and the baselines, we used the same configuration and setting as previous works (Mirzaee and Kordjamshidi, 2022).For SREQA models we use learning rates of 2  10 5 , 4  10 6 for SREQA(story) and SREQA(question) respectively.To run the models we use machine with Intel Core i9-9820X (10 cores, 3.30 GHz) CPU and Titan RTX with NVLink as GPU.</p>
<p>For GPT3.5, we use Instruct-GPT, davinci-00313 .The cost for running GPT3.5 on the humangenerated benchmarks was 0.002$ per 1k tokens.For GPT3.5 as information extraction, we use GPT3.5 turbo (a.k.a ChatGPT) with a cost of 0.0001$ per 1k tokens.We also use the GPT4 playground in OpenAI and PaLM2 playground to find the prediction of examples in Figure 11.</p>
<p>C Extraction and Reasoning Modules</p>
<p>Here, we discuss each module used in PISTAQ and their performance including the Spatial Role Labeling (SPRL), Coreference Resolution, and Spatial reasoner.As an example, triplet (a grey house, front , A grey car) is correct and the "spatial_type = FRONT", and (A grey car, front, a grey house) is incorrect, and the "spatial_type = NaN".Image from (Mirzaee and Kordjamshidi, 2022) C.1 Spatial Role Labeling (SPRL)</p>
<p>The SPRL module, shown in Figure 8 is divided into three sub-modules, namely, spatial role extraction (SRole), spatial relation extraction (SRel) 14 , and spatial type classification (SType).We only use these modules on sentences that convey spatial information in each benchmark.To measure the performance of SPRL modules, we use the macro average of F1 measure for each label.These modules are evaluated on three datasets that provide SPRL annotations, MSPRL, SPARTQA, and SPARTUN.When training the SPRL module on auto-generated benchmarks, we achieved a performance of 100% using only a quarter of the training data, therefore we stopped further training.</p>
<p>As shown in Table 4, all SPRL sub-modules achieve a high performance on synthetic datasets, SPARTQA and SPARTUN.This good performance is because these datasets may contain less ambiguity in the natural language expressions.Therefore, the BERT-base models can easily capture the syntactic patterns needed for extracting the roles and direct relations from the large training set.We implement a coreference resolution model based on (Lee et al., 2017) to extract all antecedents for each entity (check Figure 9a).Compared to the previous works, we have the entities (phrase) annotations.Hence, we ignore the phrase scoring modules and use this annotation instead.We first collect all mentions of each predicted entity from spatial role extraction or role annotations, then assign an "id" to the same mentions and include that id in each triplet.For example, for BELOW(a cat, a grey car), Front(the car, a church), id 1= a cat, 2 = a grey car, the car, and 3 = a church.So we create new triplets in the form of BELOW(1, 2) and Front(2, 3).</p>
<p>C.2 Coreference Resolution (Coref) in Spatial Reasoning</p>
<p>To train the model, we pair each mention with its previous antecedent and use cross-entropy loss to penalize the model if the correct pair is not chosen.For singletons and starting mention of objects, the model should return class 0, which is the [CLS] token.Since the previous model does not support the plural antecedent (e.g., two circles), we include that by considering shared entity in pairs like both (two circles, the black circle) and (two circles, the blue circle) are true pairs.</p>
<p>As an instance of the importance of coreference resolution in spatial reasoning, consider this context "block A has one black and one green circle.The black circle is above a yellow square.The yellow square is to the right of the green circle.Which object in block A is to the left of a yellow square" The reasoner must know that the NTPPI(block A, one green circle) and RIGHT( the yellow square, the green circle) are talking about the same object to connect them via transitivity and find the answer.</p>
<p>To evaluate the coreference resolution module (Coref in Table 4), we compute the accuracy of the pairs predicted as Corefs.The Coref model achieves a high performance on all datasets.The performance is slightly lower on the SPARTQA-HUMAN dataset when SPARTUN is employed for additional pre-training.However, we observed that there are many errors in the annotations in SPARTQA-HUMAN, and the pre-trained model is, in fact, making more accurate predictions than what is reflected in the evaluation.</p>
<p>C.3 Logic-based Spatial Reasoner</p>
<p>To solely evaluate the performance of the logicbased reasoner, we use the "GT-PISTAQ".We look into the errors of this model and categorize them based on the source of errors.The categories are missing/wrong ground truth direct annotations (A), rule-based Coreference Error (C) in connecting the extracted information before passing to the reasoner, and the low coverage of spatial concepts in the reasoner (R).As is shown in Table 13, spatial Reasoner causes no errors for SPARTUN since the same reasoner has been used to generate it.However, the reasoner does not cover spatial properties of entities (e.g., right edge in "touching right edge") in SPARTQA and causes wrong predictions in those cases.Table 14 displays the results of the SREQA model trained and tested solely on all the story's relation extraction parts (step 1).During the evaluation, we also possess the same data preprocessing and gather annotations of all relations between stories' entities and select the best model based on performance on the development set.</p>
<p>D SREQA on All Story Relations</p>
<p>Notably, the performance on the humangenerated datasets, SPARTQA-HUMAN and RESQ, is significantly lower compared to the autogenerated datasets.As discussed in , the MSPRL datasets contain missed annotations, resulting in the omission of several relations from the stories' entities and incomplete training data for this phase.Similarly, the SPARTQA-HUMAN SPRL annotation, as discussed in Appendix C, exhibits some noise, particularly in coreference annotation, leading to similar issues as observed in MSPRL regarding annotation of all story relations.Consequently, this reduced performance in all story relation extraction impacts the overall performance of the main SREQA model trained using two steps; however, as illustrated in the results of SREQA* in Table7, which utilizes Spartun instead of MSPRL for training on all story's relations, the performance substantially improves on the RESQ dataset.</p>
<p>E Large Language Models (LLMs)</p>
<p>Figure 11 presents examples showcasing predictions made by three Large Language Models (LLMs): GPT3.5-DaVinci,GPT4, and PaLM2, on a story from the SPARTQA-HUMAN dataset.These examples demonstrate that while these models, specifically GPT4 and PaLM2, excel in multihop reasoning tasks, solving spatial question answering remains a challenging endeavor.</p>
<p>To evaluate the LLMs' performance on spatial reasoning, we use Zero_shot, F ew_shot, and F ew_shot+CoT.In the Zero_shot setting, the prompt given as input to the model is formatted as "Context: story.Question: question?" and the model returns the answer to the question.</p>
<p>In the F ew_shot setting, we add two random examples from the training data with a story, all its questions, and their answers.Figure 12 depicts a prompt example for SPARTQA-HUMAN YN questions, passed to GPT3.5.</p>
<p>For F ew_shot+CoT, we use the same idea as (Wei et al., 2022) and manually write the reasoning steps for eight questions (from two random stories).The input then is formatted as "Context: story.Question: CoT.Answer.Asked Context: story.Question: question?". Figure 13 shows an example of these reasoning steps on RESQ.</p>
<p>E.1 LLMs for Information Extraction</p>
<p>As discussed in Section 5.3.1, we utilize LLM, GPT3.5-Turbo, for information extraction from human-generated texts.The extraction process encompasses Entity, Relation, Relation Type, and coreference extractions from the story, as well as entity and relation extraction from the question.Additionally, LLM is employed to identify mentions of question entities within the text.</p>
<p>We construct multiple manually crafted prompt examples for each extraction task, as depicted in Figure 14.Subsequently, the extracted information is inputted into the reasoner module of PISTAQ to compute the answers.</p>
<p>In addition to our experiment, we attempted to incorporate LLMs as neural spatial reasoners but in a pipeline structure of extraction and reasoning.To do so, as illustrated in Figure 10, we add the extracted information of LLM with the written CoTs based on this extracted information to the prompt of a GPT3.5-DaVinci.The results, however, become even lower (62.62%)compared to GPT3.5-CoT with the main text (67.05%) when evaluated on Story: a man in white shirt , black jacket , grey pants and black shoes is sitting on a wooden chair and talking on the phone .on the right a wooden bed with white bedcovers .on the left ( before the man ) a wooden desk and a vase with flowers .there is a black brief-case in front of the chair , and there is also a picture hanging on the wall above the bed .</p>
<p>Relation_in_story= [("a man", "in", "white shirt"), ("a man", "in", "black jacket"), ("a man", "in", "grey pants"), ("a man", "in", "black shoes"), ("A man", "sitting on", "a wooden chair"), ("a man", "talking on", "the phone"), ("a wooden bed", "on", "the right")("a wooden bed", "with", "white bedcovers"),("a wooden desk", "on", "the left"),("a wooden desk", "before", "the man"), ("a vase", "on", "the left"),("a vase", "before", "the man"),("a vase", "with", "flowers"),("a black brief-case", "in front of", "the chair"),("a picture", "hanging on", "the wall"),("a picture", "above", "the bed")] Question: Is the man touching the chair?(the man, touching, the chair), Chain of reasoning: ("A man", "sitting on", "a wooden chair") which is externally connected and above.The answer is Yes Question: Is the bedcover below the picture?(the bedcover, below, the picture), Chain of reasoning: ("a wooden bed", "with", "white bedcovers") which means bedcover has the same position as bedcover, ("a picture", "above", "the bed"), so (the picture, above, the bedcover), so (the bedcover, below, the picture).The answer is Yes. the RESQ dataset.This outcome highlights the superior ability of LLMs to capture information from natural language compared to structured data without fine-tuning.</p>
<p>AFigure 1 :
1
Figure1: An example of steps of spatial reasoning on RESQ dataset.We begin by searching for the plants from the question triplet within the text, enabling us to extract explicit triplets (1,2).Next, we apply rules such as INVERSE to deduce implicit triplets(3,4,5).Then, utilizing triplets 5 and 6 we determine the final answer, 'Yes'.NTPP: Non-Tangential Proper Part (Table1).</p>
<p>structure.First, entity mentions such as 'plants' and 'grey car' are selected from the BERT output and the entity representation is formed.Next, triplets like('plants', 'car',  [CLS]) are generated and fed into the reasoning component.The collective output of all relation classifiers determines the relationships between each pair.*All hatched parts are trained end-to-end.The rest of the data is obtained from annotations or off-the-shelf modules.The source of supervision in each step of training.In step#1, we train the model on all story relations, and in step#2, we only train it on question relations.These modules and data are the same as the ones used in PISTAQ.</p>
<p>Figure 3 :
3
Figure 3: The SREQA model with explicit neural layers to disentangle extraction and reasoning part.</p>
<p>. The large yellow triangle is touching the top edge of B. The green triangle is touching the right edge of B... Question 1: Is the green thing above the yellow thing?Answer: No Question 2: Does B have all of the triangles?green triangle is to the right of the large yellow triangle and the large yellow triangle is touching the top edge of B. Using inverse rule, the green triangle is below the yellow triangle.The answer is No.</p>
<p>Figure 4: An example of Large Language Models (LLMs) prediction on SQA task evaluated with CoT prompting.The last row shows an example of using GPT3.5-Turbo for information extraction in PISTAQ.See Appendix E for zero_shot examples.</p>
<p>Figure 4, shows an example of three LLMs, GPT3.5, GPT4 and PaLM2 on SPARTQA-HUMAN example 11 (complete figure 11 Due to the limited resources, we only use GPT4 and PaLM2 on a few examples to evaluate their performance on Story:</p>
<p>Question:Figure 5 :
5
Figure 5: An example of using BERT-based SPRL and GPT3.5 as information extraction in PISTAQ on a RESQ example.</p>
<p>Story:Question 1 :
1
behind it a bar with chairs and two people , and a bench with one person lying on it .Upper level with doors and a blue rail.Are the people behind the bar?Answer: Yes Predicted answer based on mSpRL annotations: No Question 2: Is the door above the bar?Answer: Yes Predicted answer based on relation in text: No Predicted answer based on the commonsense (upper level is above the main level): Yes mSpRL annotation: Triplet: Behind (a bar (id: t1), behind, it (id: l1))</p>
<p>Figure 8 :
8
Figure8: Spatial role labeling model includes two separately trained modules.E: entity, SP: spatial_indicators.As an example, triplet (a grey house, front , A grey car) is correct and the "spatial_type = FRONT", and (A grey car, front, a grey house) is incorrect, and the "spatial_type = NaN".Image from(Mirzaee and Kordjamshidi, 2022)</p>
<p>Figure9: The coreference resolution model(Lee et al., 2017).</p>
<p>Figure 10 :
10
Figure10: We employ LLMs in both extraction and reasoning tasks, but in a disentangled manner.Initially, we extract information using LLMs, and subsequently incorporate this extracted information into the prompt alongside written CoTs based on the extracted data.</p>
<p>Table 1 :
1
List of spatial relation formalism and types.
Formalism (General Type)Spatial TypeExpressions (e.g.)DC (disconnected)disjointEC (Externally Connected)touchingPO (Partially Overlapped)overlappedTopologicalEQ (Equal)equal(RCC8)TPPcovered byNTPPin, insideTPPIcoversNTPPIhasDirectional (Relative)LEFT, RIGHT BELOW, ABOVE BEHIND, FRONTleft of, right of under, over behind, in frontDistanceFar, Nearfar, close</p>
<p>grey car is parking in front of a grey house ... Are the plants behind the car?
Facts Extraction 2: {a grey house} Story triplets A 1: {grey car} Role Extraction Relation Extraction Story processing Coref Resolution Role Extraction Relation Extraction Question processing Corefernce info BEHIND(the plants , the car) Question triplets FRONT( 1, 2) NTPPI ( 2, 3) 3: {plants} R-CorefQuery BEHIND(3, 1)?Reasoning (Rules) Spatial Reasoner FRONT(X, Z) :-FRONT(X,Y), NTPPI(Y,Z). BEHIND(X,Y) :-FRONT(Y,X).LEFT: False RIGHT: True . . . BEHIND: True NTPPI: Flase Answer Generation All relations: YesFigure 2: PISTAQ pipeline based on disentangled ex-traction and reasoning. In this model, facts, e.g.,FRONT(grey car, grey house), are extracted from thestory and linked by coreference modules. The R-Corefequates 'the car' from the question with 'a grey car' inthe story and forms a query. This query, along withfacts, is sent to the spatial reasoner. Finally, the spatialreasoner employs FRONT and BEHIND rules and re-turns True as the answer.entity, I-entity, B-indicator, and I-indicator) tags.Finally, a softmax layer on the BIO tagger out-put selects the spatial entities 4 (e.g., 'grey car' or'plants' in Figure
3 PIpeline model for SpaTiAl Question answering</p>
<p>Table 2 :
2
(Mirzaee and Kordjamshidi, 2022)nd Kordjamshidi, 2022).
Dir: Directional relations (e.g., LEFT), Dis:</p>
<p>Table 3 :
3
The list of annotations from the target benchmarks and rule-based modules employed in each model.
We use a quarter of SPRL annotations to train the mod-ules on auto-generated benchmarks. S: Stories, Q: Ques-tions, All: Stories+Questions.</p>
<p>Table 4 :
4
Performance of the extraction modules.
DatasetCorefSRoleSRelSTypeMSPRL-88.5969.1219.79MSPRL<em>-88.0371.2323.65HUMAN82.1655.8S: 57.43 Q: 52.5543.79 39.34HUMAN</em>81.5172.53S: 60.24 Q: 61.5348.74 48.07SPARTQA99.8399.92S: 99.72 Q: 98.3699.05 98.62SPARTUN99.3599.96S: 99.18 Q: 97.6898.57 98.11
Q: question.S: stories.HUMAN: SPARTQA-HUMAN.SPARTQA: SPARTQA-AUTO.*Further pretraining modules on SPARTUN.We report macro F1 for SPRL and the accuracy of the Coref modules.</p>
<p>Table 5 :
5
Results on auto-generated datasets.We use the accuracy metric for both YN and FR questions.</p>
<h1>ModelsSPARTUN YN FRSPARTQA-AUTO YN FR1 Majority baseline 53.62 14.23 51.8244.352 GT-PISTAQ99.07 99.43 99.5198.993 BERT91.80 91.80 84.8894.174 BERT-EQ90.71 N/A 85.60N/A5 SREQA88.21 83.31 85.1186.886 PISTAQ96.37 94.52 97.5698.02</h1>
<p>Table 7 :
7
Result on RESQ.</p>
<h1>ModelsYN AccPFR RF11 Majority baseline52.44 29.87 14.28 6.572 GT-PISTAQ79.72 96.38 66.04 75.163 BERT51.74 30.74 30.13 28.174 BERT<em>48.95 60.96 49.10 50.565 GPT3.5 Zero_shot45.45 40.13 22.42 16.516 GPT3.5 F ew_shot60.13 45.20 54.10 44.287 GPT3.5 F ew_shot +CoT 62.93 57.18 37.92 38.478 BERT-EQ50.34---9 BERT-EQ</em>45.45---10 SREQA53.23 15.68 13.85 13.7011 SREQA<em>46.96 18.70 25.79 24.6112 PISTAQ75.52 72.11 35.93 46.80Table 6: Results on SPARTQA-HUMAN. We use ac-curacy on YN questions and average Precision (P), Re-call (R), and Macro-F1 on FR question types. </em>UsingSPARTUN supervision for further training.#ModelsAccuracy1Majority baseline50.212BERT57.373BERT<em> Zero_shot49.184BERT</em>63.605GPT3.5 Zero_shot60.326GPT3.5 F ew_shot65.907GPT3.5 F ew_shot +CoT67.058BERT-EQ56.559BERT-EQ<em> Zero_shot51.9610BERT-EQ</em>61.4711SREQA53.1512SREQA<em> Zero_shot53.3213SREQA</em>69.5014PISTAQ MSPRL41.9615PISTAQ SPARTUN+MSPRL47.2116Human90.38</h1>
<p>*Further training on SPAR-TUN.The Zero_shot refers to evaluation without further training on RESQ or MSPRL training data.</p>
<p>Table 8 :
8
Table 11 illustrates a comprehensive breakdown of the size of Role and Relation sets in the MSPRL dataset.Number of questions in training and evaluation sets of SQA benchmarks.
DatasetTrain DevTestSPARTQA-AUTO (YN)26152 3860 3896SPARTQA-AUTO (FR)25744 3780 3797SPARTQA-HUMAN (YN)16251143SPARTQA-HUMAN (FR)1492877SPARTUN (YN)20334 3152 3193SPARTUN (FR)18400 2818 2830RESQ(YN)1008333610DatasetTrain DevTestSPARTQA-AUTO (story)36420 16214 16336SPARTQA-AUTO (question)53488 15092 15216SPARTQA-HUMAN (story)389213584SPARTQA-HUMAN (question)623190549SPARTUN (story)68048 9720 10013SPARTUN (question)41177 63556340MSPRL600-613</p>
<p>Table 9 :
9
Number of sentences in SPRL annotations of each benchmarks.To train models on the auto-generated benchmarks, we only use the quarter of training examples from SPARTUN and SPARTQA-AUTO.
DatasetTrainDevTestSPARTQA-AUTO (story)159712 22029 21957SPARTQA-AUTO (question)232187 34903 35011SPARTQA-HUMAN (story)17699272SPARTQA-HUMAN (question)155127367SPARTUN (story)4836870317191SPARTUN (question)3873459706023MSPRL761-939Table 10: Number of triplets in SPRL annotations ofeach benchmarks.
A.1 Analyzing SPARTQA-HUMAN YNWe conducted additional evaluations on the superior performance of PISTAQ over other baseline models on SPARTQA-HUMAN YN questions.As explained before, PISTAQ tends to predict No when</p>
<p>Table 11 :
11
(Kordjamshidi et al., 2017)al., 2017).Yes Three boxes called one, two and three exist in an image.Box one contains a big yellow melon and a small orange watermelon.Box two has a small yellow apple.A small orange apple is inside and touching this box.Box one is in box three.Box two is to the south of, far from and to the west of box
information is not available, resulting in more Noand fewer Yes predictions compared to other mod-els, as presented in Table 12. The number of truepositive predictions for PISTAQ is more than twoother baselines, and as a result, it achieves higheraccuracy.Predictions/ Answer Yes No No predictionGround Truth7469 -BERT131 12 -BERT*8954 -PISTAQ4397 3Table 12: Detailed information about the prediction ofPISTAQ and BERT on SPARTQA-HUMAN YN ques-tions. "No prediction" is related to the PISTAQ modelwhen no correct SPRL extraction was made for the textof the question, and as a result, we have no answer pre-diction.
Q: Is the yellow apple to the west of the yellow watermelon?three.A small yellow watermelon is inside box three.Q:Where is box two relative to the yellow watermelon?Left, Below, Far Figure 6: An example of SPARTUN dataset from (Mirzaee and Kordjamshidi, 2022).</p>
<p>Table 14 :
14
The result of SREQA model only trained on all story relations of the SQA datasets.</p>
<p>We ignore "Dont know" answers in YN question and change them to No
As mentioned, we use the MSPRL annotation for RESQ dataset.
https://huggingface.co/transformers/v2.9.1/ model_doc/bert.html
from https://beta.openai.com
Since the questions(Q) and stories(S) have different annotations (questions have missing roles), we separately train and test the SRel and SType modules
AcknowledgementsSpaRTQA-HumanStoryThere is a block called A. Inside there's a red oval that is above a blue triangle.The blue triangle is touching the right edge of A and the top of a large black square.Touching the bottom edge of A is a small orange square.The is a block called B Which is to the right and touching A. Inside B there are 5 triangles, a red one, a green one, a large yellow one, a brown one, and an orange one.The red and orange triangles are touching the left edge of B and the orange triangle is above the red triangle.The large yellow triangle is touching the top edge of B. The green triangle is touching the right edge of B. The brown triangle is below all of the other triangles and is touching the bottom of B. Q3: There are five triangles mentioned in block B, a red one, a green one, a large yellow one, a brown one, and an orange one.All of these triangles are in block B, so the answer is Yes.GPT4 (Prediction)Zero-shot Q1: Don't Know Q2: The description does not provide enough information to determine if the green triangle is above the yellow triangle.It only states that the green triangle is touching the right edge of block B and the large yellow triangle is touching the top edge of B. The relative vertical positions of the green and yellow triangles are not specified.Q3: Yes, B has all of the triangles mentioned in the description.Inside block B, there are 5 triangles: a red one, a green one, a large yellow one, a brown one, and an orange one.Q2: Yes, the green thing is above the yellow thing.The green thing is a triangle that is touching the right edge of block B, and the yellow thing is a triangle that is touching the top edge of block B.Q3: Yes, B has all of the triangles.The question asks if B has all of the triangles in the block.The answer is yes because the passage says that "Inside B there are 5 triangles, a red one, a green one, a large yellow one, a brown one, and an orange one."This means that B has all of the triangles in the block.Chain-of-Thoughts Reasoning Steps.Consider Below examples:There are three blocks called A, B, and C. A is to the right of B and B is to the right of C. In A, there is a small blue square far to the right of a medium black circle.In B, there is a large black circle far below a medium yellow circle.The medium yellow circle is far below a medium black triangle.The medium black triangle is near and below a small blue square.In C, there is a large yellow square near and above a medium blue square.There is also a medium yellow square far to the left of the large yellow square.Is the medium blue thing near and below a large yellow thing?The chain of reasoning to answer this question is: The large yellow square is near and above a medium blue square.Using the inverse rule, the medium blue square is near and below the large yellow square.The answer is Yes.Is the small blue thing in B below a medium yellow thing?The chain of reasoning to answer this question is: The small blue circle in B is above the black triangle which is above the medium yellow circle.So the small blue thing is above a medium yellow thing.The answer is No.Is the small blue thing in A to the right of a medium blue thing?The chain of reasoning to answer this question is: block A is right of block B and block B is right of block C, so block A is right of block C. The medium blue thing is in block C. All objects in block A are right of objects in block C, so the small blue thing in A is right of the medium blue thing in C.The answer is Yes.Is the medium black thing in A above a small blue thing?The chain of reasoning to answer this question is: There is a small blue thing in block A and block B. It is not mentioned if block B is below or above block A or not.Also it is not mentioned if medium black thing in A is above the small blue thing in A. So the answer is Don't Know(No).Answer below questions: .Context 1: there are two social workers in the foreground .one wearing a red pullover and brown pants is bending over to access the blue paint .the other one in a red tee-shirt and black jeans is watching her .at the back of the room there is another worker wearing a white tee-shirt and blue jeans , acutally painting .there are many newspapers on the ground to protect the kindergarten floor .all three workers are wearing a mask .entities = ['two social workers', 'the foreground', 'one', 'a red pullover', 'brown pants', 'the blue paint', 'the other one', 'a red tee-shirt', 'black jeans', 'her', 'the back', 'the room', 'another worker', 'a white tee-shirt', 'blue jeans', 'many newspapers', 'the ground', 'the kindergarten floor', 'three workers', 'a mask'] Relation -a man in white shirt , black jacket , grey pants and black shoes is sitting on a wooden chair and talking on the phone.relation triplets:[("a man", "in", "white shirt"), ("a man", "in", "black jacket"), ("a man", "in", "grey pants"), ("a man", "in", "black shoes"), ("A man", "sitting on", "a wooden chair"), ("a man", "talking on", "the phone")] -on the right a wooden bed with white bedcovers.relation triplets:[("a wooden bed", "on", "the right"), ("a wooden bed", "with", "white bedcovers")]CoreferenceContext 1: Three women are sitting on a wooden bench in front of an about one metre high , red brick wall .they are all wearing skirts and jumpers ... If the list of all entities is: list_of_noun_phrases = ["three women", "they", "two of them", "a wooden bench", "an about one metre high red brick wall", "the wall", "skirts", ...] The below list shows which noun phrases in the "list_of_noun_phrases" refers to which same phrase: {"Three women": ["Three women", "they", "two of them"], they: ["Three women", "they", "two of them"], two of them: ["two of them"], a wooden bench: ["a wooden bench",], .....}QuestionRelation and TypeIf the relation set is: .... Are the lamps behind the building?: list_of_dictionary = [{"triplet": ("the lamps", "behind", "the building"), "relation type": ['BEHIND']}] Is the camera in front of the all kids?: list_of_dictionary = [{"triplet": ("the camera", "in front of", "the all kids"), "relation type": ['FRONT']}] Is a flag to the left of the stairs?: list_of_dictionary = [{"triplet": ("a flag", "to the left of", "the stairs"), "relation type": For examples,''{0: "small window", 2: "large window", 5: "three windows"}'' all can be matched with "the window" since the root is window here.Also the output should be in the form of only a python dictionary like {"the window": [0,2,5]}.
A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, 2023and interactivity</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>A computational logic approach to human spatial reasoning. Emmanuelle-Anna Dietz, Steffen Hlldobler, Raphael Hps, 2015 IEEE Symposium Series on Computational Intelligence. IEEE2015</p>
<p>Parisa Hossein Rajaby Faghihi, Kordjamshidi, Man Choh, James Teng, Allen, arXiv:2302.06829The role of semantic parsing in understanding procedural text. 2023arXiv preprint</p>
<p>The iapr tc-12 benchmark: A new evaluation resource for visual information systems. Michael Grubinger, Paul Clough, Henning Mller, Thomas Deselaers, International workshop ontoImage. 2006</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Spatial Role Labeling: Task definition and annotation scheme. Parisa Kordjamshidi, Marie-Francine Moens, Martijn Van Otterlo, Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10). the Seventh conference on International Language Resources and Evaluation (LREC'10)European Language Resources Association (ELRA2010</p>
<p>Multimodal spatial role labeling (msprl) task overview. Parisa Kordjamshidi, Taher Rahgooy, Marie-Francine Moens, James Pustejovsky, Umar Manzoor, Kirk Roberts, International Conference of the Cross-Language Evaluation Forum for European Languages. Springer2017. 2017</p>
<p>Self-attentive associative memory. Hung Le, Truyen Tran, Svetha Venkatesh, International Conference on Machine Learning. PMLR2020</p>
<p>End-to-end neural coreference resolution. Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer, 10.18653/v1/D17-1018Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational Linguistics2017</p>
<p>Focal loss for dense object detection. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2017Kaiming He, and Piotr Dollr</p>
<p>Things not written in text: Exploring spatial commonsense from visual signals. Xiao Liu, Da Yin, Yansong Feng, Dongyan Zhao, 10.18653/v1/2022.acl-long.168Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Ilya Loshchilov, Frank Hutter, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>A survey on multi-hop question answering and generation. Vaibhav Mavi, Anubhav Jangra, Adam Jatowt, arXiv:2204.091402022arXiv preprint</p>
<p>Named entity recognition in questions: Towards a golden collection. Ana Cristina Mendes, Lusa Coheur, Paula Vaz, Lobo , LREC. 2010</p>
<p>Learning reasoning strategies in end-to-end differentiable proving. Pasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, Tim Rocktschel, International Conference on Machine Learning. PMLR2020</p>
<p>Transfer learning with synthetic corpora for spatial role labeling and reasoning. Roshanak Mirzaee, Parisa Kordjamshidi, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>SPARTQA: A textual question answering benchmark for spatial reasoning. Roshanak Mirzaee, Rajaby Hossein, Qiang Faghihi, Parisa Ning, Kordjamshidi, 10.18653/v1/2021.naacl-main.364Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Named entity recognition for question answering. Diego Moll, Menno Van Zaanen, Daniel Smith, 2006</p>
<p>Improving coherence and consistency in neural sequence models with dualsystem, neuro-symbolic reasoning. Maxwell Nye, Michael Tessler, Josh Tenenbaum, Brenden M Lake, Advances in Neural Information Processing Systems. 202134</p>
<p>Batu Ozturkler, Nikolay Malkin, Zhen Wang, Nebojsa Jojic, arXiv:2210.01293Thinksum: Probabilistic reasoning over sets using large language models. 2022arXiv preprint</p>
<p>. Rasmus Berg Palm, Ulrich Paquet, Ole Winther, arXiv:1711.080282017Recurrent relational networks. arXiv preprint</p>
<p>Rnnlogic: Learning logic rules for reasoning on knowledge graphs. Meng Qu, Junkun Chen, Louis-Pascal Xhonneux, Yoshua Bengio, Jian Tang, International Conference on Learning Representations. 2021</p>
<p>Time-stamped language model: Teaching language models to understand the flow of events. Rajaby Hossein, Parisa Faghihi, Kordjamshidi, 10.18653/v1/2021.naacl-main.362Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesOnline. Association for Computational Linguistics2021</p>
<p>Using semantic roles to improve question answering. Dan Shen, Mirella Lapata, Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)Prague, Czech RepublicAssociation for Computational Linguistics2007</p>
<p>Large language models are strong zero-shot retriever. Tao Shen, Guodong Long, Xiubo Geng, Chongyang Tao, Tianyi Zhou, Daxin Jiang, arXiv:2304.142332023arXiv preprint</p>
<p>Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. Zhengxiang Shi, Qiang Zhang, Aldo Lipani, Proceedings of the Association for the Advancement of Artificial Intelligence, AAAI '22. the Association for the Advancement of Artificial Intelligence, AAAI '222022</p>
<p>Transfer learning in hand movement intention detection based on surface electromyography signals. Rahil Soroushmojdehi, Sina Javadzadeh, Alessandra Pedrocchi, Marta Gandolla, Frontiers in Neuroscience. 162022</p>
<p>Human reasoning and cognitive science. Keith Stenning, Michiel Van Lambalgen, 2012MIT Press</p>
<p>Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus, arXiv:1503.08895End-to-end memory networks. 2015arXiv preprint</p>
<p>Spatial reasoning from natural language instructions for robot manipulation. Gubbi Sagar, Anirban Venkatesh, Raviteja Biswas, Vikram Upadrashta, Partha Srinivasan, Bharadwaj Talukdar, Amrutur, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, 36th Conference on Neural Information Processing Systems. 2022. NeurIPS 2022</p>
<p>Towards navigation by reasoning over spatial configurations. Yue Zhang, Quan Guo, Parisa Kordjamshidi, 10.18653/v1/2021.splurobonlp-1.5Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics. Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for RoboticsOnline. Association for Computational Linguistics2021</p>
<p>Chen Zheng, Parisa Kordjamshidi, arXiv:2105.13449Relational gating for" what if" reasoning. 2021arXiv preprint</p>
<p>Reasoning over higher-order qualitative spatial relations via spatially explicit neural networks. Rui Zhu, Krzysztof Janowicz, Ling Cai, Gengchen Mai, International Journal of Geographical Information Science. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
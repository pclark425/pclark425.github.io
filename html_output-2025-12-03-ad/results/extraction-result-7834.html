<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7834 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7834</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7834</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-146.html">extraction-schema-146</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <p><strong>Paper ID:</strong> paper-267740366</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.10770v4.pdf" target="_blank">How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?</a></p>
                <p><strong>Paper Abstract:</strong> Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation. In this paper, we perform a meta-evaluation of such methods and assess their reliability across a broad range of tasks. We observe that while automatic evaluation methods can approximate human ratings under specific conditions, their validity is highly context-dependent. Specifically, the simple R OUGE -L metric correlates well with human ratings for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual transfer. The effectiveness of the more advanced method of using GPT-4 as a judge diminishes significantly if reference answers are not included in the prompt, which is the scenario where this method has the potential to provide the most value compared to other metrics. Our findings enhance the understanding of how automatic methods should be applied and interpreted when developing and evaluating instruction-tuned LLMs.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7834.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7834.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-gold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as an automatic judge with gold/reference answers included in the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 prompted to rate model outputs on a 1-3 Likert scale (naturalness, relatedness, correctness) with gold reference answers provided; evaluated as an automatic judge and compared to human majority-vote ratings using Pairwise Accuracy with tie calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Diverse instruction-following tasks (short-answer and long-answer generation and classification) drawn from Natural Instructions v2</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Natural Instructions v2 (subset: 20 tasks, 300 samples)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>GPT-4 API used; prompted with the same instructions as human annotators, asked to provide step-by-step reasoning then a Likert 1-3 rating; in -gold setting the gold/reference answer(s) were included in the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three bilingual (English/Swedish) expert annotators (not crowdworkers); majority-vote compressed to single gold label per sample; ratings on 1-3 Likert scale</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pairwise Accuracy (PA) with tie calibration</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.81</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>reliance_on_gold_reference; overly_strict_against_valid_but-divergent_free-form_answers; reduced_reliability_on_long-answer_tasks</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>High overall alignment with human correctness when gold references are present (avg PA=0.81); close alignment on extreme classes (alignment for human '1' and '3' reported ~88% and ~81% respectively); tends to be strict and rely on gold labels which can penalize correct but divergent free-form outputs (esp. long-answer tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>High alignment with humans on short-answer tasks when references exist; fast turnaround (evaluation across all models/tasks reported to take ~15-20 minutes and cost ~USD100), scalable compared to human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>GPT-4 prompted with human-style instruction template (same Likert criteria), asked for chain-of-thought before rating; 3 human annotators per sample; human majority-vote used as gold; PA computed with metric-level epsilon (tie calibration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7834.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7834.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4-no-gold</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 used as an automatic judge without gold/reference answers in the prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 prompted to rate outputs on the same Likert criteria but with no gold/reference answers supplied; used to assess whether LLM-as-judge can judge without references and compared to human majority-vote via Pairwise Accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Diverse instruction-following tasks (short-answer and long-answer) from Natural Instructions v2</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Natural Instructions v2 (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Same GPT-4 setup as -gold but prompts exclude gold answer(s); asked for reasoning and Likert 1-3 rating</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three bilingual expert annotators; majority-vote used as human gold</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pairwise Accuracy (PA) with tie calibration</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.62</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>positive_bias_without_references; lower_alignment_in_non-English; worse_on_short-answer_tasks_without_gold; overrating_long_responses</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Substantially lower alignment with humans when gold references are removed (PA drops from 0.81 to 0.62 overall for English); shows an overly positive bias (e.g., alignment for human '1' falls to ~65% while for human '3' rises to ~84%), especially problematic for free-form/long-answer tasks and in Swedish outputs where overgeneration occurs.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Can in principle be used when no gold reference exists (useful scenario), but results show notable reliability loss; still faster than human annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Same prompting template as GPT-4-gold but with gold-answer block removed; PA computed with tie calibration; reported per-task PAs and aggregated short- vs long-answer analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7834.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7834.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L (Longest Common Subsequence based overlap metric)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A surface-overlap automatic metric measuring longest common subsequence between generated output and reference; evaluated as a cheap alternative to LLM-as-judge and compared against human majority-vote with PA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Short-answer and long-answer instruction-following tasks from Natural Instructions v2</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Natural Instructions v2 (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>ROUGE-L</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>Standard ROUGE-L computed against gold reference(s); no model size/version applicable</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three bilingual expert annotators; majority-vote used as human gold</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pairwise Accuracy (PA) with tie calibration</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.75</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>unreliable_for_free-form_long-answer_tasks; inflated_scores_when_label_space_overlaps; language_and_format_sensitivity; cannot_capture_semantic_equivalence_beyond_surface_overlap</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>ROUGE-L matches human judgments nearly as well as GPT-4-gold on English short-answer tasks (short-answer PA: ROUGE-L ~0.90 vs GPT-4-gold ~0.93), but fails on long-answer/free-form tasks (long-answer PA ~0.48). ROUGE-L can produce high scores for wrong answers that share surface overlap with correct label phrases, making cross-task averaging misleading.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>Very low cost and fast; effective and cost-efficient alternative to GPT-4-gold for English short-answer tasks when references exist.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>ROUGE-L computed against provided gold reference(s); PA tie-calibrated per-metric epsilon; analysis separated by short-answer vs long-answer tasks and by language (English vs Swedish).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7834.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7834.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including reported differences, limitations, failure modes, and any quantitative agreement metrics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERTSCORE (semantic similarity via contextual embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic similarity metric computing cosine similarity between contextual token embeddings of candidate and reference; evaluated as an intermediate metric between surface-based and LLM-based judgments and compared to human majority-vote via PA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_task</strong></td>
                            <td>Primarily long-answer instruction-following tasks and also evaluated across task types from Natural Instructions v2</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Natural Instructions v2 (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_name</strong></td>
                            <td>BERTSCORE</td>
                        </tr>
                        <tr>
                            <td><strong>judge_model_details</strong></td>
                            <td>BERTSCORE computed using pretrained contextual encoders (standard BERTSCORE setup); measures token-level embedding similarity and aggregates to a score</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluator_type</strong></td>
                            <td>Three bilingual expert annotators; majority-vote used as human gold</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Pairwise Accuracy (PA) with tie calibration</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>0.66</td>
                        </tr>
                        <tr>
                            <td><strong>reported_loss_aspects</strong></td>
                            <td>misses_instruction-specific_constraints (e.g., required degree of change); lower_tie_rate_than_humans (produces fewer ties)</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>BERTSCORE shows relatively strong performance on long-answer tasks (long-answer PA ~0.54) and is comparable to GPT-4-gold on many long-answer tasks, but can fail to capture task-specific constraints (e.g., requirement to alter wording sufficiently) and tends to produce fewer ties than human ratings.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages_of_llm_judge</strong></td>
                            <td>More semantics-aware than surface metrics; promising for long-answer tasks where ROUGE-L fails; less costly than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>BERTSCORE computed against gold reference(s); PA with metric-level epsilon; per-task comparisons and cross-language (English/Swedish) analyses performed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration <em>(Rating: 2)</em></li>
                <li>Chatgpt outperforms crowd workers for text-annotation tasks <em>(Rating: 1)</em></li>
                <li>Are large language model-based evaluators the solution to scaling up multilingual evaluation? <em>(Rating: 2)</em></li>
                <li>A closer look into using large language models for automatic evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7834",
    "paper_id": "paper-267740366",
    "extraction_schema_id": "extraction-schema-146",
    "extracted_data": [
        {
            "name_short": "GPT-4-gold",
            "name_full": "GPT-4 used as an automatic judge with gold/reference answers included in the prompt",
            "brief_description": "GPT-4 prompted to rate model outputs on a 1-3 Likert scale (naturalness, relatedness, correctness) with gold reference answers provided; evaluated as an automatic judge and compared to human majority-vote ratings using Pairwise Accuracy with tie calibration.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
            "evaluation_task": "Diverse instruction-following tasks (short-answer and long-answer generation and classification) drawn from Natural Instructions v2",
            "dataset_name": "Natural Instructions v2 (subset: 20 tasks, 300 samples)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "GPT-4 API used; prompted with the same instructions as human annotators, asked to provide step-by-step reasoning then a Likert 1-3 rating; in -gold setting the gold/reference answer(s) were included in the prompt",
            "human_evaluator_type": "Three bilingual (English/Swedish) expert annotators (not crowdworkers); majority-vote compressed to single gold label per sample; ratings on 1-3 Likert scale",
            "agreement_metric": "Pairwise Accuracy (PA) with tie calibration",
            "agreement_score": 0.81,
            "reported_loss_aspects": "reliance_on_gold_reference; overly_strict_against_valid_but-divergent_free-form_answers; reduced_reliability_on_long-answer_tasks",
            "qualitative_findings": "High overall alignment with human correctness when gold references are present (avg PA=0.81); close alignment on extreme classes (alignment for human '1' and '3' reported ~88% and ~81% respectively); tends to be strict and rely on gold labels which can penalize correct but divergent free-form outputs (esp. long-answer tasks).",
            "advantages_of_llm_judge": "High alignment with humans on short-answer tasks when references exist; fast turnaround (evaluation across all models/tasks reported to take ~15-20 minutes and cost ~USD100), scalable compared to human annotation.",
            "experimental_setting": "GPT-4 prompted with human-style instruction template (same Likert criteria), asked for chain-of-thought before rating; 3 human annotators per sample; human majority-vote used as gold; PA computed with metric-level epsilon (tie calibration).",
            "uuid": "e7834.0",
            "source_info": {
                "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4-no-gold",
            "name_full": "GPT-4 used as an automatic judge without gold/reference answers in the prompt",
            "brief_description": "GPT-4 prompted to rate outputs on the same Likert criteria but with no gold/reference answers supplied; used to assess whether LLM-as-judge can judge without references and compared to human majority-vote via Pairwise Accuracy.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
            "evaluation_task": "Diverse instruction-following tasks (short-answer and long-answer) from Natural Instructions v2",
            "dataset_name": "Natural Instructions v2 (subset)",
            "judge_model_name": "GPT-4",
            "judge_model_details": "Same GPT-4 setup as -gold but prompts exclude gold answer(s); asked for reasoning and Likert 1-3 rating",
            "human_evaluator_type": "Three bilingual expert annotators; majority-vote used as human gold",
            "agreement_metric": "Pairwise Accuracy (PA) with tie calibration",
            "agreement_score": 0.62,
            "reported_loss_aspects": "positive_bias_without_references; lower_alignment_in_non-English; worse_on_short-answer_tasks_without_gold; overrating_long_responses",
            "qualitative_findings": "Substantially lower alignment with humans when gold references are removed (PA drops from 0.81 to 0.62 overall for English); shows an overly positive bias (e.g., alignment for human '1' falls to ~65% while for human '3' rises to ~84%), especially problematic for free-form/long-answer tasks and in Swedish outputs where overgeneration occurs.",
            "advantages_of_llm_judge": "Can in principle be used when no gold reference exists (useful scenario), but results show notable reliability loss; still faster than human annotation.",
            "experimental_setting": "Same prompting template as GPT-4-gold but with gold-answer block removed; PA computed with tie calibration; reported per-task PAs and aggregated short- vs long-answer analyses.",
            "uuid": "e7834.1",
            "source_info": {
                "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "ROUGE-L",
            "name_full": "ROUGE-L (Longest Common Subsequence based overlap metric)",
            "brief_description": "A surface-overlap automatic metric measuring longest common subsequence between generated output and reference; evaluated as a cheap alternative to LLM-as-judge and compared against human majority-vote with PA.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
            "evaluation_task": "Short-answer and long-answer instruction-following tasks from Natural Instructions v2",
            "dataset_name": "Natural Instructions v2 (subset)",
            "judge_model_name": "ROUGE-L",
            "judge_model_details": "Standard ROUGE-L computed against gold reference(s); no model size/version applicable",
            "human_evaluator_type": "Three bilingual expert annotators; majority-vote used as human gold",
            "agreement_metric": "Pairwise Accuracy (PA) with tie calibration",
            "agreement_score": 0.75,
            "reported_loss_aspects": "unreliable_for_free-form_long-answer_tasks; inflated_scores_when_label_space_overlaps; language_and_format_sensitivity; cannot_capture_semantic_equivalence_beyond_surface_overlap",
            "qualitative_findings": "ROUGE-L matches human judgments nearly as well as GPT-4-gold on English short-answer tasks (short-answer PA: ROUGE-L ~0.90 vs GPT-4-gold ~0.93), but fails on long-answer/free-form tasks (long-answer PA ~0.48). ROUGE-L can produce high scores for wrong answers that share surface overlap with correct label phrases, making cross-task averaging misleading.",
            "advantages_of_llm_judge": "Very low cost and fast; effective and cost-efficient alternative to GPT-4-gold for English short-answer tasks when references exist.",
            "experimental_setting": "ROUGE-L computed against provided gold reference(s); PA tie-calibrated per-metric epsilon; analysis separated by short-answer vs long-answer tasks and by language (English vs Swedish).",
            "uuid": "e7834.2",
            "source_info": {
                "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "BERTSCORE",
            "name_full": "BERTSCORE (semantic similarity via contextual embeddings)",
            "brief_description": "A semantic similarity metric computing cosine similarity between contextual token embeddings of candidate and reference; evaluated as an intermediate metric between surface-based and LLM-based judgments and compared to human majority-vote via PA.",
            "citation_title": "",
            "mention_or_use": "use",
            "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
            "evaluation_task": "Primarily long-answer instruction-following tasks and also evaluated across task types from Natural Instructions v2",
            "dataset_name": "Natural Instructions v2 (subset)",
            "judge_model_name": "BERTSCORE",
            "judge_model_details": "BERTSCORE computed using pretrained contextual encoders (standard BERTSCORE setup); measures token-level embedding similarity and aggregates to a score",
            "human_evaluator_type": "Three bilingual expert annotators; majority-vote used as human gold",
            "agreement_metric": "Pairwise Accuracy (PA) with tie calibration",
            "agreement_score": 0.66,
            "reported_loss_aspects": "misses_instruction-specific_constraints (e.g., required degree of change); lower_tie_rate_than_humans (produces fewer ties)",
            "qualitative_findings": "BERTSCORE shows relatively strong performance on long-answer tasks (long-answer PA ~0.54) and is comparable to GPT-4-gold on many long-answer tasks, but can fail to capture task-specific constraints (e.g., requirement to alter wording sufficiently) and tends to produce fewer ties than human ratings.",
            "advantages_of_llm_judge": "More semantics-aware than surface metrics; promising for long-answer tasks where ROUGE-L fails; less costly than GPT-4.",
            "experimental_setting": "BERTSCORE computed against gold reference(s); PA with metric-level epsilon; per-task comparisons and cross-language (English/Swedish) analyses performed.",
            "uuid": "e7834.3",
            "source_info": {
                "paper_title": "How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration",
            "rating": 2,
            "sanitized_title": "ties_matter_metaevaluating_modern_metrics_with_pairwise_accuracy_and_tie_calibration"
        },
        {
            "paper_title": "Chatgpt outperforms crowd workers for text-annotation tasks",
            "rating": 1,
            "sanitized_title": "chatgpt_outperforms_crowd_workers_for_textannotation_tasks"
        },
        {
            "paper_title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
            "rating": 2,
            "sanitized_title": "are_large_language_modelbased_evaluators_the_solution_to_scaling_up_multilingual_evaluation"
        },
        {
            "paper_title": "A closer look into using large language models for automatic evaluation",
            "rating": 1,
            "sanitized_title": "a_closer_look_into_using_large_language_models_for_automatic_evaluation"
        }
    ],
    "cost": 0.01142875,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?
2 Oct 2024</p>
<p>Ehsan Doostmohammadi ehsan.doostmohammadi@liu.se 
Linköping University</p>
<p>Oskar Holmström 
Linköping University</p>
<p>Marco Kuhlmann 
Linköping University</p>
<p>How Reliable Are Automatic Evaluation Methods for Instruction-Tuned LLMs?
2 Oct 2024E4E0588E8B2D2BE4926EE0F6C7862453arXiv:2402.10770v4[cs.CL]
Work on instruction-tuned Large Language Models (LLMs) has used automatic methods based on text overlap and LLM judgments as cost-effective alternatives to human evaluation.In this paper, we perform a meta-evaluation of such methods and assess their reliability across a broad range of tasks.In evaluating how well automatic methods align with human evaluations, correlation metrics are the most commonly employed method despite their inherent limitations when dealing with ties and different scales.To address these shortcomings, we use Pairwise Accuracy as an alternative to standard correlation measures.We observe that while automatic evaluation methods can approximate human ratings under specific conditions, their validity is highly context-dependent.Specifically, the simple ROUGE-L metric correlates very well with human ratings for short-answer English tasks but is unreliable in free-form generation tasks and cross-lingual scenarios.The effectiveness of the more advanced method of using GPT-4 as a judge diminishes significantly if reference answers are not included in the prompt, which is the scenario where this method has the potential to provide the most value compared to other metrics.Our findings enhance the understanding of how automatic methods should be applied and interpreted when developing and evaluating instruction-tuned LLMs.</p>
<p>Introduction</p>
<p>A key strength of the current generation of Large Language Models (LLMs) is their capacity to learn new tasks from instructions, either in-context (Mishra et al., 2022;Sanh et al., 2022;Wei et al., 2022) or in a dedicated fine-tuning phase (Wang et al., 2022).The field has also seen the development of methods to adapt LLMs to new languages, for example, through continued fine-tuning † Equal contribution (Muennighoff et al., 2023), alignment with translation pairs (Ranaldi et al., 2024), and instruction tuning on additional languages (Chen et al., 2024;Kew et al., 2023).</p>
<p>The gold standard for evaluating generative tasks is human annotation, but this scales poorly due to high costs and time constraints.Consequently, the most common approach for assessing generative LLMs is using automated evaluation techniques.Among these, two popular methods are measuring text overlap with ROUGE-L (Lin, 2004) and utilizing existing LLMs as automatic judges (Zheng et al., 2023); however, these methods only approximate human judgment, prompting questions about their reliability.While previous research has found that automatic evaluation methods correlate well with human assessments (Wang et al., 2022;Zheng et al., 2023), it is important to recognize that these findings generalize over tasks of very different types and in different languages.Additionally, correlation measures may not provide reliable estimates of alignment with human ratings, as they are limited in their ability to deal with ties and constant scores, which are common in human annotations (Deutsch et al., 2023).</p>
<p>In this paper, we provide a thorough analysis of two widely-used automatic methods for approximating human judgments, ROUGE-L and LLMas-a-judge.Additionally, we experiment with BERTSCORE, a semantic text similarity measure, to assess its potential utility.We study the reliability of the three measures across a broad range of English-language tasks.We also perform experiments on Swedish as an initial study on the reliability of these metrics across languages.Instead of using correlation measures, we employ Pairwise Accuracy (Deutsch et al., 2023) to quantify the alignment with human ratings.Our overall goal is to increase our understanding of the reliability of automatic evaluation methods and to establish guidelines regarding their appropriate usage.</p>
<p>Our contributions can be summarized as follows:</p>
<p>• We adopt Pairwise Accuracy (PA) with tie calibration (Deutsch et al., 2023) to enable robust comparisons between metrics, as we observe a high prevalence of tied ratings which renders common metrics, such as Kendall's τ and Spearman's ρ, unreliable.</p>
<p>• We show that GPT-4 aligns well with human judgments when gold reference answers are available.However, its reliability diminishes in the absence of these references, where it shows an overly positive bias.This is especially problematic for free-form tasks, since GPT-4 is commonly used in such settings.</p>
<p>• We find that GPT-4, while being the best tool for evaluating generations, can be replaced by faster and far less costly alternatives under certain conditions.In particular, we show that ROUGE-L offers a cost-effective alternative to GPT-4 for short-answer tasks, while BERTSCORE shows promising results in long-answer tasks.</p>
<p>• We observe a decrease in alignment with humans in non-English tasks for ROUGE-L and GPT-4 in situations where it does not have access to gold references.This suggests that it could be challenging to use automatic evaluation methods for lesser-resourced languages.</p>
<p>Related Work</p>
<p>We start by reviewing the research on the automatic evaluation of generated text, the use of LLMs as evaluators, and the methods applied to assess the alignment of metrics with human preferences.</p>
<p>Automatic Evaluation</p>
<p>For short-form tasks such as multiple-choice question answering, assessing the quality of model outputs appears feasible through standard classification metrics like accuracy and F 1 -score (Li et al., 2023a;Lai et al., 2023).While such an evaluation can be precise, it is rather strict and can only provide a fair performance assessment if the model does not deviate from the instructed format.However, this easily happens as the tasks diverge from the training data or get more complex.Surfacelevel similarity measures such as ROUGE-L (Honovich et al., 2023;Wang et al., 2023;Mishra et al., 2022;Yin et al., 2023;Lai et al., 2023;Li et al., 2023b) are more forgiving regarding formatting inconsistencies, but still lack the sophistication to be effective in tasks where free-from answers are expected.</p>
<p>Evaluation Using LLMs</p>
<p>An increasingly common method for evaluating instruction fine-tuned models is to use powerful LLMs as automatic judges (Peng et al., 2023;Gilardi et al., 2023;Chen et al., 2024;Kew et al., 2023).Zheng et al. (2023) propose three different variations:</p>
<p>(1) pairwise comparison, which asks the LLM to choose its preferred answer or declare a tie;</p>
<p>(2) single-answer grading, in which the LLM is asked to assign a score to an answer; and (3) reference-guided grading, in which the model is provided with a reference solution (if available).</p>
<p>An approach similar to the second one is used by M 3 IT (Li et al., 2023b) to evaluate the accuracy, relevance, and naturalness using GPT-4 in a multimodal scenario.</p>
<p>Meta-Evaluation</p>
<p>Human evaluation is the gold standard of assessment in natural language processing, but is not widely used in the literature due to its high costs.Instead, authors have turned to automatic evaluation measures that correlate well with human judgments.Zhang* et al. (2020) show that BERTSCORE is better correlated to human judgments than previous metrics, but Hanna and Bojar (2021) also identify setups where it fails.</p>
<p>The recent work on automating evaluation processes and leveraging LLMs has demonstrated substantial agreement with human ratings.Zheng et al. (2023) show that GPT-4's judgments align with human evaluations at over 80% agreement, reaching levels comparable to human-human agreement.Zhou et al. (2023) also report agreement levels between GPT-4 and human annotators on a par with human-human agreements.There is also work on the meta-evaluation of automatic metrics for chat and summarization (Shen et al., 2023;Chiang and Lee, 2023) using different criteria, and on aligning language model evaluations better with human preferences, such as Liu et al. (2024) and Chan et al. (2024).</p>
<p>In this section, we present the data and instructiontuned models used in our study.We provide an overview of the automatic metrics we employed to assess model performance and detail our approach to conducting a meta-evaluation of these metrics.</p>
<p>Data</p>
<p>As our training data, we use the Cleaned Alpaca Dataset1 , which corrects errors found in the original Alpaca (Taori et al., 2023).For testing, we use Natural Instructions v2 (NIv2) (Mishra et al., 2022;Wang et al., 2022), which spans a diverse range of tasks, including classification, question answering, free-form text generation, and reasoning.This enables fine-grained testing.</p>
<p>Sample Selection Because of our limited annotation budget (cf.§3.3), we select 20 from the 119 (English-language) tasks available in NIv2.We aim to find tasks that (a) cover a range of difficulty levels, (b) involve both short and long free-form answers, and (c) are diverse in task types while leaving some type overlap for control purposes.For a full description of the selected tasks, we refer to Appendix A. From each task, we pick 15 random samples, leaving us with 300 samples in total.</p>
<p>Translation To study the metrics' reliability across the language dimension, we translate both our training and our test data to Swedish using GPT-3.5-turbo.The prompt template and hyperparameters used for translation can be found in Appendix B. Previous work has shown that the automatic translation of the Alpaca dataset produces high-quality results with low noise levels (Holmström and Doostmohammadi, 2023;Li et al., 2023a).In addition to the two monolingual train datasets, we create an equally-sized English-Swedish bilingual train set by replacing a random 50% of the samples in the Cleaned Alpaca Dataset with their Swedish translations.Our purpose with this bilingual training set is to conduct a controlled study with more diverse bilingual data.</p>
<p>Instruction Tuning</p>
<p>We instruction-tune three base models in this study: LLaMA2-7b, LLaMA2-13b and GPT-SW3-6.7b.Our selection accounts for different model sizes, pretraining languages, and performance.</p>
<p>• LLaMa2 (Touvron et al., 2023) is trained on mainly English data; only 0.15% of the pretraining data is Swedish.We use both the 7B and the 13B parameter versions of the model.</p>
<p>• GPT-SW3 (Ekgren et al., 2024) is a GPT-2-based language model mainly trained on North Germanic languages and English, where 26% is Swedish, and 40% is English.GPT-SW3 exhibits the lowest perplexity on Swedish (see Appendix C.1), which is unsurprising as LLaMA2 models have seen vastly fewer Swedish tokens during pretraining.</p>
<p>For instruction tuning, we use the same training settings, hyperparameters, and prompts as Alpaca, and use DeepSpeed (Rasley et al., 2020) with the same configuration for all models.For more details regarding the implementation, see Appendix D.</p>
<p>Naming Scheme We instruction-tune each of our three base models on the three training datasets (English, Swedish, and bilingual) and test it on either the single training language (for monolingual models) or both languages (for bilingual models).This gives us a combined total of 12 different configurations for our experiments.Throughout the paper, we refer to these by the name of the base model, model size, training dataset, and testing language, all separated by underscores.For example, SW3_6.7b_ENSV_SV identifies the experiment where we train the GPT-SW3 model with 6.7 billion parameters on the bilingual English-Swedish data and test on Swedish.</p>
<p>Evaluation Methods</p>
<p>Human Assessment To establish the gold standard for our evaluation, we hire three bilingual (English and Swedish) evaluators to assess model outputs based on three criteria: naturalness (how natural and fluent the generated response is), relatedness (whether the response is related to the prompt and follows the required format), and correctness (whether the response is correct, which is our main criterion).While there are some tasks for which these criteria may not be applicable (especially correctness), they are well-suited for our chosen set of tasks.We ask each annotator to rate each criterion on a Likert scale ranging from 1 (significantly deficient) to 3 (completely proficient).For a detailed description of the annotation process and instructions, see Appendix E. The Kendall's τ is 0.74 (averaged over pairs of annotators) and the Fleiss' κ (Fleiss and Cohen, 1973) is 0.63, indicating a substantial agreement between human annotators.</p>
<p>Majority Vote To compare human ratings to other metrics, we use a variant of the majority vote.</p>
<p>More specifically, we compress the three ratings into that score which is assigned by at least two raters, and fall back to a neutral score of 2 in cases where all raters have given different scores.We prefer our method over the obvious alternative of taking an average because it reduces the impact of outlier ratings.For reference, among the human ratings of correctness (our main criterion), there is a majority vote for 93.5% of samples, providing a robust foundation for our comparisons.We treat the human majority vote as the gold standard and compare other metrics against it.</p>
<p>Performance Metrics Our selection of performance metrics is motivated by the desire to cover commonly used methods on a spectrum from surface-based semantics-based methods.On one end of this spectrum, we use ROUGE-L (Lin, 2004), which measures the textual overlap between a generated response and a reference output as the length of the longest common subsequence.On the other end of the spectrum, we use GPT-4 as a judge (Zheng et al., 2023).2Similar to previous work (Zhou et al., 2023;Kew et al., 2023), we prompt GPT-4 with the same instructions that we give to human evaluators and ask it to rate based on the same criteria on the same Likert scale.We also prompt GPT-4 to provide its reasoning before rating, similarly to Kew et al. (2023), whose framework we use for LLM-as-a-judge evaluations.The prompt template used for evaluations is found in Appendix B. In the standard evaluation setting, the gold labels are included in the prompt as a reference for the model; we mark this setting with the suffix -gold.To ablate the effects of the access to gold labels, we perform additional experiments with these labels excluded; we mark these with the suffix -no-gold.Finally, as a point in-between a purely surface-based and a powerful semanticsbased performance metric, we use BERTSCORE (Zhang* et al., 2020), which quantifies semantic overlap in terms of the cosine similarity between contextual embeddings obtained from pretrained language models.</p>
<p>Meta-Evaluation Method</p>
<p>Pairwise Accuracy with Tie Calibration In this study, we perform a meta-evaluation of both metrics that produce continuous and ordinal ratings.While Spearman's ρ and Kendall's τ are commonly used for such purposes, these metrics fail to handle tasks with constant score vectors or with different rating scales, and they do not reward correct predictions of ties.Ties are especially frequent in Likert-scale human ratings, which the automatic metrics are benchmarked against.</p>
<p>In response, we have chosen to use Pairwise Accuracy with Tie Calibration (PA) for meta-evaluating metrics.Proposed by Deutsch et al. (2023), PA addresses the shortcomings of traditional metrics by including mechanisms to explicitly account for the prevalence of ties, thus providing a fairer assessment of metrics.</p>
<p>PA measures the proportion of correctly ranked pairs, including accurately predicted ties.With values ranging from 0 to 1, the metric is more easily interpreted than traditional correlation metrics such as Spearman's ρ and Kendall's τ .PA includes a tie calibration process by defining a threshold value, ϵ, which specifies what is considered a significant difference between scores.A pair of scores with a difference smaller than ϵ is considered a tie.This is crucial as some metrics are more likely to produce tied values, as can be seen in Table 1.Tie calibration ensures that comparisons between different metrics are fair, regardless of their inclination to predict ties or having different rating scales.</p>
<p>We study the distribution of ties in our data and observe significant variation for different metrics, as shown in Table 1.The average tie proportion for human ratings is 0.557, serving as our benchmark.In contrast, metrics such as GPT-4, ROUGE-L, BERTSCORE exhibit varying degrees of tie proportions.GPT-4 has a similar degree of ties compared to human ratings, while BERTSCORE has considerably lower tie proportions.The significant amount of ties and a constant score vector validate  the use of PA to enable a reliable meta-evaluation of our metrics.As an example of a constant vector in our case, in Task 034 (cf.§4), there is a constant score of 1 from human raters, illustrating a scenario that could commonly occur in instruction tuning.Unlike Deutsch et al. (2023), who calculate ϵ for each task, we calculate the optimal ϵ for each metric using data from all tasks.We find that this produces a PA that better reflects the true correlation between human and metric scores, especially for tasks with only ties or mostly ties.Otherwise, for tasks with only ties, the ϵ could be as large as the value range for the metric and treat every pair of scores as ties.As shown in Table 1, our metric-level ϵ correlates well with the number of ties for the metric.With our pre-calculated ϵ values, we compute PA over all models per task, aligning with the No-Grouping setting in Deutsch et al. (2023).</p>
<p>Results and Analysis</p>
<p>In this section, we present a comparative analysis of the evaluation methods in terms of their alignment with human assessments.</p>
<p>Human Evaluation</p>
<p>We present the human evaluation results for each model in Table 2.All models demonstrate the capability to generate natural-sounding text (99% on average) and also perform fairly well in generating relevant responses that adhere to the required format (73% on average).The correctness scores demonstrate that the models are capable of gen-erating largely accurate answers.There is also a notable diversity among the models regarding correctness.This diversity is crucial because we seek a range of models with varied problem-solving abilities, rather than just strong models that produce highly accurate results.</p>
<p>The ratings of GPT-4 closely align with human ratings for correctness, showing slightly more distance in relatedness, and even more in naturalness.(The average differences between human and GPT-4 ratings are summarized in the final row of Table 2.) Based on these results, in the rest of the paper, we focus solely on correctness.We prioritize correctness since it is the most important criterion for determining the usefulness of LLMs.Moreover, comparing our metrics using the other criteria could be problematic.For instance, while ROUGE-L scores serve as a reasonable proxy for correctness, they are less suited for evaluating naturalness or relatedness.</p>
<p>Meta-Evaluation of Metrics</p>
<p>The metric with the highest alignment with human ratings is GPT-4-gold which achieves an average PA of 0.81 for English short-and long-answer tasks, followed by ROUGE-L with 0.75, BERTSCORE with 0.66, and GPT-4-no-gold with 0.62.For a comparison of all the results across different task types, languages, and metrics, see Table A3.Nonetheless, a more fine-grained analysis shows a different pattern, which we will discuss in this section.</p>
<p>Finding 1: All metrics struggle to assess model performance on long-answer tasks.</p>
<p>As presented in Figures 1, 2, and 4, the alignment with humans drops for long-answer tasks compared to short-answer tasks.On average, for English long-answer tasks, GPT-4-gold at 0.58 is the highest, followed by 0.54 for BERTSCORE, 0.50 for GPT-4-no-gold, and 0.48 for ROUGE-L.</p>
<p>For Tasks 613, 677 and 1659 where the models must generate free-form text, ROUGE-L scores are lower than human ratings accompanied with low to medium PAs.This is due to the large set of possible solutions and the small set of gold label answers, often consisting of a single sentence, that the output should match to receive a high ROUGE-L score.For some of these tasks, e.g., title generation, it is impossible to cover the set of conceivable solutions which makes ROUGE-L unreliable for these types of tasks.The same trend is present for GPT-4, which could be explained by this model's relying on the gold labels during evaluation, which may make it overly critical of responses that show less conformity with the provided reference answers.</p>
<p>In contrast, for Tasks 1562 and 1622, where models are tasked with modifying sentences, GPT-4 assigns higher ratings compared to humans.GPT-4 struggles to reliably assess whether our models have met the instructional criteria in such tasks.For instance, in Task 1562, the objective is to generate paraphrases of questions while making as many alterations as possible.When models only introduce minor changes, such as changing bring to take in the sentence "Can I bring my mountain bike with me to this national park?", GPT-4 often rates this 3 on correctness.</p>
<p>Finding 2: GPT-4 needs reference answers in the prompt.</p>
<p>When we remove the gold answers from the prompt, GPT-4's alignment with human ratings decreases significantly, from 0.81 PA to 0.62 PA on English.Full results are presented in Table A3.For short-answer tasks in English, the reduction is 0.25, and for long-answer tasks, it is 0.09, which underscores GPT-4's struggle when it lacks a reference for exact matching.We attribute this reduction to the increased complexity of both solving and rating the task, which is more pronounced in Swedish, where there is a higher incidence of models overgenerating.While such over-generation may lead to lower human ratings, it could be favored by GPT-4 due to its bias towards longer and more verbose outputs (Zheng et al., 2023).</p>
<p>A notable observation is that without gold label references, GPT-4-no-gold is generally more prone to higher ratings, as seen in Figure 4's lower plot.While GPT-4-gold closely aligns with human judgments for incorrect outputs (88% for 1's and 81% for 3's), GPT-4-no-gold shows an alignment of 65% for 1's and 84% for 3's, indicating an opposite trend.This demonstrates the excessively positive stance of GPT-4-no-gold, also noted by Hada et al. (2024) in a different scenario.The positive bias is particularly evident in long-answer tasks and challenging short-answer tasks, such as Tasks 190 and 401.This tendency underscores the importance of gold labels as references to help align the GPT-4's judgments with humans in most tasks.</p>
<p>However, we note that for some long generation tasks (Tasks 613, 677, and 1659) the scores of human raters are lower than for other tasks even when gold labels are present, as seen in Figure 2. Gold references can therefore be restricting for these types of long generation tasks, where models can have correct answers that diverge from the gold label.This is problematic as it is to these types of tasks that LLM-as-a-judge is often applied, and where it could bring the most value compared to other metrics, particularly due to the multitude of potential correct answers.</p>
<p>Finding 3: For short-answer tasks, ROUGE-L is as effective as GPT-4-gold.</p>
<p>With a PA at 0.90 for English short-answer tasks, ROUGE-L is nearly as well-aligned with human ratings as GPT-4-gold, which has a PA at 0.93.Our strong results for ROUGE-L are in line with findings by Wang et al. (2022), who report a high correlation with humans for classification tasks.With that said, there are instances where ROUGE-L has low alignment with human majority rating for short-answer tasks, for example when there is high word overlap between possible answer choices.For example, in Task 1615 the labels are "B entails A", "B contradicts A", or "B neutral A".A wrong answer for this task yields a ROUGE-L score of 0.66, as long as the answer is in the possible answers space.The same issue is observed for Task 392 where the label space is "Plausible" and "Not Plausible".</p>
<p>These types of issues also make it problematic to report average ROUGE-L scores across tasks since a baseline model that always makes wrong predictions could inflate its score beyond its actual performance level.However, as previously discussed, ROUGE-L correlation with humans and GPT-4-gold does not carry over to long-answer tasks, which makes it only suitable as a replacement for GPT-4 when evaluating short-answer tasks.It is important to note that while ROUGE-L demonstrates strong agreement with humans, GPT-4-gold scores are more interpretable as they better align with human judgments, as illustrated at the bottom of Figures 1 and 2.</p>
<p>Finding 4: BERTSCORE demonstrates strong performance in long-answer tasks.</p>
<p>With a PA of 0.54 for long-answer tasks, BERTSCORE shows a alignment with humans comparable with GPT-4-gold, which scores 0.58.A comparison of BERTSCORE and GPT-4-gold's PAs are shown in Figure 3.For complete results of BERTSCORE see Appendix C.3.BERTSCORE achieves comparable results to GPT-4-gold on all long-answer tasks, only underperforming on some of them, particularly Task 1622, where it captures the first criterion which requires a high semantic similarity between the two, but fails to take into account whether enough words have been changed, such as in cases involving synonyms, which is explicitly mentioned in the instructions.</p>
<p>Finding 5: Swedish presents a challenge for certain metrics.</p>
<p>For ROUGE-L, a reduction of 0.074 in PA is observed for Swedish compared to English.In contrast, GPT-4-gold experiences no significant reduction when switching from English to Swedish.However, GPT-4-no-gold is less consistent, showing a reduction of 0.044.GPT-4's decrease in performance when it does not have access to gold references for Swedish outputs suggests that the model has more difficulties solving the task when it is in another language.We believe the effect could be even more pronounced for languages with less training data than Swedish or less typologically similar to English.Further studies would be necessary to investigate this hypothesis.</p>
<p>While ROUGE-L does not take language into account, it may be less reliable a measurement for languages other than English due to models' failing to adhere to the required format.For instance, we observe that Swedish models have difficulties following instructions, even for short-answer tasks.They sometimes generate synonyms to the true labels, e.g., sannolikt "probable" instead of troligt "likely", an effect that could stem from seeing less data in Swedish and therefore having less reliable instruction-following capabilities.This is particularly concerning given the prevalence of work utilizing automatic evaluation measures across different languages.</p>
<p>Conclusions and Future Work</p>
<p>This study provides insights into the methods we use to evaluate language model generations, focusing on when automatic metrics align with human annotators and what the best metric is under different scenarios.We are the first to do a broader meta-evaluation study where we compare GPT-4as-a-judge and traditional metrics with a methodology that allows for reliable comparisons between metrics.We recommend using Pairwise Accuracy (PA) with Tie Calibration for meta-evaluation.This method effectively handles ties, which are prevalent when using human and GPT-4 ratings, making it a reliable tool for assessing metric performance against human ratings.</p>
<p>Our main finding is that GPT-4 shows strong alignment with human judgments for short-answer tasks, but only when gold references are provided.The reliability drops significantly without gold references, as the model is overly positive compared to human evaluations.The issue is particularly evident in free-form tasks, which are tasks where LLMas-a-judge could be the most valuable and where gold labels are typically not available.When gold references are available, we observe that GPT-4 is too strict compared to humans, relying to much on the gold label.For these type of tasks, even though LLM-as-a-judge is often applied to them, human evaluations still remain the gold standard.</p>
<p>ROUGE-L performs comparably to GPT-4-gold for short-answer tasks, offering a cost-effective alternative in scenarios where the use of GPT-4 is limited by cost or time constraints.For long-answer tasks, while BERTSCORE demonstrates strong performance, it does not fully replace the need for GPT-4-gold.These metrics provide valuable insights but vary in effectiveness depending on the specific task.</p>
<p>Evaluating non-English outputs, such as Swedish, presents additional challenges.There is a significant drop in alignment for GPT-4-no-gold, which highlights that GPT-4 as a judge is less reliable for languages other than English.While this is true for Swedish, we expect these findings to be more sever for lesser-resourced languages and those less similar to English.Future work should focus on expanding this study to more languages.</p>
<p>As we have observed, there is a large variation in alignment with human ratings for all metrics across task types.Previous research identifies strong correlations with human annotators, but that is often the average over tasks.Our findings underscores the necessity of task-specific evaluation metrics rather than relying on general averages which can obscure important nuances in metric alignment with human annotators.Furthermore, while GPT-4 is a valuable tool for evaluation short-answer tasks when gold references are available, alternatives like ROUGE-L and BERTSCORE can be effective for most tasks types, offering cost-efficient and reliable evaluations.</p>
<p>Limitations</p>
<p>We choose to report our results using pairwise accuracy which we believe provides more robust and reliable alignment statistics compared to common correlation metrics.With that said, PA has its shortcomings, such as when it faces constant or close-toconstant scores.For example, when the reference vector is ⃗ 1, a metric vector of ⃗ 1 or ⃗ 3 both result in very high PAs, due to the lack of prior knowledge about the metric range.However, this issue is not unique to PA; common correlation metrics also face the same challenge in the case of close-to-constant vectors.</p>
<p>Our study primarily focuses on the evaluation of tasks across English and Swedish.Consequently, the findings may not be applicable to languages that have syntax and structure significantly different from English.We deliberately made this choice to enable a broader examination of tasks and tap into expert knowledge for deeper analyses.Essentially, we prioritized expanding the range of tasks and delving deeper into analysis rather than focusing on additional languages.Furthermore, our evaluation exclusively uses GPT-4 as the language model for assessment.The rapidly evolving landscape of language models suggests the existence of other models that may yield different results or exhibit different patterns.</p>
<p>Ethical Considerations</p>
<p>Our annotators, residents of Sweden, were selected for their proficiency in both English and Swedish, ensuring precise interpretation and annotation of content.We ensured their fair compensation in line with prevailing norms for similar tasks in Sweden.Furthermore, they completed their assignments within a reasonable timeframe, enabling them to work without undue pressure.Prior to acceptance, annotators were briefed on the purpose of their annotations ensuring that they understood the objectives and context behind the tasks assigned to them.</p>
<p>A Task Descriptions</p>
<p>Task033: Winogrande Answer Generation A fill-in-the-blank task with some restriction, such as that the answer should be chosen from the two objects in the question."I planted more tomato seeds than I planted cucumber seeds since I hated eating the _ ."Goldanswer: "cucumber".</p>
<p>Task034: Winogrande Question Modification Object Similar to task033, but this time the task is to change the question so that the answer, which is given in the input, changes to the other object present in the input.</p>
<p>Task036: QASC Topic Word to Generate Related Fact Write a topic word for the given fact with at least one word overlap with the fact.Example: "Fact: a seismograph is used for measuring the size of an earthquake."Onepossible gold answer: "seismograph earthquake."</p>
<p>Task133: Winowhy Reason Plausibility Detection Indicate the plausibility of reasoning for the pronoun coreference relations.Example: "Sentence: Although they ran at about the same speed, Sue beat Sally because she had such a bad start.\nReason: The 'she' refers to sally because Sue won, sally lost.\n Question: the above reasoning correct or wrong?"Gold answer: "Correct".</p>
<p>Task190: SNLI Classification Given two sentences, classify their agreement: entailment, contradiction, or neutral.Task200: MNLI Entailment Classification From three options, choose the one that can be inferred from the given sentence.</p>
<p>Task202: MNLI Contradiction Classification</p>
<p>From three options, choose the one that disagrees with the given sentence.</p>
<p>Task329: GAP Classification Given a text, a pronoun, and two candidate names, determine which of the names the pronoun refers to.The answer should be either A, B, or neither.</p>
<p>Task349: SQuAD2.0:Answerable Unanswerable Question Classification Determine whether or not the given question is answerable by the provided passage.</p>
<p>Task392: Inverse Causal Relationship Given two sentences separated by the word "because", determine whether the second sentence can be the result of the first one (is there a cause and effect relationship?) Task401: Numeric Fused Head Reference Using your knowledge about language and commonsense, determine what element the marked number refers to.Example: "Jim Bronson: What 's your name ?\nTemple Brooks: I do n't have _ one _ !\nJim Bronson: Well everyone I have ever know had a name , that 's really weird .My name is Jim incase your interested .\nTempleBrooks: Well I 'm not !"Goldanswer: "name".Task520: AQuaMuSe Answer Given in Passage Is the answer to the given question contained in the provided passage?</p>
<p>Task613: PolitiFact Text Generation Generate the subject of a speech by a politician.</p>
<p>Task677: Ollie Sentence Answer Generation Given two noun phrases (arguments) and the relationship between them, write a sentence that expresses theses arguments with the given relationship.</p>
<p>Task891: GAP Coreference Resolution Given a passage, find the corresponding person for the provided pronoun.</p>
<p>Task1152: BARD Analogical Reasoning Causation Replace question mark with a verb which is the appropriate consequence of the given action.For example: "ignite : burn.hit : ?". Gold answer: "shatter".</p>
<p>B Prompt Templates</p>
<p>B.1 Translation</p>
<p>We use the following prompt for translating our datasets from English to Swedish using GPT-3.5-turbo:</p>
<p>Translate the following text from English to Swedish : { English text }</p>
<p>B.2 LLM-as-a-judge</p>
<p>The prompt used for GPT-4 could be found in Table A2.The prompt for GPT-4-no-gold is the same, but without the following part:</p>
<p>[ Gold Answer ] ( If there are several gold answers then they are all correct alternatives ): { gold_answer } ***</p>
<p>C Supplementary Results</p>
<p>C.1 How Good Are Our Language Models at</p>
<p>Swedish?</p>
<p>To assess the effectiveness of the models described in Section 3.2, we measure their perplexity.To ensure the generated texts meet high standards and to avoid assessing the models on data used during their pretraining, we use a custom dataset consisting of current news articles from SVT 3 , the Swedish national public television broadcaster.16th, 2023.To address the variability caused by different tokenizers across various models, we use character length normalization when calculating perplexity (Liang et al., 2022;Yong et al., 2023).</p>
<p>The perplexity for Swedish consistently remains lower in models prior to instruction tuning.However, after tuning, the poorest outcomes are noted in the SV models trained solely on Swedish data.Interestingly, the ENSV models exhibit improved performance, with the EN models showing even better results.This variation could be ascribed to the Swedish-specific model weights being less affected due to their lower exposure to Swedish data, but requires further investigations.Notably, the incre-ments in perplexity are less pronounced for the SW3 models.</p>
<p>C.2 Pairwise Accuracy per Model</p>
<p>Pairwise accuracy per model is shown in Table A2.</p>
<p>C.3 BERTSCORE Results</p>
<p>Detailed</p>
<p>D Implementation Details</p>
<p>Following Taori et al. (2023), we finetune LLAMA2_7b and SW3_6.7b,which is roughly the same size, for 3 epochs and with a learning rate of 2e − 5, and the larger LLAMA2_13b model for 5 epochs and with a learning rate of 1e−5.The batch size is set to 128 for both cases.Unlike Taori et al. (2023), we allow for a longer maximum length of 2048 and truncate longer samples.For the sake of reducing computational costs we opt for using bf16 and tf32 precision formats.We distribute the training across multiple GPUs using DeepSpeed (Rasley et al., 2020) stage 3 without offloading.</p>
<p>E Human Evaluations</p>
<p>Our evaluators are not crowd-sourced workers; instead, they are individuals with some experience and expertise in the field.They were carefully selected for their familiarity with the subject matter and were hired specifically for this evaluation task.While they were instructed to use their own judgment in the assessment, they also had access to the gold standard answers to guide their evaluations.The instructions given to human evaluators were similar to those given to GPT-4, as presented in Figure 3. user \ _message = """ You are evaluating a response that has been submitted for a particular task , using a specific set of standards .Below is the data : 1: " Not at all natural -The generated text is grammatically incorrect or sounds unnatural , including awkward phrasing or inappropriate vocabulary ."2: " Somewhat natural -The generated text has minor grammatical errors or slightly awkward phrasing but is mostly understandable and natural ."3: " Completely natural -The generated text is grammatically correct , well -phrased , and uses appropriate vocabulary , sounding completely natural ."</p>
<p>Relatedness : 1: " Not at all related -The model 's answer does not relate to the question , fails to follow the required format , or is outside the scope of possible answers ."2: " Somewhat related -The model 's answer is related to the question to some extent and mostly follows the required format , staying generally within the scope of possible answers ."3: " Completely related -The model 's answer is directly related to the question , follows the required format accurately , and fits within the scope of possible answers ."</p>
<p>Correctness :</p>
<p>1: " Not at all correct -The answer is completely incorrect or irrelevant to the question posed ."2: " Somewhat correct -The answer is partially correct but includes some inaccuracies or incomplete information ."3: " Completely correct -The answer is fully correct , accurate , and provides a complete response to the question ."</p>
<p>*** [ END DATA ]</p>
<p>Does the submission meet the criterion ?First , write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct .Avoid simply stating the correct answers at the outset .Your response must be RFC8259 compliant JSON following this schema : {{" reasoning ": str , " naturalness ": int , " relatedness ": int , " correctness ": int }} """ Figure A2: The prompt for GPT-4 as evaluator.For GPT-4-no-gold the gold answer is removed.</p>
<p>Figure 1 :
1
Figure1: Human ratings and ROUGE-L scores per task and test language at the bottom and their PA at the top.Human scores are normalized to a range between 0 and 1. Short-answer tasks are on the left and long-answer ones are on the right.</p>
<p>Figure 2 :Figure 3 :
23
Figure2: Human and GPT-4-gold's ratings per task and test language on the bottom and their PA on top.Ratings are normalized to a range of 0 to 1. Short-answer tasks are on left and long-answer ones on right.</p>
<p>Figure 4 :
4
Figure4: Human and GPT-4-no-gold's ratings per task and test language on the bottom and their PA on top.Ratings are normalized to a range of 0 to 1. Short-answer tasks are on left and long-answer ones on right.</p>
<p>Task1562: ZEST Text Modification Paraphrase the given questions to have different wording.Change it as much as possible using synonyms, etc. Example: "Does this dog breed always have spots?".Task1615: SICK Classify b Relation a Classify the relation between two sentences: B_entails_A, B_contradicts_A, or B_neutral_A.Task1622: Disfl-QA: Text Modification Convert a disfluent question to a proper question.Example: "Who were uh instead tell me how many quadrangles does the Main Quadrangles have?"Task1659: Title Generation Generate a title under forty words which mentions the purpose of the text.</p>
<p>Figure A1 :
A1
FigureA1: Human and BERTSCORE ratings per task and test language on the bottom and their PA on top.The human ratings are normalized to a range of 0 to 1. Short-answer tasks are on left and long-answer ones on right.</p>
<p>results of BERTSCORE are shown in Figure A1.</p>
<p>[</p>
<p>] ( If there are several gold answers then they are all correct alternatives ): { gold_answer } *** [ Criterion ]: Evaluation Criteria Naturalness :</p>
<p>Table 1 :
1
The average tie proportion per metric for English-language tasks.
MetricTie ProportionϵHuman Ratings0.557 ± 0.1620.000GPT-4-gold0.524 ± 0.1540.000ROUGE-L0.355 ± 0.2520.061BERTSCORE0.104 ± 0.1410.133</p>
<p>Table 2 :
2
Human evaluation results per model scaled to 0 to 100.For comparison, GPT-4 ratings are included in parentheses for each model and criterion.</p>
<p>Table A1 :
A1
The perplexity of our models on the SVT dataset.The abbreviations are the training language(s).</p>
<p>https://github.com/gururise/AlpacaDataCleaned
Running an evaluation (across all models, tasks, and languages) costs around USD 100 and typically takes 15-20 minutes. Though not significantly cheaper than human evaluations, it certainly surpasses it in terms of time efficiency.
AcknowledgmentsWe extend our gratitude to our annotators for their diligent contributions to this paper and to Daniel Deutsch and Richard Johansson for their valuable feedback.This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP), funded by the Knut and Alice Wallenberg Foundation and by the European Union's Horizon 2020 research and innovation program under grant agreement No 101135671 (TrustLLM).We also acknowledge support from the National Graduate School of Computer Science in Sweden (CUGS).The computations were enabled by the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the Swedish National Supercomputer Center.ModelGPT-4-gold GPT-4-no-gold
Chateval: Towards better LLM-based evaluators through multi-agent debate. Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Monolingual or multilingual instruction tuning: Which makes a better alpaca. Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov, Barry Haddow, Kenneth Heafield, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>A closer look into using large language models for automatic evaluation. Cheng- , Han Chiang, Hung-Yi Lee, 10.18653/v1/2023.findings-emnlp.599Findings of the Association for Computational Linguistics: EMNLP 2023. Singapore2023Association for Computational Linguistics</p>
<p>Ties matter: Meta-evaluating modern metrics with pairwise accuracy and tie calibration. Daniel Deutsch, George Foster, Markus Freitag, 10.18653/v1/2023.emnlp-main.798Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>GPT-SW3: An autoregressive language model for the Scandinavian languages. Ariel Ekgren, Amaru Cuba Gyllensten, Felix Stollenwerk, Joey Öhman, Tim Isbister, Evangelia Gogoulou, Fredrik Carlsson, Judit Casademont, Magnus Sahlgren, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability. L Joseph, Jacob Fleiss, Cohen, Educational and psychological measurement. 3331973</p>
<p>Chatgpt outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, 10.1073/pnas.2305016120Proceedings of the National Academy of Sciences. 12030e23050161202023</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. St. Julian's, MaltaAssociation for Computational Linguistics2024</p>
<p>A fine-grained analysis of BERTScore. Michael Hanna, Ondřej Bojar, Proceedings of the Sixth Conference on Machine Translation. the Sixth Conference on Machine TranslationOnline. Association for Computational Linguistics2021</p>
<p>Making instruction finetuning accessible to non-English languages: A case study on Swedish models. Oskar Holmström, Ehsan Doostmohammadi, Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa). the 24th Nordic Conference on Computational Linguistics (NoDaLiDa)Tórshavn, Faroe Islands2023University of Tartu Library</p>
<p>Unnatural instructions: Tuning language models with (almost) no human labor. Or Honovich, Thomas Scialom, Omer Levy, Timo Schick, 10.18653/v1/2023.acl-long.806Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Turning english-centric llms into polyglots: How much multilinguality is needed?. Tannon Kew, Florian Schottmann, Rico Sennrich, arXiv:2312.126832023arXiv preprint</p>
<p>ChatGPT beyond English: Towards a comprehensive evaluation of large language models in multilingual learning. Viet Lai, Nghia Ngo, Amir Pouran, Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, Thien Nguyen, 10.18653/v1/2023.findings-emnlp.878Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, Timothy Baldwin, arXiv:2305.15011Bactrian-x: A multilingual replicable instruction-following model with lowrank adaptation. 2023aarXiv preprint</p>
<p>Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, arXiv:2306.04387M 3 IT: A largescale dataset towards multi-modal multilingual instruction tuning. 2023barXiv preprint</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, arXiv:2211.09110Holistic evaluation of language models. Ananya Kumar, et al. 2022arXiv preprint</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational Linguistics2004</p>
<p>Calibrating LLMbased evaluator. Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>
<p>Cross-task generalization via natural language crowdsourcing instructions. Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi, 10.18653/v1/2022.acl-long.244Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20221</p>
<p>Crosslingual generalization through multitask finetuning. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, 10.18653/v1/2023.acl-long.891Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Zaid Alyafeai, Albert Webson, Edward Raff, Colin Raffel, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao, arXiv:2304.03277Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Empowering cross-lingual abilities of instruction-tuned large language models by translation-following demonstrations. Leonardo Ranaldi, Giulia Pucci, Andre Freitas, 10.18653/v1/2024.findings-acl.473Findings of the Association for Computational Linguistics ACL 2024. Bangkok, ThailandAssociation for Computational Linguistics2024and virtual meeting</p>
<p>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He, 10.1145/3394486.3406703Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20. the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, KDD '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>Multitask prompted training enables zero-shot task generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Saiful Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Zheng Xin Shen, Harshit Yong, Rachel Pandey, Thomas Bawden, Trishala Wang, Neeraj, International Conference on Learning Representations. Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason , Alan Fries, Ryan Teehan, Le Teven, Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush2022</p>
<p>Large language models are not yet human-level evaluators for abstractive summarization. Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, Lidong Bing, 10.18653/v1/2023.findings-emnlp.278Findings of the Association for Computational Linguistics: EMNLP 2023. SingaporeAssociation for Computational Linguistics2023</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, 10.18653/v1/2023.acl-long.754Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Rohitha Phani, Pulkit Kaza, Ravsehaj Verma, Rushang Singh Puri, Savan Karia, Doshi, Keyur Shailaja, Siddhartha Sampat, Sujan Mishra, A Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, 10.18653/v1/2022.emnlp-main.340Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi2022United Arab Emirates. Association for Computational Linguistics</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>
<p>Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning. Fan Yin, Jesse Vig, Philippe Laban, Shafiq Joty, Caiming Xiong, Chien-Sheng Wu, 10.18653/v1/2023.acl-long.172Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>BLOOM+1: Adding language support to BLOOM for zero-shot prompting. Yong Zheng Xin, Hailey Schoelkopf, Niklas Muennighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, Lintang Saiful Bari, Jungo Sutawika, Ahmed Kasai, Genta Baruwa, Stella Winata, Edward Biderman, Dragomir Raff, Vassilina Radev, Nikoulina, 10.18653/v1/2023.acl-long.653Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, * , Varsha Kishore, * , Felix Wu, * , Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2020</p>
<p>Judging LLM-as-a-judge with MT-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2023</p>
<p>LIMA: Less is more for alignment. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, Omer Levy, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
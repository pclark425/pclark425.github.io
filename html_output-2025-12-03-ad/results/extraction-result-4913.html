<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4913 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4913</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4913</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-103.html">extraction-schema-103</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <p><strong>Paper ID:</strong> paper-6bca998065fd7644b4b5fca4b717529c10c32941</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/6bca998065fd7644b4b5fca4b717529c10c32941" target="_blank">Large Language Model Is Semi-Parametric Reinforcement Learning Agent</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> The proposed R EMEMBERER constitutes a semi-parametric RL agent capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4913.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4913.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>REMEMBERER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>REMEMBERER</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An evolvable LLM-based agent framework that augments a pretrained LLM with a persistent external experience memory (records of task, observation, action, Q-value) and updates that memory via reinforcement-learning-style updates to enable semi-parametric policy improvement without fine-tuning the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>REMEMBERER</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Architecture: a decision LLM (GPT-3.5 text-davinci-003 in experiments) plus an external structured experience memory storing (task g, observation o, action a, Q-value). At each step the LLM retrieves m nearest experiences by a similarity function, uses them as dynamic exemplars (encouraged/discouraged actions with Q estimates) in-context to choose actions, and the memory is updated off-policy by Q-learning with n-step bootstrapping (RLEM pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external persistent episodic experience memory (structured lookup table of (task, observation, action, Q))</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory implemented as a table of records (g, o, a, Q). Retrieval uses a composite similarity (task similarity f_g and observation similarity f_o) to select exemplars; exemplars present encouraged and discouraged actions with stored Q estimates to the LLM; memory updated by Q-learning (Q' = n-step return + bootstrap) with learning rate alpha = 1/N and discount gamma = 1.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebShop; WikiHow</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>WebShop: simulated web-store shopping tasks (agents browse pages, choose items; final score 0–1); WikiHow: mobile-app navigation tasks to follow instructions and find pages (stepwise interaction, intermediate rewards).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebShop; WikiHow</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>WebShop: Avg Score 0.68, Success Rate 0.39 (REMEMBERER). WikiHow: Avg Reward 2.63, Success Rate 0.93 (REMEMBERER).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>WebShop (LLM-only fixed exemplars baseline): Avg Score 0.55, Success Rate 0.29. ReAct baseline on WebShop: Avg Score 0.66, Success Rate 0.36. WikiHow (LLM-only baseline): Avg Reward 2.58, Success Rate 0.90.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Adding a persistent experience memory and updating it via RLEM improved performance over a single LLM with fixed exemplars (LLM-only) and slightly exceeded prior SOTA baselines (e.g., ReAct) on the tested splits: WebShop +0.03 success-rate points over ReAct (0.39 vs 0.36) and vs LLM-only larger gains (+0.10 score, +0.10-> success). WikiHow improvements were smaller but consistent (2.63 vs 2.58 reward; success 0.93 vs 0.90). Ablations show n-step bootstrapping, discouraged-action advice, and observation similarity are important for gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors report applicability concerns for longer-horizon or visual-rich environments, quick saturation of training gains (limited number of active exemplars), limited active exemplar capacity leading to performance plateau, and that more advanced RL techniques were not explored; memory Q estimates can be inaccurate without bootstrapping and Double Q-Learning introduced under-estimation in few-step regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Long-term, cross-goal persistent experience memory (structured with Q-values) can let LLM agents learn from both successes and failures without model fine-tuning; using retrieved experiences as dynamic exemplars with encouraged/discouraged actions helps guide action selection; n-step bootstrapping stabilizes Q estimates for few-step training; observation similarity matters more than coarse task-pattern similarity on tested tasks; discouraged actions prevent repeating failure patterns; precise Q-value regression by the LLM is unnecessary — relative ranking suffices for good decisions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4913.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLEM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reinforcement Learning with Experience Memory (RLEM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The pipeline and algorithmic procedure used to update the external experience memory: collect transitions (o,a,r,o'), compute n-step bootstrapped targets, and update Q-values in memory via lookup Q-learning (alpha = 1/N).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RLEM (method used with REMEMBERER)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>RLEM orchestrates interaction, retrieval, in-context decision making by the LLM, and memory updates: it computes Q' (n-step returns + bootstrap), inserts new (g,o,a) records or updates existing Q via Q-learning, and drives which experiences are available as exemplars for future decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external persistent Q-value experience memory (lookup table)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory updated by off-policy Q-learning and n-step bootstrapping; when a new transition (g,o_t,a_t,r_t,o_{t+1}) arrives compute Q'(g,o_t,a_t)=sum_{i=0}^{n-1} gamma^i r_{t+i} + gamma^n max_a Q(g,o_{t+n},a) (with gamma=1 in experiments), then insert or update using alpha = 1/N.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebShop; WikiHow (same experimental tasks where RLEM was applied)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See REMEMBERER entry: WebShop web-shopping episodes; WikiHow mobile navigation episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebShop; WikiHow</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Same as REMEMBERER (RLEM is the training/updating pipeline for the REMEMBERER agent): WebShop: Avg Score 0.68 / Success 0.39; WikiHow: Avg Reward 2.63 / Success 0.93.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>RLEM (n-step bootstrapped Q updates) produces more accurate memory-derived reward estimates (lower absolute error vs training reward) than non-bootstrapped updates; ablation shows bootstrapping is important to avoid large estimation error and to preserve test performance (especially on WikiHow).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Q estimates can be over- or under-estimated; Double Q-Learning reduced over-estimation but introduced under-estimation in few-step regimes; limited training steps make learning accurate global Q functions hard; memory capacity / exemplar selection leads to saturation.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Updating external memory with RL-style Q-learning and n-step bootstrapping enables an evolvable semi-parametric agent; accurate bootstrapped returns are important for reliable memory value estimates when training steps are few.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4913.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-only baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-only (fixed exemplars) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline consisting of a single pretrained LLM prompted with a small fixed set of exemplars (2-shot) sampled from initial experiences; exemplars are static and not updated during training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>LLM-only (fixed exemplars)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>GPT-3.5 text-davinci-003 prompted with two fixed exemplars (sampled from initial experiences). No external persistent memory or RL-based updating used; exemplars do not change across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>in-prompt fixed exemplars (transient / non-updating)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Two static exemplars presented in-context to the LLM; provides format and limited guidance but not updated from online experience.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebShop; WikiHow</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same as REMEMBERER experiments: WebShop web-shopping; WikiHow navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebShop; WikiHow</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>WebShop: Avg Score 0.55, Success Rate 0.29. WikiHow: Avg Reward 2.58, Success Rate 0.90.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Static exemplars are substantially less effective than REMEMBERER's persistent memory that evolves via RLEM; REMEMBERER improves both score and success rate compared to this baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Cannot accumulate cross-episode experience; exemplars provide limited, fixed guidance causing lower generalization to unseen tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Fixed, non-evolving exemplars are insufficient to capture cross-task experiential learning; evolving memory provides measurable gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4913.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting framework that synergizes chain-of-thought style reasoning and acting (action outputs) from a language model to generate plans and actions in interactive environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ReAct: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses LLM-generated natural-language reasoning interleaved with action outputs (ReAct prompting) to make decisions; in this paper ReAct is used as a competitive baseline run with the same LLM (text-davinci-003).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>transient in-context chain-of-thought / short-term working context (no persistent external memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Relies on in-prompt reasoning traces and immediate procedural feedback (e.g., last actions) but does not maintain a persistent cross-episode experience memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>WebShop (evaluated as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>WebShop web-shopping interactive task; in this work ReAct was run on the same trajectories for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>WebShop</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>WebShop baseline (ReAct): Avg Score 0.66, Success Rate 0.36 (reported when run by the authors with text-davinci-003).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>REMEMBERER modestly outperforms ReAct on WebShop (0.68 vs 0.66 score; success 0.39 vs 0.36), suggesting persistent experience memory provides additional benefit beyond ReAct's reasoning traces for these tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>ReAct's ephemeral reasoning traces are tied to a trial and cannot supply cross-task, long-term experiential learning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Combining LLM reasoning (ReAct) with a persistent experience memory can yield improvements over reasoning-only in-context approaches.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4913.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system that augments an LLM with a working memory of reflections (short-term reflections tied to a specific task) to improve performance via self-reflection across trials.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Reflexion: an autonomous agent with dynamic memory and self-reflection</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Reflexion</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Generates and stores LLM-produced reflections from failed trials in a working memory for use in subsequent attempts on the same task; focuses on immediate failure correction across repeated trials.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>short-term working memory (task-tied reflections)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores LLM-generated reflections bound to tasks and uses them to modify future prompts for subsequent trials on the same task; not designed for cross-goal reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper contrasts REMEMBERER's long-term cross-goal persistent memory with Reflexion's short-term task-bound working memory, arguing REMEMBERER enables reuse of experiences across different goals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Reflexion reflections are task-specific and cannot generalize across different task goals; limited reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Short-term working memory is useful for per-task improvement but insufficient for cross-task experiential learning; motivates persistent memory designs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4913.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MemPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-assisted prompt editing to improve gpt-3 after deployment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses a persistent memory of human feedback to improve a chatbot continuously by editing prompts with stored feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory-assisted prompt editing to improve gpt-3 after deployment</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MemPrompt</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Stores long-term human feedback and uses stored items to edit prompts, improving chatbot behavior over time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>persistent memory storing human feedback (long-term)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Persistent memory records human corrections/feedback which are later retrieved to edit prompts and influence LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as related work that stores long-term human feedback; contrasted with REMEMBERER which stores environment-grounded interaction experiences and learns from rewards without human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Requires human feedback to populate memory (as noted in paper discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Persistent memory can improve deployed LLMs, but dependence on human feedback differs from REMEMBERER's automated RL-driven memory updates.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4913.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RET-LLM: towards a general read-write memory for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system exploring a read-write memory for LLMs enabling storage and retrieval beyond prompt length limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RET-LLM: towards a general read-write memory for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RET-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Proposes mechanisms for a general persistent memory used by LLMs to read and write information across interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>general read-write persistent memory (long-term)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Memory that allows structured read-write operations to augment LLMs' context beyond input-length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited as related work on persistent memories for LLMs; REMEMBERER differs by storing environment-grounded experiences annotated with Q-values and updating via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Read-write memory abstractions are relevant to enabling long-term LLM augmentation; REMEMBERER implements a task-grounded, Q-valued memory instantiation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4913.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memorybank</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior work that augments LLMs with long-term memory to provide persistent context across interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memorybank: Enhancing large language models with long-term memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Memorybank</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Provides a mechanism for LLMs to access stored long-term memories to improve responses over time.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>long-term persistent memory</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores dialogues or other persistent records to be retrieved as needed to augment LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned among works using persistent memory; REMEMBERER differs by storing environment-grounded trajectories and using RL updates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Long-term memory aids continual improvement; REMEMBERER grounds the memory in RL signals and Q-values.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e4913.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that stores learned skills (as executable artifacts like JavaScript functions) in a skill library to enable reuse by an LLM agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Voyager: An open-ended embodied agent with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Voyager</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Embodied agent that accumulates learned skills into a reusable skill library (code snippets) to bootstrap future tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>skill library / persistent storage of learned skills (structured artifacts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores past learned skills as code functions (JavaScript) which can be reused by the agent for future tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited as an example of persistent skill storage; contrasts with REMEMBERER's experience-Q table rather than executable skill artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Storing reusable skills is another form of persistent memory enabling agent evolution; REMEMBERER stores action-value experiences rather than code skills.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e4913.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GITM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GITM (Ghost in the Minecraft)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that stores successful trajectories in a simple text experience pool for later retrieval to guide LLM-based agents in open-world environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GITM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>LLM-based agent using a text experience pool of successful trajectories for referencing in future decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>text experience pool (persistent)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Stores plain-text trajectories of successful episodes for later retrieval; used to inform future decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Minecraft / open-world environments (in original work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-world embodied tasks (Minecraft) where prior successful trajectories are useful as exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper notes similarity to GITM's use of persistent successful trajectories but highlights REMEMBERER's structured memory including failed experiences and Q-values.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Plain-text successful trajectories may not encode value estimates or failed-case avoidance as REMEMBERER does.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Storing successful trajectories helps future decisions, but augmenting memory with Q-values and failed experiences can offer richer guidance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e4913.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inner Monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses internal chain-of-thought (inner monologue) for planning with LLMs in embodied tasks; uses immediate feedback rather than long-term persistent memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inner monologue: Embodied reasoning through planning with language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Inner Monologue</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Interleaves planning and reasoning in natural language (internal thoughts) to guide action selection in embodied settings; leverages immediate feedback within the trial.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>transient internal scratchpad / working chain-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Uses in-trial natural-language planning traces; not focused on persistent cross-episode memory.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Mentioned as related work focusing on immediate failure feedback or in-trial reasoning; REMEMBERER differs by persisting experiences across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Immediate feedback and inner monologue help in-trial planning but do not substitute for a persistent, cross-goal experience store.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e4913.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Corrective Re-Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Planning with large language models via corrective re-prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A technique that uses corrective re-prompts from environment feedback to fix mistakes, typically applied once per episode rather than stored long-term.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Planning with large language models via corrective re-prompting</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Corrective Re-Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses corrective re-prompts based on immediate failure signals to change subsequent LLM decisions in that trial.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>immediate feedback (non-persistent corrective prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Takes advantage of immediate failure feedback to re-prompt LLMs for corrections, but not stored for long-term cross-episode use.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Paper contrasts one-off corrective re-prompting approaches with REMEMBERER's persistent learning-from-failure across episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Corrections used only once; may miss late-failure causes originating earlier in episode.</td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Single-shot corrective prompts are limited compared to persistent experience storage that enables cumulative learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4913.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e4913.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details about the agent, the type of memory used, the tasks or benchmarks, performance with and without memory, comparisons, and any reported insights or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DEPS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An interactive planning system that leverages LLMs to describe, explain, plan and select actions; uses immediate feedback loops rather than long-term experience memory.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DEPS</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Framework applying LLMs to interactive planning (describe/explain/plan/select) to enable open-world multi-task agents; uses immediate interactions rather than persistent RL-updated memory.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>immediate / in-trial interaction memory (not persistent cross-episode memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_description</strong></td>
                            <td>Leverages interactive planning steps but does not focus on an external persistent experience memory updated via RL.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_with_without_memory</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_comparison_summary</strong></td>
                            <td>Cited among systems that use immediate planning and feedback; contrasted with REMEMBERER's persistent experience memory enabling cross-goal reuse.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_insights</strong></td>
                            <td>Interactive planning is complementary to persistent memory; REMEMBERER emphasizes cumulative memory updates to evolve policy without fine-tuning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ReAct: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>Reflexion: an autonomous agent with dynamic memory and self-reflection <em>(Rating: 2)</em></li>
                <li>Memory-assisted prompt editing to improve gpt-3 after deployment <em>(Rating: 2)</em></li>
                <li>RET-LLM: towards a general read-write memory for large language models <em>(Rating: 2)</em></li>
                <li>Memorybank: Enhancing large language models with long-term memory <em>(Rating: 2)</em></li>
                <li>Voyager: An open-ended embodied agent with large language models <em>(Rating: 2)</em></li>
                <li>Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory <em>(Rating: 2)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
                <li>Planning with large language models via corrective re-prompting <em>(Rating: 1)</em></li>
                <li>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4913",
    "paper_id": "paper-6bca998065fd7644b4b5fca4b717529c10c32941",
    "extraction_schema_id": "extraction-schema-103",
    "extracted_data": [
        {
            "name_short": "REMEMBERER",
            "name_full": "REMEMBERER",
            "brief_description": "An evolvable LLM-based agent framework that augments a pretrained LLM with a persistent external experience memory (records of task, observation, action, Q-value) and updates that memory via reinforcement-learning-style updates to enable semi-parametric policy improvement without fine-tuning the LLM.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "REMEMBERER",
            "agent_description": "Architecture: a decision LLM (GPT-3.5 text-davinci-003 in experiments) plus an external structured experience memory storing (task g, observation o, action a, Q-value). At each step the LLM retrieves m nearest experiences by a similarity function, uses them as dynamic exemplars (encouraged/discouraged actions with Q estimates) in-context to choose actions, and the memory is updated off-policy by Q-learning with n-step bootstrapping (RLEM pipeline).",
            "memory_type": "external persistent episodic experience memory (structured lookup table of (task, observation, action, Q))",
            "memory_description": "Memory implemented as a table of records (g, o, a, Q). Retrieval uses a composite similarity (task similarity f_g and observation similarity f_o) to select exemplars; exemplars present encouraged and discouraged actions with stored Q estimates to the LLM; memory updated by Q-learning (Q' = n-step return + bootstrap) with learning rate alpha = 1/N and discount gamma = 1.",
            "task_name": "WebShop; WikiHow",
            "task_description": "WebShop: simulated web-store shopping tasks (agents browse pages, choose items; final score 0–1); WikiHow: mobile-app navigation tasks to follow instructions and find pages (stepwise interaction, intermediate rewards).",
            "benchmark_name": "WebShop; WikiHow",
            "performance_with_memory": "WebShop: Avg Score 0.68, Success Rate 0.39 (REMEMBERER). WikiHow: Avg Reward 2.63, Success Rate 0.93 (REMEMBERER).",
            "performance_without_memory": "WebShop (LLM-only fixed exemplars baseline): Avg Score 0.55, Success Rate 0.29. ReAct baseline on WebShop: Avg Score 0.66, Success Rate 0.36. WikiHow (LLM-only baseline): Avg Reward 2.58, Success Rate 0.90.",
            "has_performance_with_without_memory": true,
            "memory_comparison_summary": "Adding a persistent experience memory and updating it via RLEM improved performance over a single LLM with fixed exemplars (LLM-only) and slightly exceeded prior SOTA baselines (e.g., ReAct) on the tested splits: WebShop +0.03 success-rate points over ReAct (0.39 vs 0.36) and vs LLM-only larger gains (+0.10 score, +0.10-&gt; success). WikiHow improvements were smaller but consistent (2.63 vs 2.58 reward; success 0.93 vs 0.90). Ablations show n-step bootstrapping, discouraged-action advice, and observation similarity are important for gains.",
            "limitations_or_challenges": "Authors report applicability concerns for longer-horizon or visual-rich environments, quick saturation of training gains (limited number of active exemplars), limited active exemplar capacity leading to performance plateau, and that more advanced RL techniques were not explored; memory Q estimates can be inaccurate without bootstrapping and Double Q-Learning introduced under-estimation in few-step regimes.",
            "key_insights": "Long-term, cross-goal persistent experience memory (structured with Q-values) can let LLM agents learn from both successes and failures without model fine-tuning; using retrieved experiences as dynamic exemplars with encouraged/discouraged actions helps guide action selection; n-step bootstrapping stabilizes Q estimates for few-step training; observation similarity matters more than coarse task-pattern similarity on tested tasks; discouraged actions prevent repeating failure patterns; precise Q-value regression by the LLM is unnecessary — relative ranking suffices for good decisions.",
            "uuid": "e4913.0"
        },
        {
            "name_short": "RLEM",
            "name_full": "Reinforcement Learning with Experience Memory (RLEM)",
            "brief_description": "The pipeline and algorithmic procedure used to update the external experience memory: collect transitions (o,a,r,o'), compute n-step bootstrapped targets, and update Q-values in memory via lookup Q-learning (alpha = 1/N).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "RLEM (method used with REMEMBERER)",
            "agent_description": "RLEM orchestrates interaction, retrieval, in-context decision making by the LLM, and memory updates: it computes Q' (n-step returns + bootstrap), inserts new (g,o,a) records or updates existing Q via Q-learning, and drives which experiences are available as exemplars for future decisions.",
            "memory_type": "external persistent Q-value experience memory (lookup table)",
            "memory_description": "Memory updated by off-policy Q-learning and n-step bootstrapping; when a new transition (g,o_t,a_t,r_t,o_{t+1}) arrives compute Q'(g,o_t,a_t)=sum_{i=0}^{n-1} gamma^i r_{t+i} + gamma^n max_a Q(g,o_{t+n},a) (with gamma=1 in experiments), then insert or update using alpha = 1/N.",
            "task_name": "WebShop; WikiHow (same experimental tasks where RLEM was applied)",
            "task_description": "See REMEMBERER entry: WebShop web-shopping episodes; WikiHow mobile navigation episodes.",
            "benchmark_name": "WebShop; WikiHow",
            "performance_with_memory": "Same as REMEMBERER (RLEM is the training/updating pipeline for the REMEMBERER agent): WebShop: Avg Score 0.68 / Success 0.39; WikiHow: Avg Reward 2.63 / Success 0.93.",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "RLEM (n-step bootstrapped Q updates) produces more accurate memory-derived reward estimates (lower absolute error vs training reward) than non-bootstrapped updates; ablation shows bootstrapping is important to avoid large estimation error and to preserve test performance (especially on WikiHow).",
            "limitations_or_challenges": "Q estimates can be over- or under-estimated; Double Q-Learning reduced over-estimation but introduced under-estimation in few-step regimes; limited training steps make learning accurate global Q functions hard; memory capacity / exemplar selection leads to saturation.",
            "key_insights": "Updating external memory with RL-style Q-learning and n-step bootstrapping enables an evolvable semi-parametric agent; accurate bootstrapped returns are important for reliable memory value estimates when training steps are few.",
            "uuid": "e4913.1"
        },
        {
            "name_short": "LLM-only baseline",
            "name_full": "LLM-only (fixed exemplars) baseline",
            "brief_description": "Baseline consisting of a single pretrained LLM prompted with a small fixed set of exemplars (2-shot) sampled from initial experiences; exemplars are static and not updated during training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "LLM-only (fixed exemplars)",
            "agent_description": "GPT-3.5 text-davinci-003 prompted with two fixed exemplars (sampled from initial experiences). No external persistent memory or RL-based updating used; exemplars do not change across episodes.",
            "memory_type": "in-prompt fixed exemplars (transient / non-updating)",
            "memory_description": "Two static exemplars presented in-context to the LLM; provides format and limited guidance but not updated from online experience.",
            "task_name": "WebShop; WikiHow",
            "task_description": "Same as REMEMBERER experiments: WebShop web-shopping; WikiHow navigation.",
            "benchmark_name": "WebShop; WikiHow",
            "performance_with_memory": null,
            "performance_without_memory": "WebShop: Avg Score 0.55, Success Rate 0.29. WikiHow: Avg Reward 2.58, Success Rate 0.90.",
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Static exemplars are substantially less effective than REMEMBERER's persistent memory that evolves via RLEM; REMEMBERER improves both score and success rate compared to this baseline.",
            "limitations_or_challenges": "Cannot accumulate cross-episode experience; exemplars provide limited, fixed guidance causing lower generalization to unseen tasks.",
            "key_insights": "Fixed, non-evolving exemplars are insufficient to capture cross-task experiential learning; evolving memory provides measurable gains.",
            "uuid": "e4913.2"
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct",
            "brief_description": "A prompting framework that synergizes chain-of-thought style reasoning and acting (action outputs) from a language model to generate plans and actions in interactive environments.",
            "citation_title": "ReAct: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct (baseline)",
            "agent_description": "Uses LLM-generated natural-language reasoning interleaved with action outputs (ReAct prompting) to make decisions; in this paper ReAct is used as a competitive baseline run with the same LLM (text-davinci-003).",
            "memory_type": "transient in-context chain-of-thought / short-term working context (no persistent external memory)",
            "memory_description": "Relies on in-prompt reasoning traces and immediate procedural feedback (e.g., last actions) but does not maintain a persistent cross-episode experience memory.",
            "task_name": "WebShop (evaluated as a baseline)",
            "task_description": "WebShop web-shopping interactive task; in this work ReAct was run on the same trajectories for comparison.",
            "benchmark_name": "WebShop",
            "performance_with_memory": "WebShop baseline (ReAct): Avg Score 0.66, Success Rate 0.36 (reported when run by the authors with text-davinci-003).",
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "REMEMBERER modestly outperforms ReAct on WebShop (0.68 vs 0.66 score; success 0.39 vs 0.36), suggesting persistent experience memory provides additional benefit beyond ReAct's reasoning traces for these tasks.",
            "limitations_or_challenges": "ReAct's ephemeral reasoning traces are tied to a trial and cannot supply cross-task, long-term experiential learning.",
            "key_insights": "Combining LLM reasoning (ReAct) with a persistent experience memory can yield improvements over reasoning-only in-context approaches.",
            "uuid": "e4913.3"
        },
        {
            "name_short": "Reflexion",
            "name_full": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "brief_description": "A prior system that augments an LLM with a working memory of reflections (short-term reflections tied to a specific task) to improve performance via self-reflection across trials.",
            "citation_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "mention_or_use": "mention",
            "agent_name": "Reflexion",
            "agent_description": "Generates and stores LLM-produced reflections from failed trials in a working memory for use in subsequent attempts on the same task; focuses on immediate failure correction across repeated trials.",
            "memory_type": "short-term working memory (task-tied reflections)",
            "memory_description": "Stores LLM-generated reflections bound to tasks and uses them to modify future prompts for subsequent trials on the same task; not designed for cross-goal reuse.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper contrasts REMEMBERER's long-term cross-goal persistent memory with Reflexion's short-term task-bound working memory, arguing REMEMBERER enables reuse of experiences across different goals.",
            "limitations_or_challenges": "Reflexion reflections are task-specific and cannot generalize across different task goals; limited reuse.",
            "key_insights": "Short-term working memory is useful for per-task improvement but insufficient for cross-task experiential learning; motivates persistent memory designs.",
            "uuid": "e4913.4"
        },
        {
            "name_short": "MemPrompt",
            "name_full": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "brief_description": "A method that uses a persistent memory of human feedback to improve a chatbot continuously by editing prompts with stored feedback.",
            "citation_title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "mention_or_use": "mention",
            "agent_name": "MemPrompt",
            "agent_description": "Stores long-term human feedback and uses stored items to edit prompts, improving chatbot behavior over time.",
            "memory_type": "persistent memory storing human feedback (long-term)",
            "memory_description": "Persistent memory records human corrections/feedback which are later retrieved to edit prompts and influence LLM outputs.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mentioned as related work that stores long-term human feedback; contrasted with REMEMBERER which stores environment-grounded interaction experiences and learns from rewards without human intervention.",
            "limitations_or_challenges": "Requires human feedback to populate memory (as noted in paper discussion).",
            "key_insights": "Persistent memory can improve deployed LLMs, but dependence on human feedback differs from REMEMBERER's automated RL-driven memory updates.",
            "uuid": "e4913.5"
        },
        {
            "name_short": "RET-LLM",
            "name_full": "RET-LLM: towards a general read-write memory for large language models",
            "brief_description": "A system exploring a read-write memory for LLMs enabling storage and retrieval beyond prompt length limitations.",
            "citation_title": "RET-LLM: towards a general read-write memory for large language models",
            "mention_or_use": "mention",
            "agent_name": "RET-LLM",
            "agent_description": "Proposes mechanisms for a general persistent memory used by LLMs to read and write information across interactions.",
            "memory_type": "general read-write persistent memory (long-term)",
            "memory_description": "Memory that allows structured read-write operations to augment LLMs' context beyond input-length limits.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Cited as related work on persistent memories for LLMs; REMEMBERER differs by storing environment-grounded experiences annotated with Q-values and updating via RL.",
            "limitations_or_challenges": null,
            "key_insights": "Read-write memory abstractions are relevant to enabling long-term LLM augmentation; REMEMBERER implements a task-grounded, Q-valued memory instantiation.",
            "uuid": "e4913.6"
        },
        {
            "name_short": "Memorybank",
            "name_full": "Memorybank: Enhancing large language models with long-term memory",
            "brief_description": "A prior work that augments LLMs with long-term memory to provide persistent context across interactions.",
            "citation_title": "Memorybank: Enhancing large language models with long-term memory",
            "mention_or_use": "mention",
            "agent_name": "Memorybank",
            "agent_description": "Provides a mechanism for LLMs to access stored long-term memories to improve responses over time.",
            "memory_type": "long-term persistent memory",
            "memory_description": "Stores dialogues or other persistent records to be retrieved as needed to augment LLM outputs.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mentioned among works using persistent memory; REMEMBERER differs by storing environment-grounded trajectories and using RL updates.",
            "limitations_or_challenges": null,
            "key_insights": "Long-term memory aids continual improvement; REMEMBERER grounds the memory in RL signals and Q-values.",
            "uuid": "e4913.7"
        },
        {
            "name_short": "Voyager",
            "name_full": "Voyager: An open-ended embodied agent with large language models",
            "brief_description": "A system that stores learned skills (as executable artifacts like JavaScript functions) in a skill library to enable reuse by an LLM agent.",
            "citation_title": "Voyager: An open-ended embodied agent with large language models",
            "mention_or_use": "mention",
            "agent_name": "Voyager",
            "agent_description": "Embodied agent that accumulates learned skills into a reusable skill library (code snippets) to bootstrap future tasks.",
            "memory_type": "skill library / persistent storage of learned skills (structured artifacts)",
            "memory_description": "Stores past learned skills as code functions (JavaScript) which can be reused by the agent for future tasks.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Cited as an example of persistent skill storage; contrasts with REMEMBERER's experience-Q table rather than executable skill artifacts.",
            "limitations_or_challenges": null,
            "key_insights": "Storing reusable skills is another form of persistent memory enabling agent evolution; REMEMBERER stores action-value experiences rather than code skills.",
            "uuid": "e4913.8"
        },
        {
            "name_short": "GITM",
            "name_full": "GITM (Ghost in the Minecraft)",
            "brief_description": "A system that stores successful trajectories in a simple text experience pool for later retrieval to guide LLM-based agents in open-world environments.",
            "citation_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "mention_or_use": "mention",
            "agent_name": "GITM",
            "agent_description": "LLM-based agent using a text experience pool of successful trajectories for referencing in future decisions.",
            "memory_type": "text experience pool (persistent)",
            "memory_description": "Stores plain-text trajectories of successful episodes for later retrieval; used to inform future decisions.",
            "task_name": "Minecraft / open-world environments (in original work)",
            "task_description": "Open-world embodied tasks (Minecraft) where prior successful trajectories are useful as exemplars.",
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper notes similarity to GITM's use of persistent successful trajectories but highlights REMEMBERER's structured memory including failed experiences and Q-values.",
            "limitations_or_challenges": "Plain-text successful trajectories may not encode value estimates or failed-case avoidance as REMEMBERER does.",
            "key_insights": "Storing successful trajectories helps future decisions, but augmenting memory with Q-values and failed experiences can offer richer guidance.",
            "uuid": "e4913.9"
        },
        {
            "name_short": "Inner Monologue",
            "name_full": "Inner Monologue: Embodied reasoning through planning with language models",
            "brief_description": "A method that uses internal chain-of-thought (inner monologue) for planning with LLMs in embodied tasks; uses immediate feedback rather than long-term persistent memory.",
            "citation_title": "Inner monologue: Embodied reasoning through planning with language models",
            "mention_or_use": "mention",
            "agent_name": "Inner Monologue",
            "agent_description": "Interleaves planning and reasoning in natural language (internal thoughts) to guide action selection in embodied settings; leverages immediate feedback within the trial.",
            "memory_type": "transient internal scratchpad / working chain-of-thought",
            "memory_description": "Uses in-trial natural-language planning traces; not focused on persistent cross-episode memory.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Mentioned as related work focusing on immediate failure feedback or in-trial reasoning; REMEMBERER differs by persisting experiences across episodes.",
            "limitations_or_challenges": null,
            "key_insights": "Immediate feedback and inner monologue help in-trial planning but do not substitute for a persistent, cross-goal experience store.",
            "uuid": "e4913.10"
        },
        {
            "name_short": "Corrective Re-Prompting",
            "name_full": "Planning with large language models via corrective re-prompting",
            "brief_description": "A technique that uses corrective re-prompts from environment feedback to fix mistakes, typically applied once per episode rather than stored long-term.",
            "citation_title": "Planning with large language models via corrective re-prompting",
            "mention_or_use": "mention",
            "agent_name": "Corrective Re-Prompting",
            "agent_description": "Uses corrective re-prompts based on immediate failure signals to change subsequent LLM decisions in that trial.",
            "memory_type": "immediate feedback (non-persistent corrective prompts)",
            "memory_description": "Takes advantage of immediate failure feedback to re-prompt LLMs for corrections, but not stored for long-term cross-episode use.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Paper contrasts one-off corrective re-prompting approaches with REMEMBERER's persistent learning-from-failure across episodes.",
            "limitations_or_challenges": "Corrections used only once; may miss late-failure causes originating earlier in episode.",
            "key_insights": "Single-shot corrective prompts are limited compared to persistent experience storage that enables cumulative learning.",
            "uuid": "e4913.11"
        },
        {
            "name_short": "DEPS",
            "name_full": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
            "brief_description": "An interactive planning system that leverages LLMs to describe, explain, plan and select actions; uses immediate feedback loops rather than long-term experience memory.",
            "citation_title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
            "mention_or_use": "mention",
            "agent_name": "DEPS",
            "agent_description": "Framework applying LLMs to interactive planning (describe/explain/plan/select) to enable open-world multi-task agents; uses immediate interactions rather than persistent RL-updated memory.",
            "memory_type": "immediate / in-trial interaction memory (not persistent cross-episode memory)",
            "memory_description": "Leverages interactive planning steps but does not focus on an external persistent experience memory updated via RL.",
            "task_name": null,
            "task_description": null,
            "benchmark_name": null,
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_performance_with_without_memory": false,
            "memory_comparison_summary": "Cited among systems that use immediate planning and feedback; contrasted with REMEMBERER's persistent experience memory enabling cross-goal reuse.",
            "limitations_or_challenges": null,
            "key_insights": "Interactive planning is complementary to persistent memory; REMEMBERER emphasizes cumulative memory updates to evolve policy without fine-tuning.",
            "uuid": "e4913.12"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ReAct: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "rating": 2
        },
        {
            "paper_title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
            "rating": 2
        },
        {
            "paper_title": "RET-LLM: towards a general read-write memory for large language models",
            "rating": 2
        },
        {
            "paper_title": "Memorybank: Enhancing large language models with long-term memory",
            "rating": 2
        },
        {
            "paper_title": "Voyager: An open-ended embodied agent with large language models",
            "rating": 2
        },
        {
            "paper_title": "Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory",
            "rating": 2
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1
        },
        {
            "paper_title": "Planning with large language models via corrective re-prompting",
            "rating": 1
        },
        {
            "paper_title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents",
            "rating": 1
        }
    ],
    "cost": 0.022625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Large Language Models Are Semi-Parametric Reinforcement Learning Agents</h1>
<p>Danyang Zhang ${ }^{1}$ Lu Chen ${ }^{1,2 \dagger}$ Situo Zhang ${ }^{1}$<br>Hongshen Xu ${ }^{1}$ Zihan Zhao ${ }^{1}$ Kai Yu ${ }^{1,2}$<br>${ }^{1}$ X-LANCE Lab, Department of Computer Science and Engineering<br>MoE Key Lab of Artificial Intelligence, SJTU AI Institute<br>Shanghai Jiao Tong University, Shanghai, China<br>${ }^{2}$ Suzhou Laboratory, Suzhou, China<br>{zhang-dy20,chenlusz,situozhang,xuhongshen,zhao_mengxin,kai.yu}@sjtu.edu.cn</p>
<h4>Abstract</h4>
<p>Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a longterm experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by $4 \%$ and $2 \%$ for the success rate on two task sets and demonstrate the superiority and robustness of REMEMBERER. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Reasoning is remembering. As declared by Seifert et al. [1997], the episodic memory of the experiences from past episodes plays a crucial role in the complex decision-making processes of human [Suddendorf and Corballis, 2007]. By recollecting the experiences from past episodes, the human can learn from success to repeat it and learn from failure to avoid it. Similarly, an agent should optimize its policy for a decision-making task with the help of reminiscence of the interaction experiences. In this work, we primarily investigate how to utilize large language models (LLMs) as agents and equip them with interaction experiences to solve sequential decision-making tasks.
Despite the impressive performance of LLMs on many natural language processing (NLP) tasks [Wei et al., 2022, Kojima et al., 2022, Wang et al., 2022, Yao et al., 2022b], existing approaches still struggle to enable LLMs to effectively learn from interaction experiences. On the one hand, the most common approach for an agent to utilize the experiences is to fine-tune the model parameters through reinforcement learning (RL). However, it requires a considerable expenditure to deploy and fine-tune an LLM, which makes it difficult to apply task-aware RL to the LLM to preserve the experiences. On the other hand, recent work like Algorithm Distillation [Laskin et al., 2022] presents an in-context</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Comparison of the LLM-based agents with short-term working memory and long-term experience memory. The working memory stores only the historical information of the current episode (H). while the experience memory stores the interaction experiences (E) permanently.</p>
<p>Reinforcement learning by embedding the RL training trajectories into the input prompt of a pretrained decision transformer. This method manages to make use of past interaction experiences without model fine-tuning. However, existing LLMs suffer from a serious limitation of the input length to embed the whole experience. Hence, to better store a plethora of interaction histories and aid LLMs in learning during the interaction process, we introduce <strong>RLEM</strong>, <em>i.e.</em>, <strong>Reinforcement Learning</strong> with <strong>Experience Memory</strong>, which accomplishes agent learning by updating the experience memory through the RL process, rather than modifying the model parameters.</p>
<p>An external experience memory is different from the existing work like Reflexion [Shinn et al., 2023] which combines the LLM with a short-term working memory. As depicted in Figure 1 (a), a working memory is tied to a specific task goal, and the stored histories cannot be leveraged in future episodes for different goals. This analogy can be drawn to the Random Access Memory (RAM) of a computer, where stored information is lost in the event of power removal. On the other side, learning from the successful or failed experiences stored in the memory is different from the existing work like Inner Monologue [Huang et al., 2022b], Corrective Re-Prompting [Raman et al., 2022], and DEPS [Wang et al., 2023b] that takes advantage of immediate failure feedback only once. Storing long-term experiences in a persistent memory gives an opportunity to discover the late failure and learn from the experiences in the past episodes even for different task goals (see Figure 1 (b)).</p>
<p>By combining RLEM with LLM, we propose REMEMBERER, an evolvable LLM-based agent framework for decision-making tasks. REMEMBERER can utilize the experiences stored in the memory selectively in accordance with the current interaction state to optimize the decision. Meanwhile, the experience memory can be updated through an RL process constantly. Such a joint system is regarded as a semi-parametric RL agent, which can evolve its ability through its interaction experiences analogically to a full-parametric system, however, without fine-tuning the LLM parameters. We evaluate REMEMBERER on two recent RL task sets with the promising performance of LLM-based agents, WebShop [Yao et al., 2022a] and WikiHow [Zhang et al., 2023]. The agent is trained on a few tasks and tested on some other tasks to check whether the experiences from different tasks can help the agent in the decision of the unseen episodes. REMEMBERER demonstrates a significant performance boost compared to both previous SOTA and our fixed-exemplar LLM baselines. Specifically, it achieves an average improvement of 2 points and 4 points on the Webshop and WikiHow tasks, respectively, compared to the SOTA models.</p>
<p>Our contributions are summarized as follows: 1) A new agent framework is proposed as REMEMBERER for LLM to learn from the experiences of past episodes. The experiences are stored in an external persistent memory instead of fine-tuning the LLM parameters or forming an extremely long prompt. 2) We introduce RLEM, which updates experience memory through analogical RL training so that REMEMBERER is capable of self-evolving. 3) REMEMBERER manages to bypass the baseline</p>
<p>and the prior advanced performances and set up a new state of the art on two recent benchmarks, WebShop (+2 points on SOTA) and WikiHow (+4 points on SOTA).</p>
<h1>2 Related work</h1>
<p>LLM with external information External information is usually adopted to augment the LLM with the environment-grounded information, or to reduce the hallucination, or to unleash the ability to process longer context. Connecting with an external knowledge base is a common choice for question-answering and conversational tasks [Peng et al., 2023, Schick et al., 2023, Trivedi et al., 2022, Pan et al., 2022]. However, an external knowledge base is usually not directly corresponding to an RL environment and cannot provide environment-grounded assistance to an automatic agent. Meanwhile, the update to a mature knowledge base may not be in time for the instant interaction of the agent with the environment. In contrast, Schuurmans [2023] simulates a universal Turing machine with a RAM-augmented LLM and demonstrates the capability of a quickly-updatable working memory. Liang et al. [2023] and Zhong et al. [2023] adopt memory to store the conversational history and handle extremely long contexts. Relational database is leveraged to track states in a dynamic process by ChatDB [Hu et al., 2023]. Reflexion [Shinn et al., 2023] exploits a working memory to store experiences for a dedicated task to improve the performance of the agent through several trials. However, as illustrated in Figure 1, the histories stored in working memory cannot benefit the episode for different task goals. Instead, a long-term cross-goal experience memory should be considered. MemPrompt [Madaan et al., 2022] and Ret-LLM [Modarressi et al., 2023] adopt a persistent memory to store human feedback and remind the chatbot of the conversational knowledge and improve it continuously. Voyager [Wang et al., 2023a] designs a skill library to store the past learned skills as JavaScript functions. A simple text experience pool is adopted by GITM [Zhu et al., 2023] to store the successful trajectories for future referencing. Somewhat similar to GITM, REMEMBERER adopts a persistent environment-grounded experience memory to store the experiences and assist in future decision-making even for different task goal. However, instead of plain text records of successful trajectories, REMEMBERER uses a structured memory and designs a mechanism to task advantage of both successful and failed experiences. The experiences come from the interaction of the agent with the environment, and no human intervention is needed.</p>
<p>LLM learning from failure Learning from failure is one of the characteristic capabilities of human and turns to be an important topic for general artificial intelligence. Some work has explored the ability of the LLM to learn from its failure [Huang et al., 2022b, Raman et al., 2022, Wang et al., 2023b]. Nonetheless, most of such work takes advantage of immediate feedback from the environment and the correction is used only once. In practice, several late failures may be due to some early mistaken actions in an episode. Reflexion [Shinn et al., 2023] designs a heuristic function to detect late failure from the interaction history and stores the LLM-generated reflection in a working memory for use in the next trial. However, these reflections cannot be applied to different task goals. Madaan et al. [2022] stores the failure corrections for a long term, but relies on human feedback. In contrast, REMEMBERER adopts RL to learn from both late and immediate failure from the environment rewards without need for human feedback. Also, REMEMBERER enables the experiences to be reused in the future episode even for a different task goal with a long-term experience memory.</p>
<p>LLM for decision-making The powerful capability of LLM is exploited by recent work [Huang et al., 2022a, Raman et al., 2022, Mees et al., 2022, Chen et al., 2022, Ichter et al., 2022, Huang et al., 2022b, Liang et al., 2022] to generate better control plans for various robots and agents. Kim et al. [2023] and Zhang et al. [2023] design LLM-based agents for user interface (UI) interaction. ReAct [Yao et al., 2022b] combines the action decision with natural language reasoning and achieves a promising performance. To our best knowledge, This work is the first one that combines the LLMbased agent with RL algorithm to learn from the interaction experiences and achieve self-evolving.
The proposed REMEMBERER equips the LLM with an external experience memory to help it to learn from both successful and failed experiences. This is also the first work to combine the LLM-based agent with RL algorithm to improve the capability of the agent.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Pipeline of RLEM and architecture of REMEMBERER</p>
<h1>3 Method</h1>
<h3>3.1 RLEM pipeline</h3>
<p>RLEM (Reinforcement Learning with Experience Memory) is proposed for an LLM-based agent to learn from its interaction experiences by updating an external persistent memory. The pipeline of RLEM and the architecture of REMEMBERER agent are depicted in Figure 2. REMEMBERER agent consists of two components: an LLM making decisions and an experience memory storing the interaction experiences. At the decision step, the LLM first takes an observation $o_{t}$ from the environment. The observation $o_{t}$ is then adopted to retrieve several related experiences from the connected experience memory according to some similarity functions. The experiences are represented as a group of observations $O_{x}$, actions $A_{x}$, and the corresponding $Q$ value estimations $Q_{x}$. Here $x$ denotes the index set of retrieved experiences and depends on the specific runtime observation $o_{t}$. Subsequently, LLM will decide the action $a_{t}$ in accordance with $o_{t}$, the feedback from the last interaction (e.g., the reward $r_{t-1}$ ), as well as the retrieved experiences $\left(O_{x}, A_{x}, Q_{x}\right) . a_{t}$ will be executed in the environment and the resulted reward $r_{t}$ will be returned to the LLM as the feedback. And the transition tuple, $\left(o_{t}, a_{t}, r_{t}, o_{t+1}\right)$, comprising the last observation, the taken action, the corresponding reward, and the new observation will be used to update the experience memory. The following subsections will detail the structure and updating policy of REMEMBERER experience memory and the usage of the retrieved experiences.</p>
<h3>3.2 Experience memory of REMEMBERER</h3>
<p>The experience memory is one of the pivotal components of the proposed REMEMBERER framework. It is adopted to store the interaction experiences, and the LLM is expected to benefit from the stored experiences in future decision-making. The memory can be regarded as a group of external parameters of the LLMbased agent. Such an agent is a semi-parametric system that can evolve through RL process. During the interaction, new experiences are added to the experience memory so that the overall system can attain a more capable interaction ability</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task \&amp; Obsv.</th>
<th style="text-align: center;">Action</th>
<th style="text-align: center;">Q Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\left(g_{1}, o_{1}\right)$</td>
<td style="text-align: center;">$a_{1}$</td>
<td style="text-align: center;">$q_{1}$</td>
</tr>
<tr>
<td style="text-align: center;">$\left(g_{2}, o_{2}\right)$</td>
<td style="text-align: center;">$a_{2}$</td>
<td style="text-align: center;">$q_{2}$</td>
</tr>
<tr>
<td style="text-align: center;">$\left(g_{3}, o_{3}\right)$</td>
<td style="text-align: center;">$a_{3}$</td>
<td style="text-align: center;">$q_{3}$</td>
</tr>
<tr>
<td style="text-align: center;">$\vdots$</td>
<td style="text-align: center;">$\vdots$</td>
<td style="text-align: center;">$\vdots$</td>
</tr>
</tbody>
</table>
<p>Figure 3: An example of the records stored in the proposed experience memory.
compared to the agents with just a fixed LLM and fixed exemplars. This procedure can be considered analogous to the training stage of conventional parametric agents.</p>
<p>To be specific, the experience memory is designed as a table storing the task information, observation, action, and the corresponding $Q$ value estimation. The $Q$ value is the expectation of the accumulated future reward and gives an assessment of the value of the action candidate. Figure 3 depicts a demonstration of the proposed experience memory. There are two stages to build a practical REMEMBERER agent with experience memory: initialization and training. The experience memory is supposed to be first initialized with some initial records before the training stage. The initial</p>
<p>records are necessary to inform the LLM of the format of the input and the output. Then, during the analogical training stage, the agent interacts with the environment to collect new experiences, and conducts off-policy learning [Sutton and Barto, 1999]. Particularly, given the task information $g$ and the new transition $\left(o_{t}, a_{t}, r_{t}, o_{t+1}\right)$, as a quadruple of the last observation, action, reward, and the new observation, a new estimation is calculated first according to the estimated Bellman optimality equation [Bellman, 1952] as</p>
<p>$$
Q^{\prime}\left(g, o_{t}, a_{t}\right)=r_{t}+\gamma \max <em t_1="t+1">{a} Q\left(g, o</em>, a\right)
$$</p>
<p>Here $\max$ can be calculated from the actions already recorded for $\left(g, o_{t+1}\right)$ by considering the $Q$ value of unrecorded actions 0 , if the action space cannot be traversed, e.g., action space involving free-form language. Then a new record is inserted directly if there does not exist a record associated to $\left(g, o_{t}, a_{t}\right)$ in the memory:</p>
<p>$$
Q\left(g, o_{t}, a_{t}\right)=Q^{\prime}\left(g, o_{t}, a_{t}\right)
$$</p>
<p>If $\left(g, o_{t}, a_{t}\right)$ has been already inserted into the record, the recorded $Q$ value estimation will be updated by Q-Learning [Watkins and Dayan, 1992]:</p>
<p>$$
Q\left(g, o_{t}, a_{t}\right) \leftarrow(1-\alpha) Q\left(g, o_{t}, a_{t}\right)+\alpha Q^{\prime}\left(g, o_{t}, a_{t}\right)
$$</p>
<p>Here the learning rate, $\alpha$, is $1 / N$ where $N$ denotes the times this value is updated. As Equation 1 may lead to an inaccurate estimation owing to insufficient sampling out of few training steps of REMEMBERER, $n$-step bootstrapping [Mnih et al., 2016] is adopted to ameliorate this problem, which estimates $Q^{\prime}$ by</p>
<p>$$
Q^{\prime}\left(g, o_{t}, a_{t}\right)=\sum_{i=0}^{n-1} \gamma^{i} r_{t+i}+\gamma^{n} \max <em t_n="t+n">{a} Q\left(g, o</em>, a\right)
$$</p>
<p>where $n$ is the steps to expand. The ablation study in Subsection 4.4 proves this perspective.</p>
<h1>3.3 Usage of the experiences</h1>
<p>In order to assist the LLM in making decisions, the stored experiences are adopted as dynamic exemplars for few-shot in-context learning. Given the task goal $g$ and the current observation $o_{t}$, a similarity function $f$ is used to calculate the similarity of $\left(g, o_{t}\right)$ with $\left(g_{i}, o_{i}\right)$ from the memory.</p>
<p>$$
S_{i}=f\left(\left(g, o_{t}\right),\left(g_{i}, o_{i}\right)\right)
$$</p>
<p>Commonly, a similarity function $f$ can be divided into two components, task similarity $f_{g}$ and observation similarity $f_{o}$ :</p>
<p>$$
S_{i}=\lambda f_{g}\left(g, g_{i}\right)+(1-\lambda) f_{o}\left(o_{t}, o_{i}\right)
$$</p>
<p>The $m$ records with the highest similarities are retrieved to form the exemplars in the prompt. The particular similarity function designed for each task set is detailed in Subsection 4.1.
The exemplar is supposed to demonstrate the format of the input and the output to the LLM. The input part usually comprises the task information and the observation, along with some interaction feedback or auxiliary information. The particular input format depends on the task domain and will be detailed in Subsection 4.1. The output part indicates the action decision. Specifically, we propose to present the action</p>
<div class="codehilite"><pre><span></span><code><span class="k">Last</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="nl">Actions</span><span class="p">:</span>
<span class="k">search</span><span class="o">[</span><span class="n">3 ounce bottle bright citrus deodorant</span>
<span class="n">    sensitive skin</span><span class="o">]</span>
<span class="nl">Observation</span><span class="p">:</span>
<span class="nl">Instruction</span><span class="p">:</span>
<span class="n">i</span><span class="w"> </span><span class="n">would</span><span class="w"> </span><span class="ow">like</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="n">ounce</span><span class="w"> </span><span class="n">bottle</span><span class="w"> </span><span class="p">...</span>
<span class="o">[</span><span class="n">Back to Search</span><span class="o">]</span>
<span class="p">...</span>
<span class="o">[</span><span class="n">B078QWRC1J</span><span class="o">]</span>
<span class="p">...</span>
<span class="o">[</span><span class="n">B078GTKVXY</span><span class="o">]</span>
<span class="p">...</span>
<span class="o">[</span><span class="n">B08KBVJ4XN</span><span class="o">]</span>
<span class="p">...</span>
<span class="n">Available</span><span class="w"> </span><span class="nl">Actions</span><span class="p">:</span>
<span class="n">back</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="k">search</span>
</code></pre></div>

<p>$\square$
Encouraged:
click[b078gwrc1j] -&gt; 1.0 b078gwrc1j and b078gtkvxy are bright citrus deodorant less than 50 dollars. I can check b078gwrc1j first.
Discouraged:
click[b087wksr2g] -&gt; 0.0 b087wksr2g is not the desired item.</p>
<p>Figure 4: An exemplar for WebShop task set [Yao et al., 2022a]. The input part is depicted in the upper box and the output part is depicted in the lower box. Action candidates are advised along with their $Q$ value estimations and some optional extra information.</p>
<p>remembering" to exploit both successful and failed experiences. To form the output part in the exemplar, the actions with the highest $Q$ value estimations from the retrieved record are given as the encouraged actions, while the actions with poor $Q$ value estimations (e.g., zero or negative estimations) are given as the discouraged actions. It is believed that the advice with high value expectations can lead the LLM to follow the past success, while the advice with poor expectations will teach the LLM to avoid a similar failure. A clear depiction of the exemplar format can be found in Figure 4 Prompted by such exemplars, the LLM will also predict both encouraged and discouraged actions and speculate their $Q$ values given a new input. The predicted $Q$ values are used to select the optimal action, to be specific, the encouraged action with the highest $Q$ value speculation will be executed in the environment.</p>
<p>It is worth noting that REMEMBERER agent necessitates only a limited number of training steps to achieve a promising performance, which leads to a non-exhaustive action record within its memory. Consequently, instances may arise where there is only one action associated with a given context $\left(g, o_{t}\right)$, or the highest $Q$ value remains deficient, or no sufficiently unfavorable action exists to discourage. In such cases, randomly sampled action advice is favored over encouraging an action with low expectations or discouraging an action with moderate expectations. Our ablation study in Subsection 4.4 sheds light on various strategies for generating advice in such scenarios.</p>
<h1>4 Experiments \&amp; results</h1>
<h3>4.1 Experiment setup \&amp; implementation details</h3>
<p>To assess the effectiveness of REMEMBERER, we evaluate it on two recent task sets with the promising performance of LLM-based agents: WebShop and WikiHow. All the experiments are conducted based on the OpenAI API of GPT-3.5 [Brown et al., 2020] text-davinci-003.</p>
<p>WebShop [Yao et al., 2022a] WebShop is a task set simulating a web store site. The agent is instructed to browse the site and shop for the target goods. The information of over 1 M products is crawled from the Amazon store ${ }^{3}$. About 12 K product requests are re-written by crowd laborers to generate more diverse instructions. A score between 0 and 1 will be rated after shopping by assessing the correspondence between the product and the instruction. We followed Shinn et al. [2023] and conducted our experiments on the first 100 tasks from the same shuffled task list released along with the task set. At each interaction step, the LLM takes the web page representation and a list of available actions as input. The task instruction is omitted, for there is always an instruction present on the top of the web page. As there are no intermediate rewards during the episode, only the last 5 performed actions serve as procedure feedback. Inspired by the chain-of-thought technique [Wei et al., 2022] and the ReAct mechanism [Yao et al., 2022b], the LLM is prompted to predict a reason for its decision as the extra information depicted in Figure 4. The representation of the web pages is simplified in the same way as ReAct. The task similarity $f_{g}$ is calculated using the all-MiniLM-L12-v2 model from Sentence-Transformers [Reimers and Gurevych, 2019]. As it is noticed that the web pages in WebShop are instantiated from some templates, we categorize the web pages into four patterns and design a similarity lookup table to compute the observation similarity $f_{o}$ according to the web page patterns. The details about the similarity table should be referred to in the supplementary. It is observed that most of the tasks end in 5 steps, thus we directly conduct a full-trajectory expanding while performing multi-step bootstrapping:</p>
<p>$$
Q^{\prime}\left(o_{t}, a_{t}\right)=\sum_{\tau=t}^{T} \gamma^{\tau-t} r_{\tau}
$$</p>
<p>WikiHow [Zhang et al., 2023] WikiHow is a task set based on the collaborative wiki app WikiHow ${ }^{4}$ running on the interaction platform Mobile-Env [Zhang et al., 2023]. The task set contains amounts of navigation tasks. The target of the agent is to follow the instructions and navigate to the required page. Intermediate rewards and instructions may be triggered during the episode. We followed Zhang et al. [2023] and evaluated the proposed REMEMBERER on the "canonical subset" comprising 70</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Results on WebShop. The result of the prior state of the art, ReAct [Yao et al., 2022b], is attained with the public implementation released by the original authors. The RL, IL, and IL+RL results are retrieved directly from Yao et al. [2022a].</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Avg Score</th>
<th>Success Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM only</td>
<td>0.55</td>
<td>0.29</td>
</tr>
<tr>
<td>ReAct</td>
<td>0.66</td>
<td>0.36</td>
</tr>
<tr>
<td>RMMBR.</td>
<td>$\mathbf{0 . 6 8}$</td>
<td>$\mathbf{0 . 3 9}$</td>
</tr>
<tr>
<td>RL</td>
<td>0.55</td>
<td>0.18</td>
</tr>
<tr>
<td>IL</td>
<td>0.60</td>
<td>0.29</td>
</tr>
<tr>
<td>IL+RL</td>
<td>0.62</td>
<td>0.29</td>
</tr>
</tbody>
</table>
<p>Table 2: Results on WikiHow. “Mobile-Env” indicates the prior result from Zhang et al. [2023]. “RMMBR. (A)” denotes the results by directly running the evaluation of REMEMBERER with a human-annotated experience memory.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Avg Reward</th>
<th>Success Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>LLM only</td>
<td>2.58</td>
<td>0.90</td>
</tr>
<tr>
<td>Mobile-Env</td>
<td>2.50</td>
<td>0.89</td>
</tr>
<tr>
<td>RMMBR.</td>
<td>$\mathbf{2 . 6 3}$</td>
<td>$\mathbf{0 . 9 3}$</td>
</tr>
<tr>
<td>RMMBR. (A)</td>
<td>2.56</td>
<td>0.91</td>
</tr>
</tbody>
</table>
<p>Table 3: Results on WebShop with different exemplar combinations (initial experiences for REMEMBERER) and different training sets (for REMEMBERER). $E_{i}$ denotes the different exemplar combinations, while $S_{i}$ denotes the different training sets. The first line of each method shows the mean scores, and the second line shows the success rates.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Different (Initial) Exemplars</th>
<th></th>
<th>Different Training Sets</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$E_{0}+S_{0}$</td>
<td>$E_{1}+S_{0}$</td>
<td>$E_{2}+S_{0}$</td>
<td>$E_{0}+S_{1}$</td>
<td>Avg</td>
<td>Std</td>
</tr>
<tr>
<td>ReAct</td>
<td>$\mathbf{0 . 7 2}$</td>
<td>0.65</td>
<td>0.60</td>
<td>-</td>
<td>0.66</td>
<td>0.06</td>
</tr>
<tr>
<td></td>
<td>$\mathbf{0 . 4 2}$</td>
<td>0.35</td>
<td>0.30</td>
<td>-</td>
<td>0.36</td>
<td>0.06</td>
</tr>
<tr>
<td>LLM only</td>
<td>0.52</td>
<td>0.54</td>
<td>0.59</td>
<td>-</td>
<td>0.55</td>
<td>0.04</td>
</tr>
<tr>
<td></td>
<td>0.26</td>
<td>0.28</td>
<td>0.32</td>
<td>-</td>
<td>0.29</td>
<td>0.03</td>
</tr>
<tr>
<td>RMMBR.</td>
<td>0.66</td>
<td>$\mathbf{0 . 7 1}$</td>
<td>$\mathbf{0 . 6 6}$</td>
<td>$\mathbf{0 . 6 7}$</td>
<td>$\mathbf{0 . 6 8}$</td>
<td>$\mathbf{0 . 0 2}$</td>
</tr>
<tr>
<td></td>
<td>0.37</td>
<td>$\mathbf{0 . 4 1}$</td>
<td>$\mathbf{0 . 3 7}$</td>
<td>$\mathbf{0 . 4 0}$</td>
<td>$\mathbf{0 . 3 9}$</td>
<td>$\mathbf{0 . 0 2}$</td>
</tr>
</tbody>
</table>
<p>tasks. Specifically, the LLM is input with the task description, the screen representation, and the step instruction. The screen is represented in an HTML element sequence following Zhang et al. [2023]. Additionally, the last 5 performed actions along with the last reward are given to the LLM as the procedure feedback. As for the output, the LLM is prompted to print the HTML representation of the operated element as the extra information. This is expected to force the LLM to discover the relation between the element id and the certain element. The task similarity $f_{g}$ designed for WikiHow is computed from the step instructions. It is noticed that the instructions follow some patterns, thus, we inspect the instructions and categorize them into six types. Then a similarity lookup table is designed according to the instruction types. The details should be referred to in the supplementary. The observation similarity $f_{o}$ is computed based on the length of the longest common sequence of the HTML elements in the screen representation:</p>
<p>$$
f_{o}\left(s c_{1}, s c_{2}\right)=\frac{l c s\left(s c_{1}, s c_{2}\right)}{\max \left{\operatorname{len}\left(s c_{1}\right), \operatorname{len}\left(s c_{2}\right)\right}}
$$</p>
<p>The full-trajectory expanding is adopted, as most of the tasks will end in 5 steps as well.</p>
<h1>4.2 Results on WebShop</h1>
<p>REMEMBERER is applied to WebShop with 2-shot in-context learning. The experience memory is initialized with four annotated experiences of the decision step from one trajectory. The agent is trained for 3 epochs on a training set containing 10 different tasks outside the test sets used by Yao et al. [2022b] and Shinn et al. [2023]. To control the total expense and achieve bootstrapping, the succeeded tasks in the first epoch are excluded from training in the following two epochs. The trajectories exceeding 15 steps are considered to be failed, as most of the tasks can end in 5 steps. The main results are shown in Table 1. We used the public ReAct [Yao et al., 2022b] implementation released by the authors and run with text-davinci-003 instead of text-davince-002 in Yao et al. [2022b]. The run of ReAct shares the same trajectory as the exemplar with REMEMBERER. The "LLM only"</p>
<p>Table 4: Comparison of the number of annotated trajectories and steps of REMEMBERER and the IL baseline. The number of steps of the training set of IL is estimated according to the average human trajectory length on the test split as 11.3 in <em>Yao et al. (2022a)</em>.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>#Trajectories</th>
<th>#Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>IL</td>
<td>1,012</td>
<td>$\sim$11,436</td>
</tr>
<tr>
<td>REMEMBERER</td>
<td>1</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Table 5: Comparison of the number of the tasks in the training set and the updating steps of REMEMBERER with the IL and RL baselines. The number of the updating steps of IL is estimated from 10 epochs on 1,012 trajectories with an average trajectory length of 11.3.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>#Tasks</th>
<th>#Steps</th>
</tr>
</thead>
<tbody>
<tr>
<td>RL</td>
<td>10,587</td>
<td>100,000</td>
</tr>
<tr>
<td>IL</td>
<td>-</td>
<td>$\sim$114,356</td>
</tr>
<tr>
<td>REMEMBERER</td>
<td>10</td>
<td>74</td>
</tr>
</tbody>
</table>
<p>Table 6: Results on WikiHow with different exemplar combinations (initial experiences for REMEMBERER) and different training sets (for REMEMBERER).</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th>Different (Initial) Exemplars</th>
<th></th>
<th>Different Training Sets</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>$E_{0}+S_{0}$</td>
<td>$E_{1}+S_{0}$</td>
<td>$E_{2}+S_{0}$</td>
<td>$E_{0}+S_{1}$</td>
<td>Avg</td>
<td>Std</td>
</tr>
<tr>
<td>LLM only</td>
<td>2.56</td>
<td>2.60</td>
<td>2.59</td>
<td>-</td>
<td>2.58</td>
<td>0.02</td>
</tr>
<tr>
<td></td>
<td>0.90</td>
<td>0.90</td>
<td>0.89</td>
<td>-</td>
<td>0.90</td>
<td>0.01</td>
</tr>
<tr>
<td>RMMBR.</td>
<td>2.63</td>
<td>2.63</td>
<td>2.59</td>
<td>2.66</td>
<td>2.63</td>
<td>0.03</td>
</tr>
<tr>
<td></td>
<td>0.93</td>
<td>0.91</td>
<td>0.90</td>
<td>0.97</td>
<td>0.93</td>
<td>0.03</td>
</tr>
</tbody>
</table>
<p>baseline indicates a single LLM with 2 fixed exemplars sampled from the initial experiences of REMEMBERER. The average performance of REMEMBERER exceeds the baseline by a large extent and surpasses the prior state of the art, ReAct, as well. This proves the effectiveness of augmenting the LLM with an external evolvable experience memory. The proposed REMEMBERER also outperforms the RL, IL (imitation learning), and IL+RL baselines on both metrics.</p>
<p>In order to verify the robustness of REMEMBERER, experiments with different initial experience combinations or a different training set are conducted. The results are depicted in Table 3. The initial experience combination $E_{0}$ denotes the certain trajectory adopted by the original implementation of ReAct while $E_{1}$ and $E_{2}$ are randomly sampled from $S_{0}$. It is observed that the proposed REMEMBERER can achieve better and more stable results with different initialization and training sets compared to ReAct. Thus, REMEMBERER can mitigate the workload to some extent to search for an optimal exemplar combination.</p>
<p>We compare the training efficiency of REMEMBERER with the conventional IL and RL methods in Table 4 and Table 5. In contrast to the IL, REMEMBERER requires quite few annotated samples to initialize the experience memory, while IL is in need of much more human annotations. REMEMBERER agent can be trained on only 10 tasks for 74 steps, while the RL and IL are expected to be trained for about 100 thousand steps to achieve an acceptable performance. Consequently, the proposed REMEMBERER offers a much more efficient way to build a practical agent agilely.</p>
<h3>4.3 Results on WikiHow</h3>
<p>REMEMBERER is applied to WikiHow with 2-shot in-context learning. The experience memory is initialized with two annotated experiences of the decision step. The agent is trained for 3 epochs on a training set containing 10 different tasks selected from WikiHow excluding the test tasks. Similar to the experiments on WebShop, the succeeded tasks in the first epoch are excluded from training in the following two epochs. As observed that most of the tasks require an interaction of less than 5 steps, the trajectory exceeding 15 steps will be regarded as failed. The main results are depicted in Table 2. The exemplars of the “LLM only” baseline are the initial experiences of REMEMBERER. The proposed REMEMBERER surpasses the baseline as well as the original result in <em>Zhang et al. (2023)</em>. In addition, 10 tasks are annotated to form an annotated experience memory. REMEMBERER agent with this annotated experience memory is evaluated without further training, and the result is denoted as “RMMBR. (A)” in the table. This result demonstrates that REMEMBERER is capable of</p>
<p>Table 7: Comparison of the average reward estimation of the full model and the ablation model without bootstrapping policy. The error is the absolute difference between the average reward estimation from the experience memory and the real training reward.</p>
<table>
<thead>
<tr>
<th>Task Set</th>
<th>Setting</th>
<th>Avg Reward Estimation</th>
<th>Avg Training Reward</th>
<th>Abs Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebShop</td>
<td>Full Model</td>
<td>0.86</td>
<td>0.84</td>
<td>$\mathbf{0 . 0 2}$</td>
</tr>
<tr>
<td></td>
<td>w/o bootstrp.</td>
<td>0.62</td>
<td>0.84</td>
<td>0.22</td>
</tr>
<tr>
<td>WikiHow</td>
<td>Full Model</td>
<td>2.48</td>
<td>2.60</td>
<td>$\mathbf{0 . 1 2}$</td>
</tr>
<tr>
<td></td>
<td>w/o bootstrp.</td>
<td>1.98</td>
<td>2.70</td>
<td>0.72</td>
</tr>
</tbody>
</table>
<p>Table 8: Results of ablation study</p>
<table>
<thead>
<tr>
<th>Task Set</th>
<th>Setting</th>
<th>Avg Reward/Score</th>
<th>Success Rate</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebShop</td>
<td>Full Model</td>
<td>$\underline{0.66}$</td>
<td>$\underline{0.37}$</td>
</tr>
<tr>
<td></td>
<td>w/o bootstrp.</td>
<td>$\overline{0.67}$</td>
<td>$\overline{0.36}$</td>
</tr>
<tr>
<td></td>
<td>w/o random</td>
<td>0.65</td>
<td>0.37</td>
</tr>
<tr>
<td>WikiHow</td>
<td>Full Model</td>
<td>$\underline{2.63}$</td>
<td>$\underline{0.93}$</td>
</tr>
<tr>
<td></td>
<td>w/o bootstrp.</td>
<td>$\overline{2.54}$</td>
<td>$\overline{0.89}$</td>
</tr>
<tr>
<td></td>
<td>w/o random</td>
<td>2.64</td>
<td>0.90</td>
</tr>
<tr>
<td></td>
<td>w/o discouraged</td>
<td>2.48</td>
<td>0.81</td>
</tr>
<tr>
<td></td>
<td>w/o task sim. $f_{g}$</td>
<td>2.63</td>
<td>0.94</td>
</tr>
<tr>
<td></td>
<td>w/o obsrv. sim. $f_{o}$</td>
<td>2.47</td>
<td>0.87</td>
</tr>
</tbody>
</table>
<p>exploiting expert experiences, which can be regarded as analogous to conventional imitation learning. Nevertheless, the annotated experiences may not offset the exact shortage of the particular LLM. In contrast, the RL training will have an opportunity to collect more specific experiences and gain a more promising performance.</p>
<p>The experiments with different initial experience combinations or a different training set are conducted on WikiHow as well, and the results are shown in Table 6. The proposed REMEMBERER achieves a consistent improvement compared to the baseline with fixed exemplars, which proves the effectiveness and robustness of REMEMBERER.</p>
<h1>4.4 Ablation study</h1>
<p>Several ablation studies are conducted to verify the design of REMEMBERER framework.</p>
<p>Ablation on $n$-step bootstrapping policy Ablation studies are conducted to verify the necessity of $n$-step bootstrapping policy to update the $Q$ value estimations in the experience memory. As stated in Subsection 3.2, updating without bootstrapping may lead to inaccurate value estimations owing to few training steps to explore and exploit. In order to verify this perspective, an average reward estimation is calculated by averaging the sum of the maximum $Q$ value and the history reward stored for each observation in the experience memory:</p>
<p>$$
\hat{R}=\frac{1}{M} \sum_{i=1}^{M}\left(R_{h}\left(g_{i}, o_{i}\right)+\max <em i="i">{a} Q\left(g</em>, a\right)\right)
$$}, o_{i</p>
<p>where $R_{h}$ denotes the total reward of the steps before $\left(g_{i}, o_{i}\right)$ on the trajectory and $M$ is the size of the memory. The deduced average reward estimation $\hat{R}$ is compared to the real training reward, and an absolute error is calculated in Table 7. It can be observed that the reward estimation from the experience memory trained without bootstrapping suffers a far greater error than that with bootstrapping. Meanwhile, the performance on the test set is demonstrated in Table 8. Although there is no apparent disparity in the final performance on the WebShop task set, a visible degradation is observed on WikiHow, which reveals the latent risk of a non-bootstrapping update.</p>
<p>Ablation on the advice generation strategy As stated in Subsection 3.3, owing to the nonexhaustive exploration in the brief training stage, there may be no suitable candidates for the action advice in the exemplars. For instance, there may be no actions recorded with a poor enough $Q$ value estimation or no actions recorded as high-reward. Under this case, action advice can be generated with a randomly sampled action that is not in the record, or it can be given by directly encouraging the action with the highest $Q$ value estimation and discouraging the action with the lowest estimation without regard to the certain value. These two strategies are compared in Table 8. As the results illustrate, the random plan appears to have a minor superiority over the non-random plan. This is attributed to that advice with improper value expectations will mislead the LLM to take wrong judgments about the true value of the available actions.</p>
<p>Additional experiments are conducted to investigate the necessity of the discourage actions in the output part of exemplars and the impact of similarity function components. Owing to limit of budgets, these experiments are only conducted on WikiHow task set.</p>
<p>Ablation on necessity of the discouraged actions The proposed output format "action advice" comprises both encouraged and discouraged actions. The discouraged actions are believed to help the LLM to avoid similar failures. Results in Table 8 prove necessity of the discouraged actions. Without access to the discouraged actions, the agent can only achieve a much poorer performance than the full model. In the case shown in the supplementary, it can be seen that there may not be proper actions to encourage in the retrieved experience. In such cases, the discouraged actions are especially crucial for the agent to prevent repeating similar mistakes.</p>
<p>Ablation on the similarity function As stated in Subsection 3.3, a similarity function is required to select related experiences from the memory. In experiments, the similarity is implemented as two components: task similarity $f_{g}$ and observation similarity $f_{o}$. Ablation studies are conducted to draw a brief perspective on the impact of these two components. As shown in Table 8, removal of task similarity seems not to affect the performance remarkably, while removal of observation similarity causes a serious degradation. This may indicate that on these tasks, the tested LLM benefits more from experiences that have similar observations rather than similar instruction patterns. On the other side, the pattern-based task similarity for WikiHow introduced in Subsection 4.1 may be too coarse to cluster the experiences. During interaction, the agent may receive instructions of the same pattern (e.g., "access article ABC") while facing different types of observation (e.g., search result page or category page). The appropriate actions in two situations are also different. Removal of observation similarity will eliminate this difference in experience selection and results in misleading. Case study in the supplementary shows this perspective.</p>
<h1>5 Conclusion</h1>
<p>We introduce Reinforcement Learning with Experience Memory (RLEM) to aid the LLM in learning from its interaction experiences for decision-making tasks. A novel LLM-based agent framework called REMEMBERER is then designed with RLEM by equipping the LLM with a persistent experience memory and updating the memory with the RL algorithm. REMEMBERER agent is capable of exploiting the interaction experiences to improve its policy and gains a significant improvement compared to the baseline. Our experimental results demonstrate the superiority. Owing to the simplicity and effectiveness of REMEMBERER, we believe that this work provides a valuable perspective on designing evolvable LLM-based agents with RLEM.</p>
<h2>6 Limitations</h2>
<p>The proposed REMEMBERER agent demonstrates strong superiority on the tested benchmarks. Nevertheless, it is wondered how this framework will be applied to the environments with more long-term episodes or with more extensive or visual-rich observations. Besides, it is observed that the performance of REMEMBERER will encounter quick saturation in training process. This may be due to the limited number of active exemplars. Further efforts are expected to be dedicated in to make the agent performance evolve continuously. Furthermore, as an early exploration, we didn't make use of complicated RL techniques. How recent advancement in RL domain works under RLEM is also an interesting problem.</p>
<h1>Acknowledgements</h1>
<p>This work is funded by the China NSFC Project (No. 62106142 and No. 62120106006 ) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102).</p>
<h2>References</h2>
<p>Richard Bellman. On the theory of dynamic programming. Proceedings of the national Academy of Sciences, 38(8):716-719, 1952.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.</p>
<p>Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin Stone, and Daniel Kappler. Open-vocabulary queryable scene representations for real world planning. CoRR, abs/2209.09874, 2022. doi: 10.48550/arXiv.2209.09874. URL https: //doi.org/10.48550/arXiv.2209.09874.</p>
<p>Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. CoRR, abs/2306.03901, 2023. doi: 10.48550/arXiv. 2306.03901. URL https://doi.org/10.48550/arXiv.2306.03901.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zeroshot planners: Extracting actionable knowledge for embodied agents. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 9118-9147. PMLR, 2022a. URL https://proceedings.mlr.press/v162/huang22a.html.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Tomas Jackson, Noah Brown, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. Inner monologue: Embodied reasoning through planning with language models. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 1769-1782. PMLR, 2022b. URL https://proceedings.mlr.press/v205/huang23c.html.</p>
<p>Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as I can, not as I say: Grounding language in robotic affordances. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski, editors, Conference on Robot Learning, CoRL 2022, 14-18 December 2022, Auckland, New Zealand, volume 205 of Proceedings of Machine Learning Research, pages 287-318. PMLR, 2022. URL https://proceedings.mlr.press/ v205/ichter23a.html.</p>
<p>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. CoRR, abs/2303.17491, 2023. doi: 10.48550/arXiv.2303.17491. URL https://doi.org/10. 48550/arXiv.2303.17491.</p>
<p>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In NeurIPS, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/ 8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html.</p>
<p>Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan A. Brooks, Maxime Gazeau, Himanshu Sahni, Satinder Singh, and Volodymyr Mnih. In-context reinforcement learning with algorithm distillation. CoRR, abs/2210.14215, 2022. doi: 10.48550/arXiv.2210.14215. URL https://doi.org/10. 48550/arXiv. 2210.14215.</p>
<p>Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. CoRR, abs/2209.07753, 2022. doi: 10.48550/arXiv.2209.07753. URL https://doi.org/10.48550/ arXiv. 2209.07753.</p>
<p>Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. CoRR, abs/2304.13343, 2023. doi: 10.48550/arXiv.2304.13343. URL https: //doi.org/10.48550/arXiv. 2304.13343.</p>
<p>Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after deployment. arXiv preprint arXiv:2201.06009, 2022.</p>
<p>Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with visual affordances over unstructured data. CoRR, abs/2210.01911, 2022. doi: 10.48550/arXiv.2210.01911. URL https://doi.org/10.48550/arXiv. 2210.01911.</p>
<p>Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria-Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 1928-1937. JMLR.org, 2016. URL http://proceedings.mlr.press/v48/mniha16.html.</p>
<p>Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. RET-LLM: towards a general read-write memory for large language models. CoRR, abs/2305.14322, 2023. doi: 10.48550/arXiv. 2305.14322. URL https://doi.org/10.48550/arXiv. 2305.14322.</p>
<p>Xiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu, Dong Yu, and Jianshu Chen. Knowledge-incontext: Towards knowledgeable semi-parametric language models. CoRR, abs/2210.16433, 2022. doi: 10.48550/arXiv.2210.16433. URL https://doi.org/10.48550/arXiv.2210.16433.</p>
<p>Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao. Check your facts and try again: Improving large language models with external knowledge and automated feedback. CoRR, abs/2302.12813, 2023. doi: 10.48550/arXiv.2302.12813. URL https://doi.org/10.48550/arXiv.2302.12813.</p>
<p>Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie Tellex. Planning with large language models via corrective re-prompting. CoRR, abs/2211.09935, 2022. doi: 10.48550/arXiv.2211.09935. URL https://doi.org/10.48550/arXiv.2211.09935.</p>
<p>Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 3980-3990. Association for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1410. URL https://doi.org/10.18653/v1/D19-1410.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. CoRR, abs/2302.04761, 2023. doi: 10.48550/arXiv.2302.04761. URL https: //doi.org/10.48550/arXiv.2302.04761.</p>
<p>Dale Schuurmans. Memory augmented large language models are computationally universal. CoRR, abs/2301.04589, 2023. doi: 10.48550/arXiv.2301.04589. URL https://doi.org/10.48550/ arXiv. 2301.04589.</p>
<p>Colleen M Seifert, Andrea L Patalano, Kristian J Hammond, and Timothy M Converse. Experience and expertise: The role of memory in planning for opportunities. 1997.</p>
<p>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. CoRR, abs/2303.11366, 2023. doi: 10.48550/arXiv.2303.11366. URL https://doi.org/10.48550/arXiv.2303.11366.</p>
<p>Thomas Suddendorf and Michael C Corballis. The evolution of foresight: What is mental time travel, and is it unique to humans? Behavioral and brain sciences, 30(3):299-313, 2007.</p>
<p>Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. Robotica, 17(2): 229-235, 1999.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. CoRR, abs/2212.10509, 2022. doi: 10.48550/arXiv.2212.10509. URL https://doi.org/10.48550/ arXiv. 2212.10509.</p>
<p>Hado van Hasselt. Double q-learning. In John D. Lafferty, Christopher K. I. Williams, John ShaweTaylor, Richard S. Zemel, and Aron Culotta, editors, Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada, pages 2613-2621. Curran Associates, Inc., 2010. URL https://proceedings.neurips.cc/paper/2010/hash/ 091d584fced301b442654dd8c23b3fc9-Abstract.html.</p>
<p>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. CoRR, abs/2305.16291, 2023a. doi: 10.48550/arXiv.2305.16291. URL https://doi.org/10.48550/ arXiv. 2305.16291.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, and Denny Zhou. Selfconsistency improves chain of thought reasoning in language models. CoRR, abs/2203.11171, 2022. doi: 10.48550/arXiv.2203.11171. URL https://doi.org/10.48550/arXiv.2203.11171.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. CoRR, abs/2302.01560, 2023b. doi: 10.48550/arXiv.2302.01560. URL https://doi.org/10.48550/ arXiv. 2302.01560.</p>
<p>Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8:279-292, 1992.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. CoRR, abs/2201.11903, 2022. URL https://arxiv.org/abs/2201.11903.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. CoRR, abs/2207.01206, 2022a. doi: 10.48550/arXiv.2207.01206. URL https://doi.org/10.48550/arXiv.2207.01206.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. CoRR, abs/2210.03629, 2022b. doi: 10.48550/arXiv.2210.03629. URL https://doi.org/10.48550/arXiv.2210.03629.</p>
<p>Danyang Zhang, Lu Chen, and Kai Yu. Mobile-Env: A universal platform for training and evaluation of mobile interaction. CoRR, abs/2305.08144, 2023. URL https://arxiv.org/abs/2305. 08144 .</p>
<p>Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. CoRR, abs/2305.10250, 2023. doi: 10.48550/arXiv. 2305.10250. URL https://doi.org/10.48550/arXiv.2305.10250.</p>
<p>Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost in the minecraft: Generally capable agents for open-world environments via large language models with text-based knowledge and memory. CoRR, abs/2305.17144, 2023. doi: 10.48550/arXiv.2305.17144. URL https://doi.org/10.48550/arXiv.2305.17144.</p>
<h1>A Details about the observation formats</h1>
<p>Instruction:
i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 40.00 dollars
[Back to Search]
Page 1 (Total results: 50)
[Next &gt;]
[B078GWRC1J]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce $\$ 10.99$
[B078GTKVXY]
Ginger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce $\$ 10.99$
[B08KBVJ4XN]
Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar \&amp; Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)
$\$ 15.95$
Figure 5: Example of the observation of WebShop
The observation of WebShop is simplified based on the text_rich format of WebShop [Yao et al., 2022a] in exactly the same way with Yao et al. [2022b]. Specifically, the HTML markups are omitted, and the buttons are represented in [text] or [[text]] instead of the complicated [button] text [button_] or [clicked button] text [clicked button_]. Furthermore, the number of displayed search results per page is clipped to 3 instead of 10 . An example is shown in Figure 5.
The observation of WikiHow is represented in exactly the same way with Zhang et al. [2023]. Specifically, the page is converted into a sequence of HTML elements corresponding to the visible leaf nodes on the Android ${ }^{\mathrm{TM}}$ view hierarchy (VH). The node classes are converted into HTML tags and a few VH properties are converted into similar HTML attributes. The text property is converted to the text content of the common HTML element or the value attribute of the input element.</p>
<h2>B Lookup table of the pattern-based similarity functions</h2>
<h2>B. 1 Lookup table of the page similarity function of WebShop</h2>
<p>We inspected the pages from WebShop and categorized them into 4 patterns as depicted in Table 9. The similarity lookup table is defined in Table 10.</p>
<h2>B. 2 Lookup table of the instruction similarity function of WikiHow</h2>
<p>We inspected the step instructions from WikiHow and categorized them into 6 patterns as depicted in Table 11.</p>
<p>Table 9: Patterns of WebShop pages</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pattern</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">search</td>
<td style="text-align: left;">The page to search for an item</td>
</tr>
<tr>
<td style="text-align: center;">itemlisting</td>
<td style="text-align: left;">The page listing the search results</td>
</tr>
<tr>
<td style="text-align: center;">item</td>
<td style="text-align: left;">The information page of a specific item</td>
</tr>
<tr>
<td style="text-align: center;">others</td>
<td style="text-align: left;">The item description page, item feature page, and review page</td>
</tr>
</tbody>
</table>
<p>Table 10: Lookup table of the page similarity of WebShop</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">search</th>
<th style="text-align: center;">itemlisting</th>
<th style="text-align: center;">item</th>
<th style="text-align: center;">others</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">search</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">itemlisting</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">item</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">others</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>The similarity lookup table is defined in Table 12.</p>
<h1>C Hyper-parameters</h1>
<p>The discount factor $\gamma$ to accumulate the rewards in the formula of $Q$ value is 1 , which means no discounts are considered. The learning rate $\alpha$ is $1 / N$ where $N$ denotes the times the value is updated. Such a learning rate is chosen, as the tested environments are stationary and each estimation to the value is expected to be equally weighted. The similarity weight factor $\lambda$ is 0.5 , indicating two parts of the similarity function contribute equally.</p>
<h2>D Capability evolving of REMEMBERER</h2>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 6: Performance on a random subset at epochs 1, 3, 5, and 10
We further conducted experiments to see how the capability of REMEMBERER evolves during training. Owing to the limit of budgets, a subset of only 20 tasks is sampled from the full test set. We visualize the performance on the subset of REMEMBERER at epochs 1, 5, and 10. The performance at epoch 3 , which is used for the experiments in the main paper, is visualized as well. The visualization is available in Figure 6. It can be seen that the performance of REMEMBERER improves during the training procedure. However, there seems to be a saturation for the performance, which may be attributed to the limited number of the active exemplars and training tasks. The saturation of the average reward comes later than that of the success rate. This fact indicates that REMEMBERER can still seize more rewards through training on several unsuccessful tasks even the success rate has</p>
<p>Table 11: Patterns of WikiHow instructions</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Pattern Name</th>
<th style="text-align: center;">Pattern Template</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">search</td>
<td style="text-align: center;">Search an article to learn ...</td>
</tr>
<tr>
<td style="text-align: center;">article</td>
<td style="text-align: center;">Access the article ...</td>
</tr>
<tr>
<td style="text-align: center;">author</td>
<td style="text-align: center;">Check the author page of ...</td>
</tr>
<tr>
<td style="text-align: center;">category</td>
<td style="text-align: center;">Access the page of category ...</td>
</tr>
<tr>
<td style="text-align: center;">reference</td>
<td style="text-align: center;">Check the reference list.</td>
</tr>
<tr>
<td style="text-align: center;">about</td>
<td style="text-align: center;">Access the about page ...</td>
</tr>
</tbody>
</table>
<p>Table 12: Lookup table of the instruction similarity of WikiHow</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">search</th>
<th style="text-align: center;">article</th>
<th style="text-align: center;">author</th>
<th style="text-align: center;">category</th>
<th style="text-align: center;">reference</th>
<th style="text-align: center;">about</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">search</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">article</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">author</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">category</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
</tr>
<tr>
<td style="text-align: center;">reference</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.8</td>
</tr>
<tr>
<td style="text-align: center;">about</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1</td>
</tr>
</tbody>
</table>
<p>already saturated. In other words, the hard tasks benefit more from the later phase of training than the easy tasks. Besides, Rememberer reaches saturation on WebShop earlier than on WikiHow. To give an explanation, the number of the experiences in the memory after each training epoch is inspected. As shown in Figure 7, there are much fewer new experiences added into the memory in the later epochs for WebShop than for WikiHow. The certain reason may be due to the specific training set or some internal characteristics of the task domain, which will be further investigated in the future work.</p>
<h1>E $Q$ function fitting ability of REMEMBERER</h1>
<p>Ablation study in the main paper has demonstrated that $n$-step bootstrapping manages to improve precision of the learned $Q$ values in the memory. This section will give further discussion about over-estimation of learned $Q$ values in the memory and whether the LLM can learn the certain $Q$ function through in-context learning (ICL).
Double Q-Learning [van Hasselt, 2010] is usually leveraged to ameliorate over-estimation for lookup-based Q-Learning. Table 13 shows the $Q$ value estimation results with Double Q-Learning applied. Over-estimation does be suppressed, however, serious under-estimation is introduced, and the estimation error fails to ameliorate. This is explained by that Double Q-Learning iteratively updates two $Q$ value lookups and requires more steps to converge to an accurate enough estimation. In contrast, plain Q-Learning performs better in few-step circumstances.</p>
<p>Table 13: Comparison of the average reward estimation of the full model and the Double Q-Learning model</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Task Set</th>
<th style="text-align: center;">Setting</th>
<th style="text-align: center;">#Epochs</th>
<th style="text-align: center;">Avg Reward <br> Estimation</th>
<th style="text-align: center;">Avg <br> Training <br> Reward</th>
<th style="text-align: center;">Abs Error</th>
<th style="text-align: center;">Relative Error</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">WebShop</td>
<td style="text-align: center;">Full Model</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">$\mathbf{0 . 0 2}$</td>
<td style="text-align: center;">$\mathbf{2 . 3 8}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DoubleQL</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">5.33</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DoubleQL</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">10.39</td>
</tr>
<tr>
<td style="text-align: center;">WikiHow</td>
<td style="text-align: center;">Full Model</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2.48</td>
<td style="text-align: center;">2.60</td>
<td style="text-align: center;">$\mathbf{0 . 1 2}$</td>
<td style="text-align: center;">$\mathbf{4 . 6 2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DoubleQL</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2.47</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">14.83</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DoubleQL</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2.70</td>
<td style="text-align: center;">2.90</td>
<td style="text-align: center;">0.20</td>
<td style="text-align: center;">6.90</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 7: Variation of the experience number in the training process</p>
<p>As regards whether the LLM learns the certain $Q$ value function, predicted values of LLM during the test phase on WebShop are inspected. The average absolute error is 0.417 . This fact indicates that the LLM does not really learn the certain $Q$ function, as the reward in WebShop is always between 0 and 1. Nevertheless, the LLM can still predict the appropriate actions. This is due to the inessentiality of absolutely precise $Q$ value prediction during test. It is the relative relation between the values of candidate actions that is truly important. Once LLM can distinguish the valuable actions from candidates, it can take the right policy.</p>
<h1>F Example of the exemplars</h1>
<p>An example of the input exemplar for WebShop and WikiHow is given in Figure 8 and Figure 9, respectively.</p>
<h2>G Case study</h2>
<p>Figure 10 gives a case from the ablation study on necessity of the discouraged actions. If the discouraged actions are omitted in the action advice from an experience without encouraged actions, the LLM will have no ability to avoid failures of the same pattern.</p>
<p>A case from the ablation study on the similarity function on WikiHow task set is depicted in Figure 11. Once the observation similarity $f_{o}$ is omitted, the agent will retrieve experience only according to the instruction and cannot adjust the selection in accordance with the particular observation. This will cause improper experience retrieval and lead to poorer performance.</p>
<p>Last 5 Actions:</p>
<ul>
<li>search[3 ounce bottle bright citrus deodorant sensitive skin]</li>
</ul>
<p>Observation: |
Instruction:
i would like a 3 ounce bottle of bright citrus deodorant for sensitive skin, and price lower than 40.00 dollars
[Back to Search]
Page 1 (Total results: 50)
$[$ Next $&gt;]$
[B078GWRC1J]
Bright Citrus Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce $\$ 10.99$
[B078GTKVXY]
Ginger Fresh Deodorant by Earth Mama | Natural and Safe for Sensitive Skin, Pregnancy and Breastfeeding, Contains Organic Calendula 3-Ounce $\$ 10.99$
[B08KBVJ4XN]
Barrel and Oak - Aluminum-Free Deodorant, Deodorant for Men, Essential Oil-Based Scent, 24-Hour Odor Protection, Cedar \&amp; Patchouli Blend, Gentle on Sensitive Skin (Mountain Sage, 2.7 oz, 2-Pack)
$\$ 15.95$
Available Actions:</p>
<ul>
<li>back to search</li>
<li>next $&gt;$</li>
<li>b078gwrc1j</li>
<li>b078gtkvxy</li>
<li>b08kbvj4xn
...</li>
</ul>
<p>Encouraged:
click[b078gwrc1j] -&gt; 1.0 b078gwrc1j and b078gtkvxy are bright citrus deodorant less then 50 dollars. I can check b078gwrc1j first.
Discouraged:
click[b087wksr2g] -&gt; 0.0 b087wksr2g is not the desired item.</p>
<p>Figure 8: Exemplar for WebShop. YAML markups are adopted to avoid confusing the keywords like "Observation:" with the colon-ended titles in the page representation.</p>
<h1>$\because$</h1>
<p>Task:
Search an article to learn how to hide gauges.
Then, access the article "How to Hide Gauges"
Last 5 Actions:
Screen:
<button alt="Open navigation drawer" id="0" clickable="true"></button>
<img class="wikihow toolbar logo" id="1" clickable="false">
<img class="search button" alt="Search" id="2" clickable="true"></p>
<div class="webView" id="3" clickable="true"></div>
<div class="statusBarBackground" id="4" clickable="false"></div>
<p>Instruction:
Last Reward:
0.0</p>
<p>Total Reward:
0.0</p>
<hr />
<p>Encouraged:
INPUT(2, hide gauges) -&gt; 2.0 <img class="search button" alt="Search" id="2"
clickable="true">
Discouraged:
SCROLL(RIGHT) -&gt; 0.0</p>
<p>Figure 9: Exemplar for WikiHow</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 10: Case of the ablation study on the discouraged actions. As there are no valuable actions to encourage in the experience, a random action is generated. When the discouraged actions with low value are omitted, the LLM may repeat the failure with the same pattern.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 11: Case of the ablation study on the similarity function. Encouraged actions recorded in the experiences are marked by red rectangles.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://openai.com/api/
${ }^{4}$ https://www.amazon.com/
${ }^{5}$ https://www.wikihow.com/Main-Page&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9165 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9165</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9165</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-161.html">extraction-schema-161</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <p><strong>Paper ID:</strong> paper-265506202</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.18658v1.pdf" target="_blank">ArcMMLU: A Library and Information Science Benchmark for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> In light of the rapidly evolving capabilities of large language models (LLMs), it becomes imperative to develop rigorous domain-specific evaluation benchmarks to accurately assess their capabilities. In response to this need, this paper introduces ArcMMLU, a specialized benchmark tailored for the Library&Information Science (LIS) domain in Chinese. This benchmark aims to measure the knowledge and reasoning capability of LLMs within four key sub-domains: Archival Science, Data Science, Library Science, and Information Science. Following the format of MMLU/CMMLU, we collected over 6,000 high-quality questions for the compilation of ArcMMLU. This extensive compilation can reflect the diverse nature of the LIS domain and offer a robust foundation for LLM evaluation. Our comprehensive evaluation reveals that while most mainstream LLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a notable performance gap, suggesting substantial headroom for refinement in LLM capabilities within the LIS domain. Further analysis explores the effectiveness of few-shot examples on model performance and highlights challenging questions where models consistently underperform, providing valuable insights for targeted improvements. ArcMMLU fills a critical gap in LLM evaluations within the Chinese LIS domain and paves the way for future development of LLMs tailored to this specialized area.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9165.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9165.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art multilingual large language model from OpenAI evaluated in this paper as a text-based simulator for Library & Information Science (LIS) multiple-choice questions; shown to be the top-performing model on the ArcMMLU benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multilingual large transformer-based LLM developed by OpenAI (exact parameter count and training details not specified in this paper); treated as a high-performance reference model in the evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Library & Information Science (subdomains: Archival Science, Data Science, Library Science, Information Science)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering single-choice (4-way) domain-specific exam-style questions from ArcMMLU (text-only), i.e., using the LLM as a text-based simulator of domain knowledge and reasoning to select the correct option (A/B/C/D).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (percentage of questions answered correctly); evaluations conducted under zero-shot and few-shot (1–5-shot) prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported in-text: zero-shot 73.46% and five-shot 75.08% (paper tables also provide a five-shot breakdown: Archival 66.38%, Data 82.12%, Library 78.55%, Information 66.79%, average 73.46%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model pretraining exposure to Chinese/in-domain data; reasoning capability (multistep reasoning advantages in Data Science); prompt setting (zero-shot vs few-shot); dataset/domain specificity (interdisciplinary overlap increases exposure); presence/absence of test-data leakage in pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared directly against multiple open-source Chinese-oriented LLMs (Qwen, Baichuan, XVERSE, InternLM, ChatGLM series, ChatGPT). GPT-4 substantially outperforms these models on ArcMMLU (lead of ~5–10+ percentage points vs second best in reported results). A random-choice baseline is implicitly 25%.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even GPT-4 fails on a curated hard subset (ArcMMLU-Hard): average accuracy 20.25% on that subset, with especially low performance on obscure, highly detailed domain-specific facts and easily confusable names/concepts; struggles remain on some multistep or deeply specialized items.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors note GPT-4's strong reasoning and possible heavy consumption of Chinese data during pretraining; still recommend injecting broader and deeper in-domain (especially archival and information science) data during pretraining to close remaining gaps on specialized knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArcMMLU: A Library and Information Science Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9165.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9165.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen-14B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen-14B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source Chinese-oriented LLM (Qwen family) evaluated as a text-based simulator on ArcMMLU; competitive second-ranked model in this benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen-14B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A Qwen-family model from Alibaba Cloud; paper reports Qwen models pretrained on a very large corpus (2.4 trillion tokens); this variant is denoted 14B implying ~14 billion parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Library & Information Science (Archival Science, Data Science, Library Science, Information Science)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering ArcMMLU single-choice questions (domain knowledge + reasoning) as a text-only simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) under zero-shot and few-shot prompt settings.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported averages: zero-shot 68.11% and five-shot 69.07% (five-shot table breakdown: Archival 66.65%, Data 71.51%, Library 71.21%, Information 63.06%, average 68.11%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size (14B) and large-scale pretraining corpus; overlap of subdomain with other domains (interdisciplinary exposure); number of few-shot examples (performance generally improves with more shots for many models).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4 (worse by multiple points) and to other open-source models; Qwen-14B is the top open-source performer but below GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Lower performance in Information Science relative to Data/Library Science; still deficient on highly specialized or obscure items in ArcMMLU-Hard.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Increasing in-domain (LIS) data exposure in pretraining could improve performance; few-shot helps but effectiveness varies across subdomains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArcMMLU: A Library and Information Science Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9165.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9165.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baichuan2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baichuan2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Baichuan-family bilingual LLM used as a text-based simulator on ArcMMLU; shows mid-tier performance among evaluated models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Baichuan2-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Baichuan-series bilingual model (Baichuan2 reported pretraining on large corpora; this evaluated variant is 13B in size).</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Library & Information Science (Archival Science, Data Science, Library Science, Information Science)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Selecting correct answers to ArcMMLU single-choice domain questions (text-only).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) in zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Five-shot reported average 63.81% (table five-shot breakdown: Archival 61.59%, Data 65.58%, Library 66.49%, Information 61.57%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size (13B), pretraining corpus coverage of LIS-related material, interdisciplinary overlap of subdomains, and few-shot prompt examples quantity and choice.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4 and other open-source LLMs; outperformed by GPT-4 and some other large models (e.g., Qwen-14B) but performs reasonably among open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Deficiency on ArcMMLU-Hard items and on subdomains with self-contained disciplinary knowledge (archival and information science) when pretraining lacks such data.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors recommend adding archival and information science data during pretraining to mitigate specific domain knowledge gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArcMMLU: A Library and Information Science Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9165.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9165.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's instruction-tuned conversational LLM evaluated as a text-based simulator for ArcMMLU domain questions; achieves mid-level performance comparable to smaller open-source models.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned conversational LLM from OpenAI (exact model family/version not pinned in this paper); used as one multilingual reference model in evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Library & Information Science (Archival Science, Data Science, Library Science, Information Science)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering ArcMMLU single-choice domain questions in text-only prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) under zero-shot and few-shot settings.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Five-shot reported average 59.20% (five-shot breakdown: Archival 52.37%, Data 65.64%, Library 66.19%, Information 52.61%).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Pretraining exposure to Chinese and LIS-related data; generalist conversational fine-tuning; few-shot examples (generally beneficial but effect varies by model and subdomain).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4 (substantially lower) and to open-source LLMs (roughly comparable to Qwen-7B in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Relatively weaker on archival and information science subdomains; fails on ArcMMLU-Hard items requiring very specific domain knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>To improve performance in LIS subdomains, increase in-domain training data and guard against test-data leakage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArcMMLU: A Library and Information Science Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9165.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9165.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used as text-based simulators in specific scientific subdomains, including details on the simulation task, the accuracy or evaluation results, and any factors or variables identified as affecting simulation accuracy.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilingual GLM-based open-source LLM from the ChatGLM series evaluated on ArcMMLU; notable for an observed decline in some few-shot settings, illustrating sensitivity to prompt/context and potential data-leakage effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM-6B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>ChatGLM-series model developed using GLM pretraining strategies; this variant has ~6B parameters as denoted by name.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Library & Information Science (Archival Science, Data Science, Library Science, Information Science)</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Answering ArcMMLU single-choice domain questions (text-only) as a simulation of domain knowledge and reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Accuracy (%) across subdomains, compared between zero-shot and few-shot (1–5-shot) prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_accuracy</strong></td>
                            <td>Reported values indicate modest performance (paper lists a five-shot average of 44.19% in Table 2); the authors also report a decline in performance for some ChatGLM variants when adding few-shot examples (example in text: ChatGLM-6B showed a drop reported as from 44.19% to 39.67% in the paper's analysis).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Model size (6B), pretraining data coverage, prompt/few-shot example selection; potential test-data leakage in pretraining causing zero-shot to appear artificially stronger and few-shot to degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to larger or better pretrained models (Qwen-14B, GPT-4 etc.); ranks lower overall.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Observed unexpected performance decreases when adding few-shot examples (interpreted as evidence of pretraining/test overlap or sensitivity to prompt format); struggles on ArcMMLU-Hard specialized items.</td>
                        </tr>
                        <tr>
                            <td><strong>author_recommendations_or_insights</strong></td>
                            <td>Authors highlight the risk of test-data leakage and recommend preventing it, and they suggest more careful curation of in-domain training data to address domain-specific shortcomings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArcMMLU: A Library and Information Science Benchmark for Large Language Models', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Measuring massive multitask language understanding <em>(Rating: 2)</em></li>
                <li>Cmmlu: Measuring massive multitask language understanding in chinese <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models <em>(Rating: 2)</em></li>
                <li>Glm: General language model pretraining with autoregressive blank infilling <em>(Rating: 1)</em></li>
                <li>Qwen <em>(Rating: 1)</em></li>
                <li>Baichuan2 <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9165",
    "paper_id": "paper-265506202",
    "extraction_schema_id": "extraction-schema-161",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A state-of-the-art multilingual large language model from OpenAI evaluated in this paper as a text-based simulator for Library & Information Science (LIS) multiple-choice questions; shown to be the top-performing model on the ArcMMLU benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Multilingual large transformer-based LLM developed by OpenAI (exact parameter count and training details not specified in this paper); treated as a high-performance reference model in the evaluations.",
            "scientific_subdomain": "Library & Information Science (subdomains: Archival Science, Data Science, Library Science, Information Science)",
            "simulation_task": "Answering single-choice (4-way) domain-specific exam-style questions from ArcMMLU (text-only), i.e., using the LLM as a text-based simulator of domain knowledge and reasoning to select the correct option (A/B/C/D).",
            "evaluation_metric": "Accuracy (percentage of questions answered correctly); evaluations conducted under zero-shot and few-shot (1–5-shot) prompting.",
            "simulation_accuracy": "Reported in-text: zero-shot 73.46% and five-shot 75.08% (paper tables also provide a five-shot breakdown: Archival 66.38%, Data 82.12%, Library 78.55%, Information 66.79%, average 73.46%).",
            "factors_affecting_accuracy": "Model pretraining exposure to Chinese/in-domain data; reasoning capability (multistep reasoning advantages in Data Science); prompt setting (zero-shot vs few-shot); dataset/domain specificity (interdisciplinary overlap increases exposure); presence/absence of test-data leakage in pretraining.",
            "comparison_baseline": "Compared directly against multiple open-source Chinese-oriented LLMs (Qwen, Baichuan, XVERSE, InternLM, ChatGLM series, ChatGPT). GPT-4 substantially outperforms these models on ArcMMLU (lead of ~5–10+ percentage points vs second best in reported results). A random-choice baseline is implicitly 25%.",
            "limitations_or_failure_cases": "Even GPT-4 fails on a curated hard subset (ArcMMLU-Hard): average accuracy 20.25% on that subset, with especially low performance on obscure, highly detailed domain-specific facts and easily confusable names/concepts; struggles remain on some multistep or deeply specialized items.",
            "author_recommendations_or_insights": "Authors note GPT-4's strong reasoning and possible heavy consumption of Chinese data during pretraining; still recommend injecting broader and deeper in-domain (especially archival and information science) data during pretraining to close remaining gaps on specialized knowledge.",
            "uuid": "e9165.0",
            "source_info": {
                "paper_title": "ArcMMLU: A Library and Information Science Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Qwen-14B",
            "name_full": "Qwen-14B",
            "brief_description": "An open-source Chinese-oriented LLM (Qwen family) evaluated as a text-based simulator on ArcMMLU; competitive second-ranked model in this benchmark.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Qwen-14B",
            "model_description": "A Qwen-family model from Alibaba Cloud; paper reports Qwen models pretrained on a very large corpus (2.4 trillion tokens); this variant is denoted 14B implying ~14 billion parameters.",
            "scientific_subdomain": "Library & Information Science (Archival Science, Data Science, Library Science, Information Science)",
            "simulation_task": "Answering ArcMMLU single-choice questions (domain knowledge + reasoning) as a text-only simulator.",
            "evaluation_metric": "Accuracy (%) under zero-shot and few-shot prompt settings.",
            "simulation_accuracy": "Reported averages: zero-shot 68.11% and five-shot 69.07% (five-shot table breakdown: Archival 66.65%, Data 71.51%, Library 71.21%, Information 63.06%, average 68.11%).",
            "factors_affecting_accuracy": "Model size (14B) and large-scale pretraining corpus; overlap of subdomain with other domains (interdisciplinary exposure); number of few-shot examples (performance generally improves with more shots for many models).",
            "comparison_baseline": "Compared to GPT-4 (worse by multiple points) and to other open-source models; Qwen-14B is the top open-source performer but below GPT-4.",
            "limitations_or_failure_cases": "Lower performance in Information Science relative to Data/Library Science; still deficient on highly specialized or obscure items in ArcMMLU-Hard.",
            "author_recommendations_or_insights": "Increasing in-domain (LIS) data exposure in pretraining could improve performance; few-shot helps but effectiveness varies across subdomains.",
            "uuid": "e9165.1",
            "source_info": {
                "paper_title": "ArcMMLU: A Library and Information Science Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Baichuan2-13B",
            "name_full": "Baichuan2-13B",
            "brief_description": "A Baichuan-family bilingual LLM used as a text-based simulator on ArcMMLU; shows mid-tier performance among evaluated models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Baichuan2-13B",
            "model_description": "Baichuan-series bilingual model (Baichuan2 reported pretraining on large corpora; this evaluated variant is 13B in size).",
            "scientific_subdomain": "Library & Information Science (Archival Science, Data Science, Library Science, Information Science)",
            "simulation_task": "Selecting correct answers to ArcMMLU single-choice domain questions (text-only).",
            "evaluation_metric": "Accuracy (%) in zero-shot and few-shot settings.",
            "simulation_accuracy": "Five-shot reported average 63.81% (table five-shot breakdown: Archival 61.59%, Data 65.58%, Library 66.49%, Information 61.57%).",
            "factors_affecting_accuracy": "Model size (13B), pretraining corpus coverage of LIS-related material, interdisciplinary overlap of subdomains, and few-shot prompt examples quantity and choice.",
            "comparison_baseline": "Compared to GPT-4 and other open-source LLMs; outperformed by GPT-4 and some other large models (e.g., Qwen-14B) but performs reasonably among open-source models.",
            "limitations_or_failure_cases": "Deficiency on ArcMMLU-Hard items and on subdomains with self-contained disciplinary knowledge (archival and information science) when pretraining lacks such data.",
            "author_recommendations_or_insights": "Authors recommend adding archival and information science data during pretraining to mitigate specific domain knowledge gaps.",
            "uuid": "e9165.2",
            "source_info": {
                "paper_title": "ArcMMLU: A Library and Information Science Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT",
            "brief_description": "OpenAI's instruction-tuned conversational LLM evaluated as a text-based simulator for ArcMMLU domain questions; achieves mid-level performance comparable to smaller open-source models.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "Instruction-tuned conversational LLM from OpenAI (exact model family/version not pinned in this paper); used as one multilingual reference model in evaluations.",
            "scientific_subdomain": "Library & Information Science (Archival Science, Data Science, Library Science, Information Science)",
            "simulation_task": "Answering ArcMMLU single-choice domain questions in text-only prompts.",
            "evaluation_metric": "Accuracy (%) under zero-shot and few-shot settings.",
            "simulation_accuracy": "Five-shot reported average 59.20% (five-shot breakdown: Archival 52.37%, Data 65.64%, Library 66.19%, Information 52.61%).",
            "factors_affecting_accuracy": "Pretraining exposure to Chinese and LIS-related data; generalist conversational fine-tuning; few-shot examples (generally beneficial but effect varies by model and subdomain).",
            "comparison_baseline": "Compared to GPT-4 (substantially lower) and to open-source LLMs (roughly comparable to Qwen-7B in this study).",
            "limitations_or_failure_cases": "Relatively weaker on archival and information science subdomains; fails on ArcMMLU-Hard items requiring very specific domain knowledge.",
            "author_recommendations_or_insights": "To improve performance in LIS subdomains, increase in-domain training data and guard against test-data leakage.",
            "uuid": "e9165.3",
            "source_info": {
                "paper_title": "ArcMMLU: A Library and Information Science Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "ChatGLM-6B",
            "name_full": "ChatGLM-6B",
            "brief_description": "A bilingual GLM-based open-source LLM from the ChatGLM series evaluated on ArcMMLU; notable for an observed decline in some few-shot settings, illustrating sensitivity to prompt/context and potential data-leakage effects.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGLM-6B",
            "model_description": "ChatGLM-series model developed using GLM pretraining strategies; this variant has ~6B parameters as denoted by name.",
            "scientific_subdomain": "Library & Information Science (Archival Science, Data Science, Library Science, Information Science)",
            "simulation_task": "Answering ArcMMLU single-choice domain questions (text-only) as a simulation of domain knowledge and reasoning.",
            "evaluation_metric": "Accuracy (%) across subdomains, compared between zero-shot and few-shot (1–5-shot) prompting.",
            "simulation_accuracy": "Reported values indicate modest performance (paper lists a five-shot average of 44.19% in Table 2); the authors also report a decline in performance for some ChatGLM variants when adding few-shot examples (example in text: ChatGLM-6B showed a drop reported as from 44.19% to 39.67% in the paper's analysis).",
            "factors_affecting_accuracy": "Model size (6B), pretraining data coverage, prompt/few-shot example selection; potential test-data leakage in pretraining causing zero-shot to appear artificially stronger and few-shot to degrade performance.",
            "comparison_baseline": "Compared to larger or better pretrained models (Qwen-14B, GPT-4 etc.); ranks lower overall.",
            "limitations_or_failure_cases": "Observed unexpected performance decreases when adding few-shot examples (interpreted as evidence of pretraining/test overlap or sensitivity to prompt format); struggles on ArcMMLU-Hard specialized items.",
            "author_recommendations_or_insights": "Authors highlight the risk of test-data leakage and recommend preventing it, and they suggest more careful curation of in-domain training data to address domain-specific shortcomings.",
            "uuid": "e9165.4",
            "source_info": {
                "paper_title": "ArcMMLU: A Library and Information Science Benchmark for Large Language Models",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Measuring massive multitask language understanding",
            "rating": 2,
            "sanitized_title": "measuring_massive_multitask_language_understanding"
        },
        {
            "paper_title": "Cmmlu: Measuring massive multitask language understanding in chinese",
            "rating": 2,
            "sanitized_title": "cmmlu_measuring_massive_multitask_language_understanding_in_chinese"
        },
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models",
            "rating": 2,
            "sanitized_title": "ceval_a_multilevel_multidiscipline_chinese_evaluation_suite_for_foundation_models"
        },
        {
            "paper_title": "Glm: General language model pretraining with autoregressive blank infilling",
            "rating": 1,
            "sanitized_title": "glm_general_language_model_pretraining_with_autoregressive_blank_infilling"
        },
        {
            "paper_title": "Qwen",
            "rating": 1
        },
        {
            "paper_title": "Baichuan2",
            "rating": 1
        }
    ],
    "cost": 0.013257,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ArcMMLU: A Library and Information Science Benchmark for Large Language Models
30 Nov 2023</p>
<p>Shitou Zhang shitouzhang@whu.edu 
Key Laboratory of Archival Intelligent Development and Service
NAAC</p>
<p>School of Information Management
Wuhan University</p>
<p>Zuchao Li 
School of Computer Science
Wuhan University</p>
<p>Xingshen Liu xingshenliu@whu.edu 
School of Information Management
Wuhan University</p>
<p>Liming Yang 
School of Law
Tsinghua University</p>
<p>Ping Wang wangping@whu.edu 
Key Laboratory of Archival Intelligent Development and Service
NAAC</p>
<p>School of Information Management
Wuhan University</p>
<p>Jinze Bai 
Shuai Bai 
Yunfei Chu 
Zeyu Cui 
Kai Dang 
Xiaodong Deng 
Yang Fan 
Wenbin Ge 
Yu Han 
Fei Huang 
Binyuan Hui 
Luo Ji 
Mei Li 
Junyang Lin 
Runji Lin 
Dayiheng Liu 
Gao Liu 
Chengqiang Lu 
Keming Lu 
Jianxin Ma 
Rui Men 
Xingzhang Ren 
Xuancheng Ren 
Chuanqi Tan 
Sinan Tan 
Jianhong Tu 
Peng Wang 
Shijie Wang 
Wei Wang 
Sheng- Guang Wu 
Benfeng Xu 
Jin Xu 
An Yang 
Hao Yang 
Jian Yang 
Shusheng Yang 
Yang Yao 
Bowen Yu 
Hongyi Yuan 
Zheng Yuan 
Jianwei Zhang 
Xingx- Uan Zhang 
Yichang Zhang 
Zhenru Zhang 
Chang Zhou 
Jingren Zhou 
Xiaohuan Zhou 
Tianhang 2023 Zhu 
Qwen 
Tom B Brown 
Benjamin Mann 
Nick Ryder 
Melanie Subbiah 
Jared Kaplan 
Prafulla Dhariwal 
Arvind Neelakantan 
Pranav Shyam 
Girish Sastry 
Amanda Askell 
Sandhini Agarwal 
Ariel Herbert-Voss 
Gretchen Krueger 
Tom Henighan 
Rewon Child 
Aditya Ramesh 
Daniel M Ziegler 
Jeffrey Wu 
Clemens Winter 
Christopher Hesse 
Mark Chen 
Eric Sigler 
Mateusz Litwin 
Scott Gray 
Benjamin Chess 
Jack Clark 
Christopher Berner 
Sam Mccandlish 
Alec Radford 
Ilya Sutskever 
Dario Amodei 
Jerry Tworek 
Heewoo Jun 
Qiming Yuan 
Henrique Ponde 
Oliveira Pinto 
Jared Ka- Plan 
Harri Edwards 
Yuri Burda 
Nicholas Joseph 
Greg Brockman 
Alex Ray 
Raul Puri 
Michael Petrov 
Heidy Khlaaf 
Girish Sas- Try 
Pamela Mishkin 
Brooke Chan 
Mikhail Pavlov 
Alethea Power 
Lukasz Kaiser 
Mohammad Bavarian 
Philippe Tillet 
Felipe Petroski Such 
Dave Cum- Mings 
Matthias Plappert 
Fotios Chantzis 
Eliza- Beth Barnes 
William Hebgen Guss 
Alex Nichol 
Alex Paino 
Nikolas Tezak 
Jie Tang 
Igor Babuschkin 
Suchir Balaji 
Shantanu Jain 
William Saunders 
Andrew N Carr 
Jan Leike 
Josh Achiam 
Vedant Misra 
Evan Morikawa 
Matthew Knight 
Miles Brundage 
Mira Murati 
Katie Mayer 
Peter Welinder 
Bob Mcgrew 
Wojciech 2021 Zaremba 
Evaluating 
Karl Cobbe 
Vineet Kosaraju 
Jacob Hilton 
Reiichiro Nakano </p>
<p>Quality Checking 05 Filtering 03 Post processing 06</p>
<p>ArcMMLU: A Library and Information Science Benchmark for Large Language Models
30 Nov 20239EDB9FEF63784CD838F4D37303A2EC41arXiv:2311.18658v1[cs.CL]
In light of the rapidly evolving capabilities of large language models (LLMs), it becomes imperative to develop rigorous domain-specific evaluation benchmarks to accurately assess their capabilities.In response to this need, this paper introduces ArcMMLU, a specialized benchmark tailored for the Library &amp; Information Science (LIS) domain in Chinese.This benchmark aims to measure the knowledge and reasoning capability of LLMs within four key sub-domains: Archival Science, Data Science, Library Science, and Information Science.Following the format of MMLU/CMMLU, we collected over 6,000 high-quality questions for the compilation of ArcMMLU.This extensive compilation can reflect the diverse nature of the LIS domain and offer a robust foundation for LLM evaluation.Our comprehensive evaluation reveals that while most mainstream LLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a notable performance gap, suggesting substantial headroom for refinement in LLM capabilities within the LIS domain.Further analysis explores the effectiveness of few-shot examples on model performance and highlights challenging questions where models consistently underperform, providing valuable insights for targeted improvements.ArcMMLU fills a critical gap in LLM evaluations within the Chinese LIS domain and paves the way for future development of LLMs tailored to this specialized area 1 .</p>
<p>Introduction</p>
<p>Large language models (LLMs) have made remarkable advancements in recent years, pushing the boundaries of AI to unprecedented heights (Brown et al., 2020;Hoffmann et al., 2022;Touvron et al., 2023a,b;OpenAI, 2023;Li et al., 2023b;Zhang et al., 2023).Consequently, evaluating LLM capabilities has become increasingly complex and challenging.Traditional benchmarks like GLUE (Wang et al., 2019) and SuperGLUE (Wang et al., 2020) have become insufficient for LLMs evaluation, as they lack the nuance and difficulty to effectively differentiate the advanced capabilities of newly developed models.</p>
<p>To address this, comprehensive benchmarks such as MMLU (Hendrycks et al., 2021) and CMMLU (Li et al., 2023a) have been introduced.These benchmarks cover a broad spectrum of subjects, ranging from mathematics to social science, becoming essential tools for assessing LLMs' knowledge and reasoning capabilities.</p>
<p>However, there exists a notable void in LLM evaluation within the Library &amp; Information Science (LIS) domain.The LIS domain possesses a distinct set of disciplinary knowledge that general benchmarks like MMLU and CMMLU cannot fully capture.Professionals in the LIS field, such as archivists and librarians, urgently require the assistance of LLMs to enhance their work efficiency.Yet, the development of LIS-domainfocused LLMs is hindered by the lack of comprehensive, domain-specific model evaluation tools.Such tools are essential for accurately accessing the effectiveness of LLMs in addressing the unique challenges and tasks faced by LIS professionals, such as complex information retrieval, data organization, and user interaction in archiveslibrary settings.Therefore, there is a compelling need for a specialized LIS domain benchmark to accurately and thoroughly assess models' capabilities in this specialized area.</p>
<p>In this study, we introduce ArcMMLU, a Chinese-oriented benchmark specifically designed for evaluating LLMs in the LIS domain.We put our focus on four key subdomains: Archival Science, Data Science, Library Science, and Information Science.To compile a comprehensive benchmark that can effectively reflect the nature of LIS and provide robust evaluation, we have collected Figures</p>
<p>ArcMMLU Construction</p>
<p>Data Annotation 04</p>
<p>Team Formation Manual Annotation Cross Inspection Inconsistency Resolving Textual Questions Annotation over 6,000 high-quality single-choice questions following the same format of MMLUCMMLU.Also, meticulous cleaning, filtering, and inspection have been performed to ensure the data quality.</p>
<p>Based on the constructed ArcMMLU, we conduct an extensive evaluation of the mainstream LLMs, including GPT-4.The evaluation results reveal that while most mainstream LLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a notable performance gap, suggesting substantial headroom for improvement.</p>
<p>To uncover insights for guiding future development, we conduct further analysis which includes:</p>
<p>(1) exploring the effect of few-shot examples on model performance; (2) performing error analysis on the challenging questions where current models consistently underperform.From the results, we observe that some models' performance diminishes with the insertion of more few-shot examples, indicating potential test data leakage during their training phase.Additionally, our error analysis reveals a notable deficiency in LIS-domain-specific knowledge.This finding suggests the need for incorporating more LIS-related data in training to enhance LLMs' domain knowledge.</p>
<p>The contribution of this work can be summarized in three folds:</p>
<ol>
<li>
<p>We construct a comprehensive LIS-domain-focused LLM evaluation benchmark, Ar-cMMLU, which covers four key subdomains with over 6,000 high-quality real-world questions.</p>
</li>
<li>
<p>We evaluate the mainstream LLMs on Ar-cMMLU and the results reflect the current models' capabilities and limitations across different subdomains.</p>
</li>
<li>
<p>We conduct further analysis to identify critical areas for potential improvement, providing insights for future LLM development in LIS domain.</p>
</li>
</ol>
<p>Related Work</p>
<p>Benchmark evaluations play an essential role in tracking AI advancements, particularly in the rapidly evolving field of LLM.In the LLM era, traditional NLP benchmarks (Wang et al., 2018;Xu et al., 2020;Wang et al., 2020) are increasingly proving to be inadequate.On one hand, these benchmarks focus on assessing model performance in specific tasks or dimensions but do not comprehensively reflect a model's capabilities across diverse domains and tasks.On the other hand, the level of challenge posed by these benchmarks is often too simplistic for cutting-edge LLMs (OpenAI, 2023;Touvron et al., 2023b), failing to differentiate between various models.</p>
<p>To address the limitations of traditional benchmarks, LLM-oriented benchmarks have been introduced (Hendrycks et al., 2021;Chen et al., 2021;Cobbe et al., 2021;Srivastava et al., 2023;Zellers et al., 2019;Lin et al., 2022).These benchmarks are either more comprehensive or more challenging, making them robust platforms for evaluating advanced LLMs.For Chinese-oriented models, similar benchmarks such as C-Eval (Huang et al., 2023) and CMMLU (Li et al., 2023a) have been developed.These benchmarks are designed to evaluate models across a broad spectrum of subjects, posing a high challenge for model knowledge.Moreover, other benchmarks focus on task-specific or domainspecific evaluation, such as LawBench (Fei et al., 2023) and PIXIU (Xie et al., 2023), which assess models in specialized areas.</p>
<p>Despite that various benchmarks (Zeng, 2023;Liu et al., 2023;Zhong et al., 2023) have been introduced, there exists a void in the LIS domain.Such void stems from the uniqueness of the LIS domain.Unlike more general fields, LIS encompasses a distinct and relatively self-contained set of disciplinary knowledge, often not adequately covered by comprehensive benchmarks like CMMLU.ArcMMLU aims to bridge such gap by providing a tailored evaluation platform.It focuses on the specific needs and knowledge inherent to LIS, such as information retrieval and archival management.Based on ArcMMLU, LLMs that are better suited to the unique requirements of the LIS field can be developed.</p>
<p>ArcMMLU</p>
<p>Task Overview ArcMMLU, our LIS-focused evaluation benchmark in Chinese, covers four key subdomains: Archival Science, Data Science, Library Science, and Information Science.Each of these subdomains has its distinct focus: Archival Science on the management and utilization of archival resources; Data Science on the processing and analysis of data; Library Science on the organization of library resources; and Information Science on information retrieval and management.We recognize that definitions of LIS disciplinary scope vary across different countries and regions and LIS encompasses more than these four areas.We plan to incorporate more subareas of LIS in future updates to provide a more comprehensive view of the field.</p>
<p>Construction To build a high-quality benchmark, we designed and implemented a meticulous construction process.The overall workflow of constructing ArcMMLU is presented in Figure 1.In the preparation phase of the project, we formed a team comprising 8 graduate students all with a background in LIS and 2 experienced professors as supervisors.The raw data was sourced from real-world questions of past postgraduate entrance exams, professional exams, course quizzes, and academic competitions to authentically reflect the characteristics of the LIS domain.Some original examples were stored in image format, from which we extracted textual content using OCR, followed by manual post-OCR result checking and quality filtering.During the filtering stage, we removed all samples containing images or tables that could not fit into text-based queries for LLMs.For the unlabeled samples in the raw data, we conducted manual labeling, with the supervisors resolving any inconsistencies among different annotators.The total labor cost of data collection, preprocessing, annotation, and inspection is around 300 hours.After quality filtering, we obtained a total of 6,210 samples.From these, we selected 20 representative samples as few-shot examples.The final statistics of ArcMMLU are detailed in Table 1</p>
<p>Experiments</p>
<p>To gain a comprehensive understanding of existing open-source Chinese-oriented LLMs, our study evaluates models from six leading model families.</p>
<p>By comparing and analyzing their results on the ArcMMLU benchmark, we can assess their knowledge and reasoning capabilities in the LIS domain and identify potential areas for improvement.</p>
<p>Setup</p>
<p>In ArcMMLU, all examples are single-choice questions with four options, where only one option is  correct.For a given question, models need to select the label representing the correct answer from 'A', 'B', 'C', and 'D'.We designed regular expressions to ensure that the model's output label is correctly recognized and extracted.To ensure fairness, if a model's output does not contain any meaningful label, we randomly sample a label from a uniform distribution as the response for that sample.Thus, a randomly initialized model would have a baseline performance of around 25% on ArcMMLU.We followed the prompt format of CMMLU.For each question, we prefix it with "以下是关 于[主题]的单项选择题，请直接给出正确答案 的选项(Below is a single-choice question about [subject], please provide the correct answer choice directly)."In zero-shot evaluations, questions are immediately presented following the prompt.In few-shot evaluations, up to 5 examples with their corresponding answers are provided prior to the question.Prompt examples from ArcMMLU are shown in Figure 2.</p>
<p>Models</p>
<p>In our comprehensive evaluation, we test mainstream Chinese-oriented open-source Large Lan-guage Models (LLMs) using the ArcMMLU benchmark.We also include the leading multilingual models, ChatGPT and GPT-4, for broader comparative analysis.Specifically, these models belong to six distinct model families:</p>
<p>ChatGLM Series Developed by Tsinghua University (Zeng et al., 2022), the ChatGLM series are bilingual models in Chinese and English.They are based on the unique GLM pre-training strategy (Du et al., 2022) and are highly popular in the open-source community.In this study, we test both ChatGLM and ChatGLM2.</p>
<p>Qwen Series Created by Alibaba Cloud (Bai et al., 2023), Qwen models have been pre-trained on a large corpus of 2.4 trillion tokens.We evaluate both the 7B and 14B versions to access their capabilities.</p>
<p>Baichuan Series Developed by Baichuan Inc. (Yang et al., 2023), these bilingual models are also trained on extensive high-quality corpora.For instance, Baichuan2 was pre-trained with 2.6 trillion tokens.</p>
<p>XVERSE Series</p>
<p>The XVERSE models, developed by Shenzhen Yuanxiang Technology and available in 7B and 13B sizes, have shown leading performance in various comprehensive benchmarks.</p>
<p>InternLM Series InternLM, developed by Sense-Time, comes in two sizes: 7B and 20B.We evaluated both to assess their performance on Ar-cMMLU.</p>
<p>ChatGPT and GPT-4 Developed by OpenAI, these globally leading LLMs are included to provide additional comparisons.By comparing existing open-source models with these, we aim to provide guiding insights for the future development of Chinese-oriented models.</p>
<p>Main Results</p>
<p>Table 2 and Table 3 present the performance of all models under a five-shot setting and a zero-shot setting, respectively.</p>
<p>By Model Analyzing the results presented in Table 2 and Table 3, we gain insights into the performance of various LLMs across different subdomains.In both the five-shot and zero-shot settings, GPT4 demonstrates exceptionally powerful performance, reaching 73.46% in zero-shot and 75.08% in five-shot, significantly outperforming the second-ranked Qwen-14B, which scores 68.11% and 69.07% in zero-shot and five-shot, respectively.Notably, GPT4's lead is particularly significant in the data science and library science subdomains.For example, under a zero-shot setting, it outperforms other models by at least 10.61% and 7.34% in these two subdomains, respectively.Unlike GPT-4, ChatGPT, though also developed by OpenAI, demonstrates moderate performance, roughly matching the open-source Qwen-7B.Another observable trend is that larger models generally exhibit better performance, with XVERSE-7B being an exception.Despite having only 7B parameters, it ranks just behind the larger Qwen-14B and Baichuan2-13B in both zero-shot and five-shot settings, even surpassing its own 13B version.</p>
<p>When comparing the zero-shot and five-shot methods, we note a difference in performance.Most models show performance improvements with the introduction of five-shot examples.However, some models, like ChatGLM-6B (dropping from 44.19% to 39.67%) and ChatGLM2-6B (from 52.93% to 51.84%), exhibit a decline in performance.In Section 5, we delve further into the analysis of these models that exhibit unexpected performance trends in the five-shot setting, exploring potential factors influencing these outcomes.</p>
<p>By subject Figure 3 shows the averaged accuracy of all models across four subdomains.We observe that models generally perform better in library science and data science, with averaged accuracies reaching 60.87% and 60.56%, respectively.In contrast, performance in archival science and information science is notably lower, at 56.31% and 55.15%, respectively.We speculate that this phenomenon may be caused by the interdisciplinary nature of data science and library science, which extensively overlap with other fields such as computer science and management.Hence, models are likely more exposed to information from these domains during the pre-training phase, resulting in stronger performance in these areas.On the other hand, archival science and information science have more self-contained disciplinary knowledge, and if models lack sufficient exposure to data distributions closely aligned to these subdomains during pre-training, it could lead to a deficiency in their capabilities.Based on this observation, future development of LIS-domain-specialized models should consider incorporating more archival and information science-related data in the pre-training phase to specifically enhance model performance in these areas.Comparing the results under different few-shot settings, from 1-shot to 5-shot, all subdomains exhibit a trend where more few-shot examples lead to better performance.However, when contrasting zero-shot with few-shot results, the impact of introducing few-shot examples on performance varies across different subdomains.For instance, archival science surpasses its zero-shot result at 2-shot, but the other three subdomains only exceed their zeroshot performances at 5-shot.This suggests that the effectiveness of few-shot learning differs among subdomains, providing insights for tailored training approaches.</p>
<p>Analysis</p>
<p>To further explore the behavior of LLMs in the LIS domain, we conduct a detailed analysis.Specifically, our analysis centers around four key aspects:</p>
<p>(1) the usefulness of few-shot examples; (2) error analysis; (3) comparison with the leading model, GPT-4; and (4) potential directions for improvement.</p>
<p>Usefulness of Few-Shot Examples</p>
<p>As mentioned in the previous section, we observed an unexpected phenomenon where some models' performance actually declines in the five-shot setting compared to zero-shot.To investigate the underly-  (Wei et al., 2023) reveals the risk that some existing LLMs have been exposed to data with a similar distribution to test data during pre-training, leading to overfitting issues on test data.This results in better performance under a zero-shot setting and a decline under a relatively unfamiliar few-shot setting.This observation suggests the necessity of highlighting the prevention of potential data leakage in future model development.Additionally, apart from direct data source restrictions, data from closely related fields that overlap extensively with the target domain can also indirectly lead to data leakage.For example, similar to ChatGLM-6B, ChatGLM2-6B also shows a performance drop in the data science and library science subdomains, which have more interdisciplinary overlap, whereas there is no such drop in the more self-contained subdomains of archival science and information science.</p>
<p>Error Analysis &amp; Comparison with GPT-4 In LLM evaluations, error analysis is essential for highlighting their strengths and identifying their weaknesses.To understand the limitations of existing open-source LLMs, we extract questions that all open-source LLMs fail to answered correctly and compile them into a challenging test subset, named ArcMMLU-Hard.This subset implies a higher level of difficulty and challenge.We evaluate the performance of GPT-4 on this subset, and the results are detailed in Table 4 and Figure 5. Notably, GPT-4 performs well on ArcMMLU-Hard, achieving an average accuracy of 20.25%.Its performance is particularly high in Data Science, reaching 38.55%.Upon manual inspection, we found that many of the Data Science questions in the ArcMMLU-Hard subset involve complex multistep reasoning and calculations, posing a significant challenge for other models.GPT-4, however, manages these tasks more effectively, demonstrating its robust reasoning capabilities.By manually checking GPT-4's correct answers in other subdomains of the ArcMMLU-Hard subset, we observe that GPT-4 not only possesses strong reasoning capabilities but also an impressive level of knowledge.For example, as shown in Appendix Figure 6, GPT-4 correctly answers a question from Archival Science that tests a detailed regulation in Chinese archival management.This indicates that, although it is a multi-lingual general model, GPT-4 might have consumed a vast amount of Chinese data during its pre-training to achieve such powerful performance on a Chinese domain-specific evaluation.</p>
<p>Subdomain</p>
<p>Despite GPT-4's impressive performance, it is not infallible.Appendix Figure 7 presents questions that both GPT-4 and other models answer incorrectly.These questions often involve (1) highly detailed and obscure domain-specific knowledge, and (2) easily confusable names and concepts.This observation highlights a critical limitation in the training of LLMs.Despite their extensive and diverse training data, these models struggle with questions requiring specific in-domain knowledge, which is not adequately injected during their training.This underscores the importance of incorporating a more comprehensive dataset that spans a broader spectrum of specialized knowledge domains.Addressing this limitation is essential for enhancing the models' ability to handle a wider range of challenging and specialized questions, thereby improving their overall applicability and reliability.</p>
<p>Conclusion</p>
<p>In this study, we introduce ArcMMLU, a comprehensive LLM evaluation benchmark specifically designed for the Chinese Library &amp; Information Science (LIS) domain.ArcMMLU covers four key subdomains: Archival Science, Data Science, Information Science, and Library Science, with over 6,000 high-quality questions obtained through meticulous collection, annotation, and inspection.Our benchmark addresses the existing gap in LLM evaluation within the LIS domain.On one hand, it offers a robust platform to test existing models' knowledge and reasoning capabilities in the LIS domain.On the other hand, it provides insightful guidance for future development of LIS-domainfocused LLMs.Through our evaluation of mainstream LLMs on ArcMMLU, we identify significant areas for improvement.These include, but are not limited to, (1) preventing both direct and indirect test data leakage, and (2) incorporating more in-domain data during the pre-training stage to enhance models' domain knowledge and capabilities.Overall, ArcMMLU is a valuable addition to the current LLM evaluation landscape in the LIS domain and serves as a foundation for future development of LLMs within this specialized area.</p>
<p>Figure 1 :
1
Figure 1: The overall construction workflow of ArcMMLU.</p>
<p>Figure 2 :
2
Figure 2: Prompt examples from ArcMMLU.</p>
<p>Figure 4 :
4
Figure 4: The four knowledge domain's average performance of every LLMs from 0-shot to 5-shot</p>
<p>Figure 5 :
5
Figure 5: Performance of GPT-4 on the ArcMMLU-Hard subset.</p>
<p>档案学(Figure 6 :
6
Figure 6: Examples where GPT-4 provides correct answers while other open-source LLMs do not.</p>
<p>Figure 7 :
7
Figure 7: Example where both GPT-4 and other open-source LLMs provide incorrect answers.</p>
<p>Table 1 :
1
. Statistics of the dev and test sets of ArcMMLU.
Knowledge AreaDev TestArchive Science5 2213Information Science5 1674data Science5 1499Library Science5804Total20 6190</p>
<p>When we want to predict a discrete variable, such as a person's marital status.)C.当我们想预测一个分类变量，例如人的性别。 (C.When we want to predict a categorical variable, such as a person's gender.)D.当我们有一组变量，并且想找出它们之间的关系。 (D.When we have a set of variables and want to find out their relationship.)
答案: A (Answer: A)Subject: 图书馆学 (Library Science)问题: 《中图法》中基本大类由22个字母表示，"E"和"O"分别表示的是( )。(Question: In the "Chinese Library Classification," the major categories are represented by 22 letters. What do "E" and "O" respectively represent?)A.军事，数理科学和化学 (A. Military, mathematics and chemistry)B.文学，环境科学 (B. Literature, environmental science)C.语言文学、天文学 (C. Language and literature, astronomy)D.经济，法律 (D. Economics, law)答案: A (Answer: A)
以下是关于[主题]的单项选择题，请直接给出正确答案的选项 (Below is a single-choice question about [subject], please provide the correct answer choice directly.)Subject: 档案学 (Archival Science) 问题: 档案与图书、文献资料相比，其特有属性是( )。 (Question: Compared with books and literature, what is the unique attribute of archives?)A.原始记录性 (A.Original record nature) B.知识性 (B.Knowledge nature) C.信息性 (C.Informational nature) D.服务型 (D. Service-oriented) 答案: A (Answer: A) Subject: 数据科学 (Data Science) 问题: 在下列哪种情况下，我们通常使用回归分析？( ) (Question: Under which of the following situations do we typically use regression analysis?)A.当我们想预测一个连续的变量，例如人的身高。 (A.When we want to predict a continuous variable, such as a person's height.)B.当我们想预测一个离散的变量，例如人的婚姻状态。 (B.Subject: 信息学 (Information Science) 问题: 在信息分布的规律和特征中，揭示论文在科学期刊中的分布规律的定律是( )。 (Question: Among the laws and characteristics of information distribution, which law reveals the distribution pattern of papers in scientific journals?)A.马太效应 (A.Matthew Effect) B.布拉德福定律 (B.Bradford's Law) C.齐夫定律 (C.Zipf's Law) D.洛特卡定律 (D. Lotka's Law) 答案: B (Answer: B)</p>
<p>Table 2 :
2
Results of all models under a five-shot setting.All results are reported in accuracy (%).
ModelArchivalDataLibraryInformationAverageGPT466.3882.1278.5566.7973.46Qwen-14B66.6571.5171.2163.0668.11Baichuan2-13B61.5965.5866.4961.5763.81XVERSE-7B61.8265.6466.6759.5863.43Qwen-7B59.7864.3163.6856.4761.06ChatGPT52.3765.6466.1952.6159.20InternLM-20B53.8261.3763.1455.7258.51Baichuan2-7B56.0861.1161.1155.7258.50Baichuan-13B54.4157.1761.0554.4856.78XVERSE-13B53.3756.7359.6257.2156.73InternLM-7B51.8859.9761.2951.3756.13ChatGLM2-6B52.0656.9753.7249.3852.93Baichuan-7B49.2146.3648.6945.2747.38ChatGLM-6B43.0646.9647.9138.8144.19</p>
<p>Table 3 :
3
Results of all models under a zero-shot setting.All results are reported in accuracy (%).</p>
<p>Table 4 :
4
Statistics of the ArcMMLU-Hard subset and GPT-4's results.
All Corr. Wrong Acc. (%)Archive113159813.27Data83325138.55Information93177618.28Library5564910.91</p>
<p>Table 8 :
8
Results of all models under a four-shot setting.All results are reported in accuracy (%).</p>
<p>A Additional Few-shot ResultsIn this section, we present the experimental results under 1-shot to 4-shot settings, shown in Tables5  to 8.B Error Analysis Examples
Glm: General language model pretraining with autoregressive blank infilling. Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational Linguistics20221</p>
<p>Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge, arXiv:2309.16289Lawbench: Benchmarking legal knowledge of large language models. 2023arXiv preprint</p>
<p>Measuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, 2021</p>
<p>. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Tom Clark, Eric Hennigan, Katie Noland, George Millican, Bogdan Van Den Driessche, Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre2022Training compute-optimal large language models</p>
<p>Maosong Sun, and Junxian He. 2023. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, </p>
<p>Cmmlu: Measuring massive multitask language understanding in chinese. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, 2023aYeyun Gong, Nan Duan, and Timothy Baldwin</p>
<p>Batgpt: A bidirectional autoregessive talker from generative pre-trained transformer. Zuchao Li, Shitou Zhang, Hai Zhao, Yifei Yang, Dongjie Yang, 2023b</p>
<p>Truthfulqa: Measuring how models mimic human falsehoods. Stephanie Lin, Jacob Hilton, Owain Evans, 2022</p>
<p>M3ke: A massive multi-level multi-subject knowledge evaluation benchmark for chinese large language models. Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, Xiaowen Su, Qun Liu, Deyi Xiong, 2023</p>
<p>Gpt-4 technical report. 2023OpenAI</p>
<p>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal, Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D Manning, Christopher Potts, Cindy Ramirez, Clara E Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard De Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden, John Miller, John U Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B Tenenbaum, Joshua S Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, D Kaustubh, Kevin Dhole, Kevin Gimpel, Kory Omondi, Kristen Mathewson, Ksenia Chiafullo, Kumar Shkaruta, Kyle Shridhar, Kyle Mc-Donell, Laria Richardson, Leo Reynolds, Li Gao, Liam Zhang, Lianhui Dugan, Lidia Qin, Louis-Philippe Contreras-Ochando, Luca Morency, Lucas Moschella, Lucy Lam, Ludwig Noble, Luheng Schmidt, Luis He, Luke Oliveros Colón, Metz ; Maheen, Manaal Farooqi, Mantas Faruqui, Marco Mazeika, Marco Baturan, Marco Marelli, Maria Maru, Jose Ramírez, Marie Quintana, Mario Tolkiehn, Martha Giulianelli, Martin Lewis, Matthew L Potthast, Matthias Leavitt, Mátyás Hagen, Medina Schubert, Melody Orduna Baitemirova, Melvin Arnaud, Michael A Mcelrath, Michael Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michał Strube, Michele Swędrowski, Michihiro Bevilacqua, Mihir Yasunaga, Mike Kale, Mimee Cain, Mirac Xu, Mitch Suzgun, Mo Walker, Mohit Tiwari, Moin Bansal, Mor Aminnaseri, Mozhdeh Geva, Mukund Gheini, T Varma, Nanyun Peng, Nathan A Chi, Nayeon Lee, Neta Gur-, Ari Krakover, ; Samuel, S Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh ; Tao, Tao Li, Tariq Yu, Tatsu Ali, Hashimoto, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve. Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal. Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T Piantadosi, Stuart M Shieber, Summer Misherghi, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff; Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang; Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang; Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman; Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick; Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz; Uri Shaham; Yichi Yang, YidingStefan DivicShyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene ; Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster,Ruslan Salakhutdinov. Te-Lin Wu, Théo Desbordes. Timofei Kornev, Titus Tunduny, Tobias Gerstenberg. Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi</p>
<p>. Yifu Hao, Yonatan Chen, Yu Belinkov, Yufang Hou, Yuntao Hou, Zachary Bai, Zhuoye Seid, Zijian Zhao, Zijie J Wang, Zirui Wang, Ziyi Wang, Wu, </p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. </p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. </p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, ; Jian, Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Pushkar Mishra, Igor Molybog. Andrew Nie, Jeremy Poulton, Rashi Reizenstein, Kalyan Rungta, Alan Saladi, Ruan Schelten, Eric Michael Silva, Ranjan Smith, Xiaoqing Subramanian, Ellen Tan, Binh Tang, Ross Taylor, Adina Williams,; Angela Fan, Melanie Kambadur, Sharan Narang; Robert Stojnic, Sergey EdunovAurelien Rodriguezand Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 2020</p>
<p>GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, 10.18653/v1/W18-5446Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPBrussels, BelgiumAssociation for Computational Linguistics2018</p>
<p>Glue: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, 2019</p>
<p>Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie Wang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei Lü, Rui Hu, Chenxia Li, Liu Yang, Xilin Luo, Xuejie Wu, Lunan Liu, Wenjun Cheng, Peng Cheng, Jianhao Zhang, Xiaoyu Zhang, Lei Lin, Xiaokun Wang, Yutuan Ma, Chuanhai Dong, Yanqi Sun, Yifu Chen, Yongyi Peng, Xiaojuan Liang, Shuicheng Yan, Han Fang, Yahui Zhou, Skywork: A more open bilingual foundation model. 2023</p>
<p>Pixiu: A large language model, instruction data and evaluation benchmark for finance. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, Jimin Huang, 2023</p>
<p>CLUE: A Chinese language understanding evaluation benchmark. Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan, 10.18653/v1/2020.coling-main.419Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (Online2020International Committee on Computational Linguistics</p>
<p>Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Dian Da Pan, Dong Wang, Fan Yan, Yang, arXiv:2309.10305Open large-scale language models. 20232arXiv preprint</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, 2019</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, arXiv:2210.02414Glm-130b: An open bilingual pre-trained model. 2022arXiv preprint</p>
<p>Measuring massive multitask chinese understanding. Hui Zeng, 2023</p>
<p>Shitou Zhang, Jingrui Hou, Siyuan Peng, Zuchao Li, Qibiao Hu, Ping Wang, Arcgpt: A large language model tailored for real-world archival applications. 2023</p>
<p>Agieval: A human-centric benchmark for evaluating foundation models. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, Nan Duan, 2023</p>
<p>. Qwen-14b , </p>
<p>Table 6: Results of all models under a two-shot setting. All results are reported in accuracy (%). Model Archival Data Library Information Average Qwen-14B</p>
<p>Table 7: Results of all models under a three-shot setting. All results are reported in accuracy (%). </p>            </div>
        </div>

    </div>
</body>
</html>
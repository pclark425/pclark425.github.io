<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-526 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-526</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-526</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-fccada3fc530ea98d612126399f13ecb0844fc21</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fccada3fc530ea98d612126399f13ecb0844fc21" target="_blank">Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> ReCon is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student.</p>
                <p><strong>Paper Abstract:</strong> Mainstream 3D representation learning approaches are built upon contrastive or generative modeling pretext tasks, where great improvements in performance on various downstream tasks have been achieved. However, we find these two paradigms have different characteristics: (i) contrastive models are data-hungry that suffer from a representation over-fitting issue; (ii) generative models have a data filling issue that shows inferior data scaling capacity compared to contrastive models. This motivates us to learn 3D representations by sharing the merits of both paradigms, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose Contrast with Reconstruct (ReCon) that unifies these two paradigms. ReCon is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student. An encoder-decoder style ReCon-block is proposed that transfers knowledge through cross attention with stop-gradient, which avoids pretraining over-fitting and pattern difference issues. ReCon achieves a new state-of-the-art in 3D representation learning, e.g., 91.26% accuracy on ScanObjectNN. Codes have been released at https://github.com/qizekun/ReCon.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e526.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e526.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RECON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrast with Reconstruct (RECON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D representation learning framework that uses masked generative modeling (local reconstruction) to guide cross-modal/global contrastive learning (global semantics) via an encoder-decoder RECON-block with cross-attention and stop-gradient.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RECON 3D Transformer (encoder-decoder with RECON-block)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>43.6M</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Plain Transformer backbone (12-layer RECON-blocks) that performs dense masked point reconstruction in the encoder and sparse query-based global contrastive decoding; uses cross-attention connections (with stop-gradient) to transfer local reconstruction-oriented embeddings as guidance for global contrastive learning. Supports single-modal (point clouds) and cross-modal (point, image, text) pretraining; image and text teachers are optionally frozen.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>3D object representation pretraining + transfer (zero-shot & finetune classification, part segmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Pretraining on ShapeNet with reconstruction-guided contrastive objectives; downstream transfers include zero-shot 3D classification (ModelNet10/40, ScanObjectNN), few-shot, full fine-tuning classification, and part segmentation (ShapeNetPart). Zero-shot aligns text prompts to learned global embeddings for classification without supervised finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object recognition / zero-shot classification (not an embodied planning benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (local geometric structure encoded alongside global object-level semantics via text/image alignment)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Self-supervised pretraining on ShapeNet point clouds plus optional frozen pretrained 2D image teacher (ImageNet ViT) and frozen CLIP text encoder; generative masked point modeling provides local spatial cues, contrastive distillation provides global semantic alignment</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>pretraining with ensemble distillation (masked generative loss + single- or cross-modal contrastive loss), zero-shot via text prompt embedding cosine similarity; contrastive learning from frozen teachers (stop-grad)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Local point-token embeddings (encoder) encode spatial/geometric relations; a small set of learned global query embeddings (decoder) encode whole-object semantics and are aligned to image/text teacher embeddings; ensemble distillation implicitly stores knowledge in model weights and query embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Overall accuracy (%) on ScanObjectNN (PB_T50_RS), ModelNet40 Top-1 accuracy (%), zero-shot Top-1 accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Fine-tuned: ScanObjectNN PB_T50_RS = 90.63% (RECON w/ frozen teachers), ModelNet40 (full fine-tune) = 94.1%; Zero-shot ModelNet40 = 60.6% (single prompt) and 61.7% (ensemble prompts); Zero-shot ModelNet10 = 74.2% (single) and 75.6% (ensemble).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Learned decoupled local and global representations: local tokens capture fine-grained spatial geometry (local neighborhoods, symmetry cues) while global queries capture whole-object semantics and align with language/image embeddings enabling zero-shot recognition; generative guidance prevents contrastive overfitting on limited 3D data and stabilizes contrastive loss.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Contrastive-only pretraining overfits under limited 3D data; naive multi-tasking (shared encoder) suffers pattern conflict and poor transfer; removing stop-gradient in cross-attention collapses performance (global contrastive gradients degrade generative guidance); CLIP-as-2D teacher sometimes reduces diversity and hurts contrastive benefit; unpaired multimodal data degrades zero-shot by several percent.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Point-MAE (generative only): ScanObjectNN 88.42%, ModelNet40 ~93.5%; ACT (cross-modal autoencoder): ScanObjectNN 89.01%; Vanilla multi-task: ScanObjectNN 82.53%; Two-tower: ScanObjectNN 85.05%. RECON outperforms these baselines (e.g., RECON 90.63% vs Point-MAE 88.42% on ScanObjectNN).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Key ablations: removing reconstruction guidance (no REC) reduces transfer accuracy substantially; stop-gradient in cross-attention critical (without stop-grad ScanObjectNN drops to 81.60% from 90.63%); pretraining targets ablation showed best performance when both reconstruction and cross-modal contrastive (image+text) are used; freezing cross-modal teachers + Smooth L1 positive-only loss gave highest accuracy (see Table 17); contrastive pretraining learning rate sensitivity: high LR leads to poor generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Generative masked modeling encodes local spatial/geometric knowledge that can be used as reliable semantic guidance for contrastive global learning; guiding contrastive learning with reconstruction-oriented features via cross-attention (with stop-gradient) disentangles differing learning patterns (local vs global) and prevents contrastive overfitting on low-data 3D domains; frozen text/image teachers provide object-relational (semantic) anchors enabling zero-shot classification by aligning global queries to language embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e526.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e526.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-text</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP text encoder (from CLIP: Learning transferable visual models from natural language supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained language-image encoder whose text tower maps free-form text (prompts, labels) into a semantic embedding space used as a frozen teacher to align 3D global embeddings for zero-shot classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CLIP text encoder</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pretrained multimodal encoder (text tower) trained contrastively with images; here used frozen as a cross-modal teacher providing text embeddings (prompt-based) as positive targets for the 3D global queries during RECON cross-modal contrastive distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Zero-shot 3D object classification (via text prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>At inference, text prompts (prefix + category description + suffix) are encoded by CLIP text encoder to produce label embeddings; model computes cosine similarity between pretrained RECON global features and text embeddings to perform zero-shot classification without task-specific finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>zero-shot classification / semantic alignment</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (category semantics, label descriptions) + some scene semantics via prompt context</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Pretraining of CLIP on large image-text corpora (external to this work); used as frozen teacher during RECON pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>frozen teacher distillation (stop-grad); zero-shot classification via prompt encoding and cosine similarity retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Text embeddings in CLIP embedding space; RECON learns to align global query embeddings to these text embeddings so semantics are represented as vectors in a shared multimodal space.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot Top-1 accuracy (%) on ModelNet and ScanObjectNN</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Zero-shot: ModelNet40 = 60.6% (single prompt), 61.7% (ensemble); ModelNet10 = 74.2% (single), 75.6% (ensemble); ScanObjectNN PB_T50_RS zero-shot = 29.5% (single), 30.5% (ensemble) as reported for RECON using these text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Text embeddings provide strong object-relational semantic anchors enabling RECON global queries to support zero-shot recognition across synthetic and real-world 3D datasets; prompt engineering and ensembling of prompts improves zero-shot accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When images/text are pre-aligned (e.g., CLIP teacher), contrastive diversity sometimes decreases and can degrade some transfer performance vs using an ImageNet ViT teacher; unpaired or shuffled image-text pairing during pretraining reduces zero-shot accuracy (paired→unpaired induces ~6.4% drop in zero-shot ModelNet40).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to earlier point-based zero-shot pipelines (PointCLIP, CLIP2Point), RECON+CLIP-text achieves markedly higher zero-shot accuracy (e.g., ModelNet40: RECON 61.7% vs CLIP2Point 49.4% reported in Table 3 with ensemble).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Prompt ablations show variations in zero-shot accuracy by prompt choice; freezing the CLIP text encoder during distillation (stop-grad) is important — unfrozen teachers with Smooth L1 degrade performance (Table 17).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Frozen CLIP text encoder provides object-level semantic signals that RECON can align to, enabling zero-shot 3D classification; language supervision thereby supplies object-relational knowledge without requiring paired large 3D datasets and can be used when the text encoder itself has no direct sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e526.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e526.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Global queries</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned global [IMG] and [TXT] query embeddings (RECON decoder queries)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small set of learned decoder query tokens (one per modality view) that attend to reconstruction-oriented encoder outputs via cross-attention (stop-grad) and are supervised by global contrastive objectives to align with frozen image/text teacher embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RECON global query mechanism (learned [IMG], [TXT] queries)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Sparse decoder queries added to the contrastive decoder; they cross-attend to encoder local token embeddings (stop-gradient) producing global embeddings that are trained to match frozen image/text teacher embeddings via Smooth L1 distillation or InfoNCE.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-modal alignment for zero-shot & fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Global queries serve as the learned whole-object semantic representation used for cross-modal contrastive alignment with image/text teachers during pretraining and later used (alone or fused) for zero-shot classification or as part of the finetuning representation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>representation alignment / zero-shot classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial + object-relational (global semantic summaries built from local spatial features)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Guided by local encoder outputs (masked reconstruction) and frozen multimodal teachers (image/text) during pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>learned query tokens cross-attend to encoder outputs (stop-grad) during pretraining; supervised by contrastive distillation to teacher embeddings; fused for downstream inference</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Dense-to-sparse summarization: local spatial features (encoder tokens) are aggregated into global vector query embeddings representing whole-object semantics and aligned to language/image embedding spaces</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Used as part of RECON overall accuracy metrics (fine-tune and zero-shot), and analyzed via attention visualizations</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Using fused pretrained global [IMG] and [TXT] queries yields improved zero-shot/finetune performance (see RECON numbers): e.g., zero-shot ModelNet40 60.6% (single) and 61.7% (ensemble); including global queries in finetuning contributes to reported 94.1% on ModelNet40.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Global queries capture complementary semantic knowledge from image and text teachers and, when fused, improve zero-shot classification; attention maps show global queries attend to full object parts while local tokens focus on neighborhoods.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>If cross-attention gradients are not cut (no stop-grad), global contrastive gradients can corrupt local reconstruction learning and harm both global and local representations; naive multitask sharing without disentanglement underperforms.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Two-tower or vanilla multi-task approaches that do not use reconstruction-guided global queries underperform (Two-Tower 85.05% ScanObjectNN, Vanilla multi-task 82.53% vs RECON 90.63%).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Removing stop-gradient on CA connection (allowing gradient flow from global decoder to local encoder) causes significant performance drop; decoder depth, masking ratio ablated (Fig.5) show decoder depth and masking ratio consistent with Point-MAE optimum and matter for generative guidance quality.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A small set of learned global queries can effectively summarize local spatial structure (via guided cross-attention) and be aligned to language/image embedding spaces to encode object-relational semantics; enforcing stop-gradient prevents task conflict between local reconstruction and global contrastive objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e526.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e526.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPM / Point-MAE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Masked Point Modeling (MPM) / Point-MAE (generative masked autoencoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The generative masked modeling stream (reconstruction-oriented student) that reconstructs masked point subsets (Chamfer-L2 loss), producing local, geometry-focused embeddings used as guidance for the contrastive stream.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Masked autoencoders for point cloud self-supervised learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Point-MAE (MPM) generative student</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Masked autoencoder variant for point clouds that reconstructs masked points using an encoder-decoder; used here as the generative student whose encoder outputs are passed (via stop-grad cross-attention) to guide the contrastive decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Local spatial/geometric representation learning (masked reconstruction)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Dense masked point reconstruction encourages learning of local geometric associations, symmetry, and part relationships; encoder features serve as local spatial knowledge for the contrastive decoder queries.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial representation learning</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (local geometry, part structure) and some object-relational via reconstruction of occluded parts</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Self-supervised masked reconstruction on ShapeNet point clouds</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>masked point modeling with Chamfer-L2 reconstruction loss; encoder outputs are cross-attended (stop-grad) by contrastive decoder queries</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Dense local token embeddings encoding neighborhood geometry and structural cues; reconstructed output (3D point coordinates) used to train encoder</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream transfer accuracy (classification & segmentation), pretraining contrastive test loss behaviour</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Generative-only baseline (Point-MAE) fine-tuned: ScanObjectNN 88.42%, ModelNet40 ~93.5%; using MPM as guidance in RECON improves to RECON results (e.g., ScanObjectNN 90.63%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>MPM induces focused local attention, captures local geometry and symmetry (e.g., one wing attends to the symmetric counterpart), and provides stable features that prevent contrastive shortcuts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>MPM generative stream alone has limited data-scaling compared to contrastive methods when pretraining data is large (data-filling issue); by itself it yields lower asymptotic gains vs combined RECON.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Point-MAE (generative only) vs RECON (generative+contrastive): RECON improves ScanObjectNN by ~+2.2% (88.42→90.63%) and ModelNet40 by ~+0.6-0.7% depending on config.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Combining MPM with naive multi-task contrastive leads to task conflict and limited gains; using MPM as teacher within RECON-block with stop-grad yields consistent improvement; masking ratio and decoder depth ablations (Fig.5) follow Point-MAE optimal settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Masked point reconstruction encodes robust local spatial knowledge (neighborhood structure, symmetry) that, when used as a frozen guidance signal, regularizes contrastive training and prevents over-fitting to trivial global shortcuts in low-data 3D settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e526.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e526.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CMC (cross-modal contrast)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-Modal Contrastive Learning (CMC) with frozen 2D image teacher and CLIP text teacher</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A positive-only cross-modal contrastive distillation where 3D global embeddings are trained to match frozen image and text teacher embeddings using Smooth L1 loss or InfoNCE; used to inject object-level semantics from language/image into 3D representations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RECON contrastive decoder (CMC)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Contrastive decoder takes a small set of learned global queries (decoder) and supervises them to match frozen image/text teacher embeddings via Smooth L1 (positive-only) or InfoNCE losses; teachers (ImageNet ViT or CLIP) are typically frozen (stop-grad) during distillation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Cross-modal semantic alignment for zero-shot & improved finetune</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Align global 3D embeddings with image and language embeddings to transfer semantic knowledge and enable zero-shot classification by computing embedding similarities to text prompts at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>semantic alignment / zero-shot classification</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (category semantics) + spatial (indirect via guided local features)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>Pretrained 2D image teacher (ImageNet ViT) and CLIP text teacher; RECON pretraining on ShapeNet point clouds</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>frozen teacher distillation (stop-grad) using Smooth L1 positive-only loss or InfoNCE; global queries supervised by teacher embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>Vector embeddings in a joint multimodal space representing semantic labels and object-level relations, stored implicitly in decoder/query weights</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Downstream overall accuracy (%) and zero-shot Top-1 accuracy (%)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>CMC-only (with frozen teachers) baseline: ScanObjectNN ~82.48% when trained from scratch with default hyperparams; when integrated in RECON (with reconstruction guidance) achieves ScanObjectNN 90.63% and ModelNet40 94.1%.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>CMC supplies semantic anchors (text/image) that, when combined with reconstruction guidance, produce robust global embeddings enabling zero-shot transfer; Smooth L1 positive-only loss worked best under frozen teacher setup.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>CMC without reconstruction guidance is prone to overfitting on low-data 3D pretraining and leads to poor downstream generalization; using CLIP teacher alone can reduce feature diversity and hurt contrastive gains in some configs.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>CMC alone (from-scratch) gives weaker transfer vs RECON: e.g., CMC-only 82.48% ScanObjectNN vs RECON+CMC 90.63%; Point-MAE generative yields 88.42% showing that CMC must be combined with reconstruction guidance for best results.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Contrastive metric ablation: Smooth L1 > InfoNCE > L2 > Cosine for RECON frozen-teacher positive-only training (Smooth L1 gave best ScanObjectNN 90.63%, ModelNet40 94.1%). Freezing teachers critical for positive-only setups (unfrozen teachers with Smooth L1 degraded performance). Pretraining LR sensitive for contrastive-only setups.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cross-modal contrastive distillation from frozen image/text teachers injects object-relational semantics into 3D global embeddings, but on limited 3D data it must be regularized by reconstruction-guided local features and stop-gradient connections; positive-only Smooth L1 distillation with frozen teachers yields stable, high-performing alignment for zero-shot and finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Masked autoencoders for point cloud self-supervised learning <em>(Rating: 2)</em></li>
                <li>Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? <em>(Rating: 2)</em></li>
                <li>Point-M2AE: Multi-scale masked autoencoders for hierarchical point cloud pre-training <em>(Rating: 1)</em></li>
                <li>PointCLIP: Point cloud understanding by CLIP <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-526",
    "paper_id": "paper-fccada3fc530ea98d612126399f13ecb0844fc21",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "RECON",
            "name_full": "Contrast with Reconstruct (RECON)",
            "brief_description": "A 3D representation learning framework that uses masked generative modeling (local reconstruction) to guide cross-modal/global contrastive learning (global semantics) via an encoder-decoder RECON-block with cross-attention and stop-gradient.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RECON 3D Transformer (encoder-decoder with RECON-block)",
            "model_size": "43.6M",
            "model_description": "Plain Transformer backbone (12-layer RECON-blocks) that performs dense masked point reconstruction in the encoder and sparse query-based global contrastive decoding; uses cross-attention connections (with stop-gradient) to transfer local reconstruction-oriented embeddings as guidance for global contrastive learning. Supports single-modal (point clouds) and cross-modal (point, image, text) pretraining; image and text teachers are optionally frozen.",
            "task_name": "3D object representation pretraining + transfer (zero-shot & finetune classification, part segmentation)",
            "task_description": "Pretraining on ShapeNet with reconstruction-guided contrastive objectives; downstream transfers include zero-shot 3D classification (ModelNet10/40, ScanObjectNN), few-shot, full fine-tuning classification, and part segmentation (ShapeNetPart). Zero-shot aligns text prompts to learned global embeddings for classification without supervised finetuning.",
            "task_type": "object recognition / zero-shot classification (not an embodied planning benchmark)",
            "knowledge_type": "spatial + object-relational (local geometric structure encoded alongside global object-level semantics via text/image alignment)",
            "knowledge_source": "Self-supervised pretraining on ShapeNet point clouds plus optional frozen pretrained 2D image teacher (ImageNet ViT) and frozen CLIP text encoder; generative masked point modeling provides local spatial cues, contrastive distillation provides global semantic alignment",
            "has_direct_sensory_input": true,
            "elicitation_method": "pretraining with ensemble distillation (masked generative loss + single- or cross-modal contrastive loss), zero-shot via text prompt embedding cosine similarity; contrastive learning from frozen teachers (stop-grad)",
            "knowledge_representation": "Local point-token embeddings (encoder) encode spatial/geometric relations; a small set of learned global query embeddings (decoder) encode whole-object semantics and are aligned to image/text teacher embeddings; ensemble distillation implicitly stores knowledge in model weights and query embeddings",
            "performance_metric": "Overall accuracy (%) on ScanObjectNN (PB_T50_RS), ModelNet40 Top-1 accuracy (%), zero-shot Top-1 accuracy (%)",
            "performance_result": "Fine-tuned: ScanObjectNN PB_T50_RS = 90.63% (RECON w/ frozen teachers), ModelNet40 (full fine-tune) = 94.1%; Zero-shot ModelNet40 = 60.6% (single prompt) and 61.7% (ensemble prompts); Zero-shot ModelNet10 = 74.2% (single) and 75.6% (ensemble).",
            "success_patterns": "Learned decoupled local and global representations: local tokens capture fine-grained spatial geometry (local neighborhoods, symmetry cues) while global queries capture whole-object semantics and align with language/image embeddings enabling zero-shot recognition; generative guidance prevents contrastive overfitting on limited 3D data and stabilizes contrastive loss.",
            "failure_patterns": "Contrastive-only pretraining overfits under limited 3D data; naive multi-tasking (shared encoder) suffers pattern conflict and poor transfer; removing stop-gradient in cross-attention collapses performance (global contrastive gradients degrade generative guidance); CLIP-as-2D teacher sometimes reduces diversity and hurts contrastive benefit; unpaired multimodal data degrades zero-shot by several percent.",
            "baseline_comparison": "Point-MAE (generative only): ScanObjectNN 88.42%, ModelNet40 ~93.5%; ACT (cross-modal autoencoder): ScanObjectNN 89.01%; Vanilla multi-task: ScanObjectNN 82.53%; Two-tower: ScanObjectNN 85.05%. RECON outperforms these baselines (e.g., RECON 90.63% vs Point-MAE 88.42% on ScanObjectNN).",
            "ablation_results": "Key ablations: removing reconstruction guidance (no REC) reduces transfer accuracy substantially; stop-gradient in cross-attention critical (without stop-grad ScanObjectNN drops to 81.60% from 90.63%); pretraining targets ablation showed best performance when both reconstruction and cross-modal contrastive (image+text) are used; freezing cross-modal teachers + Smooth L1 positive-only loss gave highest accuracy (see Table 17); contrastive pretraining learning rate sensitivity: high LR leads to poor generalization.",
            "key_findings": "Generative masked modeling encodes local spatial/geometric knowledge that can be used as reliable semantic guidance for contrastive global learning; guiding contrastive learning with reconstruction-oriented features via cross-attention (with stop-gradient) disentangles differing learning patterns (local vs global) and prevents contrastive overfitting on low-data 3D domains; frozen text/image teachers provide object-relational (semantic) anchors enabling zero-shot classification by aligning global queries to language embeddings.",
            "uuid": "e526.0",
            "source_info": {
                "paper_title": "Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "CLIP-text",
            "name_full": "CLIP text encoder (from CLIP: Learning transferable visual models from natural language supervision)",
            "brief_description": "A pretrained language-image encoder whose text tower maps free-form text (prompts, labels) into a semantic embedding space used as a frozen teacher to align 3D global embeddings for zero-shot classification.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_name": "CLIP text encoder",
            "model_size": null,
            "model_description": "Pretrained multimodal encoder (text tower) trained contrastively with images; here used frozen as a cross-modal teacher providing text embeddings (prompt-based) as positive targets for the 3D global queries during RECON cross-modal contrastive distillation.",
            "task_name": "Zero-shot 3D object classification (via text prompts)",
            "task_description": "At inference, text prompts (prefix + category description + suffix) are encoded by CLIP text encoder to produce label embeddings; model computes cosine similarity between pretrained RECON global features and text embeddings to perform zero-shot classification without task-specific finetuning.",
            "task_type": "zero-shot classification / semantic alignment",
            "knowledge_type": "object-relational (category semantics, label descriptions) + some scene semantics via prompt context",
            "knowledge_source": "Pretraining of CLIP on large image-text corpora (external to this work); used as frozen teacher during RECON pretraining",
            "has_direct_sensory_input": false,
            "elicitation_method": "frozen teacher distillation (stop-grad); zero-shot classification via prompt encoding and cosine similarity retrieval",
            "knowledge_representation": "Text embeddings in CLIP embedding space; RECON learns to align global query embeddings to these text embeddings so semantics are represented as vectors in a shared multimodal space.",
            "performance_metric": "Zero-shot Top-1 accuracy (%) on ModelNet and ScanObjectNN",
            "performance_result": "Zero-shot: ModelNet40 = 60.6% (single prompt), 61.7% (ensemble); ModelNet10 = 74.2% (single), 75.6% (ensemble); ScanObjectNN PB_T50_RS zero-shot = 29.5% (single), 30.5% (ensemble) as reported for RECON using these text embeddings.",
            "success_patterns": "Text embeddings provide strong object-relational semantic anchors enabling RECON global queries to support zero-shot recognition across synthetic and real-world 3D datasets; prompt engineering and ensembling of prompts improves zero-shot accuracy.",
            "failure_patterns": "When images/text are pre-aligned (e.g., CLIP teacher), contrastive diversity sometimes decreases and can degrade some transfer performance vs using an ImageNet ViT teacher; unpaired or shuffled image-text pairing during pretraining reduces zero-shot accuracy (paired→unpaired induces ~6.4% drop in zero-shot ModelNet40).",
            "baseline_comparison": "Compared to earlier point-based zero-shot pipelines (PointCLIP, CLIP2Point), RECON+CLIP-text achieves markedly higher zero-shot accuracy (e.g., ModelNet40: RECON 61.7% vs CLIP2Point 49.4% reported in Table 3 with ensemble).",
            "ablation_results": "Prompt ablations show variations in zero-shot accuracy by prompt choice; freezing the CLIP text encoder during distillation (stop-grad) is important — unfrozen teachers with Smooth L1 degrade performance (Table 17).",
            "key_findings": "Frozen CLIP text encoder provides object-level semantic signals that RECON can align to, enabling zero-shot 3D classification; language supervision thereby supplies object-relational knowledge without requiring paired large 3D datasets and can be used when the text encoder itself has no direct sensory input.",
            "uuid": "e526.1",
            "source_info": {
                "paper_title": "Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "Global queries",
            "name_full": "Learned global [IMG] and [TXT] query embeddings (RECON decoder queries)",
            "brief_description": "A small set of learned decoder query tokens (one per modality view) that attend to reconstruction-oriented encoder outputs via cross-attention (stop-grad) and are supervised by global contrastive objectives to align with frozen image/text teacher embeddings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RECON global query mechanism (learned [IMG], [TXT] queries)",
            "model_size": null,
            "model_description": "Sparse decoder queries added to the contrastive decoder; they cross-attend to encoder local token embeddings (stop-gradient) producing global embeddings that are trained to match frozen image/text teacher embeddings via Smooth L1 distillation or InfoNCE.",
            "task_name": "Cross-modal alignment for zero-shot & fine-tuning",
            "task_description": "Global queries serve as the learned whole-object semantic representation used for cross-modal contrastive alignment with image/text teachers during pretraining and later used (alone or fused) for zero-shot classification or as part of the finetuning representation.",
            "task_type": "representation alignment / zero-shot classification",
            "knowledge_type": "spatial + object-relational (global semantic summaries built from local spatial features)",
            "knowledge_source": "Guided by local encoder outputs (masked reconstruction) and frozen multimodal teachers (image/text) during pretraining",
            "has_direct_sensory_input": null,
            "elicitation_method": "learned query tokens cross-attend to encoder outputs (stop-grad) during pretraining; supervised by contrastive distillation to teacher embeddings; fused for downstream inference",
            "knowledge_representation": "Dense-to-sparse summarization: local spatial features (encoder tokens) are aggregated into global vector query embeddings representing whole-object semantics and aligned to language/image embedding spaces",
            "performance_metric": "Used as part of RECON overall accuracy metrics (fine-tune and zero-shot), and analyzed via attention visualizations",
            "performance_result": "Using fused pretrained global [IMG] and [TXT] queries yields improved zero-shot/finetune performance (see RECON numbers): e.g., zero-shot ModelNet40 60.6% (single) and 61.7% (ensemble); including global queries in finetuning contributes to reported 94.1% on ModelNet40.",
            "success_patterns": "Global queries capture complementary semantic knowledge from image and text teachers and, when fused, improve zero-shot classification; attention maps show global queries attend to full object parts while local tokens focus on neighborhoods.",
            "failure_patterns": "If cross-attention gradients are not cut (no stop-grad), global contrastive gradients can corrupt local reconstruction learning and harm both global and local representations; naive multitask sharing without disentanglement underperforms.",
            "baseline_comparison": "Two-tower or vanilla multi-task approaches that do not use reconstruction-guided global queries underperform (Two-Tower 85.05% ScanObjectNN, Vanilla multi-task 82.53% vs RECON 90.63%).",
            "ablation_results": "Removing stop-gradient on CA connection (allowing gradient flow from global decoder to local encoder) causes significant performance drop; decoder depth, masking ratio ablated (Fig.5) show decoder depth and masking ratio consistent with Point-MAE optimum and matter for generative guidance quality.",
            "key_findings": "A small set of learned global queries can effectively summarize local spatial structure (via guided cross-attention) and be aligned to language/image embedding spaces to encode object-relational semantics; enforcing stop-gradient prevents task conflict between local reconstruction and global contrastive objectives.",
            "uuid": "e526.2",
            "source_info": {
                "paper_title": "Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "MPM / Point-MAE",
            "name_full": "Masked Point Modeling (MPM) / Point-MAE (generative masked autoencoder)",
            "brief_description": "The generative masked modeling stream (reconstruction-oriented student) that reconstructs masked point subsets (Chamfer-L2 loss), producing local, geometry-focused embeddings used as guidance for the contrastive stream.",
            "citation_title": "Masked autoencoders for point cloud self-supervised learning",
            "mention_or_use": "use",
            "model_name": "Point-MAE (MPM) generative student",
            "model_size": null,
            "model_description": "Masked autoencoder variant for point clouds that reconstructs masked points using an encoder-decoder; used here as the generative student whose encoder outputs are passed (via stop-grad cross-attention) to guide the contrastive decoder.",
            "task_name": "Local spatial/geometric representation learning (masked reconstruction)",
            "task_description": "Dense masked point reconstruction encourages learning of local geometric associations, symmetry, and part relationships; encoder features serve as local spatial knowledge for the contrastive decoder queries.",
            "task_type": "spatial representation learning",
            "knowledge_type": "spatial (local geometry, part structure) and some object-relational via reconstruction of occluded parts",
            "knowledge_source": "Self-supervised masked reconstruction on ShapeNet point clouds",
            "has_direct_sensory_input": true,
            "elicitation_method": "masked point modeling with Chamfer-L2 reconstruction loss; encoder outputs are cross-attended (stop-grad) by contrastive decoder queries",
            "knowledge_representation": "Dense local token embeddings encoding neighborhood geometry and structural cues; reconstructed output (3D point coordinates) used to train encoder",
            "performance_metric": "Downstream transfer accuracy (classification & segmentation), pretraining contrastive test loss behaviour",
            "performance_result": "Generative-only baseline (Point-MAE) fine-tuned: ScanObjectNN 88.42%, ModelNet40 ~93.5%; using MPM as guidance in RECON improves to RECON results (e.g., ScanObjectNN 90.63%).",
            "success_patterns": "MPM induces focused local attention, captures local geometry and symmetry (e.g., one wing attends to the symmetric counterpart), and provides stable features that prevent contrastive shortcuts.",
            "failure_patterns": "MPM generative stream alone has limited data-scaling compared to contrastive methods when pretraining data is large (data-filling issue); by itself it yields lower asymptotic gains vs combined RECON.",
            "baseline_comparison": "Point-MAE (generative only) vs RECON (generative+contrastive): RECON improves ScanObjectNN by ~+2.2% (88.42→90.63%) and ModelNet40 by ~+0.6-0.7% depending on config.",
            "ablation_results": "Combining MPM with naive multi-task contrastive leads to task conflict and limited gains; using MPM as teacher within RECON-block with stop-grad yields consistent improvement; masking ratio and decoder depth ablations (Fig.5) follow Point-MAE optimal settings.",
            "key_findings": "Masked point reconstruction encodes robust local spatial knowledge (neighborhood structure, symmetry) that, when used as a frozen guidance signal, regularizes contrastive training and prevents over-fitting to trivial global shortcuts in low-data 3D settings.",
            "uuid": "e526.3",
            "source_info": {
                "paper_title": "Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining",
                "publication_date_yy_mm": "2023-02"
            }
        },
        {
            "name_short": "CMC (cross-modal contrast)",
            "name_full": "Cross-Modal Contrastive Learning (CMC) with frozen 2D image teacher and CLIP text teacher",
            "brief_description": "A positive-only cross-modal contrastive distillation where 3D global embeddings are trained to match frozen image and text teacher embeddings using Smooth L1 loss or InfoNCE; used to inject object-level semantics from language/image into 3D representations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "RECON contrastive decoder (CMC)",
            "model_size": null,
            "model_description": "Contrastive decoder takes a small set of learned global queries (decoder) and supervises them to match frozen image/text teacher embeddings via Smooth L1 (positive-only) or InfoNCE losses; teachers (ImageNet ViT or CLIP) are typically frozen (stop-grad) during distillation.",
            "task_name": "Cross-modal semantic alignment for zero-shot & improved finetune",
            "task_description": "Align global 3D embeddings with image and language embeddings to transfer semantic knowledge and enable zero-shot classification by computing embedding similarities to text prompts at inference.",
            "task_type": "semantic alignment / zero-shot classification",
            "knowledge_type": "object-relational (category semantics) + spatial (indirect via guided local features)",
            "knowledge_source": "Pretrained 2D image teacher (ImageNet ViT) and CLIP text teacher; RECON pretraining on ShapeNet point clouds",
            "has_direct_sensory_input": false,
            "elicitation_method": "frozen teacher distillation (stop-grad) using Smooth L1 positive-only loss or InfoNCE; global queries supervised by teacher embeddings",
            "knowledge_representation": "Vector embeddings in a joint multimodal space representing semantic labels and object-level relations, stored implicitly in decoder/query weights",
            "performance_metric": "Downstream overall accuracy (%) and zero-shot Top-1 accuracy (%)",
            "performance_result": "CMC-only (with frozen teachers) baseline: ScanObjectNN ~82.48% when trained from scratch with default hyperparams; when integrated in RECON (with reconstruction guidance) achieves ScanObjectNN 90.63% and ModelNet40 94.1%.",
            "success_patterns": "CMC supplies semantic anchors (text/image) that, when combined with reconstruction guidance, produce robust global embeddings enabling zero-shot transfer; Smooth L1 positive-only loss worked best under frozen teacher setup.",
            "failure_patterns": "CMC without reconstruction guidance is prone to overfitting on low-data 3D pretraining and leads to poor downstream generalization; using CLIP teacher alone can reduce feature diversity and hurt contrastive gains in some configs.",
            "baseline_comparison": "CMC alone (from-scratch) gives weaker transfer vs RECON: e.g., CMC-only 82.48% ScanObjectNN vs RECON+CMC 90.63%; Point-MAE generative yields 88.42% showing that CMC must be combined with reconstruction guidance for best results.",
            "ablation_results": "Contrastive metric ablation: Smooth L1 &gt; InfoNCE &gt; L2 &gt; Cosine for RECON frozen-teacher positive-only training (Smooth L1 gave best ScanObjectNN 90.63%, ModelNet40 94.1%). Freezing teachers critical for positive-only setups (unfrozen teachers with Smooth L1 degraded performance). Pretraining LR sensitive for contrastive-only setups.",
            "key_findings": "Cross-modal contrastive distillation from frozen image/text teachers injects object-relational semantics into 3D global embeddings, but on limited 3D data it must be regularized by reconstruction-guided local features and stop-gradient connections; positive-only Smooth L1 distillation with frozen teachers yields stable, high-performing alignment for zero-shot and finetuning.",
            "uuid": "e526.4",
            "source_info": {
                "paper_title": "Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Masked autoencoders for point cloud self-supervised learning",
            "rating": 2
        },
        {
            "paper_title": "Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning?",
            "rating": 2
        },
        {
            "paper_title": "Point-M2AE: Multi-scale masked autoencoders for hierarchical point cloud pre-training",
            "rating": 1
        },
        {
            "paper_title": "PointCLIP: Point cloud understanding by CLIP",
            "rating": 1
        }
    ],
    "cost": 0.0213865,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining</h1>
<p>Zekun Qi ${ }^{\dagger 1}$ Runpei Dong ${ }^{\dagger 1 \text { A }}$ Guofan Fan ${ }^{1}$ Zheng Ge ${ }^{2}$ Xiangyu Zhang ${ }^{2}$ Kaisheng Ma ${ }^{3}$ Li Yi ${ }^{345}$</p>
<h4>Abstract</h4>
<p>Mainstream 3D representation learning approaches are built upon contrastive or generative modeling pretext tasks, where great improvements in performance on various downstream tasks have been achieved. However, we find these two paradigms have different characteristics: (i) contrastive models are data-hungry that suffer from a representation over-fitting issue; (ii) generative models have a data filling issue that shows inferior data scaling capacity compared to contrastive models. This motivates us to learn 3D representations by sharing the merits of both paradigms, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose Contrast with Reconstruct (RECON) that unifies these two paradigms. RECON is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student. An encoder-decoder style RECON-block is proposed that transfers knowledge through cross attention with stop-gradient, which avoids pretraining over-fitting and pattern difference issues. RECON achieves a new state-of-the-art in 3D representation learning, e.g., $\mathbf{9 1 . 2 6 \%}$ accuracy on ScanObjectNN. Codes have been released at https://github.com/qizekun/ReCon.</p>
<h2>1. Introduction</h2>
<p>Self-supervised representation learning (SSRL) has witnessed a booming era of foundational models (Bommasani</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Data efficiency comparison and attention distance visualization. (a) Fine-tuning overall accuracy on ScanObjectNN PB_T50_RS with models pretrained with different methods on ShapeNet of different data ratios. (b) Averaged attention distance of models pretrained on ShapeNet with generative masked point modeling (MPM) (Pang et al., 2022) and cross-modal contrastive (CMC) modeling (Radford et al., 2021) (texts, images, and point clouds are used), respectively. SUP: supervised classification pretraining on ShapeNet. SMC: single-modal contrastive pretraining (Khosla et al., 2020). RECON-SMC and RECONCMC represent our proposed RECON with single-modal and crossmodal contrastive modeling variants, respectively. MPM+SMC and MPM+CMC represent vanilla multi-task learning.
et al., 2021), significant advancements are being made in natural language processing (NLP) (Radford et al., 2018; Devlin et al., 2019; Brown et al., 2020; Wei et al., 2022b; Ouyang et al., 2022), 2D machine vision (He et al., 2020; 2022), and both (vision-language, VL) (Radford et al., 2021; Rombach et al., 2022; Alayrac et al., 2022). While this great course toward foundational machine intelligence is trending, the success of these methods generally demands training on data of extreme size. However, compared to 2D vision and NLP, 3D vision is faced with a challenging data desert issue (Dong et al., 2023) due to collection difficulty.</p>
<p>Though under this low-data regime, numerous 3D SSRL methods have been developed, which can be grouped into two categories, i.e., contrastive (single/cross-modal) (Xie et al., 2020; Zhang et al., 2021; Chen et al., 2022; Liu et al., 2021a; Afham et al., 2022) and generative (reconstruct/predict) (Wang et al., 2021; Yu et al., 2022b; Pang et al., 2022) methods. However, we investigate the pretraining efficiency of the two paradigms by scaling the pretraining data of ShapeNet ranging from $1 \%$ to $100 \%$, and we find that these two paradigms have their issues (see Fig. 1(a)):
Figure 1. Data efficiency comparison and attention distance visualization. (a) Fine-tuning overall accuracy on ScanObjectNN PB_T50_RS with models pretrained with different methods on ShapeNet of different data ratios. (b) Averaged attention distance of models pretrained on ShapeNet with generative masked point modeling (MPM) (Pang et al., 2022) and cross-modal contrastive (CMC) modeling (Radford et al., 2021) (texts, images, and point clouds are used), respectively. SUP: supervised classification pretraining on ShapeNet. SMC: single-modal contrastive pretraining (Khosla et al., 2020). RECON-SMC and RECONCMC represent our proposed RECON with single-modal and crossmodal contrastive modeling variants, respectively. MPM+SMC and MPM+CMC represent vanilla multi-task learning.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Concept comparison of contrastive, generative, and our RECON paradigms. Here, we illustrate the methods in a unified view of knowledge distillation (see Sec. 3.1). (a) Contrastive students are trained to learn invariance from the teacher. (b) Generative masked modeling encourages students to reconstruct clean signals provided by the teacher. (c) RECon unifies the two paradigms by learning from multi-teachers, where the generative local student is also a "teacher" that guides the contrastive global student.</p>
<ul>
<li>Representation over-fitting (contrastive). Contrastive models fail to bring generalization when the pretraining data is lacking $(&lt;90 \%)$, while generative models bring significant improvements with only $\sim 25 \%$ data. It shows that contrastive models can easily find shortcuts with trivial representations that over-fit the limited data (He et al., 2020), and generative models are less data-hungry that learn decent initialization with very few data (Kong \&amp; Zhang, 2023).</li>
<li>Data filling (generative). Contrastive models present a better potential with scaled-up data, while generative models only provide a little improvement. It shows that contrastive learning may bring superior data-scaling capacity when the pretraining data is sufficient. This is observed in 2D where contrastive models surpass generative models (Dong et al., 2022) that have less scaling capability (Xie et al., 2022b).
These observations motivate the design that shares both merits without mentioned issues. However, as shown in Fig. 1(a), simply combining these two paradigms as multitask learning leads to unsatisfactory results - lower than the generative model baseline and the representation over-fitting issue remains. To understand why, we visualize the averaged attention distance ${ }^{1}$ of the contrastive and generative models in Fig. 1(b). A pattern difference issue is observed that the attention of contrastive models is mainly paid to a global field, while generative models have an appetite for focused local attention, which is consistent with the observations by Xie et al. (2022a) in 2D. We conject that this pattern difference issue causes a task conflict in the naive multi-task representation learning setting, indicating it is non-trivial to combine the merits of contrastive and generative modeling.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>To address the above-mentioned issues, we propose Contrast with Reconstruct (RECON) that trains generative modeling as guidance for contrastive learning while sharing both merits. As shown in Fig. 2, from the perspective of knowledge distillation (Sec. 3.1), contrastive and generative methods can be viewed as vanilla student-teacher paradigms (Fig. 2(a-b)). In contrast, our RECON unifies the two paradigms as ensemble distillation from multi-teachers, while the generative student is also a "teacher" which guides the contrastive learning (student-teacher knowledge distillation with student-student assistance).</p>
<p>In particular, inspired by Vaswani et al. (2017), a novel encoder-decoder style RECon-block is proposed where cross attention with stop-gradient is used to transfer guidance from reconstruction to contrastive modeling. In this fashion, the knowledge from multi-teachers is disentangled and reinforced, which addresses the representation over-fitting issue and avoids the learned pattern difference. Further, our RECON utilizes single-modal or cross-modal contrastive learning of 3D point clouds, 2D RGB images, and languages that significantly enlarge the pretraining data diversity and capacity. Meanwhile, the data-filling issue of the generative student is alleviated due to the promising scaling capacity of the contrastive student. Fig. 1(b) shows that our RECON learns 3D representations with high generalization capacity. By transferring the learned representations to various benchmarks, a new state-of-the-art in self-supervised 3D learning is achieved. For example, an average improvement of $\mathbf{+ 9 . 2 \%}$ and $\mathbf{+ 2 . 9 \%}$ accuracy are achieved on ScanObjectNN and ModelNet40, respectively. These results show that RECON learns foundational geometric understanding, and this simple and general framework successfully unifies contrastive and generative modeling.</p>
<h2>2. Related Works</h2>
<p>Contrastive Representation Learning is one of the mainstream self-supervised learning paradigms (Hadsell et al., 2006), which learns potential semantics from constructed invariance or equivaiance (Dangovski et al., 2022). Generally, Instance Discrimination (Wu et al., 2018) is widely adopted to align and distinguish representations of views with the same high-level semantics or not. The views could be constructed by augmentations to single-modal (Chen et al., 2020; He et al., 2020; Chen et al., 2021) or multimodal data (Radford et al., 2021; Li et al., 2022a). Most works use global features for processing. For example, SimCLR (Chen et al., 2020) uses samples with different augmentation policies to construct positive and negative pairs. CLIP (Radford et al., 2021) proposes a two-tower network that aligns the global representation of languages and images. Driven by InforMAX principle (Hjelm et al., 2019), they generally use InfoNCE (van den Oord et al., 2018) as the loss function to maximize mutual information. In 3D, PointContrast (Xie et al., 2020) proposes geometric augmentation to generate positive and negative pairs. CrossPoint (Afham et al., 2022) uses both inter and intra-modal contrastive learning. PointCLIP (Zhang et al., 2022c) realizes image-point alignment by projecting point clouds to 2D depth images. In this work, we focus on single/cross-modal contrastive learning by discriminative contrast (Khosla et al., 2020) or global feature alignment like Radford et al. (2021), which is guided by masked generative modeling.</p>
<p>Generative Masked Representation Learning has emerged as another paradigm of self-supervised learning from NLP (Devlin et al., 2019) to Vision (He et al., 2022). It requires the models to learn structured knowledge by reconstructing masked input data, which encourages the association of different local patches. In NLP, it has been a dominant approach to probing knowledge by recovering or predicting words in sentences (Devlin et al., 2019; Brown et al., 2020). With the rapid development of Transformers in vision (Dosovitskiy et al., 2021; Liu et al., 2021b), abundant works have been proposed to realize mask image modeling (MIM). He et al. (2022) propose masked autoencoder (MAE) to reconstruct RGB pixels. Bao et al. (2022) reconstructs the VQVAE (Ramesh et al., 2021) codebook with encoded semantics. Some works propose to reconstruct online teacher tokens (Zhou et al., 2022) or HOG features (Wei et al., 2022a). In 3D, PointMAE (Pang et al., 2022) extends MAE (He et al., 2022) by reconstructing masked point clouds. PointM2AE (Zhang et al., 2022b) uses a hierarchical Transformer and designs the corresponding masking strategy. MaskPoint (Liu et al., 2022) proposes to add some noise points and classify whether they belong to masking tokens. Recently, ACT (Dong et al., 2023) uses a cross-modal autoencoder as the reconstruction target to acquire dark knowledge from other modalities.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Illustration of the proposed RECon-block for pretraining. The local reconstruction task is used to train the encoder, while the global contrastive task is used to train the decoder guided by reconstruction-oriented embeddings with cross attention (CA). Stop-gradient (stop-grad) is applied to every CA connection to avoid misleading gradient flow from global to local.</p>
<h2>3. RECON: Contrast with Reconstruct</h2>
<p>We begin with a review in a unified view of knowledge distillation for the two mainstream representation learning methods: masked generative modeling and contrastive modeling. We then introduce RECON that unifies these two representation learning methods by reconstruction-guided contrastive learning using an encoder-decoder style RECONblock based architecture, where the overall representation learning is formulated as distillation with both teachers with student-student assistance.</p>
<h3>3.1. Knowledge Distillation: A Unified View of Generative and Contrastive Learning</h3>
<p>Contrastive Modeling The key insight of contrastive learning lies in invariance learning (Kosmann-Schwarzbach, 2011; He et al., 2020; Kong \&amp; Zhang, 2023), where the abstraction of semantics is generally invariant or equivariant (Dangovski et al., 2022) to multiple transformed views like augmentations (Chen et al., 2020) or modalities (Tian et al., 2020a). From the perspective of knowledge distillation (Hinton et al., 2015), it can be viewed as a student network learning the invariance knowledge transferred from the encoded views of the teacher. Formally, given input data $x \sim \mathcal{D}$ with distribution $\mathcal{D}$, the student network is $\mathcal{F}<em _phi="\phi">{\theta}^{S}(\cdot)$ with parameters $\theta$ and $\mathcal{F}</em>(\cdot)$ is the teacher network with parameters $\phi$. The optimization target can be written:}^{T</p>
<p>$$
\begin{aligned}
&amp; \min <em i="i">{\theta} \underset{\left(\mathcal{T}</em>}, \mathcal{T<em _mathrm_C="\mathrm{C">{j}\right) \in \mathcal{T}}{\mathbb{E}} \mathcal{L}</em>\right) \
&amp; \quad z_{i}=\mathcal{F}}}^{\mathrm{CON}}\left(z_{i}, z_{j<em i="i">{\theta}^{S}\left(\mathcal{T}</em>}(x)\right), z_{j}=\mathcal{F<em j="j">{\phi}^{T}\left(\mathcal{T}</em>(x)\right)
\end{aligned}
$$</p>
<p>where</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Overview of RECON. RECON can be applied to either single-modal 3D point clouds inputs or cross-modal inputs with rendered RGB images and text descriptions, which will be encoded as sequential tokens. The 3D token embeddings are then masked for generative reconstruction for the local 3D encoder, where the encoded intermediate embeddings are fed to the global 3D decoder with stop-gradient (stop-grad) through cross-attention. The global queries are learnable and supervised by global contrastive learning.</p>
<ul>
<li>The teacher parameters $\phi$ can be the same as $\theta$ with (Chen \&amp; He, 2021) or without (Chen et al., 2020) stop-gradient, $\phi$ can also be the momentum counterpart updated as $\phi \leftarrow m \phi+(1-m) \theta$ where $m \in(0,1]$ is the momentum coefficient (He et al., 2020; Chen et al., 2021). For multi-modal inputs ${ }^{2}, \phi$ can be different from $\theta$ that encodes views from other modalities (Tian et al., 2020a; Radford et al., 2021; Jing et al., 2021).</li>
<li>$\left{\mathcal{T}<em j="j">{i}(\cdot), \mathcal{T}</em>$ invariance targets, respectively. In general, this transformation generates copied views by data augmentations (Caron et al., 2021; Wang \&amp; Qi, 2021), we extend it to multi-modal inputs where the transformation becomes generating views from different modalities that share the same high-order concept (Radford et al., 2021; Liu et al., 2021a).}(\cdot)\right} \in \mathcal{T}$ are two transformations of the input data that belongs to constructed transformation pairs $\mathcal{T}$, where $\mathcal{T}_{j}(\cdot)$ constructs the positive or negative ${ }^{3</li>
<li>$\mathcal{L}<em 2="2">{\mathbb{C}}^{\mathrm{CON}}(\cdot, \cdot)$ is the distance function defined in some metric space $\mathbb{C}$. To avoid representation collapsing (He et al., 2020), InfoMax-Principle (Hjelm et al., 2019; Bachman et al., 2019) based metrics like MINE (Belghazi et al., 2018), InfoNCE (van den Oord et al., 2018) are often used (He et al., 2020; Tian et al., 2020c; Zhang et al., 2022a). For methods using positive-only transformations, the metric function can be feature correlation measurement (Zbontar et al., 2021), $\ell</em>$ distance (Ermolov et al., 2021), or cosine similarity (Grill et al., 2020; Chen \&amp; He, 2021).</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Generative Masked Modeling Similarly, given input data $x \sim \mathcal{D}$ with distribution $\mathcal{D}$, the student network as $\mathcal{F}<em _phi="\phi">{\theta}^{S}(\cdot)$ with parameters $\theta$ and $\mathcal{F}</em>$ with parameters $\phi$. generative masked modeling can be formulated as follows:}^{T}(\cdot)$ as the teacher network ${ }^{4</p>
<p>$$
\begin{aligned}
&amp; \min <em i="i">{\theta} \underset{\left{\mathcal{M}</em>}, \widetilde{\mathcal{M}<em _mathbb_D="\mathbb{D">{i}\right} \in \mathcal{M}}{\mathbb{E}} \mathcal{L}</em>\right) \
&amp; \quad z_{i}=\mathcal{F}}}^{\mathrm{REC}}\left(z_{i}, z_{j<em i="i">{\theta}^{S}\left(\mathcal{M}</em>}(x)\right), z_{j}=\mathcal{F<em i="i">{\phi}^{T}\left(\widetilde{\mathcal{M}}</em>(x)\right)
\end{aligned}
$$</p>
<p>where</p>
<ul>
<li>$\mathcal{L}<em 2="2">{\mathbb{D}}^{\mathrm{REC}}$ is a distance function defined in some metric space $\mathbb{D}$. It can be $\ell</em>$ distance (He et al., 2022), crossentropy (Devlin et al., 2019; Bao et al., 2022), or Chamfer-Distance (Fan et al., 2017; Pang et al., 2022).</li>
<li>$\left{\mathcal{M}<em i="i">{i}, \widetilde{\mathcal{M}}</em>}\right} \in \mathcal{M}$ are the masking corruptions where $\mathcal{M<em i="i">{i}(\cdot)$ samples a subset of the input tokens and performs masking (Devlin et al., 2019), and $\widetilde{\mathcal{M}}</em>(\cdot)$ correspondingly samples the subset while without masking.</li>
</ul>
<p>Ensemble Representation Distillation Ensemble distillation ${ }^{5}$ is known to be more informative and instructive (Tian et al., 2020b; Park \&amp; Kwak, 2020; Xiang et al., 2020; Wang \&amp; Yoon, 2022), which encourages the student network to learn ensembled and disentangled knowledge representation. We motivate our method as an ensembled knowledge distillation of Eq. (1) and Eq. (2), where the student is trained with merits from both contrastive and generative aspects. The overall loss $\mathcal{L}^{\text {RECON }}$ is defined as follows:</p>
<p>$$
\mathcal{L}^{\mathrm{RECon}}=\mathcal{L}^{\mathrm{REC}}+\mathcal{L}^{\mathrm{CON}}
$$</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>Table 1. Classification results on the ScanObjectNN and ModelNet40 datasets. The inference model parameters #P (M), FLOPS #F (G), and overall accuracy (\%) are reported. The dagger ( ${ }^{\dagger}$ ) denotes that the model was reproduced using our proposed $\star$ RECON-block and the fine-tuning techniques used in RECON. We compare with methods using the $\circ$ hierarchical Transformer architectures (e.g., Point-M2AE (Zhang et al., 2022b)), $\star$ plain Transformer architectures, and $\circ$ dedicated architectures for 3D understanding. PT: pretrained teacher is used, MD: multi-modal data is used. Single-Modal means that only point clouds are used as pre-training data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">#P</th>
<th style="text-align: center;">#F</th>
<th style="text-align: center;">PT</th>
<th style="text-align: center;">MD</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ModelNet40</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">OBJ_BG</td>
<td style="text-align: center;">OBJ_ONLY</td>
<td style="text-align: center;">PB_T50_RS</td>
<td style="text-align: center;">1k P</td>
<td style="text-align: center;">8k P</td>
</tr>
<tr>
<td style="text-align: center;">Supervised Learning Only</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ PointNet (Qi et al., 2017a)</td>
<td style="text-align: center;">3.5</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">73.3</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">68.0</td>
<td style="text-align: center;">89.2</td>
<td style="text-align: center;">90.8</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ PointNet++ (Qi et al., 2017b)</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">84.3</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">90.7</td>
<td style="text-align: center;">91.9</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ DGCNN (Wang et al., 2019)</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">86.2</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">92.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ PointCNN (Li et al., 2018)</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">86.1</td>
<td style="text-align: center;">85.5</td>
<td style="text-align: center;">78.5</td>
<td style="text-align: center;">92.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ SimpleView (Goyal et al., 2021)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$80.5 \pm 0.3$</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ MVTN (Hamdi et al., 2021)</td>
<td style="text-align: center;">11.2</td>
<td style="text-align: center;">43.7</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">92.6</td>
<td style="text-align: center;">92.3</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ PCT (Guo et al., 2021)</td>
<td style="text-align: center;">2.88</td>
<td style="text-align: center;">2.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ PointMLP (Ma et al., 2022)</td>
<td style="text-align: center;">12.6</td>
<td style="text-align: center;">31.4</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$85.4 \pm 0.3$</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ PointNeXt (Qian et al., 2022)</td>
<td style="text-align: center;">1.4</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$87.7 \pm 0.4$</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ P2P-HorNet (Wang et al., 2022)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.6</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">89.3</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">with Single-Modal Self-Supervised Representation Learning (FULL)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\star$ Transformer (Vaswani et al., 2017)</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">83.04</td>
<td style="text-align: center;">84.06</td>
<td style="text-align: center;">79.11</td>
<td style="text-align: center;">91.4</td>
<td style="text-align: center;">91.8</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ Transformer ${ }^{\dagger}$ (Vaswani et al., 2017)</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">84.90</td>
<td style="text-align: center;">86.12</td>
<td style="text-align: center;">81.64</td>
<td style="text-align: center;">91.6</td>
<td style="text-align: center;">92.0</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ Point-BERT (Yu et al., 2022b)</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">87.43</td>
<td style="text-align: center;">88.12</td>
<td style="text-align: center;">83.07</td>
<td style="text-align: center;">93.2</td>
<td style="text-align: center;">93.8</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ Point-MAE (Pang et al., 2022)</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">90.02</td>
<td style="text-align: center;">88.29</td>
<td style="text-align: center;">85.18</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">94.0</td>
</tr>
<tr>
<td style="text-align: center;">$\circ$ Point-M2AE (Zhang et al., 2022b)</td>
<td style="text-align: center;">15.3</td>
<td style="text-align: center;">3.6</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">91.22</td>
<td style="text-align: center;">88.81</td>
<td style="text-align: center;">86.43</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ Point-MAE ${ }^{\dagger}$ (Pang et al., 2022)</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">92.60</td>
<td style="text-align: center;">91.91</td>
<td style="text-align: center;">88.42</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">94.0</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON w/o vot.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">94.15</td>
<td style="text-align: center;">93.12</td>
<td style="text-align: center;">89.73</td>
<td style="text-align: center;">93.6</td>
<td style="text-align: center;">93.8</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON w/ vot.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">94.49</td>
<td style="text-align: center;">93.29</td>
<td style="text-align: center;">90.35</td>
<td style="text-align: center;">93.9</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: center;">with Cross-Modal Self-Supervised Representation Learning (FULL)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\star$ ACT (Dong et al., 2023)</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">93.29</td>
<td style="text-align: center;">91.91</td>
<td style="text-align: center;">88.21</td>
<td style="text-align: center;">93.7</td>
<td style="text-align: center;">94.0</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON-Tiny w/o vot.</td>
<td style="text-align: center;">11.4</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">93.80</td>
<td style="text-align: center;">92.94</td>
<td style="text-align: center;">89.10</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">93.6</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON-Small w/o vot.</td>
<td style="text-align: center;">19.0</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">94.15</td>
<td style="text-align: center;">93.12</td>
<td style="text-align: center;">89.52</td>
<td style="text-align: center;">93.5</td>
<td style="text-align: center;">93.8</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON w/o vot.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">94.66</td>
<td style="text-align: center;">93.29</td>
<td style="text-align: center;">90.32</td>
<td style="text-align: center;">94.0</td>
<td style="text-align: center;">94.2</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON w/o vot.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">95.18</td>
<td style="text-align: center;">93.63</td>
<td style="text-align: center;">90.63</td>
<td style="text-align: center;">94.1</td>
<td style="text-align: center;">94.3</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON w/ vot.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">95.35</td>
<td style="text-align: center;">93.80</td>
<td style="text-align: center;">91.26</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">94.7</td>
</tr>
<tr>
<td style="text-align: center;">with Self-Supervised Representation Learning (MLP-LINEAR)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\star$ Point-MAE ${ }^{\dagger}$ (Pang et al., 2022)</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$82.77 \pm 0.30$</td>
<td style="text-align: center;">$83.23 \pm 0.16$</td>
<td style="text-align: center;">$74.13 \pm 0.21$</td>
<td style="text-align: center;">$90.22 \pm 0.09$</td>
<td style="text-align: center;">$90.73 \pm 0.09$</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ ACT (Dong et al., 2023)</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$85.20 \pm 0.83$</td>
<td style="text-align: center;">$85.84 \pm 0.15$</td>
<td style="text-align: center;">$76.31 \pm 0.26$</td>
<td style="text-align: center;">$91.36 \pm 0.17$</td>
<td style="text-align: center;">$91.75 \pm 0.18$</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON w/o vot.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$89.50 \pm 0.20$</td>
<td style="text-align: center;">$89.72 \pm 0.17$</td>
<td style="text-align: center;">$81.36 \pm 0.14$</td>
<td style="text-align: center;">$92.47 \pm 0.22$</td>
<td style="text-align: center;">$92.68 \pm 0.07$</td>
</tr>
<tr>
<td style="text-align: center;">with Self-Supervised Representation Learning (MLP-3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\star$ Point-MAE ${ }^{\dagger}$ (Pang et al., 2022)</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$85.78 \pm 0.31$</td>
<td style="text-align: center;">$85.51 \pm 0.16$</td>
<td style="text-align: center;">$80.38 \pm 0.21$</td>
<td style="text-align: center;">$91.25 \pm 0.24$</td>
<td style="text-align: center;">$91.68 \pm 0.19$</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ ACT (Dong et al., 2023)</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$87.14 \pm 0.22$</td>
<td style="text-align: center;">$87.90 \pm 0.40$</td>
<td style="text-align: center;">$81.52 \pm 0.19$</td>
<td style="text-align: center;">$92.69 \pm 0.18$</td>
<td style="text-align: center;">$92.95 \pm 0.10$</td>
</tr>
<tr>
<td style="text-align: center;">$\star$ RECON w/o vot.</td>
<td style="text-align: center;">43.6</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$90.62 \pm 0.22$</td>
<td style="text-align: center;">$90.71 \pm 0.30$</td>
<td style="text-align: center;">$83.80 \pm 0.42$</td>
<td style="text-align: center;">$93.00 \pm 0.10$</td>
<td style="text-align: center;">$93.39 \pm 0.05$</td>
</tr>
</tbody>
</table>
<h3>3.2. Reconstruction Guided Contrastive Learning</h3>
<p>Network Architecture As discussed in Sec. 1, since the two distillation result in different learning patterns, it is non-trivial to learn from both targets jointly. To tackle this issue, we propose contrast with reconstruct (RECON). The reconstruction-oriented representations that focus on local patterns are used as semantic guidance for global contrastive learning. Inspired by the Transformer encoder-decoder architecture (Vaswani et al., 2017), we conduct dense masked modeling with an encoder, which produces features to guide global contrastive learning through a sparse query-based decoder. The encoder and decoder share the same Transformer
architecture, and they are layer-wisely associated with cross attention (CA). Due to limited 3D data, the contrastive model can easily learn trivial representations as shortcuts, and this could lead to noisy training signals, which may harm generative student learning. Hence, to avoid the task conflicts between these two students, we use stop-gradient for every CA connection to cut the misleading training signal from global contrast to local reconstruction. We call the proposed network architecture RECON-block, which is illustrated in Fig. 3. In this fashion, the ensemble "studentteacher" distillation from multi-teacher is learned jointly with "student-student" assistance where the contrastive stu-</p>
<p>Table 2. Few-shot classification results on ModelNet40. ${ }^{1}$ represent results of our proposed $*$ RECon-block built backbone architecture. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">5-way</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">10-way</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10-shot</td>
<td style="text-align: center;">20-shot</td>
<td style="text-align: center;">10-shot</td>
<td style="text-align: center;">20-shot</td>
</tr>
<tr>
<td style="text-align: center;">- DGCNN</td>
<td style="text-align: center;">$31.6 \pm 2.8$</td>
<td style="text-align: center;">$40.8 \pm 4.6$</td>
<td style="text-align: center;">$19.9 \pm 2.1$</td>
<td style="text-align: center;">$16.9 \pm 1.5$</td>
</tr>
<tr>
<td style="text-align: center;">- OcCo</td>
<td style="text-align: center;">$90.6 \pm 2.8$</td>
<td style="text-align: center;">$92.5 \pm 1.9$</td>
<td style="text-align: center;">$82.9 \pm 1.3$</td>
<td style="text-align: center;">$86.5 \pm 2.2$</td>
</tr>
<tr>
<td style="text-align: center;">with Self-Supervised Representation Learning (Full)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">* Transformer</td>
<td style="text-align: center;">$87.8 \pm 5.2$</td>
<td style="text-align: center;">$93.3 \pm 4.3$</td>
<td style="text-align: center;">$84.6 \pm 5.5$</td>
<td style="text-align: center;">$89.4 \pm 6.3$</td>
</tr>
<tr>
<td style="text-align: center;">* Transformer ${ }^{\dagger}$</td>
<td style="text-align: center;">$90.2 \pm 5.9$</td>
<td style="text-align: center;">$94.3 \pm 4.4$</td>
<td style="text-align: center;">$85.2 \pm 5.9$</td>
<td style="text-align: center;">$89.9 \pm 6.1$</td>
</tr>
<tr>
<td style="text-align: center;">- OcCo</td>
<td style="text-align: center;">$94.0 \pm 3.6$</td>
<td style="text-align: center;">$95.9 \pm 2.3$</td>
<td style="text-align: center;">$89.4 \pm 5.1$</td>
<td style="text-align: center;">$92.4 \pm 4.6$</td>
</tr>
<tr>
<td style="text-align: center;">* Point-BERT</td>
<td style="text-align: center;">$94.6 \pm 3.1$</td>
<td style="text-align: center;">$96.3 \pm 2.7$</td>
<td style="text-align: center;">$91.0 \pm 5.4$</td>
<td style="text-align: center;">$92.7 \pm 5.1$</td>
</tr>
<tr>
<td style="text-align: center;">* MaskPoint</td>
<td style="text-align: center;">$95.0 \pm 3.7$</td>
<td style="text-align: center;">$97.2 \pm 1.7$</td>
<td style="text-align: center;">$91.4 \pm 4.0$</td>
<td style="text-align: center;">$93.4 \pm 3.5$</td>
</tr>
<tr>
<td style="text-align: center;">* Point-MAE</td>
<td style="text-align: center;">$96.3 \pm 2.5$</td>
<td style="text-align: center;">$97.8 \pm 1.8$</td>
<td style="text-align: center;">$92.6 \pm 4.1$</td>
<td style="text-align: center;">$95.0 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: center;">* Point-M2AE</td>
<td style="text-align: center;">$96.8 \pm 1.8$</td>
<td style="text-align: center;">$98.3 \pm 1.4$</td>
<td style="text-align: center;">$92.3 \pm 4.5$</td>
<td style="text-align: center;">$95.0 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: center;">* Point-MAE ${ }^{\dagger}$</td>
<td style="text-align: center;">$96.4 \pm 2.8$</td>
<td style="text-align: center;">$97.8 \pm 2.0$</td>
<td style="text-align: center;">$92.5 \pm 4.4$</td>
<td style="text-align: center;">$95.2 \pm 3.9$</td>
</tr>
<tr>
<td style="text-align: center;">* ACT</td>
<td style="text-align: center;">$96.8 \pm 2.3$</td>
<td style="text-align: center;">$98.0 \pm 1.4$</td>
<td style="text-align: center;">$93.3 \pm 4.0$</td>
<td style="text-align: center;">$95.6 \pm 2.8$</td>
</tr>
<tr>
<td style="text-align: center;">* RECon</td>
<td style="text-align: center;">$97.3 \pm 1.9$</td>
<td style="text-align: center;">$98.9 \pm 1.2$</td>
<td style="text-align: center;">$93.3 \pm 3.9$</td>
<td style="text-align: center;">$95.8 \pm 3.0$</td>
</tr>
<tr>
<td style="text-align: center;">with Self-Supervised Representation Learning (MLP-LinEAR)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">* Point-MAE ${ }^{\dagger}$</td>
<td style="text-align: center;">$91.1 \pm 5.6$</td>
<td style="text-align: center;">$91.7 \pm 4.0$</td>
<td style="text-align: center;">$83.5 \pm 6.1$</td>
<td style="text-align: center;">$89.7 \pm 4.1$</td>
</tr>
<tr>
<td style="text-align: center;">* ACT</td>
<td style="text-align: center;">$91.8 \pm 4.7$</td>
<td style="text-align: center;">$93.1 \pm 4.2$</td>
<td style="text-align: center;">$84.5 \pm 6.4$</td>
<td style="text-align: center;">$90.7 \pm 4.3$</td>
</tr>
<tr>
<td style="text-align: center;">* RECon</td>
<td style="text-align: center;">$96.9 \pm 2.6$</td>
<td style="text-align: center;">$98.2 \pm 1.4$</td>
<td style="text-align: center;">$93.6 \pm 4.7$</td>
<td style="text-align: center;">$95.4 \pm 2.6$</td>
</tr>
<tr>
<td style="text-align: center;">with Self-Supervised Representation Learning (MLP-3)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">* Point-MAE ${ }^{\dagger}$</td>
<td style="text-align: center;">$95.0 \pm 2.8$</td>
<td style="text-align: center;">$96.7 \pm 2.4$</td>
<td style="text-align: center;">$90.6 \pm 4.7$</td>
<td style="text-align: center;">$93.8 \pm 5.0$</td>
</tr>
<tr>
<td style="text-align: center;">* ACT</td>
<td style="text-align: center;">$95.9 \pm 2.2$</td>
<td style="text-align: center;">$97.7 \pm 1.8$</td>
<td style="text-align: center;">$92.4 \pm 5.0$</td>
<td style="text-align: center;">$94.7 \pm 3.9$</td>
</tr>
<tr>
<td style="text-align: center;">* RECon</td>
<td style="text-align: center;">$97.4 \pm 2.2$</td>
<td style="text-align: center;">$98.5 \pm 1.4$</td>
<td style="text-align: center;">$93.6 \pm 4.7$</td>
<td style="text-align: center;">$95.7 \pm 2.7$</td>
</tr>
</tbody>
</table>
<p>dent is guided by the generative "teacher". As a result, the contrastive student is trained with a good data scaling capacity without the risk of representation over-fitting, and the pattern difference issue is avoided with no task conflicts.</p>
<p>Implementation We use the standard plain Transformer (Vaswani et al., 2017) built with 12 RECon-blocks with dimension 384 and a lightweight PointNet patch embedding (Qi et al., 2017a;b; Yu et al., 2022b) as the 3D representation learner. ShapeNet (Chang et al., 2015) is used to pretrain RECon, which contains $\sim 51 \mathrm{~K}$ unique 3D CAD models covering 55 object categories. We follow Khosla et al. (2020) for single-modal contrastive modeling. For cross-modal setting, we utilize point clouds, RGB images, and free-form languages, where the limited 3D data is enlarged with rich texture and semantic knowledge within images and languages (Afham et al., 2022; Dong et al., 2023). We obtain RGB images by rendering from 3D meshes and language descriptions by concatenating text prompts with category descriptions. We use by default an ImageNet (Deng et al., 2009) pretrained Vision Transformer (ViT-B) (Dosovitskiy et al., 2021) as the image view teacher, and we use the text encoder from CLIP (Radford et al., 2021) as the text view teacher. The image and text teacher encoders are frozen during pretraining, and Smooth $\ell_{1}$-based positive-only distillation (Chen \&amp; He, 2021; Ermolov et al., 2021) is used. The masked generative modeling follows Pang et al. (2022), where the reconstruction metric is $\ell_{2}$ Chamfer-Distance (Fan et al., 2017). Fig. 4 shows an overall illustration and more details can be found in Appendix B.</p>
<p>Table 3. Zero-shot 3D object classification domain transfer on ModelNet40 (MN-40) and ModelNet10 (MN-10). Top-1 accuracy (\%) is reported. Ensemb. denotes whether to use the ensemble strategy with multiple text inputs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Backbone</th>
<th style="text-align: left;">Ensemb.</th>
<th style="text-align: center;">MN-10</th>
<th style="text-align: center;">MN-40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">- PointCLIP (Zhang et al., 2022c)</td>
<td style="text-align: left;">ResNet-50</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: center;">30.2</td>
<td style="text-align: center;">20.2</td>
</tr>
<tr>
<td style="text-align: left;">* CLIP2Point (Huang et al., 2022)</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: center;">66.6</td>
<td style="text-align: center;">49.4</td>
</tr>
<tr>
<td style="text-align: left;">* RECon</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: center;">$\mathbf{7 4 . 2}$</td>
<td style="text-align: center;">$\mathbf{6 0 . 6}$</td>
</tr>
<tr>
<td style="text-align: left;">* RECon</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{7 5 . 6}$</td>
<td style="text-align: center;">$\mathbf{6 1 . 7}$</td>
</tr>
</tbody>
</table>
<h2>4. Experiments</h2>
<h3>4.1. Transfer Learning on Downstream Tasks</h3>
<p>Transfer Protocol We use the same classification heads and transfer learning protocols following Dong et al. (2023): Full, MLP-LinEAR, and MLP-3.</p>
<p>3D Real-World Object Recognition ScanObjectNN (Uy et al., 2019) is one of the most challenging 3D datasets, which covers $\sim 15 \mathrm{~K}$ real-world objects from 15 categories. For a fair comparison, we report the results with and without the voting strategy (Liu et al., 2019) separately. Note that we only use simple Rotation as data augmentation in training following Dong et al. (2023). We report single-modal (RECon-SMC) and cross-modal (RECon-CMC, default if not otherwise specified) results on three model variants (see Appendix B.) The results are shown in Table 1, it is observed that: (i) With a comparable GFLOPS, the performance of our RECon-block (from scratch) is improved by $2.5 \%$ compared with that of standard Transformer under Full tuning protocol. Further, after the pre-training of reconstruction guided contrastive learning, RECON gains a significant improvement of $+11.3 \%$ accuracy averaged on the three variant ScanObjectNN benchmarks. (ii) Compared to other selfsupervised learning (SSL) methods, our RECon achieves the best generalization across both single-modal and crossmodal on all transferring protocols. e.g., RECon outperforms Point-MAE by $+5.6 \%$ on three ScanObjectNN variants. (iii) Compared with any supervised or self-supervised method, our RECon achieves a new state-of-the-art that outperforms existing methods by a large margin.</p>
<p>3D Synthetic Object Recognition ModelNet (Wu et al., 2015) is one of the most classical datasets for synthetic 3D object recognition. It contains $\sim 12 \mathrm{~K}$ meshed 3D CAD objects of 40 (ModelNet40) or 10 (ModelNet10) categories. We conduct the evaluation on the ModelNet40 dataset, including fine-tuning and few-shot learning. We use Scale\&amp;Translate as data augmentation in training following Qi et al. (2017a;b). The results are shown in Table 1 and Table 2, respectively. It can be observed that our RECON achieves $94.7 \%$ classification accuracy of ModelNet40 under Full protocol, improved by $2.7 \%$ compared with the Transformer baseline. In the few-shot task, our RECon has also achieved the best performance under all protocols, especially under the MLP-3 and MLP-LINEAR protocols.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Ablation study of masking ratio, decoder depth, and the 2D teacher encoder choice used during RECON pretraining. The masking ratio and decoder depth represent the ablation for the generative masked modeling stream, and the 2D teachers are used for contrastive cross-modal learning.</p>
<p>Table 4. Ablation study on pretraining targets. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Reconstruction</th>
<th style="text-align: center;">Contrastive</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">ScanObjectNN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">text</td>
<td style="text-align: center;">image</td>
<td style="text-align: center;">self</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">85.60</td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">86.02</td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">85.57</td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">86.49</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">88.42</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">89.77</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">90.18</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">89.50</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">90.63</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">89.98</td>
</tr>
</tbody>
</table>
<p>3D Zero-Shot Recognition Similar to CLIP (Radford et al., 2021), our model aligns the feature space of languages and other modalities. Therefore, our model has a strong zeroshot capability. We use ModelNet (Wu et al., 2015) dataset to conduct zero-shot evaluation, including ModelNet10 and ModelNet40. The results are shown in Table 3. Following PointCLIP (Zhang et al., 2022c), We use prompt templates with the category label to generate text features. From Table 3, it can be seen that our RECON surpasses all the zero-shot methods with CNN-based or Transformer-based backbones. Further, by using ensemble methods such as multi-prompt templates (Huang et al., 2022), our RECON achieves a Top-1 accuracy of $61.7 \%$ on ModelNet40, which significantly outperforms PointCLIP and CLIP2Point by $41.5 \%$ and $12.3 \%$, respectively.</p>
<h3>4.2. Ablation Study</h3>
<p>Hyper Parameter In Fig. 5, We show the ablation study on masking ratio, decoder depth, and the selection of the 2D image teacher during RECON pretraining. It can be observed that the optimal masking ratio and decoder depth is consistent with Point-MAE (Pang et al., 2022), indicating that the pretraining model, which is friendly to downstream tasks, is also helpful for the guidance of contrastive learning. The results also show that ViT (Dosovitskiy et al., 2021), as a 2D teacher, is superior to CLIP (Radford et al., 2021), Swin-Transformer (Swin) (Liu et al., 2021b), ResNet (He</p>
<p>Table 5. Ablation study on the contrastive metric. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Contrastive Metric</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">InfoNCE</td>
<td style="text-align: center;">90.11</td>
<td style="text-align: center;">93.8</td>
</tr>
<tr>
<td style="text-align: left;">$\ell_{2}$ Distance</td>
<td style="text-align: center;">89.64</td>
<td style="text-align: center;">93.6</td>
</tr>
<tr>
<td style="text-align: left;">Smooth $\ell_{1}$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Cosine Similarity</td>
<td style="text-align: center;">90.17</td>
<td style="text-align: center;">93.8</td>
</tr>
</tbody>
</table>
<p>et al., 2016) and BEiT (Bao et al., 2022). In addition, we find that CLIP, which is already aligned with languages, brings inferior performance to ViT. This may be due to the reduction of diversity caused by the high similarity of features from the pre-aligned text teacher, which can be considered degenerating to only a single vision-language pretrained teacher.</p>
<p>Pretraining Targets To analyze the importance of the pretraining targets and verify the effect of reconstructionguided contrastive modeling, we conduct an ablation study on the pretraining target. The results are shown in Table 4. It can be seen that: (i) When reconstruction guidance is not used, the performance of the contrastive model is very poor due to over-fitting on the limited 3D data. (ii) The performance of single-modal contrastive learning is slightly weaker than that of cross-modal contrastive learning, and the improvements can not be shared. Besides, we find that both 2D and text teachers can help improve performance without contradictions, and 2D teachers bring better improvement in learned representations generalization.</p>
<p>Contrastive Metric Table 5 shows the ablation study on the contrastive metric. Smooth $\ell_{1}$ distance achieves the best results in both tasks and is higher than the commonly used InfoNCE (van den Oord et al., 2018). We argue that the reasons are two-fold: (i) The cross-modal positive-only contrastive learning with the frozen teachers (stop-grad) has no risk of representation collapsing (Chen \&amp; He, 2021), and it is not necessary to introduce negative samples. (ii) ShapeNet dataset is full of household objects with limited semantic diversity, unlike ImageNet, which makes the negative samples noisy and confusing. These hard negatives are generally not easy for mining and may bring unsatisfactory optimization challenges (Faghri et al., 2018).</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6. Pretraining contrastive loss on ShapeNet test split vs. finetuning accuracy (\%) on ScanObjectNN. The pretraining test loss (left) is plotted in orange, and the corresponding fine-tuning accuracy (right) is plotted in blue. LR: pretraining learning rate.</p>
<p>Table 6. Study of the stop-gradient operation in RECon-block. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Stop-grad</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">81.60</td>
<td style="text-align: center;">89.7</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 1}$</td>
</tr>
</tbody>
</table>
<h2>5. Discussions</h2>
<h3>5.1. What role does the reconstruction guidance play in contrastive learning?</h3>
<p>To analyze the reason why RECON works, we record the contrastive loss on the test split of ShapeNet (not used for pretraining) using cross-modal contrastive (CMC) and reconstruction guidance contrastive (RECON-CMC), along with the corresponding fine-tuning accuracy on ScanObjectNN. The results are shown in Figure 6. It can be seen that the test contrastive loss of our RECON-CMC is consistently lower than vanilla CMC, which converges to a lower value more stably ( 0.034 vs. 0.052 ), indicating that our RECON brings superior generalization performance of the pretraining contrastive task. As a result, RECON-CMC learns to contrast without falling into shortcuts of trivial solutions, and the over-fitting issue during pretraining is alleviated. The corresponding fine-tuning accuracy demonstrates this point, where a significantly superior generalization performance with better training efficiency of our RECON-CMC compared to vanilla CMC is achieved ( $90.63 \%$ vs. $82.48 \%$ ). Besides, we find that a lower learning rate improves vanilla CMC performance, demonstrating that contrastive models are prone to over-fit (see Appendix D.2).</p>
<h3>5.2. Can contrastive learning guide reconstruction?</h3>
<p>As discussed in Sec. 1, the learned patterns of contrastive and generative modeling are different, and we build RECON where the generative student guides the contrastive student. What if we use global contrastive learning to guide the generative masked modeling? To answer this question, we analyze the role of stop-gradient in RECON-block. The results are shown in Table 6. It can be seen that without stop-
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Attention distribution visualization of local tokens and global queries learned by RECON. We randomly select three local tokens, which are highlighted in red dashed circles.
gradient, the performance of RECON is seriously degraded ( $-9.03 \%$ on ScanObjectNN and $-4.4 \%$ on ModelNet40). We argue that the contrastive task can easily converge to a degenerated solution due to the limited 3D data, as discussed before, which in turn leads to noisy gradient and training signal to the generative modeling part during pretraining. As a result, the guidance itself is disturbed and harmed, while the model fails to learn representations with valid semantics.</p>
<h3>5.3. What is learned in RECON?</h3>
<p>According to the design, RECON should have learned both locally focused and global 3D geometric understandings. In Figure 7, we visualize the attention distribution of randomly selected local tokens and global queries. The red and yellow parts are the key areas of attention, while the blue and purple parts are the areas of less attention. It can be seen that the local tokens in 3D point clouds focus more on the geometric structure around the tokens themselves, while the global queries focus on the whole parts of the object. Interestingly, the local tokens may have learned some geometric understanding of symmetry. For example, the token on the left wing of the airplane also noticed the right wing. In addition, we find that global image and text queries may have learned some complementary knowledge.</p>
<h2>6. Conclusions</h2>
<p>In this paper, we propose contrast with reconstruct (RECON), which enjoys the merits of both generative masked modeling and contrastive modeling, while scalable to multimodal data to facilitate stronger 3D representation learning. Our results show high-capacity data efficiency and generalizations on both pretraining and downstream representation transferring. In particular, RECON achieves a new state-of-the-art on challenging real-world 3D object recognition. By diving deeply into RECON, we emphasize the importance of reconstruction, which avoids the contrastive over-fitting issue due to limited 3D data. Visualizations show that RECON indeed learns decoupled local and global representations in the proposed RECON-block. RECON is a simple framework, and we hope more RECON-style models to be produced in the multi-modal learning community.</p>
<h2>References</h2>
<p>Afham, M., Dissanayake, I., Dissanayake, D., Dharmasiri, A., Thilakarathna, K., and Rodrigo, R. Crosspoint: Self-supervised cross-modal contrastive learning for 3d point cloud understanding. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 1, 3, 6, 16, 17</p>
<p>Alayrac, J., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han, T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O., Zisserman, A., and Simonyan, K. Flamingo: a visual language model for few-shot learning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 1</p>
<p>Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2019. 4</p>
<p>Bao, H., Dong, L., Piao, S., and Wei, F. Beit: BERT pre-training of image transformers. In Int. Conf. Learn. Represent. (ICLR). OpenReview.net, 2022. 3, 4, 7</p>
<p>Belghazi, M. I., Baratin, A., Rajeswar, S., Ozair, S., Bengio, Y., Hjelm, R. D., and Courville, A. C. Mutual information neural estimation. In Proc. Int. Conf. Mach. Learn. (ICML), volume 80 of Proceedings of Machine Learning Research, pp. 530-539. PMLR, 2018. 4</p>
<p>Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N. D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M. S., Krishna, R., Kuditipudi, R., and et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021. 1</p>
<p>Boser, B. E., Guyon, I., and Vapnik, V. A training algorithm for optimal margin classifiers. In ACM Conf. Comput. Learn. Theory (COLT), pp. 144-152. ACM, 1992. 16</p>
<p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language models are few-shot learners. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2020. 1, 3</p>
<p>Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in selfsupervised vision transformers. In Int. Conf. Comput. Vis. (ICCV), pp. 9630-9640. IEEE, 2021. 4</p>
<p>Chang, A. X., Funkhouser, T. A., Guibas, L. J., Hanrahan, P., Huang, Q., Li, Z., Savarese, S., Savva, M., Song, S., Su, H., Xiao, J., Yi, L., and Yu, F. Shapenet: An information-rich 3d model repository. CoRR, abs/1512.03012, 2015. 6, 15, 17</p>
<p>Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. A simple framework for contrastive learning of visual representations. In Proc. Int. Conf. Mach. Learn. (ICML), volume 119 of Proceedings of Machine Learning Research, pp. 1597-1607. PMLR, 2020. 3, 4</p>
<p>Chen, X. and He, K. Exploring simple siamese representation learning. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 15750-15758, 2021. 4, 6, 7, 14</p>
<p>Chen, X., Xie, S., and He, K. An empirical study of training self-supervised vision transformers. In Int. Conf. Comput. Vis. (ICCV), pp. 9620-9629. IEEE, 2021. 3, 4</p>
<p>Chen, Y., Nießner, M., and Dai, A. 4dcontrast: Contrastive learning with dynamic correspondences for 3d scene understanding. In Eur. Conf. Comput. Vis. (ECCV), 2022. 1</p>
<p>Dangovski, R., Jing, L., Loh, C., Han, S., Srivastava, A., Cheung, B., Agrawal, P., and Soljacic, M. Equivariant self-supervised learning: Encouraging equivariance in representations. In Int. Conf. Learn. Represent. (ICLR). OpenReview.net, 2022. 3</p>
<p>Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2009. 6</p>
<p>Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: pretraining of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171-4186. Association for Computational Linguistics, 2019. 1, 3, 4</p>
<p>Dong, R., Qi, Z., Zhang, L., Zhang, J., Sun, J., Ge, Z., Yi, L., and Ma, K. Autoencoders as cross-modal teachers: Can pretrained 2d image transformers help 3d representation learning? In Int. Conf. Learn. Represent. (ICLR), 2023. 1, 3, 4, 5, 6, 15, 17, 20</p>
<p>Dong, X., Bao, J., Zhang, T., Chen, D., Gu, S., Zhang, W., Yuan, L., Chen, D., Wen, F., and Yu, N. CLIP itself is a strong finetuner: Achieving $85.7 \%$ and $88.0 \%$ top-1 accuracy with vit-b and vit-l on imagenet. CoRR, abs/2212.06138, 2022. 2</p>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent. (ICLR), 2021. 2, 3, 6, 7, 15</p>
<p>Ermolov, A., Siarohin, A., Sangineto, E., and Sebe, N. Whitening for self-supervised representation learning. In Proc. Int. Conf. Mach. Learn. (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 3015-3024. PMLR, 2021. 4, 6</p>
<p>Faghri, F., Fleet, D. J., Kiros, J. R., and Fidler, S. VSE++: improving visual-semantic embeddings with hard negatives. In Brit. Mach. Vis. Conf. (BMVC), pp. 12. BMVA Press, 2018. 7</p>
<p>Fan, H., Su, H., and Guibas, L. J. A point set generation network for 3d object reconstruction from a single image. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2017. 4, 6, 14</p>
<p>Goyal, A., Law, H., Liu, B., Newell, A., and Deng, J. Revisiting point cloud shape classification with a simple and effective baseline. In Proc. Int. Conf. Mach. Learn. (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 3809-3820. PMLR, 2021. 5</p>
<p>Goyal, P., Mahajan, D., Gupta, A., and Misra, I. Scaling and benchmarking self-supervised visual representation learning. In Int. Conf. Comput. Vis. (ICCV), pp. 6390-6399. IEEE, 2019. 16</p>
<p>Grill, J., Strub, F., Altché, F., Tallec, C., Richemond, P. H., Buchatskaya, E., Doersch, C., Pires, B. Á., Guo, Z., Azar, M. G., Piot, B., Kavukcuoglu, K., Munos, R., and Valko, M. Bootstrap your own latent - A new approach to self-supervised learning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2020. 4</p>
<p>Guo, M., Cai, J., Liu, Z., Mu, T., Martin, R. R., and Hu, S. PCT: point cloud transformer. Comput. Vis. Media, 7(2):187-199, 2021. 5</p>
<p>Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 1735-1742, 2006. 3</p>
<p>Hamdi, A., Giancola, S., and Ghanem, B. MVTN: multi-view transformation network for 3d shape recognition. In Int. Conf. Comput. Vis. (ICCV), pp. 1-11. IEEE, 2021. 5</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 770-778. IEEE Computer Society, 2016. 7</p>
<p>He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. Momentum contrast for unsupervised visual representation learning. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 97269735. Computer Vision Foundation / IEEE, 2020. 1, 2, 3, 4</p>
<p>He, K., Chen, X., Xie, S., Li, Y., Dollár, P., and Girshick, R. B. Masked autoencoders are scalable vision learners. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022. 1, 3, 4</p>
<p>Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. In Adv. Neural Inform. Process. Syst. (NeurIPS), volume abs/1503.02531, 2015. 3</p>
<p>Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization. In Int. Conf. Learn. Represent. (ICLR), 2019. 3, 4</p>
<p>Huang, T., Dong, B., Yang, Y., Huang, X., Lau, R. W. H., Ouyang, W., and Zuo, W. Clip2point: Transfer CLIP to point cloud classification with image-depth pre-training. CoRR, abs/2210.01055, 2022. 6, 7, 17</p>
<p>Jing, L., Vahdani, E., Tan, J., and Tian, Y. Cross-modal center loss for 3d cross-modal retrieval. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 3142-3151. Computer Vision Foundation / IEEE, 2021. 4</p>
<p>Khosla, P., Teterwak, P., Wang, C., Sarna, A., Tian, Y., Isola, P., Maschinot, A., Liu, C., and Krishnan, D. Supervised contrastive learning. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2020. $1,3,6,14$</p>
<p>Kong, X. and Zhang, X. Understanding masked image modeling via learning occlusion invariant feature. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2023. 2, 3</p>
<p>Kosmann-Schwarzbach, Y. The Noether Theorems, pp. 55-64. Springer New York, New York, NY, 2011. ISBN 978-0-387-87868-3. doi: 10.1007/978-0-387-87868-3_3. URL https: //doi.org/10.1007/978-0-387-87868-3_3. 3</p>
<p>Li, J., Selvaraju, R. R., Gotmare, A., Joty, S. R., Xiong, C., and Hoi, S. C. Align before fuse: Vision and language representation learning with momentum distillation. In Adv. Neural Inform. Process. Syst. (NeurIPS), pp. 9694-9705, 2021. 13</p>
<p>Li, J., Li, D., Savarese, S., and Hoi, S. C. H. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. CoRR, abs/2301.12597, 2023. 18</p>
<p>Li, L. H., Zhang, P., Zhang, H., Yang, J., Li, C., Zhong, Y., Wang, L., Yuan, L., Zhang, L., Hwang, J., Chang, K., and Gao, J. Grounded language-image pre-training. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 10955-10965. IEEE, 2022a. 3</p>
<p>Li, Y., Bu, R., Sun, M., Wu, W., Di, X., and Chen, B. Pointcnn: Convolution on x-transformed points. In Adv. Neural Inform. Process. Syst. (NeurIPS), pp. 828-838, 2018. 5</p>
<p>Li, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K. Scaling language-image pre-training via masking. CoRR, abs/2212.00794, 2022b. 13</p>
<p>Liu, H., Cai, M., and Lee, Y. J. Masked discrimination for selfsupervised learning on point clouds. In Eur. Conf. Comput. Vis. (ECCV), 2022. 3</p>
<p>Liu, Y., Fan, B., Xiang, S., and Pan, C. Relation-shape convolutional neural network for point cloud analysis. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 8895-8904. Computer Vision Foundation / IEEE, 2019. 6</p>
<p>Liu, Y., Fan, Q., Zhang, S., Dong, H., Funkhouser, T. A., and Yi, L. Contrastive multimodal fusion with tupleinfonce. In Int. Conf. Comput. Vis. (ICCV), pp. 734-743. IEEE, 2021a. 1, 4</p>
<p>Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Int. Conf. Comput. Vis. (ICCV), pp. 9992-10002. IEEE, 2021b. 3, 7</p>
<p>Loshchilov, I. and Hutter, F. SGDR: stochastic gradient descent with warm restarts. In Int. Conf. Learn. Represent. (ICLR). OpenReview.net, 2017. 15</p>
<p>Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In Int. Conf. Learn. Represent. (ICLR), 2019. 15</p>
<p>Ma, X., Qin, C., You, H., Ran, H., and Fu, Y. Rethinking network design and local geometry in point cloud: A simple residual MLP framework. In Int. Conf. Learn. Represent. (ICLR). OpenReview.net, 2022. 5, 17</p>
<p>OpenAI. Introducing chatgpt. 2022. URL https://openai. com/blog/chatgpt. 21</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155, 2022. 1</p>
<p>Pang, Y., Wang, W., Tay, F. E. H., Liu, W., Tian, Y., and Yuan, L. Masked autoencoders for point cloud self-supervised learning. In Eur. Conf. Comput. Vis. (ECCV), 2022. 1, 3, 4, 5, 6, 7, 14, $15,16,17,20$</p>
<p>Park, S. and Kwak, N. Feature-level ensemble knowledge distillation for aggregating knowledge from multiple networks. In Giacomo, G. D., Catalá, A., Dilkina, B., Milano, M., Barro, S., Bugarín, A., and Lang, J. (eds.), Eur. Conf. Artif. Intell. (ECAI), volume 325 of Frontiers in Artificial Intelligence and Applications, pp. 1411-1418. IOS Press, 2020. 4</p>
<p>Qi, C. R., Su, H., Mo, K., and Guibas, L. J. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. $77-85,2017 a .5,6,17$</p>
<p>Qi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Adv. Neural Inform. Process. Syst. (NIPS), pp. 5099-5108, 2017b. 5, 6, 17</p>
<p>Qian, G., Li, Y., Peng, H., Mai, J., Hammoud, H. A. A. K., Elhoseiny, M., and Ghanem, B. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 5</p>
<p>Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018. 1</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. In Proc. Int. Conf. Mach. Learn. (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 8748-8763. PMLR, 2021. 1, 3, 4, 6, 7, 14, 15, 18, 21</p>
<p>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In Proc. Int. Conf. Mach. Learn. (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 8821-8831. PMLR, 2021. 3</p>
<p>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 10674-10685. IEEE, 2022. 1</p>
<p>Ruder, S. An overview of multi-task learning in deep neural networks. CoRR, abs/1706.05098, 2017. 15</p>
<p>Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy, S., Crowson, K., Schmidt, L., Kaczmarczyk, R., and Jitsev, J. LAION-5B: an open large-scale dataset for training next generation image-text models. CoRR, abs/2210.08402, 2022. 21</p>
<p>Tao, C., Zhu, X., Huang, G., Qiao, Y., Wang, X., and Dai, J. Siamese image modeling for self-supervised vision representation learning. CoRR, abs/2206.01204, 2022. 13</p>
<p>Tian, Y., Krishnan, D., and Isola, P. Contrastive multiview coding. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J. (eds.), Eur. Conf. Comput. Vis. (ECCV), volume 12356 of Lecture Notes in Computer Science, pp. 776-794. Springer, 2020a. 3, 4</p>
<p>Tian, Y., Krishnan, D., and Isola, P. Contrastive representation distillation. In Int. Conf. Learn. Represent. (ICLR), 2020b. 4</p>
<p>Tian, Y., Sun, C., Poole, B., Krishnan, D., Schmid, C., and Isola, P. What makes for good views for contrastive learning? In $A d v$. Neural Inform. Process. Syst. (NeurIPS), 2020c. 4</p>
<p>Uy, M. A., Pham, Q.-H., Hua, B.-S., Nguyen, T., and Yeung, S.-K. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 1588-1597, 2019. 6
van den Oord, A., Li, Y., and Vinyals, O. Representation learning with contrastive predictive coding. CoRR, abs/1807.03748, 2018. 3, 4, 7, 14, 19</p>
<p>Vapnik, V. Statistical learning theory. Wiley, 1998. ISBN 978-0-471-03003-4. 16</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Adv. Neural Inform. Process. Syst. (NIPS), pp. 5998-6008, 2017. 2, 5, 6, 17</p>
<p>Wang, H., Liu, Q., Yue, X., Lasenby, J., and Kusner, M. J. Unsupervised point cloud pre-training via occlusion completion. In Int. Conf. Comput. Vis. (ICCV), pp. 9782-9792, 2021. 1, 16</p>
<p>Wang, L. and Yoon, K. Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI), 44(6):30483068, 2022. 4</p>
<p>Wang, X. and Qi, G. Contrastive learning with stronger augmentations. CoRR, abs/2104.07713, 2021. 4</p>
<p>Wang, Y., Sun, Y., Liu, Z., Sarma, S. E., Bronstein, M. M., and Solomon, J. M. Dynamic graph CNN for learning on point clouds. ACM Trans. Graph., 38(5):146:1-146:12, 2019. 5, 17</p>
<p>Wang, Z., Yu, X., Rao, Y., Zhou, J., and Lu, J. P2P: tuning pre-trained image models for point cloud analysis with point-to-pixel prompting. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022. 5</p>
<p>Wei, C., Fan, H., Xie, S., Wu, C.-Y., Yuille, A., and Feichtenhofer, C. Masked feature prediction for self-supervised visual pre-training. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022a. 3</p>
<p>Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022b. 1</p>
<p>Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J. 3d shapenets: A deep representation for volumetric shapes. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 1912-1920, 2015. 6, 7</p>
<p>Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised feature learning via non-parametric instance discrimination. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 37333742. Computer Vision Foundation / IEEE Computer Society, 2018. 3</p>
<p>Xiang, L., Ding, G., and Han, J. Learning from multiple experts: Self-paced knowledge distillation for long-tailed classification. In Eur. Conf. Comput. Vis. (ECCV), volume 12350 of Lecture Notes in Computer Science, pp. 247-263. Springer, 2020. 4</p>
<p>Xie, S., Gu, J., Guo, D., Qi, C. R., Guibas, L. J., and Litany, O. Pointcontrast: Unsupervised pre-training for 3d point cloud understanding. In Eur. Conf. Comput. Vis. (ECCV), volume 12348 of Lecture Notes in Computer Science, pp. 574-591. Springer, 2020. 1, 3, 17</p>
<p>Xie, Z., Geng, Z., Hu, J., Zhang, Z., Hu, H., and Cao, Y. Revealing the dark secrets of masked image modeling. CoRR, abs/2205.13543, 2022a. 2</p>
<p>Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Wei, Y., Dai, Q., and Hu, H. On data scaling in masked image modeling. CoRR, abs/2206.04664, 2022b. 2</p>
<p>Yi, L., Kim, V. G., Ceylan, D., Shen, I.-C., Yan, M., Su, H., Lu, C., Huang, Q., Sheffer, A., and Guibas, L. A scalable active framework for region annotation in 3d shape collections. ACM Trans. Graph., 35(6):1-12, 2016. 16</p>
<p>Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., and Wu, Y. Coca: Contrastive captioners are image-text foundation models. Trans. Mach. Learn. Res. (TMLR), 2022a. ISSN 28358856. 13</p>
<p>Yu, X., Tang, L., Rao, Y., Huang, T., Zhou, J., and Lu, J. Point-bert: Pre-training 3d point cloud transformers with masked point modeling. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022b. 1, 5, 6, 16, 17</p>
<p>Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. Barlow twins: Self-supervised learning via redundancy reduction. In Proc. Int. Conf. Mach. Learn. (ICML), volume 139 of Proceedings of Machine Learning Research, pp. 12310-12320. PMLR, 2021. 4</p>
<p>Zhang, L., Chen, X., Zhang, J., Dong, R., and Ma, K. Contrastive deep supervision. In Eur. Conf. Comput. Vis. (ECCV), volume 13686 of Lecture Notes in Computer Science, pp. 1-19. Springer, 2022a. 4</p>
<p>Zhang, R., Guo, Z., Gao, P., Fang, R., Zhao, B., Wang, D., Qiao, Y., and Li, H. Point-m2AE: Multi-scale masked autoencoders for hierarchical point cloud pre-training. In Adv. Neural Inform. Process. Syst. (NeurIPS), 2022b. 3, 5, 16</p>
<p>Zhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y., Gao, P., and Li, H. Pointclip: Point cloud understanding by CLIP. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. (CVPR), 2022c. 3, 6, 7, 17</p>
<p>Zhang, Z., Girdhar, R., Joulin, A., and Misra, I. Self-supervised pretraining of 3d features on any point-cloud. In Int. Conf. Comput. Vis. (ICCV), pp. 10232-10243. IEEE, 2021. 1</p>
<p>Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A. L., and Kong, T. ibot: Image BERT pre-training with online tokenizer. In Int. Conf. Learn. Represent. (ICLR), 2022. 3, 13</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Pipeline of RECon executing zero-shot and fine-tuning. We fuse the pretrained global image ([IMG]) and text ([TXT]) query features by summation for zero-shot prediction. During fine-tuning, a new [CLS] token is added and fused with pretrained global [IMG] and [TXT] queries by concatenation for fine-tuning prediction. No stop-grad is used in CA connections during fine-tuning.</p>
<h1>A. Additional Related Works</h1>
<p>Contrastive-Generative Representation Learning In 2D vision and NLP, some works have been proposed for contrastivegenerative representation learning. Zhou et al. (2022) propose to use an online tokenizer to distill local tokens and global class tokens, respectively. Tao et al. (2022) further proposes to perform three tasks at the same time: reconstruction, intra-view matching, and intra-image contrastive learning. This paradigm is also trending in the vision-language (VL) community, Li et al. (2021) propose ALBEF that uses contrastive learning before modality fusion to make the reconstructed features encoded with more semantics. Through cross attention of different modalities, CoCa (Yu et al., 2022a) fuses visual-language features while performing mask language modeling, where contrastive learning aligns the multi-modal global features. Recently, Li et al. (2022b) propose to use masked signals for contrastive VL learning, which greatly improves the training efficiency without losing performance. Different from these methods, our RECon is built for 3D representation learning, which is faced with a unique and serious low-data challenge. Besides, we propose a novel RECon-block that models contrastive-generative representation learning as a generative pretraining guided contrastive learning.</p>
<p>Table 7. Training recipes for pretraining and downstream fine-tuning.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Pretraining</th>
<th style="text-align: center;">Classification</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Segmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Config</td>
<td style="text-align: center;">ShapeNet</td>
<td style="text-align: center;">ScanObjectNN</td>
<td style="text-align: center;">ModelNet</td>
<td style="text-align: center;">ShapeNetPart</td>
</tr>
<tr>
<td style="text-align: left;">optimizer</td>
<td style="text-align: center;">AdamW</td>
<td style="text-align: center;">AdamW</td>
<td style="text-align: center;">AdamW</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">learning rate</td>
<td style="text-align: center;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
<td style="text-align: center;">$1 \mathrm{e}-5$</td>
<td style="text-align: center;">$2 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: left;">weight decay</td>
<td style="text-align: center;">$5 \mathrm{e}-2$</td>
<td style="text-align: center;">$5 \mathrm{e}-2$</td>
<td style="text-align: center;">$5 \mathrm{e}-2$</td>
<td style="text-align: center;">$5 \mathrm{e}-2$</td>
</tr>
<tr>
<td style="text-align: left;">learning rate scheduler</td>
<td style="text-align: center;">cosine</td>
<td style="text-align: center;">cosine</td>
<td style="text-align: center;">cosine</td>
<td style="text-align: center;">cosine</td>
</tr>
<tr>
<td style="text-align: left;">training epochs</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">300</td>
</tr>
<tr>
<td style="text-align: left;">warmup epochs</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">drop path rate</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.2</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">image resolution</td>
<td style="text-align: center;">$224 \times 224$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">image patch size</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">number of points</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">$1024 / 8192$</td>
<td style="text-align: center;">2048</td>
</tr>
<tr>
<td style="text-align: left;">number of point patches</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">$64 / 512$</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">point patch size</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">augmentation</td>
<td style="text-align: center;">Rotation</td>
<td style="text-align: center;">Rotation</td>
<td style="text-align: center;">Scale\&amp;Trans</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">GPU device</td>
<td style="text-align: center;">PH402 SKU 200</td>
<td style="text-align: center;">RTX 2080Ti</td>
<td style="text-align: center;">RTX 2080Ti</td>
<td style="text-align: center;">RTX 2080Ti</td>
</tr>
</tbody>
</table>
<h1>B. Additional Implementation Details</h1>
<h2>B.1. Loss Function</h2>
<p>In this section, we detail the loss functions in our implementation of contrastive models, generative models, and our RECON models. Given a randomly sampled multimodal minibatch $\left{\mathbf{p}<em k="k">{k}, \mathbf{t}</em>}, \mathbf{t<em k="1">{k}\right}</em>}^{N}$ with $N$ paired samples, where $\mathbf{p<em k="k">{k}$ is the $k$-th 3D point cloud sample, and $\mathbf{i}</em>}, \mathbf{t<em k="k">{k}$ are the corresponding multimodal views, i.e., rendered RGB image (rendered from the single view of the default pose) and text category description (e.g., chair), respectively. By dividing the text category descriptions into $K$ groups $\left{T</em>\right}<em k="k">{k=1}^{K}$ where $T</em>}$ is the $k$-th unique text description, we obtain a fine-grained label set $\left{\mathbf{y<em k="k">{k}=\ell: \mathbf{t}</em>\right}}=T_{\ell<em k="k">{k=1}^{N}$ of ShapeNet. Hence, the minibatch becomes $\left{\mathbf{p}</em>}, \mathbf{i<em k="k">{k}, \mathbf{t}</em>}, \mathbf{y<em k="1">{k}\right}</em>$, and we can use part of them or all of them to perform different self-supervised representation learning methods.}^{N</p>
<p>Single-Modal Contrastive Loss For the single-modal contrastive models (SMC), we use a supervised contrastive loss following Khosla et al. (2020), where the supervision is generalized from the constructed label set. Let $k \in \mathcal{K} \equiv{1, \ldots, N}$ be the index of the minibatch samples where $\mathcal{A}<em k="k">{k}=\mathcal{K} \backslash{k}$ is the index set for all samples other than the $k$-th anchor sample. $\mathcal{P}</em>} \equiv\left{p \in \mathcal{A<em p="p">{k}: \mathbf{y}</em>}=\mathbf{y<em t="t">{k}\right}$ is the index for the positive samples, and other samples with different labels are considered as the negative samples. Given the 3D student network $\mathcal{F}^{\mathrm{P}}(\cdot)$, where $z</em>$ can be written as:}^{\mathrm{P}}=\mathcal{F}^{\mathrm{P}}\left(\mathbf{p}_{\ell}\right), \ell \in \mathcal{K}$. The loss $\mathcal{L}^{\mathrm{SMC}</p>
<p>$$
\mathcal{L}^{\mathrm{SMC}}=\sum_{k \in \mathcal{K}} \frac{-1}{\left|\mathcal{P}<em _in="\in" _mathcal_P="\mathcal{P" p="p">{k}\right|} \sum</em><em k="k">{k}} \log \frac{\exp \left(z</em>}^{\mathrm{P}} \cdot z_{p}^{\mathrm{P}} / \tau\right)}{\sum_{a \in \mathcal{A<em k="k">{i}} \exp \left(z</em>
$$}^{\mathrm{P}} \cdot z_{a}^{\mathrm{P}} / \tau\right)</p>
<p>where $\tau \in \mathbb{R}$ is the scalar temperature parameter which is set as 0.1 (Khosla et al., 2020) for all contrastive models.
Cross-Modal Contrastive Loss For the cross-modal contrastive models (CMC), we use the contrastive InfoNCE loss (van den Oord et al., 2018) following Radford et al. (2021). Given the minibatch, we can construct two groups of $N \times N$ pairs, i.e., point-image pairs $\left{\left{\mathbf{p}<em n="n">{m}, \mathbf{i}</em>}\right}: \forall m \in \mathcal{K}, \forall n \in \mathcal{K}\right}$ and point-text pairs $\left{\left{\mathbf{p<em n="n">{m}, \mathbf{t}</em>}\right}: \forall m \in \mathcal{K}, \forall n \in \mathcal{K}\right}$. Then the encoded representation of another modality from the same sample is used as positive, while the others in the minibatch are used as negative samples. Given a pretrained 2D teacher network $\mathcal{F}^{\mathrm{T}}(\cdot)$ and a pretrained language teacher network $\mathcal{F}^{\mathrm{T}}(\cdot)$, where $z_{t}^{\mathrm{I}}=\mathcal{F}^{\mathrm{I}}\left(\mathbf{i<em t="t">{\ell}\right), \ell \in \mathcal{K}$ and $z</em>$ :}^{\mathrm{T}}=\mathcal{F}^{\mathrm{T}}\left(\mathbf{t}_{\ell}\right), \ell \in \mathcal{K}$ represent the decoded representations, respectively. Similar to Eq. (4), the loss $\mathcal{L}^{\mathrm{CMC}}$ is the summation of multimodal point-image loss $\mathcal{L}^{\mathrm{CMC}-\mathrm{PI}}$ and point-text loss $\mathcal{L}^{\mathrm{CMC}-\mathrm{PT}</p>
<p>$$
\mathcal{L}^{\mathrm{CMC}}=-\sum_{k \in \mathcal{K}}\left[\underbrace{\log \frac{\exp \left(z_{k}^{\mathrm{P}} \cdot \operatorname{stopgrad}\left(z_{k}^{\mathrm{I}}\right) / \tau\right)}{\sum_{a \in \mathcal{A}<em k="k">{i}} \exp \left(z</em>}^{\mathrm{P}} \cdot z_{a}^{\mathrm{I}} / \tau\right)}<em k="k">{\mathcal{L}^{\mathrm{CMC}-\mathrm{PI}}}+\underbrace{\log \frac{\exp \left(z</em>}^{\mathrm{P}} \cdot \operatorname{stopgrad}\left(z_{k}^{\mathrm{T}}\right) / \tau\right)}{\sum_{a \in \mathcal{A<em k="k">{i}} \exp \left(z</em>\right]
$$}^{\mathrm{P}} \cdot z_{a}^{\mathrm{T}} / \tau\right)}}_{\mathcal{L}^{\mathrm{CMC}-\mathrm{PT}}</p>
<p>where $\tau \in \mathbb{R}$ is the scalar temperature parameter, and $\operatorname{stopgrad}(\cdot)$ is the stop-gradient operation. Here, no gradient is back-propagated to the image or text teachers, and hence the two cross-modal teachers are frozen.</p>
<p>Masked Point Modeling Reconstruction Loss For the masked point modeling reconstruction (MPM), we use the $\ell_{2}$ Chamfer-Distance (Fan et al., 2017) following Pang et al. (2022). Denote $\mathcal{M}<em _ell="\ell">{\ell}(\cdot), \ell \in \mathcal{K}$ as the masking operation. Let $\mathcal{R}</em>} \equiv \mathcal{F}^{\mathrm{P}}\left(\mathcal{M<em _ell="\ell">{\ell}\left(\mathbf{p}</em>}\right)\right), \ell \in \mathcal{K}$ and $\mathcal{G<em _ell="\ell">{\ell} \equiv \mathbf{p}</em>$ can be written as:}, \ell \in \mathcal{K}$ be the reconstructed point clouds and ground truth point clouds, respectively. The reconstruction loss $\mathcal{L}^{\mathrm{MPM}</p>
<p>$$
\mathcal{L}^{\mathrm{MPM}}=\sum_{k \in \mathcal{K}}\left[\frac{1}{\left|\mathcal{R}<em _in="\in" _mathcal_R="\mathcal{R" r="r">{k}\right|} \sum</em><em _in="\in" _mathcal_G="\mathcal{G" g="g">{k}} \min </em><em 2="2">{k}}|r-g|</em>}^{2}+\sum_{g \in \mathcal{G<em _in="\in" _mathcal_R="\mathcal{R" r="r">{k}} \min </em><em 2="2">{k}}|r-g|</em>\right]
$$}^{2</p>
<p>RECON Loss The RECON loss is the ensemble distillation as defined in Eq. (3). The first term $\mathcal{L}^{\mathrm{REC}}$ is the same as Eq. (6), and the contrastive loss can be either single-modal contrastive loss defined in Eq. (4) or the cross-modal contrastive loss defined in Eq. (5). Similar to SimSiam (Chen \&amp; He, 2021), we find that the 3D student learning from the frozen cross-modal teachers with stop-gradient does not fall into the representation collapsing trap. Therefore, we use the positive-only representation learning with Smooth $\ell_{1}$ loss Smooth- $\ell_{1}(\cdot, \cdot)$, which we find achieves the best performance (see Sec. 4.2). In this case, the cross-modal contrastive term $\mathcal{L}^{\mathrm{CON}}$ can be written as follows:</p>
<p>$$
\mathcal{L}^{\mathrm{CON}}=\sum_{k \in \mathcal{K}}\left[\text { Smooth- } \ell_{1}\left(z_{k}^{\mathrm{P}}, \operatorname{stopgrad}\left(z_{k}^{\mathrm{I}}\right)\right)+\text { Smooth- } \ell_{1}\left(z_{k}^{\mathrm{P}}, \operatorname{stopgrad}\left(z_{k}^{\mathrm{T}}\right)\right)\right]
$$</p>
<p>Table 8. Details of RECON model variants. This table format follows Dosovitskiy et al. (2021).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">Hidden size</th>
<th style="text-align: center;">MLP size</th>
<th style="text-align: center;">Heads</th>
<th style="text-align: center;">#Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">* RECON-Tiny</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">192</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">11.4 M</td>
</tr>
<tr>
<td style="text-align: left;">* RECON-Small</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">19.0 M</td>
</tr>
<tr>
<td style="text-align: left;">* RECON</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">384</td>
<td style="text-align: center;">1536</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">43.6 M</td>
</tr>
</tbody>
</table>
<h1>B.2. Experimental Details</h1>
<p>Pretraining Details We use ShapeNetCore from ShapeNet (Chang et al., 2015) as the pretraining dataset. ShapeNet is a clean set of 3D CAD object models with rich annotations, including $\sim 51 \mathrm{~K}$ unique 3D models from 55 common object categories. For the generation of paired data, we take surface point samples of the 3D object model to generate 3D point clouds, the 3D models are lighted with textures for rendering 2D RGB images. Specifically, we use the MacOS Preview ${ }^{\circledR}$ to generate high-quality rendered images. The text description comprises category labels and manually designed prompt templates. During pretraining, a unique image query [IMG] token and text query [TXT] token are trained to align the global representation from the image teacher and the text teacher, respectively. The overall pretraining includes 300 epochs, and we use a cosine learning rate (Loshchilov \&amp; Hutter, 2017) of 5e-4 warming up for 10 epochs. AdamW optimizer (Loshchilov \&amp; Hutter, 2019) is used, and the batch size is 128 . More details are shown in Table 7.</p>
<p>Downstream Transferring Details Fig. 8 shows the pipeline when RECON transfers to downstream tasks, including zero-shot and fine-tuning. For zero-shot, we use simple summation to fuse multi-modal features, and the cosine similarity is used as the classification metric (Radford et al., 2021). During fine-tuning, we concatenate the pooled representation of local tokens and the learned global query tokens as model features. For classification, we add a new global classification [CLS] token and concatenate it with the other queries before being fused to the classification head. It is worth noting that due to the consistency of the optimization objective during fine-tuning, we cancel the stop gradient of the cross attention connections. Without specifications, we report overall accuracy (OA) results without voting on the most challenging ScanObjectNN PB_T50_RS benchmark using 2,048 input points and ModelNet40 using 1,024 input points ( 1 k P ), and the zero-shot classification results are reported on the test split. More detailed training configurations are shown in Table 7.</p>
<p>Model Variants Table 8 summarizes the RECON model configurations, which are grounded in a similar fashion of ViT variants (Dosovitskiy et al., 2021). The default version "RECON" (or RECON-Base) is directly adopted from previous works (Pang et al., 2022; Dong et al., 2023), except that the network is configured as two-stream rather than single-stream. We add the smaller "Tiny" and "Small" models, which have the same number of layers but with reduced channel dimension.</p>
<h2>C. Additional Baselines</h2>
<p>We show two additional simple fusion methods (Ruder, 2017), including Vanilla Multi-task Learning and a Two-Tower network. Here, we make an analysis of these two baseline methods.</p>
<p>Vanilla Multi-task Learning Fusion As shown in Fig. 9(a), Vanilla Multi-task Learning directly shares a standard Transformer as the encoder. The input embedding tokens take masked reconstruction as the pretext task, and the global tokens take the global contrast as the pretext task. Vanilla Multi-task Learning doesn't consider the pattern difference issue of the two tasks (see Fig. 1 and Fig. 7). The transfer performances of Vanilla Multi-task Learning on ScanObjectNN and ModelNet40 are reported in Table 9. It is observed that this vanilla design leads to limited performance, which only improves the from scratch OA by $+0.89 \%$ on ScanObjectNN, and no improvement on ModelNet40 is achieved. This indicates task conflicts, and it is consistent with the analysis in Sec. 1 that it is non-trivial for joint learning of these two tasks.</p>
<p>Two-Tower Network To verify whether the performance improvement of RECON comes from the form of a twotower architecture, we design a simple Two-Tower network, shown in Fig. 9(b). The Two-Tower network uses standard Transformers as the encoder for masked reconstruction and global contrastive learning, respectively. During fine-tuning, it concatenates features from both streams for an ensemble (similar to Fig. 8(b)). Clearly, the Two-Tower network doesn't suffer the pattern difference issue. The transfer performances of the Two-Tower network on ScanObjectNN and ModelNet40 are reported in Table 9. It can be seen that the Two-Tower network brings unsatisfactory performance, i.e., only $+3.41 \%$ and $+0.5 \%$ accuracy improvements by the from scratch baseline. In comparison, our RECON uses the reconstruction task as guidance for global contrastive learning. As a result, RECON successfully disentangles the two tasks while preserving both merits, and significantly better improvements are achieved.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Illustration of the vanilla multi-task learning and two-tower network baselines.
Table 9. Study of the additional baseline. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Vanilla Multi-task Learning</td>
<td style="text-align: center;">82.53</td>
<td style="text-align: center;">91.6</td>
</tr>
<tr>
<td style="text-align: left;">Two-Tower Network</td>
<td style="text-align: center;">85.05</td>
<td style="text-align: center;">92.1</td>
</tr>
<tr>
<td style="text-align: left;">RECON</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{9 4 . 1}$</td>
</tr>
</tbody>
</table>
<h1>D. Additional Experiments</h1>
<h2>D.1. Additional Evaluations</h2>
<p>Linear SVM Evaluation Linear SVM evaluation (Boser et al., 1992; Vapnik, 1998) can be used to evaluate the discriminative quality of pretrained features (Goyal et al., 2019). The results on ModelNet40 are shown in Table 10. It shows that our RECON outperforms Point-BERT, which also uses plain Transformers, by a clear margin of $+6.0 \%$. Compared to methods using hierarchical Transformers, our RECON outperforms PointM2AE (Zhang et al., 2022b) by $+0.5 \%$.</p>
<p>Table 10. Linear SVM classification on ModelNet40. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Hierachical</th>
<th style="text-align: center;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\star$ Point-BERT (Yu et al., 2022b)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">87.4</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ OcCo (Wang et al., 2021)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">89.2</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ CrossPoint (Afham et al., 2022)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">91.2</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ PointM2AE (Zhang et al., 2022b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">92.9</td>
</tr>
<tr>
<td style="text-align: left;">$\star$ RECON</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\mathbf{9 3 . 4}$</td>
</tr>
</tbody>
</table>
<p>3D Part Segmentation To evaluate the geometric understanding performance within objects, we conduct the part segmentation experiment on ShapeNetPart (Yi et al., 2016). Specifically, we concatenate the cross-modal feature of RECON into the global feature and use the same segmentation head as Point-MAE for a fair comparison. From Table 11, it can be observed that RECON improves the from scratch baseline by $+1.4 \%$ and $+1.7 \%$ Cls. mIoU and Inst. mIoU, respectively. Besides, RECON outperforms the SSL counterpart Point-MAE (Pang et al., 2022) by $+0.4 \%$ Inst. mIoU. It shows that the cross-modal global knowledge from RECON cross-modal pretraining can still play a certain role in part segmentation.</p>
<p>Zero-Shot Recognition on Real-World Dataset In Table 12, we show the zero-shot evaluation results on the real-world ScanObjectNN dataset. Though our RECON is pretrained on the synthetic dataset ShapeNet, it outperforms PointCLIP and CLIP2Point, which leverage depth images, by a clear margin. For example, on the most challenging PB_T50_RS benchmark, RECON achieves a Top-1 accuracy of $30.5 \%$, which is $+7.2 \%$ and $+15.1 \%$ higher than CLIP2Point and PointCLIP, respectively.</p>
<h2>D.2. Additional Ablation Study</h2>
<p>Data Augmentation We study the impact of using different data augmentations (DA) when fine-tuning RECON pretrained models on different downstream tasks, as shown in Table 13. The results show that Rotation has the best performance improvement on ScanObjectNN, and Scale\&amp;Translate has the highest performance improvement on ModelNet40. Therefore, we use by default Rotation on ScanObjectNN and Scale\&amp;Translate on ModelNet40 if without specifications.</p>
<p>Table 11. Part segmentation on ShapeNetPart dataset. The mIoU over all classes (Cls.) and the mIoU over all instances (Inst.) are reported. ${ }^{1}$ denotes results with our proposed $\bullet$ RECon-block built backbone architecture.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Methods</th>
<th style="text-align: center;">Cls. mIoU (\%)</th>
<th style="text-align: center;">Inst. mIoU (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\circ$ PointNet (Qi et al., 2017a)</td>
<td style="text-align: center;">80.4</td>
<td style="text-align: center;">83.7</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ PointNet++ (Qi et al., 2017b)</td>
<td style="text-align: center;">81.9</td>
<td style="text-align: center;">85.1</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ DGCNN (Wang et al., 2019)</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ PointMLP (Ma et al., 2022)</td>
<td style="text-align: center;">84.6</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ Transformer (Vaswani et al., 2017)</td>
<td style="text-align: center;">83.4</td>
<td style="text-align: center;">84.7</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ Transformer ${ }^{\dagger}$ (Vaswani et al., 2017)</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">85.2</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ PointContrast (Xie et al., 2020)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.1</td>
</tr>
<tr>
<td style="text-align: left;">$\circ$ CrossPoint (Afham et al., 2022)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">85.5</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ Point-BERT (Yu et al., 2022b)</td>
<td style="text-align: center;">84.1</td>
<td style="text-align: center;">85.6</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ Point-MAE (Pang et al., 2022)</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ Point-MAE ${ }^{\dagger}$ (Pang et al., 2022)</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ ACT (Dong et al., 2023)</td>
<td style="text-align: center;">84.7</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECon</td>
<td style="text-align: center;">$\mathbf{8 4 . 8}$</td>
<td style="text-align: center;">$\mathbf{8 6 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 12. Zero-shot 3D object classification on ScanObjectNN dataset. Top-1 accuracy (\%) is reported. Ensemb. denotes whether to use the ensemble strategy with multiple text inputs.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Backbone</th>
<th style="text-align: center;">Ensemb.</th>
<th style="text-align: center;">OBJ_ONLY</th>
<th style="text-align: center;">OBJ_BG</th>
<th style="text-align: center;">PB_T50_RS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\circ$ PointCLIP (Zhang et al., 2022c)</td>
<td style="text-align: left;">ResNet-50</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">15.4</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ CLIP2Point (Huang et al., 2022)</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">30.5</td>
<td style="text-align: center;">23.3</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECon</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\mathbf{3 9 . 6}$</td>
<td style="text-align: center;">$\mathbf{3 8 . 0}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 5}$</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECon</td>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{4 3 . 7}$</td>
<td style="text-align: center;">$\mathbf{4 0 . 4}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 5}$</td>
</tr>
</tbody>
</table>
<p>Table 13. Ablation study of data augmentations (DA) during fine-tuning. We report the fine-tuning overall accuracy (\%) without voting of RECON pretrained models.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">DA Strategy</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">-</td>
<td style="text-align: center;">87.44</td>
<td style="text-align: center;">93.4</td>
</tr>
<tr>
<td style="text-align: left;">Scale \&amp; Translate</td>
<td style="text-align: center;">87.02</td>
<td style="text-align: center;">$\mathbf{9 4 . 1}$</td>
</tr>
<tr>
<td style="text-align: left;">Jitter</td>
<td style="text-align: center;">90.22</td>
<td style="text-align: center;">92.9</td>
</tr>
<tr>
<td style="text-align: left;">Rotation</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 3}$</td>
<td style="text-align: center;">92.3</td>
</tr>
<tr>
<td style="text-align: left;">Dropout</td>
<td style="text-align: center;">87.40</td>
<td style="text-align: center;">93.5</td>
</tr>
<tr>
<td style="text-align: left;">Horizontal Flip</td>
<td style="text-align: center;">87.27</td>
<td style="text-align: center;">93.6</td>
</tr>
</tbody>
</table>
<p>Paired Data Ablation During pretraining, we use the point cloud, rendered image, and text inputs generated from ShapeNet (Chang et al., 2015), which makes the data contain clear matching attributes. To verify the dependence of RECon on paired data, we shuffle the rendered images under the same category for pretraining. The results are shown in Table 14. It shows that RECON has a performance degradation of less than $1 \%$ in fine-tuning tasks and $6.4 \%$ in the zero-shot tasks.</p>
<p>Table 14. Ablation study on the paired data during pretraining. Paired denotes the paired 3D point clouds, rendered images, and category text descriptions; unpaired data denotes data with shuffled images under the same category. Overall accuracy (\%) without voting is reported. ModelNet40-FT represents the fine-tuning accuracy, and ModelNet40-ZS is the zero-shot result on the train+test split.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Paired</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;">ModelNet40-FT</th>
<th style="text-align: center;">ModelNet40-ZS</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">90.22</td>
<td style="text-align: center;">93.8</td>
<td style="text-align: center;">60.4</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">90.63</td>
<td style="text-align: center;">94.5</td>
<td style="text-align: center;">66.8</td>
</tr>
</tbody>
</table>
<p>Table 15. Ablation study on the prompts for zero-shot learning. [C] denotes the category text description, [P] denotes the prefix prompt, and [S] denotes the suffix prompt. Acc. (\%) represents the ModelNet40 zero-shot Top-1 Accuracy (\%).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$[\mathrm{P}]+[\mathrm{C}]$</th>
<th style="text-align: center;">Acc. (\%)</th>
<th style="text-align: center;">$[\mathrm{C}]+[\mathrm{S}]$</th>
<th style="text-align: center;">Acc. (\%)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">' + [C]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$[\mathrm{C}]+{ }^{\prime}$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A ' + [C]</td>
<td style="text-align: center;">52.27</td>
<td style="text-align: center;">$[\mathrm{C}]+{ }^{\prime}{ }^{\prime}$</td>
<td style="text-align: center;">49.11</td>
</tr>
<tr>
<td style="text-align: left;">'A model of ' + [C]</td>
<td style="text-align: center;">53.04</td>
<td style="text-align: center;">$[\mathrm{C}]+{ }^{\prime}$ with white background.'</td>
<td style="text-align: center;">56.93</td>
</tr>
<tr>
<td style="text-align: left;">'A model of a ' + [C]</td>
<td style="text-align: center;">54.05</td>
<td style="text-align: center;">$[\mathrm{C}]+{ }^{\prime}$ with black context.'</td>
<td style="text-align: center;">60.57</td>
</tr>
<tr>
<td style="text-align: left;">'An image of ' + [C]</td>
<td style="text-align: center;">57.50</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'An image of a ' + [C]</td>
<td style="text-align: center;">56.36</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A 3D model of ' + [C]</td>
<td style="text-align: center;">55.63</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A 3D model of a ' + [C]</td>
<td style="text-align: center;">55.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A rendered model of ' + [C]</td>
<td style="text-align: center;">56.48</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A rendered model of a ' + [C]</td>
<td style="text-align: center;">56.52</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A point cloud of ' + [C]</td>
<td style="text-align: center;">52.71</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A point cloud of a ' + [C]</td>
<td style="text-align: center;">55.27</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A point cloud model of' + [C]</td>
<td style="text-align: center;">56.65</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A point cloud model of a ' + [C]</td>
<td style="text-align: center;">54.46</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A 3D rendered model of ' + [C]</td>
<td style="text-align: center;">55.83</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A 3D rendered model of a ' + [C]</td>
<td style="text-align: center;">54.78</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A rendered image of ' + [C]</td>
<td style="text-align: center;">58.79</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A rendered image of a ' + [C]</td>
<td style="text-align: center;">56.48</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A 3D rendered image of ' + [C]</td>
<td style="text-align: center;">59.07</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">'A 3D rendered image of a ' + [C]</td>
<td style="text-align: center;">57.70</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Prompt Ablation For zero-shot evaluation, we construct the prompt by concatenating the prefix prompt [P] with the category text description [C], followed by a suffix [S], and the final prompt is $[\mathrm{P}]+[\mathrm{C}]+[\mathrm{C}]$. For example, A point cloud of a chair with white background. Table 15 shows the ablation study of results with only the prefix or suffix prompts we used, i.e., 20 prefixes and 4 suffixes. Note that when we use the ensemble strategy, the results are ensembled by all the combinations of prefixes and suffixes. The prompts can also be constructed from rendered images by powerful Vison-Language Foundation Models such as BLIP-2 (Li et al., 2023), which may further bring improvements.</p>
<p>Pretraining Learning Rate Ablation on Contrastive Models As discussed before, contrastive models can easily fall into the representation over-fitting trap. We find that it generally leads to a sensitivity to the pretraining learning rate. As shown in Table 16, when using the default learning rate for RECON pretraining (i.e., 5e-4), the contrastive model fails to generalize with unsatisfactory results. And when the learning rate is small, where the model learns slowly, a relatively better generalization performance is achieved with improved training stability (see Fig. 6 and the discussions). We speculate that it is due to the low-data challenge, which largely makes the model easily learn over-fitted representations. As a result, the contrastive models are very sensitive to the pretraining learning rate, which has to be carefully adjusted. Hence, without specifications, we use this adjusted smaller learning rate for contrastive models.</p>
<p>Table 16. Ablation study on the pretraining learning rate (LR) of contrastive models. We show the results of fine-tuned cross-modal contrastive models that are pretrained with different LRs. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LR</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">85.11</td>
<td style="text-align: center;">92.1</td>
</tr>
<tr>
<td style="text-align: left;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">86.49</td>
<td style="text-align: center;">92.4</td>
</tr>
<tr>
<td style="text-align: left;">$5 \mathrm{e}-4$</td>
<td style="text-align: center;">82.48</td>
<td style="text-align: center;">90.3</td>
</tr>
<tr>
<td style="text-align: left;">$1 \mathrm{e}-3$</td>
<td style="text-align: center;">67.45</td>
<td style="text-align: center;">86.3</td>
</tr>
</tbody>
</table>
<p>Ablation Study on Freezing Cross-Modal Teachers Unlike CLIP (Radford et al., 2021), RECON uses frozen crossmodal teachers in contrastive learning to acquire the dark knowledge of other modalities. We show the influence of freezing</p>
<p>parameters on downstream tasks in Table 17. It can be seen that if the model uses the InfoNCE (van den Oord et al., 2018) loss function containing negative pairs, the impact of unfrozen parameters during pretraining is not significant. In contrast, for contrastive learning with only positive pairs, unfrozen parameters will cause serious performance degradation on ScanObjectNN and ModelNet.</p>
<p>Table 17. Ablation study on the freezing cross-modal teachers. Freeze denotes whether both image and text teachers are frozen or not. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Freeze</th>
<th style="text-align: center;">Contrastive Metric</th>
<th style="text-align: center;">ScanObjectNN</th>
<th style="text-align: center;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">InfoNCE</td>
<td style="text-align: center;">89.87</td>
<td style="text-align: center;">93.7</td>
</tr>
<tr>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">Smooth $\ell_{1}$</td>
<td style="text-align: center;">84.77</td>
<td style="text-align: center;">91.2</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">InfoNCE</td>
<td style="text-align: center;">90.11</td>
<td style="text-align: center;">93.8</td>
</tr>
<tr>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">Smooth $\ell_{1}$</td>
<td style="text-align: center;">90.63</td>
<td style="text-align: center;">94.1</td>
</tr>
</tbody>
</table>
<p>Training Costs We report the singe-GPU without distributed training GPU hours and GPU memory usage of RECON and Point-MAE during pretraining and fine-tuning. We study RECONwith three network configurations: RECON, RECON-Small, and RECON, which are described in Table 8. Although the parameter count of RECON is almost doubled compared to Point-MAE, the memory consumption is mainly for storing intermediate variables of the model. However, RECON only adds three global queries to its intermediate variables compared to Point-MAE. Therefore, the memory consumption of RECON has mostly stayed the same compared to Point-MAE.</p>
<p>Table 18. Training costs comparison. We report the single-GPU training GPU hours and GPU memory costs during pretraining on ShapeNet and fine-tuning on ScanObjectNN.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">#Params</th>
<th style="text-align: center;">GFLOPS</th>
<th style="text-align: center;">Pretrain GPU Hours</th>
<th style="text-align: center;">Fine-tune GPU Hours</th>
<th style="text-align: center;">GPU Memory</th>
<th style="text-align: center;">ScanObjectNN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\bullet$ Point-MAE</td>
<td style="text-align: center;">22.1 M</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">$\mathbf{3 2 . 7 h}$</td>
<td style="text-align: center;">5.3 h</td>
<td style="text-align: center;">7766 MB</td>
<td style="text-align: center;">85.18</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON-Tiny</td>
<td style="text-align: center;">11.4 M</td>
<td style="text-align: center;">$\mathbf{2 . 4}$</td>
<td style="text-align: center;">40.9 h</td>
<td style="text-align: center;">$\mathbf{3 . 8 h}$</td>
<td style="text-align: center;">$\mathbf{4 5 7 8 M B}$</td>
<td style="text-align: center;">89.10</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON-Small</td>
<td style="text-align: center;">19.0 M</td>
<td style="text-align: center;">3.2</td>
<td style="text-align: center;">46.3 h</td>
<td style="text-align: center;">4.5 h</td>
<td style="text-align: center;">4710 MB</td>
<td style="text-align: center;">89.52</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON</td>
<td style="text-align: center;">43.6 M</td>
<td style="text-align: center;">5.3</td>
<td style="text-align: center;">54.5 h</td>
<td style="text-align: center;">6.1 h</td>
<td style="text-align: center;">7906 MB</td>
<td style="text-align: center;">$\mathbf{9 0 . 6 3}$</td>
</tr>
</tbody>
</table>
<p>Edge Device Deployments We deploy the models using ONNX ${ }^{7}$ and tested it on several common edge devices, including laptops, pads, smartphones, and single chip microcomputers (SCM), all of which are tested on CPU. Table 19 shows the number of frames per second (FPS) the model can make with inputs from the ModelNet40 dataset, which uses 1K point clouds per sample. The results demonstrate that our RECON network is easy to be deployed on various edge devices. For example, our small variant RECON-Tiny is even more efficient that surpasses the widely used PointNet++.</p>
<p>Table 19. Real-time deployments on edge devices. We report the number of frames per second (FPS).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Device</th>
<th style="text-align: center;">Macbook Air <br> Laptop <br> Apple M2 Silicon</th>
<th style="text-align: center;">HUAWEI MatePad Pro <br> Pad <br> Hisilicon Kirin 990</th>
<th style="text-align: center;">Honor X30 Android <br> Smartphone <br> Qualcomm Snapdragon 695</th>
<th style="text-align: center;">Raspberry Pi 3B <br> Single Chip Microcomputer <br> Broadcom BCM2837</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\bullet$ PointNet++</td>
<td style="text-align: center;">83.6</td>
<td style="text-align: center;">25.6</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">2.5</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ Point-MAE</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">12.8</td>
<td style="text-align: center;">2.1</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON-Tiny</td>
<td style="text-align: center;">$\mathbf{1 0 3 . 5}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 8}$</td>
<td style="text-align: center;">$\mathbf{2 0 . 2}$</td>
<td style="text-align: center;">$\mathbf{3 . 7}$</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON-Small</td>
<td style="text-align: center;">78.1</td>
<td style="text-align: center;">24.5</td>
<td style="text-align: center;">14.6</td>
<td style="text-align: center;">2.7</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON</td>
<td style="text-align: center;">51.7</td>
<td style="text-align: center;">15.6</td>
<td style="text-align: center;">8.1</td>
<td style="text-align: center;">1.8</td>
</tr>
</tbody>
</table>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 20. Ablation study on pretrained teachers and multimodal data. Pretrained Teacher denotes whether a pretrained teacher is used, and Multimodal Data denotes whether multimodal data is used during pretraining. SMC and CMC denote single-modal and cross-modal contrastive modeling methods, respectively. All results except the Vanilla Multi-Task Learning and Two-Tower Network baselines are conducted with our proposed $\bullet$ RECon-block built backbone architecture during fine-tuning for a fair comparison. Overall accuracy (\%) without voting is reported.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Pretrained Teacher</th>
<th style="text-align: left;">Multimodal Data</th>
<th style="text-align: left;">ScanObjectNN</th>
<th style="text-align: left;">ModelNet40</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Contrastive Methods</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ SMC Only</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">81.70</td>
<td style="text-align: left;">91.2</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ CMC Only</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">82.48</td>
<td style="text-align: left;">91.4</td>
</tr>
<tr>
<td style="text-align: left;">Generative Methods</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ Point-MAE (Pang et al., 2022)</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">88.42</td>
<td style="text-align: left;">93.5</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ ACT (Dong et al., 2023)</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">89.01</td>
<td style="text-align: left;">93.5</td>
</tr>
<tr>
<td style="text-align: left;">Generative + Contrastive Methods</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Vanilla Multi-task Learning</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">82.53</td>
<td style="text-align: left;">91.6</td>
</tr>
<tr>
<td style="text-align: left;">Two-Tower Network</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">85.05</td>
<td style="text-align: left;">92.1</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON+ SMC</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">89.73</td>
<td style="text-align: left;">94.0</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON+ CMC (from scratch)</td>
<td style="text-align: left;">$\times$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">90.32</td>
<td style="text-align: left;">94.0</td>
</tr>
<tr>
<td style="text-align: left;">$\bullet$ RECON+ CMC</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\checkmark$</td>
<td style="text-align: left;">$\mathbf{9 0 . 6 3}$</td>
<td style="text-align: left;">$\mathbf{9 4 . 1}$</td>
</tr>
</tbody>
</table>
<h1>E. Discussions on Cross-Modal Teachers and Multimodal Training</h1>
<p>In Table 20, we conduct an ablation study on pretrained teachers and multimodal data during pretraining. This analysis clearly demonstrates that (i) RECON+SMC that leverages single-modal contrastive learning still exhibits excellent performance on downstream tasks without including any other modality data or pretrained teachers. It achieves an overall accuracy of $89.73 \%$ on ScanObjectNN, which is $+1.31 \%$ better than the generative-only baseline Point-MAE and $+0.72 \%$ better than the generative method ACT that leverages a pretrained 2D teacher. It demonstrates that our design of generative guidance for contrastive modeling is critical and essential for combining the merits of these two paradigms, which already yields superior results compared to other methods and has well tackled the raised issues. (ii) RECON+CMC (from scratch) uses cross-modal contrastive learning on multimodal data while without any pretrained teachers, further bringing an improvement of $+0.59 \%$ to a remarkable $90.32 \%$ overall accuracy on ScanObjectNN. It demonstrates that multimodal data is beneficial since 3D data are seriously lacking. (iii) RECON+CMC uses cross-modal contrastive learning with both multimodal data and pretrained teachers, further leading to an improvement of $+0.31 \%$ on ScanObjectNN. It demonstrates that pretrained teachers from other modality data and the usage of multimodal data in RECON (not for other methods) can indeed further improve the performance for tackling the data dessert issue. (iv) Vanilla Multi-Task Learning and Two-Tower Network baselines that simply transfer cross-modal knowledge from pretraining weights do not produce satisfactory results. We speculate that this is due to the pattern differences issue demonstrated in Fig. 1, which is precisely the motivation behind our RECON-block. In contrast, our RECON+CMC outperforms these two baselines by a large margin. This shows that the benefits do not merely come from pretrained teachers but rather the fact that RECON design is an effective framework that guides contrastive learning with generative modeling. It also shows that pretrained teachers are not all you need, and the benefits of pretrained teachers or multimodal data can not be obtained without our proposed RECON.</p>
<h2>F. Limitations and Future Works</h2>
<p>RECON is a general multimodal representation learning framework that leverages both merits of contrastive and generative modeling, which is demonstrated effective in 3D but is also general to any other modalities. However, there are some limitations of RECON, which may be two-fold. (i) The first limitation may come from the multimodal data and domain. This paper mainly explores RECON in 3D representation learning, and future explorations on multimodal problems like 2D Vision-Language may be intriguing. (ii) Another limitation may come from the architecture design, i.e., the RECON-block proposed in this work. It is our future exploration to extend RECON to be architecture-agnostic.</p>
<h1>Broader Impact</h1>
<p>The proposed RECon is a general framework that can be used for not only 3D representation learning but also all multimodal learning problems. For example, by leveraging large-scale multimodal data like from the web (Schuhmann et al., 2022), one may obtain a foundational VL RECon that shares a similar property of CLIP (Radford et al., 2021) since a multimodal alignment contrastive learning is used. Besides, with the rapid development of Large Language Models (LLMs), RECon may also enable the potential for leveraging LLM like ChatGPT (OpenAI, 2022) for LLM-assisted multimodal understanding. Since RECON successfully unifies generative and contrastive modeling in a decent fashion, future applications may also involve AI-generated content (AIGC) but with cross-modal discriminative capability. For example, RECon can be trained for generative modeling that could be extended to generate contents based on input text instructions or other modalities. We hope this work could motivate and facilitate future explorations on representation learning with multimodal or low-data inputs, which is critical for AI deployments in real-life. However, all the potential impacts of the aforementioned applications should be taken into consideration while developing AI systems in human society.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ https://onnx.ai/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{4}$ Note that for methods like MAE (He et al., 2022; Pang et al., 2022), the teacher network is an identity mapping with no parameters, and therefore $\phi$ is an empty set, see Dong et al. (2023).
${ }^{5}$ Term taken from Tian et al. (2020b).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
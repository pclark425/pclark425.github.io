<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5337 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5337</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5337</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-259951557</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.09042v2.pdf" target="_blank">Emotional Intelligence of Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning. However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated. Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions. Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs. This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score). With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs. Most achieved above-average EQ scores, with GPT-4 exceeding 89% of human participants with an EQ of 117. Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans. In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ. In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence. Project website: https://emotional-intelligence.github.io/</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5337.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5337.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>State-of-the-art closed-source large transformer-based language model by OpenAI; evaluated here for emotional understanding using the SECEU psychometric test.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Advanced transformer-based LLM from OpenAI; trained with large-scale text corpora and instruction/RLHF fine-tuning per OpenAI reports (specific dataset and parameter count not public).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Unknown</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>Situational Evaluation of Complex Emotional Understanding (SECEU)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence (social cognition)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items (school/family/social) each with four plausible emotions; participants allocate 10 points across four emotions per item. Human standard scores derived from N=541 undergraduates/postgraduates; model responses scored by Euclidean distance to human standard and normalized to EQ (mean=100, SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 1.89 (lower better), EQ = 117, outperformed 89% of human participants; LLM-to-Human pattern similarity r = 0.28 (>67% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Norm established from N=541 young adults (mean EQ = 100, SD = 15). Human SECEU Euclidean distance mean = 2.79 (SD = 0.822). Human-to-Human pattern similarity mean r = 0.199 (SD = 0.166).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 scored EQ 117 (one SD above human mean), outperforming the majority of the human sample (89%). Its representational pattern similarity (r = 0.28) is moderately high, indicating closer-to-human qualitative patterning than many other models but not identical to the human template.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Although GPT-4 attains expert-level EQ, the paper notes representational differences remain; prompts had little effect on GPT-4. The model size and proprietary training make causal attribution of its performance to specific factors uncertain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5337.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-davinci-003 (OpenAI GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A GPT-3.5-family model from OpenAI with instruction fine-tuning and RLHF; evaluated on SECEU and benefited from prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3.5-family transformer model with instruction fine-tuning and RLHF reported; large-parameter model (~175B family).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (table attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>Same as above: 40 scenario items, allocate 10 points across four emotions; scored by Euclidean distance to human consensus and normalized to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.01, EQ = 114, outperformed 83% of human participants; LLM-to-Human pattern similarity r = 0.31 (exceeds ~73% of humans). Performance improved with prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ=100 (SD=15); human SECEU mean Euclidean distance = 2.79 (SD=0.822); human pattern similarity mean r=0.199 (SD=0.166).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>text-davinci-003 reaches 'expert' range (EQ 114), comparable to high-performing humans and close to GPT-4; prompting (Two-shot chain-of-thought) improved both EQ and representational similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Model required prompting to reach best performance (authors report prompts substantially improved representational similarity). As a closed model, specifics of training are not fully disclosed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5337.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used OpenAI model (GPT-3.5 family) with instruction tuning; evaluated on SECEU under zero-/few-shot conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based instruction-tuned GPT-3.5 family model (reported in table as ~175B family), optimized for chat/instruction performance (SFT + RLHF indicated).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B (table attribution)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; allocate 10 points across four emotions; consensus scoring; Euclidean distance to human standard converted to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.63, EQ = 103, outperformed 52% of humans; LLM-to-Human pattern similarity r = 0.04 (~17% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ=100 (SD=15). Human SECEU mean Euclidean distance = 2.79 (SD=0.822).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-3.5-turbo achieved slightly above-average EQ (103), roughly matching median human performance. Representational similarity to humans was low (r≈0.04), indicating quantitative alignment without strong qualitative pattern similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Model performance was relatively robust without heavy prompting (zero-shot succeeded for GPT-3.5-turbo per supplement). However, pattern similarity remained low, suggesting differing item-level strengths/weaknesses vs. humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5337.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaVinci (text-davinci-002/001 grouped)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaVinci family (text-davinci-001 / text-davinci-002 / text-davinci-001 variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Older GPT-3-based models from OpenAI; some required prompting to complete test and showed variable EU performance across versions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DaVinci / text-davinci-001 / text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3-era transformer models (DaVinci variants). Different releases have differing fine-tuning/RLHF histories; some versions required prompts or failed without prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>text-davinci-002/001: <175B (table indicates <175B / ~175B family for variants); DaVinci listed as 175B in table</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; allocate 10 points across four emotions; scored by Euclidean distance to human consensus and standardized to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>text-davinci-001: SECEU=2.41, EQ=107 (64% of humans); text-davinci-002: SECEU=3.39, EQ=91 (23% of humans), pattern r = -0.04; DaVinci (older variant) reported SECEU=3.587, EQ=87 (18% of humans) but with high pattern similarity when prompted (r=0.41, >91%).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ=100 (SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Performance varied across versions: text-davinci-001 performed above average (EQ 107), text-davinci-002 underperformed (EQ 91), and older DaVinci reported low EQ 87 but could achieve high pattern similarity with chain-of-thought prompts. Authors note substantial improvement from text-davinci-002 to text-davinci-003 with RLHF.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Many DaVinci-era models required prompt engineering to complete the SECEU (some failed without prompts). Representational patterns differed across versions; some versions with prompts achieved human-like pattern similarity despite low unprompted EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5337.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Curie (OpenAI GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mid-sized OpenAI GPT model (GPT-3 family) evaluated on SECEU showing near-average EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Curie</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Historically a mid-size GPT-3-family model; table lists as ~13B parameter family (exact public attribution uncertain).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (table attribution / 'Unknown' per table metadata)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; allocate 10 points across four emotions; judged by Euclidean distance to human consensus and converted to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.71, EQ = 102, outperformed 50% of humans; pattern similarity r = 0.11 (~29% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Curie performed slightly above the human average (EQ 102), roughly at the human median. Representational similarity to humans was modestly positive but below typical Human-to-Human mean.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Model required prompt engineering in some cases; exact training/finetuning details not fully available for this variant.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5337.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Babbage</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Babbage (OpenAI GPT family)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A smaller GPT-family model evaluated on SECEU; achieved near-average EQ but showed qualitatively different item-level response patterns compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Babbage</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller GPT-family transformer model historically used as a lower-parameter variant; table lists ~3B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items with four emotions each; allocation of 10 points across emotions per item; scoring by Euclidean distance to human consensus and normalization to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.78, EQ = 100, outperformed 44% of humans; LLM-to-Human pattern similarity r = -0.12 (~4% of humans), indicating negative correlation with human template.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Babbage's overall EQ matches the human mean (EQ 100), but its item-wise discriminability pattern is qualitatively different (negative correlation), implying different underlying mechanisms of item success/failure compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Despite average EQ, the negative pattern correlation suggests Babbage succeeds on different items than humans do; authors flag this as evidence that high EQ can arise from non-human-like mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5337.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Alpaca (LLaMA-based instruction-tuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source instruction-following LLaMA-based model (13B) evaluated on SECEU; achieved above-average EQ but low representational similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Alpaca</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based open-source instruction-tuned model (13B parameters) produced via supervised fine-tuning on instruction datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; allocate 10 points across four emotions; consensus-based standard from humans; model score = Euclidean distance to human standard converted to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.56, EQ = 104, outperformed 56% of humans; LLM-to-Human pattern similarity r = 0.03 (~15% of humans), just below the one-SD-below threshold used by authors to flag qualitative differences.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Alpaca's EQ is above the human mean (104) but its representational pattern correlation is extremely low (r≈0.03), indicating its item-level response profile differs qualitatively from most humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Though EQ is satisfactory, low pattern similarity suggests Alpaca does not rely on human-like item sensitivity; model required prompt engineering in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5337.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (LLaMA-based chat model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source LLaMA-based chat model (13B) evaluated on SECEU; achieved above-average EQ but showed near-zero/negative representational similarity to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned LLaMA derivative (~13B parameters) optimized for chat-style interactions using user-shared instruction data.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; four emotions per item, sum-to-10 allocations; scored by Euclidean distance to human consensus and converted to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.51, EQ = 105, outperformed 59% of humans; LLM-to-Human pattern similarity r = -0.02 (~10% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Vicuna attains above-average EQ (105) but its item-wise pattern correlation with the human template is essentially zero/negative, indicating qualitative differences in which items it finds easy or hard relative to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors caution that above-average EQ does not imply human-like representation; Vicuna is an example of good overall score but dissimilar item-level patterning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5337.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Koala</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Koala (LLaMA-based dialogue model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A LLaMA-derived dialogue model (13B) that scored poorly in EQ but exhibited the most human-like item-wise representational pattern in the study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Koala</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-based instruction-tuned dialogue model (13B) developed for academic research; supervised fine-tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; allocate 10 points across four emotions; scoring via Euclidean distance to human consensus and normalization to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 3.72, EQ = 83, outperformed 13% of humans (i.e., below average); LLM-to-Human pattern similarity r = 0.43 (exceeds ~93% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Koala underperforms on aggregate EQ (83) compared to humans, but its item-level discriminability correlates with human template very strongly (r=0.43), indicating it succeeds and fails on the same items as humans—quantitative difference rather than qualitative divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>This dissociation (low EQ but high pattern similarity) highlights that low overall score can nonetheless reflect human-like item sensitivity; suggests qualitative similarity despite lower accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5337.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dolly</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dolly (Pythia-based open instruction-tuned model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source Pythia-based instruction-following model (13B) evaluated with SECEU and achieving near-average EQ with moderate pattern similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dolly</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pythia-based open instruction-tuned model (listed at ~13B in table) intended as an open instruction-tuned baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; allocate 10 points total among four emotion options; model responses compared to human consensus via Euclidean distance then converted to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.89, EQ = 98, outperformed 38% of humans; LLM-to-Human pattern similarity r = 0.26 (~62% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Dolly is slightly below average EQ-wise (98) but exhibits positive pattern similarity, indicating partly human-like item-level discriminability.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors note Dolly performed within the 'normal' range; model reliant on supervised fine-tuning but no RLHF per table; prompt engineering affected some models differentially.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5337.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Oasst</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Oasst (OpenAssistant/Alpaca-style open model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Open-source instruction-tuned model evaluated on SECEU; achieved above-average EQ with moderate pattern similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Oasst</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAssistant-style instruction-tuned open-source model (table lists ~13B parameters); reportedly fine-tuned with supervised data and in some cases RLHF-like techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 items scenarios; allocate 10 points across four emotions; model evaluated by Euclidean distance to human standard and converted to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.41, EQ = 107, outperformed 64% of humans; LLM-to-Human pattern similarity r = 0.24 (~59% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541; mean EQ = 100 (SD = 15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Oasst achieved above-average EQ (107), comparable to many instruction-tuned models; pattern similarity positive and moderate, suggesting partial alignment with human item sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors highlight that open-source fine-tuning choices can produce competitive EQ despite smaller model size; interpretability of training-data effects remains limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e5337.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM (GLM-based bilingual model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilingual GLM-based model evaluated on SECEU achieving below-average EQ and low pattern similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-family bilingual pre-trained model (table lists ~6B parameters) with instruction tuning reported.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>6B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; four emotions per item with sum-to-10 allocations; consensus scoring to derive human standard and EQ normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 3.12, EQ = 94, outperformed 28% of humans; LLM-to-Human pattern similarity r = 0.09 (~24% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ=100 (SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>ChatGLM scored below the human average (EQ 94) and displayed low pattern similarity, indicating both quantitative underperformance and qualitative deviation from human item-response profiles.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Being a bilingual model and smaller parameter size (6B) likely influenced performance; some models required prompt engineering to complete the SECEU (ChatGLM could complete but needed specific prompts in a few items).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e5337.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source chatbot/LLM from Anthropic evaluated on SECEU, achieving above-average EQ and modest pattern similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Anthropic's transformer-based assistant model; specific model size and dataset details not publicly specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Unknown</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario items; allocate 10 points across four emotions; scored versus human consensus and normalized to EQ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>SECEU score = 2.46, EQ = 106, outperformed 61% of humans; LLM-to-Human pattern similarity r = 0.11 (~28% of humans).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541 (mean EQ=100, SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Claude performed above human average (EQ 106) but its representational similarity to humans is modest, indicating partial alignment in item-level sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Size not disclosed; authors note closed-source models' internal training differences limit causal interpretation. Prompt effects minimal for some advanced models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e5337.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA (base)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA (Meta base model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Base LLaMA model (open-source foundation model) was attempted but failed to complete SECEU in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA (base)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source foundation language model family (various sizes; table indicates a 13B variant was attempted), not instruction-tuned in base form.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (attempted variant per table)</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenarios; four emotions; allocation of 10 points; scoring by Euclidean distance to human standard.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FAILED to complete the SECEU (no meaningful responses for majority of items).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ=100 (SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Base LLaMA could not be evaluated due to failure to generate valid test-format responses, so no direct performance comparison to humans is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Base foundation models without instruction tuning often fail to follow the required response format; authors used prompt engineering and instruction-fine-tuned derivatives (Alpaca, Vicuna, Koala) to obtain usable outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e5337.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fastchat / Flan-T5 variant</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fastchat (Flan-T5 based chat variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder family chat model variant attempted in study but failed to complete SECEU items sufficiently for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Fastchat (Flan-T5-based)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer chat variant (Flan-T5 family); table indicates ~3B attempted but failed to produce valid outputs for many items.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario-items, four emotions per item; requires sum-to-10 formatted numeric allocations; models that could not produce this format were marked failed.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FAILED to complete the SECEU (failed on majority of items).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ=100 (SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>No direct comparison possible because model responses were insufficient/invalid for scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Encoder-decoder chat models in this configuration struggled to follow the required numeric allocation output format; authors report decoder-only models tended to perform better on this task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5337.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e5337.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RWKV-v4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RWKV-v4 (RNN-based Recurrent Weighted Key-Value)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RNN-style model architecture tested but failed to complete SECEU in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emotional Intelligence of Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RWKV-v4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Recurrent Weighted Key-Value (RWKV) architecture, an RNN-style alternative to transformers; table lists attempted 13B variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SECEU</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>Emotion understanding / Emotional intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>40 scenario-based items with sum-to-10 numeric allocations across four emotions; scoring by Euclidean distance to human consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>FAILED to complete the SECEU (failed on many items even with prompting).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human norm: N=541, mean EQ=100 (SD=15).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>RWKV-v4 could not be scored on SECEU due to failure to produce valid responses; thus no quantitative comparison to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Authors note RWKV (RNN) architecture struggled relative to transformer models; suggests architecture impacts ability to follow test format and reason about scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Emotional Intelligence of Large Language Models', 'publication_date_yy_mm': '2023-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Theory of Mind May Have Spontaneously Emerged in Large Language Models <em>(Rating: 2)</em></li>
                <li>Boosting Theory-of-Mind Performance in Large Language Models via Prompting <em>(Rating: 2)</em></li>
                <li>Sparks of Artificial General Intelligence: Early experiments with GPT-4 <em>(Rating: 2)</em></li>
                <li>Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT <em>(Rating: 1)</em></li>
                <li>Administration of the text-based portions of a general IQ test to five different large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5337",
    "paper_id": "paper-259951557",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "Generative Pre-trained Transformer 4",
            "brief_description": "State-of-the-art closed-source large transformer-based language model by OpenAI; evaluated here for emotional understanding using the SECEU psychometric test.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "Advanced transformer-based LLM from OpenAI; trained with large-scale text corpora and instruction/RLHF fine-tuning per OpenAI reports (specific dataset and parameter count not public).",
            "model_size": "Unknown",
            "cognitive_test_name": "Situational Evaluation of Complex Emotional Understanding (SECEU)",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence (social cognition)",
            "cognitive_test_description": "40 scenario items (school/family/social) each with four plausible emotions; participants allocate 10 points across four emotions per item. Human standard scores derived from N=541 undergraduates/postgraduates; model responses scored by Euclidean distance to human standard and normalized to EQ (mean=100, SD=15).",
            "llm_performance": "SECEU score = 1.89 (lower better), EQ = 117, outperformed 89% of human participants; LLM-to-Human pattern similarity r = 0.28 (&gt;67% of humans).",
            "human_baseline_performance": "Norm established from N=541 young adults (mean EQ = 100, SD = 15). Human SECEU Euclidean distance mean = 2.79 (SD = 0.822). Human-to-Human pattern similarity mean r = 0.199 (SD = 0.166).",
            "performance_comparison": "GPT-4 scored EQ 117 (one SD above human mean), outperforming the majority of the human sample (89%). Its representational pattern similarity (r = 0.28) is moderately high, indicating closer-to-human qualitative patterning than many other models but not identical to the human template.",
            "notable_differences_or_limitations": "Although GPT-4 attains expert-level EQ, the paper notes representational differences remain; prompts had little effect on GPT-4. The model size and proprietary training make causal attribution of its performance to specific factors uncertain.",
            "uuid": "e5337.0",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "text-davinci-003",
            "name_full": "text-davinci-003 (OpenAI GPT family)",
            "brief_description": "A GPT-3.5-family model from OpenAI with instruction fine-tuning and RLHF; evaluated on SECEU and benefited from prompting.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "text-davinci-003",
            "model_description": "GPT-3.5-family transformer model with instruction fine-tuning and RLHF reported; large-parameter model (~175B family).",
            "model_size": "175B (table attribution)",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "Same as above: 40 scenario items, allocate 10 points across four emotions; scored by Euclidean distance to human consensus and normalized to EQ.",
            "llm_performance": "SECEU score = 2.01, EQ = 114, outperformed 83% of human participants; LLM-to-Human pattern similarity r = 0.31 (exceeds ~73% of humans). Performance improved with prompts.",
            "human_baseline_performance": "Human norm: N=541, mean EQ=100 (SD=15); human SECEU mean Euclidean distance = 2.79 (SD=0.822); human pattern similarity mean r=0.199 (SD=0.166).",
            "performance_comparison": "text-davinci-003 reaches 'expert' range (EQ 114), comparable to high-performing humans and close to GPT-4; prompting (Two-shot chain-of-thought) improved both EQ and representational similarity.",
            "notable_differences_or_limitations": "Model required prompting to reach best performance (authors report prompts substantially improved representational similarity). As a closed model, specifics of training are not fully disclosed.",
            "uuid": "e5337.1",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "A widely used OpenAI model (GPT-3.5 family) with instruction tuning; evaluated on SECEU under zero-/few-shot conditions.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "Transformer-based instruction-tuned GPT-3.5 family model (reported in table as ~175B family), optimized for chat/instruction performance (SFT + RLHF indicated).",
            "model_size": "175B (table attribution)",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; allocate 10 points across four emotions; consensus scoring; Euclidean distance to human standard converted to EQ.",
            "llm_performance": "SECEU score = 2.63, EQ = 103, outperformed 52% of humans; LLM-to-Human pattern similarity r = 0.04 (~17% of humans).",
            "human_baseline_performance": "Human norm: N=541, mean EQ=100 (SD=15). Human SECEU mean Euclidean distance = 2.79 (SD=0.822).",
            "performance_comparison": "GPT-3.5-turbo achieved slightly above-average EQ (103), roughly matching median human performance. Representational similarity to humans was low (r≈0.04), indicating quantitative alignment without strong qualitative pattern similarity.",
            "notable_differences_or_limitations": "Model performance was relatively robust without heavy prompting (zero-shot succeeded for GPT-3.5-turbo per supplement). However, pattern similarity remained low, suggesting differing item-level strengths/weaknesses vs. humans.",
            "uuid": "e5337.2",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "DaVinci (text-davinci-002/001 grouped)",
            "name_full": "DaVinci family (text-davinci-001 / text-davinci-002 / text-davinci-001 variants)",
            "brief_description": "Older GPT-3-based models from OpenAI; some required prompting to complete test and showed variable EU performance across versions.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "DaVinci / text-davinci-001 / text-davinci-002",
            "model_description": "GPT-3-era transformer models (DaVinci variants). Different releases have differing fine-tuning/RLHF histories; some versions required prompts or failed without prompting.",
            "model_size": "text-davinci-002/001: &lt;175B (table indicates &lt;175B / ~175B family for variants); DaVinci listed as 175B in table",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; allocate 10 points across four emotions; scored by Euclidean distance to human consensus and standardized to EQ.",
            "llm_performance": "text-davinci-001: SECEU=2.41, EQ=107 (64% of humans); text-davinci-002: SECEU=3.39, EQ=91 (23% of humans), pattern r = -0.04; DaVinci (older variant) reported SECEU=3.587, EQ=87 (18% of humans) but with high pattern similarity when prompted (r=0.41, &gt;91%).",
            "human_baseline_performance": "Human norm: N=541, mean EQ=100 (SD=15).",
            "performance_comparison": "Performance varied across versions: text-davinci-001 performed above average (EQ 107), text-davinci-002 underperformed (EQ 91), and older DaVinci reported low EQ 87 but could achieve high pattern similarity with chain-of-thought prompts. Authors note substantial improvement from text-davinci-002 to text-davinci-003 with RLHF.",
            "notable_differences_or_limitations": "Many DaVinci-era models required prompt engineering to complete the SECEU (some failed without prompts). Representational patterns differed across versions; some versions with prompts achieved human-like pattern similarity despite low unprompted EQ.",
            "uuid": "e5337.3",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Curie",
            "name_full": "Curie (OpenAI GPT family)",
            "brief_description": "A mid-sized OpenAI GPT model (GPT-3 family) evaluated on SECEU showing near-average EQ.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Curie",
            "model_description": "Historically a mid-size GPT-3-family model; table lists as ~13B parameter family (exact public attribution uncertain).",
            "model_size": "13B (table attribution / 'Unknown' per table metadata)",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; allocate 10 points across four emotions; judged by Euclidean distance to human consensus and converted to EQ.",
            "llm_performance": "SECEU score = 2.71, EQ = 102, outperformed 50% of humans; pattern similarity r = 0.11 (~29% of humans).",
            "human_baseline_performance": "Human norm: N=541, mean EQ = 100 (SD = 15).",
            "performance_comparison": "Curie performed slightly above the human average (EQ 102), roughly at the human median. Representational similarity to humans was modestly positive but below typical Human-to-Human mean.",
            "notable_differences_or_limitations": "Model required prompt engineering in some cases; exact training/finetuning details not fully available for this variant.",
            "uuid": "e5337.4",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Babbage",
            "name_full": "Babbage (OpenAI GPT family)",
            "brief_description": "A smaller GPT-family model evaluated on SECEU; achieved near-average EQ but showed qualitatively different item-level response patterns compared to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Babbage",
            "model_description": "Smaller GPT-family transformer model historically used as a lower-parameter variant; table lists ~3B parameters.",
            "model_size": "3B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items with four emotions each; allocation of 10 points across emotions per item; scoring by Euclidean distance to human consensus and normalization to EQ.",
            "llm_performance": "SECEU score = 2.78, EQ = 100, outperformed 44% of humans; LLM-to-Human pattern similarity r = -0.12 (~4% of humans), indicating negative correlation with human template.",
            "human_baseline_performance": "Human norm: N=541, mean EQ = 100 (SD = 15).",
            "performance_comparison": "Babbage's overall EQ matches the human mean (EQ 100), but its item-wise discriminability pattern is qualitatively different (negative correlation), implying different underlying mechanisms of item success/failure compared to humans.",
            "notable_differences_or_limitations": "Despite average EQ, the negative pattern correlation suggests Babbage succeeds on different items than humans do; authors flag this as evidence that high EQ can arise from non-human-like mechanisms.",
            "uuid": "e5337.5",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Alpaca",
            "name_full": "Alpaca (LLaMA-based instruction-tuned model)",
            "brief_description": "Open-source instruction-following LLaMA-based model (13B) evaluated on SECEU; achieved above-average EQ but low representational similarity.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Alpaca",
            "model_description": "LLaMA-based open-source instruction-tuned model (13B parameters) produced via supervised fine-tuning on instruction datasets.",
            "model_size": "13B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; allocate 10 points across four emotions; consensus-based standard from humans; model score = Euclidean distance to human standard converted to EQ.",
            "llm_performance": "SECEU score = 2.56, EQ = 104, outperformed 56% of humans; LLM-to-Human pattern similarity r = 0.03 (~15% of humans), just below the one-SD-below threshold used by authors to flag qualitative differences.",
            "human_baseline_performance": "Human norm: N=541, mean EQ = 100 (SD = 15).",
            "performance_comparison": "Alpaca's EQ is above the human mean (104) but its representational pattern correlation is extremely low (r≈0.03), indicating its item-level response profile differs qualitatively from most humans.",
            "notable_differences_or_limitations": "Though EQ is satisfactory, low pattern similarity suggests Alpaca does not rely on human-like item sensitivity; model required prompt engineering in some cases.",
            "uuid": "e5337.6",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna (LLaMA-based chat model)",
            "brief_description": "Open-source LLaMA-based chat model (13B) evaluated on SECEU; achieved above-average EQ but showed near-zero/negative representational similarity to humans.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Vicuna",
            "model_description": "Open-source instruction-tuned LLaMA derivative (~13B parameters) optimized for chat-style interactions using user-shared instruction data.",
            "model_size": "13B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; four emotions per item, sum-to-10 allocations; scored by Euclidean distance to human consensus and converted to EQ.",
            "llm_performance": "SECEU score = 2.51, EQ = 105, outperformed 59% of humans; LLM-to-Human pattern similarity r = -0.02 (~10% of humans).",
            "human_baseline_performance": "Human norm: N=541, mean EQ = 100 (SD = 15).",
            "performance_comparison": "Vicuna attains above-average EQ (105) but its item-wise pattern correlation with the human template is essentially zero/negative, indicating qualitative differences in which items it finds easy or hard relative to humans.",
            "notable_differences_or_limitations": "Authors caution that above-average EQ does not imply human-like representation; Vicuna is an example of good overall score but dissimilar item-level patterning.",
            "uuid": "e5337.7",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Koala",
            "name_full": "Koala (LLaMA-based dialogue model)",
            "brief_description": "A LLaMA-derived dialogue model (13B) that scored poorly in EQ but exhibited the most human-like item-wise representational pattern in the study.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Koala",
            "model_description": "LLaMA-based instruction-tuned dialogue model (13B) developed for academic research; supervised fine-tuning reported.",
            "model_size": "13B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; allocate 10 points across four emotions; scoring via Euclidean distance to human consensus and normalization to EQ.",
            "llm_performance": "SECEU score = 3.72, EQ = 83, outperformed 13% of humans (i.e., below average); LLM-to-Human pattern similarity r = 0.43 (exceeds ~93% of humans).",
            "human_baseline_performance": "Human norm: N=541, mean EQ = 100 (SD = 15).",
            "performance_comparison": "Koala underperforms on aggregate EQ (83) compared to humans, but its item-level discriminability correlates with human template very strongly (r=0.43), indicating it succeeds and fails on the same items as humans—quantitative difference rather than qualitative divergence.",
            "notable_differences_or_limitations": "This dissociation (low EQ but high pattern similarity) highlights that low overall score can nonetheless reflect human-like item sensitivity; suggests qualitative similarity despite lower accuracy.",
            "uuid": "e5337.8",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Dolly",
            "name_full": "Dolly (Pythia-based open instruction-tuned model)",
            "brief_description": "Open-source Pythia-based instruction-following model (13B) evaluated with SECEU and achieving near-average EQ with moderate pattern similarity.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Dolly",
            "model_description": "Pythia-based open instruction-tuned model (listed at ~13B in table) intended as an open instruction-tuned baseline.",
            "model_size": "13B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; allocate 10 points total among four emotion options; model responses compared to human consensus via Euclidean distance then converted to EQ.",
            "llm_performance": "SECEU score = 2.89, EQ = 98, outperformed 38% of humans; LLM-to-Human pattern similarity r = 0.26 (~62% of humans).",
            "human_baseline_performance": "Human norm: N=541, mean EQ = 100 (SD = 15).",
            "performance_comparison": "Dolly is slightly below average EQ-wise (98) but exhibits positive pattern similarity, indicating partly human-like item-level discriminability.",
            "notable_differences_or_limitations": "Authors note Dolly performed within the 'normal' range; model reliant on supervised fine-tuning but no RLHF per table; prompt engineering affected some models differentially.",
            "uuid": "e5337.9",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Oasst",
            "name_full": "Oasst (OpenAssistant/Alpaca-style open model)",
            "brief_description": "Open-source instruction-tuned model evaluated on SECEU; achieved above-average EQ with moderate pattern similarity.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Oasst",
            "model_description": "OpenAssistant-style instruction-tuned open-source model (table lists ~13B parameters); reportedly fine-tuned with supervised data and in some cases RLHF-like techniques.",
            "model_size": "13B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 items scenarios; allocate 10 points across four emotions; model evaluated by Euclidean distance to human standard and converted to EQ.",
            "llm_performance": "SECEU score = 2.41, EQ = 107, outperformed 64% of humans; LLM-to-Human pattern similarity r = 0.24 (~59% of humans).",
            "human_baseline_performance": "Human norm: N=541; mean EQ = 100 (SD = 15).",
            "performance_comparison": "Oasst achieved above-average EQ (107), comparable to many instruction-tuned models; pattern similarity positive and moderate, suggesting partial alignment with human item sensitivity.",
            "notable_differences_or_limitations": "Authors highlight that open-source fine-tuning choices can produce competitive EQ despite smaller model size; interpretability of training-data effects remains limited.",
            "uuid": "e5337.10",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "ChatGLM",
            "name_full": "ChatGLM (GLM-based bilingual model)",
            "brief_description": "A bilingual GLM-based model evaluated on SECEU achieving below-average EQ and low pattern similarity.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "ChatGLM",
            "model_description": "GLM-family bilingual pre-trained model (table lists ~6B parameters) with instruction tuning reported.",
            "model_size": "6B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; four emotions per item with sum-to-10 allocations; consensus scoring to derive human standard and EQ normalization.",
            "llm_performance": "SECEU score = 3.12, EQ = 94, outperformed 28% of humans; LLM-to-Human pattern similarity r = 0.09 (~24% of humans).",
            "human_baseline_performance": "Human norm: N=541, mean EQ=100 (SD=15).",
            "performance_comparison": "ChatGLM scored below the human average (EQ 94) and displayed low pattern similarity, indicating both quantitative underperformance and qualitative deviation from human item-response profiles.",
            "notable_differences_or_limitations": "Being a bilingual model and smaller parameter size (6B) likely influenced performance; some models required prompt engineering to complete the SECEU (ChatGLM could complete but needed specific prompts in a few items).",
            "uuid": "e5337.11",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Claude",
            "name_full": "Claude (Anthropic)",
            "brief_description": "Closed-source chatbot/LLM from Anthropic evaluated on SECEU, achieving above-average EQ and modest pattern similarity.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Claude",
            "model_description": "Anthropic's transformer-based assistant model; specific model size and dataset details not publicly specified in the paper.",
            "model_size": "Unknown",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario items; allocate 10 points across four emotions; scored versus human consensus and normalized to EQ.",
            "llm_performance": "SECEU score = 2.46, EQ = 106, outperformed 61% of humans; LLM-to-Human pattern similarity r = 0.11 (~28% of humans).",
            "human_baseline_performance": "Human norm: N=541 (mean EQ=100, SD=15).",
            "performance_comparison": "Claude performed above human average (EQ 106) but its representational similarity to humans is modest, indicating partial alignment in item-level sensitivity.",
            "notable_differences_or_limitations": "Size not disclosed; authors note closed-source models' internal training differences limit causal interpretation. Prompt effects minimal for some advanced models.",
            "uuid": "e5337.12",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "LLaMA (base)",
            "name_full": "LLaMA (Meta base model)",
            "brief_description": "Base LLaMA model (open-source foundation model) was attempted but failed to complete SECEU in this study.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "LLaMA (base)",
            "model_description": "Open-source foundation language model family (various sizes; table indicates a 13B variant was attempted), not instruction-tuned in base form.",
            "model_size": "13B (attempted variant per table)",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenarios; four emotions; allocation of 10 points; scoring by Euclidean distance to human standard.",
            "llm_performance": "FAILED to complete the SECEU (no meaningful responses for majority of items).",
            "human_baseline_performance": "Human norm: N=541, mean EQ=100 (SD=15).",
            "performance_comparison": "Base LLaMA could not be evaluated due to failure to generate valid test-format responses, so no direct performance comparison to humans is provided.",
            "notable_differences_or_limitations": "Base foundation models without instruction tuning often fail to follow the required response format; authors used prompt engineering and instruction-fine-tuned derivatives (Alpaca, Vicuna, Koala) to obtain usable outputs.",
            "uuid": "e5337.13",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "Fastchat / Flan-T5 variant",
            "name_full": "Fastchat (Flan-T5 based chat variant)",
            "brief_description": "An encoder-decoder family chat model variant attempted in study but failed to complete SECEU items sufficiently for scoring.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "Fastchat (Flan-T5-based)",
            "model_description": "Encoder-decoder transformer chat variant (Flan-T5 family); table indicates ~3B attempted but failed to produce valid outputs for many items.",
            "model_size": "3B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario-items, four emotions per item; requires sum-to-10 formatted numeric allocations; models that could not produce this format were marked failed.",
            "llm_performance": "FAILED to complete the SECEU (failed on majority of items).",
            "human_baseline_performance": "Human norm: N=541, mean EQ=100 (SD=15).",
            "performance_comparison": "No direct comparison possible because model responses were insufficient/invalid for scoring.",
            "notable_differences_or_limitations": "Encoder-decoder chat models in this configuration struggled to follow the required numeric allocation output format; authors report decoder-only models tended to perform better on this task.",
            "uuid": "e5337.14",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        },
        {
            "name_short": "RWKV-v4",
            "name_full": "RWKV-v4 (RNN-based Recurrent Weighted Key-Value)",
            "brief_description": "An RNN-style model architecture tested but failed to complete SECEU in this study.",
            "citation_title": "Emotional Intelligence of Large Language Models",
            "mention_or_use": "use",
            "model_name": "RWKV-v4",
            "model_description": "Recurrent Weighted Key-Value (RWKV) architecture, an RNN-style alternative to transformers; table lists attempted 13B variant.",
            "model_size": "13B",
            "cognitive_test_name": "SECEU",
            "cognitive_test_type": "Emotion understanding / Emotional intelligence",
            "cognitive_test_description": "40 scenario-based items with sum-to-10 numeric allocations across four emotions; scoring by Euclidean distance to human consensus.",
            "llm_performance": "FAILED to complete the SECEU (failed on many items even with prompting).",
            "human_baseline_performance": "Human norm: N=541, mean EQ=100 (SD=15).",
            "performance_comparison": "RWKV-v4 could not be scored on SECEU due to failure to produce valid responses; thus no quantitative comparison to humans.",
            "notable_differences_or_limitations": "Authors note RWKV (RNN) architecture struggled relative to transformer models; suggests architecture impacts ability to follow test format and reason about scenarios.",
            "uuid": "e5337.15",
            "source_info": {
                "paper_title": "Emotional Intelligence of Large Language Models",
                "publication_date_yy_mm": "2023-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Theory of Mind May Have Spontaneously Emerged in Large Language Models",
            "rating": 2,
            "sanitized_title": "theory_of_mind_may_have_spontaneously_emerged_in_large_language_models"
        },
        {
            "paper_title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
            "rating": 2,
            "sanitized_title": "boosting_theoryofmind_performance_in_large_language_models_via_prompting"
        },
        {
            "paper_title": "Sparks of Artificial General Intelligence: Early experiments with GPT-4",
            "rating": 2,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT",
            "rating": 1,
            "sanitized_title": "will_affective_computing_emerge_from_foundation_models_and_general_ai_a_first_evaluation_on_chatgpt"
        },
        {
            "paper_title": "Administration of the text-based portions of a general IQ test to five different large language models",
            "rating": 1,
            "sanitized_title": "administration_of_the_textbased_portions_of_a_general_iq_test_to_five_different_large_language_models"
        }
    ],
    "cost": 0.022286749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Emotional Intelligence of Large Language Models</p>
<p>Xuena Wang 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>Xueting Li 
Department of Psychology
Renmin University of China</p>
<p>Zi Yin 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>Yue Wu 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>&amp; Liu 
Department of Psychology &amp; Tsinghua Laboratory of Brain and Intelligence
Tsinghua University</p>
<p>Emotional Intelligence of Large Language Models
51E780518DA5969F098747A04075F340Emotional IntelligenceEmotional UnderstandingLLMhuman-likeness
Large Language Models (LLMs) have demonstrated remarkable abilities across numerous disciplines, primarily assessed through tasks in language generation, knowledge utilization, and complex reasoning.However, their alignment with human emotions and values, which is critical for real-world applications, has not been systematically evaluated.Here, we assessed LLMs' Emotional Intelligence (EI), encompassing emotion recognition, interpretation, and understanding, which is necessary for effective communication and social interactions.Specifically, we first developed a novel psychometric assessment focusing on Emotion Understanding (EU), a core component of EI, suitable for both humans and LLMs.This test requires evaluating complex emotions (e.g., surprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite feeling underperformed, John surprisingly achieved a top score).With a reference frame constructed from over 500 adults, we tested a variety of mainstream LLMs.Most achieved above-average EQ scores, with GPT-4 exceeding 89% of human participants with an EQ of 117.Interestingly, a multivariate pattern analysis revealed that some LLMs apparently did not reply on the human-like mechanism to achieve human-level performance, as their representational patterns were qualitatively distinct from humans.In addition, we discussed the impact of factors such as model size, training method, and architecture on LLMs' EQ.In summary, our study presents one of the first psychometric evaluations of the human-like characteristics of LLMs, which may shed light on the future development of LLMs aiming for both high intellectual and emotional intelligence.Project website: https://emotional-2 intelligence.github.io/</p>
<p>Introduction</p>
<p>Imagine an ancient male making a necklace from a pile of shells as a gift for a female.This endeavor would require at least two distinct types of abilities.First, he would need the foresight to conceptualize that if a hole were punched in each shell and a string threaded through these holes, the shells could form a necklace.Second, he must possess a rudimentary level of empathy, inferring that the female recipient of the necklace would likely experience joy.The former ability is a manifestation of the Systemizing Mechanism (Baron-Cohen, 2020), enabling humans to become the scientific and technological masters of our physical world.The latter, on the other hand, is referred to as Emotional Intelligence (EI), which allows us to think about our own and others' thoughts and feelings, thereby aiding us in navigating the social world (Mayer, Perkins, et al., 2001;Mayer, Salovey, et al., 2001;Salovey &amp; Mayer, 1990).</p>
<p>In recent years, Large Language Models (LLMs) have made substantial strides, showcasing their expertise across multiple disciplines including mathematics, coding, visual comprehension, medicine, law, and psychology (Bubeck et al., 2023).Their impressive performance in logic-based tasks implies that LLMs, such as GPT-4, might be equipped with the Systemizing Mechanism comparable to human intelligence.Indeed, GPT-4 outperformed 99% of human participants in a modified text-based IQ test, a feat aligning with the elite MENSA level of general intelligence (King, 2023).</p>
<p>In contrast, investigations into the empathy of LLMs are relatively scarce and less systematic.Previous studies have mainly used the Theory of Mind (ToM) task, which measures the ability to understand and interpret others' mental states.LLMs launched before 2022 showed virtually no ability of ToM (Kosinski, 2023;Sap et al., 2023), whereas more recent models have shown significant improvement.For example, LLM "text-davinci-002" (January 2022) achieved an accuracy of 70%, comparable to that of six-year-old children, while LLM "text-davinci-003" (November 2022) reached 93%, on pair with seven-year-old children (Kosinski, 2023).Specifically, the most advanced model, GPT-4, attained 100% accuracy with in-context learning (Moghaddam &amp; Honey, 2023).While the ToM task provides valuable insights, it is not suitable to serve as a standardized test on EI for two reasons.First, ToM is a heterogeneous concept, spanning from false belief, the understanding that others can hold beliefs about the world that diverge from reality (Baron-Cohen et al., 1985), to pragmatic reasoning, the ability to incorporate contextual information and practical considerations when solving problems in real-world situations (Sperber &amp; Wilson, 2002).Consequently, the heterogeneous nature of the ToM task may not meet the reliability and validity standards of psychometric tests.Second, the ToM task is simple for a typical human participant in general, rendering it more suitable to serve as a diagnostic tool for EI-related disorders such as the autism spectrum disorder rather than a discriminative test for general population.Consequently, standardized tests on EI, such as Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT, Mayer et al., 2003), do not include the ToM task.</p>
<p>According to EI theories (Mayer et al., 2016;Mayer &amp; Salovey, 1995;Salovey &amp; Mayer, 1990), emotion understanding (EU) is a fundamental component of EI, which serves as a subscale in MSCEIT.EU refers to the ability to recognize, interpret, and understand emotions in a social context, which lays the groundwork for effective communication, empathy, and social interaction (Mayer et al., 2016).Specifically, the test on EU is suitable for measuring the empathy of LLMs because they do not possess internal emotional states or experiences, and therefore they have to rely solely on accurately understanding and interpreting the social context to create more engaging and empathetic interactions.</p>
<p>In this study, we first developed a standardized EU test suitable for both humans and LLMs, termed the Situational Evaluation of Complex Emotional Understanding (SECEU).Data from more than 500 young adults were collected to establish a norm for the SECEU.Then, we evaluated a variety of mainstream and popular LLMs, including OpenAI GPT series (GPT-4, Curie,Babbage,DaVinci,, Claude, LLaMA-based models (Alpaca, Koala, LLaMA, and Vicuna), Fastchat, Pythia-based models (Dolly and Oasst), GLM-based models (ChatGLM), and RWKV (Recurrent Weighted Key-Value) with the SECEU.Finally, we standardized the LLMs' scores against the norm, allowing for direct comparison with humans.We also compared the multivariate response patterns of the LLMs and human participants to compare their representation similarities.</p>
<p>Results</p>
<p>The development of a standardized test on EU</p>
<p>The SECEU is designated to measure EU, which comprises 40 items (see https://emotional-intelligence.github.io/for both English and Chinese versions).Each item describes a scenario set either in a school, family, or social context with twists and turns designed to evoke a mixture of positive and negative emotions (e.g., "Wang participated in a mathematics competition but felt he had not performed to his full potential.However, when the results were announced, he found that he was in a position of top 10.").The scenarios feature a varying number of characters, and emotions can be self-directed, other-directed, or both.For each scenario, four of the most plausible emotions (e.g., surprised, joyful, puzzled, proud) are listed.Participants were asked to evaluate the intensity of each emotion with numbers that were added up to 10 (e.g., 3, 3, 1, 3, indicating a multifaceted emotion response comprising 30% surprise, 30% joy, 10% puzzlement, and 30% pride).Fig. 1 shows exemplars of the SECEU test and the standard scores by averaging answers across the participants.Under the assumption that groups possess collective knowledge of emotion (Legree et al., 2005), we adopted a consensus scoring method to standardize the SECEU (Palmer et al., 2005).To do this, we administered the SECEU to a large sample of undergraduate and postgraduate students (N = 541; females: 339, males: 202; mean age: 22.33, SD: 2.49, ranging from 17 to 30 years).Then, we calculated the standard score for each emotion of each item by averaging corresponding scores across the participants.</p>
<p>Accordingly, we measured each participant's EU ability on each item by calculating the Euclidean distance between the participant's individual score and the standard score derived from the whole group for each item, with smaller distances indicating better EU ability.This analysis revealed significant variance in individual differences in EU ability (M = 2.79, SD = 0.822, range from 1.40 to 6.29), suggesting that the SECEU is well-suited to serve as a discriminative test for assessing EU in a general population.</p>
<p>To evaluate the reliability of the SECEU, we assessed the internal consistency of participants' performance (i.e., the Euclidean distance) across the 40 items, and revealed a high reliability of the test (Cronbach's α = 0.94).We further examined the distribution of participants' performance on each item (Fig. S1) and found no evidence of ceiling or floor effects, with mean distances varying from 2.19 to 3.32 and SD ranging from 1.13 to 1.82.In addition, there was no significant sex difference (male: 2.85, female: 2.76, t(539) = 1.34, p = 0.18).</p>
<p>To evaluate the validity of the SECEU, we invited three experts known for their high EI to take the test.All experts' performance exceeded at least 73% of the population, indicating that the test is effective in differentiating experts from the general population.Specifically, the average score of the three experts exceeded 99% of the whole population tested, further confirming the validity of using the consensus scoring method in standardizing the SECEU.</p>
<p>Finally, we constructed the norm for EU by converting participants' raw scores in the SECEU into standard EQ (Emotional Quotient) scores, designed to follow a normal distribution with the average score set at 100 and standard deviation at 15.In practical terms, an individual with an EQ of 100 possesses an EU ability corresponding to the population average.Meanwhile, an individual with an EQ of 115 outperforms approximately 84% of the population (i.e., one SD above the population average), and an individual with an EQ score of 130 exceeds 97.7% of the population.</p>
<p>The assessment of LLMs' EQ</p>
<p>We evaluated a variety of mainstream LLMs using the SECEU, and then standardized their scores based on the norm of the human participants for a direct comparison between LLMs and humans.These LLMs included OpenAI GPT series (GPT-4, Curie,Babbage,DaVinci,, Claude, as well as open-source models such as LLaMA-based models (Alpaca, Koala, LLaMA, and Vicuna), Pythia-based models (Dolly and Oasst), GLMbased models (ChatGLM), and Fastchat.Recurrent Weighted Key-Value (RWKV), which utilizes recurrent neural networks (RNNs) instead of transformers, was also included.Some models, including LLaMA, Fastchat, and RWKV-v4, were unable to complete the test even with the assistance of prompts (Table 1).A few LLMs, including DaVinci, Curie, Babbage, text-davinci-001, and text-davinci-002 managed to complete the test with prompts such as Two-shot Chain of Thought (COT) and Step-by-Step prompts (See Supplementary for the prompt engineering).In addition, other models, such as text-davinci-003 was able to complete the test but its performance was significantly improved with prompts.Here, we only included models' best performance to examine how closely they can approach human-level performance under ideal conditions (Table 1; see also Table S1 &amp; S2).To directly compare to human participants, the performance of each model was standardized by calculating the Euclidean distance between the model's responses and the standard scores of humans, which was then normalized into an EQ score (Table 1).Finally, these LLMs were categorized as either expert (above 115), normal (between 85 and 115), or poor (below 85) based on their EQ scores.EQ scores, with the y-axis indicating the EQ score and the x-axis showing the percentage of total participants.The grey kernel density estimation (KDE) line demonstrates the probability density of the EQ scores.Key points are highlighted with colored square markers for LLMs (e.g., GPT-4's EQ score is 117, marked by the purple square, exceeding 89% of the human participants).For simplicity, here we only present the performance from GPT-4, Vicuna, GPT-3.5-turbo,ChatGLM, and Koala.The results revealed a substantial variation in EU among the LLMs tested (Fig. 2).</p>
<p>Within the OpenAI GPT series, GPT-4 achieved the highest EQ of 117, exceeding 89% of humans.In contrast, DaVinci scored the lowest, with an EQ of 87, only outperforming 18% of humans.</p>
<p>The LLaMA-based models generally scored lower than the OpenAI GPT series, with Alpaca and Vicuna achieving the highest EQ of 104 and 105, respectively.</p>
<p>Conversely, Koala showed the poorest performance, with an EQ score of 83, only surpassing 13% of humans.The base model LLaMA was unable to complete the test.</p>
<p>Other models, such as Oasst (EQ: 107), Dolly (EQ: 98), ChatGLM (EQ: 94), and Claude (EQ: 106), all fell within the normal range.</p>
<p>In short, the majority of the LLMs tested showed satisfactory EU scores, comparable to those of the average human population.Specifically, GPT-4 reached the expert level of humans.</p>
<p>The assessment of LLMs' representational pattern</p>
<p>The measurement of the LLMs' EQ scores provides an index of their EU ability within the reference frame of humans.A further question is whether they employ human-like mechanisms to evaluate complex emotions in scenarios.The univariate analysis used to compare EQ scores between human participants and LLMs only suggests weak equivalence, as a model could achieve a high EQ score using mechanisms that qualitatively differ from humans.Therefore, to establish strong equivalence between the LLMs and humans, we examined whether they employed similar representations to conduct the test.</p>
<p>One approach is to use the item-wise correlation analysis (Izard &amp; Spelke, 2009;Tian et al., 2020) to compare response patterns between the LLMs and human participants.To do this, we first constructed a multi-item discriminability vector (i.e., an item-wise response pattern) for each participant by using the distance of each item to the standard score, and thus this vector's length corresponded to the number of items (i.e., 40).Then, we created a template of response patterns by averaging the multi-item discriminability patterns across all human participants, along with the distribution of the correlation coefficients between each participant's response pattern and the template (Human-to-Human similarity) to serve as a norm for pattern similarity (M = 0.199, SD = 0.166).Finally, we quantified the similarity in response patterns between the LLMs and humans by calculating the correlation coefficient between the multi-item discriminability vector of each LLM and the human template (LLM-to-Human, Table 1).An LLM that has an LLM-to-Human correlation coefficient one SD deviation below the mean of Human-to-Human distribution is considered as employing a qualitatively different mechanism from humans in EU.Surprisingly, despite its lower performance in the SECEU, Koala showed the highest similarity in representational patterns to humans (r = 0.43, p &lt; 0.01, exceeding 93% of human participants) (Fig. 3).This suggests that Koala may represent emotions in the same way as humans do, as it performed well on items where humans excelled and struggled on items where humans faced challenges.That is, the discrepancies in understanding emotions between Koala and humans are rather quantitative than qualitative.On the other hand, the representational patterns of models such as Babbage, text-davinci-002, Alpaca, and Vicuna differed qualitatively from humans' representational patterns (Babbage: r = -0.12,&gt; 4%; text-davinci-002: r = -0.04,&gt; 8%;</p>
<p>Alpaca: r = 0.03, &gt; 15%; Vicuna: r = -0.02,&gt; 10%).This suggests that, despite their above-average EQ scores, these models likely employed mechanisms that are qualitatively different from human processes.</p>
<p>GPT-4, the most advanced model to date, showed high similarity in representational pattern (r = 0.28, &gt; 67%).This result implies that GPT-4 may have significantly changed its architecture or implemented novel training methods to align its EU ability more closely to humans.Interestingly, prompts apparently played a critical role in improving representational similarity.With two-shot COT prompts,</p>
<p>DaVinci and text-davinci-003 showed high similarity in representational pattern to humans (Davinci: r = 0.41, p &lt; 0.01, &gt; 91%; text-davinci-003: r = 0.31, p &lt; 0.05, &gt; 73%), higher than that of GPT-4.Note that without prompts, they failed to complete the SECEU test.In contrast, prompts had little effect on GPT-4 and ChatGPT-3.5.</p>
<p>Discussion</p>
<p>Since the debut of ChatGPT, a great number of tasks and benchmarks have been developed to examine the capacities.These empirical evaluations and analyses mainly focus on language generation (e.g., conditional text generation), knowledge utilization (e.g., closed-book and open-book QAs), and complex reasoning (e.g., symbolic and mathematical reasoning) (Zhao et al., 2023).However, tests on human alignment of LLMs to human values and needs, a core ability for the broad use of LLMs in the real world, are relatively scarce.Here in this study, we used traditional psychometric methods to develop a valid and reliable test on emotional understanding, the SECEU, to evaluate the EI of LLMs.We found that a majority of the LLMs tested performed satisfactorily in the test, achieving above-average EQ scores, although significant individual differences were present across the LLMs.Critically, some LLMs apparently did not reply on the human-like representation to achieve human-level performance, as their representational patterns diverged significantly from human patterns, suggesting a qualitative difference in the underlying mechanisms.In summary, our study provides the first comprehensive psychometric examination of the emotional intelligence of LLMs, which may shed light on the development of future LLMs that embody high levels of both intellectual and emotional intelligence.Various factors appear to influence the EQ scores of the LLMs (Fig. 4).The most conspicuous one is the model size, which is essential to emergent abilities (Bubeck et al., 2023), making AI algorithms unprecedently powerful and effective.While the larger models generally scored higher in the test, certain smaller models such as Oasst and Alpaca still managed to achieve satisfactory EQ scores.This suggests that factors beyond the mere size may have a more profound influence on models' EU.</p>
<p>The effectiveness of various training methods, such as supervised training, reinforcement learning, self-supervised learning, and a combination thereof, likely substantially influences the EQ scores.For example, despite architectural differences (Pythia versus LLaMA), Oasst and Alpaca yielded similar scores in the test, demonstrating the potential of well-implemented fine-tuning techniques.In fact, these enhancements may be achieved through two main avenues.The first involves supervised fine-tuning (SFT), which allows for more structured and targeted fine-tuning, thereby improving models' linguistic ability and their grasp of contextual nuances (Köpf et al., 2023;Taori et al., 2023a).The other approach employs reinforcement learning from human feedback (RLHF), enabling the models to learn from human insights and thereby fostering more human-like responses.Indeed, there is a giant leap in EU seen between text-davinci-002 (&gt;23%) to text-davinci-003 (&gt;83%), two different versions of the same model with the latter employing RLHF.</p>
<p>Another influential factor is the model architecture.Models using the Transformer architecture, such as the GPT series and the LLaMA-based models, generally performed well in this test.In contrast, models using RNNs, such as RWKV-v4, failed to complete the test even with the help of various prompts.Besides, within the Transformer architecture, the "decoder-only" or causal decoder models (e.g., the GPT series), which generate sequences based solely on a self-attention mechanism (Brown et al., 2020;Vaswani et al., 2017), outperformed the "encoder-decoder" models (e.g., Fastchat-t5), which incorporate an extra step to interpret input data into meaningful representations (Devlin et al., 2019;Zheng et al., 2023).</p>
<p>In summary, our study provides novel evaluation on the human-like characteristics of LLMs, along with the tests on self-awareness (Kosinski, 2023) and affective computing (Amin et al., 2023).However, because only a limited number of LLMs were tested in this study (results on more LLMs will be continuously updated in https://emotional-intelligence.github.io/),our findings are biased and inconclusive.</p>
<p>Further, there are more questions that need to be explored in future studies.First, this study focused solely on the EU ability of the LLMs, while EI is a multi-faceted construct encompassing not only EU but also emotion perception, emotion facilitation, and emotion management (e.g., Mayer et al., 2016;Mayer &amp; Salovey, 1995;Salovey &amp; Mayer, 1990).Therefore, future studies could design scenarios to examine whether LLMs can assist humans in leveraging emotions to facilitate cognitive processes and effectively manage their emotional responses.</p>
<p>Second, EI requires the integration of various facets to execute complex tasks, which necessitate not only an understanding of emotions, but also the comprehension of thoughts, beliefs, and intentions.Future studies should adopt broader scope assessments, akin to ToM tests, while avoiding their lack of discriminative power.</p>
<p>Besides, with recent advancements, LLMs are now capable of processing multimodal information (Wang et al., 2023).Therefore, future studies should investigate how LLMs interpret complex emotions from multimodal inputs, such as text combined with facial expressions or the tone of voice.In short, tests that combine emotions with cognitive factors based on multimodal clues likely furnish a more comprehensive understanding of LLMs' EI, which is critical for LLMs' effective and ethically responsible deployment in real-world scenarios of mental health, interpersonal dynamics, work collaboration, and career achievement (e.g., Dulewicz &amp; Higgs, 2000;Hanafi &amp; Noor, 2016;Lea et al., 2019;Mayer et al., 2016;McCleskey, 2014;Warwick &amp; Nettelbeck, 2004).</p>
<p>Finally, while the factors examined in this study contribute to our standing of LLM's EU, they are largely descriptive and thus do not establish causal relationships.</p>
<p>With the recent progress of open-source LLMs (Bai et al., 2022;Chiang et al., 2023;Conover et al., 2023;Geng et al., 2023;Köpf et al., 2023;Peng et al., 2023;Taori et al., 2023b;Touvron et al., 2023;Zeng et al., 2022;Zheng et al., 2023), direct manipulation of the potentially influential factors, such as training approaches and model size, has become plausible.Such manipulations will facilitate the establishment of causal relationships between these factors and models' EI ability, offering valuable insights for the development of future LLMs with better EI.</p>
<p>emotion options based on the intensity of each emotion experienced by the person in the scenario.There were no correct or incorrect answers.Participants were encouraged to respond according to their own understanding and interpretation of the scenarios.</p>
<p>The SECEU test for LLMs</p>
<p>A variety of mainstream LLMs, including the OpenAI GPT series (GPT-4, GPT-3.5turbo,Curie, Babbage, DaVinci, text-davinci-001, text-davinci-002, and text-davinci-003), Claude, LLaMA-based models (Alpaca, Koala, LLaMA, and Vicuna), Fastchat, Pythia-based models (Dolly and Oasst), GLM-based models (ChatGLM), and RNNbased models (RWKV), were evaluated by the SECEU test.Given that the majority of these models are primarily trained on English datasets, using the English version of the SECEU provides a more accurate assessment of their performance, allowing for a clearer comparison between their abilities and those of a human.As a result, the English version of SECEU was presented to the LLMs instead of the Chinese version.</p>
<p>The task was in a direct question-and-answer format.We asked, for example, "Story: Wang participated in a mathematics competition but felt he had not performed to his full potential.However, when the results were announced, he found that he was in a position of top 10.He would feel: Options: (1) Surprised; (2) Joyful; (3) Puzzled;</p>
<p>(4) Proud.Assign a score to each option based on the story, sum up to 10".There could be very subtle changes of the direct prompt.For instance, we used "provide a score for each emotion based on the emotion (sum of four options should be of 10 points)" for Dolly.There were a set of paraphrases of the direct prompt to get the best performance.</p>
<p>To decrease the randomness, a constant temperature parameter was set to 0.1 and the top_p parameter was set to 1 across all these models.To dictate the maximum length of the generated text, the max_tokens parameter was set to 512.</p>
<p>Before being processed by the models, text data underwent several preprocessing steps to normalize it.This normalization process ensures that data fed into the models is in a consistent and appropriate format, enhancing the output's quality.If a model did not provide any meaningful response to an item, the response for this item was predefined as a null vector (0, 0, 0, 0).A few models failed to generate a response for a majority of items (LLaMA: 40; Fastchat: 31; RWKV-v4: 31; DaVinci: 40; Curie: 40;</p>
<p>Babbage:40; text-davinci-001: 26; text-davinci-002: 28; marked as "FAILED" in Table S1).Several models were unable to provide the answer which the summation of the four scores was 10:</p>
<p>(i) Answer vectors signifying null responses, i.e., (0, 0, 0, 0), were preserved as such (Alpaca: 1; ChatGLM: 1).</p>
<p>(ii) For datasets encompassing negative values, an addition operation involving the absolute value of the lowest number was performed across all elements, followed by a subsequent normalization to maintain consistency with the original scale.For instance, an original data vector of (-4, -2, -2, 2) would be adjusted to (0, 2, 2, 6).</p>
<p>(iii) The remaining answer vectors were normalized to achieve a cumulative score of 10.This involved proportionally distributing a score of 10 among the answer vector based on the contribution of each value to the total score on this item.</p>
<p>LLMs' EQ</p>
<p>The standard score (a 40 × 4 symmetric matrix, see https://emotionalintelligence.github.io/for the standard score) for each emotion of each item in the SECEU test was calculated by averaging corresponding scores across the human participants.The performance of each LLM was standardized by calculating the Euclidean distance between the model's responses (LLM) and the standard scores of humans (SS) on each item i (from 1 to 40) and then averaged to yield the SECEU score.</p>
<p>Lower SECEU scores indicate greater alignment with human standards.
SECEU score = 1 40 � �(LLM i1 − SS i1 ) 2 + (LLM i2 − SS i2 ) 2 + (LLM i3 − SS i3 ) 2 + (LLM i4 − SS i4 ) 2 40 i=1
The SECEU score was then normalized into an EQ score which was designed to follow a normal distribution with the average score set at 100 and the standard deviation at 15.The standardization process involved the following steps: (1) the original SECEU score was subtracted from the mean value of the human norm and divided by its standard deviation, and (2) the resulting value was then multiplied by the new standard deviation (15) and added to the new mean value (100), yielding the EQ score.Thus, the EQ score represents a normalized measure of the LLM's EQ, permitting easier comparison across different models.</p>
<p>LLM ' s EQ = 15 × M − SECEU score SD + 100</p>
<p>LLMs' representational pattern</p>
<p>To establish strong equivalence between the LLMs and humans, we examined whether they employed similar representations to conduct the test.Item-wise correlation analysis (Izard &amp; Spelke, 2009;Tian et al., 2020) was applied to compare response patterns between the LLMs and human participants.The human template (a vector with a length of 40, see https://emotional-intelligence.github.io/for the human pattern template) was generated by averaging the multi-item discriminability patterns across all human participants, where each pattern was constructed based on the distance of each item to the standard score.The multi-item discriminability pattern of a specific LLM was also calculated based on the distance of each item i (from 1 to 40) to the standard scores of humans (SS).
Discriminability = ��(LLM i1 − SS i1 ) 2 , �(LLM i2 − SS i2 ) 2 , �(LLM i3 − SS i3 ) 2 , �(LLM i4 − SS i4 ) 2 �
We calculated the Pearson correlation coefficient between the discriminability pattern of each participant and the human template (Human-to-Human similarity).To avoid the inflation in calculating correlation, the template was constructed excluding the individual whose Human-to-Human correlation coefficient was calculated.The distribution of the Human-to-Human similarity served as a norm for pattern similarity.</p>
<p>The Pearson correlation coefficient between the discriminability pattern of each LLM and the human template was calculated as the LLM-to-Human similarity.Here, X i and Y i represent the item i of the "Discriminability" vector and the human template vector, respectively.The length of both vectors is 40.X � and Y � denote the mean of   and   , respectively.If the LLM-to-Human similarity is less than one SD below the population, such LLM is considered as employing a qualitatively different mechanism from humans in EU.</p>
<p>LLM</p>
<p>Prompt engineering</p>
<p>Prompt engineering-the meticulous development and choice of prompts-plays a pivotal role in the efficacy of LLMs (Hebenstreit et al., 2023;Hendrycks et al., 2021;Nair et al., 2023;OpenAI, 2023;Shinn et al., 2023).In essence, prompt engineering refers to the strategy of designing and selecting prompts that can substantially guide and influence the responses of LLMs.The necessity of prompt engineering lies in its potential to enhance the precision and relevance of the responses generated by these models, thereby leading to more effective and reliable outcomes.In the realm of emotional intelligence, prompts serve a crucial function.To decease the randomness, a constant temperature parameter was set to 0 and the top_p parameter to was set to 0.9 across all these models.To dictates the maximum length of the generated text, the max_tokens parameter was set to 2048.The normalization process was the same as the one without prompts.S2 shows the SECEU scores, EQ scores, pattern similarity, and properties of OpenAI GPT series with the assistance of prompt.Failed: The LLMs cannot complete the test.%: The percent of humans whose performance was below that of an LLM in the test.Pattern Similarity: The degree of similarity is indexed Pearson correlation coefficient (r).<em>: p &lt; 0.05; </em>*: p &lt; 0.01.</p>
<p>Figure 1 :
1
Figure 1: Exemplars of the SECEU test and the standard scores from the population.For the whole set of the test, see: https://emotional-intelligence.github.io/</p>
<p>Figure 2 :
2
Figure 2: LLMs' EQ.The light-grey histogram represents the distribution of human participants'</p>
<p>Figure 3 :
3
Figure 3: The pattern similarity between LLMs and humans.The light-grey histogram represents the distribution of Human-to-Human pattern similarity, with the y-axis indicating the Pearson correlation coefficients and the x-axis showing the percentage of total participants.The KDE line</p>
<p>Figure 4 :
4
Figure 4: The family tree of LLMs.Each node in the tree represents an LLM, whose vertical position along the x-axis indicates the launch time.The size of each node corresponds to the parameter size of the LLM.Note that the size of GPT-4 and Claude was estimated based on publicly available information.Color donates the EQ scores, with red color for higher scores and blue color for lower scores.Note that white color shows that models failed to conduct the SECEU.The color of the branches distinguishes between open-source (light gray) and closed-source (dark gray) models.</p>
<p>− to − Human similarity Pearson = ∑ (  −  � )(  −  � )</p>
<p>The variance in response to different prompting techniques among models emphasizes the importance of a deeper understanding of factors such as model architecture, training data, fine-tuning process, and optimization objectives.The interplay of these factors might influence a model's receptiveness and response to different prompting techniques.Looking forward, there is a need for further exploration into the impact of various types of prompts on LLMs during emotional intelligence tests, including the investigation of more diverse categories of prompts or hybrid prompts.In-depth studies into why certain models respond more favorably to specific prompts can also inform the development of more advanced LLMs with superior human emotional understanding capabilities.These studies could also provide valuable insights for optimizing the instructive fine-tuning process and the application of Reinforcement Learning from Human Feedback (RLHF) techniques.Ultimately, enhancing our understanding of the relationship between prompts and LLM performance in emotional intelligence tests can significantly contribute to the ongoing evolution and ------FAILED------Zero-Shot + Step-by-step ------FAILED------</p>
<p>Figure S1 :
S1
Figure S1: The distribution of individual performance (i.e., the Euclidean distance between the individual's answer and the objective standards) on each item (M±SD)</p>
<p>Table 1 :
1
LLMs' EQ, representational patterns, and properties
Based ModelsSECEU scoreEQEQ%Pattern Similarity r %SizeProperties TimeSFT RLHFOpenAI GPT seriesDaVinci #3.587 18% 0.41<strong>91%175B2020/05××Curie #2.7102 50%0.1129%13BUnknown××Babbage #2.78100 44%-0.124%3BUnknown××text-davinci-001 #2.4107 64%0.247%&lt;175BUnknown××text-davinci-002 #3.391 23%-0.048%&lt;175BUnknown√×text-davinci-003 ##2.01114 83%0.31*73%175B2022/11/28√√GPT-3.5-turbo2.63103 52%0.0417%175B2022/11/30√√GPT-41.89117 89%0.2867%Unknown 2023/03/14√√LLaMALLaMA------FAILED------13B2023/02/24××Alpaca2.56104 56%0.0315%13B2023/03/09√×Vicuna2.5105 59%-0.0210%13B2023/03/30√×Koala3.7283 13% 0.43</strong>93%13B2023/04/03√×Flan-t5Fastchat------FAILED------3B2023/04/30√×PythiaDolly2.8998 38%0.2662%13B2023/04/12√×Oasst2.41107 64%0.2459%13B2023/04/15√√GLMChatGLM3.1294 28%0.0924%6B2023/03/14√√RWKVRWKV-v4------FAILED------13B2023/02/15√×ClaudeClaude2.46106 61%0.1128%Unknown 2023/03/14√√</p>
<p>Table 1
1
Footnote: Table1shows the SECEU scores, EQ scores, representational pattern similarity, and properties of mainstream LLMs evaluated in the current study.</p>
<h1>: models require prompts to complete the test.##: models' performance benefits from prompts.Failed: even with prompts, the LLMs cannot complete the test.%: The percent of humans whose performance was below that of an LLM in the test.Pattern Similarity: The degree of similarity is indexed Pearson correlation coefficient (r).<em>: p &lt; 0.05; </em>*: p &lt; 0.01.Size: The parameter size of LLMs in the unit of billions (B).Time: The launch time in the format YYYY/MM/DD.SFT: Supervised fine-tune; RLHF: Reinforcement learning from human feedback; √: yes; ×: no.</h1>
<p>Geng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P.,Levine, S., &amp; Song, D. (2023).
Koala:ADialogueModelforAcademicResearch.https://bair.berkeley.edu/blog/2023/04/03/koala/Hanafi, Z., &amp; Noor, F. (2016). Relationship between Emotional Intelligence andAcademic Achievement in Emerging Adults: A Systematic Review.International Journal of Academic Research in Business and Social Sciences,6(6), Pages 268-290. https://doi.org/10.6007/IJARBSS/v6-i6/2197Hebenstreit, K., Praas, R., Kiesewetter, L. P., &amp; Samwald, M. (2023). An automaticallydiscovered chain-of-thought prompt generalizes to novel models and datasets(2021). Measuring Massive Multitask Language Understanding(arXiv:2009.03300). arXiv. http://arxiv.org/abs/2009.03300Izard, V., &amp; Spelke, E. S. (2009). Development of Sensitivity to Geometry in VisualForms. Human Evolution, 23(3), 213-248.King, M. (2023). Administration of the text-based portions of a general IQ test to fivedifferentlargelanguagemodels[Preprint].https://doi.org/10.36227/techrxiv.22645561.v1Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum,A., Duc, N. M., Stanley, O., Nagyfi, R., &amp; others. (2023). OpenAssistantConversations-Democratizing Large Language Model Alignment. ArXivPreprint ArXiv:2304.07327.
(arXiv:2305.02897).arXiv.http://arxiv.org/abs/2305.02897Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., &amp; Steinhardt, J.</p>
<p>Table S2
S2
Footnote: Table</p>
<p>AcknowledgementsThis study was funded by Shuimu Tsinghua Scholar Program (X.W.), Beijing Municipal Science &amp; Technology Commission, Administrative Commission of Zhongguancun Science Park (Z221100002722012), and Tsinghua University Guoqiang Institute (2020GQG1016).Declaration of conflicting interestsThe author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.Data availabilityThe SECEU test (both English and Chinese Versions), the code for the test on human participants, the standardized scores, the norm, and the prompts are available at https://emotional-intelligence.github.io/.The raw data of human participants are available from the corresponding author upon reasonable request.Contribution of authorsJ.L. conceived the study and provided advice.X.L. developed the SECEU test, and X.L. and X.W. translated it into English.Z.Y. built the online SECEU test, and X.W. carried out the SECEU test for human participants, built the norm of EQ, and analyzed the data.Y.W. performed the SECEU test for LLMs, and Z. Y. wrote the prompts.X.W. and J.L. wrote the manuscript with suggestions and revisions from X.L., Z.Y., and Y.W..MethodsParticipantsA total of five hundred and forty-one human participants with valid responses were collected in this study.The participants (N = 541; females: 339, males: 202; mean age: 22.33, SD: 2.49, ranging from 17 to 30 years) were all undergraduate and postgraduate college students in China.Informed consent was obtained prior to the SECEU test and participants were reimbursed after they completed the whole test.To ensure anonymity and data privacy, participants did not input any information that could identify them during the process.This study was approved by the Institutional Review Board at Tsinghua University.We also invited three experts to take the SECEU test.Expert 1 is an accomplished Human Resources professional who has over 20 years of experience in navigating human emotions within diverse work environments, strengthening her discernment in emotional intelligence.Expert 2 is a renowned figure in psychometrics and her expertise in creating tests assessing psychological variables lends exceptional rigor to our process.Expert 3 is an associate professor of psychology, whose deep understanding of human emotions, backed by her extensive academic achievements, makes her especially suitable for this test.ProcedureThe SECEU test for human participantsThe online SECEU test was built on the JATOS platform(Lange et al., 2015)based on the Jspsych plugin(de Leeuw et al., 2023), which was written in the React Framework (https://reactjs.org/).Each item was presented to the participants with a scenario and followed by four emotion options (40 items in total, see https://emotionalintelligence.github.io/for both English and Chinese versions).Participants were instructed to read the scenario and then allocate a total of 10 points across the fourSupplementary Prompt engineeringSee TableS2for the results of prompt engineering.The majority of LLMs were unable to complete the task without the use of Twoshot Chain of Thought prompts.This could be due to the inherent limitations of the models in long-term memory and context understanding, necessitating such prompts to maintain continuity and consistency in emotional responses.The only exception was GPT-3.5-turbo, which effectively utilized Zero-shot prompts, achieving a notable EQ score of 94.This success could be attributed to the model's architecture, the training data used, and the fine-tuning process, which likely enhanced its ability to understand and generate emotionally intelligent responses with minimal guidance.In terms of Step-by-Step Thinking prompts, they did not improve the performance of DaVinci, Curie, and Babbage.The likely reason is that these models have not undergone instruct fine-tuning, and therefore, cannot effectively understand or respond to step-by-step prompts.Additionally, we noticed that Step-by-Step Thinking prompts also did not improve the performance of text-davinci-002, even though it is based on GPT-3.5-turbo.As there are no publicly available details about this model, we speculate, based on third-party information, that as an intermediate state model, its optimization objective might have reduced its capability to follow instructions.However, Step-by-Step prompts had a pronounced impact on GPT-3.5-turbo, which increased the correlation between humans and the model, indicating substantial progress in the model's ability to mimic human emotional understanding and thought processes.The combination of Two-shot Chain of Thought Reasoning and Step-by-StepThinking prompts did not lead to higher EQ scores for models like GPT-3.5-turbo, textdavinci-001, and text-davinci-003.However, it did result in increased pattern similarity.This result aligns with the official statements from OpenAI about the impact of instruct fine-tuning and RLHF techniques in making models' responses more human-like.It also suggests that these models have the potential to master patterns of emotional understanding that are similar to those used by humans.TableS1Footnote: TableS1shows the SECEU scores, EQ scores, pattern similarity, and properties of mainstream LLMs which we evaluated in the current study under the direct prompts.evaluated in the current study.Failed: The LLMs cannot complete the test.%: The percent of humans whose performance was below that of an LLM in the test.Pattern Similarity: The degree of similarity is indexed Pearson correlation coefficient (r).<em>: p &lt; 0.05; </em>*: p &lt; 0.01.
M M Amin, E Cambria, B W Schuller, 10.48550/ARXIV.2303.03186Will Affective Computing Emerge from Foundation Models and General AI? A First Evaluation on ChatGPT. 2023</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, C Chen, C Olsson, C Olah, D Hernandez, D Drain, D Ganguli, D Li, E Tran-Johnson, E Perez, J Kaplan, arXiv:2212.08073Constitutional AI: Harmlessness from AI Feedback. 2022</p>
<p>The pattern seekers: How autism drives human invention. S Baron-Cohen, 2020Basic Books</p>
<p>Does the autistic child have a "theory of mind. S Baron-Cohen, A M Leslie, U Frith, 10.1016/0010-0277Cognition. 2111985</p>
<p>Language models are few-shot learners. T Brown, &amp; others.B Mann, &amp; others.N Ryder, &amp; others.M Subbiah, &amp; others.J D Kaplan, &amp; others.P Dhariwal, &amp; others.A Neelakantan, &amp; others.P Shyam, &amp; others.G Sastry, &amp; others.A Askell, &amp; others.Advances in Neural Information Processing Systems. 202033</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, H Nori, H Palangi, M T Ribeiro, Y Zhang, 10.48550/ARXIV.2303.12712Sparks of Artificial General Intelligence: Early experiments with GPT-4. 2023</p>
<p>W.-L Chiang, Z Li, Z Lin, Y Sheng, Z Wu, H Zhang, L Zheng, S Zhuang, Y Zhuang, J E Gonzalez, I Stoica, E P Xing, Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. 2023</p>
<p>. M Conover, &amp; others.M Hayes, &amp; others.A Mathur, &amp; others.X Meng, &amp; others.J Xie, &amp; others.J Wan, &amp; others.S Shah, &amp; others.A Ghodsi, &amp; others.P Wendell, &amp; others.M Zaharia, &amp; others.2023Free dolly: Introducing the world's first truly open instruction-tuned llm</p>
<p>jsPsych: Enabling an opensource collaborative ecosystem of behavioral experiments. J R De Leeuw, R A Gilbert, B Luchterhandt, 10.21105/joss.05351Journal of Open Source Software. 88553512023</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, arXiv:1810.04805BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. 2019</p>
<p>Emotional intelligence -A review and evaluation study. V Dulewicz, M Higgs, 10.1108/02683940010330993Journal of Managerial Psychology. 1542000</p>
<p>M Kosinski, arXiv:2302.02083Theory of Mind May Have Spontaneously Emerged in Large Language Models. 2023</p>
<p>Just Another Tool for Online Studies" (JATOS): An Easy Solution for Setup and Management of Web Servers Supporting Online Studies. K Lange, S Kühn, E Filevich, 10.1371/journal.pone.0130834PLOS ONE. 1062015</p>
<p>Does Emotional Intelligence Buffer the Effects of Acute Stress? A Systematic Review. R G Lea, S K Davis, B Mahoney, P Qualter, 10.3389/fpsyg.2019.00810Frontiers in Psychology. 108102019</p>
<p>Using Consensus Based Measurement to Assess Emotional Intelligence. P J Legree, J Psotka, T Tremble, D R Bourne, 2005</p>
<p>The Ability Model of Emotional Intelligence: Principles and Updates. J D Mayer, D R Caruso, P Salovey, 10.1177/1754073916639667Emotion Review. 842016</p>
<p>Emotional intelligence and giftedness. J D Mayer, D M Perkins, D R Caruso, P Salovey, 10.1080/02783190109554084Roeper Review. 2332001</p>
<p>Emotional intelligence and the construction and regulation of feelings. J D Mayer, P Salovey, S0962-1849(05)80058-7Applied and Preventive Psychology. 431995</p>
<p>Emotional intelligence as a standard intelligence. J D Mayer, P Salovey, D R Caruso, G Sitarenios, 10.1037/1528-3542.1.3.232Emotion. 132001</p>
<p>Measuring emotional intelligence with the MSCEIT V2.0. J D Mayer, P Salovey, D R Caruso, G Sitarenios, 10.1037/1528-3542.3.1.97Emotion. 312003</p>
<p>Emotional intelligence and leadership: A review of the progress, controversy, and criticism. J Mccleskey, 10.1108/IJOA-03-2012-0568International Journal of Organizational Analysis. 2212014</p>
<p>Boosting Theory-of-Mind Performance in Large Language Models via Prompting. S R Moghaddam, C J Honey, 10.48550/ARXIV.2304.114902023</p>
<p>V Nair, E Schumacher, G Tso, A Kannan, arXiv:2303.17071DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. 2023</p>
<p>. Openai, arXiv:2303.087742023GPT-4 Technical Report</p>
<p>A psychometric evaluation of the Mayer-Salovey-Caruso Emotional Intelligence Test Version 2.0. Intelligence. B R Palmer, G Gignac, R Manocha, C Stough, 10.1016/j.intell.2004.11.003200533</p>
<p>. B Peng, E Alcaide, Q Anthony, A Albalak, S Arcadinho, H Cao, X Cheng, M Chung, M Grella, K K Gv, X He, H Hou, P Kazienko, J Kocon, J Kong, B Koptyra, H Lau, K S I Mantri, F Mom, R.-J Zhu, 2023RWKV: Reinventing RNNs for the Transformer Era</p>
<p>Emotional Intelligence. Imagination. P Salovey, J D Mayer, 10.2190/DUGG-P24E-52WK-6CDGCognition and Personality. 931990</p>
<p>M Sap, R Lebras, D Fried, Y Choi, arXiv:2210.13312Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs. 2023</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, F Cassano, B Labash, A Gopinath, K Narasimhan, S Yao, 10.48550/ARXIV.2303.113662023</p>
<p>Pragmatics, modularity and mind-reading. D Sperber, D Wilson, 10.1111/1468-0017.00186Mind &amp; Language. 171-22002</p>
<p>R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Alpaca: A strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. 2023a37</p>
<p>Stanford Alpaca: An Instruction-following LLaMA model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, GitHub repository. GitHub. 2023b</p>
<p>Multi-Item Discriminability Pattern to Faces in Developmental Prosopagnosia Reveals Distinct Mechanisms of Face Processing. X Tian, R Wang, Y Zhao, Z Zhen, Y Song, J Liu, 10.1093/cercor/bhz289Cerebral Cortex. 3052020</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, A Rodriguez, A Joulin, E Grave, G Lample, arXiv:2302.13971LLaMA: Open and Efficient Foundation Language Models. 2023</p>
<p>Attention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, Kaiser, I Polosukhin, Advances in Neural Information Processing Systems. 201730</p>
<p>Emotional intelligence is…?. J Warwick, T Nettelbeck, 10.1016/j.paid.2003.12.003Personality and Individual Differences. 3752004</p>
<p>A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, W L Tam, Z Ma, Y Xue, J Zhai, W Chen, P Zhang, Y Dong, J Tang, arXiv:2210.02414GLM-130B: An Open Bilingual Pre-trained Model. 2022</p>
<p>W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, Y Du, C Yang, Y Chen, Z Chen, J Jiang, R Ren, Y Li, X Tang, Z Liu, J.-R Wen, arXiv:2303.18223A Survey of Large Language Models. 2023</p>
<p>L Zheng, W.-L Chiang, Y Sheng, S Zhuang, Z Wu, Y Zhuang, Z Lin, Z Li, D Li, E P Xing, H Zhang, J E Gonzalez, I Stoica, Judging LLMas-a-judge with MT-Bench and Chatbot Arena. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
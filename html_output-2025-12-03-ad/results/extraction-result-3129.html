<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3129 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3129</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3129</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-afed6dc6900d3b37e528b9086661bba583d60bf6</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/afed6dc6900d3b37e528b9086661bba583d60bf6" target="_blank">Analysing Mathematical Reasoning Abilities of Neural Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> This paper conducts a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and finds notable differences in their ability to resolve mathematical problems and generalize their knowledge.</p>
                <p><strong>Paper Abstract:</strong> Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3129.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3129.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer (Attention Is All You Need architecture)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A character-level sequence-to-sequence Transformer used to map freeform math questions to freeform answers; uses multi-head self- and encoder-decoder attention with d_model=512, 8 heads, d_ff=2048.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder Transformer with embedding size d_model=512, h=8 attention heads, d_ff=2048, ~30M parameters; operates on character sequences for questions and answers and trained end-to-end on the synthetic mathematics dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Wide range: basic addition/subtraction, multi-term add/subtract, multiplication/division, mixed arithmetic with parentheses, multi-digit arithmetic, base conversion, polynomial evaluation/expansion/collection/composition/differentiation, factoring/primality-related tasks, sampling-without-replacement probability sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learns largely non-algorithmic, pattern-based or shallow-parallel computational heuristics using attention as a distributed internal memory; appears to compute some arithmetic in parallel (e.g. add/sub/mul modules) rather than by stepwise algorithmic iteration; effective internal representations for simultaneous coefficient manipulation in polynomials.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>1) High accuracy on pure add/sub and multiply/divide modules (Transformer: >~90% on 'add or subtract several numbers' and 'multiply or divide several numbers'). 2) Much lower accuracy (~50%) on mixed arithmetic with parentheses requiring explicit intermediate computations. 3) Superior performance on polynomial manipulation (expansion, collect, composition) consistent with ability to hold multiple coefficients in parallel. 4) Failure modes (missing one of repeated identical terms, brittle punctuation sensitivity) consistent with non-algorithmic heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some extrapolation successes (rounding bigger numbers, comparing more numbers, adding/subtracting larger magnitudes) indicate limited generalization beyond rote memorization; however overall behaviour argues against learned exact algorithmic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Model architecture choice (attention), bidirectional encoder vs unidirectional, 'thinking' padding steps (zero-input steps) before decoding, hyperparameter choices; no external calculators or chain-of-thought prompting used.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Bidirectional encoder slightly improved accuracy (attentional LSTM results reported similarly). Adding a number (up to 16) of 'thinking' steps before answer decoding increased performance for attentional recurrent models; Transformer inherently benefited from attention/parallel computation and outperformed recurrent models. No benefit from Adaptive Computation Time was reported (worse).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Average interpolation accuracy across modules: 0.76; extrapolation accuracy: 0.50 (Figure 3). >~90% on 'add or subtract several numbers' and 'multiply or divide several numbers' modules. ~50% on mixed arithmetic with parentheses. On a held-out set of 40 GCSE exam questions the trained Transformer got 14/40 correct (35%). Specific failure on repeated-summand test: correct for n<=6 of 1+1+...+1, incorrect (gave 6) at n=7.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>1) Fails tasks requiring stepwise intermediate computation (mixed arithmetic, bracketed expressions, polynomial evaluation via intermediate values). 2) Extrapolation to more terms or longer compositions often fails. 3) Missing one repeated identical term in long sums. 4) Plausible-looking but incorrect factorizations and prime-factoring errors. 5) Brittleness to minor syntactic variations (punctuation changes changed output).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>On a 40-question GCSE set the model achieved a human-equivalent E grade (14/40); compared to symbolic algorithms (exact arithmetic/computer algebra), the Transformer is far less reliable — it produces plausible but incorrect outputs and lacks guaranteed correctness; unlike symbolic methods it seems to rely on learned heuristics rather than exact algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysing Mathematical Reasoning Abilities of Neural Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3129.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3129.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Attentional LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encoder-Decoder LSTM with Attention (Attentional LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent encoder-decoder LSTM architecture with attention over encoder positions; encoder LSTM (512) and decoder LSTM (2048) used to map question character sequences to answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Attentional LSTM (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder with attention: encoder LSTM hidden size 512 (or bidirectional variant), decoder LSTM hidden size 2048, key/value sizes k=v=256, roughly 24–26M parameters depending on encoder; character-level input/output trained on the mathematics dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same broad range as Transformer: integer/decimal add/sub/mul/div, mixed arithmetic, polynomial tasks, number theory (primality/factors), probability (without-replacement), conversions, and composed function tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Relies on learned sequence-to-sequence mappings and attention to focus on relevant input tokens; appears to learn non-algorithmic, pattern-based solutions and sometimes uses attention context as working memory instead of explicit algorithmic manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Performance similar to simple LSTM on many tasks; increasing 'thinking' (zero-input) steps before decoding improved accuracy (suggests need for internal iterative integration). Attentional LSTM did not show evidence of learning true algorithmic parsing — authors speculate attention isn't being used to implement algorithmic steps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Attentional mechanism did not yield substantial algorithmic advantages over simple LSTM; comparable performance indicates attention did not enable discovered algorithmic procedures.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Attention mechanism, bidirectional encoder variant, insertion of fixed 'thinking' zero-input steps, experiment with Adaptive Computation Time (ACT) which was worse.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Bidirectional encoder gave a small performance boost (Attentional LSTM bidir: interpolation 0.58, extrapolation 0.42). Increasing fixed 'thinking' steps from 0 up to 16 improved performance; ACT gave worse results than fixed padding.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Attentional LSTM (LSTM encoder) average interpolation ~0.57 and extrapolation ~0.38; Attentional LSTM with bidirectional encoder interpolation ~0.58, extrapolation ~0.42 (Figure 3). Performance on add/sub and mul/div modules strong but inferior to Transformer; mixed arithmetic and tasks requiring intermediate computation were notably weaker.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Same systematic failures as other models: poor extrapolation to more terms/compositions, errors on mixed arithmetic requiring intermediate values, brittle to phrasing, and failures on number-theoretic modules (primality/factorization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Performs substantially worse than symbolic/algebraic solvers and worse than Transformer on many modules; behaviour suggests it learns heuristics rather than human-like algorithmic strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysing Mathematical Reasoning Abilities of Neural Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3129.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3129.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Simple LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Simple Sequence LSTM</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A single LSTM that encodes the question into a single vector and decodes the answer autoregressively without explicit attention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Simple LSTM (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single LSTM encoder-decoder where the question is fed then the answer is generated; hidden size used in experiments was 2048, roughly 18M parameters; character-level sequence-to-sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same synthetic math suite: arithmetic (add/sub/mul/div), mixed arithmetic, polynomials, number tasks, comparisons, probability modules, and composed function tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Learns end-to-end sequence mapping that appears to rely on compressed vector memorization and pattern recognition, limited capacity for holding multiple intermediate values, thus failing at computations requiring more working memory.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Lower performance than Transformer and similar to attentional models on some modules; fails on tasks requiring intermediate value storage; experiments where attention or bidirectional encoders were added improved performance, suggesting the simple LSTM's single-vector bottleneck limits algorithmic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Simple LSTM can still achieve reasonable performance on perceptual tasks (place value, rounding) indicating it can learn mappings but not robust algorithmic arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Baselining against attentional variants and Transformer; experiments adding 'thinking' steps not described specifically for simple LSTM but reported for attentional models.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Adding attention or switching to Transformer markedly improved performance; simple LSTM baseline indicates bottleneck limits arithmetic competence.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simple LSTM average interpolation accuracy 0.57 and extrapolation 0.41 (Figure 3). Correct adding sequences up to certain lengths but shares failure modes on mixed/compositional tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Bottleneck compression causes failures on tasks requiring storing multiple intermediate values, limited extrapolation, and brittle handling of syntactic variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Substantially below symbolic solvers and less capable than Transformer; similar human-comparison not provided specifically for this model but overall models remain far from reliable symbolic computation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysing Mathematical Reasoning Abilities of Neural Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3129.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3129.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RMC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relational Memory Core (RMC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent architecture with multiple interacting memory slots (relational recurrent neural network) evaluated as a potential mechanism for storing/manipulating multiple mathematical entities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Relational Memory Core (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Relational recurrent unit with multiple memory slots and attention-based interactions; experiments used ~38–39M parameters, 16 attention heads, 1 block; tuned but best hyperparameter setting used essentially 1 memory slot.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Same synthetic math tasks: arithmetic, polynomial manipulation, compositions, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Intended to store multiple entities in separate memory slots and use attention between slots to implement relational operations; in practice authors observed the model did not make effective use of multiple memory slots for mathematical manipulation and behaved similarly to other recurrent models.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>RMC was more data-efficient in early training but trained more slowly; hyperparameter sweeps showed best configuration reduced to 1 slot, implying failure to exploit multi-slot relational memory for math reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Performance did not exceed LSTM baselines asymptotically; RMC with more than one slot did not improve results, indicating the relational-memory mechanism was not learned/used for arithmetic tasks in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Hyperparameter sweeps varying number of memory slots, attention heads, and block count.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Empirically best setting collapsed to 1 memory slot; more slots did not improve and slowed learning, so no positive intervention effect for arithmetic.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simple RMC: interpolation 0.53, extrapolation 0.38; Attentional RMC with bidir LSTM encoder: interpolation 0.54, extrapolation 0.43 (Figure 3). Did not surpass LSTM asymptotic performance.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Did not learn to allocate separate memory slots to mathematical entities; same failure modes as LSTMs (intermediate computations, extrapolation, number-theoretic tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>RMC did not demonstrate human-like algorithmic storage/use of intermediate values nor match symbolic computation reliability; intended relational-memory advantage was not realized in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysing Mathematical Reasoning Abilities of Neural Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3129.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3129.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DNC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Differentiable Neural Computer (DNC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recurrent neural network with an external differentiable memory (memory matrix and read/write heads) that was attempted but failed to reach competitive performance on the math tasks within resource/time constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Differentiable Neural Computer (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>DNC architecture (Graves et al., 2016) comprising controller RNN and external differentiable memory with read/write heads; attempted with hyperparameter sweeps but did not converge to useful performance in the allotted training time.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Intended for the same synthetic math tasks but not successfully used due to training difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Would in principle use external memory to store intermediate values and implement algorithmic procedures; in practice the authors could not get it to learn effective strategies for arithmetic within their training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>None positive in this work — DNC reached only ~10% validation after a day of training while other models reached that in less than an hour, implying failure to learn successful memory-using arithmetic strategies under the experimental setup.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Poor training and low validation accuracy provide evidence that the DNC did not implement the desired algorithmic arithmetic behaviors in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Hyperparameter sweeps on memory size and training settings were attempted.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Despite hyperparameter tuning (memory slot sizes, etc.) DNC performance remained poor (10% validation) and was not included in main comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported only as failing to reach competitive performance: ~10% validation after a day of training versus other models reaching 10% within an hour; no detailed per-module metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Failure to train to competence under the experimental conditions; did not demonstrate use of external memory for arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>No successful comparisons possible due to failure to train; speculative potential to emulate algorithmic reasoning not realized here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Analysing Mathematical Reasoning Abilities of Neural Models', 'publication_date_yy_mm': '2019-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Learning to execute <em>(Rating: 2)</em></li>
                <li>Neural GPUs learn algorithms <em>(Rating: 2)</em></li>
                <li>Hybrid computing using a neural network with dynamic external memory <em>(Rating: 2)</em></li>
                <li>Attention is all you need <em>(Rating: 2)</em></li>
                <li>Relational recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Can neural networks understand logical entailment? <em>(Rating: 1)</em></li>
                <li>Learning a sat solver from single-bit supervision <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3129",
    "paper_id": "paper-afed6dc6900d3b37e528b9086661bba583d60bf6",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "Transformer",
            "name_full": "Transformer (Attention Is All You Need architecture)",
            "brief_description": "A character-level sequence-to-sequence Transformer used to map freeform math questions to freeform answers; uses multi-head self- and encoder-decoder attention with d_model=512, 8 heads, d_ff=2048.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (this paper)",
            "model_description": "Encoder-decoder Transformer with embedding size d_model=512, h=8 attention heads, d_ff=2048, ~30M parameters; operates on character sequences for questions and answers and trained end-to-end on the synthetic mathematics dataset.",
            "arithmetic_task_type": "Wide range: basic addition/subtraction, multi-term add/subtract, multiplication/division, mixed arithmetic with parentheses, multi-digit arithmetic, base conversion, polynomial evaluation/expansion/collection/composition/differentiation, factoring/primality-related tasks, sampling-without-replacement probability sequences.",
            "reported_mechanism": "Learns largely non-algorithmic, pattern-based or shallow-parallel computational heuristics using attention as a distributed internal memory; appears to compute some arithmetic in parallel (e.g. add/sub/mul modules) rather than by stepwise algorithmic iteration; effective internal representations for simultaneous coefficient manipulation in polynomials.",
            "evidence_for_mechanism": "1) High accuracy on pure add/sub and multiply/divide modules (Transformer: &gt;~90% on 'add or subtract several numbers' and 'multiply or divide several numbers'). 2) Much lower accuracy (~50%) on mixed arithmetic with parentheses requiring explicit intermediate computations. 3) Superior performance on polynomial manipulation (expansion, collect, composition) consistent with ability to hold multiple coefficients in parallel. 4) Failure modes (missing one of repeated identical terms, brittle punctuation sensitivity) consistent with non-algorithmic heuristics.",
            "evidence_against_mechanism": "Some extrapolation successes (rounding bigger numbers, comparing more numbers, adding/subtracting larger magnitudes) indicate limited generalization beyond rote memorization; however overall behaviour argues against learned exact algorithmic procedures.",
            "intervention_type": "Model architecture choice (attention), bidirectional encoder vs unidirectional, 'thinking' padding steps (zero-input steps) before decoding, hyperparameter choices; no external calculators or chain-of-thought prompting used.",
            "effect_of_intervention": "Bidirectional encoder slightly improved accuracy (attentional LSTM results reported similarly). Adding a number (up to 16) of 'thinking' steps before answer decoding increased performance for attentional recurrent models; Transformer inherently benefited from attention/parallel computation and outperformed recurrent models. No benefit from Adaptive Computation Time was reported (worse).",
            "performance_metrics": "Average interpolation accuracy across modules: 0.76; extrapolation accuracy: 0.50 (Figure 3). &gt;~90% on 'add or subtract several numbers' and 'multiply or divide several numbers' modules. ~50% on mixed arithmetic with parentheses. On a held-out set of 40 GCSE exam questions the trained Transformer got 14/40 correct (35%). Specific failure on repeated-summand test: correct for n&lt;=6 of 1+1+...+1, incorrect (gave 6) at n=7.",
            "notable_failure_modes": "1) Fails tasks requiring stepwise intermediate computation (mixed arithmetic, bracketed expressions, polynomial evaluation via intermediate values). 2) Extrapolation to more terms or longer compositions often fails. 3) Missing one repeated identical term in long sums. 4) Plausible-looking but incorrect factorizations and prime-factoring errors. 5) Brittleness to minor syntactic variations (punctuation changes changed output).",
            "comparison_to_humans_or_symbolic": "On a 40-question GCSE set the model achieved a human-equivalent E grade (14/40); compared to symbolic algorithms (exact arithmetic/computer algebra), the Transformer is far less reliable — it produces plausible but incorrect outputs and lacks guaranteed correctness; unlike symbolic methods it seems to rely on learned heuristics rather than exact algorithmic computation.",
            "uuid": "e3129.0",
            "source_info": {
                "paper_title": "Analysing Mathematical Reasoning Abilities of Neural Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Attentional LSTM",
            "name_full": "Encoder-Decoder LSTM with Attention (Attentional LSTM)",
            "brief_description": "A recurrent encoder-decoder LSTM architecture with attention over encoder positions; encoder LSTM (512) and decoder LSTM (2048) used to map question character sequences to answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Attentional LSTM (this paper)",
            "model_description": "Encoder-decoder with attention: encoder LSTM hidden size 512 (or bidirectional variant), decoder LSTM hidden size 2048, key/value sizes k=v=256, roughly 24–26M parameters depending on encoder; character-level input/output trained on the mathematics dataset.",
            "arithmetic_task_type": "Same broad range as Transformer: integer/decimal add/sub/mul/div, mixed arithmetic, polynomial tasks, number theory (primality/factors), probability (without-replacement), conversions, and composed function tasks.",
            "reported_mechanism": "Relies on learned sequence-to-sequence mappings and attention to focus on relevant input tokens; appears to learn non-algorithmic, pattern-based solutions and sometimes uses attention context as working memory instead of explicit algorithmic manipulation.",
            "evidence_for_mechanism": "Performance similar to simple LSTM on many tasks; increasing 'thinking' (zero-input) steps before decoding improved accuracy (suggests need for internal iterative integration). Attentional LSTM did not show evidence of learning true algorithmic parsing — authors speculate attention isn't being used to implement algorithmic steps.",
            "evidence_against_mechanism": "Attentional mechanism did not yield substantial algorithmic advantages over simple LSTM; comparable performance indicates attention did not enable discovered algorithmic procedures.",
            "intervention_type": "Attention mechanism, bidirectional encoder variant, insertion of fixed 'thinking' zero-input steps, experiment with Adaptive Computation Time (ACT) which was worse.",
            "effect_of_intervention": "Bidirectional encoder gave a small performance boost (Attentional LSTM bidir: interpolation 0.58, extrapolation 0.42). Increasing fixed 'thinking' steps from 0 up to 16 improved performance; ACT gave worse results than fixed padding.",
            "performance_metrics": "Attentional LSTM (LSTM encoder) average interpolation ~0.57 and extrapolation ~0.38; Attentional LSTM with bidirectional encoder interpolation ~0.58, extrapolation ~0.42 (Figure 3). Performance on add/sub and mul/div modules strong but inferior to Transformer; mixed arithmetic and tasks requiring intermediate computation were notably weaker.",
            "notable_failure_modes": "Same systematic failures as other models: poor extrapolation to more terms/compositions, errors on mixed arithmetic requiring intermediate values, brittle to phrasing, and failures on number-theoretic modules (primality/factorization).",
            "comparison_to_humans_or_symbolic": "Performs substantially worse than symbolic/algebraic solvers and worse than Transformer on many modules; behaviour suggests it learns heuristics rather than human-like algorithmic strategies.",
            "uuid": "e3129.1",
            "source_info": {
                "paper_title": "Analysing Mathematical Reasoning Abilities of Neural Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "Simple LSTM",
            "name_full": "Simple Sequence LSTM",
            "brief_description": "A single LSTM that encodes the question into a single vector and decodes the answer autoregressively without explicit attention.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Simple LSTM (this paper)",
            "model_description": "Single LSTM encoder-decoder where the question is fed then the answer is generated; hidden size used in experiments was 2048, roughly 18M parameters; character-level sequence-to-sequence.",
            "arithmetic_task_type": "Same synthetic math suite: arithmetic (add/sub/mul/div), mixed arithmetic, polynomials, number tasks, comparisons, probability modules, and composed function tasks.",
            "reported_mechanism": "Learns end-to-end sequence mapping that appears to rely on compressed vector memorization and pattern recognition, limited capacity for holding multiple intermediate values, thus failing at computations requiring more working memory.",
            "evidence_for_mechanism": "Lower performance than Transformer and similar to attentional models on some modules; fails on tasks requiring intermediate value storage; experiments where attention or bidirectional encoders were added improved performance, suggesting the simple LSTM's single-vector bottleneck limits algorithmic computation.",
            "evidence_against_mechanism": "Simple LSTM can still achieve reasonable performance on perceptual tasks (place value, rounding) indicating it can learn mappings but not robust algorithmic arithmetic.",
            "intervention_type": "Baselining against attentional variants and Transformer; experiments adding 'thinking' steps not described specifically for simple LSTM but reported for attentional models.",
            "effect_of_intervention": "Adding attention or switching to Transformer markedly improved performance; simple LSTM baseline indicates bottleneck limits arithmetic competence.",
            "performance_metrics": "Simple LSTM average interpolation accuracy 0.57 and extrapolation 0.41 (Figure 3). Correct adding sequences up to certain lengths but shares failure modes on mixed/compositional tasks.",
            "notable_failure_modes": "Bottleneck compression causes failures on tasks requiring storing multiple intermediate values, limited extrapolation, and brittle handling of syntactic variants.",
            "comparison_to_humans_or_symbolic": "Substantially below symbolic solvers and less capable than Transformer; similar human-comparison not provided specifically for this model but overall models remain far from reliable symbolic computation.",
            "uuid": "e3129.2",
            "source_info": {
                "paper_title": "Analysing Mathematical Reasoning Abilities of Neural Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "RMC",
            "name_full": "Relational Memory Core (RMC)",
            "brief_description": "A recurrent architecture with multiple interacting memory slots (relational recurrent neural network) evaluated as a potential mechanism for storing/manipulating multiple mathematical entities.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Relational Memory Core (this paper)",
            "model_description": "Relational recurrent unit with multiple memory slots and attention-based interactions; experiments used ~38–39M parameters, 16 attention heads, 1 block; tuned but best hyperparameter setting used essentially 1 memory slot.",
            "arithmetic_task_type": "Same synthetic math tasks: arithmetic, polynomial manipulation, compositions, etc.",
            "reported_mechanism": "Intended to store multiple entities in separate memory slots and use attention between slots to implement relational operations; in practice authors observed the model did not make effective use of multiple memory slots for mathematical manipulation and behaved similarly to other recurrent models.",
            "evidence_for_mechanism": "RMC was more data-efficient in early training but trained more slowly; hyperparameter sweeps showed best configuration reduced to 1 slot, implying failure to exploit multi-slot relational memory for math reasoning.",
            "evidence_against_mechanism": "Performance did not exceed LSTM baselines asymptotically; RMC with more than one slot did not improve results, indicating the relational-memory mechanism was not learned/used for arithmetic tasks in this setup.",
            "intervention_type": "Hyperparameter sweeps varying number of memory slots, attention heads, and block count.",
            "effect_of_intervention": "Empirically best setting collapsed to 1 memory slot; more slots did not improve and slowed learning, so no positive intervention effect for arithmetic.",
            "performance_metrics": "Simple RMC: interpolation 0.53, extrapolation 0.38; Attentional RMC with bidir LSTM encoder: interpolation 0.54, extrapolation 0.43 (Figure 3). Did not surpass LSTM asymptotic performance.",
            "notable_failure_modes": "Did not learn to allocate separate memory slots to mathematical entities; same failure modes as LSTMs (intermediate computations, extrapolation, number-theoretic tasks).",
            "comparison_to_humans_or_symbolic": "RMC did not demonstrate human-like algorithmic storage/use of intermediate values nor match symbolic computation reliability; intended relational-memory advantage was not realized in these experiments.",
            "uuid": "e3129.3",
            "source_info": {
                "paper_title": "Analysing Mathematical Reasoning Abilities of Neural Models",
                "publication_date_yy_mm": "2019-04"
            }
        },
        {
            "name_short": "DNC",
            "name_full": "Differentiable Neural Computer (DNC)",
            "brief_description": "A recurrent neural network with an external differentiable memory (memory matrix and read/write heads) that was attempted but failed to reach competitive performance on the math tasks within resource/time constraints.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Differentiable Neural Computer (this paper)",
            "model_description": "DNC architecture (Graves et al., 2016) comprising controller RNN and external differentiable memory with read/write heads; attempted with hyperparameter sweeps but did not converge to useful performance in the allotted training time.",
            "arithmetic_task_type": "Intended for the same synthetic math tasks but not successfully used due to training difficulties.",
            "reported_mechanism": "Would in principle use external memory to store intermediate values and implement algorithmic procedures; in practice the authors could not get it to learn effective strategies for arithmetic within their training regime.",
            "evidence_for_mechanism": "None positive in this work — DNC reached only ~10% validation after a day of training while other models reached that in less than an hour, implying failure to learn successful memory-using arithmetic strategies under the experimental setup.",
            "evidence_against_mechanism": "Poor training and low validation accuracy provide evidence that the DNC did not implement the desired algorithmic arithmetic behaviors in these experiments.",
            "intervention_type": "Hyperparameter sweeps on memory size and training settings were attempted.",
            "effect_of_intervention": "Despite hyperparameter tuning (memory slot sizes, etc.) DNC performance remained poor (10% validation) and was not included in main comparisons.",
            "performance_metrics": "Reported only as failing to reach competitive performance: ~10% validation after a day of training versus other models reaching 10% within an hour; no detailed per-module metrics reported.",
            "notable_failure_modes": "Failure to train to competence under the experimental conditions; did not demonstrate use of external memory for arithmetic tasks.",
            "comparison_to_humans_or_symbolic": "No successful comparisons possible due to failure to train; speculative potential to emulate algorithmic reasoning not realized here.",
            "uuid": "e3129.4",
            "source_info": {
                "paper_title": "Analysing Mathematical Reasoning Abilities of Neural Models",
                "publication_date_yy_mm": "2019-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Learning to execute",
            "rating": 2
        },
        {
            "paper_title": "Neural GPUs learn algorithms",
            "rating": 2
        },
        {
            "paper_title": "Hybrid computing using a neural network with dynamic external memory",
            "rating": 2
        },
        {
            "paper_title": "Attention is all you need",
            "rating": 2
        },
        {
            "paper_title": "Relational recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Can neural networks understand logical entailment?",
            "rating": 1
        },
        {
            "paper_title": "Learning a sat solver from single-bit supervision",
            "rating": 1
        }
    ],
    "cost": 0.0129575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Analysing Mathematical Reasoning Abilities of Neural Models</h1>
<p>David Saxton<br>DeepMind<br>saxton@google.com</p>
<p>Edward Grefenstette
DeepMind
egrefen@fb.com</p>
<p>Felix Hill<br>DeepMind<br>felixhill@google.com</p>
<p>Pushmeet Kohli<br>DeepMind<br>pushmeet@google.com</p>
<h4>Abstract</h4>
<p>Mathematical reasoning-a core ability within human intelligence-presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.</p>
<h2>1 INTRODUCTION</h2>
<p>Deep learning, powered by convolutional and recurrent networks, has had remarkable success in areas involving pattern matching (such as in images (Krizhevsky et al., 2012), machine translation (Bahdanau et al., 2014; Vaswani et al., 2017), and reinforcement learning (Mnih et al., 2015; Silver et al., 2016)). However, deep models are far from achieving the robustness and flexibility exhibited by humans. They are limited in their ability to generalize beyond the environments they have experienced and are extremely brittle in the presence of adversarially constructed inputs (Szegedy et al., 2013).</p>
<p>One area where human intelligence still differs and excels compared to neural models is discrete compositional reasoning about objects and entities, that "algebraically generalize" (Marcus, 2003). Our ability to generalise within this domain is complex, multi-faceted, and patently different from the sorts of generalisations that permit us to, for example, translate new sentence of French into English. For example, consider the following question from mathematics, with answer " $-70 x-165$ ".</p>
<p>What is $g(h(f(x)))$, where $f(x)=2 x+3, g(x)=7 x-4$, and $h(x)=-5 x-8$ ?
To solve this problem, humans use a variety of cognitive skills:</p>
<ul>
<li>Parsing the characters into entities such as numbers, arithmetic operators, variables (which together form functions) and words (determining the question).</li>
<li>Planning (for example, identifying the functions in the correct order to compose).</li>
<li>Using sub-algorithms for function composition (addition, multiplication).</li>
<li>
<p>Exploiting working memory to store intermediate values (such as the composition $h(f(x))$ ).</p>
</li>
<li>
<p>Generally applying acquired knowledge of rules, transformations, processes, and axioms.</p>
</li>
</ul>
<p>In this paper, we introduce a dataset consisting of many different types of mathematics problems, with the motivation that it should be harder for a model to do well across a range of problem types (including generalization, which we detail below) without possessing at least some part of these abilities that allow for algebraic generalization.</p>
<p>This domain is an important one for the analysis of neural architectures in general. In addition to providing a wide range of questions, there are several other advantages: Mathematics offers a self-consistent universe; notation is the same across different problem types, which allows for an easily extendable dataset; and rules and methods learnt on one problem type often apply elsewhere. Addition of numbers (for example) obeys the same rules everywhere, and occurs as a "subroutine" in other problems (such as concretely in multiplication, and both concretely and more abstractly in addition of polynomials); models that possess the ability to transfer knowledge will do well on the dataset (and knowledge transfer may be a necessity for solving harder problems).</p>
<p>Mathematics is also an interesting domain in its own right; although models solving the mostly school-level problems in this dataset would not themselves have applications, they may lead on to more powerful models that can solve interesting and substantial new mathematical problems. But more generally, it is no coincidence that experiments seeking to validate new architectures which aim capture algorithmic/systematic reasoning have often been drawn from this domain (Graves et al., 2016; Kaiser \&amp; Sutskever, 2015; Joulin \&amp; Mikolov, 2015), and thus in providing a large scale training and evaluation framework for such models, we hope to provide a solid foundation upon which to continue such research into machine reasoning beyond mathematics.</p>
<p>Question: Solve $-42 * \mathrm{r}+27 * \mathrm{c}=-1167$ and $130 * \mathrm{r}+4 * \mathrm{c}=372$ for r .
Answer: 4
Question: Calculate $-841880142.544+411127$.
Answer: $-841469015.544$
Question: Let $x(g)=9 * g+1$. Let $q(c)=2 * c+1$. Let $f(i)=3 * i-$ 39. Let $w(j)=q(x(j))$. Calculate $f(w(a))$.</p>
<p>Answer: $54 * a-30$
Question: Let e(1) = 1 - 6. Is 2 a factor of both e(9) and 2?
Answer: False
Question: Let $u(n)=-n * * 3-n * * 2$. Let $e(c)=-2 * c * * 3+c$. Let $l(j)$ $=-118 * e(j)+54 * u(j)$. What is the derivative of $l(a)$ ?
Answer: $546 * a * * 2-108 * a-118$
Question: Three letters picked without replacement from qqqkkklkqkkk. Give prob of sequence qq1.
Answer: $1 / 110$
Figure 1: Examples from the dataset.</p>
<h1>1.1 OUR CONTRIBUTIONS</h1>
<p>Dataset and generalization tests We release ${ }^{1}$ a sequence-to-sequence dataset consisting of many different types of mathematics questions (see Figure 1) for measuring mathematical reasoning, with the provision of both generation code and pre-generated questions. The dataset comes with two sets of tests: interpolation tests, one for each type of question occurring in the training set; and extrapolation tests, that measure generalization along various axes of difficulty to beyond that seen during training. We include extrapolation tests as an additional measure of whether models are employing abilities that allow them to algebraically generalize.</p>
<p>Experiments and model analysis We perform an experimental evaluation to investigate the algebraic abilities of state-of-the-art neural architectures, and show that they do well on some types of questions, but certainly not all, and furthermore have only moderate amounts of generalization. We give some insights into how they learn to answer mathematics questions, and their failure modes.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>1.2 RELATED WORK</h1>
<p>There are various papers with datasets with a discrete reasoning nature. Kaiser \&amp; Sutskever (2015) use an adapted convolutional architecture to solve addition and multiplication with good generalization; Allamanis et al. (2016) and Evans et al. (2018) use tree networks to predict polynomial or logical equivalence or logical entailment; Selsam et al. (2018) uses message passing networks with a bipartite graph structure to decide satisfiability in formulas in conjunctive normal form, and so on. The difference between those problems and the dataset in this paper is that the former all have a single well-defined input structure that can be easily mapped into narrow architectures suited to the problem structure, avoiding the need for general reasoning skills like parsing or generic working memory.</p>
<p>Zaremba \&amp; Sutskever (2014) analyze the ability of LSTMs to map short Python programs (addition or for-loops) to their output. Some mathematics problems are of a similar imperative nature (e.g. arithmetic), but we also cover many other types of problems, so our dataset subsumes learning-to-execute. There are a few other synthetically generated datasets designed to assess reasoning of some form. The bAbI dataset of Weston et al. (2015) consists of textual questions, testing the ability to extract knowledge from a story-like sequence of questions. The CLEVR dataset of Johnson et al. (2017) consists of image-question pairs, where the image is of a set of objects, and the question asks for some property of the scene; this dataset is designed to assess visual analysis. Santoro et al. (2018b) use Raven's progressive matrix puzzles to measure abstract reasoning of networks.</p>
<p>There has also been a recent interest in solving algebraic word problems. These questions tend to be crowd sourced or obtained from exercise books, and existing datasets include Allen Institute for AI (2014); Kushman et al. (2014); Huang et al. (2016); Upadhyay \&amp; Chang (2016); Wang et al. (2017); Ling et al. (2017). These range in size from hundreds to up to one hundred thousand examples, with different variations and focuses; for example, containing supervised "answer rationale", or focusing on more narrow types of problems, or additionally containing geometry problems (although some of these are too small to train deep learning models without extensive prior mathematical knowledge). Our dataset differs from these in that our focus is mathematical reasoning rather than linguistic comprehension; we cover more areas of mathematics, but with less variation in problem specification, and we see mathematical reasoning as a partially orthogonal and complementary direction to linguistic understanding existing in these other datasets.</p>
<h2>2 THE DATASET</h2>
<h3>2.1 DESIGN CHOICES</h3>
<p>Modular structure and procedural generation There are two choices for obtaining mathematical questions: either crowd-sourced, or synthetically generated. While crowd-sourcing has the advantage of introducing linguistic diversity, as well as a diversity of problem types, it is difficult to collect and validate such data at scale. In contrast, procedural generation is sufficient for our purposes in most respects: it (1) easily provides a larger number of training examples, with (2) precise controls over difficulty levels, permitting (3) analysis of performance by question type, and (4) better guarantees on question correctness, with (5) potential for more efficient model training by varying the time spent on each module, and (6) ease of testing generalization (since one can precisely vary different axes of difficulty in different question types).</p>
<p>Freeform question/answers Given that we synthetically generate the data, we could of course provide the questions as parsed into some structure appropriate for each question type (e.g. a tree or graph). However, we opt for freeform-as a sequence of characters-because (1) it is a powerful and flexible format, allowing us to express many question types (whereas trees or graphs are only appropriate for some problems), (2) the ability to properly semantically parse is a non-negligible part of cognition, and (3) sequences are much simpler objects than graphs and trees, which simplifies development of the dataset and models.</p>
<p>Perhaps most importantly, using freeform inputs and outputs means that the input and output space for models evaluated on the benchmark tasks in this dataset is the same as required to address a variety of "real world" mathematics exams questions. While it is not plausible that models trained on our data would perform well on such actual tests due to restricted linguistic variation in how questions</p>
<p>and answers are formulated, it is nonetheless a desirable feature of our data that future models which do attack real world tests can be "unit tested" on our benchmarks during their development.</p>
<p>Compositionality The questions can be seen as mappings with input and output types. For example, function evaluation maps a function and an integer to another integer, function composition maps a pair of functions to a function, and so on. We use this to generate additional composed questions by chaining modules with matching types, where intermediate values from one sub-problem are used as inputs to the next sub-problem. For example, for a single intermediate value, this composition may be phrased as Let $\mathrm{x}=$ <description>. <question (x) >. See Figure 1 for examples. This makes the dataset more interesting and challenging in several ways. Many rules in mathematics appear when different concepts are composed. For example, when differentiation is composed with function composition, the chain rule appears; when addition is composed with factorization, distributivity can emerge; and so on. Composition moves the questions away from pure perception, since intermediate results must be stored (working memory) and manipulated (reuse of sub-routines).</p>
<h1>2.2 BRIEF OVERVIEW OF MODULES</h1>
<p>What types of mathematics problems should be included in the dataset? The original content was based on a national school mathematics curriculum (up to age 16), restricted to textual questions (thus excluding geometry questions), which gave a comprehensive range of mathematics topics that worked together as part of a learning curriculum. We extended this with additional areas that offer good tests for algebraic reasoning. We cover the following areas (Appendix B contains the full list of modules). (1) Algebra, such as solving linear systems in 1 and 2 variables, finding roots of polynomials (presented in simplified or unsimplified forms), and extending sequences and finding their general form. (2) Arithmetic, such as basic addition etc, evaluating nested expressions, and simplifying expressions involving square roots. (3) Calculus and differentiating polynomials. (4) Comparisons, such as establishing which of two numbers is bigger, or sorting a list of numbers, or finding the closest number to a given one in a list. (5) Measurement, such as converting between different length scales, and calculating time intervals. (6) Numbers, such as finding divisors, rounding, place value, factorization, and primality. (7) Manipulating polynomials, such as simplification, expansion, evaluation, composition, and addition. (8) Probability, such as probability of obtaining a given sequence when sampling without replacement. Many modules participate in composition where possible. For example, one might have to compare two numbers (a composition module), one of which is the solution of a linear system, and the other is the evaluation of a function.</p>
<h3>2.3 GENERATING DIVERSE QUESTIONS FOR TRAINING AND TESTING</h3>
<p>Most questions involve evaluating one or more randomly generated mathematical objects (e.g. arithmetic expressions, linear systems, polynomials, compositions of these, etc). The biggest challenge in producing the dataset is generating diverse questions that are neither trivial nor impossibly hard. During testing we also want to generate questions that have not been seen in training.</p>
<p>These requirements rule-out naive unconditional sampling of such objects. For example, the product of a sequence of rationals will evaluate to zero if any of the rationals are zero; an arithmetic expression generated by randomly sampling a binary tree will often evaluate to zero or some large number; and a linear system in two variables will rarely have integer solutions. So instead for most modules we employ a different approach: we first sample the answer, and then work backwards to generate the question (including if we are doing module composition). The details of how we do this are diverse and depend on the question type, and we refer the reader to the generation code for more detail.</p>
<p>Training and interpolation tests Per module, we generate $2 \times 10^{6}$ train questions, and $10^{5}$ test (interpolation) questions. To ensure the train questions are diverse, and the test questions are distinct from the train questions, the generation code guarantees lower bounds on the probability of a given question appearing. (Post-generation hashing does not in general work, since the same question may occur with linguistic variation, although we use it in a few limited cases.) We generate test questions such that any particular question has a probability of at most $10^{-8}$, thus guaranteeing that at most $10^{-8} \times 2 \times 10^{6}=2 \%$ of the test questions to have already appeared in the training data. (To be more precise, each module generator accepts an input $\alpha$, such that the output question has probability at</p>
<p>most $10^{-\alpha}$; train questions are generated by sampling $\alpha$ uniformly from $[3,10]$ (typically), and test questions are generated by taking $\alpha=8$.)</p>
<p>The various mechanisms by which we achieve these probabilistic guarantees are again diverse and question dependent, so again we refer the reader to the generation code. But to give an example, many questions involve one or more integers (which includes rationals, a quotient of two integers). If we need to generate $n$ integers, then provided the $i$ th integer is sampled from a set of size at least $a_{i}$, then the probability of a given sequence of integers is at most $\prod_{i} 1 / a_{i}$. We then simply need to choose these sets of integers appropriately (e.g. a symmetric set about zero, or the first positive integers, or integers coprime to some other integer, etc).</p>
<p>Extrapolation tests Mathematical generalization exists along a variety of axes (e.g. length, number of symbols, depth of composition/recursion). We therefore include, in our extrapolation test sets, a range of modules that measure extrapolation along different axes, such as to problems involving larger numbers, more numbers, more compositions, and (for probability questions) larger samplers. Full details are in Appendix B.</p>
<h1>2.4 Evaluation CRITERION</h1>
<p>Given a model that maps an input question to an output answer, we score each question either 0 or 1 according to whether the answer matches the correct answer character-for-character. The performance on a given test module is the average of this score across all questions. Performance across the interpolation and extrapolation test sets is then the average across all modules inside the test set. This choice of criterion is appropriate given the restricted nature of the answers generated in our dataset (but see Section 5 for possible future extensions).</p>
<h3>2.5 RELEASE</h3>
<p>We will release $2 \times 10^{6}$ training examples and $10^{4}$ pre-generated test examples per module upon publication of this paper. In the dataset, the questions and answers use a common alphabet of size 95 (upper and lower case characters, digits, and punctuation characters). The questions are capped to 160 characters in length and answers to 30 , which is sufficient for a wide range of question types. Mathematical equations are formatted according to Python/SymPy (Meurer et al., 2017) conventions (for example, ** is used for power rather than ^); these rules are consistent for all modules.</p>
<h2>3 MODELS EXAMINED</h2>
<p>Due to the construction process underlying this dataset, there are a large number of existing models, which could be adapted, purpose-built, or tailored to solve the sort of problems we present here, especially with the help of symbolic solvers or computer algebra systems. Setting aside the possible brittleness or limits in scalability of traditional symbolic approaches as the complexity or linguistic diversity of questions and answers grows, we are interested here in evaluating general purpose models, rather than ones with their mathematics knowledge already inbuilt. What makes such models (which are invariably neural architectures) so ubiquitous from translation to parsing via image captioning is the lack of bias these function approximators present due to having relatively little (or no) domain-specific knowledge encoded in their design. Although there are some neural network-driven approaches with direct access to mathematical operations (such as addition or multiplication (Ling et al., 2017), or more complex mathematical templates like in (Kushman et al., 2014)), which would undoubtedly perform competitively on the tasks we present in this paper, we will limit ourselves to general sequence-processing architectures which are used in other non-mathematical tasks to present the most general baselines possible for future comparison.</p>
<p>We investigate two (broad classes of) models that have demonstrated themselves to be state-of-the-art on sequence-to-sequence problems: recurrent neural architectures, and the more recently introduced attentional/transformer (Vaswani et al., 2017) architecture. We also tried to use Differentiable Neural Computers (Graves et al., 2016), which is a recurrent model with an "external memory" (whose size is independent of the number of parameters in the network). In theory this could be well suited for solving mathematical questions, since it can store intermediate values for later usage. However we were unable to get decent performance out of it. (Even with hyperparameter sweeps for the number</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The attentional LSTM and Transformer architectures are both consist of an encoder, that parses the question, and a decoder, which maps the correct answer right-shifted by 1 to a distribution of the next character in the answer at every position (thus allowing auto-regressive prediction). (a) The Attentional LSTM encodes the question to a sequence of (key, value) positions, which are then attended over by the decoder. (b) The Transformer has several stages of self- and input-attention; see (Vaswani et al., 2017) for details.
and size of memory slots, etc, we were only able to get to $10 \%$ validation performance after a day of training, whereas most models obtain this in less than an hour).</p>
<h1>3.1 RECURRENT ARCHITECTURES</h1>
<p>The LSTM (Hochreiter \&amp; Schmidhuber, 1997) is a powerful building block of sequence-to-sequence models that have achieved state of the art results in many domains, and despite its simplicity, continues to be a central building block for recurrent neural networks. We benchmark two standard recurrent architectures (described in more detail in Appendix A).</p>
<p>The first and simplest model we analyze (referred to in results below as "Simple LSTM") is to simply feed the question into the LSTM, one character at a time (using a 1-hot encoding), before outputting the answer one character at a time (the output is a distribution over possible characters, and at every answer step, the previous correct answer character is fed in). In the results below, we use a hidden size of 2048 (obtained via a hyperparameter sweep).</p>
<p>The second model we analyze (referred to as "Attentional LSTM") is the encoder/decoder-withattention architecture introduced in (Bahdanau et al., 2014) which has been prevalent in neural machine translation, and overcomes two problems with the simple LSTM model above, which affect both language translation and mathematical question-answering: (1) information that is presented in the input may be out-of-order for the purpose of calculations required for the output (for example, to calculate $8 /(1+3)$, the expression $1+3$ must be evaluated first); and (2) all information for the answer must be contained within the single vector of cell activations of the LSTM, which is a bottleneck. The attentional LSTM architecture consists of a recurrent encoder that encodes the question to a sequence of keys and values (of the same length as the question), and a recurrent decoder that has as input the correct answer right-shifted by 1 , and at every time step attends to the encoded question, and outputs a distribution over the next character. We use an encoding LSTM with 512 hidden units and a decoding LSTM with 2048 hidden units. (These settings were obtained using a hyperparameter sweep.)</p>
<p>In both these architecture, we also employ a simple change that improves performance. The models as described must output the answer straight after parsing the question. However, it may be necessary for the models to expend several computation steps integrating information from the question. To allow for this, we add additional steps (with zero input) before outputting the answer. We also experimented with Adaptive Computation Time as introduced in (Graves, 2016), although this yielded worse results than simply having a fixed number of "thinking" steps.</p>
<p>Recently a recurrent architecture known as relational recurrent neural network (Santoro et al., 2018a), or relational memory core (RMC), has been developed as a replacement for the LSTM. This recurrent unit has multiple memory slots that interact via attention. This seems like a natural candidate for</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Parameters</th>
<th style="text-align: center;">Interpolation</th>
<th style="text-align: center;">Extrapolation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Simple LSTM</td>
<td style="text-align: center;">18 M</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.41</td>
</tr>
<tr>
<td style="text-align: left;">Simple RMC</td>
<td style="text-align: center;">38 M</td>
<td style="text-align: center;">0.53</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">Attentional LSTM, LSTM encoder</td>
<td style="text-align: center;">24 M</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">Attentional LSTM, bidir LSTM encoder</td>
<td style="text-align: center;">26 M</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.42</td>
</tr>
<tr>
<td style="text-align: left;">Attentional RMC, bidir LSTM encoder</td>
<td style="text-align: center;">39 M</td>
<td style="text-align: center;">0.54</td>
<td style="text-align: center;">0.43</td>
</tr>
<tr>
<td style="text-align: left;">Transformer</td>
<td style="text-align: center;">30 M</td>
<td style="text-align: center;">$\mathbf{0 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{0 . 5 0}$</td>
</tr>
</tbody>
</table>
<p>Figure 3: Model accuracy (probability of correct answer) averaged across modules. RMC is the relational recurrent neural network model.
mathematical reasoning, for example if the model can learn to use the slots to store mathematical entities. However, a comprehensive hyperparameter sweep gave the best setting as 1 memory slot (i.e., without making full use of the RMC). We include these results below, also with 2048 total units, 16 attention heads, and 1 block.</p>
<h1>3.2 Transformer (Attention Is All You Need)</h1>
<p>The Transformer model (Vaswani et al., 2017) is a sequence-to-sequence model achieving state-of-the-art results in machine translation. We briefly describe it here (see Figure 2b). The model consists of an encoder, which transforms the question (represented as a sequence of vectors) to another sequence of the same length, and a decoder (which transforms the encoded question, and the answer autoregressively shifted right, into the answer prediction). Internally the input is transformed via attentional mechanisms (both self- and input-attention), and position-wise fully connected layers. We use an embedding size of $d_{\text {model }}=512$, with $h=8$ attentional heads, and thus key and value sizes of $d_{k}=d_{v}=d_{\text {model }} / h=64$. Each layer has an intermediate representation with dimension $d_{\mathrm{ff}}=2048$. For translation tasks, it is typically applied to sequences of embedded words; here we instead treat the question and answer as a sequence of characters, since we need to be able to embed arbitrary mathematical expressions.</p>
<h2>4 ANALYSIS</h2>
<h3>4.1 TRAINING AND EVALUATION METHODS</h3>
<p>As is common in sequence-to-sequence models, the models predict the answer autoregressively using a greedy decoder (output majority class at each step). We minimize the sum of log probabilities of the correct character via the Adam optimizer (Kingma \&amp; Ba, 2014) with learning rate of $6 \times 10^{-4}$, $\beta_{1}=0.9, \beta_{2}=0.995, \epsilon=10^{-9}$. We use a batch size of 1024 split across 8 NVIDIA P100 GPUs for 500 k batches, with absolute gradient value clipping of 0.1 .</p>
<h3>4.2 RESULTS AND INSIGHTS</h3>
<p>Figure 3 shows the average interpolation and extrapolation performances for the different architectures. Full per-module performance results are in Appendix C.</p>
<p>LSTMs vs RMCs Using a RMC with more than one memory slot did not help performance; perhaps it is hard for the RMC to learn to use slots for manipulating mathematical entities. For a given number of hidden units, RMCs were more data efficient but trained more slowly (since they had more parameters), and LSTMs had better asymptotic performance.</p>
<p>Simple vs attentional LSTM The attentional LSTM and the simple LSTM have similar performance. One might suspect that the attentional LSTM does nothing, however this is not the case, since a simple LSTM model of the same size as the parsing LSTM obtains much worse performance. We speculate that the attentional model is not learning to algorithmically parse the question, and so the ability to change attention focus per-step does not count for as much.</p>
<p>Number of thinking steps For the attentional LSTM model, we observed that increasing the number of "thinking" steps (as defined above) from 0 up to 16 increased the performance.</p>
<p>Transformer vs best non-transformer model The Transformer performs the same as or significantly better than recurrent models across nearly all modules. Both architectures have a comparable number of parameters. One might a-priori expect the LSTM to perform better, since its sequential architecture is perhaps more similar to sequential reasoning steps that a human performs. However, evidence above and below suggest that neither of the networks are doing much "algorithmic reasoning", and the Transformer has various advantages over LSTM architectures, such as (1) doing more calculations with the same number of parameters, (2) having a shallower architecture (with better gradient propagation), and (3) having an internal "memory" that is sequential, which is more pre-disposed to mathematical objects like sequences of digits.</p>
<p>Easiest maths for neural networks The easiest question types were finding the place value in a number, and rounding decimals and integers, which all models got nearly perfect scores on. Questions involving comparisons also tended to be quite easy, possible because such tasks are quite perceptual (e.g. comparing lengths or individual digits). This success includes questions with module composition, for example Let k (c) $=-611 * c+2188857$. Is k (-103) != 2251790? (False) and mixtures of decimals and rationals, for example, Sort -139/4, 40.8, -555, 607 in increasing order. Overall it seems that magnitude is easy for neural networks to learn.</p>
<p>Hardest maths for neural networks Perhaps not surprisingly, some of the hardest modules include more number-theoretic questions which are also hard for humans, such as detecting primality and factorization. The Transformer model still gives plausible-looking answers, such as factoring 235232673 as $3,11,13,19,23,1487$ (the correct answer is $3,13,19$, 317453).</p>
<p>The Transformer model has a performance of $90 \%$ or more on the "add or subtract several numbers" module and the "multiply or divide several numbers" module (which is just addition and subtraction in log space). However on the mixed arithmetic module (mixing all four operations together with parentheses), the performance drops to around $50 \%$. (Note the distribution of the value of the expression is the same for all these modules, so it is not the case that difficulty increases due to different answer magnitudes.) We speculate that the difference between these modules in that the former can be computed in a relatively linear/shallow/parallel manner (so that the solution method is relatively easier to discover via gradient descent), whereas there are no shortcuts to evaluating mixed arithmetic expressions with parentheses, where intermediate values need to be calculated. This is evidence that the models do not learn to do any algebraic/algorithmic manipulation of values, and are instead learning relatively shallow tricks to obtain good answers on many of the modules. The same holds true for other modules that require intermediate value calculation, such as evaluating polynomials, and general composition.</p>
<p>Performance on polynomial manipulation One notable difference between the Transformer and the recurrent models was polynomial manipulation. The Transformer did significantly better on polynomial expansion, collecting terms, addition, composition, differentiation, and extracting named coefficients. Speculatively, the parallel sequential nature of the Transformer is better at manipulating polynomials where several coefficients must be kept in memory simultaneously where they can interact.</p>
<p>Other insights Examining the performance on adding multiple integers, we tested the models on adding $1+1+\cdots+1$, where 1 occurs $n$ times. Both the LSTM and Transformer models gave the correct answer for $n \leq 6$, but the incorrect answer of 6 for $n=7$ (seemingly missing one of the 1 s ), and other incorrect values for $n&gt;7$. (The models are trained on sequences of random integers up to length 10, and are capable of giving the correct answer on longer sequences of far bigger numbers, for example $-34+53+-936+-297+162+-242+-128$.) We do not have a good explanation for this behaviour; one hypothesis is that the models calculate subsums and then combine these, but rely on different input numbers to align the subsums, and fail when the input is "camouflaged" by consisting of the same number repeated multiple times.</p>
<p>Robustness to question phrasing Although we do not train for linguistic variation and do not expect models to be robust to it, the failure modes are still interesting. For example, on one trained Transformer, the question "Calculate 1? * 4." gave the correct answer 68, but the same question without the final full stop gave 69.</p>
<p>Extrapolation performance Modules on which good extrapolation performance was obtained include rounding larger numbers than seen during training, comparing more numbers, and adding and subtracting larger numbers. However for example models completely failed to add together more numbers than seen during training, which agrees with the suspicion that models have learnt to add numbers in parallel rather than calculating subsums.</p>
<h1>4.3 PERFORMANCE ON REAL MATHEMATICS QUESTIONS</h1>
<p>To provide an external benchmark for the capability of neural network models trained on our dataset, we tested the trained Transformer model on a set of 40 questions selected from publicly-available maths exams for British 16 year old schoolchildren ${ }^{2}$. These questions were gathered from four exam papers after excluding those involving graphs, tables or other figures - the full set is reproduced in the supplementary materials.</p>
<p>On these exam questions, the Transformer model got 14/40 questions correct, which is (proportionally) equivalent to that of an E grade student ${ }^{3}$. The model showed some promise by correctly solving the simultaneous equations $5 x+2 y=11$ and $4 x-3 y=18$, identified the correct next number in the sequence $3,9,15,27$. The disappointing grade also assumes that no marks were awarded for plausible but incorrect attempts, such as the factorisation $1(y-2)(y+4)$ of the expression $y^{2}-10 y+16$. Overall, this analysis suggests that, with knowledge of the exam syllabus to inform the training data generation, and the ability to receive graphical inputs, it may be possible to encode the knowledge necessary to excel at unseen exams in an out-of-the-box neural network, although the pattern of errors and ability to generalise would likely differ from typical school-age students.</p>
<h2>5 CONCLUSIONS AND FUTURE WORK</h2>
<p>We have created a dataset on which current state-of-the-art neural models obtain moderate performance. Some modules are largely unsolved (for example those requiring several intermediate calculations), for which a human would find easy, and extrapolation performance is low. We hope this dataset will become a robust analyzable benchmark for developing models with more algebraic/symbolic reasoning abilities.</p>
<p>The dataset is easily extendable, since it is modular, with all modules using a common input/output format and the common language of mathematics. The main restriction is that the answers must be well-determined (i.e. unique), but this still allows for covering a lot of mathematics up to university level. At some point it becomes harder to cover more of mathematics (for example, proofs) while maintaining the sequence-to-sequence format, but hopefully by this point the dataset in its current format will have served its purpose in developing models that can reason mathematically. Alternatively, we could consider methods for assessing answers where there is not a single unique answer; for now the full scope of possibilities is too large to include in this paper, but a few possibilities include metrics such as BLEU (Papineni et al., 2002), by extending the data generation process to provide several reference answers, or by obtaining human paraphrases following the data augmentation process proposed by Wang et al. (2015).</p>
<p>We have not addressed linguistic variation or complexity in this dataset. Although to some extent linguistic complexity is orthogonal to the difficulty of the maths problems involved, the two cannot be entirely separated. The most obvious example of this for school-level mathematics is in algebraic word problems, where much of the difficulty lies in translating the description of the problem into an algebraic problem. Thus it would be useful to extend the dataset with "linguistic complexity", where the same underlying mathematical problem is phrased in quite distinct, and not-at-first-obvious, translations. One option may be to do joint training on this dataset, and that of (Ling et al., 2017);</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>another would be to obtain more question templates via mechanical turking, as proposed by Wang et al. (2015).</p>
<p>Finally one completely distinct direction the dataset could be extended is to include visual (e.g. geometry) problems as well. For humans, visual reasoning is an important part of mathematical reasoning, even concerning problems that are not specified in a visual format. Therefore we want to develop questions along these lines, including those that require "intermediate visual representations" (in a similar way to how the textual module composition requires intermediate digital representations) and visual working memory. Note that reasoning with intermediate visual representations or ideas is richer than simply analyzing a visual domain (such as is typical in visual question-answering datasets).</p>
<h1>REFERENCES</h1>
<p>Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli, and Charles Sutton. Learning continuous semantic representations of symbolic expressions. arXiv preprint arXiv:1611.01423, 2016.</p>
<p>Allen Institute for AI. Project Euclid. http://allenai.org/euclid/, 2014.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? arXiv preprint arXiv:1802.08535, 2018.</p>
<p>Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016.</p>
<p>Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626):471, 2016.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.</p>
<p>Danqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin, and Wei-Ying Ma. How well do computers solve math word problems? large-scale dataset construction and evaluation. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 887-896, 2016.</p>
<p>Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pp. 1988-1997. IEEE, 2017.</p>
<p>Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in neural information processing systems, pp. 190-198, 2015.</p>
<p>Łukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.</p>
<p>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. Learning to automatically solve algebra word problems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pp. 271-281, 2014.</p>
<p>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.</p>
<p>Gary F Marcus. The algebraic mind: Integrating connectionism and cognitive science. MIT press, 2003.</p>
<p>Aaron Meurer, Christopher P Smith, Mateusz Paprocki, Ondřej Čertík, Sergey B Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu Ivanov, Jason K Moore, Sartaj Singh, et al. Sympy: symbolic computing in python. PeerJ Computer Science, 3:e103, 2017.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529, 2015.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311-318. Association for Computational Linguistics, 2002.</p>
<p>Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy Lillicrap. Relational recurrent neural networks. arXiv preprint arXiv:1806.01822, 2018a.</p>
<p>Adam Santoro, Felix Hill, David Barrett, Ari Morcos, and Timothy Lillicrap. Measuring abstract reasoning in neural networks. In International Conference on Machine Learning, pp. 4477-4486, 2018b.</p>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bunz, Percy Liang, Leonardo de Moura, and David L Dill. Learning a sat solver from single-bit supervision. arXiv preprint arXiv:1802.03685, 2018.</p>
<p>David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with deep neural networks and tree search. nature, 529(7587):484-489, 2016.</p>
<p>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.</p>
<p>Shyam Upadhyay and Ming-Wei Chang. Annotating derivations: A new evaluation strategy and dataset for algebra word problems. arXiv preprint arXiv:1609.07197, 2016.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000-6010, 2017.</p>
<p>Yan Wang, Xiaojiang Liu, and Shuming Shi. Deep neural solver for math word problems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 845-854, 2017.</p>
<p>Yushi Wang, Jonathan Berant, and Percy Liang. Building a semantic parser overnight. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. $1332-1342,2015$.</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merriënboer, Armand Joulin, and Tomas Mikolov. Towards AI-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698, 2015.</p>
<p>Wojciech Zaremba and Ilya Sutskever. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.</p>
<h1>A RECURRENT ENCODER AND DECODER WITH ATTENTION</h1>
<p>This model consists of an encoder and a decoder (see Figure 2a). The encoder maps the question (as a sequence of characters represented as 1-hot vectors) to a sequence of pairs of keys and values, where each key is a vector of length $k$ and each value is a vector of length $v$. We take $k=v=256$.</p>
<p>We experiment with two different encoder cores. (1) An LSTM with hidden size $k+v$. The hidden state is split to obtain the keys and values. (2) A bidirectional LSTM, i.e. two LSTMs both with hidden size $k+v$, one operating in reverse. The keys and values are generated by concatenating the hidden states and mapping through a linear transformation.</p>
<p>The decoder LSTM has hidden size 2048. At each step, the output of the decoder is passed through a linear transformation to obtain (1) $h$ query vectors each of length $k$, where $h$ is the number of attention heads, and (2) a logits vector of length 96 (the number of possible answer characters, plus a special ignored character). The query vectors are dot-producted with the keys to obtain a softmax weighting over the encoded question values (the standard attention mechanism, as done by e.g. Vaswani et al. (2017)). At every time step, the input to the decoder LSTM is the result of this attention mechanism (the soft-weighted values), concatenated with the 1-hot embedding of the current answer character. (The answer is right-shifted by 1 , so that the LSTM does not get to see the character it is attempting to predict.) In addition we have 15 initial steps where no answer character is fed in to allow the LSTM to integrate information from the question, and the output predictions are ignored. The model is trained using a cross-entropy loss on the output logits for predicting the correct answer.</p>
<h2>B AREAS OF MATHEMATICS</h2>
<h2>B. 1 ALGEBRA</h2>
<p>Some of the algebra modules participate in module composition.</p>
<ul>
<li>linear_1d Solve linear equations in one variable, e.g. solve $2(x-10)+3=17 x+10$ for $x$.</li>
<li>linear_2d Solve simultaneous linear equations in two variables.</li>
<li>polynomial_roots Find roots of polynomials or factorize them, e.g. factorize $2 x^{2}+5 x+3$.</li>
<li>sequence_next_term Find continuations of a sequence given the first few terms. E.g. what comes next in the sequence $2,6,12,20$ ?</li>
<li>sequence_nth_term Find an expression for the $n$th term in a sequence, given the first few terms.</li>
</ul>
<p>For extrapolation tests, we include:</p>
<ul>
<li>polynomial_roots_big Same as polynomial_roots, but with polynomials larger than those seen during training.</li>
</ul>
<h2>B. 2 ARITHMETIC</h2>
<p>Many of the arithmetic modules participate in module composition.</p>
<ul>
<li>add_or_sub Add or subtract a pair of integers or decimals.</li>
<li>add_or_sub_in_base Add or subtract a pair of integers given in a different base (between 2 and 16).</li>
<li>add_sub_multiple Add and subtract multiple integers.</li>
<li>div Divide one integer by another, with the answer a simplified fraction.</li>
<li>mixed Arithmetic involving addition, subtraction, multiplication, division, and brackets.</li>
<li>mul Multiply pair of integers or decimals.</li>
<li>
<p>mul_div_multiple Find simplest fraction of expression involving integers, multiplication, division, and brackets.</p>
</li>
<li>
<p>nearest_integer_root Calculate the nearest integer to an $n$th root of another integer.</p>
</li>
<li>simplify_surd Simplify an expression involving square-roots, e.g. simplify $(\sqrt{10} \times$ $-9) /(\sqrt{2} \times 12) \times-8$.</li>
</ul>
<p>For extrapolation tests, we include:</p>
<ul>
<li>add_or_sub_big Add or subtract a pair of integers bigger than seen during training.</li>
<li>add_sub_multiple Like add_sub_multiple but with more terms than seen during training.</li>
<li>div_big Divide one integer by another, with bigger integers than seen during training.</li>
<li>mixed_longer Like mixed but with more terms.</li>
<li>mul_big Multiply pair of integers bigger than seen during training.</li>
<li>mul_div_multiple_longer Like mul_div_multiple but with more terms.</li>
</ul>
<h1>B. 3 CALCULUS</h1>
<p>The differentiate module fully participates in module composition, accepting inputs from and passing outputs to other modules.</p>
<ul>
<li>differentiate First and higher order derivatives of multivariate polynomials, either specified directly or as a result of module composition. E.g. let $f(x)=2 * x+3$, let $g(x)=x * * 2-17$; what is the derivative of $f(g(x))$ ?</li>
</ul>
<h2>B. 4 COMPARISON</h2>
<p>All comparison modules accept numbers from other modules as inputs.</p>
<ul>
<li>closest Finding the closest to a given number in a list.</li>
<li>kth_biggest Finding the $k$ th biggest or smallest number in a list.</li>
<li>pair Pairwise comparison between pairs of numbers. E.g. which is bigger: $4 / 37$ or $7 / 65$ ?</li>
<li>sort Sorting lists of numbers into ascending or descending order.</li>
</ul>
<p>For extrapolation tests, we include:</p>
<ul>
<li>closest_more Like closest but with larger lists than seen during training.</li>
<li>kth_biggest_more Like kth_biggest but with larger list.</li>
<li>sort_more Sorting longer lists of numbers than seen during training.</li>
</ul>
<h2>B. 5 MEASUREMENT</h2>
<ul>
<li>conversion Conversion between different units of length, time, mass, and volume. E.g. how many millilitres are there in $13 / 8$ of a litre?</li>
<li>time Working with clock times: time differences, and time before or after. E.g. how many minutes are there between 8:05 PM and 9:12 PM?</li>
</ul>
<p>For extrapolation tests, we include:</p>
<ul>
<li>conversion With larger values than seen during training.</li>
</ul>
<h2>B. 6 NUMBERS</h2>
<p>All number modules accept numbers from other modules as inputs.</p>
<ul>
<li>base_conversion Conversion between bases (e.g. give 1011001 (base 2) in base 16).</li>
<li>
<p>div_remainder Calculate remainders under division.</p>
</li>
<li>
<p>gcd Calculating greatest common divisors.</p>
</li>
<li>is_factor Recognizing factors, e.g. is 15 a factor of 60 ?</li>
<li>is_prime Testing for primality.</li>
<li>lcm Calculating least common multiples.</li>
<li>list_prime_factors Factoring numbers into primes. E.g. give the prime factors of 64372.</li>
<li>place_value Give the place value of a number, e.g. what is the tens digit of 3585792 ?</li>
<li>round_number Rounding integers and decimals. E.g. give 432.1058 to three decimal places.</li>
</ul>
<p>For extrapolation tests, we include:</p>
<ul>
<li>round_number_big Like round_number but with larger numbers than seen during training.</li>
<li>place_value_big Like place_value but with larger numbers than seen during training.</li>
</ul>
<h1>B. 7 Polynomials</h1>
<p>All function modules are fully compositional: they accept functions specified by other questions as inputs, and define functions for use in other modules.</p>
<ul>
<li>add Adding functions. E.g. calculating $2 f(x)+17 g(x)$ given $f$ and $g$.</li>
<li>collect Simplify polynomial expressions by collecting terms.</li>
<li>compose Calculating the composition of functions.</li>
<li>coefficient_named E.g. rearrange $(x+1)(2 x+3)$ to $a x^{2}+b x+c$ and give $b$.</li>
<li>evaluate E.g. value of $x^{2} y^{2}+2 x y$ when $x=2, y=3$.</li>
<li>expand Expand and simplify polynomials, e.g. expand $(x+1)(2 x+3)$.</li>
<li>simplify_power Simplify powers, testing rules of power indices. E.g. simplify $x^{3} / x^{2}$.</li>
</ul>
<h2>B. 8 Probability</h2>
<p>There are two modules here, both based on sampling without replacement from a bag of repeated letters, specified using either: (1) counts (e.g. ${\mathrm{a}: 1, \mathrm{~b}: 7}$ ), or (2) an unsorted list of letters that require counting, e.g. ecggccdcdceeeeg.</p>
<ul>
<li>swr_p_level_set Calculating probability of obtaining certain counts of different letters.</li>
<li>swr_p_sequence Calculating probability of obtaining a given sequence of letters.</li>
</ul>
<p>For extrapolation tests, we include the same modules, but with more letters sampled from the bag than seen during training:</p>
<ul>
<li>swr_p_level_set_more_samples</li>
<li>swr_p_sequence_more_samples</li>
</ul>
<h2>C PER-MODULE PERFORMANCE</h2>
<p>Interpolation test performance is shown in Figure 4 and extrapolation test performance is shown in Figure 5. Of the different encoders for the recurrent attention architecture, we show the per-module performance of the bidirectional LSTM encoder which has the greatest performance.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 4: Interpolation test performance on the different modules.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 5: Extrapolation test performance on the different modules.</p>
<h1>D HIGH-SCHOOL MATHEMATICS QUESTIONS</h1>
<ol>
<li>Factorise $x^{2}+7 x$</li>
<li>Factorise $y^{2}-10 y+16$</li>
<li>Factorise $2 t^{2}+5 t+2$</li>
<li>Simplify $\frac{(x+1)}{2}+\frac{(x+3)}{3}$</li>
<li>Solve $2 x^{2}+9 x+7$</li>
<li>Solve $\frac{2}{y^{2}}+\frac{9}{y}-7=0$</li>
<li>Expand $3(x+4)+2(5 x-1)$</li>
<li>Expand $(2 x+1)(x-4)$</li>
<li>Factor $6 y^{2}-9 x y$</li>
<li>Solve $3 p-7&gt;11$</li>
<li>$A=4 b c, A=100, b=2$, calculate $c$</li>
<li>Make $k$ the subject of $m=\sqrt{\left(\frac{k+1}{4}\right)}$</li>
<li>Expand $(p+9)(p-4)$</li>
<li>Solve $\frac{(5 w-8)}{3}=4 w+2$</li>
<li>Factorise $x^{2}-49$</li>
<li>Expand $(x-7)(x+1)$</li>
<li>Simplify $\sqrt{9 x^{8} y^{3}}$ assuming x is positive.</li>
<li>$p^{2}=\frac{(x-y)}{x y}, x=8.5, y=4$, find p</li>
<li>Make $t$ the subject of $2(d-t)=4 t+7$</li>
<li>Solve $3 x^{2}-4 x-2=0$</li>
<li>Expand $3(2 y-5)$</li>
<li>Factorise $8 x^{2}+4 x y$</li>
<li>Make $h$ the subject of $t=\frac{y h}{10}$</li>
<li>Simplify $\left(m^{-2}\right)^{5}$</li>
<li>Factorise $x^{2}+3 x-10$</li>
<li>
<p>Solve $5 x+2 y=11$ and $4 x-3 y=18$ for $x$</p>
</li>
<li>
<p>Simplify $\frac{\left(x^{2}+3 x-4\right)}{\left(2 x^{2}-5 x+3\right)}$</p>
</li>
<li>Simplify $\frac{4}{(x+2)}+\frac{3}{(x-2)}$</li>
<li>Expand $4(3 x+5)$</li>
<li>Expand $2(x-4)+3(x+5)$</li>
<li>Expand $(x+4)(x+6)$</li>
<li>Simplify $\frac{m^{5}}{m^{3}}$</li>
<li>Simplify $\left(5 x^{4} y^{3}\right)\left(x^{2} y\right)$</li>
<li>Solve $3 x+2 y=4$ and $4 x+5 y=17$ for x</li>
<li>Complete the sequence: $3,9,15,21,27$</li>
<li>Simplify $5 x+4 y+x-7 y$</li>
<li>Complete the sequence: $3,10,17,24$</li>
<li>Simplify $x^{10} x^{3}$</li>
<li>Solve $7 *(x+2)=7$</li>
<li>Factorise $x^{2}-12 x+27$</li>
</ol>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ Edexcel exam board Higher Tier, 2012-2013.
${ }^{3} \mathrm{https}: / /$ qualifications.pearson.com/content/dam/pdf/Support/Grade-boundaries/GCSE/1211-GCSE-Unit-UMS-Boundaries(Science-Mathematics).pdf&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
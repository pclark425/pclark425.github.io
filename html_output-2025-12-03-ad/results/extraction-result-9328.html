<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9328 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9328</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9328</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-163.html">extraction-schema-163</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <p><strong>Paper ID:</strong> paper-279464153</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2506.16584v1.pdf" target="_blank">Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework</a></p>
                <p><strong>Paper Abstract:</strong> Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surface-level patterns-is central to assessing their reliability, especially in high-stakes applications. We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents. We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability. An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation. This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording. We apply this framework to evaluate LLMs across diverse domains. Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model. This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest. These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9328.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9328.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BT-paraphrase (70B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Back-translation paraphrase presentation (evaluated on LLaMA 3.3 70B Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prompt presentation using chained back-translation to generate semantically equivalent but surface-diverse paraphrases; used to measure articulation vs. intent sensitivity in the paper's experiments with LLaMA 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.3 70B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-domain numeric-estimation tasks (health, logistics, travel, finance, social planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended user-style queries where a key variable/value (e.g., number of stops, dose quantity, budget) changes the intent; evaluator extracts a numeric final value from free-form responses.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Back-translation chains: original English prompt → two randomly sampled intermediate languages (chain must include at least one of Chinese/Japanese/Arabic) → back to English; generate 500 paraphrases per base prompt, verify semantic equivalence with GPT-4o-mini, embed and greedily select 50 maximally diverse prompts; present each prompt at temperature=1 and collect 50 model responses per prompt; extract numeric value via GPT-4o-mini post-processor.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Survey-style prompt generation (LLM asked to write prompts as human survey participants) and the original base prompt; paper reports results from both the back-translation pipeline and a survey-based variant (Appendix) for 8B and 70B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported decomposition shares (Purpose Sensitivity PS, Articulation Sensitivity AS, Model Uncertainty MU) for representative tasks: Personal finance (moving boxes): PS=0.805, AS=0.023, MU=0.173; Logistics (deliveries in downtown SF): PS=0.482, AS=0.077, MU=0.441; Health (prednisone duration): PS=0.947, AS=0.006, MU=0.048.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to smaller LLaMA models on same tasks: 3B (moving boxes) PS=0.039, AS=0.037, MU=0.924; 8B (moving boxes) PS=0.024, AS=0.048, MU=0.928. (Tables 1-4 provide full per-task decompositions across models.)</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large PS differences on some tasks: e.g., moving-boxes task PS(70B)=0.805 vs PS(3B)=0.039 (Δ ≈ +0.766 PS share); many tasks show more modest differences. Effect sizes vary strongly by task/domain.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Back-translation produces paraphrases that preserve intent while varying surface form; models with stronger world-modeling should allocate more variance to purpose (PS) and less to articulation (AS). The paper finds larger models (70B) generally attribute a greater share of variance to intent (higher PS) but sometimes also show slightly higher AS, indicating a trade-off between semantic generalization and sensitivity to surface cues.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Per task: 3 intents × 50 prompt variants × 50 responses = 7,500 responses; responses generated at temperature=1 and limited to 600 tokens; extraction of numeric values via GPT-4o-mini; paraphrase pipeline required at least one typologically distant language (Chinese/Japanese/Arabic) in the chain; 50 paraphrases chosen by embedding-based greedy diversity from 500 candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9328.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BT-paraphrase (8B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Back-translation paraphrase presentation (evaluated on LLaMA 3.2 8B Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same back-translation prompt-generation pipeline as above, evaluated on the 8B LLaMA model to quantify PS/AS/MU decomposition under varied articulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.2 8B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-domain numeric-estimation tasks (health, logistics, travel, finance, social planning)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Open-ended numeric extraction tasks with small input changes that alter intent (e.g., number of stops, dose, income level).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Back-translation chain paraphrases (same pipeline as used for 70B): 50 retained diverse paraphrases per base prompt; 50 responses per prompt at temperature=1; numeric extraction via GPT-4o-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to the same tasks for 3B, 70B, Gemma models; also compared qualitatively to survey-based prompts (Appendix).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Representative decomposition shares from paper tables: Personal finance (moving boxes): PS=0.024, AS=0.048, MU=0.928; Logistics (deliveries SF): PS=0.242, AS=0.046, MU=0.712; Health (prednisone): PS=0.316, AS=0.093, MU=0.590.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to 70B: e.g., moving boxes PS(70B)=0.805 vs PS(8B)=0.024 (Δ ≈ -0.781); deliveries PS(70B)=0.482 vs PS(8B)=0.242 (Δ ≈ -0.240).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Substantial PS deficits relative to 70B on some tasks (e.g., moving-boxes ΔPS ≈ -0.78).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller models show lower Purpose Sensitivity and higher residual uncertainty (MU), suggesting less stable mapping from paraphrase to intent and more reliance on surface cues or internal uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same sampling/collection details as for 70B; results appear in Tables 1-4 for each task/domain with bootstrap standard errors (500 stratified bootstraps).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9328.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BT-paraphrase (3B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Back-translation paraphrase presentation (evaluated on LLaMA 3.2 3B Instruct)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Back-translation paraphrase evaluation pipeline applied to the 3B LLaMA model to quantify sensitivity to articulation vs. intent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA 3.2 3B Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-domain numeric-estimation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of tasks used across models (health, logistics, travel, finance, social planning) requiring numeric extraction from free-form responses.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Back-translation paraphrase pipeline; 50 diverse prompts per intent; 50 responses per prompt at temperature=1; numeric extraction and evaluation via GPT-4o-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to 8B, 70B, Gemma 12B/27B on same tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Representative decomposition shares: Personal finance (moving boxes): PS=0.039, AS=0.037, MU=0.924; Logistics (commute commute distance example): PS=0.163, AS=0.055, MU=0.782; Health (hike calories): PS=0.106, AS=0.033, MU=0.861.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Lower PS and higher MU compared to larger models (e.g., moving-boxes PS(70B)=0.805 vs PS(3B)=0.039).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Large negative effect relative to 70B on some tasks (see moving-boxes ΔPS ≈ -0.766).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Smaller model exhibits limited ability to map divergent surface forms to the same intent and therefore less variance explained by intent (low PS) and more unexplained variability (high MU).</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same experimental pipeline; per-task decompositions and bootstrap standard errors reported in Tables 1-5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9328.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BT-paraphrase (Gemma12B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Back-translation paraphrase presentation (evaluated on Google's Gemma 3 12B-IT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Gemma 12B with the paper's back-translation prompt pipeline, reporting PS/AS/MU decomposition across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma 3 12B-IT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-domain numeric-estimation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same set of domains/tasks (health, logistics, travel, finance, social planning) with extracted numeric outputs from free-form model responses.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Back-translation chain paraphrases (same pipeline), 50 diverse prompts per intent, 50 responses per prompt at temperature=1; numeric extraction via GPT-4o-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared to LLaMA 3B/8B/70B and Gemma 27B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Representative decomposition shares: Personal finance (moving boxes) PS=0.515, AS=0.065, MU=0.419; Logistics (deliveries SF) PS=0.317, AS=0.048, MU=0.635; Health (prednisone) PS=0.697, AS=0.045, MU=0.257.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Intermediate PS performance: better than smaller LLaMA models on some tasks but often below 70B (e.g., moving boxes PS(70B)=0.805 vs PS(Gemma12B)=0.515).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Moderate PS differences versus both smaller and larger models depending on the task (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Gemma 12B often shows higher PS than small LLaMA models, indicating model- and architecture-dependent differences in mapping paraphrases to intent; AS and MU vary by domain.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same paraphrase-generation and response-sampling pipeline; full per-task decompositions with bootstrap standard errors in Tables 1-5.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9328.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BT-paraphrase (Gemma27B)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Back-translation paraphrase presentation (evaluated on Google's Gemma 3 27B-IT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of Gemma 27B with the back-translation prompt-generation pipeline; provides PS/AS/MU decompositions showing stronger sensitivity to intent on many tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma 3 27B-IT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>27B</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Multi-domain numeric-estimation tasks</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same domains/tasks as other models; numeric-value extraction from natural-language model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Back-translation paraphrases (same pipeline), 50 prompts × 50 responses per intent at temperature=1; extraction via GPT-4o-mini.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared across LLaMA 3B/8B/70B and Gemma 12B.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Representative decomposition shares: Personal finance (moving boxes) PS=0.423, AS=0.027, MU=0.550; Logistics (deliveries SF) PS=0.419, AS=0.042, MU=0.539; Health (prednisone) PS=0.834, AS=0.032, MU=0.134.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Generally higher PS than smaller models on several tasks; on some tasks PS approaches or exceeds that of 70B (task-dependent).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Task-dependent PS differences; e.g., prednisone PS(Gemma27B)=0.834 vs PS(8B)=0.316 (Δ ≈ +0.518).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Gemma 27B shows relatively high PS on medically grounded tasks, indicating architecture and training data interact with prompt phrasing to affect world-model-like generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Same experimental pipeline and measurement procedures; tables include bootstrap confidence intervals per task.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9328.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Survey prompts</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Survey-style prompt generation (LLM-played human prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Alternative prompt-presentation format where an LLM is instructed to act like a survey participant and produce natural rephrasings of scenario prompts; used as an alternative prompt generation method in the Appendix.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Subset of the same multi-domain tasks (Appendix analyses)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>The same numeric extraction tasks, but prompts were created by asking an LLM to simulate a human writing a question rather than via back-translation.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Prompt generation by instructing GPT-4o-mini to act as a survey participant and write a single, naturally worded prompt for the scenario; used to create an alternative set of semantically equivalent prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Compared qualitatively and quantitatively (Appendix) to back-translation paraphrases for 8B and 70B LLaMA models.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper reports qualitatively similar PS/AS/MU patterns for the survey-generated prompts as for back-translation (Appendix figures show comparable decompositions for 8B and 70B). No new numerical decomposition table distinct from the main back-translation results is presented beyond qualitative similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Patterns across the two prompt-generation methods were qualitatively similar for the evaluated models (8B and 70B), i.e., the decomposition trends (relative PS/AS/MU) remained consistent.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Survey-style prompts are a human-like alternative to back-translation; results suggest the findings (PS higher for larger models, AS/task dependence) are not an artifact of the translation pipeline alone.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Appendix contains the survey prompt list and figures; analyses for 8B and 70B show qualitatively similar decomposition patterns to the back-translation pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9328.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProSA / PSS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ProSA framework / PromptSensiScore (PSS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A previously proposed metric that quantifies average performance variation across semantically equivalent prompt variants at the instance level (from related work cited in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Related-work metric for prompt sensitivity measurement (instance-level variation in task performance across prompt variants).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not used in this paper's experiments; described in related work as an alternate scalar measure of prompt sensitivity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Cited as prior work showing prompt sensitivity varies across tasks, models, and prompt types, especially in reasoning and creative tasks; the authors position their variance-decomposition as complementary (distributional and behaviorally grounded) to such scalar measures.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e9328.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FORMATSPREAD</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FORMATSPREAD (sensitivity to spurious prompt formatting features)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A metric from related work that captures the spread in task accuracy across equivalent prompt formats, highlighting sensitivity to punctuation/spacing/capitalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Related-work measure of prompt-format sensitivity (difference in best and worst format accuracy).</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not used in this paper's experiments; referenced in related work for demonstrating large swings in accuracy due to superficial formatting changes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>Reported in the cited work (not this paper) as up to ~76 accuracy points across formats on some tasks; paper references this to motivate need for semantic diagnostics.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>FORMATSPREAD demonstrates that superficial formatting can produce large performance differences; the present paper argues for variance-decomposition (PS/AS/MU) focused on semantic consistency rather than only accuracy spreads.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9328.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e9328.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how different problem or prompt presentation formats affect the performance of large language models (LLMs) on tasks, including details about the formats, tasks, models, performance metrics, comparisons, and any explanations or findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt aggregation (ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aggregating responses across multiple prompts (ensemble of prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior methodological recommendation (related work) to reduce prompt-specific bias by aggregating model outputs over multiple semantically equivalent prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Technique to counteract individual prompt biases by averaging/ensembling results across prompt variants.</td>
                        </tr>
                        <tr>
                            <td><strong>presentation_format</strong></td>
                            <td>Not executed as a primary experimental condition in this paper, but referenced as a recommended mitigation in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Prior work recommends aggregating across prompts to reduce idiosyncratic prompt-induced biases; the paper notes this idea but focuses on diagnosing how much variation stems from intent vs. articulation to inform such mitigation choices.</td>
                        </tr>
                        <tr>
                            <td><strong>null_or_negative_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework', 'publication_date_yy_mm': '2025-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ProSA: Assessing and understanding the prompt sensitivity of llms <em>(Rating: 2)</em></li>
                <li>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting <em>(Rating: 2)</em></li>
                <li>When benchmarks are targets: Revealing the sensitivity of large language model leaderboards <em>(Rating: 2)</em></li>
                <li>On the dangers of stochastic parrots: Can language models be too big <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9328",
    "paper_id": "paper-279464153",
    "extraction_schema_id": "extraction-schema-163",
    "extracted_data": [
        {
            "name_short": "BT-paraphrase (70B)",
            "name_full": "Back-translation paraphrase presentation (evaluated on LLaMA 3.3 70B Instruct)",
            "brief_description": "Prompt presentation using chained back-translation to generate semantically equivalent but surface-diverse paraphrases; used to measure articulation vs. intent sensitivity in the paper's experiments with LLaMA 70B.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.3 70B Instruct",
            "model_size": "70B",
            "task_name": "Multi-domain numeric-estimation tasks (health, logistics, travel, finance, social planning)",
            "task_description": "Open-ended user-style queries where a key variable/value (e.g., number of stops, dose quantity, budget) changes the intent; evaluator extracts a numeric final value from free-form responses.",
            "presentation_format": "Back-translation chains: original English prompt → two randomly sampled intermediate languages (chain must include at least one of Chinese/Japanese/Arabic) → back to English; generate 500 paraphrases per base prompt, verify semantic equivalence with GPT-4o-mini, embed and greedily select 50 maximally diverse prompts; present each prompt at temperature=1 and collect 50 model responses per prompt; extract numeric value via GPT-4o-mini post-processor.",
            "comparison_format": "Survey-style prompt generation (LLM asked to write prompts as human survey participants) and the original base prompt; paper reports results from both the back-translation pipeline and a survey-based variant (Appendix) for 8B and 70B.",
            "performance": "Reported decomposition shares (Purpose Sensitivity PS, Articulation Sensitivity AS, Model Uncertainty MU) for representative tasks: Personal finance (moving boxes): PS=0.805, AS=0.023, MU=0.173; Logistics (deliveries in downtown SF): PS=0.482, AS=0.077, MU=0.441; Health (prednisone duration): PS=0.947, AS=0.006, MU=0.048.",
            "performance_comparison": "Compared to smaller LLaMA models on same tasks: 3B (moving boxes) PS=0.039, AS=0.037, MU=0.924; 8B (moving boxes) PS=0.024, AS=0.048, MU=0.928. (Tables 1-4 provide full per-task decompositions across models.)",
            "format_effect_size": "Large PS differences on some tasks: e.g., moving-boxes task PS(70B)=0.805 vs PS(3B)=0.039 (Δ ≈ +0.766 PS share); many tasks show more modest differences. Effect sizes vary strongly by task/domain.",
            "explanation_or_hypothesis": "Back-translation produces paraphrases that preserve intent while varying surface form; models with stronger world-modeling should allocate more variance to purpose (PS) and less to articulation (AS). The paper finds larger models (70B) generally attribute a greater share of variance to intent (higher PS) but sometimes also show slightly higher AS, indicating a trade-off between semantic generalization and sensitivity to surface cues.",
            "null_or_negative_result": false,
            "experimental_details": "Per task: 3 intents × 50 prompt variants × 50 responses = 7,500 responses; responses generated at temperature=1 and limited to 600 tokens; extraction of numeric values via GPT-4o-mini; paraphrase pipeline required at least one typologically distant language (Chinese/Japanese/Arabic) in the chain; 50 paraphrases chosen by embedding-based greedy diversity from 500 candidates.",
            "uuid": "e9328.0",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BT-paraphrase (8B)",
            "name_full": "Back-translation paraphrase presentation (evaluated on LLaMA 3.2 8B Instruct)",
            "brief_description": "Same back-translation prompt-generation pipeline as above, evaluated on the 8B LLaMA model to quantify PS/AS/MU decomposition under varied articulations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.2 8B Instruct",
            "model_size": "8B",
            "task_name": "Multi-domain numeric-estimation tasks (health, logistics, travel, finance, social planning)",
            "task_description": "Open-ended numeric extraction tasks with small input changes that alter intent (e.g., number of stops, dose, income level).",
            "presentation_format": "Back-translation chain paraphrases (same pipeline as used for 70B): 50 retained diverse paraphrases per base prompt; 50 responses per prompt at temperature=1; numeric extraction via GPT-4o-mini.",
            "comparison_format": "Compared to the same tasks for 3B, 70B, Gemma models; also compared qualitatively to survey-based prompts (Appendix).",
            "performance": "Representative decomposition shares from paper tables: Personal finance (moving boxes): PS=0.024, AS=0.048, MU=0.928; Logistics (deliveries SF): PS=0.242, AS=0.046, MU=0.712; Health (prednisone): PS=0.316, AS=0.093, MU=0.590.",
            "performance_comparison": "Compared to 70B: e.g., moving boxes PS(70B)=0.805 vs PS(8B)=0.024 (Δ ≈ -0.781); deliveries PS(70B)=0.482 vs PS(8B)=0.242 (Δ ≈ -0.240).",
            "format_effect_size": "Substantial PS deficits relative to 70B on some tasks (e.g., moving-boxes ΔPS ≈ -0.78).",
            "explanation_or_hypothesis": "Smaller models show lower Purpose Sensitivity and higher residual uncertainty (MU), suggesting less stable mapping from paraphrase to intent and more reliance on surface cues or internal uncertainty.",
            "null_or_negative_result": false,
            "experimental_details": "Same sampling/collection details as for 70B; results appear in Tables 1-4 for each task/domain with bootstrap standard errors (500 stratified bootstraps).",
            "uuid": "e9328.1",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BT-paraphrase (3B)",
            "name_full": "Back-translation paraphrase presentation (evaluated on LLaMA 3.2 3B Instruct)",
            "brief_description": "Back-translation paraphrase evaluation pipeline applied to the 3B LLaMA model to quantify sensitivity to articulation vs. intent.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA 3.2 3B Instruct",
            "model_size": "3B",
            "task_name": "Multi-domain numeric-estimation tasks",
            "task_description": "Same set of tasks used across models (health, logistics, travel, finance, social planning) requiring numeric extraction from free-form responses.",
            "presentation_format": "Back-translation paraphrase pipeline; 50 diverse prompts per intent; 50 responses per prompt at temperature=1; numeric extraction and evaluation via GPT-4o-mini.",
            "comparison_format": "Compared to 8B, 70B, Gemma 12B/27B on same tasks.",
            "performance": "Representative decomposition shares: Personal finance (moving boxes): PS=0.039, AS=0.037, MU=0.924; Logistics (commute commute distance example): PS=0.163, AS=0.055, MU=0.782; Health (hike calories): PS=0.106, AS=0.033, MU=0.861.",
            "performance_comparison": "Lower PS and higher MU compared to larger models (e.g., moving-boxes PS(70B)=0.805 vs PS(3B)=0.039).",
            "format_effect_size": "Large negative effect relative to 70B on some tasks (see moving-boxes ΔPS ≈ -0.766).",
            "explanation_or_hypothesis": "Smaller model exhibits limited ability to map divergent surface forms to the same intent and therefore less variance explained by intent (low PS) and more unexplained variability (high MU).",
            "null_or_negative_result": false,
            "experimental_details": "Same experimental pipeline; per-task decompositions and bootstrap standard errors reported in Tables 1-5.",
            "uuid": "e9328.2",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BT-paraphrase (Gemma12B)",
            "name_full": "Back-translation paraphrase presentation (evaluated on Google's Gemma 3 12B-IT)",
            "brief_description": "Evaluation of Gemma 12B with the paper's back-translation prompt pipeline, reporting PS/AS/MU decomposition across tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma 3 12B-IT",
            "model_size": "12B",
            "task_name": "Multi-domain numeric-estimation tasks",
            "task_description": "Same set of domains/tasks (health, logistics, travel, finance, social planning) with extracted numeric outputs from free-form model responses.",
            "presentation_format": "Back-translation chain paraphrases (same pipeline), 50 diverse prompts per intent, 50 responses per prompt at temperature=1; numeric extraction via GPT-4o-mini.",
            "comparison_format": "Compared to LLaMA 3B/8B/70B and Gemma 27B.",
            "performance": "Representative decomposition shares: Personal finance (moving boxes) PS=0.515, AS=0.065, MU=0.419; Logistics (deliveries SF) PS=0.317, AS=0.048, MU=0.635; Health (prednisone) PS=0.697, AS=0.045, MU=0.257.",
            "performance_comparison": "Intermediate PS performance: better than smaller LLaMA models on some tasks but often below 70B (e.g., moving boxes PS(70B)=0.805 vs PS(Gemma12B)=0.515).",
            "format_effect_size": "Moderate PS differences versus both smaller and larger models depending on the task (task-dependent).",
            "explanation_or_hypothesis": "Gemma 12B often shows higher PS than small LLaMA models, indicating model- and architecture-dependent differences in mapping paraphrases to intent; AS and MU vary by domain.",
            "null_or_negative_result": false,
            "experimental_details": "Same paraphrase-generation and response-sampling pipeline; full per-task decompositions with bootstrap standard errors in Tables 1-5.",
            "uuid": "e9328.3",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "BT-paraphrase (Gemma27B)",
            "name_full": "Back-translation paraphrase presentation (evaluated on Google's Gemma 3 27B-IT)",
            "brief_description": "Evaluation of Gemma 27B with the back-translation prompt-generation pipeline; provides PS/AS/MU decompositions showing stronger sensitivity to intent on many tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma 3 27B-IT",
            "model_size": "27B",
            "task_name": "Multi-domain numeric-estimation tasks",
            "task_description": "Same domains/tasks as other models; numeric-value extraction from natural-language model outputs.",
            "presentation_format": "Back-translation paraphrases (same pipeline), 50 prompts × 50 responses per intent at temperature=1; extraction via GPT-4o-mini.",
            "comparison_format": "Compared across LLaMA 3B/8B/70B and Gemma 12B.",
            "performance": "Representative decomposition shares: Personal finance (moving boxes) PS=0.423, AS=0.027, MU=0.550; Logistics (deliveries SF) PS=0.419, AS=0.042, MU=0.539; Health (prednisone) PS=0.834, AS=0.032, MU=0.134.",
            "performance_comparison": "Generally higher PS than smaller models on several tasks; on some tasks PS approaches or exceeds that of 70B (task-dependent).",
            "format_effect_size": "Task-dependent PS differences; e.g., prednisone PS(Gemma27B)=0.834 vs PS(8B)=0.316 (Δ ≈ +0.518).",
            "explanation_or_hypothesis": "Gemma 27B shows relatively high PS on medically grounded tasks, indicating architecture and training data interact with prompt phrasing to affect world-model-like generalization.",
            "null_or_negative_result": false,
            "experimental_details": "Same experimental pipeline and measurement procedures; tables include bootstrap confidence intervals per task.",
            "uuid": "e9328.4",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Survey prompts",
            "name_full": "Survey-style prompt generation (LLM-played human prompts)",
            "brief_description": "Alternative prompt-presentation format where an LLM is instructed to act like a survey participant and produce natural rephrasings of scenario prompts; used as an alternative prompt generation method in the Appendix.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": null,
            "model_size": null,
            "task_name": "Subset of the same multi-domain tasks (Appendix analyses)",
            "task_description": "The same numeric extraction tasks, but prompts were created by asking an LLM to simulate a human writing a question rather than via back-translation.",
            "presentation_format": "Prompt generation by instructing GPT-4o-mini to act as a survey participant and write a single, naturally worded prompt for the scenario; used to create an alternative set of semantically equivalent prompts.",
            "comparison_format": "Compared qualitatively and quantitatively (Appendix) to back-translation paraphrases for 8B and 70B LLaMA models.",
            "performance": "Paper reports qualitatively similar PS/AS/MU patterns for the survey-generated prompts as for back-translation (Appendix figures show comparable decompositions for 8B and 70B). No new numerical decomposition table distinct from the main back-translation results is presented beyond qualitative similarity.",
            "performance_comparison": "Patterns across the two prompt-generation methods were qualitatively similar for the evaluated models (8B and 70B), i.e., the decomposition trends (relative PS/AS/MU) remained consistent.",
            "format_effect_size": null,
            "explanation_or_hypothesis": "Survey-style prompts are a human-like alternative to back-translation; results suggest the findings (PS higher for larger models, AS/task dependence) are not an artifact of the translation pipeline alone.",
            "null_or_negative_result": null,
            "experimental_details": "Appendix contains the survey prompt list and figures; analyses for 8B and 70B show qualitatively similar decomposition patterns to the back-translation pipeline.",
            "uuid": "e9328.5",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "ProSA / PSS",
            "name_full": "ProSA framework / PromptSensiScore (PSS)",
            "brief_description": "A previously proposed metric that quantifies average performance variation across semantically equivalent prompt variants at the instance level (from related work cited in the paper).",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": null,
            "task_description": "Related-work metric for prompt sensitivity measurement (instance-level variation in task performance across prompt variants).",
            "presentation_format": "Not used in this paper's experiments; described in related work as an alternate scalar measure of prompt sensitivity.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Cited as prior work showing prompt sensitivity varies across tasks, models, and prompt types, especially in reasoning and creative tasks; the authors position their variance-decomposition as complementary (distributional and behaviorally grounded) to such scalar measures.",
            "null_or_negative_result": null,
            "experimental_details": null,
            "uuid": "e9328.6",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "FORMATSPREAD",
            "name_full": "FORMATSPREAD (sensitivity to spurious prompt formatting features)",
            "brief_description": "A metric from related work that captures the spread in task accuracy across equivalent prompt formats, highlighting sensitivity to punctuation/spacing/capitalization.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": null,
            "task_description": "Related-work measure of prompt-format sensitivity (difference in best and worst format accuracy).",
            "presentation_format": "Not used in this paper's experiments; referenced in related work for demonstrating large swings in accuracy due to superficial formatting changes.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": "Reported in the cited work (not this paper) as up to ~76 accuracy points across formats on some tasks; paper references this to motivate need for semantic diagnostics.",
            "explanation_or_hypothesis": "FORMATSPREAD demonstrates that superficial formatting can produce large performance differences; the present paper argues for variance-decomposition (PS/AS/MU) focused on semantic consistency rather than only accuracy spreads.",
            "null_or_negative_result": null,
            "experimental_details": null,
            "uuid": "e9328.7",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        },
        {
            "name_short": "Prompt aggregation (ensemble)",
            "name_full": "Aggregating responses across multiple prompts (ensemble of prompts)",
            "brief_description": "A prior methodological recommendation (related work) to reduce prompt-specific bias by aggregating model outputs over multiple semantically equivalent prompts.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "task_name": null,
            "task_description": "Technique to counteract individual prompt biases by averaging/ensembling results across prompt variants.",
            "presentation_format": "Not executed as a primary experimental condition in this paper, but referenced as a recommended mitigation in prior work.",
            "comparison_format": null,
            "performance": null,
            "performance_comparison": null,
            "format_effect_size": null,
            "explanation_or_hypothesis": "Prior work recommends aggregating across prompts to reduce idiosyncratic prompt-induced biases; the paper notes this idea but focuses on diagnosing how much variation stems from intent vs. articulation to inform such mitigation choices.",
            "null_or_negative_result": null,
            "experimental_details": null,
            "uuid": "e9328.8",
            "source_info": {
                "paper_title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
                "publication_date_yy_mm": "2025-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ProSA: Assessing and understanding the prompt sensitivity of llms",
            "rating": 2,
            "sanitized_title": "prosa_assessing_and_understanding_the_prompt_sensitivity_of_llms"
        },
        {
            "paper_title": "Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting",
            "rating": 2,
            "sanitized_title": "quantifying_language_models_sensitivity_to_spurious_features_in_prompt_design_or_how_i_learned_to_start_worrying_about_prompt_formatting"
        },
        {
            "paper_title": "When benchmarks are targets: Revealing the sensitivity of large language model leaderboards",
            "rating": 2,
            "sanitized_title": "when_benchmarks_are_targets_revealing_the_sensitivity_of_large_language_model_leaderboards"
        },
        {
            "paper_title": "On the dangers of stochastic parrots: Can language models be too big",
            "rating": 1,
            "sanitized_title": "on_the_dangers_of_stochastic_parrots_can_language_models_be_too_big"
        }
    ],
    "cost": 0.01942375,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework
19 Jun 2025</p>
<p>Nadav Kunievsky nadavkunievsky@uchicago.edu 
Knowledge Lab University of Chicago</p>
<p>James A Evans jevans@uchicago.edu 
Knowledge Lab Data Science Institute Department of Sociology
University of Chicago</p>
<p>Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework
19 Jun 202513DFC0D67E9F43B442BADD7A13A51BFFarXiv:2506.16584v1[cs.CL]
Understanding whether large language models (LLMs) possess a world model-a structured understanding of the world that supports generalization beyond surfacelevel patterns-is central to assessing their reliability, especially in high-stakes applications.We propose a formal framework for evaluating whether an LLM exhibits a sufficiently robust world model, defined as producing consistent outputs across semantically equivalent prompts while distinguishing between prompts that express different intents.We introduce a new evaluation approach to measure this that decomposes model response variability into three components: variability due to user purpose, user articulation, and model instability.An LLM with a strong world model should attribute most of the variability in its responses to changes in foundational purpose rather than superficial changes in articulation.This approach allows us to quantify how much of a model's behavior is semantically grounded rather than driven by model instability or alternative wording.We apply this framework to evaluate LLMs across diverse domains.Our results show how larger models attribute a greater share of output variability to changes in user purpose, indicating a more robust world model.This improvement is not uniform, however: larger models do not consistently outperform smaller ones across all domains, and their advantage in robustness is often modest.These findings highlight the importance of moving beyond accuracy-based benchmarks toward semantic diagnostics that more directly assess the structure and stability of a model's internal understanding of the world.</p>
<p>Introduction</p>
<p>A long-standing goal in natural language AI research is to build models that move beyond pattern recognition and exhibit a structured understanding of the world.In natural language processing, this goal is often framed in terms of a world model-an internal representation of how concepts, entities, and user goals relate to one another [ [13,14,12,18,25,24,17]]. Evaluating whether a generative language model possesses such a model is challenging because it is difficult to distinguish genuine understanding from mere memorization of training data [3].</p>
<p>Despite the centrality of the idea, there is no agreed upon definition of what it means for a language model to possess a world model.In this paper, we propose that for a large language model to have a world model, it must generalize meaningfully across diverse inputs, distinguish between semantically distinct requests, and maintain internal coherence in its outputs.Our key insight is to treat user intent, or purpose, as a latent variable that underlies every user prompt.While users may express the same purpose in many ways, a model with a coherent world model should produce consistent response Preprint.Under review.distributions across these surface variations and should shift its responses when the users underlying purpose changes.A language model possesses the world model property if its responses are invariant to alternate phrasing or articulation when purpose is fixed but vary when purpose changes.</p>
<p>To operationalize this conception, we first introduce the more practical notion of a sufficient world model.Instead of requiring distributional invariance across prompts, we require invariance across extracted meaning from model responses.We then present a diagnostic method that decomposes variation from model outputs into three components: Purpose Sensitivity, the share of variation due to changes in purpose or intent; Articulation Sensitivity, the share due to articulation or phrasing; and Model Uncertainty, the residual that stems from the model's own uncertainty about the world (see Figure 1 for illustration).A robust world model implies high Purpose Sensitivity and low Articulation Sensitivity, indicating consistent extraction of meaning that disregards superficial cues.</p>
<p>Because intent is unobserved and generating equivalent prompts that differ only in form is nontrivial, we construct semantically equivalent prompts through cross-lingual translation, inspired by [32].</p>
<p>Starting from a base prompt, we translate it through a sequence of typologically diverse languages and back to English, inducing natural variation in phrasing while preserving intent.Translation suits this purpose because its core objective is to alter surface form while maintaining meaning, and LLMs excel at automated translation ( [29]).</p>
<p>Using this pipeline, we evaluate five language models of different sizes on tasks in health, logistics, finance, travel and social planning.We find that larger models attribute a greater share of output variability to changes in user intent (Purpose Sensitivity), indicating stronger alignment with user goals and more coherent internal representations.This improvement is not uniform, however: the 70B model does not consistently outperform smaller models across all domains, and its advantage in robustness is often modest.Larger models also show slightly greater sensitivity to prompt articulation, reflecting a trade-off between semantic generalization and responsiveness to surface cues.Lastly, model robustness varies across domains.These findings demonstrate the diagnostic value of our framework and highlight the need for more targeted evaluations of model generalization.</p>
<p>Contributions.This paper makes three key contributions.First, we propose a formal definition of a world model tailored to language models, grounded in user intent and semantic invariance.Second, we introduce a diagnostic variance decomposition that quantifies the relative contributions of intent, phrasing, and uncertainty to model outputs.Third, we apply this method to real-world tasks, demonstrating how it reveals meaningful differences in the world-modeling capabilities of various LLMs.</p>
<p>Conceptual Framework</p>
<p>In this section, we provide the conceptual foundation for our proposed measure in section 3. Let T ∈ T denote users' intent.This intent or purpose represents the communicative goal underlying a user's request to the model.Users articulate their intent through prompts p i , which may vary widely in surface form even when seeking to convey the same underlying objective.For example, the intent of knowing the capital of France may be expressed through prompts such as "What's the capital of France?" or "Which city is France's capital?".Finally, let a ∈ A be the model response.</p>
<p>We say that a language model possesses a world model if it produces identical response distributions for requests or prompts that express the same underlying intent, and distinct response distributions for prompts that express different intents.We formalize this as follows:</p>
<p>Definition 2.1 (World Model) Let τ : P → T be the ground-truth mapping that assigns each prompt an intent label, and write τ −1 (T ) = { p ∈ P | τ (p) = T } for the set of prompts that express intent T .A language model that generates a response distribution π over A, is said to possess a world model if, for every pair of prompts p i , p j ∈ P, the following properties hold:</p>
<ol>
<li>Consistency: If p i , p j ∈ τ −1 (T ) for some T ∈ T , then
π( • | p i ) = π( • | p j ).</li>
<li>Sensitivity:
If τ (p i ) ̸ = τ (p j ), then π( • | p i ) ̸ = π( • | p j ).
Our definition requires that a model with a robust world model distinguish between surface-level variations in articulation and substantive changes in user intent or purpose.In particular, a model that varies its response distribution in reaction to inconsequential articulation changes is likely overfitting to superficial correlations in the training data rather than genuinely understanding the user's underlying purpose.This mirrors how world models are conceptualized in vision and navigation: the model must infer latent state variables that govern the dynamics of an environment, not simply react to pixel-level inputs.In our context, user intent plays the role of the latent state-an abstract, unobserved variable that the model must infer from the prompt's articulation and then condition its response on.A strong world model in language, as in navigation, should learn to generalize across input articulations that preserve that state (i.e., purpose or intent) while being sensitive to those that signal a change in it.In the Appendix we discuss further how our definition relates to other definitions of world models in the literature.</li>
</ol>
<p>Remark 1 In our framework, intent captures what the user wants the model to say-specifically, the expected distribution over exact responses.Two prompts have the same intent if they reliably elicit the same output distribution from the model, even if they differ in wording or domain.For example, "Will a fair coin land heads or tails?Answer 1 for heads, 0 for tails" and "I drew a number from [-1, 1].Is it positive or negative?Answer 1 for positive, 0 for negative" differ in content but share the same intent: both aim to produce a 50/50 distribution over identical outputs-'1' and '0'.</p>
<p>Our criteria for identifying an LLM's world model may be over-exacting.Precisely defining intent is inherently difficult due to its latency and potential ambiguity.Evaluating model responses adds another layer of complexity.This often requires estimation of the distribution of full-text responses, which are inherently high-dimensional (e.g., a detailed description of a driving itinerary from Boston to Miami.).Focusing on the distribution of responses, an evaluator may view distinct answers as the same in underlying meaning.For example, in response to the prompt "How much is 1 + 2?", a model might answer "three" or "3."Although slight variations in the prompt may influence the likelihood of generating "three" versus "3," we should still regard the model as consistent.</p>
<p>To overcome these challenges, we propose a more relaxed notion of world model: a sufficient world model.This concept softens the standard definition by introducing an evaluator who judges prompts and responses.Specifically, we assume the existence of two functions: one that determines whether two prompts express the same intent, and another that assigns values to different responses.We say that a language model has a sufficient world model with respect to the evaluator if, whenever two prompts are judged to share the same intent, the distributions over evaluator-assigned values of responses remain identical.In this sense, the model's representation of the world is sufficient by capturing enough of the world's consistency such that the evaluator cannot distinguish whether the model possesses a full world model or not.</p>
<p>Formally, let τ : P → T map from prompts to intents (i.e., articulations to purposes), defined according to some evaluation criterion.Let V : A → R be a function that maps responses to values.Finally, let π V (• | p) denote the distribution over response values induced by the model's responses to prompt p, as evaluated by V .The relaxed definition is as follows:</p>
<p>Definition 2.2 (Sufficient World Model with respect to an evaluator τ and V ) Let τ : P → T be a fixed intent-evaluator that assigns an intent label for each prompt, and let π V (• | p) be the value distribution induced by the model, in response to prompt p.A language model possesses a sufficient world model with respect to τ and V if for every pair of prompts p i , p j ∈ P:
1. (Sufficient Consistency) If τ (p i ) = τ (p j ), then π V ( • | p i ) = π V ( • | p j ). 2. (Sufficient Sensitivity) If τ (p i ) ̸ = τ (p j ), then π V ( • | p i ) ̸ = π V ( • | p j ).
Remark 2 Both the world and sufficient world model criteria focus on measuring consistency, rather than assessing whether a language model's responses are factually correct.This is because our primary goal is to evaluate how well the model understands and generalizes user intent-not whether it produces the correct answer.This approach differs from traditional benchmarks, which typically rely on a binary notion of correctness, classifying answers as strictly right or wrong.To understand why consistency matters, consider a language model trained only on economic data from 2020.If asked about economic conditions in 2023, it may consistently produce outdated yet internally coherent answers across differently phrased questions, reflecting the information on which it was trained.While these answers are incorrect in light of present-day facts, their internal consistency suggests that the model has formed a coherent and unified world model.Therefore, evaluating whether a language model has developed a robust world model depends more on its consistency across diverse contexts than on its alignment with external truth.</p>
<p>Our definition of a (sufficient) world model is motivated by LLM training data, which consists of human-generated text, replete with spurious correlations between writing style and user intent.Small stylistic changes-like tone or word choice-often co-occur with shifts in latent attributes of the writer, such as personality, mood, or communicative norms, even when the underlying intent and objective remains constant.This makes prompt phrasing a confounded signal, blending true intent with incidental articulation.As a result, a model may change its responses not because users change their intent, but because it has learned correlations between superficial cues and different responses.</p>
<p>Evaluating whether a model has a world model thus mirrors the causal inference problem: Can it separate variation due to intent from variation due to articulation?In a hypothetical world where prompt phrasing perfectly reveals intent and carries no noise, models would trivially satisfy our definition.But in reality, disentangling intent from correlation is non-trivial-making consistency a meaningful test of deeper understanding.</p>
<p>Evaluating Whether LLMs Have a Sufficient World Model</p>
<p>In this section, we present a simple method, motivated by our conceptual framework, for evaluating whether an LLM possesses a sufficient world model.We assume access to a sample of prompts along with corresponding model responses.Some prompts are designed to share the same underlying intent, while others are not.In the experimental section, we describe in detail how we construct such a sample.</p>
<p>Our evaluation approach is based on a variance decomposition of model responses.Specifically, we break down response variance into three components: Purpose Sensitivity (PS), which captures the model's responsiveness to changes in the intent or purpose of the input prompt; Articulation Sensitivity (AS), which reflects the model's sensitivity to variations in how users express the same intent; and Model Uncertainty (MU), which accounts for the inherent ambiguity or variability in the model's responses.</p>
<p>To formalize these components, we consider a domain D consisting of related purposes or intents I ∈ D. Each intent can be expressed through multiple prompts p I ∈ τ (I), and we denote the model's value of response to a prompt p as v.To enable comparability across tasks, domains, and models, we define the standardized response as ṽ = v std(v|D) .Furthermore, let R 2 I as the standard coefficient of determination, the proportion of total variance explained by differences in intent, and R 2 p I as the proportion of variance explained by differences in prompt phrasing, holding intent I fixed.With this notation, we define our three core measures, sketched in Figure 1, as:
P S = V ar I (E[ṽ|I]) = R 2 I (Purpose Sensitivity), AS = E I V ar p|I (E [ṽ | p I ]) = E I R 2 p I (Articulation Sensitivity), M U = E I E p|I (Var (ṽ | p I )) = E I 1 − R 2 p I (Model Uncertainty).
The first term, P S, is defined as the variance of the conditional expectation of responses given intent.It captures the sufficient sensitivity property, measuring how much the model's mean response shifts when the user's underlying purpose or intent changes.A model appropriately sensitive to intent should produce distinctly different responses for different tasks.Therefore, we expect PS to be high in models responsive to user intent or purpose.The second term, AS, corresponds to the consistency property.It quantifies how much, on average, the model's mean response varies with changes in phrasing or wording of the prompt, holding the user's purpose fixed.A model with a robust world model should exhibit low sensitivity to surface-level changes in articulation, indicating that it understands the user's underlying request rather than being swayed by syntactic or stylistic differences.</p>
<p>The third term, M U , captures the residual variance that remains after conditioning on both intent and phrasing.This component conflates at least three sources: (i) sampling stochasticity (e.g., due to temperature or nucleus sampling); (ii) epistemic uncertainty, where the model lacks a confident internal representation and spreads probability mass over multiple plausible answers; and (iii) aleatoric uncertainty, which is inherent to the task itself (e.g., when the prompt is "Tell me a random joke").Whether a high MU is desirable depends on the context.For deterministic tasks with a clear correct answer (e.g., "What is 1+1?"), high MU reflects unwanted uncertainty.In contrast, for inherently open-ended or subjective tasks-such as those we examine in the experimental section-some degree of response variability is expected and even appropriate.</p>
<p>These three measures are all non-negative and sum to one: P S + AS + M U = 1.This identity ensures that each term represents the relative contribution of a distinct source of variation in model responses.Together, they provide a principled way to assess the robustness and interpretability of a model's behavior.</p>
<p>An alternative perspective on this decomposition is in terms of R 2 : how much of the variation in model responses could be predicted using only information about the request.For example, consider asking the model how long it takes to travel from a given city to Paris.If the origin is New York versus London, we would expect the response distribution to change accordingly.A model with a robust world model should produce responses such that a simple predictor using only the origin can explain nearly all the variation, apart from irreducible noise.Thus, better world models lead to response patterns where most of the variation reflects structured knowledge rather than noise or inconsistency.</p>
<p>For each domain, we also report the Meaningful Variability Share (MVS), defined as MVS = P S P S + AS .</p>
<p>This quantity, analogous to a signal-to-noise ratio, captures the extent to which the model's response variability is driven by differences in intent (signal) rather than differences in prompt phrasing (noise).</p>
<p>A high MVS indicates that most of the model's explainable variation reflects meaningful distinctions between user purposes.In contrast, a low MVS suggests the model is overly influenced by superficial differences in wording, indicating a weaker underlying world model.Another interpretation of the MVS is that it approximates the variance decomposition obtained when the model temperature is set to zero, responses are deterministic, and variability stems only from intent and phrasing.</p>
<p>Remark 3 In practice, when estimating the components of the decomposition, we deliberately restrict attention to domains where requests share a common structure but differ in intent.This controlled setup reflects a conservative and more stringent approach to evaluating whether the model possesses a world model.By fixing the overall structure of the query and varying only a key element that changes the underlying intent (such as modifying the income level in a tax-related prompt while keeping the rest of the wording unchanged), we eliminate spurious cues and force the model to rely on its understanding.If the model systematically adapts its output to such minimal yet meaningful changes, it suggests the presence of a coherent internal representation capable of capturing user intent and purpose.</p>
<p>Experiments</p>
<p>In this section, we describe how we construct our experiments.We begin by selecting a set of using everyday categories-health, logistics, travel, finance, and social planning-where real-world interactions with language models commonly occur.These domains involve typical usage scenarios that demand assumptions and context-specific reasoning.</p>
<p>For each category, we select three tasks.These tasks (i) require reasoning about physical constraints and numerical trade-offs, and (ii) allow for a variety of plausible answers depending on assumptions made.For example, under the category of Travel, one task asks the model to estimate the travel time from Boston to Paris.To answer this question, the model may reasonably assume different modes of travel, direct or indirect flights, average delay times, or varying traffic conditions in Boston.</p>
<p>Moreover, as discussed in Remark 3, we include tasks designed so that small variations in input lead to large changes in outcome.This setup allows us to measure the model's Purpose Sensitivity, as introduced in Section 3.For instance, we prompt the model with a question like: "I've got {stops} deliveries in downtown SF.Any idea how long the whole route will take?"Varying the number of stops implies a change in user intent, which enables us to evaluate how the model adapts its responses.A complete list of tasks is provided in the Appendix.</p>
<p>Given the set of categories and tasks, our next challenge is to generate a collection of prompts that express the same intent but differ in surface form and semantics.This is a non-trivial challenge.</p>
<p>Even for humans, writing multiple prompts that convey exactly the same intent without introducing subtle shifts in meaning is difficult.Language models are highly sensitive to linguistic cues, and small differences in phrasing can unintentionally signal different goals or assumptions.This makes it essential to generate paraphrases that are semantically faithful to the original prompt but lexically and syntactically distinct.</p>
<p>To tackle this, we adopt a two-stage approach.Our primary method relies on cross-lingual translation, a process inherently designed to preserve meaning while altering surface expression (e.g., [32]).Translation captures the core idea of transferring intent across languages while abstracting away from specific phrasings.LLMs have demonstrated impressive performance in this task, often on par with human translators in consistency and fluency ([15, 30, 31]).We leverage this ability by using GPT-4o-mini to perform the translations: starting with an original English prompt, we translate it sequentially through two randomly selected intermediate languages and then back into English.The result is a paraphrase that retains the original intent but exhibits different syntactic and semantic features due to translation-induced variation.Different languages vary in how they structure, resulting in varying cross-translations as we vary the source languages into and from which we translate the prompts ( [16]).</p>
<p>To maximize semantic divergence while preserving intent, we randomly sample intermediate languages from a diverse set. 1 Each translation chain is required to include at least one of Chinese, Japanese, or Arabic, given their significant structural and lexical distance from English ( [7,16]), which helps introduce greater variation in the back-translated output.</p>
<p>To ensure the resulting prompts still reflect the original intent, we also use GPT-4o-mini as an additional check to verify that the intent remains unchanged.Given the original and translated prompt, the model assesses whether they express the same underlying request.Only those pairs judged equivalent are retained.To further enrich prompt set diversity, we generate 500 such paraphrases for each original question, embed all prompts using sentence embeddings, and apply a greedy selection algorithm to identify the 50 most diverse prompts-those with the highest mutual semantic distance.This final step ensures that our prompt set not only shares intent but also spans a wide semantic space, enabling robust evaluation of the model Articulation Sensitivity.</p>
<p>In the Appendix, we present results from an alternative prompt-generation approach.In this setup, we instruct GPT-4o-mini to act as a survey participant tasked with writing a prompt for a given scenario-for example: "You want to travel from Boston to Paris and would like to know how long the trip will take."A complete list of these prompts is provided in the Appendix.</p>
<p>After generating intent-equivalent prompts, we present them to the language model under evaluation.Each prompt is paired with three input values that vary the prompt main intent -for example, income level in a tax question.Then, for each of the 50 prompts, and for each of the three values we prompt the candidate LLM and elicit 50 model responses to capture the within prompt variation.This yields a total of As discussed in the definition of a sufficient world model, we need to determine how to evaluate the models' responses.We focus on extracting a clear bottom-line value: if the model provides a range, we take the average.Responses without a definitive answer are discarded, and we continue generating responses until we obtain the desired 50 responses.Importantly, to better mimic real user interactions, we do not constrain the model to produce numeric outputs.Instead, we allow it to generate natural, free-form responses, as it would in real-world settings.All outputs are generated with a temperature of 1 to accurately reflect the model's response distribution.These responses often include extended explanations, which we limit to 600 tokens.To extract a usable numeric value from each response, we employ GPT-4o-mini as a post-processor that identifies and retrieves the relevant quantity from the model's output.</p>
<p>Finally, in our analysis, we evaluate five models: Meta's LLaMA 3.2 3B Instruct,3.1 8B Instruct and LLaMA 3.3 70B Instruct, as well as Google's Gemma 3 12B-IT and 27B-IT.We use the OpenRouter API to generate multiple responses from each model.</p>
<p>Results</p>
<p>We begin by examining the total variance in model outputs across different domains.Figure 2 presents the average variance within the five domain categories introduced in the previous section.The figure reveals inconsistencies in which model exhibits the highest variance.In the personal finance and logistics/transportation domains, the Gemma 12B and 27B models display lower variance.In contrast, across the remaining domains, the 70B model consistently shows the lowest variance.As discussed earlier, high variance does not necessarily indicate poor model performance; rather, it may arise from three main sources: (i) sensitivity to the intended meaning of the prompt, (ii) inconsistency across prompt variants, and (iii) inherent stochasticity in the model's responses.The latter is expected, given that our tasks often permit a wide range of plausible interpretations.For instance, when asked to estimate delivery times in downtown San Francisco, the 8B model assumed moderate traffic and estimated 5-10 minutes per stop, while the 70B model used specific figures-12.5 miles at 12.5 mph plus 3 minutes per stop-to estimate over an hour.Both are plausible but reflect different underlying assumptions, highlighting how model-specific reasoning shapes response variability.</p>
<p>Figure 3a provides a more granular view by examining the response distributions for two tasks, drawn from the logistics and health and nutrition domains.5a summarizes our main results across the five categories and across models, showing average shares of PS, AS, and MU.Across all models, MU accounts for the largest portion of total variance, consistent with the nature of the questions we ask the model, which permit a wide range of reasonable assumptions and responses.The figure shows that the share of PS responses is lowest for the 8B and 3B models and generally highest for the 70B model, suggesting that larger models are more responsive to the intent behind a prompt than smaller ones.At the same time-though to a much lesser extent-the larger models exhibit slightly higher average shares of AS responses, indicating greater sensitivity to the specific phrasing of the prompt.</p>
<p>Figure 5b shows the MVS across all five models.In all five categories-except for personal finance-the 70B model exhibits the highest MVS.This indicates that its responses are more sensitive to changes in underlying intent than to superficial prompt rewordings.Surprisingly, however, the 70B model's world model is not significantly more robust than those of the smaller models.This suggests that simply increasing the number of parameters does not necessarily lead to a better world model.The figure also highlights that all five models tend to have more robust world models in domains such as health and nutrition, logistics, and personal finance, compared to social planning and travel.This points to variation in model understanding across domains, with stronger generalization in some areas and weaker performance in others.</p>
<p>In the Appendix we show additional results.Tables 1-5 show the full decomposition across all 15 questions, along with bootstrap standard errors.Finally, we include results from the survey-based prompt variant for the 8B and 70B LLaMA models, which exhibit qualitatively similar patterns.In this project, we present a formal framework to assess whether LLMs possess a sufficient world model.Instead of focusing solely on correctness, we decompose response variability into user intent (Purpose Sensitivity), prompt phrasing (Articulation Sensitivity), and intrinsic randomness (Model Uncertainty).Applying this framework across models of varying sizes and domains, we find that larger models generally exhibit more robust world models-but not uniformly.In some cases, smaller models outperform larger ones, and robustness varies by domain, underscoring the need for evaluation across contexts.Our findings emphasize semantic consistency and generalization as key indicators of model quality and offer a scalable method to diagnose true understanding versus pattern matching.</p>
<p>For practitioners, the PS-AS-MU decomposition provides a high-level risk assessment framework suitable for decision-making in LLM-enabled products.Accuracy tests alone tell a developer or user whether a model can answer a benchmark question; our metric tells them how the model will behave when real users inevitably rephrase, typo, or embed multiple requests in one prompt.Our metric also informs practitioners on potential fairness issues.Articulation sensitivity often correlates with dialect, accent, or education level; a model whose answers swing with phrasing differences can systematically disadvantage certain groups.Tracking AS gives model producers a concrete, quantitative way to demonstrate equity, complementing broader accuracy and bias checks.</p>
<p>Our measurement approach has several limitations: First, we only consider the first and second moments of response distributions.Although some decision-makers might require higher-order statistics or full distribution analyses, our method strikes a balance between complexity and practicality.Capturing the mean and variance provides essential insights for risk-averse decision-makers and can be easily applied across various models and scenarios.Second, our method is primarily designed for numerical responses.However, some evaluators may be more interested in assessing discrete outputs that lack a natural cardinal or ordinal structure.In the Appendix, we extend our decomposition framework to handle such cases by analyzing the entropy of the discrete response distribution-providing an analogous breakdown to the variance-based approach used in the paper.</p>
<p>Our research points toward the importance of understanding the intent or purpose that brings users to use LLMs and other AI models.In this study, we show that LLMs often miss user intent and respond to differences in superficial phrasing.Future research could more deeply and directly quantify user intent, processes through which intent emerges from preferences or through interaction with generative models, and the inherent limitations of fully expressing complex and original intent.Improved intent identification will enable to better measure models' ability to capture and respond appropriately.Consider recent work that demonstrates the difficulty users experience in trying to express their intent to generate a specific image [26].</p>
<p>Appendix</p>
<p>A Equivalent decomposition for discrete outputs</p>
<p>In the main text we use the variance decomposition for our measure
Var(v) = Var E[v | I] P S + E I Var(E[v | p I ] | I) AS + E I E Var(v | p I ) | I M U
, which is appropriate when v is continuous (or ordinal), as Var(•) is then well-defined.For categorical or otherwise non-ordinal outcomes, however, variance cannot be uniquely specified without imposing an arbitrary numerical scale.For example, consider a travel agency that uses a chatbot to recommend destinations based on user preferences.From the agency's perspective, the recommendations correspond to discrete destinations.To handle such cases, we propose an information-theoretic analogue, derived directly by applying the chain rule of entropy twice.</p>
<p>For a discrete random variable X with distribution P (X) we write H(X) = − x P (X = x) log P (X = x) for its Shannon entropy.We denote the mutual information between random variables X and Y as
I(X; Y ) = H(X) − H(X | Y ) = H(Y ) − H(Y | X).
Let a denote the one-hot (indicator) representation of the discrete outcome v.The variables I (intent) and p I (prompt constructed from intent) are defined in the main text.</p>
<p>With these definitions we obtain the decomposition
H(v) = I I; v P S + E i I p I ; v | I = i AS + E i H v | p I , I = i M U
.</p>
<p>where P S (Purpose Sensitivity) is the mutual information between the intent I and the model output quantifies how much output reveals about the intended meaning before the prompt is supplied.AS (Articulation Sensitivity) is now the conditional mutual information measures the extra information carried by the prompt p I , given the intent.Finally, M U (Model uncertainty) is the remaining conditional entropy captures irreducible uncertainty in the model's prediction once both intent and prompt are known.</p>
<p>We can further divide by the total entropy to get a unit-sum normalization that facilitates comparison across models:
1 = I(I; v) H(v) + E i I(p I ; v | I = i) H(v) + E i H(v | p I , I = i) H(v) .
This expression mirrors the continuous-outcome variance decomposition in structure while remaining well-defined for any discrete output space.</p>
<p>B Related Literature</p>
<p>LLMs Benchmarks.LLMs have attained high scores on a wide range of natural language processing benchmarks, sometimes creating the impression that they possess human-like reasoning or understanding.Nevertheless, many researchers have highlighted that strong benchmark performance does not necessarily indicate genuine mastery of the underlying abilities those benchmarks are intended to test [19,27,8].Such performance often stems from shallow pattern recognition or memorization rather than a deep grasp of the underlying concepts.A common issue is that current evaluation datasets frequently contain statistical cues or overlaps with training data, which models can exploit to produce correct answers without actual understanding.Even when the questions are new, models may still take advantage of predictable structural patterns.For example, Alzahrani et al. [1] demonstrated that simply rearranging the order of multiple-choice options in MMLU can substantially impact performance and even cause significant shifts in leaderboard rankings.This sensitivity suggests that models often rely on specific formatting habits or prompt templates rather than applying robust, generalizable knowledge.Small changes in wording can yield different outputs, indicating that models may be responding to surface-level lexical patterns rather than the intended semantics [2].Moreover, the use of aggregate scores in benchmark evaluations can mask important insights into where models systematically succeed or fail, resulting in an overly optimistic portrayal of their capabilities [6].Overall, many current benchmarks are static and susceptible to shortcuts, offering limited insight into whether models truly understand concepts.In contrast, our proposed evaluation emphasizes semantic consistency: it examines whether models provide stable responses when intent remains fixed and whether their outputs shift appropriately when intent changes.</p>
<p>stochastic parrots: Recent literature has raised the concern that LM operate as "stochastic parrots"-producing fluent text by statistically mimicking training data without true understanding-was sharply articulated by ([3]).They argue that such models risk misleading users by generating plausible-sounding outputs detached from meaning or context.This critique hinges on the absence of a world model, an internal representation that allows models to generalize across contexts and tasks.Our work responds directly to this concern by operationalizing what it means for a model to have a world model and proposing empirical criteria to test for it.Specifically, we develop a framework that assesses whether a model's outputs remain stable across semantically equivalent prompts, and whether variation in outputs tracks variation in intent rather than surface form.In this sense, our work transforms the stochastic parrot critique from a normative warning into a testable hypothesis-providing a principled way to distinguish shallow pattern matching from deeper semantic modeling.</p>
<p>LLMs Prompt Robustness Recent papers have investigated the sensitivity of large language models (LLMs) to prompting using a variety of frameworks and metrics.[33] introduce the ProSA framework, which quantifies prompt sensitivity through a novel metric called PromptSensiScore (PSS).PSS measures the average variation in performance across semantically equivalent prompt variants at the instance level, offering insight into how much a model's output changes with different prompt formulations.Their findings show that prompt sensitivity varies across tasks, models, and prompt types, with particularly high sensitivity observed in reasoning and creative tasks.Similarly, [20] examine the role of "spurious features" in prompt formatting-such as punctuation, spacing, and capitalization-and propose FORMATSPREAD, a metric that captures the spread in task accuracy across equivalent prompt formats.FORMATSPREAD is defined as the difference in performance between the best and worst format, and their results reveal that such superficial changes can lead to performance swings of up to 76 accuracy points-effects that are not mitigated by model size or few-shot learning.[5] take a different perspective, framing prompt sensitivity as a methodological artifact inspired by the concept of choice architecture.Using full-factorial experiments, they show that prompt order, labeling, framing, and justification systematically bias model responses-and that even instructing the model to ignore these features does not eliminate the effect.Their central claim is that no prompt is neutral.To address this, they recommend aggregating responses across multiple prompts, akin to ensemble methods, to counteract individual prompt biases.</p>
<p>Our behavioral framing complements and extends this literature by offering a diagnostic framework that quantifies prompt sensitivity in terms of structured variance.Unlike prior work that focuses on scalar measure of how model responses change, we take seriously the idea that model outputs should be stochastic-but emphasize that the source of this randomness should not arise from arbitrary variations in prompt phrasing.As we know no other measure of prompts sensitivity linked the importance of robustness to world model and focused on the variance aspect of responses.</p>
<p>Measuring Semantic Robustness and Fairness.Our decomposition also offers practical diagnostic value.High articulation sensitivity suggests that small variations in phrasing disproportionately affect model outputs, raising concerns for fairness and accessibility.Prior work shows that models may perform differently for users with dialectal, non-standard, or less "typical" phrasing [4,22,23,11].</p>
<p>Our framework allows developers to quantify this fragility and track improvements over time.In this respect, our method serves as a form of semantic reliability auditing, applicable in safety-critical domains such as healthcare, finance, and legal reasoning, where output variability under minor phrasing shifts can lead to unacceptable inconsistency.</p>
<p>Translation Chains Back-translation is a data augmentation technique in natural language processing, particularly in neural machine translation ( [21]).Back-translation involves translating monolingual target-language text into the source language using a reverse translation model, and then using the resulting synthetic parallel data to train the forward model.Subsequent work has extended the approach to include round-trip translation and multilingual back-translation, where intermediate languages are introduced to further diversify the training data and improve generalization [10,9,28,32].These variants exploit linguistic variation introduced during translation to create richer training distributions, which have been beneficial not only for translation tasks but also for classification, question answering, and style transfer.In our setup we do not use translation chains as a data augmentation tool, but instead think of them as a way to diversify language while preserving meaning.</p>
<p>B.1 Relation to other Work on Measuring World Models</p>
<p>Our notion of a world model draws inspiration from research in reinforcement learning (RL) and model-based control, where a world model captures the dynamics of an environment and supports planning.In these contexts, a world model is typically a learned transition function or latent representation that abstracts the environment's relevant state space [ [13,14,12]].</p>
<p>Analogously, we treat user intent as a latent variable and evaluate whether a model can infer and represent this variable consistently across diverse inputs.This is conceptually related to state abstraction in RL [18], where different observations map to the same underlying state if they yield equivalent value functions.In our case, different prompts map to the same intent if they elicit the same output distribution (or value-weighted distribution).</p>
<p>Recent work has examined whether generative models develop an internal world model in the context of games.[24] and [17] helped establish games-such as chess and Othello-as testbeds for evaluating world model emergence.A common approach in this literature involves using probes to assess whether a model's internal representations encode latent game states.In contrast, our evaluation metrics are model-agnostic: rather than probing representations or relying on an external notion of state feasibility, we focus on the internal consistency of a model's responses as an indicator of world model quality.</p>
<p>Most closely aligned with our work is [25], who test an LLM's world model by asking whether it recovers the deterministic finite automaton (DFA) governing a sequence-generation task.Leveraging the Myhill-Nerode theorem, they introduce two metrics: (i) sequence compression-do any two prefixes that land in the same DFA state admit the same set of valid continuations? and (ii) sequence distinction-do prefixes that reach different states generate different permissible continuations?</p>
<p>Both their framework and ours impose a common behavioral mandate: (1) inputs sharing a latent condition (DFA state or user intent) must elicit indistinguishable outputs, and (2) inputs differing in that condition must yield measurably different outputs.Crucially, both approaches diagnose world-model quality by outputs alone, never inspecting internal parameters.</p>
<p>The divide lies in how "correctness" is anchored.[25] use a built-in, binary oracle-the known DFA-to label continuations as right or wrong at the syntax level.Our variance-decomposition framework instead delegates that judgment to an external evaluator V , which maps a 〈prompt, response〉 pair to latent intent and task value.V can imitate a hard 0/1 oracle, but it can also apply graded semantic scores.Importantly, our test goes further: when V deems two prompts equivalent, we require the entire distribution of (possibly stochastic) responses to match across those prompts.If V itself enforces strict correctness, this collapses to zero model uncertainty-any variability in outputs must stem from differing intents.Thus, while both methods rely on an evaluator, Keyon's oracle is intrinsic and binary, whereas ours is external, tunable, and distribution-aware.</p>
<p>Finally, our notion of a sufficient world model demands distributional equality across equivalent prompts, whereas the DFA test assumes deterministic continuations.Extending Myhill-Nerode to stochastic automata would miss the point we target: open-ended tasks with no unique correct answer.</p>
<p>In such settings, the likelihoods assigned to alternative continuations encode the model's assumptions.A genuine world model, therefore, aligns those likelihoods whenever the underlying intent is the same, preserving semantic sufficiency even under uncertainty.</p>
<p>C Prompts</p>
<p>The following list includes the base prompts used in our experiments, the corresponding values, the extraction prompt employed to obtain the numerical values, and the survey question used in our prompt-generation analysis based on a survey design.The system text for all the extraction prompts is:</p>
<p>You are an assistant designed to extract the final numerical answer from a given text.Return only the number, with no additional words.If the answer is a range, return it in the format a-b (e.g., [10][11][12][13][14][15], using a single hyphen with no spaces.The system prompt for all the survey questions is:</p>
<p>You are simulating a survey participant who needs to write a question to an AI assistant about transportation and travel.</p>
<p>Health and Nutrition</p>
<p>• Prompt: I'm sticking to {cal_limit} calories a day for a month.I weigh 80kg and stay lightly active.About how many kilos could I expect to lose (or gain) in  -Extraction Prompt: Extract the commuting time from the text.Return only the number-no words or units.If no commuting time is mentioned, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You want to find out how long your commute to downtown Boston is likely to take.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, write a single question you would ask the AI to help you.\nDo not copy the scenario exactly-rephrase it naturally.\nWhen mentioning how far you live from downtown, use the placeholder {distance_km} instead of a specific number.\nNow, please write your prompt.Submit only one prompt.</p>
<p>C.1 Translation Prompts</p>
<p>We iteratively go through the following prompt:</p>
<p>System: "You are a professional translator.Translate the user text to target_lang.Return <em>only</em> the translation-no commentary.</p>
<p>User: {Prompt}</p>
<p>D Results from Generating Equal Purpose Prompts using Survey Style design</p>
<p>These figures show the equivalent graphs to those in Figure 3 of the main text, but for prompts generated using the survey approach.In the survey approach, we present a scenario to an LLM and ask it to write a prompt for a chatbot.A list of the exact survey prompts is provided in the section above.These results show for four topics and the 8b and 70b models.</p>
<p>Figure 1 :
1
Figure 1: Conceptual Illustration of World Model Measures</p>
<p>[3 intents ] × [50 prompts variations] × [50 responses] = 7, 500 responses per task.</p>
<p>Figure 2 :(
2
Figure 2: Variance Differences Across Categories and ModelsPanel B displays the response distributions for variants of the prompt: "I have {QUANTITY} mg of prednisone and I'm currently taking 20mg per day, but take 40mg when my symptoms are worsening.How many days is it likely to last?" Here, both models exhibit a clear upward shift in predicted duration as 'QUANTITY' increases, along with relatively minor variation across prompt formulations, evidence of a robust world model in this domain.</p>
<p>Figure 3 :
3
Figure 3: Conditional Response Distributions For Logistics and Health and Nutrition intents</p>
<p>Figure 4 :
4
Figure 4: Average Decomposition weights and MVS across categories</p>
<p>5 .
5
Travel and Transportation • Prompt: I live in {city} and need to travel to Miami.What's the total travel time likely to be with a flight?-Values:[Boston, New York, Paris] -Extraction Prompt: Extract the total travel time from the text.Return only the number-no words or units.If no commuting time is mentioned, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You are planning a trip to Miami, and you're wondering how long the travel time would be by flight.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the city you are currently in, use the placeholder {city} instead of a specific name.\nNow, please write your prompt.Submit only one prompt.• Prompt: I live in {city} and need to travel to Sydney.How long is the total travel time likely to be? -Values:[Boston,New York, Paris] -Extraction Prompt: Extract the total travel time from the text.Return only the number-no words or units.If no commuting time is mentioned, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You are planning a trip to Sydney, Australia, and you're wondering how long the travel time would be.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the city you are currently in, use the placeholder {city} instead of a specific name.\nNow, please write your prompt.Submit only one prompt.• Prompt: Each morning I commute by car for {distance_km} miles to Boston downtown.How long would my commute be likely to take?-Values:[5,20,60]</p>
<p>Figure 5 :
5
Figure 5: Average Decomposition weights and MVS across categories</p>
<p>Each kernel density estimate corresponds to a single prompt variant; red vertical lines indicate the mean response for each prompt, and blue the overall mean.Panel A focuses on variations of the prompt: "We need to send '{num_boxes}' standard moving boxes.How many 10-foot trucks should we rent?" Rows correspond to different values of '{num_boxes}'.The 8B model exhibits high variability, and the closeness of the red lines suggests that prompt rewording has minimal effect on the mean prediction-indicating low</p>
<p>inconsistency (≈ 0.4%, see Table1in the Appendix).Increasing the number of boxes has little effect on the predicted number of trucks, however, implying limited sensitivity to intent.By contrast, the 70B model exhibits clear shifts in the distribution of responses as the number of boxes increases, indicating stronger responsiveness to task intent.Nevertheless, the increased spread in prompt means suggests greater sensitivity to prompt phrasing and thus higher inconsistency.</p>
<p>Table 1 :
1
Decomposition Across Transportation Domain.Standard Errors are stratified bootstrap, 500 times.
Base PromptModelVariancePSASMUI live in {city} and3B llama7.00e+010.0080.0490.943need to travel to7.00e+01 [0.005, 0.012] [0.040, 0.057] [0.934, 0.952]Sydney. How longis the total travel70B llama1.49e+010.0010.0920.907time likely to be?1.49e+01 [0.000, 0.003] [0.081, 0.106] [0.893, 0.918]8B llama6.33e+010.0010.0580.9406.33e+01 [0.000, 0.003] [0.050, 0.068] [0.930, 0.949]Gemma 12B 2.30e+010.0140.0810.9052.30e+01 [0.009, 0.019] [0.071, 0.093] [0.892, 0.917]Gemma 27B 3.48e+010.0110.3790.6103.48e+01 [0.008, 0.016] [0.330, 0.425] [0.564, 0.660]I live in {city} and3B llama1.62e+030.0080.0450.947need to travel to1.62e+03 [0.005, 0.012] [0.036, 0.056] [0.935, 0.957]Miami. What's thetotal travel time70B llama4.97e+030.1040.0960.800likely to be with4.97e+03 [0.094, 0.115] [0.081, 0.113] [0.779, 0.817]a flight?8B llama2.91e+030.0200.0540.9262.91e+03 [0.014, 0.025] [0.043, 0.068] [0.912, 0.940]Gemma 12B 2.27e+030.0260.2010.7742.27e+03 [0.022, 0.031] [0.177, 0.229] [0.743, 0.800]Gemma 27B 5.71e+030.0070.1240.8695.71e+03 [0.004, 0.010] [0.105, 0.146] [0.846, 0.889]each morning I3B llama7.43e+020.1630.0550.782commute by car7.43e+02 [0.150, 0.178] [0.043, 0.067] [0.762, 0.800]for {distance_km}miles to Boston70B llama7.96e+020.1790.0670.753downtown. How7.96e+02 [0.173, 0.186] [0.055, 0.081] [0.739, 0.768]long wouldmy commute is8B llama6.61e+020.1950.0470.758likely to take?6.61e+02 [0.183, 0.208] [0.037, 0.059] [0.741, 0.773]Gemma 12B 1.00e+030.1470.1190.7341.00e+03 [0.137, 0.157] [0.100, 0.136] [0.715, 0.754]Gemma 27B 9.92e+020.4310.0370.5329.92e+02 [0.411, 0.453] [0.030, 0.045] [0.510, 0.553]</p>
<p>Table 2 :
2
Decomposition Across Personal Finance.Standard Errors are stratified bootstrap, 500 times.
Base PromptmodelVariancePSASMUWe need to send3B llama5.11e+040.0390.0370.924{boxes} standard5.11e+04 [0.034, 0.045] [0.028, 0.048] [0.911, 0.935]moving boxes. Howmany 10-foot70B llama2.43e+000.8050.0230.173trucks should we2.43e+00 [0.795, 0.815] [0.018, 0.027] [0.162, 0.183]rent?8B llama3.41e+030.0240.0480.9283.41e+03 [0.019, 0.030] [0.037, 0.061] [0.914, 0.941]Gemma 12B 1.24e+010.5150.0650.4191.24e+01 [0.497, 0.532] [0.056, 0.074] [0.401, 0.439]Gemma 27B 2.13e+010.4230.0270.5502.13e+01 [0.390, 0.461] [0.021, 0.036] [0.510, 0.580]We need to send3B llama1.54e+090.0510.0690.880{boxes} standard1.54e+09 [0.045, 0.057] [0.054, 0.085] [0.862, 0.897]size boxes.Could you70B llama1.33e+080.0900.0920.818estimate the likely1.33e+08 [0.081, 0.100] [0.079, 0.106] [0.802, 0.834]cost of shippingthem from San8B llama9.85e+080.0510.0770.873Francisco9.85e+08 [0.046, 0.057] [0.063, 0.092] [0.854, 0.888]to Chicago?Gemma 12B 3.34e+080.1230.1420.7353.34e+08 [0.112, 0.134] [0.125, 0.161] [0.713, 0.757]Gemma 27B 4.17e+080.0910.0780.8314.17e+08 [0.084, 0.098] [0.064, 0.097] [0.811, 0.846]I've got {stops}3B llama2.54e+050.1680.0570.775deliveries to make2.54e+05 [0.159, 0.178] [0.044, 0.072] [0.758, 0.791]around downtownSan Francisco. Any 70B llama3.90e+040.4820.0770.441idea how long3.90e+04 [0.470, 0.494] [0.068, 0.087] [0.428, 0.454]the full routewill take?8B llama1.60e+050.2420.0460.7121.60e+05 [0.231, 0.253] [0.037, 0.058] [0.696, 0.725]Gemma 12B 1.03e+050.3170.0480.6351.03e+05 [0.299, 0.336] [0.039, 0.059] [0.615, 0.655]Gemma 27B 7.82e+040.4190.0420.5397.82e+04 [0.404, 0.434] [0.034, 0.051] [0.522, 0.555]</p>
<p>Table 3 :
3
Decomposition Across Logistics.Decomposition Across Social Planning Domains.Standard Errors are stratified bootstrap, 500 times.
Base PromptModelVariancePSASMUI'm planning to3B llama2.35e+070.1060.0330.861do a challenging2.35e+07 [0.098, 0.115] [0.024, 0.042] [0.848, 0.873]hike of {miles}miles in Yosemite70B llama4.79e+060.4270.0290.545National Park.4.79e+06 [0.411, 0.443] [0.023, 0.035] [0.529, 0.561]How many caloriescan I expect8B llama3.09e+070.1320.0310.837to burn?3.09e+07 [0.123, 0.144] [0.023, 0.041] [0.823, 0.849]Gemma 12B 4.25e+060.1410.0370.8224.25e+06 [0.131, 0.152] [0.029, 0.047] [0.808, 0.834]Gemma 27B 5.27e+060.2190.0380.7435.27e+06 [0.206, 0.231] [0.029, 0.050] [0.727, 0.759]Im sticking to3B llama4.00e+040.0010.0380.961{cal_limit} calories4.00e+04 [0.000, 0.002] [0.022, 0.076] [0.923, 0.977]a day for a month.I weigh 80kg70B llama9.26e+020.0920.0420.866and stay lightly9.26e+02 [0.002, 0.265] [0.018, 0.071] [0.672, 0.978]active. About howmany kilos could8B llama7.01e+020.0020.0360.961I expect to7.01e+02 [0.000, 0.010] [0.022, 0.067] [0.932, 0.977]lose (or gain)in 30 days?Gemma 12B 3.02e+070.0100.0400.9513.02e+07 [0.006, 0.013] [0.030, 0.051] [0.938, 0.962]Gemma 27B 1.12e+070.0100.0390.9511.12e+07 [0.005, 0.014] [0.028, 0.054] [0.935, 0.964]I have {QUAN-3B llama3.25e+040.2860.1380.577TITY}mg of prednisone3.25e+04 [0.271, 0.299] [0.126, 0.150] [0.560, 0.593]and I'm currentlytaking 20 mg per70B llama2.35e+040.9470.0060.048day,but take 40 mg2.35e+04 [0.942, 0.950] [0.005, 0.007] [0.044, 0.051]when my symptomsare worsening. How 8B llama4.36e+040.3160.0930.590many days is it4.36e+04 [0.292, 0.342] [0.081, 0.107] [0.562, 0.617]likely to last?Gemma 12B 3.79e+040.6970.0450.2573.79e+04 [0.682, 0.714] [0.038, 0.052] [0.243, 0.271]Gemma 27B 3.47e+040.8340.0320.1343.47e+04 [0.820, 0.848] [0.025, 0.041] [0.124, 0.143]</p>
<p>Table 4 :
4
Decomposition Across Health and Nutrition Domains.Decomposition Across Social Planning Domains.Standard Errors are stratified bootstrap, 500 times.
Base PromptModelVariancePSASMUThe city plans to3B llama6.29e+100.0060.0370.957invest${bud-6.29e+10 [0.005, 0.008] [0.028, 0.050] [0.944, 0.967]get_million}million in sidewalkrepairs. How much 70B llama4.25e+100.0120.0380.950money could that4.25e+10 [0.009, 0.016] [0.026, 0.055] [0.931, 0.964]save in injury-related costs over8B llama1.46e+110.0070.0430.950the next 10 years?1.46e+11 [0.004, 0.010] [0.029, 0.058] [0.932, 0.965]Gemma 12B 3.52e+110.0020.0390.9583.52e+11 [0.001, 0.005] [0.030, 0.054] [0.944, 0.968]Gemma 27B 2.07e+110.0140.0990.8872.07e+11 [0.008, 0.021] [0.062, 0.152] [0.832, 0.928]Estimate the total3B llama2.87e+060.0080.0640.929costs (direct and in-2.87e+06 [0.005, 0.011] [0.048, 0.085] [0.907, 0.946]direct)of implementing afree transportation70B llama1.96e+040.2180.0850.697program for1.96e+04 [0.204, 0.232] [0.070, 0.100] [0.676, 0.716]{group_residents}inSan Francisco?8B llama3.18e+050.0050.0470.9473.18e+05 [0.002, 0.009] [0.039, 0.058] [0.937, 0.955]Gemma 12B 2.81e+040.0580.0790.8632.81e+04 [0.044, 0.073] [0.057, 0.107] [0.834, 0.885]Gemma 27B 8.30e+040.0210.0430.9368.30e+04 [0.012, 0.031] [0.033, 0.054] [0.923, 0.946]Estimate the total3B llama8.56e+070.0210.0420.937(direct and indirect8.56e+07 [0.017, 0.025] [0.029, 0.060] [0.919, 0.952]costs)of the programin Cook County,70B llama2.91e+050.1550.0560.789IL, that offers2.91e+05 [0.143, 0.168] [0.047, 0.068] [0.772, 0.803]free meals to{group_students} in 8B llama3.14e+060.0400.0380.922public schools?3.14e+06 [0.034, 0.047] [0.027, 0.050] [0.907, 0.934]Gemma 12B 1.33e+060.0970.0660.8371.33e+06 [0.088, 0.107] [0.054, 0.079] [0.822, 0.851]Gemma 27B 1.77e+050.1660.1020.7321.77e+05 [0.136, 0.197] [0.087, 0.118] [0.695, 0.768]</p>
<p>Table 5 :
5
Decomposition Across Social Planning Domains.Standard Errors are stratified bootstrap, 500 times.</p>
<p>Extract the number of kilos expected to lose (or gain) in 30 days from the provided text.Return only the numeric value (include units like %, km, or $ if present) with no extra words.\n\n Answer text: \n{answer_text} -Survey Question: Scenario:\n You've started a diet where you limit yourself to a fixed number of calories per day for a month.You weigh 80kg and maintain a lightly active lifestyle.You want to know how many kilos you might expect to lose (or gain) over 30 days.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to your daily calorie limit, use the placeholder {cal_limit} instead of a specific number.\nNow, please write your prompt.Submit only one prompt.• Prompt: I have {QUANTITY} mg of prednisone and I'm currently taking 20 mg per day, but take 40 mg when my symptoms are worsening.Extract the number of days the prednisone will last from the provided text.Return only the number of days as a number, without any extra text.If no duration can be inferred from the text, return "None".\n \n Answer text:\n{answer_text} -Survey Question: Scenario:\n You have a prescription for prednisone and currently have some on hand.You normally take 20mg per day, but occasionally increase to 40mg when your symptoms get worse.You'd like to know how many days your current supply will last.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a question you would ask the AI to help you.\n\n Do not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the amount of medication you have, use the placeholder {QUANTITY} instead of a specific number.\nNow, please write your prompt.Submit only one prompt.• Prompt: I'm planning to do a challenging hike of {miles} miles in Yosemite National Park.How many calories can I expect to burn? -Values: [5,10,20] -Extraction Prompt: Extract the expected number of calories burned from the provided text.Return only the number as a numeric value, without any extra text.If no calorie estimate can be inferred from the text, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You are planning a hike in Yosemite Park that includes steep climbs.You're curious about how many calories you're likely to burn.\n \n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the length of the hike, use the placeholder {miles} instead of a specific number.\nNow, please write your prompt.Submit only one prompt.2. Logistics • Prompt: I've got {stops} deliveries to make around downtown San Francisco.Any idea how long the full route will take?-Values: [5,25,60] -Extraction Prompt: Extract the expected total time to complete the deliveries from the provided text.Return only the number of minutes as a numeric value, without any extra text.If no time estimate can be inferred from the text, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You need to make deliveries in downtown San Francisco and want to know how long your route will take.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the number of deliveries you need to make, use the placeholder {stops} instead of a specific number.\nNow, please write your prompt.Submit only one prompt.• Prompt: We need to send {boxes} standard moving boxes.How many 10-foot trucks should we rent?-Values: [150,500,1000] -Extraction Prompt: Extract the number of 10-foot trucks needed to ship the boxes from the provided text.Return only the number as a numeric value, without any extra text.If no truck estimate can be inferred from the text, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You need to send several boxes and are trying to figure out how many 10-foot trucks you'll need to rent.\n \n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the number of boxes you want to ship, use the placeholder {boxes} instead of a specific number.\nNow, please write your prompt.Submit only one prompt.• Prompt: We need to send {boxes} standard size boxes.Could you estimate the likely cost of shipping them from San Francisco to Chicago? -Values: [150,500,1000] -Extraction Prompt: Extract the estimated cost of shipping the boxes from the provided text.Return only the number as a numeric value in dollars, without any extra text.Return the value as dollars (not thousands or millions of dollars).If no cost estimate can be inferred from the text, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You need to send standard-sized boxes from Chicago to San Francisco and want to estimate how much it will likely cost.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the number of boxes you want to ship, use the placeholder {boxes} instead of a specific number.\nNow, please write your prompt.Scenario:\n You live in Boston, and want to figure out how much money to set aside for groceries each month, given your net income.\n\n Your Task:\n Imagine you're asking an AI chatbot for help.\nIn your own words, write one natural, concise question you'd pose to the AI to get a grocery budget recommendation.\nDon't copy the scenario word-for-word-rephrase it naturally.\nUse the placeholder {net_income} when referring to your monthly net income, and mention that you live in Boston.\nNow, write just that single prompt.• Prompt: I drive {daily_commute_km} miles roundtrip to work, five days a week.Based on this, how much should I expect to budget annually for car expenses?-Values: [10,40,80] -Extraction Prompt: Extract the annual car expenses expected from the provided text.Return only the numeric value with no extra words.Return the value as dollars (not thousands or millions of dollars).If no car expenses amount is mentioned, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You drive a certain number of miles roundtrip to work each day, five days a week, and want to estimate your yearly car expenses.\n\n Your Task:\n Imagine you're asking an AI chatbot for budgeting advice.\nIn your own words, write one natural, concise question you'd pose to the AI asking how much you should expect to budget annually for car costs based on your commute.\nDon't copy the scenario verbatim-rephrase it naturally.\nWhen specifying your daily commute distance, use the placeholder {daily_commute_km}.\nNow, write just that single prompt.• Prompt: What's the monthly rent amount that would make me just as well off financially as buying a $300,000 house with a {LENGTH}-year mortgage at 3.5% interest?-Values: [5,10,15] -Extraction Prompt: Extract the final equivalent monthly rent from the provided text.Return only the rent amount as a number, without any extra text.Return the value as dollars (not thousands or millions of dollars).If not rent amount ins provided in the text return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You're thinking about buying a $300,000 home with a fixed-rate mortgage at a 3.5% annual interest rate over a LENGTH-year term.Alternatively, you could keep renting.To make a well-informed choice, you want to figure out what monthly rent would make you financially indifferent between buying and renting in the long run.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, write a question you would ask the AI to help you.\n\n Do not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the mortgage term, use the placeholder {LENGTH}-year instead of a specific number.\nNow, please write your prompt.Submit only one prompt.4. Social Planning • Prompt: The city plans to invest ${budget_million} million in sidewalk repairs.How much money could that save in injury-related costs over the next 10 years?-Values: [1,5,15] -Extraction Prompt: Extract the estimated savings in injury-related costs over the next 10 years from the provided text.Return only the number as a numeric value in millions of dollars (e.g., 1 for one million, 0.5 for half a million), without any extra text.If no savings estimate can be inferred from the text, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n The city plans to invest in sidewalk repairs.You want to estimate how much money could be saved in injury-related costs over the next 10 years.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the size of the city's investment budget, use the placeholder {budget_million} instead of a specific number.\nNow, please write your prompt.Submit only one prompt.• Prompt: Estimate the total (direct and indirect costs) of the program in Cook County, IL, that offers free meals to {group_students} in public schools?-Values: [all students, students with disabilities, disadvantaged students] -Extraction Prompt:Extract the estimated the total cost of the program from the provided text.Return only the number as a numeric value in millions of dollars (e.g., 1 for one million, 0.5 for half a million), without any extra text.If no savings estimate can be inferred from the text, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You want to estimate the total costs (direct and indirect) of a program in Cook County that provides free meals to a specific group of students attending public schools.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the group of students, use the placeholder {group_students} instead of naming a specific group.This placeholder will be replaced with actual group names later.\nNow, please write your prompt.Submit only one prompt.• Prompt: What would be the total costs (direct and indirect) of implementing a free transportation program for {group_residents} in San Francisco?-Values: [unemployed,seniors,low-income] -Extraction Prompt: Extract the total cost (direct and indirect) of the free public transportation program from the provided text.Return only the number as a numeric value in millions of dollars (e.g., 1 for one million, 0.5 for half a million), without any extra text.If no cost estimate can be inferred from the text, return "None".\n \n Answer text:\n {answer_text} -Survey Question: Scenario:\n You want to know the total costs (direct and indirect costs) of a program in San Francisco that provides free transportation to a specific group of residents.\n\n Your Task:\n Imagine you are using an AI chatbot.\nIn your own words, craft a single question you would ask the AI to help you.\nDo not copy the scenario verbatim-rephrase it naturally.\nWhen referring to the group of residents, use the placeholder {group_residents} instead of naming a specific group.This placeholder will be replaced with actual group names later.\nNow, please write your prompt.Submit only one prompt.
30 days?-Values: [600,2000,3500]-Extraction Prompt: How many days is it likely tolast?-Values: [1000,7500,10000 ]-Extraction Prompt: Submit only one prompt.3. Personal Finance• Prompt: I'm filing taxes as a single person with no dependents and earn {INCOME}annually. Can you help me estimate how much federal tax I might owe?-Values: [$30,000, $50,000,$100,000]-Extraction Prompt:Extract the final tax liability from the provided text. Returnonly the final owed amount as a number, without any extra text. Return the valueas dollars (not thousands or millions of dollars). If no tax liability is mentioned,return "None".\n \n Answer text:\n {answer_text}-Survey Question:
Chinese, Japanese, Arabic, Turkish, Finnish, Korean, Polish, German, Russian, Indonesian, Vietnamese, Hindi, Swahili, Spanish, and Hebrew.</p>
<p>When benchmarks are targets: Revealing the sensitivity of large language model leaderboards. Norah Alzahrani, Hisham Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yousef Almushayqih, Faisal Mirza, Nouf Alotaibi, Nora Al-Twairesh, Areeb Alowisheq, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Semantic sensitivities and inconsistent predictions: measuring the fragility of nli models. Erik Arakelyan, Zhaoqi Liu, Isabelle Augenstein, Proceedings of the 18th Conference of the European Chapter. Long Papers. the 18th Conference of the European Chapterthe Association for Computational Linguistics12024</p>
<p>On the dangers of stochastic parrots: Can language models be too big. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. the 2021 ACM conference on fairness, accountability, and transparency2021</p>
<p>Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, Adam T Kalai, Advances in neural information processing systems. 292016</p>
<p>Prompt architecture induces methodological artifacts in large language models. Melanie Brucks, Olivier Toubia, PloS one. 204e03191592025</p>
<p>Rethink reporting of evaluation results in ai. Ryan Burnell, Wout Schellaert, John Burden, Fernando Tomer D Ullman, Joshua B Martinez-Plumed, Danaja Tenenbaum, Lucy G Rutar, Jascha Cheke, Melanie Sohl-Dickstein, Mitchell, Science. 38066412023</p>
<p>Linguistic distance: A quantitative measure of the distance between english and other languages. R Barry, Paul W Chiswick, Miller, Journal of multilingual and multicultural development. 2612005</p>
<p>Benchmarks for automated commonsense reasoning: A survey. Ernest Davis, ACM Computing Surveys. 5642023</p>
<p>Understanding back-translation at scale. Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language Processing2018</p>
<p>Data augmentation for low-resource neural machine translation. Marzieh Fadaee, Arianna Bisazza, Christof Monz, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational Linguistics20172Short Papers)</p>
<p>Yufei Guo, Muzhe Guo, Juntao Su, Zhou Yang, Mengqiu Zhu, Hongfei Li, Mengyang Qiu, Shuo Shuo Liu, arXiv:2411.10915Bias in large language models: Origin, evaluation, and mitigation. 2024arXiv preprint</p>
<p>Recurrent world models facilitate policy evolution. Advances in neural information processing systems. David Ha, Jürgen Schmidhuber, 201831</p>
<p>. David Ha, Jürgen Schmidhuber, arXiv:1803.101222018World models. arXiv preprint</p>
<p>Dream to control: Learning behaviors by latent imagination. Danijar Hafner, Timothy Lillicrap, Jimmy Ba, Mohammad Norouzi, arXiv:1912.016032019arXiv preprint</p>
<p>Large language models effectively leverage documentlevel context for literary translation, but critical errors persist. Marzena Karpinska, Mohit Iyyer, arXiv:2304.032452023arXiv preprint</p>
<p>Local similarity and global variability characterize the semantic space of human languages. Molly Lewis, Aoife Cahill, Nitin Madnani, James Evans, Proceedings of the National Academy of Sciences. 12051e23009861202023</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg, Proceedings of The Eleventh International Conference on Learning Representations. The Eleventh International Conference on Learning Representations2023</p>
<p>Towards a unified theory of state abstraction for mdps. Lihong Li, Thomas J Walsh, Michael L Littman, AI&amp;M. 122006</p>
<p>Teo Timothy R Mcintosh, Nalin Susnjak, Tong Arachchilage, Paul Liu, Malka N Watters, Halgamuge, arXiv:2402.09880Inadequacies of large language model benchmarks in the era of generative artificial intelligence. 2024arXiv preprint</p>
<p>Quantifying language models' sensitivity to spurious features in prompt design or: How i learned to start worrying about prompt formatting. Melanie Sclar, Yejin Choi, Yulia Tsvetkov, Alane Suhr, arXiv:2310.113242023arXiv preprint</p>
<p>Improving neural machine translation models with monolingual data. Rico Sennrich, Barry Haddow, Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational Linguistics20161</p>
<p>Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang, arXiv:2210.09150Prompting gpt-3 to be reliable. 2022arXiv preprint</p>
<p>Assessing social and intersectional biases in contextualized word representations. Yu Tan, Elisa Celis, NeurIPS. 2021</p>
<p>Chess as a testbed for language model state tracking. Shubham Toshniwal, Sam Wiseman, Karen Livescu, Kevin Gimpel, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202236</p>
<p>Evaluating the world model implicit in a generative model. Keyon Vafa, Justin Chen, Ashesh Rambachan, Jon Kleinberg, Sendhil Mullainathan, Advances in Neural Information Processing Systems. 372024</p>
<p>What's producible may not be reachable: Measuring the steerability of generative models. Keyon Vafa, Sarah Bentley, Jon Kleinberg, Sendhil Mullainathan, arXiv:2503.174822025arXiv preprint</p>
<p>Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks. Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Akyürek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim, Proceedings of the 2024 Conference of the North American Chapter. Long Papers. the 2024 Conference of the North American Chapterthe Association for Computational Linguistics20241</p>
<p>Unsupervised data augmentation for consistency training. Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, Quoc V Le, Advances in Neural Information Processing Systems. 202033</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, arXiv:2010.11934Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer. 2020arXiv preprint</p>
<p>Benchmarking gpt-4 against human translators: A comprehensive evaluation across languages, domains, and expertise levels. Jianhao Yan, Pingchuan Yan, Yulong Chen, Jing Li, Xianchao Zhu, Yue Zhang, arXiv:2411.137752024arXiv preprint</p>
<p>Gpt-4 vs. human translators: A comprehensive evaluation of translation quality across languages, domains, and expertise levels. Jianhao Yan, Pingchuan Yan, Yulong Chen, Judy Li, Xianchao Zhu, Yue Zhang, arXiv:2407.036582024arXiv preprint</p>
<p>On the universal structure of human lexical semantics. Hyejin Youn, Logan Sutton, Eric Smith, Cristopher Moore, Jon F Wilkins, Ian Maddieson, William Croft, Tanmoy Bhattacharya, Proceedings of the National Academy of Sciences. 11372016</p>
<p>Jingming Zhuo, Songyang Zhang, Xinyu Fang, Haodong Duan, Dahua Lin, Kai Chen, arXiv:2410.12405Prosa: Assessing and understanding the prompt sensitivity of llms. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
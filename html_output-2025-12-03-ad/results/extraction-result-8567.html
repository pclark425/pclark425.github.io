<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8567 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8567</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8567</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-154.html">extraction-schema-154</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-273375453</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.12509v1.pdf" target="_blank">Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have gained prominence in the AI landscape due to their exceptional performance. Thus, it is essential to gain a better understanding of their capabilities and limitations, among others in terms of nonmonotonic reasoning. This paper proposes a benchmark that corresponds to various defeasible rule-based reasoning patterns. We modified an existing benchmark for defeasible logic reasoners by translating defeasible rules into text suitable for LLMs. We conducted preliminary experiments on nonmonotonic rule-based reasoning using ChatGPT and compared it with reasoning patterns defined by defeasible logic.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8567.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8567.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated on strict logical reasoning tasks, including details about the models, the logical reasoning tasks or benchmarks, the methods or approaches used to improve logical reasoning, the performance or results, comparisons to baselines or ablations, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (used via ChatGPT interface)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-following generative language model (ChatGPT instance, GPT-4 family variant) evaluated on an adapted defeasible (nonmonotonic) reasoning benchmark by translating rule-sets into natural-language prompts and asking the model to produce stepwise logical conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Defeasible Reasoning with Large Language Models -Initial Experiments and Future Directions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o (ChatGPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Described as a state-of-the-art large language model / ChatGPT system (GPT family), trained as a deep neural transformer with large-scale text corpora and instruction tuning / human feedback (RLHF); used here via ChatGPT with an expert-role instruction and an explicit 'Let's think step by step' prompt to elicit reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Defeasible reasoning benchmark (adapted from Maher et al. 2001)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A collection of scalable propositional defeasible (nonmonotonic) theories translated into natural-language statements (using 'typically' to denote defeasible rules). Included patterns: chain / chains (linear inference chains, defeasible vs strict), circle / circles (cyclic rules), dag (DAG / conjunctions of premises), levels- and levels (conflicting rules with/without superiority priorities), and hierarchies (tree-structured disputes with priorities). Task: given the theory as natural-language statements, decide whether a target literal (A0000000 being an Arkon) is defeasibly provable or not, and provide step-by-step justification.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Translation of formal defeasible-rule theories into controlled natural-language (facts as statements; rules as 'If ... then typically ...' for defeasible rules; 'unless' for superiority), prompting GPT-4o with an expert-role instruction and chain-of-thought style cue ('Let's think step by step'), and four input-condition ablations: (i) statements in random order vs sequential order, and (ii) positive (+∂) vs negative (−∂) target answer contexts. No fine-tuning or external symbolic engine; evaluation is purely prompt-based.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative, theory-dependent: GPT-4o reliably performs monotonic chain inferences (chain(n), chains(n)) in most settings and correctly identifies many hierarchy/priority-resolved cases; for circle(n) it refrains from drawing conclusions when no supporting fact is given and can infer when a fact breaks the circle; dag experiments showed at least one hallucination (incorrect atom substitution) in a random-order negative setting leading to incorrect inference; levels-(n) and levels(n) (conflicts without/with priorities) produced mixed and sometimes incorrect explanations, including introducing unstated facts. No numeric accuracy figures reported.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>No quantitative baseline numbers provided; comparison is conceptual/behavioral against expected defeasible-logic inferences (as defined by Maher et al. 2001 and defeasible logic provability rules). Ablations performed: statement ordering (random vs sequential) and presence/absence of priorities or facts; these showed that ordering and priorities materially affected GPT-4o's outputs. There is no direct numerical comparison to dedicated symbolic defeasible reasoners in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Reported failure modes include: (1) hallucinations (introducing additional facts or substituting atoms, as in dag case), (2) sensitivity to input order (random vs sequential ordering changed inferences and explanations), (3) inability to correctly render the non-provability outcome when conflicting rules lack explicit priorities (i.e., fails to produce neither-p-nor-not-p conclusion), (4) occasional failure to explicitly identify cyclicity as the justification (circle case in some settings), (5) tendency to adopt a closed-world-like assumption and apply monotonic reasoning, and (6) producing explanations that do not match formal defeasible reasoning steps particularly in conflict-heavy theories (levels-(n)).</td>
                        </tr>
                        <tr>
                            <td><strong>insights_or_conclusions</strong></td>
                            <td>Authors conclude GPT-4o shows encouraging performance for monotonic reasoning and for defeasible patterns when priorities and well-structured sequences are present, but struggles with true nonmonotonic phenomena (conflicts without priorities, cycles, and maintaining non-provability). Ordering of premises and the presence of explicit priority statements significantly affect behavior; chain-of-thought style prompting elicits readable inference chains but does not guarantee formally correct defeasible reasoning. Authors recommend expanding benchmarks to more and larger theories and further systematic analysis to probe the limits of LLM nonmonotonic reasoning and to explore remedies (e.g., integrating structured knowledge, specialized prompting, or neuro-symbolic methods).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient defeasible reasoning systems <em>(Rating: 2)</em></li>
                <li>Defeasible reasoning with large language models-initial experiments and future directions <em>(Rating: 2)</em></li>
                <li>Sparks of artificial general intelligence: Early experiments with gpt-4 <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8567",
    "paper_id": "paper-273375453",
    "extraction_schema_id": "extraction-schema-154",
    "extracted_data": [
        {
            "name_short": "GPT-4o (ChatGPT)",
            "name_full": "GPT-4o (used via ChatGPT interface)",
            "brief_description": "A large instruction-following generative language model (ChatGPT instance, GPT-4 family variant) evaluated on an adapted defeasible (nonmonotonic) reasoning benchmark by translating rule-sets into natural-language prompts and asking the model to produce stepwise logical conclusions.",
            "citation_title": "Benchmarking Defeasible Reasoning with Large Language Models -Initial Experiments and Future Directions",
            "mention_or_use": "use",
            "model_name": "GPT-4o (ChatGPT)",
            "model_description": "Described as a state-of-the-art large language model / ChatGPT system (GPT family), trained as a deep neural transformer with large-scale text corpora and instruction tuning / human feedback (RLHF); used here via ChatGPT with an expert-role instruction and an explicit 'Let's think step by step' prompt to elicit reasoning chains.",
            "model_size": null,
            "reasoning_task_name": "Defeasible reasoning benchmark (adapted from Maher et al. 2001)",
            "reasoning_task_description": "A collection of scalable propositional defeasible (nonmonotonic) theories translated into natural-language statements (using 'typically' to denote defeasible rules). Included patterns: chain / chains (linear inference chains, defeasible vs strict), circle / circles (cyclic rules), dag (DAG / conjunctions of premises), levels- and levels (conflicting rules with/without superiority priorities), and hierarchies (tree-structured disputes with priorities). Task: given the theory as natural-language statements, decide whether a target literal (A0000000 being an Arkon) is defeasibly provable or not, and provide step-by-step justification.",
            "method_or_approach": "Translation of formal defeasible-rule theories into controlled natural-language (facts as statements; rules as 'If ... then typically ...' for defeasible rules; 'unless' for superiority), prompting GPT-4o with an expert-role instruction and chain-of-thought style cue ('Let's think step by step'), and four input-condition ablations: (i) statements in random order vs sequential order, and (ii) positive (+∂) vs negative (−∂) target answer contexts. No fine-tuning or external symbolic engine; evaluation is purely prompt-based.",
            "performance": "Qualitative, theory-dependent: GPT-4o reliably performs monotonic chain inferences (chain(n), chains(n)) in most settings and correctly identifies many hierarchy/priority-resolved cases; for circle(n) it refrains from drawing conclusions when no supporting fact is given and can infer when a fact breaks the circle; dag experiments showed at least one hallucination (incorrect atom substitution) in a random-order negative setting leading to incorrect inference; levels-(n) and levels(n) (conflicts without/with priorities) produced mixed and sometimes incorrect explanations, including introducing unstated facts. No numeric accuracy figures reported.",
            "baseline_comparison": "No quantitative baseline numbers provided; comparison is conceptual/behavioral against expected defeasible-logic inferences (as defined by Maher et al. 2001 and defeasible logic provability rules). Ablations performed: statement ordering (random vs sequential) and presence/absence of priorities or facts; these showed that ordering and priorities materially affected GPT-4o's outputs. There is no direct numerical comparison to dedicated symbolic defeasible reasoners in this paper.",
            "limitations_or_failures": "Reported failure modes include: (1) hallucinations (introducing additional facts or substituting atoms, as in dag case), (2) sensitivity to input order (random vs sequential ordering changed inferences and explanations), (3) inability to correctly render the non-provability outcome when conflicting rules lack explicit priorities (i.e., fails to produce neither-p-nor-not-p conclusion), (4) occasional failure to explicitly identify cyclicity as the justification (circle case in some settings), (5) tendency to adopt a closed-world-like assumption and apply monotonic reasoning, and (6) producing explanations that do not match formal defeasible reasoning steps particularly in conflict-heavy theories (levels-(n)).",
            "insights_or_conclusions": "Authors conclude GPT-4o shows encouraging performance for monotonic reasoning and for defeasible patterns when priorities and well-structured sequences are present, but struggles with true nonmonotonic phenomena (conflicts without priorities, cycles, and maintaining non-provability). Ordering of premises and the presence of explicit priority statements significantly affect behavior; chain-of-thought style prompting elicits readable inference chains but does not guarantee formally correct defeasible reasoning. Authors recommend expanding benchmarks to more and larger theories and further systematic analysis to probe the limits of LLM nonmonotonic reasoning and to explore remedies (e.g., integrating structured knowledge, specialized prompting, or neuro-symbolic methods).",
            "uuid": "e8567.0",
            "source_info": {
                "paper_title": "Benchmarking Defeasible Reasoning with Large Language Models -- Initial Experiments and Future Directions",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient defeasible reasoning systems",
            "rating": 2,
            "sanitized_title": "efficient_defeasible_reasoning_systems"
        },
        {
            "paper_title": "Defeasible reasoning with large language models-initial experiments and future directions",
            "rating": 2,
            "sanitized_title": "defeasible_reasoning_with_large_language_modelsinitial_experiments_and_future_directions"
        },
        {
            "paper_title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "rating": 1,
            "sanitized_title": "sparks_of_artificial_general_intelligence_early_experiments_with_gpt4"
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1,
            "sanitized_title": "language_models_are_fewshot_learners"
        }
    ],
    "cost": 0.00742075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking Defeasible Reasoning with Large Language Models -Initial Experiments and Future Directions
16 Oct 2024</p>
<p>Ilias Tachmazidis i.tachmazidis@hud.ac.uk 
School of Computing and Engineering
University of Huddersfield
UK</p>
<p>Sotiris Batsakis sbatsakis@hmu.gr 
School of Computing and Engineering
University of Huddersfield
UK</p>
<p>Hellenic Mediterranean University
Greece</p>
<p>Grigoris Antoniou g.antoniou@leedsbeckett.ac.uk 
Leeds Beckett University
UK</p>
<p>Benchmarking Defeasible Reasoning with Large Language Models -Initial Experiments and Future Directions
16 Oct 2024B40AC34E9CDAD986D84E1FBDCFCAB625arXiv:2410.12509v1[cs.AI]
Large Language Models (LLMs) have gained prominence in the AI landscape due to their exceptional performance.Thus, it is essential to gain a better understanding of their capabilities and limitations, among others in terms of nonmonotonic reasoning.This paper proposes a benchmark that corresponds to various defeasible rule-based reasoning patterns.We modified an existing benchmark for defeasible logic reasoners by translating defeasible rules into text suitable for LLMs.We conducted preliminary experiments on nonmonotonic rulebased reasoning using ChatGPT and compared it with reasoning patterns defined by defeasible logic.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) have caught people's attention recently due to the exceptional performance these systems achieved in various language related tasks since they are the underlying technology behind chat bots such as ChatGPT 1 .Large Language models such as LaMDA (Thoppilan et al. 2022) and GPT (OpenAI 2023) are based on training deep neural networks with billions of parameters using huge lexical datasets and often employing human judgment in a semi-supervised (e.g., reinforcement learning) training setting (Lambert et al. 2022;Ouyang et al. 2022).The exceptional -often human level-performance of LLMs in various tasks has led to a widespread discussion about the potential benefits and dangers of such technologies in various areas and human society in general including petitions to pause research on more capable LLMs (Letters 2023).For example GPT-4 achieved human lever performance in various academic and professional exams including a score in the top 10% of test takers in the Uniform Bar Examination, this performance is attributed to a large degree to scaling LLMs to larger training datasets and more complex models with larger number of parameters (OpenAI 2023).</p>
<p>Despite the impressive performance of Large Language Models, including their ability to demonstrate an emerging intelligent behaviour and reasoning capabilities, leading to the point of considering them forerunners of Artificial General Intelligence (Bubeck et al. 2023) several issues related to LLMs have been identified, such as the energy cost of training LLMs (Luccioni, Viguier, and Ligozat 2022; 1 Available at: https://chat.openai.com/Strubell, Ganesh, and McCallum 2019), difficulty to control their behaviour (Luccioni and Viviano 2021), ensuring conformity with stakeholders requirements and norms, as well as interpreting their functionality (Bowman 2023).The interpetability of LLMs is a crucial issue since neural network based LLMs appear to be 'black boxes', in contrast to logic based systems, and although various attempts exist to deal with this problem, including the use of LLMs to interpret LLMs (Bills et al. 2023), this is still an unresolved issue.In addition, since LLMs are trained on vast amounts of raw text they tend to replicate their input rather that apply robust reasoning (Bender et al. 2021).LLMs trained on raw text instead of structured knowledge bases integrating machine readable semantics contribute to the difficulty of achieving efficient reasoning and this is an issue examined in various works such as (Zhang et al. 2022) and surveyed in (Huang and Chang 2022).Various attempts to integrate Knowledge Graphs (KGs) to LLMs have been proposed (Zhen et al. 2022;Yin et al. 2022) as a solution to the last issue, but recent advances in LLMs capabilities, including high performance on academic and professional exams (OpenAI 2023), illustrated the need for an updated evaluation of the reasoning capabilities of LLMs.This updated evaluation should take into account the recent developments in the field, including the deployment of systems such as ChatGPT employing the benefits of scalability (Kaplan et al. 2020) and the LLMs demonstrated ability to adjust to new tasks given just a small number of examples (Brown et al. 2020).Furthermore LLMs capabilities with respect to important formalisms such as defeasible reasoning have not been examined in detail yet.This kind of reasoning is important for cases where knowledge is incomplete and conflicting, which is the case in many application areas including law and healthcare.In previous work (Antoniou and Batsakis 2023) preliminary experiments on LLM defeasible reasoning have been performed, but a systematic analysis involving benchmark construction containing several examples of different reasoning patters is missing.</p>
<p>This work is an initial step towards developing a deep understanding of reasoning capabilities of LLMs with emphasis of nonmonotonic reasoning.In order to achieve this we propose a benchmark for LLMs by modifying an existing benchmark for defeasible logic reasoners.The pro-posed benchmark corresponds to various reasoning patterns that will be described in the following.Furthermore we conducted preliminary experiments on nonmonotonic rulebased reasoning using ChatGPT and compared it with reasoning patterns defined by defeasible logic.</p>
<p>Background</p>
<p>A defeasible theory D is a triple (F,R,&gt;) where F is a finite set of facts (literals), R a finite set of rules, and &gt; a superiority relation (acyclic relation upon R).</p>
<p>A rule r consists (a) of its antecedent (or body) A(r) which is a finite set of literals, (b) an arrow, and, (c) its consequent (or head) C(r) which is a literal.There are three types of rules: strict rules, defeasible rules and defeaters represented by a respective arrow →, ⇒ and .Strict rules are rules in the classical sense: whenever the premises are indisputable (e.g., facts) then so is the conclusion.Defeasible rules are rules that can be defeated by contrary evidence.Defeaters are rules that cannot be used to draw any conclusions; their only use is to prevent some conclusions.</p>
<p>Given a set R of rules, we denote the set of all strict rules in R by R s , and the set of strict and defeasible rules in R by R sd .R[q] denotes the set of rules in R with consequent q.If q is a literal, ∼q denotes the complementary literal (if q is a positive literal p then ∼q is ¬p; and if q is ¬p, then ∼q is p).</p>
<p>A conclusion of D is a tagged literal and can have one of the following four forms:</p>
<p>• +∆q, meaning that q is definitely provable in D.</p>
<p>• −∆q, meaning that we have proved that q is not definitely provable in D.</p>
<p>• +∂q, meaning that q is defeasibly provable in D.</p>
<p>• −∂q, meaning that we have proved that q is not defeasibly provable in D.</p>
<p>Provability is defined below.It is based on the concept of a derivation (or proof) in D = (F, R, &gt;).A derivation is a finite sequence P = P(1), ..., P(n) of tagged literals satisfying the conditions shown below.The conditions are essentially inference rules phrased as conditions on proofs.P(1..ı) denotes the initial part of the sequence P of length i.For more details on provability and an explanation of the intuition behind the conditions below, see (Maher 2004).
+∆: We may append P(ı + 1) = +∆q if either q ∈ F or ∃r ∈ R s [q] ∀α ∈ A(r): +∆α ∈ P(1..ı) −∆: We may append P(ı + 1) = −∆q if q / ∈ F and ∀r ∈ R s [q] ∃α ∈ A(r): −∆α ∈ P(1..ı)</p>
<p>Dataset</p>
<p>We propose a dataset of scalable test theories which is inspired by (Maher et al. 2001).In (Maher et al. 2001) authors focused on evaluating the efficiency of existing defeasible reasoning systems.Here, we focus on a translation of rules into text suitable for LLMs.The proposed dataset is focused on typical defeasible inference patterns, allowing a comparison between inputs for reasoning systems and LLMs.</p>
<p>Empty.First, we skip the empty() theory as it contains no facts, rules or priorities.The empty() theory serves as a baseline for reasoning systems.However, there is no meaningful evaluation of a LLM in the absence of text.</p>
<p>Chain.Our first theory is chain(n), where a 0 is at the end of a chain of n rules a i+1 ⇒ a i , with a single fact a n initiating the chain of inference (no priorities defined).For chain (2), the defeasible rules are as follows:</p>
<blockquote>
<blockquote>
<p>A0000002 r 1 : A0000002 =&gt; A0000001 r 2 : A0000001 =&gt; A0000000
Note that "&gt;&gt; A0000002" denotes a fact following the syntax of SPINdle (Rohaninezhad, Arif, and Noah 2015).Based on the fact A0000002 rule r1 infers that A0000001 is deafeasibly provable, while rule r2 infers that A0000000 is deafeasibly provable as well.In this work, the structure of the theories is aimed at determining through logical inference whether A0000000 is provable or not.Subsequently, the translation of chain( 2) into plain text is as follows: A0000002 i s an Arkon .I f A0000002 i s an Arkon , t h e n t y p i c a l l y A0000001 i s an Arkon .I f A0000001 i s an Arkon , t h e n t y p i c a l l y A0000000 i s an Arkon .</p>
</blockquote>
</blockquote>
<p>Notice the pattern, facts are expressed as statements, while rules are expressed as if-then statements with the keyword "typically" denoting the defeasible nature of the rule.Given the already identified affinity of ChatGPT to use other background knowledge when predicates and atoms are real-world entities, we use imaginary names of species on an imaginary planet, following (Ford and Billington 2000).</p>
<p>Here, we use "Arkon" in order to ask ChatGPT: I s A0000000 an Arkon ?</p>
<p>The theory chains(n), is a version of chain(n) with strict rules.For chains (2), the defeasible rules are as follows:</p>
<blockquote>
<blockquote>
<p>A0000002 r 1 : A0000002 −&gt; A0000001 r 2 : A0000001 −&gt; A0000000
The translation of chains( 2) into plain text is as follows: A0000002 i s an Arkon .I f A0000002 i s an Arkon , t h e n A0000001 i s an Arkon .I f A0000001 i s an Arkon , t h e n A0000000 i s an Arkon .</p>
</blockquote>
</blockquote>
<p>Notice the absence of keyword "typically" in the if-then statements.</p>
<p>Circle.In defeasible logic, cyclical chains of reasoning do not lead to inferences.More specifically, in the theory circle(n), a 0 is part of a circle of n rules a i+1 mod n ⇒ a i (no facts or priorities defined).For circle (2), the defeasible rules are as follows:
r 1 : A0000000 =&gt; A0000001 r 2 : A0000001 =&gt; A0000000
Due to the cyclical nature of the rules, no defeasible conclusion is infered for either A0000000 or A0000001.The translation of circle( 2) into plain text is as follows: I f A0000000 i s an Arkon , t h e n t y p i c a l l y A0000001 i s an Arkon .I f A0000001 i s an Arkon , t h e n t y p i c a l l y A0000000 i s an Arkon .</p>
<p>The theory circles(n), is a version of circle(n) with strict rules.For circles (2), the defeasible rules are as follows:
r 1 : A0000000 −&gt; A0000001 r 2 : A0000001 −&gt; A0000000
The translation of circles( 2) into plain text is as follows: I f A0000000 i s an Arkon , t h e n A0000001 i s an Arkon .I f A0000001 i s an Arkon , t h e n A0000000 i s an Arkon .</p>
<p>Notice again the absence of keyword "typically" in the ifthen statements.</p>
<p>Directed Acyclic Graph (DAG).In order to consider more complex inference structures, we define theory dag(n,k), where a 0 is the root of a k-branching tree of depth nk in which every literal occurs n times.The inference process is initiated by k facts, namely a nk+1 , ..., a nk+k (no priorities defined).For dag (2,2), the defeasible rules are as follows:</p>
<blockquote>
<blockquote>
<p>A0000006 &gt;&gt; A0000005 r 1 : A0000006 , A0000005 =&gt; A0000004 r 2 : A0000005 , A0000004 =&gt; A0000003 r 3 : A0000004 , A0000003 =&gt; A0000002 r 4 : A0000003 , A0000002 =&gt; A0000001 r 5 : A0000002 , A0000001 =&gt; A0000000 Notice that nk + 1 (here 5) rules are generated, with k (here 2) facts, namely A0000006 and A0000005 making rule r1 applicable, inferring A0000004.By applying rules r1, r2, r3, r4 and r5 we can eventually infer A0000000.The translation of dag (2,2) into plain text is as follows: A0000006 i s an Arkon .A0000005 i s an Arkon .I f A0000006 i s an Arkon and A0000005 i s an Arkon , t h e n t y p i c a l l y A0000004 i s an Arkon .I f A0000005 i s an Arkon and A0000004 i s an Arkon , t h e n t y p i c a l l y A0000003 i s an Arkon .I f A0000004 i s an Arkon and A0000003 i s an Arkon , t h e n t y p i c a l l y A0000002 i s an Arkon .I f A0000003 i s an Arkon and A0000002 i s an Arkon , t h e n t y p i c a l l y A0000001 i s an Arkon .I f A0000002 i s an Arkon and A0000001 i s an Arkon , t h e n t y p i c a l l y A0000000 i s an Arkon .</p>
</blockquote>
</blockquote>
<p>Notice that multiple predicates in the body of a rule are connected through the keyword "and" in the if part of the if-then statement.</p>
<p>Levels.All mentioned theories above contained no conflicts.However, conflict resolution is an integral part of defeasible reasoning.Thus, theory levels-(n) defines a cascade of n conclusions, namely there are rules true ⇒ a i and a i+1 ⇒ ¬a i , for 0 ≤ i &lt; n (no facts or priorities defined).For levels-( 2), the defeasible rules are as follows: r 1 : =&gt; A0000001 r 2 : A0000002 =&gt; −A0000001 r 3 : =&gt; A0000000 r 4 : A0000001 =&gt; −A0000000 Notice that when the body of a rule (here r1 and r3) is empty then syntactically all preconditions are considered to be met.Negative conclusions such as ¬A0000001 are prefixed with the minus sign, namely -A0000001.Since there is no fact supporting A0000002, rule r2 does not apply, thus we conclude A0000001 based on rule r1.Subsequently, since both rules r3 and r4 apply, we cannot conclude A0000000.Notice an emerging pattern where A0000000 cannot be proved for even n, while A0000000 can be proved for odd n (due to alternating conflicts on subsequent levels).The translation of levels-( 2) into plain text is as follows: A0000001 i s t y p i c a l l y an Arkon .I f A0000002 i s an Arkon , t h e n t y p i c a l l y A0000001 i s n o t an Arkon .A0000000 i s t y p i c a l l y an Arkon .I f A0000001 i s an Arkon , t h e n t y p i c a l l y A0000000 i s n o t an Arkon .</p>
<p>The theory levels(n), is a version of levels-(n), where in addition there are superiority statements stating that, for odd i, rule a i+1 ⇒ ¬a i is superior to true ⇒ a i (introducing n/2 priorities).For levels (2), the defeasible rules are as follows: r 1 : =&gt; A0000001 r 2 : A0000002 =&gt; −A0000001 r 2 &gt; r 1 r 3 : =&gt; A0000000 r 4 : A0000001 =&gt; −A0000000 Notice that due to the priority rule, if rule r2 was applicable (e.g. with A0000002 given as fact) then ¬A0000001 would have been inferred (instead of A0000001 inferred here).The translation of levels( 2) into plain text is as follows: A0000001 i s t y p i c a l l y an Arkon , u n l e s s A0000002 i s a l s o an Arkon ( n am ely t h e n A0000001 i s n o t an Arkon ) .A0000000 i s t y p i c a l l y an Arkon .I f A0000001 i s an Arkon , t h e n t y p i c a l l y A0000000 i s n o t an Arkon .</p>
<p>A similar inference pattern emerges where A0000000 cannot be proved for even n, while A0000000 can be proved for odd n (even though the process of conflict resolution on subsequent levels is different for levels(n) compared to levels-(n)).</p>
<p>Hierarchies.The authors of (Maher et al. 2001) defined theories: (i) tree(n,k), where a 0 is the root of a k-branching tree of depth n in which every literal occurs once, and (ii) teams(n), where every literal is disputed, with two rules for a i and two rules for ¬a i , and the rules for a i are superior to the rules for ¬a i (this situation is repeated recursively to a depth n).In this work, we define hierarchies(n,k), where a 0 is the root of a k-branching tree of depth n in which every literal occurs once.In addition, every literal (internal node of the tree) is disputed, with k/2 rules for a i and k/2 rules for ¬a i , where k is even, and the rules for a i are superior to the rules for ¬a i .Each external node of the tree is a fact, namely there are k n facts.For hierarchies (2,2), the defeasible rules are as follows:</p>
<blockquote>
<blockquote>
<p>A0000006 &gt;&gt; A0000005 &gt;&gt; A0000004 &gt;&gt; A0000003 r 1 : A0000006 =&gt; A0000002 r 2 : A0000005 =&gt; −A0000002 r 1 &gt; r 2 r 3 : A0000004 =&gt; A0000001 r 4 : A0000003 =&gt; −A0000001 r 3 &gt; r 4 r 5 : A0000002 =&gt; A0000000 r 6 : A0000001 =&gt; −A0000000 r 5 &gt; r 6
The translation of hierarchies (2,2) into plain text is as follows: A0000006 i s an Arkon .A0000005 i s an Arkon .A0000004 i s an Arkon .A0000003 i s an Arkon .I f A0000006 i s an Arkon , t h e n t y p i c a l l y A0000002 i s an Arkon .I f A0000005 i s an Arkon , t h e n t y p i c a l l y A0000002 i s n o t an Arkon , u n l e s s A0000006 i s a l s o an Arkon ( n am ely t h e n A0000002 i s an Arkon ) .I f A0000004 i s an Arkon , t h e n t y p i c a l l y A0000001 i s an Arkon .</p>
</blockquote>
</blockquote>
<p>I f A0000003 i s an Arkon , t h e n t y p i c a l l y A0000001 i s n o t an Arkon , u n l e s s A0000004 i s a l s o an Arkon ( n am ely t h e n A0000001 i s an Arkon ) .I f A0000002 i s an Arkon , t h e n t y p i c a l l y A0000000 i s an Arkon .I f A0000001 i s an Arkon , t h e n t y p i c a l l y A0000000 i s n o t an Arkon , u n l e s s A0000002 i s a l s o an Arkon ( n am ely t h e n A0000000 i s an Arkon ) .</p>
<p>Here, A0000000 can be proved for any given parameter n and k since conflicts are always resolved in favour of a i .</p>
<p>We consider out of scope of this work theory mix(m,n,k) from (Maher et al. 2001), where there are m defeasible rules for a 0 and m defeaters against a 0 , where each rule has n atoms in its body (each atom can be established by a chain of strict rules of length k).</p>
<p>Table 1 summarises the number of facts, rules and priorities generated for each theory as a function of given parameters.Notice that the numbers in Table 1 do not necessarily match the numbers in (Maher et al. 2001) as we have modified theory definitions.</p>
<p>Theory</p>
<p>Facts Rules Priorities
chain(n) 1 n 0 chains(n) 1 n 0 circle(n) 0 n 0 circles(n) 0 n 0 dag(n,k) k nk + 1 0 levels-(n) 0 2n 0 levels(n) 0 2n n/2 hierarchies(n,k) k n k n−1 i=0 k i k 2 n−1 i=0 k i 4 Experimental Results
The proposed dataset is scalable, namely increasingly larger theories can be generated for increasing values of parameters n and k.However, as a first step in this work, we focus on relatively small and readable theories in order to assess empirically the inference patterns of ChatGPT.We used GPT-4o in order to assess theories: chain(8) 2 , chains(8) 3 , 2 https://chatgpt.com/share/d8819744-9d90-42cb-aeba-95a28769f08e</p>
<p>3 https://chatgpt.com/share/15f3f0a5-e8c9-4156-9050-7a28b62cc189circle(8) 4 , circles(8) 5 , dag(3,2) 6 , levels-(5) 7 , levels(5) 8 , hierarchies(2,4) 9 .ChatGPT was given the following instructions for each theory: You a r e an e x p e r t on d e f e a s i b l e r e a s o n i n g .Your t a s k i s t o make l o g i c a l c o n c l u s i o n s b a s e d on p r o v i d e d k n o wled g e ( d e l i m i t e d w i t h XML t a g s ) .</p>
<p>In addition, each prompt was based on the following template (namely, "{theory}" was substituted with each evaluated theory):</p>
<p>Based on t h e f o l l o w i n g k n o wled g e a l o n e :</p>
<p><k n o wled g e> ' ' { t h e o r y } ' ' &lt;/ k n o wled g e&gt; I s A0000000 an Arkon ?</p>
<p>Let ' s t h i n k s t e p by s t e p .</p>
<p>Each generated theory, such as chain(8), was evaluated over four settings:</p>
<p>• A0000000 is not an Arkon with statements provided in random order (−∂-rand),</p>
<p>• A0000000 is an Arkon with statements provided in random order (+∂-rand),</p>
<p>• A0000000 is not an Arkon with statements provided in sequential order (−∂-seq),</p>
<p>• A0000000 is an Arkon with statements provided in sequential order (+∂-seq).</p>
<p>We evaluated statements provided in random order first in order to observe any differences in generated responces when the same theory is provided in sequential order.Notice that ChatGPT conversations provided as links in footnotes contain the four settings in the following order: −∂rand, +∂-rand, −∂-seq, +∂-seq.Results are summarised in Table 2.</p>
<p>For theory chain(8) we notice that +∂-rand and +∂-seq have similar inference patterns, namely starting from provided facts, each rule is applied until a final conclusion is reached (interestingly, statements provided in random order do not change the inference sequence).We also notice that while −∂-seq evaluates all rules sequentially (even 4 https://chatgpt.com/share/a73155a9-9c3d-498d-8b2d-9532a9cd6d545 https://chatgpt.com/share/3b381c57-f881-4631-92e0-3974c446a5df6 https://chatgpt.com/share/db79edc0-1f1e-4fec-bba1-b16fb6bd15a8 7https://chatgpt.com/share/f332e764-91df-4672-b6d0-1fa95196c96e 8https://chatgpt.com/share/a1b2ae89-8f49-4f25-a7ea-fc94ac3e942d9 https://chatgpt.com/share/bc1181d4-1f94-4a2f-8c9b-cf11fb28c35d</p>
<p>Table 2: ChatGPT inference results (for results annotated with † readers are referred to comments included in the main text).after encountering A1111113), the inference of −∂-rand moves backwards once A1111113 is encountered.Theory chains(8) follows similar inference patterns based on if-then statements.</p>
<p>For theory circle(8) we notice that for both −∂-rand and −∂-seq the circle is identified and no conclusion can be drawn.For +∂-rand and +∂-seq we introduced a fact that proves the circle, inference started from the given fact leading to the inference of A0000000.Theory circles(8) follows similar inference patterns based on if-then statements.However, for −∂-rand the circle was not explicitly identified as justification for inference.</p>
<p>For theory dag(3,2), −∂-rand exhibited unusual patterns (i.e. a potential hallucination), namely A1111114 was replaced with A0000004 (incorrectly), leading to the inference of A0000000 (while a rule based on A0000005 and A0000003 leading to A0000002 was not given as input).Interestingly, for −∂-seq the inference pattern was correct with A1111114 breaking the chain of inference (this indicates that the sequence of statements can have an effect on the inference process, which is not the case for standard reasoners).The inference pattern for +∂-rand was correct, even though it is unclear why the "Chain of Reasoning" did not include the conjunction of two premises.The inference pattern for +∂-seq was correct, with a well formed "Chain of Reasoning".</p>
<p>Theory levels-(5) introduces conflicting rules, with explanations provided not matching expected inference for defeasible reasoning.This might be attributed to the fact that ChatGPT starts from A0000000 working backwards, while the lack of priorities over conflicting rules introduces confusion.It is worth pointing out that −∂-seq introduced the statement "A0000002 is typically an Arkon."(a fact not given as input), i.e. a potential hallucination.</p>
<p>Theory levels(5) contains priority rules, which provide some clarity.However, for −∂-rand, since there are no priority rules for A0000002 the inference does not match deafeasible reasoning (it seems that ChatGPT does not follow the notion of defeasible reasoning where both A0000002 and ¬A0000002 might not be provable).Conversely, for +∂-rand the combination of priorities and rules with failing premises (namely, the resolution of A0000002 knowing that A0000003 is not an Arkon) leads to conclusions that follow defeasible reasoning.Interestingly, the inference steps of −∂-seq are well structured, closely resembling defeasible reasoning (with the exception of resolving A0000002 without a clear priority, which leads to an error).The inference of +∂-seq showed that the sequence of statements (where well structured sequences lead to more intuitive inference steps) as well as the presence of priorities and rules with failing premises can affect the inference process.</p>
<p>For theory hierarchies (2,4), −∂-rand and +∂-rand lead to correct inferences due to the presence of priorities and rules with failing premises.However, due to the random order of statements, the explanation of inferences can be challenging to follow.This is not the case for −∂-seq and +∂seq, which exhibited correct and well structured inference steps.</p>
<p>Overall, the following observations can be made:</p>
<p>• ChatGPT seems to adopt the closed-world assumption, where facts are considered as true and missing information as false,</p>
<p>• Monotonic rules are applied leading to new conclusions,</p>
<p>• Conflicting rules are resolved in the presence of priorities and rules with failing premises, • The presence of conflicting rules supporting both p and ¬p, with no clear priority, does not lead to a conclusion that neither p nor ¬p can be inferred,</p>
<p>• Additional facts or rules (not provided as input) could be automatically introduced (i.e. a potential hallucination),</p>
<p>• A well structured sequence of statements (given as input) increases the readability of the inference process (compared to equivalent theories structured as statements in random order).</p>
<p>Conclusion</p>
<p>This work is a first step towards gaining a better understanding of reasoning capabilities of LLMs with respect to nonmonotonic reasoning.We proposed a benchmark tailored to LLMs through the modification of an existing benchmark for defeasible logic reasoners.A range of reasoning patterns was covered by the proposed benchmark.Preliminary experiments indicated encouraging results for monotonic reasoning as well as certain challenges in the context of nonmonotonic rule-based reasoning.Future work will focus on expanding our exploration of reasoning patterns that might pose a challenge to LLMs.Furthermore, while this work was focused on small and readable theories, future efforts will examine the effect of increasingly larger theories on the reasoning process of LLMs.</p>
<p>Table 1 :
1
Sizes of scalable theories.</p>
<p>+∂: We may append P (ı + 1) = +∂q if either (1) +∆q ∈ P(1..ı) or. </p>
<p>) ∃t ∈ R sd [q] such that ∀α ∈ A(t): +∂α ∈ P(1..ı) and t &gt; s −∂: We may append P(ı + 1) = −∂q if (1) −∆q ∈ P(1..ı) and (2) (2.1) ∀r ∈ R sd [q] ∃α ∈ A(r): −∂α ∈ P(1..ı) or (2.2) +∆ ∼q ∈ P(1..ı) or (2.3) ∃s ∈ R[∼q] such that (2.3.1) ∀α ∈ A(s): +∂α ∈ P(1..ı) and (2.3.2) ∀t ∈ R sd [q] either ∃α ∈ A(t): −∂α ∈ P(1..ı) or t ≯ s References Antoniou, G., and Batsakis, S. 2023. Defeasible reasoning with large language models-initial experiments and future directions. ; ∃r ∈ R Sd, H Gao, L Goh, G Sutskever, I Leike, J Wu, J , ∀α ∈ A(r): +∂α ∈ P(1..ı) and (2.2) −∆ ∼q ∈ P(1..ı) and (2.3) ∀s ∈ R[∼q] either (2.3.1) ∃α ∈ A(s): −∂α ∈ P(1..ı) or (2.3.2Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, the 2021 ACM conference on fairness, accountability, and transparencyBills, S; Cammarata, N.; Mossing, D3485CEUR Workshop Proceedings. Tillman. and Saunders, W. 2023. Language models can explain neurons in language models</p>
<p>S R Bowman, arXiv:2304.00612Eight things to know about large language models. 2023arXiv preprint</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, 2020</p>
<p>S Bubeck, V Chandrasekaran, R Eldan, J Gehrke, E Horvitz, E Kamar, P Lee, Y T Lee, Y Li, S Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023. 200016arXiv preprintStrategies in human nonmonotonic reasoning</p>
<p>J Huang, K C Chang, -C , arXiv:2212.10403Towards reasoning in large language models: A survey. 2022arXiv preprint</p>
<p>Scaling laws for neural language models. J Kaplan, S Mccandlish, T Henighan, T B Brown, B Chess, R Child, S Gray, A Radford, J Wu, D Amodei, N Lambert, L Castricato, L Von Werra, A Havrilla, Illustrating reinforcement learning from human feedback (rlhf). Hugging Face Blog. 2020. 2022</p>
<p>Pause giant ai experiments: An open letter. F O Letters, Future of Life Institution. 2023</p>
<p>What's in the box? an analysis of undesirable content in the common crawl corpus. A Luccioni, J Viviano, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Short Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20212</p>
<p>A S Luccioni, S Viguier, A.-L Ligozat, arXiv:2211.02001Estimating the carbon footprint of bloom, a 176b parameter language model. 2022arXiv preprint</p>
<p>Efficient defeasible reasoning systems. M J Maher, A Rock, G Antoniou, D Billington, T Miller, Int. J. Artif. Intell. Tools. 1042001</p>
<p>M J Maher, Propositional Defeasible Logic has Linear Complexity. CoRR cs.AI/0405090. OpenAI. 2023. Gpt-4 technical report. 2004</p>
<p>. L Ouyang, J Wu, X Jiang, D Almeida, C L Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, J Schulman, J Hilton, F Kelton, L Miller, M Simens, A Askell, P Welinder, P Christiano, J Leike, R Lowe, 2022Training language models to follow instructions with human feedback</p>
<p>A grounder for spindle defeasible logic reasoner. M Rohaninezhad, S M Arif, S A M Noah, Expert Systems with Applications. 42202015</p>
<p>Energy and policy considerations for deep learning in nlp. E Strubell, A Ganesh, A Mccallum, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics2019</p>
<p>Lamda: Language models for dialog applications. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Y Du, arXiv:2201.08239arXiv:2202.08772A survey of knowledgeintensive nlp with pre-trained language models. 2022. 2022arXiv preprint</p>
<p>H Zhang, L H Li, T Meng, K.-W Chang, G V Broeck, arXiv:2205.11502On the paradox of learning to reason from data. 2022arXiv preprint</p>
<p>A survey on knowledge-enhanced pre-trained language models. C Zhen, Y Shang, X Liu, Y Li, Y Chen, D Zhang, arXiv:2212.134282022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
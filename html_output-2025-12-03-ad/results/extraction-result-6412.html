<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6412 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6412</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6412</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-275515398</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.08203v2.pdf" target="_blank">ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) have shown impressive capabilities in math problem-solving tasks, their robustness to noisy inputs is not well-studied. We propose ArithmAttack to examine how robust the LLMs are when they encounter noisy prompts that contain extra noise in the form of punctuation marks. While being easy to implement, ArithmAttack does not cause any information loss since words are not added or deleted from the context. We evaluate the robustness of eight LLMs, including LLama3, Mistral, Mathstral, and DeepSeek on noisy GSM8K and MultiArith datasets. Our experiments suggest that all the studied models show vulnerability to such noise, with more noise leading to poorer performances.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6412.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6412.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mistral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mistral-7B-Instruct-v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter instruct-tuned large language model evaluated in this paper for robustness to random punctuation noise on arithmetic word problems (GSM8K and MultiArith).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mistral-7B-Instruct-v0.2</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought (prompt: 'Think step by step... Ensure the final answer is indicated by ending with {The final answer is}').</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and Attack Success Rate (ASR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 42.07%; 10% noise 41.62%; 30% noise 37.75%; 50% noise 36.39%; ASR 39.69%. MultiArith: Clean 73.88%; 10% noise 72.77%; 30% noise 71.11%; 50% noise 65.55%; ASR 23.66%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Limited: authors computed semantic similarity (Universal Sentence Encoder) showing perturbed inputs score 100% similarity to originals and performed manual inspection and answer-extraction miss-rate analysis; no probing (attention/activation/logit) or representational analysis reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance degrades as random punctuation noise increases; notable declines on GSM8K (harder set). Noisy punctuation appears to break the model's reasoning flow despite preserved semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Within paper, 7B Mistral is less robust than similarly sized Mathstral and less robust than 8B Llama models; no broad scaling law analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6412.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6412.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mathstral-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mathstral-7b-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B-parameter model oriented toward mathematical capability, evaluated for robustness to punctuation-noise attacks on arithmetic word problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mathstral-7b-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought (same prompt as other models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and ASR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 77.63%; 10% noise 75.51%; 30% noise 71.34%; 50% noise 70.65%; ASR 19.81%. MultiArith: Clean 96.11%; 10% noise 92.77%; 30% noise 86.11%; 50% noise 87.22%; ASR 9.47%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Only semantic-similarity check (USE = 100%) and manual answer-extraction checks; no internal mechanistic probing reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Accuracy degrades with higher punctuation noise, but Mathstral shows more resilience than the baseline Mistral (same family), suggesting improvements tied to mathematical specialization rather than noise-specific defenses.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Shows that model variants with stronger math-oriented training (Mathstral vs Mistral at same size) are more robust; no multi-size scaling curve provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6412.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6412.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B Llama-3 instruct-tuned model evaluated for arithmetic problem solving robustness under punctuation-noise perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought (explicit step-by-step prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and ASR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 75.43%; 10% noise 73.31%; 30% noise 73.08%; 50% noise 72.17%; ASR 11.73%. MultiArith: Clean 95.00%; 10% noise 92.77%; 30% noise 91.66%; 50% noise 88.33%; ASR 7.79%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No in-depth internal mechanism analysis; authors note manual inspection and low answer-extraction miss rates for this model; semantic similarity confirmed by USE.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Drops in accuracy with increasing punctuation noise, but the drop is smaller than most other models (i.e., relatively robust). Some inconsistencies in answer extraction pipeline required manual corrections.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Among the best performing in the paper; 8B Llama family shows higher robustness and accuracy compared to several 7B models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6412.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6412.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama3.1-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B Llama-3.1 instruct-tuned model that achieved the highest overall accuracy and strong robustness to punctuation noise across evaluated datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3.1-8B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought (explicit step-by-step prompt).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and ASR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 82.25%; 10% noise 81.04%; 30% noise 78.84%; 50% noise 77.02%; ASR 12.53%. MultiArith: Clean 99.44%; 10% noise 94.44%; 30% noise 91.66%; 50% noise 83.88%; ASR 9.67%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Only high-level analyses: USE similarity (100%), answer extraction miss-rate (near zero), and manual inspection; no representational or mechanistic probing performed.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Performance degrades with more punctuation noise, notably a larger drop at 50% noise on MultiArith (83.88%); overall most robust among tested models but still vulnerable at high noise.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Top performer in this evaluation set, supporting that the Llama-3 family at 8B is comparatively more robust than several 7B models tested here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6412.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6412.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemma-2-2B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemma-2-2b-it</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2B-parameter open model evaluated on arithmetic word problems for sensitivity to punctuation noise; shows moderate performance with accuracy drops under attack.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-2-2b-it</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>2B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and ASR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 49.65%; 10% noise 45.10%; 30% noise 36.46%; 50% noise 35.63%; ASR 41.82%. MultiArith: Clean 89.44%; 10% noise 82.77%; 30% noise 78.88%; 50% noise 72.22%; ASR 19.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No internal mechanistic analysis beyond answer-extraction miss-rate checks and USE similarity measurement.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Large accuracy drops with increased punctuation noise, particularly on GSM8K; high ASR indicates susceptibility to attack.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Smaller (2B) model shows weaker robustness compared to larger (8B) Llama models; only limited comparisons available in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6412.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6412.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Zephyr-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Zephyr-7b-beta</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7B distilled/alignment-oriented model that performed poorest in this study, showing low clean accuracy and steep declines under punctuation-noise perturbations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Zephyr-7b-beta</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and ASR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 23.27%; 10% noise 18.04%; 30% noise 18.04%; 50% noise 10.08%; ASR 74.80%. MultiArith: Clean 37.22%; 10% noise 22.22%; 30% noise 16.11%; 50% noise 12.77%; ASR 77.10%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Manual inspection revealed high answer-extraction miss rates (noted as significant in the paper for Zephyr on MultiArith); no deeper analysis of internal representations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Very low baseline math ability and extreme sensitivity to punctuation noise; very high ASR and large performance collapse even at low noise levels.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Despite 7B size, performance is much worse than other 7B models, indicating that model family/training matters more than nominal size here.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6412.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6412.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Qwen2.5-1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Qwen2.5-1.5B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 1.5B instruct-tuned model evaluated for arithmetic problem solving robustness; shows moderate to strong performance on MultiArith and moderate sensitivity on GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-1.5B-Instruct</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.5B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and ASR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 61.10%; 10% noise 56.02%; 30% noise 52.69%; 50% noise 49.35%; ASR 31.59%. MultiArith: Clean 97.22%; 10% noise 94.44%; 30% noise 85.55%; 50% noise 83.88%; ASR 11.04%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Only surface analyses: USE similarity, answer-extraction miss rates low; no internal mechanistic probing.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Accuracy declines with increased punctuation noise, particularly on GSM8K; some robustness on MultiArith until higher noise levels.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Small model (1.5B) can still perform strongly on some arithmetic sets (MultiArith) but shows larger declines on the harder GSM8K.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6412.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6412.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepSeek-8B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8B distillation/REINFORCEMENT-LEARNING-from-human-feedback (RLHF)-oriented model (DeepSeek) evaluated for robustness to punctuation noise in arithmetic problems; shows strong baseline performance and competitive robustness on MultiArith.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DeepSeek-R1-Distill-Llama-8B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>transformer (large language model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K; MultiArith</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step word problems (arithmetic and logical)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-Shot Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (%) and ASR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: Clean 73.76%; 10% noise 73.76%; 30% noise 70.43%; 50% noise 67.24%; ASR 20.46%. MultiArith: Clean 93.88%; 10% noise 90.00%; 30% noise 92.77%; 50% noise 88.88%; ASR 8.28%.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>No mechanistic interpretability; authors limited analysis to similarity (USE) and manual extraction checks. DeepSeek-specific internal training details not analyzed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Some degradation with punctuation noise but comparatively smaller ASR on MultiArith; occasional larger drops on GSM8K at high noise levels.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Per-paper, DeepSeek (8B) shows robustness comparable to Llama family on some benchmarks, suggesting model training/optimization matters alongside size.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>Large language models are zero-shot reasoners. <em>(Rating: 2)</em></li>
                <li>Reasoning robustness of llms to adversarial typographical errors. <em>(Rating: 2)</em></li>
                <li>Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. <em>(Rating: 2)</em></li>
                <li>Adversarial math word problem generation. <em>(Rating: 2)</em></li>
                <li>Mathattack: Attacking large language models towards math solving ability. <em>(Rating: 2)</em></li>
                <li>AEDA: An easier data augmentation technique for text classification. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6412",
    "paper_id": "paper-275515398",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "Mistral-7B",
            "name_full": "Mistral-7B-Instruct-v0.2",
            "brief_description": "A 7B-parameter instruct-tuned large language model evaluated in this paper for robustness to random punctuation noise on arithmetic word problems (GSM8K and MultiArith).",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "Mistral-7B-Instruct-v0.2",
            "model_family": "transformer (large language model)",
            "model_size": "7B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought (prompt: 'Think step by step... Ensure the final answer is indicated by ending with {The final answer is}').",
            "performance_metric": "accuracy (%) and Attack Success Rate (ASR)",
            "performance_value": "GSM8K: Clean 42.07%; 10% noise 41.62%; 30% noise 37.75%; 50% noise 36.39%; ASR 39.69%. MultiArith: Clean 73.88%; 10% noise 72.77%; 30% noise 71.11%; 50% noise 65.55%; ASR 23.66%.",
            "internal_analysis": "Limited: authors computed semantic similarity (Universal Sentence Encoder) showing perturbed inputs score 100% similarity to originals and performed manual inspection and answer-extraction miss-rate analysis; no probing (attention/activation/logit) or representational analysis reported.",
            "failure_modes": "Performance degrades as random punctuation noise increases; notable declines on GSM8K (harder set). Noisy punctuation appears to break the model's reasoning flow despite preserved semantics.",
            "scaling_trend": "Within paper, 7B Mistral is less robust than similarly sized Mathstral and less robust than 8B Llama models; no broad scaling law analyzed.",
            "uuid": "e6412.0",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Mathstral-7B",
            "name_full": "Mathstral-7b-v0.1",
            "brief_description": "A 7B-parameter model oriented toward mathematical capability, evaluated for robustness to punctuation-noise attacks on arithmetic word problems.",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "Mathstral-7b-v0.1",
            "model_family": "transformer (large language model)",
            "model_size": "7B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought (same prompt as other models).",
            "performance_metric": "accuracy (%) and ASR",
            "performance_value": "GSM8K: Clean 77.63%; 10% noise 75.51%; 30% noise 71.34%; 50% noise 70.65%; ASR 19.81%. MultiArith: Clean 96.11%; 10% noise 92.77%; 30% noise 86.11%; 50% noise 87.22%; ASR 9.47%.",
            "internal_analysis": "Only semantic-similarity check (USE = 100%) and manual answer-extraction checks; no internal mechanistic probing reported.",
            "failure_modes": "Accuracy degrades with higher punctuation noise, but Mathstral shows more resilience than the baseline Mistral (same family), suggesting improvements tied to mathematical specialization rather than noise-specific defenses.",
            "scaling_trend": "Shows that model variants with stronger math-oriented training (Mathstral vs Mistral at same size) are more robust; no multi-size scaling curve provided.",
            "uuid": "e6412.1",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama3-8B",
            "name_full": "Llama-3-8B-Instruct",
            "brief_description": "An 8B Llama-3 instruct-tuned model evaluated for arithmetic problem solving robustness under punctuation-noise perturbations.",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "Llama-3-8B-Instruct",
            "model_family": "transformer (large language model)",
            "model_size": "8B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought (explicit step-by-step prompt).",
            "performance_metric": "accuracy (%) and ASR",
            "performance_value": "GSM8K: Clean 75.43%; 10% noise 73.31%; 30% noise 73.08%; 50% noise 72.17%; ASR 11.73%. MultiArith: Clean 95.00%; 10% noise 92.77%; 30% noise 91.66%; 50% noise 88.33%; ASR 7.79%.",
            "internal_analysis": "No in-depth internal mechanism analysis; authors note manual inspection and low answer-extraction miss rates for this model; semantic similarity confirmed by USE.",
            "failure_modes": "Drops in accuracy with increasing punctuation noise, but the drop is smaller than most other models (i.e., relatively robust). Some inconsistencies in answer extraction pipeline required manual corrections.",
            "scaling_trend": "Among the best performing in the paper; 8B Llama family shows higher robustness and accuracy compared to several 7B models.",
            "uuid": "e6412.2",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Llama3.1-8B",
            "name_full": "Llama-3.1-8B-Instruct",
            "brief_description": "An 8B Llama-3.1 instruct-tuned model that achieved the highest overall accuracy and strong robustness to punctuation noise across evaluated datasets.",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "Llama-3.1-8B-Instruct",
            "model_family": "transformer (large language model)",
            "model_size": "8B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought (explicit step-by-step prompt).",
            "performance_metric": "accuracy (%) and ASR",
            "performance_value": "GSM8K: Clean 82.25%; 10% noise 81.04%; 30% noise 78.84%; 50% noise 77.02%; ASR 12.53%. MultiArith: Clean 99.44%; 10% noise 94.44%; 30% noise 91.66%; 50% noise 83.88%; ASR 9.67%.",
            "internal_analysis": "Only high-level analyses: USE similarity (100%), answer extraction miss-rate (near zero), and manual inspection; no representational or mechanistic probing performed.",
            "failure_modes": "Performance degrades with more punctuation noise, notably a larger drop at 50% noise on MultiArith (83.88%); overall most robust among tested models but still vulnerable at high noise.",
            "scaling_trend": "Top performer in this evaluation set, supporting that the Llama-3 family at 8B is comparatively more robust than several 7B models tested here.",
            "uuid": "e6412.3",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Gemma-2-2B",
            "name_full": "Gemma-2-2b-it",
            "brief_description": "A 2B-parameter open model evaluated on arithmetic word problems for sensitivity to punctuation noise; shows moderate performance with accuracy drops under attack.",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "Gemma-2-2b-it",
            "model_family": "transformer (large language model)",
            "model_size": "2B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought",
            "performance_metric": "accuracy (%) and ASR",
            "performance_value": "GSM8K: Clean 49.65%; 10% noise 45.10%; 30% noise 36.46%; 50% noise 35.63%; ASR 41.82%. MultiArith: Clean 89.44%; 10% noise 82.77%; 30% noise 78.88%; 50% noise 72.22%; ASR 19.45%.",
            "internal_analysis": "No internal mechanistic analysis beyond answer-extraction miss-rate checks and USE similarity measurement.",
            "failure_modes": "Large accuracy drops with increased punctuation noise, particularly on GSM8K; high ASR indicates susceptibility to attack.",
            "scaling_trend": "Smaller (2B) model shows weaker robustness compared to larger (8B) Llama models; only limited comparisons available in paper.",
            "uuid": "e6412.4",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Zephyr-7B",
            "name_full": "Zephyr-7b-beta",
            "brief_description": "A 7B distilled/alignment-oriented model that performed poorest in this study, showing low clean accuracy and steep declines under punctuation-noise perturbations.",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "Zephyr-7b-beta",
            "model_family": "transformer (large language model)",
            "model_size": "7B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought",
            "performance_metric": "accuracy (%) and ASR",
            "performance_value": "GSM8K: Clean 23.27%; 10% noise 18.04%; 30% noise 18.04%; 50% noise 10.08%; ASR 74.80%. MultiArith: Clean 37.22%; 10% noise 22.22%; 30% noise 16.11%; 50% noise 12.77%; ASR 77.10%.",
            "internal_analysis": "Manual inspection revealed high answer-extraction miss rates (noted as significant in the paper for Zephyr on MultiArith); no deeper analysis of internal representations.",
            "failure_modes": "Very low baseline math ability and extreme sensitivity to punctuation noise; very high ASR and large performance collapse even at low noise levels.",
            "scaling_trend": "Despite 7B size, performance is much worse than other 7B models, indicating that model family/training matters more than nominal size here.",
            "uuid": "e6412.5",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Qwen2.5-1.5B",
            "name_full": "Qwen2.5-1.5B-Instruct",
            "brief_description": "A 1.5B instruct-tuned model evaluated for arithmetic problem solving robustness; shows moderate to strong performance on MultiArith and moderate sensitivity on GSM8K.",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-1.5B-Instruct",
            "model_family": "transformer (large language model)",
            "model_size": "1.5B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought",
            "performance_metric": "accuracy (%) and ASR",
            "performance_value": "GSM8K: Clean 61.10%; 10% noise 56.02%; 30% noise 52.69%; 50% noise 49.35%; ASR 31.59%. MultiArith: Clean 97.22%; 10% noise 94.44%; 30% noise 85.55%; 50% noise 83.88%; ASR 11.04%.",
            "internal_analysis": "Only surface analyses: USE similarity, answer-extraction miss rates low; no internal mechanistic probing.",
            "failure_modes": "Accuracy declines with increased punctuation noise, particularly on GSM8K; some robustness on MultiArith until higher noise levels.",
            "scaling_trend": "Small model (1.5B) can still perform strongly on some arithmetic sets (MultiArith) but shows larger declines on the harder GSM8K.",
            "uuid": "e6412.6",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "DeepSeek-8B",
            "name_full": "DeepSeek-R1-Distill-Llama-8B",
            "brief_description": "An 8B distillation/REINFORCEMENT-LEARNING-from-human-feedback (RLHF)-oriented model (DeepSeek) evaluated for robustness to punctuation noise in arithmetic problems; shows strong baseline performance and competitive robustness on MultiArith.",
            "citation_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
            "mention_or_use": "use",
            "model_name": "DeepSeek-R1-Distill-Llama-8B",
            "model_family": "transformer (large language model)",
            "model_size": "8B",
            "training_data_description": null,
            "benchmark_name": "GSM8K; MultiArith",
            "task_type": "multi-step word problems (arithmetic and logical)",
            "problem_format": "natural-language word problems",
            "difficulty_level": "grade-school / middle-school (GSM8K); mixed arithmetic complexities (MultiArith)",
            "prompting_method": "Zero-Shot Chain-of-Thought",
            "performance_metric": "accuracy (%) and ASR",
            "performance_value": "GSM8K: Clean 73.76%; 10% noise 73.76%; 30% noise 70.43%; 50% noise 67.24%; ASR 20.46%. MultiArith: Clean 93.88%; 10% noise 90.00%; 30% noise 92.77%; 50% noise 88.88%; ASR 8.28%.",
            "internal_analysis": "No mechanistic interpretability; authors limited analysis to similarity (USE) and manual extraction checks. DeepSeek-specific internal training details not analyzed in this paper.",
            "failure_modes": "Some degradation with punctuation noise but comparatively smaller ASR on MultiArith; occasional larger drops on GSM8K at high noise levels.",
            "scaling_trend": "Per-paper, DeepSeek (8B) shows robustness comparable to Llama family on some benchmarks, suggesting model training/optimization matters alongside size.",
            "uuid": "e6412.7",
            "source_info": {
                "paper_title": "ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Large language models are zero-shot reasoners.",
            "rating": 2,
            "sanitized_title": "large_language_models_are_zeroshot_reasoners"
        },
        {
            "paper_title": "Reasoning robustness of llms to adversarial typographical errors.",
            "rating": 2,
            "sanitized_title": "reasoning_robustness_of_llms_to_adversarial_typographical_errors"
        },
        {
            "paper_title": "Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers.",
            "rating": 2,
            "sanitized_title": "gsmplus_a_comprehensive_benchmark_for_evaluating_the_robustness_of_llms_as_mathematical_problem_solvers"
        },
        {
            "paper_title": "Adversarial math word problem generation.",
            "rating": 2,
            "sanitized_title": "adversarial_math_word_problem_generation"
        },
        {
            "paper_title": "Mathattack: Attacking large language models towards math solving ability.",
            "rating": 2,
            "sanitized_title": "mathattack_attacking_large_language_models_towards_math_solving_ability"
        },
        {
            "paper_title": "AEDA: An easier data augmentation technique for text classification.",
            "rating": 1,
            "sanitized_title": "aeda_an_easier_data_augmentation_technique_for_text_classification"
        }
    ],
    "cost": 0.013712499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving
4 Jul 2025</p>
<p>Zain Ul 
Shahzeb Qamar 
Lucie Flek 
Lamarr Institute for ML and AI
Germany</p>
<p>Akbar Karimi 
Lamarr Institute for ML and AI
Germany</p>
<p>Aachen International Center for Information Technology
University of Bonn
BonnGermany</p>
<p>ArithmAttack: Evaluating Robustness of LLMs to Noisy Context in Math Problem Solving
4 Jul 2025E88649023150741DC6579F45DBC37240arXiv:2501.08203v2[cs.CL]
While Large Language Models (LLMs) have shown impressive capabilities in math problemsolving tasks, their robustness to noisy inputs is not well-studied.We propose ArithmAttack to examine how robust the LLMs are when they encounter noisy prompts that contain extra noise in the form of punctuation marks.While being easy to implement, ArithmAttack does not cause any information loss since words are not added or deleted from the context.We evaluate the robustness of eight LLMs, including LLama3, Mistral, Mathstral, and DeepSeek on noisy GSM8K and MultiArith datasets.Our experiments suggest that all the studied models show vulnerability to such noise, with more noise leading to poorer performances.* Equal contriubtionReal PromptTiffany baked 8 brownies, but needed 17 total for her party.If she used 8 cups of flour on each one, how many cups of flour does she still need?</p>
<p>Introduction</p>
<p>As Large Language Models (LLMs) are improving in their ability to accurately process human language, their math problem-solving is also enhancing (Saraf et al., 2024;Agrawal et al., 2024;Wu et al., 2024).However, these sets of questions might require reasoning capabilities to be answered.While LLMs have been shown to have such capabilities to some extent (Imani et al., 2023), their robustness to adversarial inputs remains a challenge.For instance, these models can be vulnerable to simple replacement of words with their synonyms (Zhou et al., 2024) and even typographical errors can negatively impact their ability to reason (Gan et al., 2024).However, such attacks can semantically alter the samples by changing the current sample features to completely different ones (e.g.amoral -&gt; moral).</p>
<p>In this paper, we further investigate the math problem-solving robustness of LLMs to a different set of changes that take the form of noisy context containing a variety of punctuation marks.Given that none of the words are changed when new punctuation marks are inserted into the input text, the semantic similarity of the perturbed sentence remains unchanged.The key research question for this study is: How do LLMs respond to noise attacks consisting of random punctuation marks in the context of math problem-solving? Figure 1 shows an example of an LLM response under ArithmAttack, where the model behaves erratically when it sees a noisy context whereas it answers the question in the clean prompt correctly.</p>
<p>Inspired by the AEDA (An Easier Data Augmentation) method (Karimi et al., 2021), we propose ArithmAttack to assess the robustness of eight LLMs (i.e. two Llama models (Dubey et al., 2024), two Mistral models (Jiang et al., 2023), Zephyr (Tunstall et al., 2023), Gemma2 (Team et al., 2024), Qwen2.5 (Yang et al., 2024), and DeepSeek (Guo et al., 2025)) to noisy data.Similarly to AEDA, we introduce this noise by randomly inserting punctuation marks into the context of math problems from two math datasets, namely GSM8K (Cobbe et al., 2021) and MultiArith (Roy and Roth, 2015).We then evaluate how these models perform under different noise levels, with the noise affecting 10%, 30%, and 50% of the sentence length (based on the number of words).</p>
<p>Our contributions are twofold: 1) We propose ArithmAttack which produces noisy contexts containing random punctuation marks to assess the robustness of LLMs in math problem-solving.2) We evaluate eight LLMs, with parameter counts of 1.5B, 2B, 7B, and 8B on math datasets and observe that all the studied models show growing vulnerability to ArithmAttack as the amount of noise increases.</p>
<p>Related Work</p>
<p>Noise insertion has been shown to be effective in deteriorating the performance of encoder models in various tasks such as toxic text classification (Hosseini et al., 2017;Eger and Benz, 2020), sentiment analysis (Formento et al., 2021), and natural language inference (Formento et al., 2023).</p>
<p>In the context of math problem solving, Large Language Models (LLMs) have been shown to be vulnerable to a variety of changes in the input context, including typographical errors (Gan et al., 2024), word replacement (Zhou et al., 2024), gibberish or irrelevant context inclusion (Shi et al., 2023), and semantic perturbations (Zhu et al., 2023).Gan et al. (2024) propose an adversarial typo attack that breaks the reasoning process of LLMs.Instead of modifying characters, Zhou et al. (2024) propose a dataset, called RobustMath, where they replace words with their synonyms to evaluate the robustness of large language models.Similarly, Li et al. (2024) propose GSM-plus dataset, based on GSM8K, modified with a variety of mathematical perturbations such as distractor insertion and arithmetic variation.In the study by Zhu et al. (2023), the authors employ different types of textual attacks on prompts, including character, word, sentence, and semantic attacks.In contrast, Xie et al. (2024) propose to modify the numeric values in the questions using abstract syntax trees resulting in examples that fool the LLMs.</p>
<p>While the literature mainly concentrates on modifying the lexical or semantic content of the prompts, we aim to keep the contextual information intact and instead focus on the model behavior changes in reasoning when encountering punctuation noise.In addition, an advantage of our method is that it is extremely straightforward to implement, and as we show in the results section, it is also effective in degrading the performance of LLMs in math problem-solving.</p>
<p>Experiments</p>
<p>To carry out our experiments, we use two wellknown math datasets and eight LLMs.</p>
<p>Datasets</p>
<p>GSM8K (Cobbe et al., 2021) contains 8.5K highquality, linguistically diverse grade school math word problems.The test set contains 1.32k data points on which we do our experiments.This dataset provides a variety of arithmetic and logical questions typical of middle school education, making it ideal for testing comprehension and problemsolving capabilities of LLMs under noisy conditions.</p>
<p>MultiArith (Roy and Roth, 2015) offers a broad examination of language model performance across multiple arithmetic problem types and complexities.The test set contains 180 data points on which we do our experiments.It serves as a crucial benchmark for understanding how contextual noise impacts the model's ability to parse and solve mathematical questions.</p>
<p>Models</p>
<p>To study a variety of language models and at the same time observe our computational budget, we opted for eight widely utilized LLMs that have been trained by different companies.These models are Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Mathstral-7b-v0.1 (Jiang et al., 2023), Llama-3-8B-Instruct and Llama-3.1-8B-Instruct(Dubey et al., 2024), Gemma-2-2b-it (Team et al., 2024), Zephyr-7b-beta (Tunstall et al., 2023), Qwen2.5-1.5B-Instruct(Yang et al., 2024), and DeepSeek-R1-Distill-Llama-8B (Guo et al., 2025).Throughout this paper, we will refer to these models as Mistral, Mathstral, Llama3, Llama3.1,Gemma2, Zephyr, Qwen2.5, and DeepSeek respectively.</p>
<p>Methodology</p>
<p>To obtain the responses from LLMs, we use the Zero-Shot CoT (Kojima et al., 2022) prompting, using the following prompt:</p>
<p>Prompt 1 Think step by step through the following problem and clearly show each step of your reasoning.Ensure the final answer is indicated by ending with {The final answer is}.</p>
<p>Noisy Dataset Creation</p>
<p>Once satisfactory results were achieved with clean datasets, we proceeded to test the models on noisy data.For the introduction of noise, we follow a similar approach to Karimi et al. (2021), by altering the hyperparameters in the logic.In their study, they insert the punctuation marks by randomly choosing a number between 1 and one-third of the length of the sequence which indicates how many insertions will be carried out.But in our case, instead of randomly choosing the number of insertions, we fix it to be 10%, 30%, and 50% of the total length of the sentence but still choose random positions to insert the noise.We employed six types of punctuation marks: {".", ',', '!', '?', ';', ':'}.</p>
<p>ASR and Similarity Calculation</p>
<p>We evaluate the models with their performance accuracy against noisy input and Attack Success Rate (ASR).ASR (Wang et al., 2021) measures how effective an adversarial attack is on a model.Specifically, it looks at how often the model's predictions are changed incorrectly after the adversarial attack.In this study, the average ASR has been taken for every model with 10%, 30% and 50% noisy dataset's responses with the help of Formula 1:
ASR = (x,y)D I [f (A(x))  = y] (x,y)D I <a href="1">f (x) = y</a>
In other words, ASR is the ratio of changed answers after attack to previously correct answers produced by the LLM.</p>
<p>We also calculate the similarity of the perturbed samples to the original ones.Similarity represents the average semantic similarity between two contexts.Given that our method does not alter the words in the sentence, the resulting samples after applying ArithmAttack are scored 100 percent similar to the original samples using Universal Sentence Encoder (Cer et al., 2018) as the scorer.This indicates that our noise insertion attack does not impose any semantic shifts on the input text.</p>
<p>Results and Analysis</p>
<p>As shown in Tables 1 and 2, Llama3.1 outperforms other models across both datasets in the majority of the cases.It achieves the highest accuracies in both clean and noise-affected settings (except in 30% and 50% noisy data of the MultiArith dataset where DeepSeek in the former and both Llama3 and DeepSeek in the latter have higher accuracies).This makes it the most reliable model for handling mathematical problems under noisy input conditions.However, in terms of ASR score, Llama3 has the lowest score and Llama3.1 with a slightly higher one, indicating that Llama models are more robust to noise than other studied models with the exception of DeepSeek only in MultiArith dataset showing comparable robustness.In addition, the Mathstral model compared to Mistral exhibits more robustness which can be attributed to its higher mathematical understanding.In contrast, Zephyr was the lowest-performing model, exhibiting low clean accuracy and suffering a significant decline in performance as noise was introduced.Its high ASR score makes it unsuitable for tasks involving noisy data, reflecting poor robustness.</p>
<p>Figure 2 shows the relationship between the model's accuracy and the noise present in the datasets.For both datasets, as the percentage of noise in the data increases, the accuracy decreases.This indicates that these models are not robust against noise in the data.This also provides a future direction for improving these models and making them more robust to noise.</p>
<p>Across all models except for Zephyr, the impact of noise was more pronounced in the GSM8K dataset than in MultiArith, with a larger drop in accuracy as the noise levels increased (Figure 3).In manual inspection, we found out that the GSM8K dataset was more difficult to solve than the Multi-Arith dataset.This suggests that the models may struggle more with noise in math datasets with more difficulty.Answer Extraction Accuracy To evaluate the accuracy of the models, we developed a script to extract answers from the LLM responses.The extraction process underwent multiple iterations, as it needed to accurately extract the answer and compare it with the ground truth.However, even with the final prompt, we observed a couple of inconsistencies in the answer extraction.Therefore, we went through outputs manually to estimate the miss rate (i.e. the rate with which the correct answer is not extracted).In manual inspection, we evaluated the entire responses for the MultiArith dataset and the first 100 responses for the GSM8K dataset from all the models except for the DeepSeek model.For this model (due to time and labor constraints), we evaluated the first 50 samples from each dataset.Table 3 shows that the miss rate is minimal for most of the models.In the cases of Mistral (for GSM8K) and Zephyr (for MultiArith), the miss rates can be significant.While this can be an indication of lower ability in following instructions in these models, considering the gap in the performance and ASR scores, this does not affect the observed trends.</p>
<p>Conclusions and Future Work</p>
<p>We evaluated how well different language models handle mathematical problem-solving tasks in both clean and noisy conditions.Our results indicate that all studied models can be vulnerable to extra noise with varying degrees, with Llama models being the highest-performing and the most robust model in the majority of the experiments.In addition, comparing the two models of Mathstral and Mistral from the same family, the one with mathematical knowledge exhibited more robustness to noise.Lastly, the findings revealed that more complex datasets such as GSM8K can become more difficult to understand as they become noisier.Future research can include datasets beyond GSM8K and MultiArith as well as other reasoning tasks such as logical and causal reasoning, which could provide deeper insights into the models' robustness in different scenarios.Further experimentation with different types of noise could also help enhance our understanding of the latent vulnerabilities in LLMs.</p>
<p>Finally, explaining why ArithmAttack can break the reasoning flow of LLMs would be another valuable follow-up to this work.</p>
<p>Limitations</p>
<p>To make the questions noisy, we have opted for one type of noise which is irregular use of punctuation marks.While some of the other noise types such as spelling and typographical errors have been studied in the literature (mentioned in related work), there are other types such as grammatical errors, wrong abbreviations, and acronyms that we have not explored.In addition, to observe our computational budget, we have utilized only two math datasets and eight LLMs.For a more comprehensive experimentation, one can experiment with other available math datasets and a larger number of LLMs.</p>
<p>?Figure 1 :
1
Figure 1: Noisy context breaks the LLM's capability to give the right answer.</p>
<p>Figure 2 :Figure 3 :
23
Figure 2: Accuracy of the studied models on different levels of noise for GSM8K (left) and MultiArith (right) datasets.Llama models show the highest robustness as well as performance.</p>
<p>Table 1 :
1
Results for GSM8K dataset (numbers are in percentages).The performance for all models drops under ArithmAttack.Llama3.1 has the top performance under all levels of noise.
ModelsClean Acc (%)Punctuation PercentageASR103050Mistral42.07 41.62 37.75 36.39 39.69Mathstral 77.63 75.51 71.34 70.65 19.81Llama375.43 73.31 73.08 72.17 11.73Llama3.1 82.25 81.04 78.84 77.02 12.53Gemma249.65 45.10 36.46 35.63 41.82Zephyr23.27 18.04 18.04 10.08 74.80Qwen2.561.10 56.02 52.69 49.35 31.59DeepSeek 73.76 73.76 70.43 67.24 20.46</p>
<p>Table 2 :
2
Results for MultiArith dataset (numbers are in percentages).The performance for all models drops under ArithmAttack.Llama3 has the lowest drop, making it more robust than others.
ModelsClean Acc (%)Punctuation PercentageASR103050Mistral73.88 72.77 71.11 65.55 23.66Mathstral 96.11 92.77 86.11 87.22 9.47Llama395.00 92.77 91.66 88.33 7.79Llama3.1 99.44 94.44 91.66 83.88 9.67Gemma289.44 82.77 78.88 72.22 19.45Zephyr37.22 22.22 16.11 12.77 77.10Qwen2.597.22 94.44 85.55 83.88 11.04DeepSeek 93.88 90.00 92.77 88.88 8.28</p>
<p>Table 3 :
3
Miss rate of the models in answer extraction
Miss Rate (%)ModelGSM8K MultiArithMistral9.01.1Mathstral0.01.1Llama31.01.1Llama3.10.00.0Gemma23.02.2Zephyr2.012.8Qwen2.51.00.5DeepSeek4.00.0
AcknowledgmentsThis work was partially supported by the AISafety Project, funded by the BMBF under the grant proposal 05D23PD1, and by the state of North Rhine-Westphalia as part of the Lamarr Institute for Machine Learning and Artificial Intelligence.We would also like to thank the reviewers for their invaluable comments, which helped strengthen the quality of this work.
Give me a hint: Can llms take a hint to solve math problems?. Pratham Vansh Agrawal, Amitoj Singla, Shivank Singh Miglani, Ayush Garg, Mangal, The 4th Workshop on Mathematical Reasoning and AI at NeurIPS'24. 2024</p>
<p>Universal sentence encoder for english. Daniel Cer, Yinfei Yang, Sheng-Yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, Proceedings of the 2018 conference on empirical methods in natural language processing: system demonstrations. the 2018 conference on empirical methods in natural language processing: system demonstrations2018</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>From hero to zroe: A benchmark of low-level adversarial attacks. Steffen Eger, Yannik Benz, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing. the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing2020</p>
<p>Using punctuation as an adversarial attack on deep learning-based nlp systems: An empirical study. Brian Formento, Chuan-Sheng Foo, Anh Luu, See Kiong Tuan, Ng, Findings of the association for computational linguistics: EACL 2023. 2023</p>
<p>Special symbol attacks on nlp systems. Brian Formento, See-Kiong Ng, Chuan-Sheng Foo, 2021 International Joint Conference on Neural Networks (IJCNN). IEEE2021</p>
<p>Reasoning robustness of llms to adversarial typographical errors. Esther Gan, Yiran Zhao, Liying Cheng, Yancan Mao, Anirudh Goyal, Kenji Kawaguchi, Min-Yen Kan, Michael Shieh, arXiv:2411.053452024arXiv preprint</p>
<p>Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, arXiv:2501.12948Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. 2025arXiv preprint</p>
<p>Deceiving google's perspective api built for detecting toxic comments. Hossein Hosseini, Sreeram Kannan, Baosen Zhang, Radha Poovendran, arXiv:1702.081382017arXiv preprint</p>
<p>Mathprompter: Mathematical reasoning using large language models. Shima Imani, Liang Du, Harsh Shrivastava, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics20235Industry Track)</p>
<p>Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego De Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, arXiv:2310.06825Mistral 7b. Renard Llio, Marie-Anne Lavaud, Pierre Lachaux, Teven Stock, Thibaut Le Scao, Thomas Lavril, Timothe Wang, William El Lacroix, Sayed, 2023Preprint</p>
<p>AEDA: An easier data augmentation technique for text classification. Akbar Karimi, Leonardo Rossi, Andrea Prati, 10.18653/v1/2021.findings-emnlp.234Findings of the Association for Computational Linguistics: EMNLP 2021. Punta Cana, Dominican RepublicAssociation for Computational Linguistics2021</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in neural information processing systems. 202235</p>
<p>Gsm-plus: A comprehensive benchmark for evaluating the robustness of llms as mathematical problem solvers. Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational Linguistics20241</p>
<p>Solving general arithmetic word problems. Subhro Roy, Dan Roth, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language Processing2015</p>
<p>Towards robust automated math problem solving: a survey of statistical and deep learning approaches. Evolutionary Intelligence. Amrutesh Saraf, Pooja Kamat, Shilpa Gite, Satish Kumar, and Ketan Kotecha. 2024</p>
<p>Large language models can be easily distracted by irrelevant context. Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schrli, Denny Zhou, International Conference on Machine Learning. PMLR2023</p>
<p>Gemma 2: Improving open language models at a practical size. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Lonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram, arXiv:2408.00118arXiv:2310.16944Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clmentine Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf2024. 2023PreprintZephyr: Direct distillation of lm alignment</p>
<p>Adversarial glue: A multi-task benchmark for robustness evaluation of language models. Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, Bo Li, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>Mathchat: Converse to tackle challenging math problems with llm agents. Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang, ICLR 2024 Workshop on Large Language Model (LLM) Agents. 2024</p>
<p>Adversarial math word problem generation. Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra, Findings of the Association for Computational Linguistics: EMNLP 2024. 2024</p>
<p>An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, arXiv:2412.15115Qwen2. 5 technical report. 2024arXiv preprint</p>
<p>Mathattack: Attacking large language models towards math solving ability. Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv eprints. 20232306</p>            </div>
        </div>

    </div>
</body>
</html>
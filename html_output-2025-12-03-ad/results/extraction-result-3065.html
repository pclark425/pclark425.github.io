<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3065 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3065</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3065</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-267500209</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.03667v2.pdf" target="_blank">Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Recently, increasing attention has been focused on improving the ability of Large Language Models (LLMs) to perform complex reasoning. Advanced methods, such as Chain-of-Thought (CoT) and its variants, are found to enhance their reasoning skills by designing suitable prompts or breaking down complex problems into more manageable sub-problems. However, little concentration has been put on exploring the reasoning process, \textit{i.e.}, we discovered that most methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning (IR). This can make LLMs difficult to solve IR tasks, which are often encountered in the real world. To address this issue, we propose a Direct-Indirect Reasoning (DIR) method, which considers DR and IR as multiple parallel reasoning paths that are merged to derive the final answer. We stimulate LLMs to implement IR by crafting prompt templates incorporating the principles of contrapositive and contradiction. These templates trigger LLMs to assume the negation of the conclusion as true, combine it with the premises to deduce a conclusion, and utilize the logical equivalence of the contrapositive to enhance their comprehension of the rules used in the reasoning process. Our DIR method is simple yet effective and can be straightforwardly integrated with existing variants of CoT methods. Experimental results on four datasets related to logical reasoning and mathematic proof demonstrate that our DIR method, when combined with various baseline methods, significantly outperforms all the original methods.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3065.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3065.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A commercial chat-oriented LLM used in the paper as a foundation model to evaluate Direct vs Indirect reasoning strategies (DIR) on logical reasoning and math proof benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A chat-oriented large language model (OpenAI family) used as a frozen foundation model for prompting experiments; evaluated with sampling temperatures (0.7 typical, 0.1 for low-temperature intermediate thought generation in some methods).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct Reasoning (DR)', 'Indirect Reasoning (IR: contradiction, contrapositive)', 'Chain-of-Thought (CoT: few-shot and zero-shot)', 'Self-Consistency (SC)', 'CR (thought-search tree / Correct-Storage variant)', 'MulAD (multi-agent debate)', 'DIR (Direct-Indirect Reasoning: combination of DR and IR via voting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DR: standard forward derivation from premises to conclusion using CoT prompts. IR: prompt templates instruct model to assume negation of conclusion and derive contradictions or use contraposition of rules; implemented both zero-shot (instruction-only) and few-shot (examples with intermediate IR steps). SC, CR, MulAD are applied as in prior work, and DIR runs equal numbers of DR and IR samples and aggregates via majority voting (voting among M sampled reasoning chains).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse: paper explicitly induces a second, distinct reasoning style (IR) in addition to standard DR and then merges results; diversity measured and reported using a 'DP' metric (diversity of reasoning processes). Prompt engineering (IR templates with contradiction/contrapositive examples) is the mechanism that produces diverse reasoning chains.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA, ProofWriter (5-hop subset), ProofNet, ProofMath</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks for logical reasoning (LogiQA), synthetic natural-language logical proofs (ProofWriter), undergraduate-level mathematical proof formalization (ProofNet), and a new 100-problem high-school mathematic proof dataset created by authors (ProofMath). Tasks evaluate both final-answer correctness and reasoning process quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Key reported results for GPT-3.5-turbo (as reported in the paper): On ProofNet (AP = accuracy of reasoning processes, DP = diversity): CoT AP=56.0%, DP=0.56; SC AP=63.0%, DP=0.65; SC-DIR AP=77.0%, DP=0.88; CR AP=57.0%, DP=0.59; CR-DIR AP=72.0%, DP=0.77; MulAD AP=63.0%, DP=0.64; MulAD-DIR AP=78.0%, DP=0.84. Additional task-level reported improvements: On LogiQA, combining SC with DIR improved accuracy by 7.2% (GPT-3.5-turbo). On ProofMath, DIR gave a 15.0% AP improvement with GPT-3.5-turbo vs DR baseline. In controlled DR-vs-IR comparison on subsets (Table 5), IR outperformed DR by +33.4% on ProofWriter_S and +25.5% on ProofMath_S when using GPT-3.5-turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper compares DR-only variants (CoT, SC, CR, MulAD) to the same methods augmented by DIR (i.e., SC-DIR, CR-DIR, MulAD-DIR). DIR variants consistently outperform their DR-only counterparts across datasets and models; for GPT-3.5-turbo on ProofNet SC->SC-DIR: +14 percentage points AP (63% -> 77%) with a concomitant DP increase (0.65 -> 0.88). Ablation experiments removing contrapositive or contradiction instructions degrade IR performance (Figure 5). Zero-shot IR instructions (no few-shot examples) also improve performance vs DR-only zero-shot prompting (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Incorporating IR (contrapositive + contradiction) via tailored prompts and merging DR and IR (DIR) yields substantially higher answer and proof-process accuracy and greater diversity of reasoning chains; DIR often solves examples that DR fails on and frequently reaches conclusions in fewer steps. IR alone dramatically outperforms DR on curated IR-friendly subsets (e.g., +33.4% on ProofWriter_S for GPT-3.5-turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Performance gains are model-dependent: the paper notes GPT-3.5-turbo implements IR more effectively and stably than Gemini-pro (limitation). No dataset-wide case is reported where DR outperforms DIR; however, ablations show that removing either contrapositive or contradiction components harms IR, indicating both submethods are necessary for observed gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3065.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3065.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-pro</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-pro</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A foundation LLM (Google AI family) evaluated in the paper to compare DR, IR, and DIR variants on logical reasoning and mathematical proof tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini-pro</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A commercial large language model (evaluated via Google AI Studio) used as a frozen foundation model in prompting experiments; sampling temperature reported as 0.9 for generation and 0.3 for low-temperature intermediate thought generation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct Reasoning (DR)', 'Indirect Reasoning (IR: contradiction, contrapositive)', 'Chain-of-Thought (CoT)', 'Self-Consistency (SC)', 'CR', 'MulAD', 'DIR (DR+IR voting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Same method implementations as for other models: IR via prompt templates instructing assumption of negated conclusion and search for contradiction/contrapositive uses; DIR aggregates DR and IR outputs by majority voting over sampled reasoning chains. Baseline CoT variants and multi-agent MulAD similarly applied.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse prompting induced by IR templates increases the set of reasoning styles beyond the model's default DR tendency, measured by the DP metric; DIR explicitly samples equal numbers of DR and IR chains to diversify outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA, ProofWriter (5-hop subset), ProofNet, ProofMath</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>As above: benchmarks for natural-language logical reasoning and mathematical proof tasks; evaluation includes answer accuracy and process accuracy/diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Selected reported numbers (ProofNet AP / DP): CoT AP=47.0%, DP=0.47; SC AP=59.0%, DP=0.62; SC-DIR AP=71.0%, DP=0.76; CR AP=54.0%, DP=0.56; CR-DIR AP=68.0%, DP=0.71; MulAD AP=58.0%, DP=0.59; MulAD-DIR AP=71.0%, DP=0.73. On ProofWriter the paper reports DIR yields an improvement >10.0% specifically highlighted for Gemini-pro. In other comparisons, IR (alone) vs DR showed large gains on curated subsets, though the single largest per-model subset gains were reported for GPT-3.5-turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Across methods, Gemini-pro benefits from DIR (SC-DIR / CR-DIR / MulAD-DIR exceed their DR-only counterparts by large margins). The paper highlights that gains vary by foundation model and are smaller or less stable for Gemini-pro than for GPT-3.5-turbo in some cases (limitation discussion). Ablations indicate removal of contrapositive or contradiction reduces IR effectiveness for Gemini-pro as well.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DIR improves reasoning accuracy and increases reasoning-process diversity for Gemini-pro, with reported >10% improvement on ProofWriter for some DIR variants; however, model-specific variability means Gemini-pro may be less stable than GPT-3.5-turbo in implementing IR.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Authors observe that Gemini-pro is less effective/stable at implementing IR than GPT-3.5-turbo; they caution that base model choice influences DIR gains. No explicit dataset where DIR reduced accuracy relative to DR-only is reported, but variability across models is a limitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3065.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3065.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-weight foundation LLM (70B parameters) used in experiments to evaluate the efficacy of combining Direct and Indirect reasoning via prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An open foundation model from the Llama-3 family (reported as 70B parameters); used as a frozen LLM in prompting experiments with sampling temperature set to 0.6 (and 0.3 for low-temperature intermediate thought generation).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Direct Reasoning (DR)', 'Indirect Reasoning (IR: contradiction, contrapositive)', 'Chain-of-Thought (CoT)', 'Self-Consistency (SC)', 'CR', 'MulAD', 'DIR (DR+IR voting)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Implemented identical prompting frameworks: DR via CoT prompting; IR via templates instructing assumption of negated conclusion and search for contradiction or use of contrapositives; DIR merges equal DR/IR sampled chains by voting.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse: IR prompts produce reasoning chains qualitatively different from DR chains, increasing the DP metric. DIR explicitly enforces multiple styles (DR + IR) per question to increase exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LogiQA, ProofWriter (5-hop subset), ProofNet, ProofMath</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same benchmark suite as above — logical reasoning and mathematic proof tasks measuring answer and reasoning-process accuracy and process diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Selected ProofNet (AP / DP) numbers reported: CoT AP=51.0%, DP=0.51; SC AP=62.0%, DP=0.64; SC-DIR AP=73.0%, DP=0.75; CR AP=58.0%, DP=0.60; CR-DIR AP=67.0%, DP=0.71; MulAD AP=63.0%, DP=0.65; MulAD-DIR AP=75.0%, DP=0.77. Across ProofNet and ProofMath, DIR variants consistently raise AP and DP relative to DR-only variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Llama-3-70B shows consistent improvements when DR methods are augmented with DIR: SC-DIR, CR-DIR, MulAD-DIR outperform their DR-only counterparts by large margins on ProofNet AP and DP. The magnitude of gains is comparable to other evaluated models though sometimes slightly lower than GPT-3.5-turbo's gains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DIR increases reasoning-process accuracy and diversity for Llama-3-70B; IR encourages alternative solution paths and resolves problems where DR failed in several case studies. DIR's improvements appear robust across model families including Llama-3-70B.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No specific negative results reported for Llama-3-70B where DIR worsened performance, but authors note variability across foundation models and that improvements' magnitude depends on model capability and sampling/temperature settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3065",
    "paper_id": "paper-267500209",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "A commercial chat-oriented LLM used in the paper as a foundation model to evaluate Direct vs Indirect reasoning strategies (DIR) on logical reasoning and math proof benchmarks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5-turbo",
            "model_description": "A chat-oriented large language model (OpenAI family) used as a frozen foundation model for prompting experiments; evaluated with sampling temperatures (0.7 typical, 0.1 for low-temperature intermediate thought generation in some methods).",
            "model_size": null,
            "reasoning_methods": [
                "Direct Reasoning (DR)",
                "Indirect Reasoning (IR: contradiction, contrapositive)",
                "Chain-of-Thought (CoT: few-shot and zero-shot)",
                "Self-Consistency (SC)",
                "CR (thought-search tree / Correct-Storage variant)",
                "MulAD (multi-agent debate)",
                "DIR (Direct-Indirect Reasoning: combination of DR and IR via voting)"
            ],
            "reasoning_methods_description": "DR: standard forward derivation from premises to conclusion using CoT prompts. IR: prompt templates instruct model to assume negation of conclusion and derive contradictions or use contraposition of rules; implemented both zero-shot (instruction-only) and few-shot (examples with intermediate IR steps). SC, CR, MulAD are applied as in prior work, and DIR runs equal numbers of DR and IR samples and aggregates via majority voting (voting among M sampled reasoning chains).",
            "diversity_of_methods": "Diverse: paper explicitly induces a second, distinct reasoning style (IR) in addition to standard DR and then merges results; diversity measured and reported using a 'DP' metric (diversity of reasoning processes). Prompt engineering (IR templates with contradiction/contrapositive examples) is the mechanism that produces diverse reasoning chains.",
            "reasoning_task_name": "LogiQA, ProofWriter (5-hop subset), ProofNet, ProofMath",
            "reasoning_task_description": "Benchmarks for logical reasoning (LogiQA), synthetic natural-language logical proofs (ProofWriter), undergraduate-level mathematical proof formalization (ProofNet), and a new 100-problem high-school mathematic proof dataset created by authors (ProofMath). Tasks evaluate both final-answer correctness and reasoning process quality.",
            "performance_by_method": "Key reported results for GPT-3.5-turbo (as reported in the paper): On ProofNet (AP = accuracy of reasoning processes, DP = diversity): CoT AP=56.0%, DP=0.56; SC AP=63.0%, DP=0.65; SC-DIR AP=77.0%, DP=0.88; CR AP=57.0%, DP=0.59; CR-DIR AP=72.0%, DP=0.77; MulAD AP=63.0%, DP=0.64; MulAD-DIR AP=78.0%, DP=0.84. Additional task-level reported improvements: On LogiQA, combining SC with DIR improved accuracy by 7.2% (GPT-3.5-turbo). On ProofMath, DIR gave a 15.0% AP improvement with GPT-3.5-turbo vs DR baseline. In controlled DR-vs-IR comparison on subsets (Table 5), IR outperformed DR by +33.4% on ProofWriter_S and +25.5% on ProofMath_S when using GPT-3.5-turbo.",
            "comparison_of_methods": "The paper compares DR-only variants (CoT, SC, CR, MulAD) to the same methods augmented by DIR (i.e., SC-DIR, CR-DIR, MulAD-DIR). DIR variants consistently outperform their DR-only counterparts across datasets and models; for GPT-3.5-turbo on ProofNet SC-&gt;SC-DIR: +14 percentage points AP (63% -&gt; 77%) with a concomitant DP increase (0.65 -&gt; 0.88). Ablation experiments removing contrapositive or contradiction instructions degrade IR performance (Figure 5). Zero-shot IR instructions (no few-shot examples) also improve performance vs DR-only zero-shot prompting (Table 6).",
            "key_findings": "Incorporating IR (contrapositive + contradiction) via tailored prompts and merging DR and IR (DIR) yields substantially higher answer and proof-process accuracy and greater diversity of reasoning chains; DIR often solves examples that DR fails on and frequently reaches conclusions in fewer steps. IR alone dramatically outperforms DR on curated IR-friendly subsets (e.g., +33.4% on ProofWriter_S for GPT-3.5-turbo).",
            "counter_examples_or_negative_results": "Performance gains are model-dependent: the paper notes GPT-3.5-turbo implements IR more effectively and stably than Gemini-pro (limitation). No dataset-wide case is reported where DR outperforms DIR; however, ablations show that removing either contrapositive or contradiction components harms IR, indicating both submethods are necessary for observed gains.",
            "uuid": "e3065.0",
            "source_info": {
                "paper_title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Gemini-pro",
            "name_full": "Gemini-pro",
            "brief_description": "A foundation LLM (Google AI family) evaluated in the paper to compare DR, IR, and DIR variants on logical reasoning and mathematical proof tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini-pro",
            "model_description": "A commercial large language model (evaluated via Google AI Studio) used as a frozen foundation model in prompting experiments; sampling temperature reported as 0.9 for generation and 0.3 for low-temperature intermediate thought generation.",
            "model_size": null,
            "reasoning_methods": [
                "Direct Reasoning (DR)",
                "Indirect Reasoning (IR: contradiction, contrapositive)",
                "Chain-of-Thought (CoT)",
                "Self-Consistency (SC)",
                "CR",
                "MulAD",
                "DIR (DR+IR voting)"
            ],
            "reasoning_methods_description": "Same method implementations as for other models: IR via prompt templates instructing assumption of negated conclusion and search for contradiction/contrapositive uses; DIR aggregates DR and IR outputs by majority voting over sampled reasoning chains. Baseline CoT variants and multi-agent MulAD similarly applied.",
            "diversity_of_methods": "Diverse prompting induced by IR templates increases the set of reasoning styles beyond the model's default DR tendency, measured by the DP metric; DIR explicitly samples equal numbers of DR and IR chains to diversify outputs.",
            "reasoning_task_name": "LogiQA, ProofWriter (5-hop subset), ProofNet, ProofMath",
            "reasoning_task_description": "As above: benchmarks for natural-language logical reasoning and mathematical proof tasks; evaluation includes answer accuracy and process accuracy/diversity.",
            "performance_by_method": "Selected reported numbers (ProofNet AP / DP): CoT AP=47.0%, DP=0.47; SC AP=59.0%, DP=0.62; SC-DIR AP=71.0%, DP=0.76; CR AP=54.0%, DP=0.56; CR-DIR AP=68.0%, DP=0.71; MulAD AP=58.0%, DP=0.59; MulAD-DIR AP=71.0%, DP=0.73. On ProofWriter the paper reports DIR yields an improvement &gt;10.0% specifically highlighted for Gemini-pro. In other comparisons, IR (alone) vs DR showed large gains on curated subsets, though the single largest per-model subset gains were reported for GPT-3.5-turbo.",
            "comparison_of_methods": "Across methods, Gemini-pro benefits from DIR (SC-DIR / CR-DIR / MulAD-DIR exceed their DR-only counterparts by large margins). The paper highlights that gains vary by foundation model and are smaller or less stable for Gemini-pro than for GPT-3.5-turbo in some cases (limitation discussion). Ablations indicate removal of contrapositive or contradiction reduces IR effectiveness for Gemini-pro as well.",
            "key_findings": "DIR improves reasoning accuracy and increases reasoning-process diversity for Gemini-pro, with reported &gt;10% improvement on ProofWriter for some DIR variants; however, model-specific variability means Gemini-pro may be less stable than GPT-3.5-turbo in implementing IR.",
            "counter_examples_or_negative_results": "Authors observe that Gemini-pro is less effective/stable at implementing IR than GPT-3.5-turbo; they caution that base model choice influences DIR gains. No explicit dataset where DIR reduced accuracy relative to DR-only is reported, but variability across models is a limitation.",
            "uuid": "e3065.1",
            "source_info": {
                "paper_title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama-3-70B",
            "name_full": "Llama-3-70B",
            "brief_description": "An open-weight foundation LLM (70B parameters) used in experiments to evaluate the efficacy of combining Direct and Indirect reasoning via prompting.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-3-70B",
            "model_description": "An open foundation model from the Llama-3 family (reported as 70B parameters); used as a frozen LLM in prompting experiments with sampling temperature set to 0.6 (and 0.3 for low-temperature intermediate thought generation).",
            "model_size": "70B",
            "reasoning_methods": [
                "Direct Reasoning (DR)",
                "Indirect Reasoning (IR: contradiction, contrapositive)",
                "Chain-of-Thought (CoT)",
                "Self-Consistency (SC)",
                "CR",
                "MulAD",
                "DIR (DR+IR voting)"
            ],
            "reasoning_methods_description": "Implemented identical prompting frameworks: DR via CoT prompting; IR via templates instructing assumption of negated conclusion and search for contradiction or use of contrapositives; DIR merges equal DR/IR sampled chains by voting.",
            "diversity_of_methods": "Diverse: IR prompts produce reasoning chains qualitatively different from DR chains, increasing the DP metric. DIR explicitly enforces multiple styles (DR + IR) per question to increase exploration.",
            "reasoning_task_name": "LogiQA, ProofWriter (5-hop subset), ProofNet, ProofMath",
            "reasoning_task_description": "Same benchmark suite as above — logical reasoning and mathematic proof tasks measuring answer and reasoning-process accuracy and process diversity.",
            "performance_by_method": "Selected ProofNet (AP / DP) numbers reported: CoT AP=51.0%, DP=0.51; SC AP=62.0%, DP=0.64; SC-DIR AP=73.0%, DP=0.75; CR AP=58.0%, DP=0.60; CR-DIR AP=67.0%, DP=0.71; MulAD AP=63.0%, DP=0.65; MulAD-DIR AP=75.0%, DP=0.77. Across ProofNet and ProofMath, DIR variants consistently raise AP and DP relative to DR-only variants.",
            "comparison_of_methods": "Llama-3-70B shows consistent improvements when DR methods are augmented with DIR: SC-DIR, CR-DIR, MulAD-DIR outperform their DR-only counterparts by large margins on ProofNet AP and DP. The magnitude of gains is comparable to other evaluated models though sometimes slightly lower than GPT-3.5-turbo's gains.",
            "key_findings": "DIR increases reasoning-process accuracy and diversity for Llama-3-70B; IR encourages alternative solution paths and resolves problems where DR failed in several case studies. DIR's improvements appear robust across model families including Llama-3-70B.",
            "counter_examples_or_negative_results": "No specific negative results reported for Llama-3-70B where DIR worsened performance, but authors note variability across foundation models and that improvements' magnitude depends on model capability and sampling/temperature settings.",
            "uuid": "e3065.2",
            "source_info": {
                "paper_title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2,
            "sanitized_title": "selfconsistency_improves_chain_of_thought_reasoning_in_language_models"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 2,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 1,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        }
    ],
    "cost": 0.014079,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning
27 Jan 2025</p>
<p>Yanfang Zhang 
Nanjing University of Science and Technology</p>
<p>Yiliu Sun 
Nanjing University of Science and Technology</p>
<p>Yibing Zhan 
JD Explore Academy</p>
<p>Dapeng Tao 
Yunnan University</p>
<p>Dacheng Tao 
Nanyang Technological University</p>
<p>Chen Gong chen.gong@sjtu.edu.cn 
Shanghai Jiao Tong University</p>
<p>Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning
27 Jan 20250030A145BA8DF5CC51691A5D9968D3E7arXiv:2402.03667v2[cs.CL]
Recently, increasing attention has been focused on improving the ability of Large Language Models (LLMs) to perform complex reasoning.Advanced methods, such as Chain-of-Thought (CoT) and its variants, are found to enhance their reasoning skills by designing suitable prompts or breaking down complex problems into more manageable sub-problems.However, little concentration has been put on exploring the reasoning process, i.e., we discovered that most methods resort to Direct Reasoning (DR) and disregard Indirect Reasoning (IR).This can make LLMs difficult to solve IR tasks, which are often encountered in the real world.To address this issue, we propose a Direct-Indirect Reasoning (DIR) method, which considers DR and IR as multiple parallel reasoning paths that are merged to derive the final answer.We stimulate LLMs to implement IR by crafting prompt templates incorporating the principles of contrapositive and contradiction.These templates trigger LLMs to assume the negation of the conclusion as true, combine it with the premises to deduce a conclusion, and utilize the logical equivalence of the contrapositive to enhance their comprehension of the rules used in the reasoning process.Our DIR method is simple yet effective and can be straightforwardly integrated with existing variants of CoT methods.Experimental results on four datasets related to logical reasoning and mathematic proof demonstrate that our DIR method, when combined with various baseline methods, significantly outperforms all the original methods.</p>
<p>Introduction</p>
<p>Recently, pre-trained Large Language Models (LLMs) (Wang et al., 2022a;Chowdhery et al., 2023;Dubey et al., 2024) have shown great success in various tasks related to language comprehension (Touvron et al., 2023;Nam et al., 2024), content generation (Agossah et al., 2023;Liu et al., 2023;Dai et al., 2024), and logical reasoning (Ko-jima et al., 2022;Wei et al., 2022;Dubey et al., 2024) due to their remarkable ability to infer from the context in zero-shot or few-shot way.To enhance the reasoning ability of LLMs, CoT (Wei et al., 2022) encourages LLMs to explain their reasoning processes by appending some intermediate steps required to reach the answer in the prompt.Besides CoT, there are other approaches using prompts to help elicit the reasoning ability of LLMs to better solve the reasoning problems, such as Self-Consistency (Wang et al., 2022a) and Least-to-Most (Zhou et al., 2022).</p>
<p>Note that most of the above mentioned methods simply perform Direct Reasoning (DR), which involves constructing logical chains from premises to the final result.However, many problems can hardly be proven or reasoned via DR.Therefore, when encountering a problem that is difficult to reach a conclusion through DR, a question arises: whether it is possible to solve the problem by performing other reasoning strategies, such as Indirect Reasoning (IR) that is logically equivalent to DR? IR, including contrapositive and contradiction in this paper, is logically equivalent to DR, and they have been well defined in the science of logic (Jourdan and Yevdokimov, 2016).</p>
<p>To stimulate LLMs with IR, we devise special prompts for contradiction and contrapositive by designing instructions or examples within the intermediate reasoning process as shown in Figure 1.As a result, the proposed approach can induce effective IR and enhance the ability of LLMs to tackle complex reasoning tasks.Note that, IR is embarrassingly simple and general that can be directly combined with DR.Therefore, we propose Direct-Indirect Reasoning (termed "DIR") to further improve the reasoning ability of LLMs.Without loss of generality, we merge the results of DR and IR using the most common voting methods.Moreover, DIR can be seamlessly integrated with existing variants of CoT based on any foundation model.</p>
<p>Given the equation a + |a| = 0, we can substitute the value of |a| based on its definition: If a &lt; 0, then |a| = -a.We get a + (-a) = 0.This equality holds true.If a ≥ 0, then |a| = a.we get a + a = 0, which simplifies to 2a = 0. From this, we can conclude that a = 0.In both cases, a is either equal to 0 or negative.However, the equation a + |a| = 0 does not hold true for a = 0. we can conclude that a must be strictly less than zero (a &lt; 0) based on the given equation.</p>
<p> Model Input</p>
<p>Model Output</p>
<p>If a+|a|=0 , try to prove that a&lt;0.step 1: List the conditions and questions in the original proposition.step 2: Merge the conditions listed in step 1 into one.Define it as wj.step 3: Let us think it step by step.Please consider all possibilities.If the intersection between wj (defined in step 2) and the negation of the question is not empty at least in one possibility, the original proposition is false.Otherwise, the original proposition is true.</p>
<p>Step 1: Condition: a + |a| = 0 Question: Prove that a &lt; 0 Step 2: Condition: wj = (a + |a| = 0)</p>
<p>Step 3: Consider all possible cases and evaluate their intersections with the condition wj.Case: a &lt; 0</p>
<p>The absolute value of a would-be equal to -a.Therefore, a + |a| = a + (-a) = 0.</p>
<p>This satisfies the condition wj = (a + |a| = 0).The intersection between wj and the negation of the question "a &lt; 0" is empty.Case: a = 0</p>
<p>The equation a + |a| = 0 becomes 0 + |0| = 0.This satisfies wj = (a + |a| = 0).However, the negation of the question intersects with the condition wj.Therefore, the intersection between wj and the negation of the question "a &lt; 0" is not empty in the case where a = 0. Hence, we can conclude that the proposition "If a + |a| = 0, then a &lt; 0" is false.</p>
<p>Model Input</p>
<p>Model Output</p>
<p>To determine the correctness of the conclusion "Bob is not rough", let's analyze the given facts and rules.Based on the given fact, there is no explicit information provided about whether Bob is rough or not.However, we know that Bob is red based on the fact "Bob is red".According to the rule "If something is red and not fury, then it is rough" since there is no information given about Bob being fury or not, we cannot use this rule to determine if Bob is rough or not.</p>
<p>&lt;~system&gt;Assuming you are a master of logical reasoning, you need to determine the correctness of the following conclusion.</system>Example 1 Facts: "Anne is blue", …… Rules: "Cold people are red ", …… Conclusion: "Anne is quiet".Answer: according to the fact "Anne is blue" and the rule "If someone is blue then they are quiet", so we can know that "Anne is quiet", the conclusion is true.…… Facts: "Bob is big", "Bob is not cold", …… Rules: "If something is red then it is big", …… Conclusion: "Bob is not rough".Answer:</p>
<p></p>
<p>Direct Reasoning Indirect Reasoning</p>
<p>Figure 1: Examples of IR using LLMs for some complex problems regarding mathematic proof and logical reasoning.Existing DR methods failed when dealing with these problems.In contrast, our method guides LLMs to use the logics of contrapositive and contradiction, resulting in accurate reasoning and successful deduction to correct answers.</p>
<p>To assess the effectiveness of our DIR method, we conducted extensive experiments on two popular tasks: logical reasoning and mathematic proof, by using various LLMs as foundation models.The results indicate that our proposed method is quite effective in inspiring LLMs to achieve IR.For example, DIR has shown a noteworthy improvement of over 10.0% in terms of the accuracy of reasoning processes of mathematic proof task.Additionally, in the logical reasoning task, it consistently outperforms various baselines and LLMs, particularly on the data that DR struggles with.The improvement is quite impressive, with a 33.4% increase in accuracy.In particular, experimental analyses have demonstrated that the utilization of IR can aid LLMs in resolving some tasks that are arduous to accomplish through the use of DR.This enriches the reasoning paths of LLMs and improves their overall reasoning ability.Our main contributions are summarized below:</p>
<p>• We introduce the IR strategy, including contrapositive and contradiction, into the reasoning process of LLMs.</p>
<p>• We devise a series of prompt templates that effectively stimulate LLMs to implement IR.</p>
<p>We further introduce the DIR method, which combines DR and IR to enhance the reasoning ability of LLMs.</p>
<p>• Experimental results indicate that our proposed DIR method can be effectively combined with the variants of CoT.These combined methods have shown significant performance improvement across four logical reasoning and mathematic proof benchmark datasets on three different baseline LLMs.Additionally, our method has demonstrated impressive performance in inspiring diverse reasoning chains and solving complex problems that can hardly be solved by DR.</p>
<p>Motivation and Problem Formulation</p>
<p>LLMs have shown strong ability to conduct logical reasoning in natural language.The aim of reasoning is to assess the answer A to a candidate conclusion or question Q, and also present the reasoning process P R from premises P which include fact set F and rule set R (Tafjord et al., 2021).</p>
<p>All the premises and conclusions are expressed in natural language.Figure 2 shows the general illustration of logical reasoning.Mathematic proving  problems are similar to logical reasoning.However, it is worth noting that it only gives fact set F and question Q, and the rule set R is usually set to prior knowledge, which means that we cannot know what rules to use in advance.</p>
<p>We notice that LLMs may encounter challenges with IR tasks, despite being proficient at DR, as illustrated in Figure 1.To further understand this phenomenon, we analyzed it by investigating whether LLMs tend to use DR in solving problems.To this end, we conducted a preliminary experiment, where 70 questions are randomly selected from each of the four datasets (i.e., ProofWriter, LogiQA, ProofNet, and ProofMath datasets).In these experiments, we prompt LLMs with "Let's think step by step" and calculate the proportion of DR and IR implemented by LLMs.</p>
<p>According to Figure 3, we see that LLMs rarely use IR on logical reasoning tasks, even when IR would be more appropriate.They still prefer to use DR to solve mathematic problems.Meanwhile, to our best knowledge, currently there are no relevant works explicitly performing IR.Therefore, we propose to stimulate LLMs to implement IR more effectively, which could improve their overall reasoning ability.</p>
<p>Methodology</p>
<p>In this section, we present a comprehensive overview of our DIR approach as shown in Figure 4. Our method begins with an introduction to the principles of IR, which include contradiction and contrapositive (Section 3.1).Then we outline a method for guiding LLMs in the application of IR by devising prompt templates that implement the reasoning process of contradiction and contrapositive (Section 3.2).Lastly, we provide a detailed description of the combination method for DR and IR in Section 3.3.</p>
<p>Contrapositive and Contradiction</p>
<p>In mathematics and some practical applications, there are circumstances where direct proof may not be feasible or effective.In such cases, the methods of indirect proof are often used to verify a statement.There are two popular methods for indirect proof, which are: contrapositive method and contradiction method.Next, we will explain these two methods in detail.</p>
<p>Contrapositive.It is based on the fact that an implication is equivalent to its contrapositive, namely:
p → q ⇔ ¬p ∨ q,
(1)
¬q → ¬p ⇔ ¬(¬q) ∨ ¬p ⇔ q ∨ ¬p. (2)
According to the commutative law, one can have:
p → q ⇔ ¬q → ¬p.(3)
Therefore, when we get a fact "If p, then q", we can also know that if ¬q then ¬p.</p>
<p>Contradiction.The world-renowned mathematician G. H. Hardy called proof-by-contradiction "one of a mathematician's finest weapons".Actually, this method has been widely used in mathematics, logic, and philosophy to establish the validity of various statements and arguments.Proof-bycontradiction involves the original statement and its negation.These two statements are opposites to each other, meaning that if the original statement is true, the negation of the original statement is false;</p>
<p>Multiple reasoning paths DR prompting</p>
<p>Facts : "Bob is big", "Bob is not cold", "Bob is quiet", "Bob is red".Rules : " Blue things are furry", " All rough things are cold".Conclusion : " Bob is not rough".# &lt;~Instruction&gt; First, take the negation of the conclusion/question and assume the negation is true; Next, use the negation and the premises to deduce its falsity until the result of that assumption is a contradiction.If necessary, consider the logical equivalence of the original rules and their contrapositive.</Instruction># &lt;~Example1&gt;Facts: and if the original statement is false, the negation of the original statement is true.Therefore, we consider a reasoning equivalence as:
¬(p → q) ⇔ ¬[(¬p) ∨ q] ⇔ p ∧ (¬q). (4)</p>
<p>Indirect Reasoning</p>
<p>In the principles of IR described in the previous section, we inspire LLMs to implement IR by designing appropriate prompt templates as shown in Figure 4. To implement contradiction, the entire reasoning process is involved.First, we take the negation of the conclusion and assume it to be true.Subsequently, we deduce the negation along with the premises until a conflict arises.Finally, we conclude that the negation of the conclusion is false, and therefore, the original conclusion must be true.</p>
<p>In addition, as depicted in Figure 2, certain rules are employed during the reasoning process.Based on the principle of contrapositive discussed earlier, we can deduce that the contrapositive of these rules and their original rules are logically equivalent.The contrapositive can assist LLMs in enhancing their comprehension of rules and their ability to apply them efficiently.For instance, when presented with the fact "Bob does not drive to work" and the rule "If the weather is fine, Bob drives to work", humans can apply the equivalence of contrapositive to deduce that the rule is equivalent to "If Bob does not drive to work, the weather is not fine".This allows them to conclude that "The weather is not fine" based on the rule.In the following, we present relevant instructions and examples with IR processes to achieve contradiction and contrapositive.</p>
<p>Zero-shot Scenario.We implement a contradiction by following instructions: "First, take the negation of the conclusion/question and assume the negation is true; Next, use the negation and the premises to deduce its falsity until the result of that assumption is a contradiction".Also for contrapositive, LLMs are prompted using "If necessary, consider the logical equivalence of the original rules and their contrapositive".</p>
<p>Few-shot Scenario.In addition to the above instructions, the examples with intermediate reasoning steps incorporating contradiction and contrapositive also facilitate LLMs to implement IR.To facilitate the effective implementation of IR for LLMs, we craft a set of prompt templates that incorporate the concepts of contradiction and contrapositive into the reasoning process (see Appendix E).</p>
<p>Direct-Indirect Reasoning</p>
<p>From the above description, it can be inferred that the proposed IR method can be directly combined with DR in existing methods, such as SC (Wang et al., 2022a), CR (Zhang et al., 2023) and Mu-lAD (Du et al., 2024).Therefore, we propose a DIR method by combining IR and DR.This will enrich the reasoning paths to solve complex problems.It can be seen that IR is a straightforward approach that involves negating the conclusion and treating the negation as a premise.That is to say, IR does not impose any additional constraints on the reasoning process.Therefore, the proposed DIR method can be easily incorporated into existing CoT variants to improve reasoning proficiency.</p>
<p>Various techniques exist for aggregating the results of multipath reasoning.One straightforward way is to select the most commonly occurring results, while another involves utilizing the log probability of the output of LLMs.In this paper, we utilize voting to select the most frequently occurring results.We sample M candidate reasoning chains from LLMs and {A i } M i=1 is the set of answers generated from these chains.Let A = A s |A| s=1 be the set of all possible answers for the question.We then select the answer from A with the highest probability P ( A s ) and P ( A s ) can be formulated as below:
P ( A s ) = 1 M M i=1 I(A i = A s ),(5)
where I(•) is the indicator function.</p>
<p>Empirical Study 4.1 Setup</p>
<p>To evaluate the effectiveness of our proposed DIR method, we apply our method to four typical reasoning datasets, namely ProofWriter, LogiQA, ProofNet, and ProofMath.Here we choose the popular CoT-based prompt methods to implement both DR and IR, which are:</p>
<p>• CoT (Wei et al., 2022).It guides LLMs to reason by utilizing a few examples containing reasoning process.</p>
<p>• SC (Wang et al., 2022a).It samples multiple reasoning chains and selects the final result by voting.</p>
<p>• CR (Zhang et al., 2023).It is similar to ToT (Yao et al., 2024) and solves problems using a thought search tree.However, CR stores all the historically correct intermediate thoughts.</p>
<p>• MulAD (Du et al., 2024).It employs multiple agents, each powered by an LLM, to propose and debate their individual reasoning processes over multiple rounds to arrive at a common final answer.In subsequent experiments, the proposed DIR algorithms are configured to have the same number of reasoning candidates sampled from LLMs as their corresponding baseline algorithms.This configuration can make the computational complexity to be consistent among DIR and baseline methods in the experiment.Additionally, the number of reasoning candidates sampled from LLMs for DR and IR in DIR is set to be equal.Specifically, for LogiQA and ProofWriter datasets, we set the number of reasoning candidates to 16, and for ProofNet and Proof-Math datasets, it is set to 4. For more detailed parameter settings in the implementation please refer to Appendix B and the designed prompt templates for the experiment are available in Appendix E.</p>
<p>Evaluation Metrics</p>
<p>The evaluation of reasoning performance for a method includes the correctness investigation on the answer and the reasoning process.Therefore, here we use three metrics, namely accuracy of answer (AA), accuracy of reasoning processes (AP), and diversity of reasoning processes (DP).We use these three indicators to comprehensively evaluate the quality of reasoning or proof.The definitions of AA, AP, and DP are:
AA = AN N , AP = P N N , DP = 1 N d i , (6)
where N is the number of examples in the test set; AN and P N are the numbers of examples with correct answer prediction and correct reasoning process prediction, respectively; and d i is the number of correct reasoning methods for the i-th example.</p>
<p>Main Results</p>
<p>LogiQA.The LogiQA (Liu et al., 2021)   choices.To assess the reasoning ability of LLMs, we conduct a thorough examination of 179 of these questions with minimal dependence on external sources.This allows us to more rigorously evaluate the logical reasoning capabilities of these models (Sun et al., 2023).AA is used to evaluate the accuracy of reasoning in this task.</p>
<p>The results of reasoning on LogiQA are presented in Table 1.The results indicate that integrating SC, CR and MulAD with DIR leads to a consistent improvement in accuracy.In the GPT-3.5-turboscenario, DIR outperforms SC by 7.2%.This improvement is mainly due to the fact that IR can offer more diverse reasoning paths.Detailed case descriptions can be found in Appendix C.</p>
<p>ProofWriter.ProofWriter (Tafjord et al., 2021) dataset is a widely used benchmark dataset regarding logical reasoning.We utilize the OWA subset of ProofWriter, which is categorized into five subsets based on the number of hops required in the reasoning.For our purposes, we choose the 5-hop subset, which consists of questions requiring 0, 1, 2, 3, and 5 hops.Following the guidelines outlined in (Kazemi et al., 2023), we randomly select 200 questions from this subset for testing.</p>
<p>The results in Table 2 suggest that the implementation of DIR can significantly enhance the reasoning performance of all baseline methods, with a performance improvement more than 10.0% on Gemini-pro.Through an analysis of the reasoning process, it is discovered that IR enhances the reasoning ability of LLMs by prompting them to explore more reasoning chains.This is shown in detail in the Case Study in Section 4.4.</p>
<p>ProofNet.The ProofNet dataset (Azerbayev et al., 2023) is a collection of problems used for assessing the ability of automated systems to formalize and verify mathematic proofs at an undergraduate level.To evaluate the accuracy of LLMs in  mathematic proof task, we use two metrics, namely AP and DP.The evaluation of the process is entrusted to undergraduate math majors.As a costefficient approach, we opt to randomly select 50 questions from ProofNet for testing purposes.</p>
<p>The findings shown in Table 3 prove that the DIR, when used in conjunction with SC, CR and MulAD, is better than the original methods, with a maximum improvement of 14.0%.At the same time, the findings disclosed by DP indicate that DIR successfully motivates LLMs to produce various reasoning chains, indicating its effectiveness.</p>
<p>ProofMath.As revealed by (Yang et al., 2024), the above ProofNet is publicly available on GitHub before the data of LLMs used in the experiment cutoff date.Therefore, there is a potential risk that LLMs are pre-trained with their standard proof.To obtain a more comprehensive and accurate evaluation, we create a new dataset called "ProofMath".This dataset contains 100 mathematic proof problems from junior and senior high schools.We make the dataset diverse in terms of problem difficulty (see Appendix B) so that we can comprehensively assess the reasoning ability of the DIR techniques.Similar to ProofNet, we employ both AP and DP metrics for evaluation purposes.The results displayed in Table 4 indicate that employing DIR instead of DR results in a 10.0% enhancement in terms of AP.It is worth noting that this enhancement rises to 15.0% in the presence of GPT-3.5-turbo.Furthermore, DP illustrates the positive influence of DIR in encouraging LLMs to explore more reasoning paths.</p>
<p>Discussion</p>
<p>IR can prompt LLMs to implement effective indirect reasonings.In the following experiments, we thoroughly evaluate whether our proposed IR method can prompt LLMs to perform effective IR in solving IR tasks.With this in mind, we carefully select 150 data items from the ProofWriter dataset termed "ProofWriter_S" and 35 data items from the ProofMath dataset termed "ProofMath_S" to showcase the advantages of IR.We use SC as a baseline method to perform DR and IR, with four reasoning candidates sampled from LLMs.To evaluate the ability of LLMs to implement effective IR, we select AP as the evaluation metric.Table 5 shows the performance comparison between DR and IR on these subsets.</p>
<p>Based on the results, it is evident that IR significantly outperforms DR counterparts across multiple LLMs.Specifically, IR showcases enhancements of 33.4% for ProofWriter and 25.5% for ProofMath when using GPT-3.5-turbo.Our analysis indicates that while DR can address certain 0-hop issues within ProofWriter_S, it fails to provide accurate reasoning for more difficult questions.In contrast, IR can solve problems of various levels of complexity by using contradiction and contrapositive techniques.</p>
<p>IR works for zero-shot prompts.We conduct a study to determine if IR can prompt LLMs to implement IR through zero-shot prompts.To achieve this, the examples from the prompt templates are removed and only the relevant instructions are utilized, as illustrated in Table 6.The results reveal that LLMs can be effectively stimulated to implement IR via zero-shot prompts, resulting in signifi-   cant improvements compared with DR.</p>
<p>The impact of contrapositive and contradiction.We conduct ablative experiments on ProofWriter_S to assess the impact of contrapositive and contradiction on IR.In the experiment, the instructions for contrapositive and contradiction of the template are respectively removed.We utilize SC as a baseline method to perform DR and IR, employing four reasoning candidates.The results, quantified by the AP metric, are presented in Figure 5.The results show that removing either contrapositive (termed "IR w/o CP") or contradiction (termed "IR w/o CD") disrupts the performance of IR.</p>
<p>Case Study.We analyze several cases to gain a better understanding of the reasoning process of IR.We find that using IR leads to more diverse reasoning paths as shown in Figure 6.In some multi-step reasoning scenarios, IR helps LLMs to reach the answer with fewer steps than DR in many circumstances, thereby decreasing the likelihood of making errors in the reasoning processes of LLMs as described in (Kazemi et al., 2023).More case studies are available in Appendix C.</p>
<p>Output of IR (by 2 steps)</p>
<p>Output of DR (by 5 steps)</p>
<p>To prove the conclusion "Harry is not green" by contradiction, we assume that Harry is green.(Step1:) According to the rule "If someone is not furry then they are not red" and the fact "Harry is red", we know that Harry is furry.(Step2:) But according to the rule "If someone is green then he is not young or he is not furry" and the fact "Harry is young", we can deduce that Harry is not furry.This is a contradiction, as we reached two contradictory conclusions about whether Harry is furry.Therefore, the assumption "Harry is green" is false and the original conclusion "Harry is not green" is true.</p>
<p>We can determine the correctness of the conclusion by analyzing the facts and rules provided.(Step1:) According to the fact "Harry is red" and the rule " All red people are kind", we can conclude that "Harry is kind".(Step2:) According to the rule "If someone is red and kind, then they are round", we can conclude that "Harry is round".(Step3:) According to the rule "All young, round people are blue", we can conclude that "Harry is blue".(Step4:) According to the rule "All blue, young people are furry", we can conclude that "Harry is furry".(Step5:) According to the rule "All furry, young people are not green", we can conclude that "Harry is not green".</p>
<p>Based on the facts and rules, the conclusion "Harry is not green" is true.</p>
<p>Facts: Anne is green, Anne is round, Harry is red, Harry is young.Rules: All furry, young people are not green.All young, round people are blue.If someone is red and kind then they are round.Green, kind people are round.If someone is green and round then they are kind.All red people are kind.If someone is not furry then they are not red.All blue, young people are furry.Conclusion: Harry is not green.</p>
<p>Question</p>
<p>Figure 6: IR uses fewer reasoning steps to reach a conclusion in logical reasoning.</p>
<p>Related Work</p>
<p>Reasoning ability, as a basic ability of LLMs, has received great attention recently due to its great importance.Despite the notable improvements made by CoT (Wei et al., 2022), LLMs are still struggling with the tasks that require complex or high-order multi-step reasoning, such as logical reasoning and mathematic proof.Therefore, intensive research efforts have been dedicated to addressing the aforementioned issues.Generally, they can be categorized as follows.</p>
<p>Fine-tuning-based methods.These methods aim to improve the reasoning ability of LLMs through supervised fine-tuning.Usually, LLMs are fine-tuned by the samples which require manual labeling of reasoning processes, such as (Ouyang et al., 2022;Wang et al., 2022b).However, it can be labor-intensive due to the costly labeling of complex reasoning processes.The works of (Shridhar et al., 2022;Zelikman et al., 2022) first used LLMs to generate reasoning processes, but only the samples with correct results are selected for fine-tuning LLMs to reduce the labeling cost.Additionally, fine-tuned LLMs on specific tasks can suffer from the problem of "catastrophic forgetting", which means that the original knowledge inherited by the pre-trained LLMs will be lost and thus the ability to generalize to downstream tasks will be weakened.To this end, Cheng et al. (2023) trained a prompt retriever using the output scores of LLMs.When fine-tuning, LLMs are frozen just as a data labeler which effectively reduces the impact on LLMs.</p>
<p>Tool-based methods.Tool-based methods propose to utilize external tools to augment the capabilities of LLMs in accomplishing complex tasks (Qin et al., 2023;Schick et al., 2024).More-over, Jin et al. (2024); Yang et al. (2023) augment LLMs with external real-time knowledge or domain-specific information through specific tools.Additionally, Retrieval-Augmented Generation (RAG) related methods (Gao et al., 2023;Ma et al., 2023;Peng et al., 2024) have received a lot of attention recently, and these methods improve the reasoning ability of LLMs by incorporating external knowledge.</p>
<p>CoT-based methods.CoT-based methods use prompts to help elicit the reasoning ability of LLMs to better solve the reasoning problems (Kojima et al., 2022;Wei et al., 2022;Zhang et al., 2022), which is also closely related to our paper.The common CoT methods contain zero-shot CoT (Kojima et al., 2022) and few-shot CoT (Wei et al., 2022).Meanwhile, recent researches show that different variants of CoT can improve the reasoning ability of LLMs.For instance, the method in (Zhang et al., 2022) enhances the performance by optimally selecting examples in the prompt.Additionally, external information can be introduced to increase the credibility of results, as proposed in (He et al., 2022).Some different approaches are proposed in (Besta et al., 2024;Drozdov et al., 2022;Yao et al., 2024) to decompose complex problems into smaller subproblems to enhance the reasoning ability of LLMs.Furthermore, recent developments indicate that multi-agent debates (Wang et al., 2024;Du et al., 2024) can improve reasoning skills in LLMs.</p>
<p>However, as mentioned in the introduction, the previous researches mainly focus on DR, which will meet difficulties in some complex reasoning procedures.Therefore, our work aims to explore IR combined with DR methods to further improve the reasoning ability of LLMs.</p>
<p>Conclusion</p>
<p>In this paper, we propose a DIR method to enhance the reasoning power of LLMs by tailored prompts.IR can well compensate for problems which are not directly derivable from known facts and rules.We validate the effectiveness of the DIR method in logical reasoning and mathematic proof tasks, and the results well confirm the usefulness of the proposed IR strategy.Considering that the IR in this paper only involves the simple thoughts of contrapositive and contradiction, in the future, we can explore the possibility of integrating other more complex logical laws to make LLMs further improve their reasoning skills.</p>
<p>Limitations</p>
<p>Our approach has yielded consistent performance improvements across various LLMs.However, the extent of these improvements varies depending on the specific LLM.Upon analyzing the experimental outcomes, we have observed that GPT-3.5-turboperforms IR more effectively and with greater stability than Gemini-pro in most cases.These findings suggest that the foundational model has an impact on the effectiveness of IR in LLMs.</p>
<p>A Illustration of Mathematic Proof</p>
<p>Figure 7 shows the general illustration of mathematic proof.Similar to the logical reasoning task mentioned earlier, the goal of a mathematic proof is to prove a conclusion based on given facts and rules.However, in mathematic proof task, the rules are often not explicitly provided but are instead treated as implicit knowledge generally embedded in LLMs.</p>
<p>B Implementation Details</p>
<p>Below are some detailed supplementary introductions that outline the relevant implementation details discussed in the paper.</p>
<p>B.1 Parameter Settings</p>
<p>In the experiment, we choose GPT-3.5-turbo,Gemini-pro and Llama-3-70B as the basic LLMs.Among them, the temperature for sampling the output of GPT-3.5-turbo is set to 0.7 as (Wang et al., 2022a), the temperature for sampling the output of Gemini-pro is set to 0.9 in Google AI Studio1 and it is set to 0.6 for Llama-3-70B in the official Llama repo2 .It is pertinent to note that the CR generates intermediate thoughts at a lower temperature when compared with the sampling temperature.To this end, the temperature is set to 0.1, 0.3 and 0.3 for GPT-3.5-turbo,Gemini-pro and Llama-3-70B, respectively.For MulAD, the number of agents is set to 3 and the number of rounds is set to 2. The prompting templates for multiple rounds of debate align with those detailed in (Du et al., 2024).</p>
<p>B.2 Details of ProofMath</p>
<p>We develop ProofMath, a collection of 100 mathematic proof questions tailored for junior and senior high school students.These questions and proofs</p>
<p>We can assume the negative conclusion is true: Harry is red.According to the assumption Harry is red and the rule If something is red then it is quiet, we can deduce that Harry is quiet.But this conflicts with the premise Harry is not quiet.So the negative conclusion is False and the conclusion is True.</p>
<p>Output of IR:</p>
<p>Output of DR: Question:</p>
<p>Let's think step by step: From premise 6, we know Harry is kind.From premise 11, we can deduce that if Harry is kind and not red, then Harry is green.However, we don't have any information about Harry's color, so we can't conclude anything about Harry's color yet.There is no direct or indirect information that contradicts or confirms the conclusion "Harry is not red."Therefore, based on the premises, we can't determine the truth value of the conclusion.</p>
<p>Facts: "Charlie is red.","Charlie is white.","Fiona is green.","Gary is red.","Gary is white.","Harry is kind.","Harry is not quiet.".Rules: "If Gary is quiet and Gary is red then Gary is cold.","If something is quiet and not white then it is nice.","If something is green and kind then it is nice.","If something is kind and not red then it is green.","Red, kind things are green.","If something is red then it is quiet.","If Charlie is nice then Charlie is cold.","Cold, quiet things are kind.".Question: Harry is not red.are presented in natural language.The questions have been drawn from exercises in junior and senior high school mathematics textbooks as well as on a specific website 3 .The selection of questions aims to encompass a wide array of topics, spanning varying degrees of complexity as shown in Table 7.The questions are carefully selected by three graduate students with robust educational backgrounds in science and engineering, each possessing considerable mathematic expertise.The proofs for these questions are adapted from the textbooks or the website and subsequently verified by three experts.</p>
<p>C Additional Experimental Results</p>
<p>Further analysis of the reasoning process of IR and DR.To better understand the impact of IR on the reasoning ability of LLMs, we conduct an in-depth analysis of the outcomes of IR and DR.We observe that the performance enhancement of DR is predominantly ascribed to two key aspects.Firstly, IR is effective in solving certain challenging problems that DR struggles with.Secondly, IR contributes to diversifying the reasoning process.</p>
<p>3 http://1v1.zuoyebang.com/Assume, for the sake of contradiction, that:</p>
<p>² + ² + ² &lt; 3. Use the identity:</p>
<p>2 +  2 +  2 =  +  +  2 − 2  +  +  , And substituting  +  +  = 3, we get:
𝑎 2 + 𝑏 2 + 𝑐 2 = 𝑎 + 𝑏 + 𝑐 2 − 6 &lt; 3 2 − 6 &lt; 3.
Hence, the assumption leads to a contradiction .Therefore, it must be the case that: ² + ² + ² ≥ 3.Thus, the proof is complete.</p>
<p>Output of IR:</p>
<p>First, we know that ( +  + )² = ² + ² + ² + 2( +  + ).Given that  +  +  = 3, we can rewrite ( +  + )²  ( +  + )² = ² + ² + ² + 2(3) = ² + ² + ² + 6.Since ( +  + )² ≥ 0, we have:</p>
<p>² + ² + ² + 6 ≥ 0 ² + ² + ² ≥ −6 ² + ² + ² + 3 ≥ −3 ² + ² + ² ≥ 3 Therefore, we have proven that if  +  +  = 3, then ² + ² + ² ≥ 3.</p>
<p>Output of DR:</p>
<p>If , ,  are positive real numbers such that  +  +  = 3 , try to prove that ² + ² + ² ≥ 3.</p>
<p>Question:</p>
<p>Figure 9: An example in ProofMath demonstrates that using IR leads to accurate reasoning while using DR leads to failure.</p>
<p>The role of IR in these two scenarios is explicated through the following case studies.</p>
<p>In Figure 8, it can be observed that the LLM lacks the ability to deduce the veracity or falsity of the issue, so it can only be judged as unknown.Through IR approach, which involves affirming the facts and rules while negating the conclusion, a contradiction can be derived.Consequently, it can be demonstrated that the negation of the conclusion is false, thereby validating the truth of the conclusion.Furthermore, Figure 9 depicts a situation where DR yields incorrect proof, while IR is successful in solving a mathematic problem.</p>
<p>In Figure 10, it is evident that IR can offer a wider variety of reasoning paths.The analysis of the reasoning process reveals that multiple reasoning paths sampled from the LLM (i.e., "Output of DR 1" and "Output of DR 2") yield similar reasoning paths, with discrepancies primarily in the selection of facts and rules during the reasoning process.However, the reasoning paths resulting from IR, which commence with the negation of the conclusion and identify contradictions with given facts and rules, differ significantly from the process of DR.IR can stimulate LLMs to generate more diverse reasoning paths.As stated in (Evans, 2010), when a problem requires more deliberate thinking and analysis, the diversity of reasoning paths that can lead to the answer also increases.This ultimately helps to enhance the performance of LLMs.</p>
<p>D Description of Manual Validation</p>
<p>In our experimental study, there is a requirement for human experts to confirm specific results for reliability.To ensure the integrity and consistency of the confirmation process, we have provided comprehensive training for the evaluators and established uniform evaluation criteria.The details are as follows:</p>
<p>• We have chosen 5 graduate students as evaluation experts to validate the experimental results.This group consists of 3 graduate students majoring in computer science and 2 graduate students majoring in mathematics.These experts possess strong logical reasoning abilities and mathematic expertise.We have also provided them with training in logical reasoning and mathematical proof tasks, along with a variety of cases to help them understand the requirements for these tasks.</p>
<p>• To ensure the objectivity and fairness of the evaluation of the reasoning process, standardized evaluation criteria have been formulated for various tasks.A correct reasoning process does not contain any incorrect or omitted steps.Furthermore, for ProofWriter dataset, it is stipulated that the reasoning process should exclusively rely on the given facts and rules, with-out incorporating external or common knowledge.</p>
<p>• Throughout the evaluation process, we maintain the concealment of the methods employed in each reasoning process from the evaluation expert.All data undergo random scrambling before being transmitted to the evaluation expert.Each evaluation expert conducts their assessment independently, and the final evaluation outcome is determined by the majority consensus of the assessments provided by multiple experts.</p>
<p>E Prompt Templates</p>
<p>Figures 12, 13, 14, 15, 16, 17, 18, and   According to the conditions, we can infer that: The administration department is above the Planning department but below the Human Resources Department.This is not consistent with choice D. This is consistent with option A. Therefore, the correct answer is A.</p>
<p>Output of IR:</p>
<p>Based on the following information: The finance department is one floor below the planning department.The administration department is on the floor above the Planning department but below the Human Resources Department.The human resources department and the Sales department are located on different floors.We can draw the following conclusions: 1) The Finance Department is at the next level of the Planning Department, and the order is Finance Department, Planning Department, 2) The Administration department is located on the floor above the Planning Department but below the Human Resources Department.The new order can be introduced as:</p>
<p>Figure 3 :
3
Figure 3: The proportion of IR and DR implementations deployed by LLMs on various datasets.</p>
<p>Figure 4 :
4
Figure4: Framework of our proposed DIR method which incorporates both DR and IR to enable multipath reasoning.Our approach involves the IR component, which stimulates LLMs through a set of crafted prompt templates that incorporate the thought of contradiction and contrapositive.Ultimately, the outcomes of both DR and IR are consolidated through majority voting.</p>
<p>Figure 5 :
5
Figure 5: The impact of contrapositive and contradiction on IR.</p>
<p>Figure 7 :
7
Figure 7: The illustration of some key notions in mathematic proof.</p>
<p>Figure 8 :
8
Figure 8: An example in ProofWriter demonstrates that using IR leads to accurate reasoning while using DR leads to failure.</p>
<p>Figure 11 illustrates an additional instance in which DIR enriches the diversity of the reasoning paths of LLMs.</p>
<p>Figure 10 :
10
Figure 10: An example in ProofWriter demonstrates that DIR enriches the diversity of the reasoning paths of LLMs.</p>
<p>Figure 11 :
11
Figure 11: An example in LogiQA demonstrates that DIR enriches the diversity of the reasoning paths of LLMs.</p>
<p>Table 2 :
2
Reasoning accuracy of various methods on ProofWriter dataset.</p>
<p>Table 3 :
3
Reasoning accuracy of various methods on ProofNet dataset.
GPT-3.5-turbo Gemini-pro Llama-3-70BAPDPAPDPAPDPCoT56.0% 0.56 47.0% 0.47 51.0% 0.51SC63.0% 0.65 59.0% 0.62 62.0% 0.64SC-DIR77.0% 0.88 71.0% 0.76 73.0% 0.75CR57.0% 0.59 54.0% 0.56 58.0% 0.60CR-DIR72.0% 0.77 68.0% 0.71 67.0% 0.71MulAD63.0% 0.64 58.0% 0.59 63.0% 0.65MulAD-DIR 78.0% 0.84 71.0% 0.73 75.0% 0.77</p>
<p>Table 4 :
4
Reasoning accuracy of various methods on ProofMath dataset.</p>
<p>Table 5 :
5
Reasoning accuracy comparison of DR and IR on ProofWriter_S and ProofMath_S datasets.</p>
<p>Table 6 :
6
Reasoning accuracy comparison of DR and IR by zero-shot prompts.</p>
<p>Table 7 :
7
The distribution of questions in ProofMath.
Scope of Mathematic KnowledgeQuantityAlgebraIntegral and Fractional Formulas Function6 11Junior SchoolLines7GeometryTriangle8Polygon8Set13AlgebraFunction19High SchoolSeries12GeometrySolid Geometry Analytic Geometry7 9</p>
<p>19 illustrate the prompt templates employed in the experiment for different tasks.These templates primarily comprise IR instructions and examples demonstrating intermediate processes of IR.</p>
<p>https://cloud.google.com/vertex-ai/docs/generativeai/model-reference/gemini
2 https://llama.meta.com/docs/llama-everywhere/runningmeta-llama-on-linux/
AcknowledgmentsThis work was supported by the Major Science and Technology Innovation 2030 "New Generation Artificial Intelligence" key project (No. 2021ZD0111700), NSF of China (Nos: 62336003, 12371510), and NTU RSR and Start Up Grants.IR prompt template for CoT in ProofWriter &lt;~system&gt; Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let's think about it step by step.You need to use proof by contradiction to judge whether the following conclusion is True, False or Unknown.First, take the negation of the conclusion and assume the negation to be true or false in turn.Then, use the premises and rules to deduce whether the assumption leads to a contradiction.If a contradiction arises, the assumption is false.If not, the assumption cannot be determined.If necessary, consider the logical equivalence of the original rules and their contrapositive.Check whether there is a conflict strictly following the premises and rules rather than introducing unsourced common knowledge and unsourced information by common sense reasoning.----</system> &lt;~each example&gt; "Premises": {premises_temp} "Rules": {rules_temp} "Conclusion": {conclusion_temp} Let's deduce step by step to reach the conclusion by making full use of the "Premises" and "Rules"."Reasoning": {reasoning_temp} "Judgement": {valid_temp} </each> ---"Premises": {premises} "Rules": {rules} "Conclusion": {conclusion} Let's deduce step by step to reach the conclusion by making full use of the "Premises" and "Rules"."Reasoning": "Judgement": IR prompt template for CR in ProofWriter &lt;~system&gt; Suppose you are one of the greatest AI scientists, logicians and mathematicians.Let's think about it step by step.You need to use proof by contradiction to judge whether the following conclusion is True, False or Unknown.First, take the negation of the conclusion and assume the negation to be true or false in turn.Then, use the premises, rules, and propositions to deduce whether the assumption leads to a contradiction.If a contradiction arises, the assumption is false.If not, the assumption cannot be determined.If necessary, consider the logical equivalence of the original rules and their contrapositive.Check whether there is a conflict strictly following the premises and rules rather than introducing unsourced common knowledge and unsourced information by common sense reasoning.----</system> &lt;~each example&gt; "Premises": {premises_temp} "Rules": {rules_temp} "Conclusion": {conclusion_temp} "Propositions": {propositions_temp} Let's deduce step by step to reach the conclusion by making full use of the "Premises", "Rules" and "Propositions"."Reasoning": {reasoning_temp} "Judgement": {valid_temp} </each> ---"Premises": {premises} "Rules": {rules} "Conclusion": {conclusion} "Propositions": {propositions} Let's deduce step by step to reach the conclusion by making full use of the "Premises", "Rules" and "Propositions"."Reasoning": "Judgement": IR prompt template for CoT in LogiQA &lt;~system&gt; Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step by step.Please use the method of "Substitution exclusion" to check the correctness of each of the four options.Specifically, you need to assume the option in turn is true and then check whether each option will cause a conflict with the content provided.If so, exclude this option, otherwise keep it.If you choose the first option, answer "First"; If you choose the second option, answer "Second"; If you choose the third option, answer "Third"; If you choose the fourth option, answer "Fourth".----</system> &lt;~each example&gt; "Statement": {statement_temp} "Question": {question_temp} "Options": {options_temp} Let's think about it step by step by Substitution exclusion method."Reasoning": {reasoning_temp} "Answer": {ans_temp} </each> ---"Statement": {statement} "Question": {question} "Options": {options} Let's think about it step by step by Substitution exclusion method."Reasoning": "Answer": IR prompt template for CR in LogiQA &lt;~system&gt; Suppose you are one of the greatest AI scientists, logicians, and mathematicians.Let's think about it step step.First, read and analyze the "Statement" and "Question", then use the "Premises", "Boundary Conditions" and "Propositions" by the method of "Substitution exclusion" to check the correctness of each of the four options.Specifically, you need to assume the option in turn is true and then check whether each option will cause a conflict with the content provided.If so, exclude this option, otherwise keep it.Make sure that your reasoning is derived directly from "Premises" and "Propositions" rather than introducing unsourced common sense and unsourced information through common sense reasoning.If you choose the first option, answer "First"; If you choose the second option, answer "Second"; If you choose the third option, answer "Third"; If you choose the fourth option, answer "Fourth".----</system> &lt;~each example&gt; "Statement": {statement_temp} "Question": {question_temp} "Premises": {premises_temp} "Boundary condition": {boundary_condition_temp} "Propositions": {propositions_temp} Let's think step by step using the Substitution exclusion method, from the "Premises", "Boundary conditions" and "Propositions"."Reasoning": {reasoning_temp} "Answer": {ans_temp} </each> ---"Statement": {statement} "Question": {question} "Premises": {premises} "Boundary condition": {boundary_condition} "Propositions": {propositions} Let's think step by step using the Substitution exclusion method, from the "Premises", "Boundary conditions" and "Propositions".. "Reasoning": "Answer": IR prompt template for CoT in ProofNet &lt;~system &gt; Suppose you are one of the best mathematicians in the world, please prove the following statement and give a complete process of proof.Please try to prove the following statement by proof by contradiction.First, take the negation of the conclusion and assume the negation is true; Next, use the negation to deduce its falsity until the result of that assumption is a contradiction.----</system> &lt;~each example&gt; "Statement": {statement_temp} "Proof ": {proof_temp} </each> "Statement": {statement} "Proof ": IR prompt template for CR in ProofNet &lt;~system &gt; Suppose you are one of the best mathematicians in the world, please prove the following statement and give a complete process of proof.Please try to prove the following statement by proof by contradiction.First, take the negation of the conclusion and assume the negation is true; Next, use the negation to deduce its falsity until the result of that assumption is a contradiction.Please try to use the generated propositions to proceed with the proof.----</system> &lt;~each example&gt; "Statement": {statement_temp} "Propositions": {proposition_temp} "Proof ": {proof_temp}" </each> "Statement": {statement} "Propositions": {proposition} "Proof ": IR prompt template for CoT in ProofMath &lt;~system &gt; Suppose you are one of the best mathematicians in the world, please prove the following statement and give a complete process of proof.Please try to prove the following statement by proof by contradiction.First, take the negation of the conclusion and assume the negation is true; Next, use the negation to deduce its falsity until the result of that assumption is a contradiction.Step 1: List the conditions and questions in the original statement.Step 2: Merge the conditions listed in Step 1 into one.Define it as wj.Step 3: Let us think about it step by step.Please consider all possibilities.If the intersection between wj (defined in Step 2) and the negation of the conclusion is not empty at least in one possibility, the original statement is false.Otherwise, the original statement is true.----</system> &lt;~each example&gt; "Statement": {statement_temp} "Proof": {proof_temp} </each> "Statement": {statement} "Proof ": IR prompt template for CR in ProofMath &lt;~system &gt; Suppose you are one of the best mathematicians in the world, please prove the following statement and give a complete process of proof.Please try to prove the following statement by proof by contradiction.First, take the negation of the conclusion and assume the negation is true; Next, use the negation to deduce its falsity until the result of that assumption is a contradiction.Please try to use the generated proposition to proceed with the proof.Step 1: List the conditions and questions in the original statement.Step 2: Merge the conditions listed in Step 1 into one.Define it as wj.Step 3: Let us think about it step by step.Please consider all possibilities.If the intersection between wj (defined in Step 2) and the negation of the conclusion is not empty at least in one possibility, the original statement is false.Otherwise, the original statement is true.----</system> &lt;~each example&gt; "Statement": {statement_temp} "Propositions": {proposition_temp} "Proof ": {proof_temp} &lt;~/each&gt; "Statement": {statement} "Propositions": {proposition} "Proof ":
LLMbased interaction for content generation: A case study on the perception of employees in an it department. Alexandre Agossah, Frédérique Krupa, Matthieu Perreira Da Silva, Patrick Le, Callet , Proceedings of the ACM International Conference on Interactive Media Experiences. the ACM International Conference on Interactive Media Experiences2023</p>
<p>Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, Jeremy Avigad, arXiv:2302.12433Proofnet: Autoformalizing and formally proving undergraduate-level mathematics. 2023arXiv preprint</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Uprise: Universal prompt retrieval for improving zero-shot evaluation. Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, Qi Zhang, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Neural retrievers are biased towards LLM-generated content. Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Gang Wang, Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 30th ACM SIGKDD Conference on Knowledge Discovery and Data MiningJun Xu. 2024</p>
<p>Compositional semantic parsing with large language models. Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, Denny Zhou, The International Conference on Learning Representations. 2022</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, International Conference on Machine Learning. PMLR2024</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, arXiv:2407.21783The llama 3 herd of models. 2024arXiv preprint</p>
<p>Intuition and reasoning: A dual-process perspective. Jonathan St, B T Evans, Psychological Inquiry. 2142010</p>
<p>Retrieval-augmented generation for large language models: A survey. Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Haofen Wang, arXiv:2312.109972023arXiv preprint</p>
<p>Hangfeng He, Hongming Zhang, Dan Roth, arXiv:2301.00303Rethinking with retrieval: Faithful large language model inference. 2022arXiv preprint</p>
<p>Genegpt: Augmenting large language models with domain tools for improved access to biomedical information. Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu, Bioinformatics. 402e0752024</p>
<p>On the analysis of indirect proofs: Contradiction and contraposition. Nicolas Jourdan, Oleksiy Yevdokimov, Australian Senior Mathematics Journal. 3012016</p>
<p>Lambada: Backward chaining for automated reasoning in natural language. Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, Deepak Ramachandran, The Annual Meeting Of The Association For Computational Linguistics. 2023</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, Advances in Neural Information Processing Systems. 202235</p>
<p>Logiqa: a challenge dataset for machine reading comprehension with logical reasoning. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, Yue Zhang, Proceedings of the International Conference on International Joint Conferences on Artificial Intelligence. the International Conference on International Joint Conferences on Artificial Intelligence2021</p>
<p>When to use large language model: Upper bound analysis of bm25 algorithms in reading comprehension task. Tingzhen Liu, Qianqian Xiong, Shengxi Zhang, The International Conference on Natural Language Processing. IEEE2023</p>
<p>Query rewriting in retrieval-augmented large language models. Xinbei Ma, Yeyun Gong, Pengcheng He, Nan Duan, The Conference on Empirical Methods in Natural Language Processing. 2023</p>
<p>Using an LLM to help with code understanding. Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, Brad Myers, Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. the IEEE/ACM 46th International Conference on Software Engineering2024</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Graph retrieval-augmented generation: A survey. Boci Peng, Yun Zhu, Yongchao Liu, Xiaohe Bo, Haizhou Shi, Chuntao Hong, Yan Zhang, Siliang Tang, arXiv:2408.089212024arXiv preprint</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, Advances in Neural Information Processing Systems. 202436</p>
<p>Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, arXiv:2212.001932022arXiv preprint</p>
<p>Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, Rui Yan, arXiv:2310.18659From indeterminacy to determinacy: Augmenting logical reasoning capabilities with large language models. 2023arXiv preprint</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics 2021. 2021</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>
<p>Qineng Wang, Zihao Wang, Ying Su, Hanghang Tong, Yangqiu Song, arXiv:2402.18272Rethinking the bounds of LLM reasoning: Are multi-agent discussions the key?. 2024arXiv preprint</p>
<p>Self-consistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Ed H Quoc V Le, Sharan Chi, Aakanksha Narang, Denny Chowdhery, Zhou, The International Conference on Learning Representations. 2022a</p>
<p>Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language Processing2022b</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 202235</p>
<p>Leandojo: Theorem proving with retrieval-augmented language models. Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan J Prenger, Animashree Anandkumar, Advances in Neural Information Processing Systems. 202436</p>
<p>Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling. Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, Xindong Wu, arXiv:2306.114892023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, Karthik Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Star: Bootstrapping reasoning with reasoning. Eric Zelikman, Yuhuai Wu, Advances in Neural Information Processing Systems. 202235Jesse Mu, and Noah Goodman</p>
<p>Yifan Zhang, Jingqin Yang, Yang Yuan, Andrew Chi-Chih Yao, arXiv:2308.04371Cumulative reasoning with large language models. 2023arXiv preprint</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc V Le, The International Conference on Learning Representations. 𝐹1: 𝑎, 𝑏, 𝑐 are positive real numbers. 𝑅1: If 𝑎. 2022𝑏 are positive real numbers, 𝑎 + 𝑏 ≥ 2 𝑎𝑏 (AM-GM</p>
<p>𝑏 + 1)(𝑐 + 1) ≥ 2 𝑎 × 2 𝑏 × 2 𝑐 = 8 𝑎𝑏𝑐. 𝐹2: 𝑎𝑏𝑐 = 1. → 𝐴: (𝑎 + 1)(𝑏 + 1)(𝑐 + 1) ≥ 8. Mathematic Proof Fact set 𝐹: 𝑎, 𝑏, 𝑐 are positive real numbers. 𝑎𝑏𝑐 = 1. Conclusion 𝑄: (𝑎 + 1)(𝑏 + 1)(𝑐 + 1) ≥ 8. 𝐹1 𝑅1 𝐼𝑛𝑡𝑒𝑟𝐶𝑜𝑛1 𝐹2 𝐴 𝑃𝑅. → 𝐼𝑛𝑡𝑒𝑟𝐶𝑜𝑛1：, 𝑎 + 1). </p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7313 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7313</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7313</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-139.html">extraction-schema-139</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being used to detect anomalies in lists or tabular data, including the methods, datasets, evaluation metrics, and results.</div>
                <p><strong>Paper ID:</strong> paper-270737494</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.18197v4.pdf" target="_blank">Human-Free Automated Prompting for Vision-Language Anomaly Detection: Prompt Optimization with Meta-guiding Prompt Scheme</a></p>
                <p><strong>Paper Abstract:</strong> Pre-trained vision-language models (VLMs) are highly adaptable to various downstream tasks through few-shot learning, making prompt-based anomaly detection a promising approach. Traditional methods depend on human-crafted prompts that require prior knowledge of specific anomaly types. Our goal is to develop a human-free prompt-based anomaly detection framework that optimally learns prompts through data-driven manner, eliminating the need for human intervention. The primary challenge in this approach is the lack of anomalous samples during the training phase. To tackle this challenge, we develop the Object-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples for training. Furthermore, to prevent learned prompt from overfit on synthesized anomaly feature, we proposed Meta-Guiding Prompt-Tuning Scheme (MPTS) that iteratively adjusts the gradient-based optimization direction of learnable prompts to avoid overfitting to the synthesized anomalies . This framework allows for the optimal prompt embeddings by searching in the continuous latent space via backpropagation, free from human semantic constraints. Ad-ditionally, the modified locality-aware attention improves the precision of pixel-wise anomaly segmentation.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7313",
    "paper_id": "paper-270737494",
    "extraction_schema_id": "extraction-schema-139",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.0025895,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Human-Free Automated Prompting for Vision-Language Anomaly Detection: Prompt Optimization with Meta-guiding Prompt Scheme
11 Sep 2024</p>
<p>Pi-Wei Chen 
Jerry Lin 
Feng-Hao Yeh yeh.feng.hao.110@gmail.com 
Jia Ji 
Ching Chen Nvidia 
Chao-Chun Chen chencc@imis.ncku.edu.tw </p>
<p>National Cheng Kung University</p>
<p>National Cheng Kung University</p>
<p>National Cheng Kung University</p>
<p>National Cheng Kung University</p>
<p>Human-Free Automated Prompting for Vision-Language Anomaly Detection: Prompt Optimization with Meta-guiding Prompt Scheme
11 Sep 202440A8B1F4669830A0668FCC187D36FEC3arXiv:2406.18197v4[cs.CV]
Pre-trained vision-language models (VLMs) are highly adaptable to various downstream tasks through few-shot learning, making prompt-based anomaly detection a promising approach.Traditional methods depend on human-crafted prompts that require prior knowledge of specific anomaly types.Our goal is to develop a human-free prompt-based anomaly detection framework that optimally learns prompts through data-driven manner, eliminating the need for human intervention.The primary challenge in this approach is the lack of anomalous samples during the training phase.To tackle this challenge, we develop the Object-Attention Anomaly Generation Module (OAGM) to synthesize anomaly samples for training.Furthermore, to prevent learned prompt from overfit on synthesized anomaly feature, we proposed Meta-Guiding Prompt-Tuning Scheme (MPTS) that iteratively adjusts the gradient-based optimization direction of learnable prompts to avoid overfitting to the synthesized anomalies .This framework allows for the optimal prompt embeddings by searching in the continuous latent space via backpropagation, free from human semantic constraints.Additionally, the modified locality-aware attention improves the precision of pixel-wise anomaly segmentation.</p>
<p>Introduction</p>
<p>In the field of anomaly detection, acquiring annotations for anomalous samples is often challenging.Consequently, prior research has focused on unsupervised learning, which involves training models on large volumes of anomaly-free data to identify deviations from normal features as anomalies.Recently, some approaches have addressed more extreme scenarios where only a few normal samples are available.These methods leverage vision-language pretrained models, Figure 1.The upper part shows previous methods requiring humandesigned prompts.The lower part illustrates our automated, datadriven approach using learnable prompts without prior knowledge.such as CLIP, to facilitate few-shot anomaly detection.</p>
<p>Vision-language models are renowned for their adaptability to various downstream tasks such as classification and segmentation.The remarkable transferability of visionlanguage models across different computer vision tasks can be attributed to their large-scale pre-training on text-image pairs [3].This training enables the model to capture relevant semantic features based on the provided prompts, thereby allowing the model to leverage the textual modality to solve visual tasks such as anomaly detection.[8] pioneered the application of VLMs for zero-shot or few-shot anomaly detection.This approach leverages lexical terms related to 'anomaly' and 'normal' to identify anomalous visual features in the test samples, providing a foundation for subsequent studies to further advance few-shot anomaly detection methodologies.Prompt engineering is a crucial technique that preserves the robustness and generalization capabilities of CLIP while efficiently adapting it to meet the demands of specific applications.The primary objective of prompt tuning is to obtain an optimal textual embedding that aligns with the target visual image embedding, ensuring that the model accurately captures the nuances of the task at hand.</p>
<p>Previous work [8] proposed a prompt ensemble technique that composes different prompt templates (e.g., "a photo of a [c] for visual inspection") and normal-abnormal-relevant prompts (e.g., "flawless" for normality, "damaged" for anomaly) to obtain text embeddings aligned with anomaly detection semantics.Recent studies [9,16] have leveraged prefix prompt tuning (context optimization [15]) to replace static prompt templates with learnable prompts, better catering to anomaly detection tasks.Although prior work has demonstrated the efficacy of prefix prompting, it still necessitates a customized, manually-designed suffix to describe the anomaly type for each class item.This process heavily relies on detailed and specific prior knowledge of the dataset and the types of anomalies that may appear in each product category, as shown in the upper part of Fig 1 .This remains a non-trivial task requiring iterative adjustments, rather than automatically finding the optimal textual embedding.</p>
<p>We are motivated by [15], which provides insights that the optimal prompts derived through prompt tuning do not necessarily align with human semantics or intuition.Previous work has often focused on combining various human semantic terms, such as 'anomaly' and 'defect,' to generate better embeddings.However, we propose a novel perspective: instead of adhering to human-centric semantics, we should employ data-driven optimization methods, such as gradient-based learning, to derive the most effective textual embeddings.By utilizing these advanced deep learning techniques, we can create more robust and precise embeddings that better capture the nuances needed for anomaly detection and adapt to various datasets without the need for prior knowledge as show in lower part in Fig 1.</p>
<p>However, to optimize the Learnable Normal Prompt (LNP) and Learnable Anomaly Prompt (LAP), samples from both normal and anomalous data are required.In the case of few-shot anomaly setting, only normal samples are available during training.To enable the acquirement of optimal textual embedding for anomaly detection, we need to selfgenerate anomaly sample to obtain the optimal prompt for better textual embedding.One intuitive method to synthesize anomalous samples is by adding Gaussian noise to the image pixels to mimic anomalies.However, Gaussian noise alone might not guarantee performance in real-world scenarios because such synthetic anomalies often lack the complex and context-specific characteristics of true anomalies.</p>
<p>Additionally, Vision-Language Models (VLMs) are not designed for pixel-wise tasks like segmentation, making them unsuitable for pixel-level anomaly detection.During VLM pre-training, only global features (e.g., class tokens in Vision Transformers) are considered, neglecting localized features that capture fine-grained image details.Consequently, the relationship between text prompts and specific image regions is not optimized, leading to unpredictable results where the target prompt may not align with the target object pixels [10,12].</p>
<p>In this paper, we propose a human-free automated prompting anomaly detection framework, namely "Metaprompting Semantic Learning for Anomaly Detection with Locality-aware Attention Image Encoder".The proposed framework follows the intuition of data-driven methods that leverage back-propagation to actively find the prompt that can generate the optimal textual embedding suited for anomaly detection, instead of passively combining all possible human semantic prompts to meet the requirements of prompt-based anomaly detection.To address the limitation of the absence of anomaly samples for context optimization, we develop the Object-Attention Anomaly Generation Module to synthesize anomaly samples for training.Meanwhile, the proposed Meta-guiding Prompt-tuning Scheme (MPTS) is developed to prevent the learnable prompt from overfitting on synthesized anomalies through iterative gradient-based calibration.</p>
<p>In Meta-Prompt Tuning Scheme (MPTS), we introduce a dynamic meta-prompt that serves as an anchor to balance specificity and generality in anomaly detection via gradientbased calibration.This meta-prompt ensures that learnable prompts remain aligned with meaningful, generalizable concepts while leveraging useful information from synthesized anomalies without deviating from the core goal of anomaly detection.Initially, a general prompt template provided by a large language model (LLM) is embedded with real-world anomaly semantics, serving as the initial meta-prompt.This helps ensure that the learnable prompts do not deviate significantly from true anomalies during fine-tuning.Throughout each training round, the optimized learnable prompt from the previous round serves as the new meta-guiding prompt, iteratively refining the process.Meta-guiding prompts ensure that the learnable prompts resemble real anomalies, enhancing detection effectiveness.Additionally, introducing Gaussian noise diversifies the patterns, improving generalization across various anomaly types.This combination allows the learnable prompts to generalize well to different anomalies while maintaining alignment with real-world characteristics.</p>
<p>To further improve the quality of synthesized anomalous samples, we introduce the "Object-Attention Anomaly Generation Module".This module leverages the general knowledge of CLIP to identify the target object in the image and applies noise specifically to that object.By doing so, the synthesized anomalies better reflect practical scenarios, as real-world anomalies usually occur on target objects rather than in the background, which is typically not the focus of anomaly detection.</p>
<p>Additionally, we introduce Locality-aware Attention, which can be adopted by any transformer-based model to mitigate the misalignment of input token features and corre-</p>
<p>Related Work</p>
<p>Vision-language model</p>
<p>Vision-language models (VLMs) are advanced AI systems that integrate computer vision (CV) and natural language processing (NLP) to understand the commonality of semantics between text embeddings and visual features.These multimodal models are trained on extensive datasets of paired images and text, utilizing techniques like contrastive learning [3] and masked language modeling to map visual and textual data into a shared semantic space.A notable open-source VLM is OPENCLIP [7], which excels in generalization, enabling rapid adaptation to various downstream tasks such as image classification and object detection without taskspecific training.Furthermore, [15] proposed context optimization, which can tune learnable prompts with few-shot samples, achieving optimal performance surpassing humandesigned prompts with.</p>
<p>Anomaly detection</p>
<p>Traditional paradigms of anomaly detection [2,6,11,13] typically utilize only normal samples for training anomaly detection models.This approach, known as unsupervised anomaly detection, does not require any anomaly samples during training.In recent years, the research focus has shifted from unsupervised anomaly detection to the more challenging few-shot scenario, where only a limited number of normal samples are available during the training phase.Patch-Core [4] employs a memory-efficient technique to capture normal patterns in few-shot data for anomaly inference.Re-gAD [14] introduces a region-based method that maximizes the information extracted from each example, effectively learning informative features for anomaly inference using only a few normal samples.</p>
<p>Recent studies [8,9] have further addressed the anomaly detection challenge with multimodal solutions, leveraging the general knowledge embedded in pre-trained visionlanguage models (VLMs) to perform zero-shot and few-shot anomaly detection with the guidance of textual modality.However, these approaches still rely on human-designed prompts, with their performance dependent on the prompt engineer's prior knowledge of the specific application scenario.This dependency highlights the need for more advanced, data-driven methods to optimize prompt selection and enhance anomaly detection performance.</p>
<p>Proposed Framework</p>
<p>Problem Definition:</p>
<p>In this paper, we address the few-shot anomaly detection task, where only a few normal samples are available during the training stage.We suppose D train = {(x i , y i = 0)} k i=1 is the k-shot sample given in the training stage (y i ∈ {0, 1}).Each x i ∈ R H×W is a normal sample when y i = 0, and abnormal when y i = 1.</p>
<p>In our proposed human-free prompt-based anomaly detection framework, we substitute traditional human prompts ("a photo of a {} without defect" as the normality prompt and "a photo of a defective {}" as the abnormality prompt) with learnable prompts.The learnable normality prompt (LNP) and the learnable abnormality prompt (LAP) are composed of a learnable general prefix (LGP) = [G 1 , . . ., G ng ] and a learnable normal or abnormal suffix (LNS = [N 1 , . . ., N nr ] or LAS = [A 1 , . . ., A na ], respectively:
LNP = [G 1 , . . . , G ng ] [cls] [N 1 , . . . , N nr ] LAP = [G 1 , . . . , G ng ] [cls] <a href="1">A 1 , . . . , A na </a>
The LNP and LAP are leveraged to calculate the anomaly score map S ∈ R H×W ×1 on the feature map F ∈ R H ′ ×W ′ ×C extracted from the testing sample.</p>
<p>In the inference stage, we have
D test = {(x i , y i , M i )} N i=1
. The mask M ∈ {0, 1} H×W is the ground truth map for anomaly pixels, where each pixel m j ∈ M indicates whether the corresponding pixel x j ∈ x is anomalous (m j = 1) or normal (m j = 0).</p>
<p>Our objective is to leverage D train to tune LAP and LNP such that they can yield an anomaly score map S that aligns with the ground truth anomaly map M .This can be represented as:
arg max Sim(S, M )(2)
where Sim() is a similarity metric that calculates the alignment degree between S and M .</p>
<p>Overview</p>
<p>The overall framework of the proposed Meta-prompting Semantic Learning for Anomaly Detection with Locality Feature Attention Image Encoder is illustrated in Fig 2 .The proposed Meta-guiding Prompt-tuning Scheme (Sec 3.3)</p>
<p>Meta-guiding Prompt-tuning Scheme</p>
<p>The Meta-guiding Prompt-tuning Scheme is illustrated in Fig 3, which an self-optimized meta-prompt will iterativly update it self and guiding the learning direction of learnable prompt with gradient correction.</p>
<p>To tune LAP and LNP to align with the anomaly detection task, the OAGM (Sec 3.4) will synthesize anomalous samples (x ′ i , y i = 1, M ′ a)i = 1 k , where M ′ a ∈ 0, 1 H×W is the abnormality binary mask indicating whether each pixel m j ∈ M ′ a is contaminated with synthesized anomaly (m j = 1) or not (m j = 0).A normality binary mask M ′ n can be obtained by:
M ′ n = 1 − M ′ a(3)
The LNP with the extracted feature map of x ′ i is dotmultiplied to calculate the normality score map S n , indicating whether each pixel in x ′ i aligns with normality.Similarly, LAP is used to obtain an abnormality score map S a .To aggregate the information from both LNP and LAP and produce a probability-like output (bounded from 0 to 1), we concatenate S n and S a and apply Softmax along each element, finally obtaining normalized score maps S ′ n and S ′ a .The training objective is to tune LNP such that its embedding perfectly aligns with the normality embedding in x ′ .This involves aligning S ′ n with the normality binary mask M ′ n using BCEloss:
L LNP ano = − j|mj =1 M ′ n (j) log S ′ n (j) + (1 − M ′ n (j)) log(1 − S ′ n (j)) (4)
For LAP, the objective is to align LAP with the abnormality embedding in x ′ :
L LAP ano = − j|mj =1 M ′ a (j) log S ′ a (j) + (1 − M ′ a (j)) log(1 − S ′ a (j))(5)
Meta-guiding: Synthesized anomaly samples, while useful for training, may contain artifacts or specific characteristics that are not representative of real-world anomalies.To address this limitation, we propose a meta-guiding scheme to regularize the tuning process with general, domain-agnostic knowledge embedded in pre-trained VLMs.</p>
<p>Initially, we provide general manual prompts as Meta Normality Prompt (MNP) and Meta Abnormality Prompt (MAP) (see appendix), which act as anchors to prevent LAP and LNP from overfitting on artifact anomalies and maintain generalization with a broader concept of anomaly.By calibrating the LNP and LAP gradients from Eq 4 and Eq 5, respectively, with the divergence loss between MAP/MNP and LAP/LNP, we can ensure that LAP and LNP are optimized toward a task-specific prompt that human semantics cannot achieve while maintaining generalization.</p>
<p>This divergence loss is designed to estimate the output score maps from LAP/LNP (S ′ a and S ′ n , respectively) and MAP/MNP (denoted as S M a and S M n , respectively).Since the values of the output score maps are already transformed to probability form, we use KL divergence as the distance metric to estimate the divergence.The divergence loss can be written as:
L A div = H×W i=1 KL(S M a (i) ∥ S ′ a (i))(6)L M div = H×W i=1 KL(S M n (i) ∥ S ′ n (i))(7)
To strike a balance between task-specific objectives and generalization, the meta-guiding scheme will only calibrate the gradient when the gradient G LAP/LNP ano from fine-tuning the anomaly loss:
G LAP/LNP ano = ∂L LAP/LNP ano ∂θ LAP/LNP (8)
contradicts the gradient from divergence loss G LAP/LNP div .
G LAP/LNP div = ∂L A/M div ∂θ LAP/LNP(9)
The LAP and LNP are optimized with the gradient G LAP/LNP ano after being calibrated with Eq 10:
G ano − λ • Cos (G ano , G div ) , if Cos (G ano , G div ) &lt; 0 G ano , otherwise(10)
The meta-guiding scheme ensures that the optimization direction will explore the optimal prompt that aligns better with the anomaly detection task while preventing overfitting on synthesized anomalies.</p>
<p>Based on the intuition that we have obtained a more optimal LAP t+1 /LNP t+1 through gradient descent instead of relying on human semantics, we substitute the MAP/MNP with LAP t+1 /LNP t+1 as the new Meta prompt for the next meta-epoch training.We iteratively refine the MAP/MNP and LAP/LNP until the loss converges.This iterative substitution strategy balances the need for task-specific adaptation and generalization.By continually referring back to the metaguiding prompts, the model maintains a connection to the initial general knowledge while fine-tuning the prompts to better detect anomalies.</p>
<p>Object-attention Anomaly Generation Module</p>
<p>To tune LAP and LNP to align with the anomaly detection task, we propose the Object-Attention Anomaly Generation Module (OAGM).Traditional methods that add Gaussian noise randomly across the entire image are suboptimal because they affect irrelevant background regions, diluting the model's ability to focus on the target object.The OAGM addresses this limitation by selectively adding noise only to the target object areas, enhancing the model's ability to detect anomalies.</p>
<p>First, we utilize a pre-trained Vision-Language Model (VLM) to produce the binary mask M obj for the target object:
M obj (i, j) = 1 if pixel (i, j) belongs to the target object 0 otherwise
Next, Gaussian noise N (0, σ 2 ) is selectively added to the pixels identified by M obj :
x ′ (i, j) = x(i, j) + N (0, σ 2 ) if M obj (i, j) = 1 x(i, j) if M obj (i, j) = 0</p>
<p>Locality-aware attention</p>
<p>The vanilla ViT is not suitable for pixel-level tasks due to the feature misalignment issue.We obtain insight from [5] that the feature misalignment issue results from the nature of the original attention mechanism, where each patch (token) feature attends to distant patch features.This leads to local features being fused with irrelevant background features, causing misalignment between input and output features.Based on this observation, we substitute the vanilla attention with the proposed Locality-Aware Attention (LAA) that restricts each token to attend only to its neighboring token features during feature extraction.To avoid model collapse caused by directly modifying the self-attention mechanism, which would lead to changes in subsequent layers and be amplified through the layers, we adopt the dual-path design from [] to maintain stable input features by preserving the original path alongside the new path.</p>
<p>The LAA applies a k-neighbor exclusive mask
M LAA ∈ R H ′ ×W ′
to mask the attention map (the dot product of Q and K) in the attention mechanism:
Attention LAA (Q, K, V) = softmax QK T + M LAA √ d k V(11)
where M LAA is constructed with the following formula to enable each token to attend only to its k-neighbors:
M (i,j),(m,n) = 0 if (i − m) 2 + (j − n) 2 ≤ k −∞ otherwise (12
) Each patch can be indexed in the 2D grid as (i, j) where 0 ≤ i &lt; H ′ and 0 ≤ j &lt; W ′ .(i, j) and (m, n) are the coordinates of the patches in the original feature map before patchification, and k is the predefined neighbor distance.</p>
<p>Experiments</p>
<p>We conduct experiment to validate that Meta-guiding Prompt-tuning Scheme with Object-attention Generation Module can optimize a prompt embedding that outperform human-semantic prompt under scenario that only 1 sample (1-shot) is provided in pixel-level anomaly segmentation.Additionally, we complete ablation experiment to verify the effectiveness of each proposed component.Dataset In this paper, we conduct experiment on MVTec [1] dataset, which is a benchmark dataset for industrial anomaly detection that provide pixel-level ground truth map indicating the anomaly location in the image.MVtec comprise 15 Categories of item, and each category contain 5-6 subcategories of anomlay type.</p>
<p>Comparison to human-semantic prompt</p>
<p>As shown in Table1, we can see that our proposed framework outperform manual design prompt by 28%, which further validate our assumption that optimal prompt for downstream task is not necessary align with human semantic.The humansemantic prompt template and suffix is followed by the literature [].We have further conduct ablation study in Sec.4.2 to verify that the meta-prompt guiding scheme is crucial for learning the optimal prompt embedding.</p>
<p>Ablation Study</p>
<p>Ablation on Meta-prompt guiding scheme The ablation study results in Table2 highlight the effectiveness of the Meta-guiding Prompt-tuning Scheme, particularly in enhancing anomaly detection performance.When λ = 0, the model lacks meta-guiding and achieves a mean score of 67.21, indicating limited effectiveness due to overfitting on synthesized anomalies.Introducing meta-guiding with λ = 0.5 significantly improves the mean score to 89.56, demonstrating enhanced generalization and alignment with broader anomaly concepts.The highest performance is achieved with λ = 1, yielding a mean score of 92.40 and near-perfect results in several classes, confirming that the scheme effectively balances task-specific tuning and general knowledge integration.These findings validate the module's contribution to improving anomaly detection accuracy and robustness.Ablation on Object-attention Anomaly Generated Module(OAGM)</p>
<p>Ablation on Locality-aware Attention</p>
<p>To evaluate the impact of the proposed Locality-Aware Attention (LAA), we conducted an ablation study comparing different configurations of attention mechanisms.Specifically, we compared the original VV-att (V-V attention as described in [10]), QK-LAA (original QK attention with LAA), and VV-LAA (V-V attention combined with LAA).The results are summarized in Table4.</p>
<p>From the results, it is evident that LAA enhances performance across all categories.The mean performance of QK-LAA (90.52) surpasses that of VV-att (89.52), demonstrating that the locality constraint improves feature alignment even when applied to the traditional QK attention mechanism.Furthermore, the combination of V-V attention with LAA (VV-LAA) achieves the highest mean performance (92.40), indicating that our LAA not only addresses the feature mis-</p>
<p>Conclusion</p>
<p>In this paper, we introduced a novel framework, Metaprompting Semantic Learning for Anomaly Detection with Locality Feature Attention Image Encoder, which optimizes vision-language models for anomaly detection in a human-free manner.Our Meta-guiding Prompt-tuning Scheme and Object-Attention Anomaly Generation Module address the absence of anomaly samples by synthesizing realistic anomalies and iteratively refining prompts via gradient-based calibration.Additionally, the Locality-Aware Transformer preserves local details and ensures alignment between input features and output tokens, enhancing pixel-wise anomaly segmentation.Our extensive experiments demonstrated significant performance improvements over manually designed prompts, showcasing the efficacy of our approach and advancing the field of few-shot and zeroshot anomaly detection.This work opens new avenues for future research in developing more sophisticated anomaly detection models that can adapt to various real-world scenarios without extensive human intervention.</p>
<p>Figure 2 .
2
Figure 2. The overall architecture of Meta-prompting Semantic Learning for Anomaly Detection with Locality Feature Attention Image Encoder</p>
<p>Figure 3 .
3
Figure 3.The illustration of Meta-guiding Prompt-tuning Scheme</p>
<p>Table 1 .
1
Comparison to Human semantic prompt
Classcarpetgrid leathertile wood bottle cable capsule hazelnut metal nutpill screw toothbrush transistor zipper meanHuman-semantic prompt 93.98 35.8690.89 43.21 57.74 82.86 42.5472.0584.0955.98 51.57 70.7775.4362.23 50.90 64.67Ours99.40 98.4599.38 96.12 96.18 89.91 87.7594.5197.7075.08 95.92 95.4897.2372.23 90.63 92.40</p>
<p>Table 2 .
2
The ablation study of guiding level λ for Meta-guiding Prompt-tuning Scheme
Ablation study on MPTSClassλ = 0 λ = 0.5 λ = 1carpet76.2799.3299.40grid60.5197.9798.45leather76.8499.1599.38tile80.8794.8696.12wood50.0695.4296.18bottle49.0184.2689.91cable48.9081.8087.75capsule87.5591.8494.51hazelnut77.8487.7497.70metal nut52.7476.7775.08pill67.3687.3395.92screw78.5495.2795.48toothbrush 52.9092.5997.23transistor60.5565.2372.23zipper88.2293.9090.63mean67.2189.5692.40</p>
<p>Table 3 .
3
Comparison of Anomaly Detection Performance with and without OAGM
Ablation study on OAGMClassWithout OAGM With OAGM (λ = 1)carpet98.3699.40grid96.5398.45leather97.9399.38tile91.1496.12wood89.1996.18bottle64.1889.91cable49.3187.75capsule85.5394.51hazelnut65.6297.70metal nut72.1275.08pill78.4295.92screw87.0695.48toothbrush77.0597.23transistor66.8572.23zipper75.6190.63mean78.5492.40The ablation study in Table3 demonstrates the effective-ness of the Object-attention Anomaly Generation Module(OAGM) across different categories. For texture items likecarpet, wood, and tile, the entire image serves as the target,leading to high baseline performance even without OAGM(98.36, 89.19, and 91.14, respectively). OAGM providesslight improvements (99.40, 96.18, and 96.12) due to theinherent nature of these items. Conversely, object-like itemssuch as bottle, cable, and capsule show significant perfor-mance boosts with OAGM, increasing from 64.18, 49.31,and 85.53 to 89.91, 87.75, and 94.51, respectively. Theoverall mean score rises from 78.54 to 92.40, highlightingOAGM's role in enhancing anomaly detection by focusingon target objects and reducing irrelevant background noise,especially in object-centric categories.</p>
<p>Table 4 .
4
Ablation study on Locality-Aware Attention also synergizes effectively with advanced attention mechanisms to boost overall performance.This significant improvement underscores the effectiveness of LAA in enhancing pixel-wise anomaly detection tasks.
Ablation study on Locality-Aware AttentionClassVV-att QK-LAA VV-LAAcarpet98.2999.2999.40grid95.9396.9398.45leather98.1099.1099.38tile94.6395.6396.12wood94.0795.0796.18bottle79.8380.8389.91cable84.5885.5887.75capsule89.0990.0994.51hazelnut94.5395.5397.70metal nut72.6473.6475.08pill85.1286.1295.92screw91.7792.7795.48toothbrush 95.5596.5597.23transistor70.1971.1972.23zipper87.4588.4590.63mean89.5290.5292.40alignment issue but</p>
<p>Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. P Bergmann, M Fauser, D Sattlegger, C Steger, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2019</p>
<p>Utrad: Anomaly detection and localization with u-transformer. L Chen, Z You, N Zhang, J Xi, X Le, Neural Networks. 14732022</p>
<p>A simple framework for contrastive learning of visual representations. T Chen, S Kornblith, M Norouzi, G Hinton, International Conference on Machine Learning. PMLR202013</p>
<p>Patchcore: Memoryefficient patch-based core-set construction for unsupervised anomaly detection. L Cohen, L Bergman, Y Hoshen, arXiv:2012.042282020arXiv preprint</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Abnormal event detection via heterogeneous information network embedding. S Fan, C Shi, X Wang, Proceedings of the 27th ACM International Conference on Information and Knowledge Management. the 27th ACM International Conference on Information and Knowledge Management2018</p>
<p>. G Ilharco, M Wortsman, R Wightman, C Gordon, N Carlini, R Taori, A Dave, V Shankar, H Namkoong, J Miller, H Hajishirzi, A Farhadi, L Schmidt, Openclip, 2021If you use this software, please cite it as below</p>
<p>Winclip: Zero-/few-shot anomaly classification and segmentation. J Jeong, Y Zou, T Kim, D Zhang, A Ravichandran, O Dabeer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition19606-19616, 202313</p>
<p>Promptad: Learning prompts with only normal samples for few-shot anomaly detection. X Li, Z Zhang, X Tan, C Chen, Y Qu, Y Xie, L Ma, arXiv:2404.05231202423arXiv preprint</p>
<p>Clip surgery for better explainability with enhancement in open-vocabulary tasks. Y Li, H Wang, Y Duan, X Li, arXiv:2304.05653202327arXiv preprint</p>
<p>f-anogan: Fast unsupervised anomaly detection with generative adversarial networks. T Schlegl, P Seeböck, S M Waldstein, G Langs, U Schmidt-Erfurth, Medical Image Analysis. 5432019</p>
<p>Sclip: Rethinking selfattention for dense vision-language inference. F Wang, J Mei, A Yuille, arXiv:2312.015972023arXiv preprint</p>
<p>Unsupervised deep embedding for clustering analysis. J Xie, R Girshick, A Farhadi, International Conference on Machine Learning. 2016</p>
<p>Regad: Regionbased anomaly detection in medical imaging. W Zhang, W Li, Y Liu, X Wang, arXiv:2103.102422021arXiv preprint</p>
<p>Learning to prompt for vision-language models. K Zhou, J Yang, C C Loy, Z Liu, International Journal of Computer Vision. 130932022</p>
<p>Anomalyclip: Object-agnostic prompt learning for zero-shot anomaly detection. Q Zhou, G Pang, Y Tian, S He, J Chen, arXiv:2310.189612023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1074 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1074</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1074</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-2342b32e245989103dbc56d6f07f1400f4fd2e06</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2342b32e245989103dbc56d6f07f1400f4fd2e06" target="_blank">CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> CausalWorld is proposed, a benchmark for causal structure and transfer learning in a robotic manipulation environment that is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer.</p>
                <p><strong>Paper Abstract:</strong> Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a given set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1074.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1074.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MF-RL baselines</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model-free reinforcement learning baselines (PPO, SAC, TD3)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of standard model-free RL algorithms (Proximal Policy Optimization, Soft Actor-Critic, Twin Delayed DDPG) used as baselines to control the TriFinger simulated robot in CausalWorld across tasks of varying complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PPO / SAC / TD3 (model-free RL agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-free reinforcement learning agents: PPO (on-policy policy-gradient), SAC (off-policy maximum-entropy actor-critic), and TD3 (off-policy actor-critic with variance reduction). All were implemented with 2-layer MLP policies and trained with hand-designed dense rewards in the CausalWorld simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated robotic agent (controller for TriFinger simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (TriFinger simulation) — tasks: Pushing, Picking, Pick and Place, Stacking2 (subset used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>A parameterized robotics manipulation benchmark based on a simulated TriFinger robot where tasks are building or moving blocks into 3D goal shapes. Environments vary by task family (single-block tasks to complex multi-block constructions), and by physics/appearance parameters (gravity, floor/stage friction, block size/mass/position/color, joint positions, etc.). Complexity arises from number of objects, required long-horizon planning and precise low-level motor control; variation arises from sampling parameters from defined training (ATS) and evaluation (ES) spaces and from curricula/domain-randomization.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of blocks / task family (single-block tasks: 1 block; Stacking2: 2 blocks; Towers/Stacked/Creative/General: variable n blocks), episode length = number_of_blocks * 10 seconds, requirement for long-horizon planning and precise motor control; existence of obstacles (Pick and Place) increases complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies by task: single-block tasks = low-to-medium; Stacking2 = medium-to-high (reported as nontrivial); Towers/Stacked/Creative/General = high</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Degree of parameter sampling from ATS/ES; three curricula used: Curriculum 0 (no environment changes, low variation), Curriculum 1 (goal pose randomization in space A, medium variation), Curriculum 2 (simultaneous randomization of all variables in space A, extreme/high variation, i.e., extreme domain randomization). Also parameter ranges explicitly defined for Space A and B (e.g. gravity, friction, block size/mass, colors) and A/B disjoint sets to create in-distribution vs out-of-distribution variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Curriculum 0 = low; Curriculum 1 = medium; Curriculum 2 = high (extreme domain randomization)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Fractional volumetric overlap between goal shape and blocks (success metric in [0,1]); reported as fractional success score averaged across episodes/models; also training learning curves (fractional success vs timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative: MF-RL agents were capable of solving single-block tasks (pushing/picking/pick-and-place) with high success given sufficient training; none of the studied methods solved Stacking2 (score < 0.5 fractional success). Under Curriculum 2 (extreme randomization) agents often failed to learn any relevant skill even after large training budgets (PPO: 100M timesteps; SAC/TD3: 10M timesteps). Exact numeric averages are reported in figures (per-protocol means over 200 episodes, averaged over 5 seeds) but not given as explicit numbers in text except 'score below 0.5' for stacking2 and failure/flat curves for Curriculum 2.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explicitly discusses trade-offs: increasing environment variation (e.g., Curriculum 2 extreme domain randomization) makes learning much harder and can prevent model-free methods from acquiring useful skills; increasing task complexity (more objects, multi-object goals) also rapidly increases difficulty and can make tasks intractable for current MF-RL methods. The paper highlights that generalization depends on the training curriculum: targeted variation (e.g., goal pose randomization) improves generalization along that axis, while indiscriminate/extreme randomization can prevent learning entirely. The benchmark's ATS/ES formulation permits controlled experiments on this relationship.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Stacking2 (high complexity relative to single-block tasks) — under low-variation curricula agents still failed: fractional success < 0.5 (i.e., did not reliably stack the top block); thus poor performance even with low variation.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Single-block tasks under extreme domain randomization (Curriculum 2): agents 'rarely manage to pick up any significant success signal' and training curves are flat even after large training budgets (PPO 100M timesteps), i.e., near-zero learned success under tested budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Not successfully solved in experiments; multi-object/high-complexity tasks combined with high variation were not demonstrated to be solved — implicitly failed (no successful numeric results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Single-block tasks (Pushing, Picking, Pick and Place) under Curriculum 0: agents were 'capable of solving' these tasks given enough experience; reported as high success qualitatively (training curves reach high fractional success for these tasks), no single numerical aggregate provided in text.</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curriculum design and domain randomization: three curricula tested — (0) no environment changes (single default task), (1) goal pose randomization (sample from space A each episode), (2) full simultaneous randomization of all task variables in space A each episode (extreme domain randomization). Agents were trained with dense hand-designed rewards; PPO used distributed workers (20) up to 100M timesteps, SAC/TD3 trained serially up to 10M timesteps.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization depended on training curriculum and axis of variation: agents trained on default (Curriculum 0) generalized to some initial block poses (P4) but overfit on goal poses (P5). Agents trained with goal pose randomization (Curriculum 1) generalized robustly to different goal poses (P5). Agents trained with extreme domain randomization (Curriculum 2) failed to learn meaningful policies, thus generalization was poor or absent. The ATS/ES framework allowed disentangling in-distribution vs out-of-distribution generalization by swapping parameter spaces A and B.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>PPO: trained up to 100 million timesteps (20 workers); SAC and TD3: trained up to 10 million timesteps (serial). Five random seeds per setup. The paper notes that Curriculum 2 may require more data/optimization and that current budgets were insufficient for effective learning in that regime.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) MF-RL baselines can learn single-block tasks in CausalWorld under low-to-moderate variation, but struggle or fail on multi-object tasks (Stacking2 and above). 2) Training curriculum crucially shapes generalization: targeted variation (e.g., goal pose randomization) improves generalization along that axis, while extreme simultaneous randomization prevents learning with the tested sample budgets. 3) There is an explicit trade-off: higher environment variation and higher task complexity both make learning harder; combined they can render current model-free methods ineffective. 4) The benchmark's parametrization (ATS/ES, intervention actors) enables controlled study of these complexity-variation relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1074.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1074.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TriFinger (simulated embodiment)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TriFinger open-source robot (simulated in Bullet) — embodied manipulation platform</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The TriFinger robot platform (open-source) is simulated in CausalWorld (Bullet physics); it provides a low-cost real-world counterpart for sim2real research and serves as the embodied agent's robot to perform manipulation tasks in the benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Trifinger: An opensource robot for learning dexterity</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TriFinger simulated robot (embodiment used by agents)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A three-finger dexterous robotic platform (simulated) used as the physical embodiment in CausalWorld; agent policies output control signals (joint positions, torques, end-effector deltas) to drive the TriFinger in tasks of manipulating blocks to match goal shapes.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated physical robot (with a real-world buildable counterpart)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>CausalWorld (TriFinger arena with blocks and variable physics/appearance parameters)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>The TriFinger arena contains blocks and a goal shape; observations can be structured vectors or pixel images (six RGB images, 3 views of current state + 3 goal images). The environment exposes many variables (gravity, floor/stage friction and color, joint positions, block size/mass/color/pose, link masses/colors, goal shape, etc.) enabling controlled variation and curricula. Complexity scales with number of blocks and goal shape complexity; variation is controlled via sampling from defined parameter spaces (Space A and Space B) and intervention actors.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same as CausalWorld: number of blocks (1..n), task family (Pushing, Picking, Pick and Place, Stacking2, Towers, Stacked Blocks, Creative, General), episode time = number_of_blocks * 10s, plus physical parameters affecting dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Low to high depending on task family: single-block tasks low, stacking and creative tasks high.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Parameter ranges for Space A and Space B (numeric ranges in Table 2), ATS vs ES selection, intervention actors that sample parameters per episode or within episodes (domain randomization). Example numeric ranges: gravity z in A=[-10,-7], B=[-7,-4]; block mass A=[0.015,0.045], B=[0.045,0.1]; block size A=[0.055,0.075]^3, B=[0.075,0.095]^3, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Explicitly controllable from low to high; experiments used low (no changes), medium (goal randomization), and high (simultaneous full randomization) variation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Fractional volumetric overlap (shared success metric between 0 and 1); evaluation protocols compute mean fractional success at final timestep over 200 episodes averaged across seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Same experimental outcomes as reported for MF-RL baselines: TriFinger-controlled policies solved single-block tasks under low/targeted variation, failed on stacking two blocks (score < 0.5), and failed to learn under extreme simultaneous randomization within provided training budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper emphasizes that the embodied TriFinger tasks demonstrate a clear relationship: as task complexity (more blocks, structural goals) increases, task difficulty rises rapidly; adding broad parameter variation (high domain randomization) further increases difficulty and can prevent learning. The TriFinger embodiment enables realistic physical interactions underlying these effects.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>Stacking2 with TriFinger under low-variation curricula: fractional success < 0.5 (agents failed to reliably stack the second block).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Single-block tasks under extreme randomization with TriFinger: agents 'rarely manage to pick up any significant success signal' in the tested budgets (near-zero learning progress reported).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Not solved in experiments; results indicate failure or no successful policies reported.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>Single-block tasks under default TriFinger settings: agents achieved successful policies (qualitatively high fractional success) given sufficient training (PPO up to 100M timesteps, SAC/TD3 up to 10M).</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Same curricula description: no changes vs goal randomization vs full variable randomization; observation modes include structured vectors and pixel images supporting different input modalities.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>TriFinger policies trained with targeted variation generalized to corresponding axes (e.g., goal pose randomization improved generalization to novel goal poses), whereas broad simultaneous randomization prevented forming robust policies within the tested training budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Policies controlling TriFinger required large sample budgets for complex/varied settings: PPO used 100M timesteps (distributed); SAC/TD3 used 10M timesteps (serial); more data likely needed for extreme variation settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The TriFinger embodiment in CausalWorld makes complexity-variation trade-offs concrete: tasks that are physically simple but trained under targeted variation generalize well on that axis; increasing physical complexity (multi-block tasks) or increasing broad unobserved variation (extreme domain randomization) markedly reduces learning success with current model-free methods and sample budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning <em>(Rating: 2)</em></li>
                <li>RLBench: The robot learning benchmark & learning environment <em>(Rating: 2)</em></li>
                <li>Trifinger: An opensource robot for learning dexterity <em>(Rating: 2)</em></li>
                <li>Quantifying generalization in reinforcement learning <em>(Rating: 1)</em></li>
                <li>CoinRun <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1074",
    "paper_id": "paper-2342b32e245989103dbc56d6f07f1400f4fd2e06",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "MF-RL baselines",
            "name_full": "Model-free reinforcement learning baselines (PPO, SAC, TD3)",
            "brief_description": "A set of standard model-free RL algorithms (Proximal Policy Optimization, Soft Actor-Critic, Twin Delayed DDPG) used as baselines to control the TriFinger simulated robot in CausalWorld across tasks of varying complexity and variation.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "PPO / SAC / TD3 (model-free RL agents)",
            "agent_description": "Model-free reinforcement learning agents: PPO (on-policy policy-gradient), SAC (off-policy maximum-entropy actor-critic), and TD3 (off-policy actor-critic with variance reduction). All were implemented with 2-layer MLP policies and trained with hand-designed dense rewards in the CausalWorld simulator.",
            "agent_type": "simulated robotic agent (controller for TriFinger simulation)",
            "environment_name": "CausalWorld (TriFinger simulation) — tasks: Pushing, Picking, Pick and Place, Stacking2 (subset used in experiments)",
            "environment_description": "A parameterized robotics manipulation benchmark based on a simulated TriFinger robot where tasks are building or moving blocks into 3D goal shapes. Environments vary by task family (single-block tasks to complex multi-block constructions), and by physics/appearance parameters (gravity, floor/stage friction, block size/mass/position/color, joint positions, etc.). Complexity arises from number of objects, required long-horizon planning and precise low-level motor control; variation arises from sampling parameters from defined training (ATS) and evaluation (ES) spaces and from curricula/domain-randomization.",
            "complexity_measure": "Number of blocks / task family (single-block tasks: 1 block; Stacking2: 2 blocks; Towers/Stacked/Creative/General: variable n blocks), episode length = number_of_blocks * 10 seconds, requirement for long-horizon planning and precise motor control; existence of obstacles (Pick and Place) increases complexity.",
            "complexity_level": "Varies by task: single-block tasks = low-to-medium; Stacking2 = medium-to-high (reported as nontrivial); Towers/Stacked/Creative/General = high",
            "variation_measure": "Degree of parameter sampling from ATS/ES; three curricula used: Curriculum 0 (no environment changes, low variation), Curriculum 1 (goal pose randomization in space A, medium variation), Curriculum 2 (simultaneous randomization of all variables in space A, extreme/high variation, i.e., extreme domain randomization). Also parameter ranges explicitly defined for Space A and B (e.g. gravity, friction, block size/mass, colors) and A/B disjoint sets to create in-distribution vs out-of-distribution variation.",
            "variation_level": "Curriculum 0 = low; Curriculum 1 = medium; Curriculum 2 = high (extreme domain randomization)",
            "performance_metric": "Fractional volumetric overlap between goal shape and blocks (success metric in [0,1]); reported as fractional success score averaged across episodes/models; also training learning curves (fractional success vs timesteps).",
            "performance_value": "Qualitative: MF-RL agents were capable of solving single-block tasks (pushing/picking/pick-and-place) with high success given sufficient training; none of the studied methods solved Stacking2 (score &lt; 0.5 fractional success). Under Curriculum 2 (extreme randomization) agents often failed to learn any relevant skill even after large training budgets (PPO: 100M timesteps; SAC/TD3: 10M timesteps). Exact numeric averages are reported in figures (per-protocol means over 200 episodes, averaged over 5 seeds) but not given as explicit numbers in text except 'score below 0.5' for stacking2 and failure/flat curves for Curriculum 2.",
            "complexity_variation_relationship": "Yes — the paper explicitly discusses trade-offs: increasing environment variation (e.g., Curriculum 2 extreme domain randomization) makes learning much harder and can prevent model-free methods from acquiring useful skills; increasing task complexity (more objects, multi-object goals) also rapidly increases difficulty and can make tasks intractable for current MF-RL methods. The paper highlights that generalization depends on the training curriculum: targeted variation (e.g., goal pose randomization) improves generalization along that axis, while indiscriminate/extreme randomization can prevent learning entirely. The benchmark's ATS/ES formulation permits controlled experiments on this relationship.",
            "high_complexity_low_variation_performance": "Stacking2 (high complexity relative to single-block tasks) — under low-variation curricula agents still failed: fractional success &lt; 0.5 (i.e., did not reliably stack the top block); thus poor performance even with low variation.",
            "low_complexity_high_variation_performance": "Single-block tasks under extreme domain randomization (Curriculum 2): agents 'rarely manage to pick up any significant success signal' and training curves are flat even after large training budgets (PPO 100M timesteps), i.e., near-zero learned success under tested budgets.",
            "high_complexity_high_variation_performance": "Not successfully solved in experiments; multi-object/high-complexity tasks combined with high variation were not demonstrated to be solved — implicitly failed (no successful numeric results reported).",
            "low_complexity_low_variation_performance": "Single-block tasks (Pushing, Picking, Pick and Place) under Curriculum 0: agents were 'capable of solving' these tasks given enough experience; reported as high success qualitatively (training curves reach high fractional success for these tasks), no single numerical aggregate provided in text.",
            "training_strategy": "Curriculum design and domain randomization: three curricula tested — (0) no environment changes (single default task), (1) goal pose randomization (sample from space A each episode), (2) full simultaneous randomization of all task variables in space A each episode (extreme domain randomization). Agents were trained with dense hand-designed rewards; PPO used distributed workers (20) up to 100M timesteps, SAC/TD3 trained serially up to 10M timesteps.",
            "generalization_tested": true,
            "generalization_results": "Generalization depended on training curriculum and axis of variation: agents trained on default (Curriculum 0) generalized to some initial block poses (P4) but overfit on goal poses (P5). Agents trained with goal pose randomization (Curriculum 1) generalized robustly to different goal poses (P5). Agents trained with extreme domain randomization (Curriculum 2) failed to learn meaningful policies, thus generalization was poor or absent. The ATS/ES framework allowed disentangling in-distribution vs out-of-distribution generalization by swapping parameter spaces A and B.",
            "sample_efficiency": "PPO: trained up to 100 million timesteps (20 workers); SAC and TD3: trained up to 10 million timesteps (serial). Five random seeds per setup. The paper notes that Curriculum 2 may require more data/optimization and that current budgets were insufficient for effective learning in that regime.",
            "key_findings": "1) MF-RL baselines can learn single-block tasks in CausalWorld under low-to-moderate variation, but struggle or fail on multi-object tasks (Stacking2 and above). 2) Training curriculum crucially shapes generalization: targeted variation (e.g., goal pose randomization) improves generalization along that axis, while extreme simultaneous randomization prevents learning with the tested sample budgets. 3) There is an explicit trade-off: higher environment variation and higher task complexity both make learning harder; combined they can render current model-free methods ineffective. 4) The benchmark's parametrization (ATS/ES, intervention actors) enables controlled study of these complexity-variation relationships.",
            "uuid": "e1074.0",
            "source_info": {
                "paper_title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "TriFinger (simulated embodiment)",
            "name_full": "TriFinger open-source robot (simulated in Bullet) — embodied manipulation platform",
            "brief_description": "The TriFinger robot platform (open-source) is simulated in CausalWorld (Bullet physics); it provides a low-cost real-world counterpart for sim2real research and serves as the embodied agent's robot to perform manipulation tasks in the benchmark.",
            "citation_title": "Trifinger: An opensource robot for learning dexterity",
            "mention_or_use": "use",
            "agent_name": "TriFinger simulated robot (embodiment used by agents)",
            "agent_description": "A three-finger dexterous robotic platform (simulated) used as the physical embodiment in CausalWorld; agent policies output control signals (joint positions, torques, end-effector deltas) to drive the TriFinger in tasks of manipulating blocks to match goal shapes.",
            "agent_type": "simulated physical robot (with a real-world buildable counterpart)",
            "environment_name": "CausalWorld (TriFinger arena with blocks and variable physics/appearance parameters)",
            "environment_description": "The TriFinger arena contains blocks and a goal shape; observations can be structured vectors or pixel images (six RGB images, 3 views of current state + 3 goal images). The environment exposes many variables (gravity, floor/stage friction and color, joint positions, block size/mass/color/pose, link masses/colors, goal shape, etc.) enabling controlled variation and curricula. Complexity scales with number of blocks and goal shape complexity; variation is controlled via sampling from defined parameter spaces (Space A and Space B) and intervention actors.",
            "complexity_measure": "Same as CausalWorld: number of blocks (1..n), task family (Pushing, Picking, Pick and Place, Stacking2, Towers, Stacked Blocks, Creative, General), episode time = number_of_blocks * 10s, plus physical parameters affecting dynamics.",
            "complexity_level": "Low to high depending on task family: single-block tasks low, stacking and creative tasks high.",
            "variation_measure": "Parameter ranges for Space A and Space B (numeric ranges in Table 2), ATS vs ES selection, intervention actors that sample parameters per episode or within episodes (domain randomization). Example numeric ranges: gravity z in A=[-10,-7], B=[-7,-4]; block mass A=[0.015,0.045], B=[0.045,0.1]; block size A=[0.055,0.075]^3, B=[0.075,0.095]^3, etc.",
            "variation_level": "Explicitly controllable from low to high; experiments used low (no changes), medium (goal randomization), and high (simultaneous full randomization) variation settings.",
            "performance_metric": "Fractional volumetric overlap (shared success metric between 0 and 1); evaluation protocols compute mean fractional success at final timestep over 200 episodes averaged across seeds.",
            "performance_value": "Same experimental outcomes as reported for MF-RL baselines: TriFinger-controlled policies solved single-block tasks under low/targeted variation, failed on stacking two blocks (score &lt; 0.5), and failed to learn under extreme simultaneous randomization within provided training budgets.",
            "complexity_variation_relationship": "The paper emphasizes that the embodied TriFinger tasks demonstrate a clear relationship: as task complexity (more blocks, structural goals) increases, task difficulty rises rapidly; adding broad parameter variation (high domain randomization) further increases difficulty and can prevent learning. The TriFinger embodiment enables realistic physical interactions underlying these effects.",
            "high_complexity_low_variation_performance": "Stacking2 with TriFinger under low-variation curricula: fractional success &lt; 0.5 (agents failed to reliably stack the second block).",
            "low_complexity_high_variation_performance": "Single-block tasks under extreme randomization with TriFinger: agents 'rarely manage to pick up any significant success signal' in the tested budgets (near-zero learning progress reported).",
            "high_complexity_high_variation_performance": "Not solved in experiments; results indicate failure or no successful policies reported.",
            "low_complexity_low_variation_performance": "Single-block tasks under default TriFinger settings: agents achieved successful policies (qualitatively high fractional success) given sufficient training (PPO up to 100M timesteps, SAC/TD3 up to 10M).",
            "training_strategy": "Same curricula description: no changes vs goal randomization vs full variable randomization; observation modes include structured vectors and pixel images supporting different input modalities.",
            "generalization_tested": true,
            "generalization_results": "TriFinger policies trained with targeted variation generalized to corresponding axes (e.g., goal pose randomization improved generalization to novel goal poses), whereas broad simultaneous randomization prevented forming robust policies within the tested training budgets.",
            "sample_efficiency": "Policies controlling TriFinger required large sample budgets for complex/varied settings: PPO used 100M timesteps (distributed); SAC/TD3 used 10M timesteps (serial); more data likely needed for extreme variation settings.",
            "key_findings": "The TriFinger embodiment in CausalWorld makes complexity-variation trade-offs concrete: tasks that are physically simple but trained under targeted variation generalize well on that axis; increasing physical complexity (multi-block tasks) or increasing broad unobserved variation (extreme domain randomization) markedly reduces learning success with current model-free methods and sample budgets.",
            "uuid": "e1074.1",
            "source_info": {
                "paper_title": "CausalWorld: A Robotic Manipulation Benchmark for Causal Structure and Transfer Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "RLBench: The robot learning benchmark & learning environment",
            "rating": 2
        },
        {
            "paper_title": "Trifinger: An opensource robot for learning dexterity",
            "rating": 2
        },
        {
            "paper_title": "Quantifying generalization in reinforcement learning",
            "rating": 1
        },
        {
            "paper_title": "CoinRun",
            "rating": 1
        }
    ],
    "cost": 0.01441175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>CAUSALWORLD: A ROBOTIC MANIPULATION BENCHMARK FOR CAUSAL STRUCTURE AND TRANSFER LEARNING</h1>
<p>Ossama Ahmed ${ }^{1, <em>}$ Frederik Träuble ${ }^{2, </em>}$ Anirudh Goyal ${ }^{3}$ Alexander Neitz ${ }^{2}$<br>Yoshua Bengio ${ }^{3}$ Bernhard Schölkopf ${ }^{2}$ Stefan Bauer ${ }^{2, \dagger}$ Manuel Wüthrich ${ }^{2, \dagger}$</p>
<h4>Abstract</h4>
<p>Despite recent successes of reinforcement learning (RL), it remains a challenge for agents to transfer learned skills to related environments. To facilitate research addressing this problem, we propose CausalWorld, a benchmark for causal structure and transfer learning in a robotic manipulation environment. The environment is a simulation of an open-source robotic platform, hence offering the possibility of sim-to-real transfer. Tasks consist of constructing 3D shapes from a given set of blocks - inspired by how children learn to build complex structures. The key strength of CausalWorld is that it provides a combinatorial family of such tasks with common causal structure and underlying factors (including, e.g., robot and object masses, colors, sizes). The user (or the agent) may intervene on all causal variables, which allows for fine-grained control over how similar different tasks (or task distributions) are. One can thus easily define training and evaluation distributions of a desired difficulty level, targeting a specific form of generalization (e.g., only changes in appearance or object mass). Further, this common parametrization facilitates defining curricula by interpolating between an initial and a target task. While users may define their own task distributions, we present eight meaningful distributions as concrete benchmarks, ranging from simple to very challenging, all of which require long-horizon planning as well as precise low-level motor control. Finally, we provide baseline results for a subset of these tasks on distinct training curricula and corresponding evaluation protocols, verifying the feasibility of the tasks in this benchmark. ${ }^{4}$</p>
<h2>1 INTRODUCTION</h2>
<p>Benchmarks have played a crucial role in advancing entire research fields, for instance computer vision with the introduction of CIFAR-10 and ImageNet (Krizhevsky et al., 2009; 2012). When it comes to the field of reinforcement learning (RL), similar breakthroughs have been achieved in domains such as game playing (Mnih et al., 2013; Silver et al., 2017), learning motor control for high-dimensional simulated robots (Akkaya et al., 2019), multi-agent settings (Baker et al., 2019; Berner et al., 2019) and for studying transfer in the context of meta-learning (Yu et al., 2019). Nevertheless, trained agents often fail to transfer the knowledge about the learned skills from a training environment to a different but related environment sharing part of the underlying task structure. This can be attributed to the fact that it is quite common to evaluate an agent on the training environments themselves, which leads to overfitting on these narrowly defined environments (Whiteson et al., 2011), or that algorithms are</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: Example tasks from the task generators provided in the benchmark. The goal shape is visualized in opaque red and the blocks in blue.
compared using highly engineered and biased reward functions which may result in learning suboptimal policies with respect to the desired behaviour; this is particularly evident in robotics.</p>
<p>In existing benchmarks (Yu et al., 2019; Goyal et al., 2019a; Cobbe et al., 2018; Bellemare et al., 2013; James et al., 2020) the amount of shared causal structure between the different environments is mostly unknown. For instance, in the Atari Arcade Learning environments, it is unclear how to quantify the underlying similarities between different Atari games and we generally do not know to which degree an agent can be expected to generalize. To overcome these limitations, we introduce a novel benchmark in a robotic manipulation environment which we call CausalWorld. It features a diverse set of environments which, in contrast to previous designs, share a large set of parameters and parts of the causal structure. Being able to intervene on these parameters (individually or collectively) permits the experimenter to evaluate agents' generalization abilities with respect to different types and extents of changes in the environment. These parameters can be varied gradually, which yields a continuum of similar environments. This allows for fine-grained control of training and test distributions and the design of learning curricula.</p>
<p>A remarkable skill that humans learn to master relatively early on in their life is building complex structures using their spatial-reasoning and dexterous manipulation abilities (Casey et al., 2008; Caldera et al., 1999; Kamii et al., 2004). Playing with toy blocks constitutes a natural environment for children to develop important visual-spatial skills, helping them 'generalize' in building complex composition designs from presented or imagined goal structures (Verdine et al., 2017; Nath \&amp; Szücs, 2014; Dewar, 2018; Richardson et al., 2014). Inspired by this, CausalWorld is designed to aid in learning and investigating these skills in a corresponding simulated robotics manipulation environment of the open-source TriFinger robot platform from Wüthrich et al. (2020), which can be built in the real world. Tasks are formulated as building 3D goal shapes using a set of available blocks by manipulating them - as seen in Fig. 1. This yields a diverse familiy of tasks, ranging from relatively simple (e.g. pushing a single object) to extremely hard (e.g. building a complex structure from a large number of objects).</p>
<p>CausalWorld improves upon previous benchmarks by exposing a large set of parameters in the causal generative model of the environments, such as weight, shape and appearance of the building blocks and the robot itself. The possibility of intervening on any of these properties at any point in time allows one to set up training curricula or to evaluate an agent's generalization capability with respect to different parameters. Furthermore, in contrast to previous benchmarks (ChevalierBoisvert et al., 2018; Cobbe et al., 2018), researchers may build their own real-world platform of this simulator at low cost, as detailed in Wüthrich et al. (2020), and transfer their trained policies to the real world.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Benchmark</th>
<th style="text-align: center;">do- <br> interventions <br> interface</th>
<th style="text-align: center;">procedurally <br> generated <br> environ- <br> ments</th>
<th style="text-align: center;">online distribution of tasks</th>
<th style="text-align: center;">setup <br> custom <br> curric- <br> ula</th>
<th style="text-align: center;">disentangle <br> general- <br> ization <br> ability</th>
<th style="text-align: center;">real- <br> world <br> similarity</th>
<th style="text-align: center;">open- <br> source <br> robot</th>
<th style="text-align: center;">low- <br> level <br> motor <br> control</th>
<th style="text-align: center;">long- <br> term <br> plan- <br> ning</th>
<th style="text-align: center;">unified <br> success <br> metric</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RLBench</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
</tr>
<tr>
<td style="text-align: center;">MetaWorld</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
</tr>
<tr>
<td style="text-align: center;">IKEA</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">MuJoBan</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">BabyAI</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">CoinRun</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">AtariArcade</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark / \boldsymbol{\mathcal { R }}$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: center;">CausalWorld</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of Causal World with RLBench (James et al., 2020), MetaWorld (Yu et al., 2019), IKEA (Lee et al., 2019), BabyAI (Chevalier-Boisvert et al., 2018), CoinRun (Cobbe et al., 2018), AtariArcade (Bellemare et al., 2013), MuJoBan etc. (Mirza et al., 2020),</p>
<p>Finally, by releasing this benchmark we hope to facilitate research in causal structure learning, i.e. learning the causal graph (or certain aspects of it) as we operate in a complex real-world environment whose dynamics follow the laws of physics which induce causal relations between the variables. Changes to the variables we expose can be considered do-interventions on the underlying structural causal model (SCM). Consequently, we believe that this benchmark offers an exciting opportunity to investigate causality and its connection to RL and robotics.</p>
<p>Our main contributions can be summarized as follows:</p>
<ul>
<li>We propose CausalWorld, a new benchmark comprising a parametrized family of robotic manipulation environments for advancing out-of-distribution generalization and causal structure learning in RL.</li>
<li>We provide a systematic way of defining curricula and disentangling generalization abilities of trained agents by allowing do-interventions to be performed on all environment variables (parameters and states).</li>
<li>We establish baseline results for some of the available tasks under different learning algorithms, thus verifying the feasibility of the tasks.</li>
<li>We show how different learning curricula affect generalization across different axes by reporting some of the in-distribution and out-of-distribution generalization capabilities of the trained agents.</li>
</ul>
<h1>2 CAUSALWORLD BENCHMARK</h1>
<p>Here we make the desiderata outlined in the introduction more precise:</p>
<ol>
<li>The set of environments should be sufficiently diverse to allow for the design of challenging transfer tasks.</li>
<li>We need to be able to intervene on different properties (e.g. masses, colors) individually, such that we can investigate different types of generalization.</li>
<li>It should be possible to convert any environment to any other environment by gradually changing its properties through interventions; this requirement is important for evaluating different levels of transfer and for defining curricula.</li>
<li>The environments should share some causal structure to allow algorithms to transfer the learned causal knowledge from one environment to another.</li>
<li>There should be a unified measure of success, such that an objective comparison can be made between different learning algorithms.</li>
<li>The benchmark should make it easy for users to define meaningful distributions of environments for training and evaluation. In particular, it should facilitate evaluation of indistribution and out-of-distribution performance.</li>
<li>The simulated benchmark should have a real-world counterpart to allow for sim2real.</li>
</ol>
<p>In light of these desiderata, we propose a setup in which a robot must build goal shapes using a set of available objects. It is worth noting that similar setups were proposed previously in a less realistic setting as in (Janner et al., 2018; Bapst et al., 2019; McCarthy et al.; Akkaya et al., 2019; Fahlman, 1974; Winston, 1970; Winograd, 1972). Specifically, a task is formulated as follows: given a set of available objects the agent needs to build a specific goal structure, see Fig. 1 for an example. The vast amount of possible target shapes and environment properties (e.g. mass, shape and appearance of objects and the robot itself) makes this a diverse and challenging setting to evaluate different generalization aspects. CausalWorld is a simulated version (using the Bullet physics engine (Coumans et al., 2013)) of the open-source TriFinger robot platform from Wüthrich et al. (2020). Each environment is defined by a set of variables such as gravity, floor friction, stage color, floor color, joint positions, various block parameters (e.g. size, color, mass, position, orientation), link colors, link masses and the goal shape. See Table 3 in the Appendix for a subset of these variables.</p>
<p>Desideratum 1 is satisfied since different environment properties and goal shapes give rise to very different tasks, ranging from relatively easy (e.g. re-positioning a single cube) to extremely hard (e.g. building a complex structure). Desideratum 2 is satisfied because we allow for arbitrary interventions on these properties, hence users or agents may change parameters individually or jointly. Desideratum 3 is satisfied because the parameters can be changed gradually. Desideratum 4 is satisfied because all the environments share the causal structure of the robot, and one may also use subsets of environments which share even more causal structure. We satisfy desideratum 5 by defining the measure of success for all environments as the volumetric overlap of the goal shape with available objects. Further, by splitting the set of parameters into a set A, intended for training and in-distribution evaluation, and a set B, intended for out-of-distribution evaluation, we satisfy desideratum 6. Finally, since the TriFinger robot (Wüthrich et al., 2020) can be built in the real-world, we satisfy desideratum 7. Desideratum 7 and 2 are in partial conflict since sim2real is only possible for the tasks which are constrained to the variables on which the robot can physically act upon.</p>
<p>Task generators: To generate meaningful families of similar goal shapes, CausalWorld allows for defining task generators which can generate a variety of different goal shapes in an environment. For instance, one task generator may generate pushing tasks, while another one may generate towerbuilding tasks (see Fig. 2). Each task generator is initialized with a default goal shape from its corresponding family and comes with a sampler to sample new goal shapes from the same family. Additionally, upon construction, one can specify the environments' initial state and initial goal shape structure when deviating from the default. The maximum episode time to build a given shape is number_of_blocks $\times 10$ seconds. CausalWorld comes with eight pre-defined task generators (see Fig. 2).</p>
<ul>
<li>Three generators create goal shapes with a single block: Pushing with the goal shape on the floor, Picking having the goal shape defined above the floor and Pick and Place where a fixed obstacle is placed between the initial block and goal pose.</li>
<li>Stacking2 involves a goal shape of two stacked blocks, which can also be considered one instance of the towers generator.</li>
<li>The remaining generators use a variable number of blocks to generate much more complex and challenging target shapes with details in the appendix: Towers, Stacked Blocks, Creative Stacked Blocks and General.</li>
</ul>
<p>Given that building new environments using current physics simulators is often tedious, we provide a simple API for users who wish to create new task generators, for new challenging shape families which may be added to CausalWorld's task generators repository.</p>
<p>Action and Observation Spaces: The robot's action space $A \in \mathbb{R}^{9}$ can be chosen to operate in either joint position control mode, joint torque control mode, end-effector position control mode, or the delta of each. To address the different challenges in using high-dimensional visual observations as well as using a structured representation, we provide two observation modes: structured as well as pixel. In the structured mode, the low-dimensional observation vector o follows a common rule for the ordering of the relevant variables, such as joints position, joints velocity, blocks linear velocity, time left for the task..etc. Thus, the observation space size depends on the number of blocks, which could potentially change with every new goal sampled, e.g. in Towers, (Creative) Stacked</p>
<p>Blocks and General; therefore $O \in \mathbb{R}^{d_{o}}$ where $d_{o}$ varies across different environments. On the contrary, in the pixel mode, the agent receives six different RGB images, where $O \in \mathbb{R}^{6 \times 3 \times 128 \times 128}$, the first three images are rendered from different cameras mounted around the TriFinger robot, and the last three images specify the goal image of the target shape rendered from the same cameras. This mode can be mirrored on the real robotic platform and aids in investigating object-based learning approaches from pixel data as well as learning visual goal conditioned policies. Additionally, CausalWorld allows for setting up a fully customized observation space, if needed.</p>
<p>Rewards: The reward function $r$ is defined uniformly across all possible goal shapes as the fractional volumetric overlap of the blocks with the goal shape, which ranges between 0 (no overlap) and 1 (complete overlap). This shared success metric can be returned at each time step where its scale is independent of the goal shape itself. Thus, an agent that learned this shared reward function $r$ from several different tasks could in principle use it to solve unseen goal structures. There is also the possibility of modifying the reward function to 1) sparsify the reward further by returning a binary reward signal instead, or 2) add a dense reward function in order to introduce inductive biases via domain knowledge and solution guidance. We hope that the considerable complexity and diversity of goal shapes motivate and accelerate the development of algorithms that are not dependent on highly tuned reward functions anymore.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Key components for generic training and evaluation of RL agents. Left: A learning curriculum which is composed of various intervention actors that decide on which variables to intervene on (for a valid intervention, values need to be in the allowed training space (ATS)). Right: Evaluation protocols are shown which may intervene on variables at episode resets or within episodes (for a valid intervention, values need to be in the evaluation space (ES)). Middle: we represent the ATS and ES, where each intervention results in one point in the spaces. As shown ATS and ES may intersect, eg. if the protocols are meant to evaluate in-distribution generalization. A learning curriculum is represented by subsequent interventions navigating the ATS resulting in the corresponding points in the space.</p>
<p>Training and evaluation spaces: In this benchmark, a learning setting consists of an allowed training space (ATS) and an evaluation space (ES), both of which are subspaces of the full parameter space. During training, in the simplest setting, parameters are sampled iid from the ATS. However, unlike existing benchmarks, CausalWorld allows for curricula within the ATS as well as settings where the agent itself intervenes on the parameters within an episode (see Fig. 3). Similarly, during evaluation, parameters may be sampled iid from the evaluation space at each episode reset, or there can be interventions within an episode. Moreover, in order to retrieve the setting considered in most RL benchmarks, we could set the ATS and the ES to be identical and intervene only on object and robot states (and keep other environment properties constant) at each episode reset. However, to evaluate out-of-distribution generalization, one should set the two spaces (ATS and ES) to be different; possibly even disjoint. Additionally, to evaluate robustness with respect to a specific parameter (e.g. object mass), one may define the training and evaluation spaces to only differ in that particular parameter. In order to facilitate the definition of appropriate training and evaluation settings, we pre-define two disjoint sets, $\mathbf{A}<em i="i">{i}$ and $\mathbf{B}</em>}$, for each parameter $i$. Through this, one can for instance define the training space to be $\mathbf{A<em 2="2">{1} \times \mathbf{A}</em>} \times \ldots$ and the evaluation space to be $\mathbf{B<em 2="2">{1} \times \mathbf{B}</em>} \times \ldots$ to assess generalization with respect to all parameters simultaneously. Alternatively, the evaluation space could be defined as $\mathbf{A<em 2="2">{1} \times \mathbf{A}</em>} \times \ldots \times \mathbf{B<em i_1="i+1">{i} \times \mathbf{A}</em> \times \ldots$ to assess generalization with respect to parameter $i$ only. Lastly, users may also define their own spaces which could then be integrated into the benchmark to give rise to new learning settings.</p>
<p>Intervention actors: To provide a convenient way of specifying learning curricula, we introduce intervention actors. At each time step, such an actor takes all the exposed variables of the environment as inputs and may intervene on them. To encourage modularity, one may combine multiple actors in a learning curriculum. This actor is defined by the episode number to start intervening, the episode number to stop intervening, the timestep within the episode it should intervene and the episode periodicity of interventions. We provide a set of predefined intervention actors, including an actor which samples parameters randomly at each episode reset, which corresponds to domainrandomization. It is also easy to define custom intervention actors, we hope that this facilitates investigation into optimal learning curricula (see Fig. 3).</p>
<h1>3 Related Work</h1>
<p>Previous benchmarks proposed for RL mostly focused on the single task learning setting such as OpenAI Gym and DM control suite (Tassa et al., 2018; Brockman et al., 2016). Although, a recent line of work, e.g. Meta-World and RLBench (Yu et al., 2019; James et al., 2020) aim at studying multi-task learning as well as meta-learning, respective benchmarks mostly exhibit nonparametric hand-designed task variations which makes it ambiguous and not explicit how much structure is shared between them. For instance, it is not clear how different it is to "open a door" compared to "opening a drawer". To address the ambiguity in the shared structure between the tasks, CausalWorld was designed to allow interventions to be performed on many environment variables giving rise to a large space of tasks with well-defined relations between them, which we believe is a missing key component to address generalization in RL.</p>
<p>Similar parametric formulations of different environments were used in experiments in the generalization for RL literature, which have played an important role in advancing the field (Packer et al., 2018; Rajeswaran et al., 2017; Pinto et al., 2017; Yu et al., 2017; Henderson et al., 2017a; DulacArnold et al., 2020; Chevalier-Boisvert et al., 2018). In these previous works, variables were mostly assigned randomly as opposed to the full control over the variables in CausalWorld by allowing do-interventions.</p>
<p>Another important remaining challenge for the RL community is the standardization of the reported learning curves and results. RL methods have been shown to be sensitive to a range of different factors (Henderson et al., 2017b). Thus it is crucial to devise a set of metrics that measure reliability of RL algorithms and ensure their reproducibility. Chan et al. (2019) distinguishes between several evaluation modes like "evaluation during training" and "evaluation after learning". Osband et al. (2019) recently proposed a benchmarking suite that disentangles the ability of an algorithm to deal with different types of challenges. Its main components are: enforcing a specific methodology for an agent's evaluation beyond the environment definition and isolating core capabilities with targeted 'unit tests' rather than integrating the general learning ability.
Moreover, causality has been historically studied from the perspective of probabilistic and causal reasoning (Pearl, 2009), cognitive psychology (Griffiths \&amp; Tenenbaum, 2005), and more recently in the context of machine learning (Goyal et al., 2019b; Schölkopf, 2019; Baradel et al., 2019; Bakhtin et al., 2019). On the contrary, we believe its link to robotics is not yet drawn systematically. To bridge this gap, one of the main motivations of CausalWorld was to facilitate research in causal learning for robotics, such as the capacity for observational discovery of causal effects in physical reality, counterfactual reasoning and causal structure learning.</p>
<h2>4 EXPERIMENTS</h2>
<p>To illustrate the usage of this benchmark and to verify the feasibility of some basic tasks, we evaluate current state-of-the-art model-free (MF-RL) algorithms on a subset of the goal shape families described in Section 2 and depticed in Fig. 2: (a) Pushing, (b) Picking, (c) Pick and Place, and (d) Stacking2. These goal shapes reflect basic skills that are required to solve more complex construction tasks.</p>
<p>Setup: The idea here is to investigate how well an agent will perform on different evaluation distributions, depending on the curriculum it has been trained with. We train each method under the following curricula:</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Fractional success curves averaged over five random seeds for the tasks and learning algorithms specified above, under three different training curricula: (0) no curriculum, (1) goal position and orientation randomization in space $\mathbf{A}$ every episode and (2) a curriculum where we intervene on all variables in space $\mathbf{A}$ simultaneously every episode.</p>
<ul>
<li>Curriculum 0: no environment changes; each episode is initialized from the default task lying in space A - note that here the initial state never changes (i.e. no interventions).</li>
<li>Curriculum 1: goal shape randomization; at the beginning of each episode a new goal shape is sampled from space $\mathbf{A}$ (i.e. interventions on goal position and orientation).</li>
<li>Curriculum 2: full randomization w.r.t. the task variables ${ }^{5}$; every episode a simultaneous intervention on all variables is sampled from space $\mathbf{A}$ (i.e. can be seen as equivalent to extreme domain randomization in one space).</li>
</ul>
<p>The curriculum will, as expected, affect the generalization capabilities of the trained agents. With CausalWorld's formulation, these generalization capabilities can easily be disentangled and benchmarked quantitatively, as explained in Section 2. For each of the goal shape families (a, b, c, d from Fig. 2), we train agents under the three described curricula using the following MF-RL algorithms: The original Proximal Policy Optimization (PPO) from Schulman et al. (2017), Soft Actor-Critic (SAC) from Haarnoja et al. (2018) and the Twin Delayed DDPG (TD3) from Fujimoto et al. (2018). We provided these methods with a hand-designed dense reward function as we did not observe any success with the sparse reward only. Each of the mentioned setups is trained for five different random seeds, resulting in 180 trained agents.</p>
<p>Training model-free RL methods: We report the training curves averaged over the random seeds in Fig. 4. As can be seen from these fractional success training curves, MF-RL methods are capable of solving the single block goal shapes (pushing, picking, pick and place) seen during training time given enough experience. However, we observe that none of the methods studied here managed to solve stacking two blocks. The score below 0.5 indicates that it only learns to push the lower cube into the goal shape. This shows that multi-object target shapes can become nontrivial quickly and that there is a need for better methods making use of the modular structure of object-based environments. To no surprise, the training curriculum has a major effect on learning, but the interpretation of generalization capabilities becomes much more explicit in the following subsection. For example,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>methods rarely manage to pick up any significant success signal under full extreme domain randomization as in curriculum 2, even after 100 million timesteps. Note that these curves represent the scores under the shapes and conditions of the actual training environments. Therefore, we need the capability of setting different protocols, in other words standardized sets of evaluation environment, that allow to benchmark learned skills of different agents.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Evaluation scores for pushing baselines. Each protocol was evaluated for 200 episodes and each bar is averaged over five models with different random seeds. The variables listed under each protocol are sampled from the specified space at the start of every episode while all other variables remain fixed [bp block pose, bm block mass, bs block size, gp goal pose, ff floor friction].</p>
<p>Benchmarking generalization capabilities along various axes: For each of the four goal shape families, we define a set of 12 evaluation protocols that we consider meaningful and representative for benchmarking the different algorithms. In the protocols presented here, we sample the values from a protocol-specific set of variables at the start of each episode while keeping all other variables fixed to their default values. After evaluating an agent on 200 episodes, we compute the fractional success score at the last time step of each episode and report the mean. These evaluation protocols allow to disentangle generalization abilities, as they show robustness with respect to different types of interventions, see Fig. 5. The following are some of the observations we made for pushing:</p>
<ul>
<li>Agents that were trained on the default pushing task environment (curriculum 0 ) do well (as expected) on the default task (P0). Interestingly, we likewise see a generalization capability to initial poses from variable space A (P4). This can be explained by a substantial exploration of the block positions via manipulation during training. Similarly, we see that the agents exhibit weaknesses regarding goal poses (P5) but overfit on their training settings instead.</li>
<li>For agents trained with goal pose randomization (curriculum 1) we see similar results as with curriculum 0 , with the difference that agents under this curriculum generalize robustly to different goal poses (P5), as one would expect.</li>
<li>Finally, agents that experience extreme domain randomization (curriculum 2) at training time, fail to learn any relevant skill as shown by the flat training curve in Fig. 4. An explanation for this behavior could be that the agent might need more data and optimization steps to handle this much more challenging setting. Another possibility is that it may simply not be possible to find a strategy which simultaneously works for all parameters (note that the agent does not have access to the randomized parameters and hence must be robust to them). This poses an interesting question for future work.</li>
</ul>
<p>As expected, we observe that an agent's generalization capabilities are related to the experience gathered under its training curriculum. CausalWorld allows us to explore this relationship in a differentiated manner, assessing which curricula lead to which generalization abilities. This will not only help uncover an agent's shortcomings but may likewise aid in investigating novel learning curricula and approaches for robustness in RL. Lastly, we note that this benchmark comprises extremely challenging tasks that appear to be out of reach of current model free methods without any additional inductive bias.</p>
<h1>5 CONCLUSION</h1>
<p>We have introduced a new benchmark - CausalWorld - to accelerate research in causal structure and transfer learning using a simulated environment of an open-source robot, where learned skills could potentially be transferred to the real world. We showed how allowing for interventions on the environment's properties yields a diverse familiy of tasks with a natural way of defining learning curricula and evaluation protocols that can disentangle different generalization capabilities.</p>
<p>We hope that the flexibility and modularity of CausalWorld will allow researchers to easily define appropriate benchmarks of increasing difficulty as the field progresses, thereby coordinating research efforts towards ever new goals.</p>
<h2>6 ACKNOWLEDGMENTS</h2>
<p>The authors would like to thank Felix Widmaier, Vaibhav Agrawal and Shruti Joshi for the useful discussions and for the development of the TriFinger robot's simulator (Joshi et al., 2020), which served as a starting point for the work presented in this paper. AG is also grateful to Alex Lamb and Rosemary Nan Ke for useful discussions. The authors are grateful for the support from CIFAR.</p>
<h2>REFERENCES</h2>
<p>Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik's cube with a robot hand. arXiv preprint arXiv:1910.07113, 2019.</p>
<p>Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019 .</p>
<p>Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: A new benchmark for physical reasoning. In Advances in Neural Information Processing Systems, pp. 5082-5093, 2019.</p>
<p>Victor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly L Stachenfeld, Pushmeet Kohli, Peter W Battaglia, and Jessica B Hamrick. Structured agents for physical construction. arXiv preprint arXiv:1904.03177, 2019.</p>
<p>Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics. arXiv preprint arXiv:1909.12000, 2019.</p>
<p>Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47: $253-279,2013$.</p>
<p>Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemysaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.</p>
<p>Yvonne M Caldera, Anne McDonald Culp, Marion O'Brien, Rosemarie T Truglio, Mildred Alvarez, and Aletha C Huston. Children's play preferences, construction play with blocks, and visualspatial skills: Are they related? International Journal of Behavioral Development, 23(4):855872, 1999 .</p>
<p>Beth M Casey, Nicole Andrews, Holly Schindler, Joanne E Kersh, Alexandra Samper, and Juanita Copley. The development of spatial skills through interventions involving block building activities. Cognition and Instruction, 26(3):269-309, 2008.</p>
<p>Stephanie CY Chan, Sam Fishman, John Canny, Anoop Korattikara, and Sergio Guadarrama. Measuring the reliability of reinforcement learning algorithms. arXiv preprint arXiv:1912.05663, 2019 .</p>
<p>Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen, and Yoshua Bengio. Babyai: A platform to study the sample efficiency of grounded language learning. In International Conference on Learning Representations, 2018.</p>
<p>Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John Schulman. Quantifying generalization in reinforcement learning. arXiv preprint arXiv:1812.02341, 2018.</p>
<p>Erwin Coumans et al. Bullet real-time physics simulation. URL http://bulletphysics. org, 2013.
Gwen Dewar. The benefits of toy blocks: The science of construction play. Parentig Science, 2018.
Gabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal, and Todd Hester. An empirical investigation of the challenges of real-world reinforcement learning. arXiv preprint arXiv:2003.11881, 2020.</p>
<p>Scott Elliott Fahlman. A planning system for robot construction tasks. Artificial intelligence, 5(1): $1-49,1974$.</p>
<p>Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods, 2018.</p>
<p>Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew Botvinick, Hugo Larochelle, Yoshua Bengio, and Sergey Levine. Infobot: Transfer and exploration via the information bottleneck. arXiv preprint arXiv:1901.10902, 2019a.</p>
<p>Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio, and Bernhard Schölkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893, 2019b.</p>
<p>Thomas L Griffiths and Joshua B Tenenbaum. Structure and strength in causal induction. Cognitive psychology, 51(4):334-384, 2005.</p>
<p>Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor, 2018.</p>
<p>Peter Henderson, Wei-Di Chang, Florian Shkurti, Johanna Hansen, David Meger, and Gregory Dudek. Benchmark environments for multitask learning in continuous domains. arXiv preprint arXiv:1708.04352, 2017a.</p>
<p>Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017b.</p>
<p>Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot learning benchmark \&amp; learning environment. IEEE Robotics and Automation Letters, 5(2):30193026, 2020.</p>
<p>Michael Janner, Sergey Levine, William T Freeman, Joshua B Tenenbaum, Chelsea Finn, and Jiajun Wu. Reasoning about physical interactions with object-oriented prediction and planning. arXiv preprint arXiv:1812.10972, 2018.</p>
<p>Shruti Joshi, Felix Widmaier, Vaibhav Agrawal, and Manuel Wüthrich. https://github.com/ open-dynamic-robot-initiative/trifinger_simulation, 2020.</p>
<p>Constance Kamii, Yoko Miyakawa, and Yasuhiko Kato. The development of logico-mathematical knowledge in a block-building activity at ages 1-4. Journal of Research in Childhood Education, 19(1):44-57, 2004.</p>
<p>Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009 .</p>
<p>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097-1105, 2012.</p>
<p>Youngwoon Lee, Edward S. Hu, Zhengyu Yang, Alex Yin, and Joseph J. Lim. Ikea furniture assembly environment for long-horizon complex manipulation tasks, 2019.</p>
<p>Will McCarthy, David Kirsh, and Judith Fan. Learning to build physical structures better over time.
Mehdi Mirza, Andrew Jaegle, Jonathan J. Hunt, Arthur Guez, Saran Tunyasuvunakool, Alistair Muldal, Théophane Weber, Peter Karkus, Sébastien Racanière, Lars Buesing, Timothy Lillicrap, and Nicolas Heess. Physically embedded planning problems: New challenges for reinforcement learning, 2020.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p>
<p>Swiya Nath and Dénes Szücs. Construction play and cognitive skills associated with the development of mathematical abilities in 7-year-old children. Learning and Instruction, 32:73-80, 2014.</p>
<p>Ian Osband, Yotam Doron, Matteo Hessel, John Aslanides, Eren Sezener, Andre Saraiva, Katrina McKinney, Tor Lattimore, Csaba Szepezvari, Satinder Singh, et al. Behaviour suite for reinforcement learning. arXiv preprint arXiv:1908.03568, 2019.</p>
<p>Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, and Dawn Song. Assessing generalization in deep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018.</p>
<p>Judea Pearl. Causality. Cambridge university press, 2009.
Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. arXiv preprint arXiv:1703.02702, 2017.</p>
<p>Aravind Rajeswaran, Kendall Lowrey, Emanuel V Todorov, and Sham M Kakade. Towards generalization and simplicity in continuous control. In Advances in Neural Information Processing Systems, pp. 6550-6561, 2017.</p>
<p>Miles Richardson, Thomas E Hunt, and Cassandra Richardson. Children's construction task performance and spatial ability: Controlling task complexity and predicting mathematics performance. Perceptual and motor skills, 119(3):741-757, 2014.</p>
<p>Bernhard Schölkopf. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017.</p>
<p>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354-359, 2017.</p>
<p>Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018.</p>
<p>Brian N Verdine, Roberta Michnick Golinkoff, Kathy Hirsh-Pasek, and Nora Newcombe. Links between spatial and mathematical skills across the preschool years. Wiley, 2017.</p>
<p>Shimon Whiteson, Brian Tanner, Matthew E Taylor, and Peter Stone. Protecting against evaluation overfitting in empirical reinforcement learning. In 2011 IEEE symposium on adaptive dynamic programming and reinforcement learning (ADPRL), pp. 120-127. IEEE, 2011.</p>
<p>Terry Winograd. Understanding natural language. Cognitive psychology, 3(1):1-191, 1972.
Patrick H Winston. Learning structural descriptions from examples. 1970.</p>
<p>Manuel Wüthrich, Felix Widmaier, Felix Grimminger, Joel Akpo, Shruti Joshi, Vaibhav Agrawal, Bilal Hammoud, Majid Khadiv, Miroslav Bogdanovic, Vincent Berenz, et al. Trifinger: An opensource robot for learning dexterity. arXiv preprint arXiv:2008.03596, 2020.</p>
<p>Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning. arXiv preprint arXiv:1910.10897, 2019.</p>
<p>Wenhao Yu, Jie Tan, C Karen Liu, and Greg Turk. Preparing for the unknown: Learning a universal policy with online system identification. arXiv preprint arXiv:1702.02453, 2017.</p>
<h1>7 APPENDIX</h1>
<h2>A ObSERVATIONS</h2>
<p>Observations in CausalWorld has two modes, "structured" and "pixel". When using "pixel" mode, 6 images are returned consisting of the current images rendered from 3 different views on top of the TriFinger platform, showing the current state of the environment, as well as the 3 equivalent goal images rendered from the same points of view, showing the goal shape that the robot have to build by the end of the episode.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Example "pixel" mode observations returned at each step of the environment.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Structured observation description. For the scene features, all the blocks feature vector are concatenated first. Following that the partial goals feature vector are concatenated in the same order. Lastly, if there is any obstacles/ fixed blocks, their feature vectors are concatenated at the end following the same description as the partial goal features.</p>
<h1>B TriFinger Platform</h1>
<p>The robot from (Wüthrich et al., 2020) shown in figure 8 is open-sourced and can be reproduced and built in any research lab; since its inexpensive (about \$5000), speeding up sim2real research.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: The TriFinger platform.</p>
<h2>C Task Generators</h2>
<ol>
<li>Pushing: task where the goal is to push one block towards a goal position with a specific orientation; restricted to goals on the floor level.</li>
<li>
<p>Picking: task where the goal is to pick one block towards a goal height above the center of the arena; restricted to goals above the floor level.</p>
</li>
<li>
<p>Pick And Place: task where the arena is divided by a fixed long block and the goal is to pick one block from one side of the arena to a goal position with a variable orientation on the other side of the fixed block.</p>
</li>
<li>Stacking2: task where the goal is to stack two blocks above each other in a specific goal position and orientation.</li>
<li>Towers: task where the goal is to stack multiple n blocks above each other in a specific goal position and orientation - exactly above each other creating a tower of blocks.</li>
<li>Stacked Blocks: task where the goal is to stack multiple n blocks above each other in an arbitrary way to create a stable structure. The blocks don't have to be exactly above each other; making it more challenging than the ordinary towers task since the its harder to come up with a stable structure that covers the goal shape volume.</li>
<li>Creative Stacked Blocks: exactly the same as the Stacked Blocks task except that the first and last levels of the goal are the only levels shown or "imposed" and the rest of the structure is not explicitly specified, leaving the rest of the goal shape to the imagination of the agent itself; this is considered the most challenging since its it needs the agent to understand how to build stable structures and imagine what can be filled in the middle to connect the two levels in a stable way.</li>
<li>General: the goal shape is an arbitrary shape created by initially dropping an arbitrary number of blocks from above the ground and waiting till all blocks come to a rest position where this becomes the goal shape that the agent needs to fill up afterwards.</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: center;">Variable</th>
<th style="text-align: center;">Sub Variable</th>
<th style="text-align: center;">Space A</th>
<th style="text-align: center;">Space B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">gravity[z]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$[-10,-7]$</td>
<td style="text-align: center;">$[-7,-4]$</td>
</tr>
<tr>
<td style="text-align: center;">floor friction</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$[0.3,0.6]$</td>
<td style="text-align: center;">$[0.6,0.8]$</td>
</tr>
<tr>
<td style="text-align: center;">stage friction</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$[0.3,0.6]$</td>
<td style="text-align: center;">$[0.6,0.8]$</td>
</tr>
<tr>
<td style="text-align: center;">stage color [rgb]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$[0.0,0.5]^{3}$</td>
<td style="text-align: center;">$[0.5,1]^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">floor color [rgb]</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$[0.0,0.5]^{3}$</td>
<td style="text-align: center;">$[0.5,1]^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">joint positions</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$\left[[-1.57,-1.2,-3.0]^{3},\right.$</td>
<td style="text-align: center;">$\left[[-0.69,0,0]^{3},\right.$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\left.[-0.69,0,0]^{3}\right]$</td>
<td style="text-align: center;">$\left[1.0,1.57,3.0]^{3}\right]$</td>
</tr>
<tr>
<td style="text-align: center;">block</td>
<td style="text-align: center;">size</td>
<td style="text-align: center;">$[0.055,0.075]^{3}$</td>
<td style="text-align: center;">$[0.075,0.095]^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">block</td>
<td style="text-align: center;">color</td>
<td style="text-align: center;">$[0.0,0.5]^{3}$</td>
<td style="text-align: center;">$[0.5,1]^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">block</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">$[0.015,0.045]$</td>
<td style="text-align: center;">$[0.045,0.1]$</td>
</tr>
<tr>
<td style="text-align: center;">block</td>
<td style="text-align: center;">position (cylindrical)</td>
<td style="text-align: center;">$\left[0,-\pi, h / 2\right]$,</td>
<td style="text-align: center;">$\left[0.11,-\pi, h / 2\right]$,</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$[0.11, \pi, 0.15]]$</td>
<td style="text-align: center;">$[0.15, \pi, 0.3]]$</td>
</tr>
<tr>
<td style="text-align: center;">goal cuboid</td>
<td style="text-align: center;">size</td>
<td style="text-align: center;">$[0.055,0.075]^{3}$</td>
<td style="text-align: center;">$[0.075,0.095]^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">goal cuboid</td>
<td style="text-align: center;">color</td>
<td style="text-align: center;">$[0.0,0.5]^{3}$</td>
<td style="text-align: center;">$[0.5,1]^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">link</td>
<td style="text-align: center;">color</td>
<td style="text-align: center;">$[0.0,0.5]^{3}$</td>
<td style="text-align: center;">$[0.5,1]^{3}$</td>
</tr>
<tr>
<td style="text-align: center;">link</td>
<td style="text-align: center;">mass</td>
<td style="text-align: center;">$[0.015,0.045]$</td>
<td style="text-align: center;">$[0.045,0.1]$</td>
</tr>
</tbody>
</table>
<p>Table 2: Description of a subset of the high level variables, exposed in CausalWorld, and their corresponding spaces, $h$ refers to the height of the block.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task Generator</th>
<th style="text-align: left;">Variable</th>
<th style="text-align: left;">Space A</th>
<th style="text-align: left;">Space B</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Picking</td>
<td style="text-align: left;">goal height</td>
<td style="text-align: left;">$[0.08,0.20]$</td>
<td style="text-align: left;">$[0.20,0.25]$</td>
</tr>
<tr>
<td style="text-align: left;">Towers</td>
<td style="text-align: left;">tower dims</td>
<td style="text-align: left;">$[[0.08,0.08,0.08]$,</td>
<td style="text-align: left;">$[[0.12,0.12,0.12]$,</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$[0.12,0.12,0.12]]$</td>
<td style="text-align: left;">$[0.20,0.20,0.20]]$</td>
</tr>
</tbody>
</table>
<p>Table 3: Example of task generators' specific high level variables, exposed in CausalWorld, and their corresponding spaces. For a full list of each task generators' variables and their corresponding spaces, please refer to the documentation at (https://sites.google.com/view/causal-world/home).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Task generators</th>
<th style="text-align: left;">Dense reward</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Pushing</td>
<td style="text-align: left;">$-750 \Delta^{t}\left(o_{1}, e\right)-250 \Delta^{t}\left(o_{1}, g_{1}\right)$</td>
</tr>
<tr>
<td style="text-align: left;">Picking</td>
<td style="text-align: left;">$-750 \Delta^{t}\left(o_{1}, e\right)-250 \Delta^{t}\left(o_{1, z}, g_{1, z}\right)-125 \Delta^{t}\left(o_{1, x, y}, g_{1, x, y}\right)-0.005\left|v^{t}-\right.$</td>
</tr>
<tr>
<td style="text-align: left;">Pick and Place</td>
<td style="text-align: left;">$\left.v^{t-1}\right|$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$-750 \Delta^{t}\left(o_{1}, e\right)-50 \Delta^{t}\left(o_{1, x, y}, g_{1, x, y}\right)-250\left(\left</td>
</tr>
<tr>
<td style="text-align: left;">Stacking</td>
<td style="text-align: left;">$0.005\left|v^{t}-v^{t-1}\right|$</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{1}<em 1="1">{d^{t}\left(o</em>\right)\right)+$}, e\right)&gt;0.02}\left(-750 \Delta^{t}\left(o_{1}, e\right)-250 \Delta^{t}\left(o_{1}, g_{1</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{1}<em 1="1">{d^{t}\left(o</em>, e\right)-250\left(\left}, e\right)&lt;0.02}\left(-750 \Delta^{t}\left(o_{2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\left.\mathbf{1}<em 2_="2," z="z">{o</em>\right|$}^{t}-g_{2, z}^{t}&gt;0}125 \Delta^{t}\left(o_{2, x, y}, g_{2, x, y}\right)\right)-0.005\left|v^{t}-v^{t-1</td>
</tr>
</tbody>
</table>
<p>Table 4: Description of the dense rewards applied in our experiments. The following notation was applied: $v^{t} \in \mathbf{R}^{3}$ joint velocities, $e_{i}^{t} \in \mathbf{R}^{3}$ i-th end-effector positions, $o_{i}^{t} \in \mathbf{R}^{3}$ i-th block position, $g_{i}^{t} \in \mathbf{R}^{3}$ i-th goal block position, $d^{t}(o, e)=\sum_{i}\left|e_{i}^{t}-o^{t}\right|$ the distance between end-effectors and the block, $\Delta_{o, e}^{t}=d^{t}(o, e)-d^{t-1}(o, e)$ the distance difference w.r.t. the previous timestep. The target height parameter $t$ for pick and place is 0.15 if block and goal are of different height. Otherwise, $t$ is half the goal height.</p>
<h1>D Training Details</h1>
<p>The experiments were carried out using the stable baselines implementation of PPO, SAC and TD3. We used a 2 layer MLP Policy [256,256] for all the policies. PPO was trained on 20 workers up to 100 million timesteps in parallel and SAC as well as TD3 were trained serially for 10 million timesteps.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">PPO</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">SAC</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">TD3</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">discount</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">discount</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">discount</td>
<td style="text-align: center;">0.96</td>
</tr>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">120000</td>
<td style="text-align: center;">entropy coeff</td>
<td style="text-align: center;">$1 \mathrm{e}-3$</td>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">$2.5 \mathrm{e}-4$</td>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
</tr>
<tr>
<td style="text-align: center;">entropy coef.</td>
<td style="text-align: center;">0.01</td>
<td style="text-align: center;">learning rate</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">buffer size</td>
<td style="text-align: center;">500000</td>
</tr>
<tr>
<td style="text-align: center;">value function coef.</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">target entropy</td>
<td style="text-align: center;">auto</td>
<td style="text-align: center;">tau</td>
<td style="text-align: center;">0.02</td>
</tr>
<tr>
<td style="text-align: center;">gradient clipping (max)</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">buffer size</td>
<td style="text-align: center;">1000000</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">n minibatches per update</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">tau</td>
<td style="text-align: center;">0.001</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">n training epochs</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 5: Learning algorithms hyper parameters used in the baselines experiments.
radar_plots_automatic_evaluation_causal_world
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: An example of model selection in CausalWorld by evaluating generalization across the various axes using the previously mentioned protocols. Here we compare two agents trained on different curricula using PPO.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Evaluation scores, for pushing, picking, pick and place and stacking2 baselines, from top to bottom respectively. Each protocol was evaluated for 200 episodes and each bar is averaged over five models with different random seeds [bp block pose, bm block mass, bs block size, gp goal pose, ff floor friction].</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ Note that each task generator can suppress interventions that would yield goal shapes outside its family.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
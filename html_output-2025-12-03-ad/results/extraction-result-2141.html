<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2141 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2141</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2141</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-56.html">extraction-schema-56</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <p><strong>Paper ID:</strong> paper-280037396</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2507.10748v1.pdf" target="_blank">LASANA: Large-Scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration</a></p>
                <p><strong>Paper Abstract:</strong> Neuromorphic systems using in-memory or event-driven computing are motivated by the need for more energy-efficient processing of artificial intelligence workloads. Emerging neuromorphic architectures aim to combine traditional digital designs with the computational efficiency of analog computing and novel device technologies. A crucial problem in the rapid exploration and co-design of such architectures is the lack of tools for fast and accurate modeling and simulation. Typical mixed-signal design tools integrate a digital simulator with an analog solver like SPICE, which is prohibitively slow for large systems. By contrast, behavioral modeling of analog components is faster, but existing approaches are fixed to specific architectures with limited energy and performance modeling. In this paper, we propose LASANA, a novel approach that leverages machine learning to derive data-driven surrogate models of analog sub-blocks in a digital backend architecture. LASANA uses SPICE-level simulations of a circuit to train ML models that predict circuit energy, performance, and behavior at analog/digital interfaces. Such models can provide energy and performance annotation on top of existing behavioral models or function as replacements to analog simulation. We apply LASANA to an analog crossbar array and a spiking neuron circuit. Running MNIST and spiking MNIST, LASANA surrogates demonstrate up to three orders of magnitude speedup over SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%, respectively.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2141.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2141.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LASANA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automated ML-based pipeline that uses SPICE-level transient simulations to train event-based surrogate predictors (output, state, dynamic/static energy, latency) for analog neuromorphic blocks, enabling large-scale architecture-level simulation with large speedups and bounded error versus SPICE.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>analog/mixed-signal neuromorphic blocks (memristive crossbar rows, LIF spiking neuron)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_tool</strong></td>
                            <td>SPICE-level simulators used to generate training data (Synopsys HSPICE for crossbar; Cadence Spectre APS for LIF neuron); behavioral SV-RNM (Cadence Xcelium) used as comparison</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>SPICE netlists using foundry-style PDK transistor models: 1T-1R PCM bitcells for crossbar with PTM HP 14nm library; 20-transistor LIF neuron implemented with FreePDK45nm LP library; explicit load capacitance (500 fF) included; tunable voltage knobs and memristive weight parameters modeled and randomized.</td>
                        </tr>
                        <tr>
                            <td><strong>parasitics_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>nonlinearities_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>tolerances_variations</strong></td>
                            <td>Randomized circuit parameters sampled uniformly within user-defined ranges during dataset generation (domain randomization over tunable parameters and inputs); no explicit Monte-Carlo/process-corner study reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>Supervised learning: gradient-boosted decision trees (CatBoost) and multi-layer perceptron (MLP); also evaluated linear, table-based baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>training_task</strong></td>
                            <td>Train event-level surrogate predictors to predict: output voltage, internal state, dynamic energy (E1 events), static energy (E2/E3 events), and latency (E1 events) from inputs, prior state, event length, and circuit parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Reported speedups up to three orders of magnitude vs SPICE (examples: crossbar HSPICE full-dataset MNIST run 10.96 h -> LASANA 0.13 h; spiking MNIST Spectre 2404 h -> LASANA 1.82 h = 1321x); standalone LASANA 34.6x faster than SV-RNM in a large simulation; accuracy vs SPICE: energy error typically <7% (dynamic energy MAPE example 6.92%), latency MAPE typically <8% (7.58% example), behavioral error <2% (spike-level accuracy within ~0.02% for one study).</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_details</strong></td>
                            <td>SPICE-level transient simulation is treated as ground-truth. LASANA compares surrogate predictions against SPICE and against simpler behavioral SV-RNM models: ML surrogates reproduce SPICE outputs with small bounded errors (energy ≲7%, latency ≲8%, behavioral error ≲2%) while offering orders-of-magnitude speedup. Different ML model classes (CatBoost vs MLP vs table vs linear) exhibit trade-offs: CatBoost tended to perform best for crossbar dynamic energy and output prediction; MLP performed well for stateful LIF neuron behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_requirements_discussed</strong></td>
                            <td>Dataset requires access to SPICE netlist and observed signals (backend clock, inputs, outputs, state where applicable) and tunable circuit parameters; dynamic energy and latency predictors require previous output value as feature; modeling is event-based at digital clock granularity (events E1/E2/E3) — these are identified as necessary for coarse-grain surrogate performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td>No physical hardware transfer experiments reported. Within simulation-only studies, using predicted states (LASANA-P) instead of oracle states increased error: latency MAPE increased by ~1.4x, dynamic energy MAPE by ~1.3x, static energy MSE by ~2.9x, and caused a small decrease in spike accuracy (~0.41%); authors note error propagation but observe MSE does not monotonically grow over time.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>frequency_range</strong></td>
                            <td>Digital clock / event timestep simulation at 250 MHz for crossbar HSPICE runs and 200 MHz for LIF Spectre runs; timescales reported in nanoseconds (example crossbar latency ~0.45 ns; full inference windows tens-to-hundreds of ns).</td>
                        </tr>
                        <tr>
                            <td><strong>electromagnetic_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>thermal_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2141.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2141.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HSPICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synopsys HSPICE (SPICE-level transient simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Industry SPICE solver used in this work to generate SPICE-level transient datasets for training LASANA surrogates for the 32×32 memristive crossbar row; treated as the ground-truth reference simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>memristive crossbar row (analog compute-in-memory)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_tool</strong></td>
                            <td>Synopsys HSPICE (SPICE transient)</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Used with PTM HP 14nm library and explicit 1T-1R PCM bitcell models; includes device-level transistor models from PTM and explicit load capacitance; netlist-level SPICE transient modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>parasitics_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>nonlinearities_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>tolerances_variations</strong></td>
                            <td>Training dataset generation randomized tunable parameters (weights, bias) uniformly over specified ranges; no Monte Carlo or process-corner study reported for HSPICE runs.</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>N/A (HSPICE used as data generator for supervised ML surrogates: CatBoost and MLP trained on HSPICE outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>training_task</strong></td>
                            <td>Generate representative transient event datasets (inputs, outputs, states, energies, latencies) to train LASANA event-level surrogate predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Crossbar HSPICE: dataset generation runs and larger MNIST-based accelerator runs reported (example: full MNIST HSPICE run 10.96 hours). Per-run dataset generation times: crossbar dataset generation took 199 s producing ~99k E1 events.</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_details</strong></td>
                            <td>HSPICE is the high-fidelity reference; ML surrogates trained on HSPICE reproduce its outputs with bounded errors (energy/latency/behavior as reported). Comparisons also made to behavioral SV-RNM which is faster but less accurate.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_requirements_discussed</strong></td>
                            <td>HSPICE netlist must expose observable signals (inputs, outputs, state) and tunable parameters for randomized dataset generation; model fidelity includes PDK transistor models and explicit load capacitors.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td>No direct sim-to-hardware transfer experiments; no reported cases of HSPICE-based training failing to produce usable surrogates, beyond typical ML prediction errors and error-propagation in iterative state prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frequency_range</strong></td>
                            <td>Crossbar HSPICE runs at 250 MHz (timestep granularity consistent with digital clock); nanosecond timescale.</td>
                        </tr>
                        <tr>
                            <td><strong>electromagnetic_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>thermal_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2141.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2141.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of circuit simulators used for training machine learning or reinforcement learning models, including details about simulation fidelity, component modeling accuracy, and transfer performance to real circuits or hardware.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Spectre APS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cadence Spectre APS (SPICE-level transient simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cadence Spectre APS used here to generate SPICE transient datasets for the 20-transistor LIF spiking neuron and for large spiking-MNIST simulations; treated as ground-truth for training and evaluation of LASANA surrogates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>circuit_type</strong></td>
                            <td>analog leaky-integrate-and-fire (LIF) spiking neuron</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_tool</strong></td>
                            <td>Cadence Spectre APS (SPICE-level transient)</td>
                        </tr>
                        <tr>
                            <td><strong>component_models</strong></td>
                            <td>Implemented with FreePDK45nm LP library transistor models for a 20-transistor LIF neuron; included tunable voltage knobs (V_leak, V_th, V_adap, V_refrac) and explicit load capacitance.</td>
                        </tr>
                        <tr>
                            <td><strong>parasitics_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>nonlinearities_modeled</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>tolerances_variations</strong></td>
                            <td>Datasets created by random runs: tunable voltage parameters sampled uniformly within ranges (e.g., p ∈ [0.5V,0.8V]); inputs randomized with parameter α controlling active/static timesteps; no separate Monte Carlo/process-corner study reported.</td>
                        </tr>
                        <tr>
                            <td><strong>ml_model_type</strong></td>
                            <td>N/A (Spectre used as data generator for supervised ML surrogates: MLP and CatBoost trained on Spectre outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>training_task</strong></td>
                            <td>Produce transient event datasets (state evolution, spikes, energy, latency) to train event-based surrogate predictors for state, output, energy, and latency.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_performance</strong></td>
                            <td>Spectre full spiking-MNIST run ~2404 hours; LASANA MLP surrogate ran same workload in 1.82 hours (~1321x speedup) with MNIST accuracy 95.63% vs Spectre 95.65%; average per-inference dynamic energy MAPE 6.92%, latency MAPE 7.58%, total energy error per inference 2.90%, latency error 5.88%.</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_tested</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>real_hardware_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_details</strong></td>
                            <td>Spectre results used as ground truth; ML surrogates trained on Spectre match behavior with small percentage errors, enabling massive speedups compared to full Spectre runs. Behavioral error propagation was quantified by comparing predicted-state runs to oracle-state runs.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_requirements_discussed</strong></td>
                            <td>Training requires access to Spectre netlists combined with randomized inputs and tunable parameters; event decomposition (E1/E2/E3) and inclusion of prior state and previous outputs are essential features for some predictors (e.g., latency and dynamic energy).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_cases</strong></td>
                            <td>No experiments transferring surrogates to physical hardware; within-simulation transfer (using predicted state vs oracle state) shows modest degradation and quantified error amplification.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>physics_informed_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frequency_range</strong></td>
                            <td>Simulations performed at 200 MHz digital timestep for LIF neuron Spectre runs; end-to-end inferences ran over hundreds of ns (example SNN runs of 500 ns per inference).</td>
                        </tr>
                        <tr>
                            <td><strong>electromagnetic_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>thermal_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>IMAC-Sim: A Circuit-level Simulator For In-Memory Analog Computing Architectures <em>(Rating: 2)</em></li>
                <li>A System-Level Simulator for RRAM-Based Neuromorphic Computing Chips <em>(Rating: 2)</em></li>
                <li>Crosssim: accuracy simulation of analog in-memory computing <em>(Rating: 2)</em></li>
                <li>Power to the Model: Generating Energy-Aware Mixed-Signal Models using Machine Learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2141",
    "paper_id": "paper-280037396",
    "extraction_schema_id": "extraction-schema-56",
    "extracted_data": [
        {
            "name_short": "LASANA",
            "name_full": "Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration",
            "brief_description": "An automated ML-based pipeline that uses SPICE-level transient simulations to train event-based surrogate predictors (output, state, dynamic/static energy, latency) for analog neuromorphic blocks, enabling large-scale architecture-level simulation with large speedups and bounded error versus SPICE.",
            "citation_title": "LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration",
            "mention_or_use": "use",
            "circuit_type": "analog/mixed-signal neuromorphic blocks (memristive crossbar rows, LIF spiking neuron)",
            "simulator_tool": "SPICE-level simulators used to generate training data (Synopsys HSPICE for crossbar; Cadence Spectre APS for LIF neuron); behavioral SV-RNM (Cadence Xcelium) used as comparison",
            "component_models": "SPICE netlists using foundry-style PDK transistor models: 1T-1R PCM bitcells for crossbar with PTM HP 14nm library; 20-transistor LIF neuron implemented with FreePDK45nm LP library; explicit load capacitance (500 fF) included; tunable voltage knobs and memristive weight parameters modeled and randomized.",
            "parasitics_modeled": true,
            "nonlinearities_modeled": true,
            "tolerances_variations": "Randomized circuit parameters sampled uniformly within user-defined ranges during dataset generation (domain randomization over tunable parameters and inputs); no explicit Monte-Carlo/process-corner study reported.",
            "ml_model_type": "Supervised learning: gradient-boosted decision trees (CatBoost) and multi-layer perceptron (MLP); also evaluated linear, table-based baselines.",
            "training_task": "Train event-level surrogate predictors to predict: output voltage, internal state, dynamic energy (E1 events), static energy (E2/E3 events), and latency (E1 events) from inputs, prior state, event length, and circuit parameters.",
            "simulation_performance": "Reported speedups up to three orders of magnitude vs SPICE (examples: crossbar HSPICE full-dataset MNIST run 10.96 h -&gt; LASANA 0.13 h; spiking MNIST Spectre 2404 h -&gt; LASANA 1.82 h = 1321x); standalone LASANA 34.6x faster than SV-RNM in a large simulation; accuracy vs SPICE: energy error typically &lt;7% (dynamic energy MAPE example 6.92%), latency MAPE typically &lt;8% (7.58% example), behavioral error &lt;2% (spike-level accuracy within ~0.02% for one study).",
            "real_hardware_tested": false,
            "real_hardware_performance": null,
            "fidelity_comparison": true,
            "fidelity_comparison_details": "SPICE-level transient simulation is treated as ground-truth. LASANA compares surrogate predictions against SPICE and against simpler behavioral SV-RNM models: ML surrogates reproduce SPICE outputs with small bounded errors (energy ≲7%, latency ≲8%, behavioral error ≲2%) while offering orders-of-magnitude speedup. Different ML model classes (CatBoost vs MLP vs table vs linear) exhibit trade-offs: CatBoost tended to perform best for crossbar dynamic energy and output prediction; MLP performed well for stateful LIF neuron behavior.",
            "minimal_requirements_discussed": "Dataset requires access to SPICE netlist and observed signals (backend clock, inputs, outputs, state where applicable) and tunable circuit parameters; dynamic energy and latency predictors require previous output value as feature; modeling is event-based at digital clock granularity (events E1/E2/E3) — these are identified as necessary for coarse-grain surrogate performance.",
            "transfer_failure_cases": "No physical hardware transfer experiments reported. Within simulation-only studies, using predicted states (LASANA-P) instead of oracle states increased error: latency MAPE increased by ~1.4x, dynamic energy MAPE by ~1.3x, static energy MSE by ~2.9x, and caused a small decrease in spike accuracy (~0.41%); authors note error propagation but observe MSE does not monotonically grow over time.",
            "domain_randomization_used": true,
            "physics_informed_approach": false,
            "frequency_range": "Digital clock / event timestep simulation at 250 MHz for crossbar HSPICE runs and 200 MHz for LIF Spectre runs; timescales reported in nanoseconds (example crossbar latency ~0.45 ns; full inference windows tens-to-hundreds of ns).",
            "electromagnetic_effects": null,
            "thermal_effects": null,
            "uuid": "e2141.0"
        },
        {
            "name_short": "HSPICE",
            "name_full": "Synopsys HSPICE (SPICE-level transient simulator)",
            "brief_description": "Industry SPICE solver used in this work to generate SPICE-level transient datasets for training LASANA surrogates for the 32×32 memristive crossbar row; treated as the ground-truth reference simulator.",
            "citation_title": "LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration",
            "mention_or_use": "use",
            "circuit_type": "memristive crossbar row (analog compute-in-memory)",
            "simulator_tool": "Synopsys HSPICE (SPICE transient)",
            "component_models": "Used with PTM HP 14nm library and explicit 1T-1R PCM bitcell models; includes device-level transistor models from PTM and explicit load capacitance; netlist-level SPICE transient modeling.",
            "parasitics_modeled": true,
            "nonlinearities_modeled": true,
            "tolerances_variations": "Training dataset generation randomized tunable parameters (weights, bias) uniformly over specified ranges; no Monte Carlo or process-corner study reported for HSPICE runs.",
            "ml_model_type": "N/A (HSPICE used as data generator for supervised ML surrogates: CatBoost and MLP trained on HSPICE outputs).",
            "training_task": "Generate representative transient event datasets (inputs, outputs, states, energies, latencies) to train LASANA event-level surrogate predictors.",
            "simulation_performance": "Crossbar HSPICE: dataset generation runs and larger MNIST-based accelerator runs reported (example: full MNIST HSPICE run 10.96 hours). Per-run dataset generation times: crossbar dataset generation took 199 s producing ~99k E1 events.",
            "real_hardware_tested": false,
            "real_hardware_performance": null,
            "fidelity_comparison": true,
            "fidelity_comparison_details": "HSPICE is the high-fidelity reference; ML surrogates trained on HSPICE reproduce its outputs with bounded errors (energy/latency/behavior as reported). Comparisons also made to behavioral SV-RNM which is faster but less accurate.",
            "minimal_requirements_discussed": "HSPICE netlist must expose observable signals (inputs, outputs, state) and tunable parameters for randomized dataset generation; model fidelity includes PDK transistor models and explicit load capacitors.",
            "transfer_failure_cases": "No direct sim-to-hardware transfer experiments; no reported cases of HSPICE-based training failing to produce usable surrogates, beyond typical ML prediction errors and error-propagation in iterative state prediction.",
            "domain_randomization_used": true,
            "physics_informed_approach": null,
            "frequency_range": "Crossbar HSPICE runs at 250 MHz (timestep granularity consistent with digital clock); nanosecond timescale.",
            "electromagnetic_effects": null,
            "thermal_effects": null,
            "uuid": "e2141.1"
        },
        {
            "name_short": "Spectre APS",
            "name_full": "Cadence Spectre APS (SPICE-level transient simulator)",
            "brief_description": "Cadence Spectre APS used here to generate SPICE transient datasets for the 20-transistor LIF spiking neuron and for large spiking-MNIST simulations; treated as ground-truth for training and evaluation of LASANA surrogates.",
            "citation_title": "LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration",
            "mention_or_use": "use",
            "circuit_type": "analog leaky-integrate-and-fire (LIF) spiking neuron",
            "simulator_tool": "Cadence Spectre APS (SPICE-level transient)",
            "component_models": "Implemented with FreePDK45nm LP library transistor models for a 20-transistor LIF neuron; included tunable voltage knobs (V_leak, V_th, V_adap, V_refrac) and explicit load capacitance.",
            "parasitics_modeled": true,
            "nonlinearities_modeled": true,
            "tolerances_variations": "Datasets created by random runs: tunable voltage parameters sampled uniformly within ranges (e.g., p ∈ [0.5V,0.8V]); inputs randomized with parameter α controlling active/static timesteps; no separate Monte Carlo/process-corner study reported.",
            "ml_model_type": "N/A (Spectre used as data generator for supervised ML surrogates: MLP and CatBoost trained on Spectre outputs).",
            "training_task": "Produce transient event datasets (state evolution, spikes, energy, latency) to train event-based surrogate predictors for state, output, energy, and latency.",
            "simulation_performance": "Spectre full spiking-MNIST run ~2404 hours; LASANA MLP surrogate ran same workload in 1.82 hours (~1321x speedup) with MNIST accuracy 95.63% vs Spectre 95.65%; average per-inference dynamic energy MAPE 6.92%, latency MAPE 7.58%, total energy error per inference 2.90%, latency error 5.88%.",
            "real_hardware_tested": false,
            "real_hardware_performance": null,
            "fidelity_comparison": true,
            "fidelity_comparison_details": "Spectre results used as ground truth; ML surrogates trained on Spectre match behavior with small percentage errors, enabling massive speedups compared to full Spectre runs. Behavioral error propagation was quantified by comparing predicted-state runs to oracle-state runs.",
            "minimal_requirements_discussed": "Training requires access to Spectre netlists combined with randomized inputs and tunable parameters; event decomposition (E1/E2/E3) and inclusion of prior state and previous outputs are essential features for some predictors (e.g., latency and dynamic energy).",
            "transfer_failure_cases": "No experiments transferring surrogates to physical hardware; within-simulation transfer (using predicted state vs oracle state) shows modest degradation and quantified error amplification.",
            "domain_randomization_used": true,
            "physics_informed_approach": null,
            "frequency_range": "Simulations performed at 200 MHz digital timestep for LIF neuron Spectre runs; end-to-end inferences ran over hundreds of ns (example SNN runs of 500 ns per inference).",
            "electromagnetic_effects": null,
            "thermal_effects": null,
            "uuid": "e2141.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "IMAC-Sim: A Circuit-level Simulator For In-Memory Analog Computing Architectures",
            "rating": 2
        },
        {
            "paper_title": "A System-Level Simulator for RRAM-Based Neuromorphic Computing Chips",
            "rating": 2
        },
        {
            "paper_title": "Crosssim: accuracy simulation of analog in-memory computing",
            "rating": 2
        },
        {
            "paper_title": "Power to the Model: Generating Energy-Aware Mixed-Signal Models using Machine Learning",
            "rating": 1
        }
    ],
    "cost": 0.014129999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration</p>
<p>Jason Ho jasonho@utexas.edu 
Electrical and Computer Engineering
The University of Texas at Austin
TexasUSA</p>
<p>James A Boyle james.boyle@utexas.edu 
Electrical and Computer Engineering
The University of Texas at Austin
TexasUSA</p>
<p>Linshen Liu linshenliu@utexas.edu 
Electrical and Computer Engineering
The University of Texas at Austin
TexasUSA</p>
<p>Andreas Gerstlauer 
Electrical and Computer Engineering
The University of Texas at Austin
TexasUSA</p>
<p>LASANA: Large-scale Surrogate Modeling for Analog Neuromorphic Architecture Exploration
C70FC1E5891E4F26C68FFF6055C215FCneuromorphic architecturesanalog computingmixed-signal simulationmachine learningsurrogate modeling
Neuromorphic systems using in-memory or eventdriven computing are motivated by the need for more energyefficient processing of artificial intelligence workloads.Emerging neuromorphic architectures aim to combine traditional digital designs with the computational efficiency of analog computing and novel device technologies.A crucial problem in the rapid exploration and co-design of such architectures is the lack of tools for fast and accurate modeling and simulation.Typical mixedsignal design tools integrate a digital simulator with an analog solver like SPICE, which is prohibitively slow for large systems.By contrast, behavioral modeling of analog components is faster, but existing approaches are fixed to specific architectures with limited energy and performance modeling.In this paper, we propose LASANA, a novel approach that leverages machine learning to derive data-driven surrogate models of analog sub-blocks in a digital backend architecture.LASANA uses SPICE-level simulations of a circuit to train ML models that predict circuit energy, performance, and behavior at analog/digital interfaces.Such models can provide energy and performance annotation on top of existing behavioral models or function as replacements to analog simulation.We apply LASANA to an analog crossbar array and a spiking neuron circuit.Running MNIST and spiking MNIST, LASANA surrogates demonstrate up to three orders of magnitude speedup over SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%, respectively.</p>
<p>I. INTRODUCTION</p>
<p>With the exponential growth in machine learning (ML) complexity and model size, the memory bottleneck and energy consumption of traditional von Neumann computation have become key concerns.Neuromorphic computing is a promising alternative that draws on brain-inspired principles to enable in-memory and/or event-driven computation.A subset of neuromorphic systems seeks to exploit analog computation for its improved energy-efficiency [1] and ability to leverage the unique properties of novel devices [2].For instance, ML architectures are replacing matrix-vector multiplication with efficient compute-in-memory (CiM) analog multiply-accumulate operations using memristive crossbars [3]- [5].Analog spiking systems are another emerging paradigm offering event-driven processing, enabling parallel computation and further energy savings [6], [7].</p>
<p>In these architectures, analog compute blocks are typically integrated into a digital backend that handles control logic and supports scalability.Each block interfaces with the digital world through converters that translate between the continuous analog and discrete digital domains, as shown in Fig. 1.Since the analog block lies on the critical path, its energy consumption and delay are important to the overall energy efficiency and performance of a design.At the same time, overall performance and efficiency are determined by the integration of analog and digital domains, necessitating their joint exploration.</p>
<p>A crucial problem in the co-design of such systems is the lack of tools that allow for fast and accurate modeling and simulation of large-scale hybrid analog/digital architectures.Traditional approaches for modeling analog components rely on low-level simulation methods such as SPICE, which is prohibitively slow for large systems.Early-stage SoC-scale simulations are often accomplished with hand-crafted behavioral models like SystemVerilog Real Number modeling (SV-RNM) and SystemC-AMS [8], which replace complex transient calculations with simplified mathematical models evaluated at discrete events.However, manual annotation of energy and performance estimates is required.Some custom-developed behavioral simulators, e.g. for analog crossbar arrays [9]- [11] include analytical energy and performance models, but they are limited in accuracy and specific to one type of architecture, restricting their flexibility.</p>
<p>In this paper, we propose LASANA, a novel data-driven approach using machine learning to derive lightweight analog surrogate models that estimate the energy, latency, and behavior of analog sub-blocks at event-based analog/digital interfaces.Given a circuit netlist, we introduce an end-toend framework that generates a representative dataset, trains ML predictors, and outputs C++ inference models.Models estimate energy and performance at the granularity of digital clock steps with high accuracy and low overhead.ML-based models can be used to augment existing behavioral models with automatically derived energy and performance estimates or to serve as standalone drop-in surrogates.To the best of our knowledge, this is the first approach to develop ML-based surrogate models of analog circuit blocks at a coarse-grain event level.The key contributions of this work are as follows:</p>
<p>• We propose a novel learning formulation and automated approach for the generation and deployment of event-arXiv:2507.10748v1[cs.AR] 14 Jul 2025</p>
<p>driven ML-based analog surrogate models integrated into digital backend simulators.Our formulation incorporates several optimizations to improve runtime by batching inferences across the system and merging inactive input periods into single events.• We examine runtime and accuracy trade-offs of ML models for coarse-grain analog surrogate modeling.We find that boosted trees and multi-layer perceptrons perform the best depending on circuit complexity.• We evaluate LASANA modeling on a crossbar array and a spiking neuron circuit.On MNIST and spiking MNIST, we demonstrate up to three orders of magnitude speedup over SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%, respectively.</p>
<p>II. RELATED WORK</p>
<p>Traditionally, analog circuits are simulated with SPICE, which uses numerical methods to solve Kirchhoff's laws.To model mixed-signal designs, tools like Verilog-AMS run SPICE simulations alongside digital simulators [12].SV-RNM and SystemC-AMS are state-of-the-art large-scale behavioral analog simulation methodologies [8].They use the digital event solver and simple mathematical equations to model analog circuits, providing simulation speedup at the cost of accuracy.Developing such models requires domain knowledge of the underlying equations of a circuit.Furthermore, they do not provide energy or performance estimation, requiring manual annotation.</p>
<p>Existing architectural simulators that incorporate analog circuits typically rely on custom-developed simulators [9]- [11].They combine behavioral modeling with analytical performance and energy estimation of one class of circuit, such as memristor or floating gate crossbar arrays, limiting the range of architectures that can be explored.</p>
<p>Several works are aimed at replacing SPICE-level transient simulations using ML models targeting one aspect, such as behavioral [13] or power [14] estimation.However, they rely on large fine-tuned ML models to provide estimates at fine temporal granularity.By contrast, our approach models behavior, energy, and performance at a coarse-grain event level using fast and simple ML models to support large-scale architecture-level simulation.III.BACKGROUND Fig. 2 shows two examples of common analog compute blocks in emerging neuromorphic architectures: memristive crossbar arrays and spiking neurons.Fig. 2(a) shows a schematic of one row of a crossbar array embedded in a digital backend.To realize positive and negative weights, each input is connected to two memristors and combined at a differential amplifier, creating an overall weight, W i ∝ G pos,i − G neg,i .</p>
<p>Fig. 2(b) shows a schematic of a leaky-integrate-and-fire (LIF) analog neuron that, unlike traditional neural networks, implements spiking neural network (SNN) behavior, which encodes information in the amplitude and timing of discrete spikes sent between neurons.Incoming spikes are modulated    by a synapse weight and passed through a spike generator, which converts the voltage into a current spike.Inside the neuron, charge is integrated into a capacitor with an internal state (V state ).The state is constantly compared against a threshold (V th ), producing V dd and resetting the circuit state if greater.The state also leaks over time and is implemented with a subthreshold transistor.
… V th V L V state Spike X ADC I + I - W1 V dd V ss V out DAC … Input n Input 1 Input 2 Out X ADC (a) (b)
Typically, parallel instances of these blocks, each with variations in input and exact circuit parameters, sit between digitalto-analog converters (DACs) and analog-to-digital converters (ADCs) while a digital backend manages the interconnect and control logic of the larger system.Inputs arrive at the analog block at digital timesteps, and the block must finish processing the input before the next clock step.</p>
<p>IV. EVENT-BASED SURROGATE MODELING</p>
<p>As shown in Fig. 3, our approach develops surrogate models by first creating a representative event-based dataset of a SPICE-level circuit, and then using the dataset to train ML inference models of circuit energy, latency, output, and state behavior.Generated models are wrapped in an inference function that can be plugged into a backend architectural simulator for energy, latency, and optionally behavioral estimation.Energy and latency models can be used to annotate traditional behavioral models, or combined with our ML-based state and output predictors to provide a complete model of analog subblocks in a larger digital simulation.In the following, we will describe LASANA modeling steps in more detail.</p>
<p>A. Dataset Creation</p>
<p>Given a SPICE netlist of an analog circuit, LASANA provides an end-to-end methodology for automated dataset generation to characterize energy, latency, and behavior.To facilitate generalizability, the circuit is treated as a black box, only requiring access to the backend clock, inputs, outputs, state (if applicable), and tunable circuit parameters such as memristive weights or voltage knobs.The dataset is created by running SPICE simulations of a template circuit with randomized inputs and parameters, followed by event processing that splits continuous traces into coarse-grain events.</p>
<p>1) Testbench Generation: For each SPICE simulation, LASANA randomly generates inputs for a number of simulated timesteps and randomly samples the tunable circuit parameters within their respective ranges uniformly.We define circuit parameters as fixed knobs during simulation (e.g., memristive weights) while inputs vary over time.Each timestep is classified as either active, where inputs change, or static, where inputs remain constant with a probability of α and 1-α, respectively.In an active timestep, inputs are randomly sampled within user-defined input ranges.Each run's inputs are written into a .PWL file as a sequence of discrete time-value pairs.At the same time, circuit netlist instances are generated from a template that is modified with the randomized circuit parameters and connected to the inputs.</p>
<p>2) SPICE Simulation: Once all inputs have been generated, LASANA runs all netlists with their .PWL files as inputs.All netlists are executed in parallel, using multithreading within each simulation and multiprocessing across the system.</p>
<p>3) Event Processing: After simulation, transient data is decomposed into events which always start and end at timestep boundaries, as shown in Fig. 4. We classify events into three categories to support event-specific ML formulations and training.E1 and E3 are one-timestep-long events triggered by a change in input and differentiated based on whether there is a change in the output.E2 is a variable-length event that captures any idle period between active timesteps.</p>
<p>4) Circuit Dataset Creation: For each event, LASANA captures the output o, the internal state v i and v i+n of the circuit at the beginning t i and end t i+n of the event, the length of the event τ , the input values x, and the randomized circuit parameters p that the simulation ran with.If a circuit is not stateful, the state value is set to 0. Energy E is defined for the duration of the event and classified as dynamic if that timestep produced changes in the output (E1) and static otherwise (E2, E3).Latency L is only calculated for E1 events as the time from the start of the input to the point where the output reaches 90% of its final value.For spiking signals, latency is defined from the start of the input to the output peak.</p>
<p>B. ML Model Training</p>
<p>As shown in Fig. 3, LASANA uses the circuit dataset to train ML models on five separate predictors: an output predictor (M O ), a state predictor (M V ), a dynamic energy predictor (M E D ) that predicts energy per E1 event, a static energy predictor (M E S ) that predicts energy for E2 and E3 events, and a latency predictor (M L ).We split predictors into separate models to enable training on event-specific data with independent loss optimization.instance, latency is only meaningful when there is change in output (E1 events), where including other events would degrade learning.All predictors take input vector x, current state v ′ i , the length of the event τ , and circuit parameter vector p as features.If there is no input, x is set to 0. Since dynamic energy and latency depend on the output voltage transition, M E D and M L also take the previous output o as input.During training, several models are evaluated on a validation dataset, and the best model for each predictor is selected for inference.</p>
<p>C. ML Inference</p>
<p>We embed the trained models in a wrapper function for integration with architectural simulators.Which predictors are invoked during inference depends on what events occurred, as shown in Fig. 5. M V is invoked for all events, and M O is invoked for events with a change in input.M L and M E D are only invoked for events with a change in output, while M E S invoked otherwise.In the process, we apply several optimizations to improve inference speed, including batching of inferences across the system and merging of inactive input periods into single events.</p>
<p>Algorithm 1 shows the wrapper function for prediction of N analog circuit sub-blocks at time t, with a digital clock period of T .This function also takes in the set S of circuit IDs with a change in input at t, a matrix X of the respective inputs to be applied to each circuit, a matrix P of the N circuit parameters, a vector t ′ of the latest state update times for each circuit, and the vector v ′ of the current state of each of the N circuits.This function returns the energy e, latency l, and output o per circuit.If LASANA models are used to provide energy and latency annotation on existing behavioral models, those models would supply the state.</p>
<p>Algorithm 1 ML Inference Wrapper</p>
<p>Input: number of circuits N Input: current time t Input: digital clock period T Input: set S of circuit IDs with changed input at time t Input: matrix X = (x 0 , x 1 , ..., x N−1 ) of input vectors Input: matrix P = (p 0 , p 0 , ..., p N−1 ) of circuit parameters State: if t ′ n &lt; t − T then 5: The algorithm first updates the state circuits that have a change in input in the current timestep but have not been updated for at least one timestep before.To do so, corresponding input parameters are collected into a batch I (lines 2-7) that is passed into state and static energy predictors (lines 8-9).Inference results are used to update corresponding state and energy values (lines 13-14) while also collecting the input event batch from the inputs x n and parameters p n (line 16).The five predictors are called on this batch (lines [18][19][20][21][22], where energy and values are computed depending on the output prediction (lines 24-29).Lastly, the latest circuit update time is set to the current timestep t (line 30).
vec t ′ = (t ′ 0 , t ′ 1 , ..., t ′ N −1 ) of latest circuit update times State: vec v ′ = (v ′ 0 , v ′ 1 , ..., v ′ N −1 )I ← I ∪ (0, v ′ n , t − t ′ n − T,</p>
<p>V. EXPERIMENTS AND RESULTS</p>
<p>We applied our approach to a 32 × 32 memristive crossbar array and an analog spiking neural network implementation.We use LASANA to derive surrogate models for independent 32-input crossbar rows from [3] using 1T-1R phase change memory (PCM) bitcells with the PTM HP 14nm library [15], and for a 20-transistor spiking LIF neuron from [16] us-  Table I shows the models evaluated with their total training times and testing times.Models are listed in order of complexity and training time.We compare a boosted tree model and a multi-layer perceptron (MLP) against a linear regressor as well as mean and table-based models that mirror analytical energy and performance estimation in existing behavioral simulators, e.g. for crossbar arrays [10].Mean is a constant estimator using the training mean.CatBoost is a gradient-boosting model that builds an ensemble of weak decision trees with a max depth of 10, where each new tree iteratively corrects errors of the prior ensemble.The MLP is a neural network trained with the Adam optimizer, employing ReLU activation, and two hidden layers of 100 and 50 neurons.Model hyperparameters (learning rate, L2 regularization) were optimized using grid search and trained until the change in validation loss fell below 10 −5 .The models</p>
<p>PCM Crossbar Row [3]</p>
<p>LIF Neuron [16] M  were implemented with Scikit-Learn [18] and CatBoost [19].
L M E D M E S M O M L M E D M E S M V M O MSE MAPE MSE MAPE MSE MSE MSE MAPE MSE MAPE MSE MSE MSE Model (ps 2 ) (%) (fJ 2 ) (%) (fJ 2 ) (V 2 ) (ns 2 ) (%) (pJ 2 ) (%) (pJ 2 ) (V 2 ) (V 2(a) M E D (b) M E S (c) M L (d) M O
To evaluate our models, we use mean squared error (MSE) and mean absolute percentage error (MAPE).We do not report MAPE for value predictors as the metric depends heavily on the underlying data distribution.We also do not report MAPE for static energy since it has small values distributed around zero that overamplify percentage error.</p>
<p>A. Energy and Latency Prediction</p>
<p>In Table II, we present the test results of the models on the two circuits, with the best performance in each column shown in bold.For the crossbar, the latency distribution is closely clustered around 0.45ns, resulting in similar MSEs and MAPEs across the models.For dynamic energy, CatBoost outperforms other models with a significant margin, achieving 7.3x better MSE and 2.9x better MAPE than the next best MLP.For static energy, the MLP achieves the best MSE, followed closely by CatBoost.Mean, table-based, and linear predictors perform poorly due to high-dimensional inputs.Overall, CatBoost is the best or near-best predictor for the crossbar, which we show in correlation plots in Fig. 6(a)-(c).</p>
<p>In the LIF neuron, energy and latency are functions of both the state and input.For latency, the CatBoost and MLP models achieve similar MSEs, but CatBoost has 1.1x better MAPE.For dynamic energy, we see a similar trend where the MSE of the MLP is 1.4x better than CatBoost, but CatBoost has 1.1x better MAPE.Lastly, for static energy, CatBoost and the MLP perform similarly well.While table-based predictors capture the neuron's complex behavior in a lower-dimensional input space more effectively than linear and mean models, more expressive models like CatBoost and MLP achieve even lower error.Overall, the MLP is the best or near-best predictor for the LIF neuron, which we show in correlation plots in Fig. 7(a)-(c).We note that mispredictions can appear visually exaggerated even when the overall error is low, as in Fig. 7(b).</p>
<p>B. Behavior Prediction</p>
<p>Table II also shows results for state and output prediction.The crossbar output range is [−2V, 2V ] while the LIF neuron state and output range are [0V, 1.5V ].For the crossbar, CatBoost achieves the best MSE of 0.0121 V 2 for output prediction.Assuming quantization to 8-bit in the ADC, the MSE reduces to 0.0090 V 2 (1.34x lower).For the LIF neuron, the CatBoost and MLP models perform similarly, with the MLP achieving better MSE for state and output prediction by 1.04x and 1.01x, respectively.When converted to spikes, the MLP's predicted output has 99.3% accuracy.Again, mean, tablebased, and linear predictors perform poorly on the crossbar.With a lower-dimensional input space in the LIF neuron, the table-based model outperforms the mean and linear models, but is still outclassed by more expressive models.We show correlation plots of CatBoost for the crossbar in Fig. 6(d) and MLP for the LIF neuron in Fig. 7(d).Again, mispredictions can appear visually exaggerated as in Fig. 7(d).</p>
<p>C. Behavioral Error Propagation</p>
<p>In this section, we study the potential for error accumulation as a result of state prediction when using LASANA models.Since the crossbar has no state, predictions have no impact on the future.As such, we focus this analysis on the LIF neuron circuit using the MLP models.</p>
<p>To evaluate the impact of ML-based behavioral modeling, we predict behavior over an entire simulation of a one-layer neural network.Since state predictions in one time step are used as input for subsequent predictions, errors can propagate.Table III shows the results of a study on a 20,000 LIF neuron layer simulated for 500 ns using randomized circuit parameters and input.LASANA-O is an oracle with perfect knowledge of the state, while LASANA-P uses the predicted state for subsequent prediction.Using LASANA-P, latency MAPE increases by 1.4x, dynamic energy MAPE by 1.3x, and static energy MSE by 2.9x compared to LASANA-O.For</p>
<p>D. Runtime Scaling Comparison</p>
<p>In Table IV, we compare the runtimes of Spectre APS SPICE simulations, purely behavioral SV-RNM models, SV-RNM models with our energy and latency annotation, and standalone ML simulations as the size of an LIF neuron layer increases.Similar runtime trends are observed in the crossbar.Versus SPICE, our speedup factor scales to more than four orders of magnitude at 20,000 neurons.ML-based annotation of energy and latency increased SV-RNM simulation runtime by less than 1.2% in the 20,000 neuron simulation.By contrast, standalone LASANA was 34.6x faster than SV-RNM.To show LASANA's scaling capability, we also simulated networks of 200,000 neurons and 200,000 32-input crossbar rows, resulting in runtimes of 65.7 s and 336.1 s, respectively.For comparison, Intel's Loihi chip has 131,072 neurons [20], and the crossbar rows would be equivalent to 6,250 32 × 32 crossbar arrays.</p>
<p>E. MNIST and Spiking MNIST Case Study</p>
<p>Using the crossbar row CatBoost models, we performed a case study on the unpadded 20 × 20 MNIST dataset from [21] using a 400 × 120 × 84 × 10 binary neural network from [3].The accelerator partitions the layers into 67 32 × 32 PCMbased crossbars based on [3].To emulate each crossbar, we instantiate 32 instances of the 32-input LASANA crossbar row models.The crossbars perform analog matrix-vector multiplication, where outputs are converted into digital values through an 8-bit ADC, passed through an inverse sigmoid activation layer, and converted back to analog and passed to the next layer through an 8-bit DAC.Running Synopsys HSPICE on the test dataset took 10.96 hours of runtime with 95.86% MNIST accuracy.By contrast, our models ran in 0.13 hours Using the LIF neuron MLP models, we perform a similar study on the 28 × 28 padded MNIST dataset from [21], where we convert each pixel into a Poisson rate-encoded spike train with firing probability proportional to pixel intensity.A 784 × 128 × 10 SNN was trained using SNNTorch [22] with the Adam optimizer and MSE count loss that encourages 60% spike activity on the correct output neuron and 20% on the incorrect neurons.Each inference ran for 100 timesteps (500 ns).All LIF neuron tunable parameters were set to 0.5 V except V leak at 0.58 V. To emulate the SNN, we instantiate a LASANA instance for each neuron with connectivity based on the network.Running Cadence Spectre APS on spiking MNIST took 2404 hours with 95.65% MNIST accuracy.By contrast, MLP LASANA models ran in 1.82 hours (1321x speedup) with MNIST accuracy of 95.63% (0.02% error), average per-inference dynamic energy and latency MAPE of 6.92% and 7.58%, and average total energy and latency error per inference of 2.90% and 5.88%, respectively.</p>
<p>Results show that LASANA produces fast and accurate models for full-system simulations of a wide variety of hybrid analog/digital architectures.In all cases, application accuracy slightly decreases from baseline results due to prediction errors propagating through successive network layers.The spiking model achieved less behavioral error due to the binary nature of spikes, which masks the propagation of small variations.</p>
<p>VI. SUMMARY AND CONCLUSIONS</p>
<p>In this paper, we propose LASANA, an automated MLbased approach for surrogate modeling to provide energy and latency annotation of behavioral models or completely replace analog circuit simulations for large-scale neuromorphic architecture exploration.Our models are general to support a wide range of circuits.To show the flexibility of our methodology, we apply it to a memristive crossbar array and a spiking LIF neuron.Experimental results for MNIST demonstrate up to three orders of magnitude speedup over SPICE, with energy, latency, and behavioral error less than 7%, 8%, and 2%, respectively.In future work, we aim to integrate LASANA into existing digital simulators, apply it to a wider range of circuits, support device variability, and explore LASANA models for circuit-aware application training.</p>
<p>Fig. 1 .
1
Fig. 1.Hybrid architecture with a digital backend and analog compute cores.</p>
<p>Fig. 2 .
2
Fig. 2. (a) An n-length crossbar row, (b) A LIF analog spiking neuron.</p>
<p>Fig. 3 .
3
Fig. 3. LASANA automated dataset and surrogate model generation flow.</p>
<p>of latest circuit states Output: vec e = (e 0 , e 1 , ..., e N −1 ) of per circuit energy Output: vec l = (l 0 , l 1 , ..., l N −1 ) of per circuit latency Output: vec o = (o 0 , o 1 , ..., o N −1 ) of per circuit output 1: e = [0, 0, . . ., 0], l = [0, 0, . . ., 0] 2: I ← ∅ 3: for n ∈ S do ▷ Create idle period batch 4:</p>
<p>8 .
8
Each of the weights and bias were chosen from w ∈ {−1, 0, 1} and 32 inputs from x ∈ [−0.8V, 0.8V ].Dataset generation took 199 s and resulted in 99,353 E1 and 19,826 E2 events.The LIF neuron dataset is composed of 2000 random runs of length 500 ns, simulated in Cadence Spectre at 200 MHz and α = 0.8.Each of the weights and voltage parameters were chosen from w ∈ [−1, 1] and p ∈ [0.5V, 0.8V ], and inputs ranged from x ∈ [0V, 1.5V ] with the number of spikes in each timestep from n ∈ [0, 5].Dataset generation took 403 s and resulted in 31,263 E1 events, 46,315 E2 events, and 68,009 E3 events.All simulations were parallelized across 20 processes, and datasets were split run-wise into 70% for training, 15% for testing, and 15% for validation.</p>
<p>Fig. 6 .
6
Fig. 6.Correlation plots for the CatBoost model on the crossbar.</p>
<p>Fig. 7 .
7
Fig. 7. Correlation plots for the MLP model on the LIF neuron.</p>
<p>Dataset Creation Model Training and Inference
E1Circuit Input Ranges Ranges Circuit Param.Testbench GenerationFiles Circuit Netlists .PWLSpice Simulati on Spice Simulati on Spice Sim. (Virtuoso, HSPICE)Event ProcessingE2 E3Circuit Dataset CreationOutputCircuit DatasetTraining Set Validation SetML Model Training ML Model Training ML Model Training and SelectionState Static Energy Dynamic EnergyWrapperML InferenceArchitectural SimulationTesting SetLatency</p>
<p>4ig.4.Simulation is discretized to three distinct events (E1, E2, E3).Inputs are in gray, and outputs are in red.
Lτt iE1t i+1E2t i+n+1E3t t i+n+2In-OutNone-NoneIn-NoneE1E2 EventE3 Event^Fig.5. transient inference of one analog circuit sub-block.</p>
<p>TABLE I TOTAL
I
MODEL TRAINING AND TESTING TIMES., and the 4 tunable voltage knobs (V leak , V th , V adap , V ref rac ) in the LIF neuron.The crossbar row has a larger input space, but the LIF neuron has stateful, more complex behavior.Behavioral models for both designs were created in SV-RNM and evaluated in Cadence Xcelium 23.03.All runs were performed on a Linux system with a 16-core Intel i7-13700 and 32GB of DDR5 memory.The crossbar dataset is composed of 1000 random runs of length 500 ns, simulated in Synopsys HSPICE at 250 MHz with α = 0.
PCM Crossbar Row [3]LIF Neuron [16]ModelTrain (s)Test (s)Train (s)Test (s)Mean0.0010.00010.0020.0001Table0.254335.34010.0650.1284Linear0.3400.15270.0730.0341CatBoost (d=10)26.0600.024911.2890.0106MLP (100,50)107.0490.079125.9310.0258ing the FreePDK 45nm LP library [17], both with a loadcapacitance of 500 fF. Circuit parameters included as MLfeatures for prediction include the 32 weights and 1 bias inthe crossbar row</p>
<p>Table is a nearest neighbors estimator similar to table-based models in traditional circuit simulators.Table-based inference time is dominated by expensive distance operations.As input dimensionality increases with the crossbar, inference time suffers.Linear minimizes least squares.</p>
<p>TABLE II COMPARISON
II
OF ML MODEL ACCURACY ON THE 32-INPUT CROSSBAR ROW AND LIF SPIKING NEURON.</p>
<p>)
Mean68.41.558106.216.65999.70.21840.162924.500.13127.410.00710.07080.7070Table135.92.011190.721.442173.40.27420.03057.180.01110.420.00340.00890.0292Linear68.31.55789.315.88162.80.19520.072413.860.04116.530.00550.05400.0283CatBoost (d=10)66.21.4972.01.9635.80.01210.01484.690.0076.330.00090.00290.0163MLP (100,50)67.31.56314.65.7534.80.02920.01465.040.0056.790.00100.00280.0161</p>
<p>TABLE III BEHAVIORAL
III
ERROR PROPAGATION FOR A 20,000 LIF NEURON LAYER.
M LM E DM E SM VM OMSE MAPE MSE MAPE MSEMSEMSEModel(ns 2 )(%)(pJ 2 )(%)(pJ 2 )(V 2 )(V 2 )LASANA-O 0.01605.000.0087.690.0021 0.0017 0.0099LASANA-P 0.03647.030.0139.680.0060 0.0052 0.0190Fig. 8. MSE per timestep across a 20,000 neuron layer for 500 ns.state prediction, MSE increased by 3.1x, and output predictionMSE by 1.9x, resulting in an overall decrease in spike accuracyof 0.41%. When MSE is calculated for all events in a timestep,normalized per predictor, and plotted in Fig. 8, MSE does notmonotonically increase. This shows that ML-based behavioralmodeling impacts predictor accuracy, but overall error doesnot worsen over simulation time.</p>
<p>TABLE IV 500
IV
NS SIMULATION RUNTIMES WITH VARYING LIF NEURON LAYER SIZE. ) with MNIST accuracy of 97.61% (1.75% error), average per-inference dynamic energy and latency MAPE of 2.44% and 2.65%, and average total energy and latency error per inference of 0.8% and 2.55%, respectively.
Runtime (s)SpeedupNeuronsSPICESV-RNM (+ ML) Ours SPICE SV-RNM103.500.41 + 0.010.0485.410.010050.290.67 + 0.050.08613.58.210001246.153.23 + 0.080.186736.617.530004101.409.13 + 0.160.38 10735.623.950007464.8914.98 + 0.230.57 13104.626.32000031681.1063.39 + 0.751.83 17275.234.6(82x speedup</p>
<p>Hardware spiking neurons design: Analog or digital?. A Joubert, B Belhadj, O Temam, R Heliot, The 2012 Int. Joint Conf. on Neural Networks (IJCNN). Brisbane, AustraliaJun. 2012</p>
<p>Emerging Materials for Neuromorphic Devices and Systems. M.-K Kim, Y Park, I.-J Kim, J.-S Lee, iScience. 2312101846Dec. 2020</p>
<p>IMAC-Sim: A Circuit-level Simulator For In-Memory Analog Computing Architectures. M H Amin, M E Elbtity, R Zand, Proc. Great Lakes Symp. on VLSI 2023. Great Lakes Symp. on VLSI 2023Knoxville TN USAACMJun. 2023</p>
<p>ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars. A Shafiee, A Nag, N Muralimanohar, R Balasubramonian, J P Strachan, M Hu, R S Williams, V Srikumar, 2016 ACM/IEEE 43rd Annu. Int. Symp. on Computer Architecture (ISCA). Jun. 2016</p>
<p>Towards 10000TOPS/W DNN Inference with Analog in-Memory Computing -A Circuit Blueprint, Device Options and Requirements. S Cosemans, B Verhoef, J Doevenspeck, I A Papistas, F Catthoor, P Debacker, A Mallik, D Verkest, pp. 22.2.1-22.2.4IEEE Int. Electron Devices Meeting (IEDM). Dec. 2019</p>
<p>The BrainScaleS-2 Accelerated Neuromorphic System With Hybrid Plasticity. C Pehle, S Billaudelle, B Cramer, J Kaiser, K Schreiber, Y Stradmann, J Weis, A Leibfried, J Müller, Schemmel, Frontiers in Neuroscience. 162022</p>
<p>RESPARC: A Reconfigurable and Energy-Efficient Architecture with Memristive Crossbars for Deep Spiking Neural Networks. A Ankit, A Sengupta, P Panda, K Roy, Proc. 54th Annu. Design Automation Conference. 54th Annu. Design Automation ConferenceNew York, NY, USAJun. 2017</p>
<p>Beyond real number modeling: Comparison of analog modeling approaches. W Scherr, K Einwich, 2020 Forum for Specification and Design Languages (FDL). Sept. 2020</p>
<p>A System-Level Simulator for RRAM-Based Neuromorphic Computing Chips. M K F Lee, Y Cui, T Somu, T Luo, J Zhou, W T Tang, W.-F Wong, R S M Goh, ACM Trans. Archit. Code Optim. (TACO). 154Jan. 2019</p>
<p>Athena: Enabling codesign for next-generation ai/ml architectures. M Plagge, B Feinberg, J Mcfarland, F Rothganger, S Agarwal, A Awad, C Hughes, S G Cardwell, IEEE Int. Conf. Rebooting Computing (ICRC). 2022</p>
<p>Crosssim: accuracy simulation of analog in-memory computing. B Feinberg, T P Xiao, C J Brinker, C H Bennett, M J Marinella, S Agarwal, </p>
<p>Language Reference Manual, Accellera Systems Initiative. -Ams Verilog, 2014</p>
<p>An ANN-Based Approach to the Modelling and Simulation of Analogue Circuits. A Amaral, A Gusmão, R Vieira, R Martins, N Horta, N Lourenc ¸o, 19th Int. Conf. on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD). Jul. 2023</p>
<p>Power to the Model: Generating Energy-Aware Mixed-Signal Models using Machine Learning. M Grabmann, F Feldhoff, G Gläser, 16th Int. Conf. on Synthesis, Modeling, Analysis and Simulation Methods and Applications to Circuit Design (SMACD). Jul. 2019</p>
<p>Exploring sub-20nm finfet design with predictive technology models. S Sinha, G Yeric, V Chandra, B Cline, Y Cao, Proc. 49th Annu. Design Automation Conference. 49th Annu. Design Automation ConferenceNew York, NY, USAACM2012</p>
<p>A low-power adaptive integrate-and-fire neuron circuit. G Indiveri, Proc. 2003 Int. Symp. on Circuits and Systems. 2003 Int. Symp. on Circuits and SystemsMay 20034</p>
<p>FreePDK: An Open-Source Variation-Aware Design Kit. J E Stine, I Castellanos, M Wood, J Henson, F Love, W R Davis, P D Franzon, M Bucher, S Basavarajaiah, J Oh, R , IEEE Int. Conf. on Microelectronic Systems Education. San Diego, CA, USAIEEEJun. 2007</p>
<p>Scikit-learn: Machine learning in python. F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Journal of Machine Learning Research. 12Nov. 2011</p>
<p>Catboost: unbiased boosting with categorical features. L Prokhorenkova, G Gusev, A Vorobev, A V Dorogush, A Gulin, Proc. 32nd Int. Conf. on Neural Information Processing Systems. 32nd Int. Conf. on Neural Information essing SystemsRed Hook, NY, USACurran Associates Inc2018</p>
<p>Loihi: A neuromorphic manycore processor with on-chip learning. M Davies, N Srinivasa, T.-H Lin, G Chinya, Y Cao, S H Choday, G Dimou, P Joshi, N Imam, S Jain, Y Liao, C.-K Lin, A Lines, R Liu, D Mathaikutty, S Mccoy, A Paul, J Tse, G Venkataramanan, Y.-H Weng, A Wild, Y Yang, H Wang, IEEE Micro. 3812018</p>
<p>Gradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proc. IEEE. IEEE199886</p>
<p>Training spiking neural networks using lessons from deep learning. J K Eshraghian, M Ward, E O Neftci, X Wang, G Lenz, G Dwivedi, M Bennamoun, D S Jeong, W D Lu, Proc. IEEE. IEEE2023111</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4574 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4574</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4574</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13" target="_blank">KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience, demonstrating how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
                <p><strong>Paper Abstract:</strong> Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4574.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4574.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KNIMEZoBot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNIMEZoBot</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A code-free literature-review system that integrates Zotero, KNIME, and OpenAI to perform retrieval-augmented generation (RAG) over a user's Zotero library: it fetches PDFs via Zotero API, chunks documents, indexes embeddings in FAISS, semantically retrieves relevant passages, and uses OpenAI LLMs to synthesize natural-language answers via a KNIME-hosted chatbot UI.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KNIMEZoBot</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>KNIMEZoBot is implemented as a KNIME workflow that (1) collects metadata and PDF attachments from a user's Zotero library via Zotero REST API (Python nodes), (2) loads PDFs using LangChain's unstructuredPDFLoader and splits them into chunks with configurable chunk size and overlap, (3) computes vector embeddings for chunks using OpenAI embeddings (text-embedding-ada-002) via the OpenAI Embeddings Connector node, (4) stores embeddings in a FAISS vector store (FAISS Vector Store Creator node) for semantic retrieval, and (5) performs semantic search (RAG) to retrieve relevant chunks which are passed along with the user query to OpenAI LLMs (selectable GPT-3.5/GPT-4) via KNIME LLM/agent nodes (OpenAI Functions Agent Creator / Agent Prompter) to generate synthesized answers shown in a KNIME-hosted chat app. The KNIME AI Extension, KNIME REST nodes, Python integration, and JSON processing nodes glue the pipeline together and provide a GUI for non-coders to configure API keys, collections, chunking parameters, and model selection.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>OpenAI GPT models (selectable GPT-3.5-Turbo and GPT-4); embedding model: text-embedding-ada-002</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (semantic search) via chunking of PDFs (LangChain) followed by vector similarity search in FAISS; document retrieval is driven by semantic embeddings produced by OpenAI embedding model.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieval-Augmented Generation (RAG): LLM (GPT-3.5/GPT-4) receives the user query plus retrieved text chunks and synthesizes a natural-language answer via agent prompting and a customizable system message.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in the paper (depends on the size of the user's Zotero library and selected collections).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature (any domain present in the user's Zotero library); intended for academic literature reviews across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Synthesized natural-language answers to queries (chatbot responses), extractive passages retrieved from papers, and downloadable chat history (CSV).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No formal quantitative evaluation metrics reported in this paper; only qualitative descriptions and demonstrations of functionality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No quantitative performance results reported (latency, retrieval accuracy, or fidelity metrics not provided).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No baseline systems or formal comparisons reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not applicable / not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Demonstrates that a RAG pipeline combining Zotero, KNIME, LangChain, FAISS, and OpenAI can be assembled in a low-/no-code environment to allow non-coders to query and synthesize information from curated literature collections; RAG mitigates context-window limits by retrieving relevant chunks; provides a user-configurable tool for speeding literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No empirical validation of fidelity; authors acknowledge need for further enhancements to accuracy and sophistication; inherits LLM issues such as hallucination and context-window constraints (mitigated but not eliminated by RAG); relies on external API keys and services; no evaluation of computational cost or robustness to contradictory evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper does not provide empirical scaling measurements; architecture (FAISS + KNIME) is described as able to index arbitrary Zotero libraries, but concrete scaling behavior with increasing numbers of papers or larger models is not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4574.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4574.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM generation with retrieved, corpus-grounded evidence: documents are chunked, embedded, indexed in a vector store, nearest-neighbor retrieval finds semantically relevant chunks for a query, and an LLM generates answers conditioned on those retrieved chunks to mitigate context-window limits and hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>RAG workflows segment large corpora into overlapping chunks, embed each chunk into a vector space using an embedding model, store embeddings in a vector index (e.g., FAISS), convert user queries to embeddings and retrieve semantically similar chunks via vector similarity, then pass the query plus retrieved text to an LLM to generate grounded responses. In this paper RAG is instantiated using LangChain for chunking, OpenAI embeddings (text-embedding-ada-002) and FAISS for indexing, and OpenAI GPT models for generation within KNIME.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>General LLMs (in this paper instantiated with OpenAI GPT models such as GPT-3.5 and GPT-4) and embedding models (text-embedding-ada-002).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval (semantic search) after chunking documents into smaller segments.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>LLM-conditioned natural language generation using retrieved passages (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified; RAG is applicable to any corpus size but this paper does not quantify numbers of papers evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General knowledge-intensive tasks; in this paper applied to academic literature and literature-review workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded natural-language answers / summaries synthesized from retrieved document passages.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Paper references RAG literature (e.g., benchmarking work) but does not itself report metrics; common metrics in RAG literature include retrieval accuracy, human evaluation of factuality, and downstream generation quality (not reported here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No empirical performance results for RAG are reported in this paper's evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared quantitatively against baselines in this work; the paper discusses RAG as an approach that addresses limitations of native chat-only LLM use.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG alleviates context-window limitations and helps ground LLM outputs by providing retrieved evidence; it enables synthesis across multiple documents and repeated Q&A over large corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Requires building and maintaining vector stores and embeddings; usually requires coding to set up (motivating KNIMEZoBot's low-code approach); retrieval failures can still lead to hallucinations if relevant evidence is not retrieved; no guarantees of correctness without verification.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Conceptually scales to large corpora via vector search; paper mentions repeated execution creates a robust Q&A framework but provides no experimental scaling trends.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4574.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4574.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KNIME AI Extension</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KNIME AI Extension (Labs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KNIME extension that exposes nodes for interacting with LLMs, embeddings, and vector stores (e.g., OpenAI, Hugging Face, GPT4ALL, Chroma, FAISS) enabling users to build RAG pipelines and agents inside the KNIME visual environment without coding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>KNIME AI Extension (Labs)</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>KNIME AI Extension (Labs)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Provides KNIME nodes to connect to LLM providers (OpenAI, Hugging Face Hub, GPT4ALL), compute embeddings, connect to vector stores (Chroma, FAISS), and assemble intelligent agents that can select vector stores dynamically and drive conversational interfaces; used in KNIMEZoBot to implement embedding creation, FAISS storage, agent prompting, and the chat interface.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Connectors to OpenAI, Hugging Face, GPT4ALL are supported; in this paper the extension is used with OpenAI GPT models.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Supports embedding-based retrieval via integrated connectors to vector stores (FAISS/Chroma) and embedding nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Agent/LLM-based generation via nodes such as OpenAI Functions Agent Creator and Agent Prompter to synthesize responses from retrieved context.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified (tool-level capability).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General-purpose; used here for scholarly literature retrieval and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Chatbot responses and synthesized answers within KNIME workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No evaluation metrics reported for the extension within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No performance numbers provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No baseline comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The extension enables no-code assembly of RAG and agent workflows inside KNIME, lowering the barrier for non-coders to build LLM-powered literature tools.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Dependency on external models and services; paper does not empirically evaluate limitations such as latency, cost, or failure modes.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Extension supports multiple vector stores and is designed to handle larger datasets through those backends, but no empirical scaling results are provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4574.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4574.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LangChain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Python library for building LLM applications used here to load PDFs (unstructuredPDFLoader) and split long documents into smaller, overlapping chunks that fit model input limits for downstream embedding and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Introduction | Langchain</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LangChain (document loader & chunker)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used inside KNIME Python nodes to ingest full-text PDFs via unstructuredPDFLoader, transform them into text, and split the text into configurable chunk sizes with overlap to respect LLM input token limits; the produced chunks are then passed to embedding and indexing steps.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>LangChain is model-agnostic; in this workflow it is used in conjunction with OpenAI models for embedding and generation.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Document loading and chunking to prepare content for embedding and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Does not synthesize itself; supports RAG by preparing chunks that are later synthesized by LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified; handles arbitrary numbers of documents subject to environment resources.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General document processing for LLM applications; here applied to academic PDFs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Text chunks suitable for embedding and retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No metrics reported in this paper for LangChain's use.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No performance numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Effective for breaking long documents into model-sized chunks to enable indexing and retrieval for RAG pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Chunking can split semantically linked content and requires careful choice of chunk size/overlap; trade-offs between chunk size and retrieval granularity are noted but not empirically explored in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Can process many documents programmatically, but the paper does not report throughput or scaling benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4574.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4574.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FAISS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FAISS Vector Store</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vector-indexing library used to store embeddings and perform efficient nearest-neighbor (semantic similarity) retrieval over document chunks to support RAG-style retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>FAISS Vector Store Creator</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>FAISS Vector Store Creator (KNIME node)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Within KNIMEZoBot the FAISS Vector Store Creator node stores numerical vector representations produced by OpenAI embeddings and provides similarity search functionality to retrieve the most semantically relevant chunks for a given query embedding; it acts as the retrieval backbone of the RAG pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>None directly (FAISS stores embeddings); used together with embedding model text-embedding-ada-002 and LLMs (OpenAI GPT) in the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Approximate nearest neighbor vector similarity search over stored chunk embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Retrieved chunks from FAISS are provided as evidence to LLMs for synthesis (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Depends on the corpus indexed; not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General-purpose semantic retrieval for textual corpora, applied to academic literature here.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Ranked retrieved document chunks (text segments).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No retrieval-accuracy or latency metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No quantitative results reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared against alternative vector stores or retrieval methods in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>FAISS enables efficient semantic retrieval which, when combined with LLMs, allows synthesis across multiple documents and mitigates input-size constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Index size and memory considerations for large corpora are implied but not measured; retrieval quality depends on embedding quality and chunking strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>FAISS is known to scale to large indexes; the paper does not provide experimental scaling data for the deployed workflow.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4574.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4574.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-embedding-ada-002</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>text-embedding-ada-002 (OpenAI embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI embedding model used to convert text chunks into vector representations for semantic retrieval in the FAISS index.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>OpenAI Embeddings Connector</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>text-embedding-ada-002 (via OpenAI Embeddings Connector)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used to compute fixed-size numerical embeddings for each document chunk produced by LangChain; embeddings are stored in FAISS to enable semantic similarity search during RAG retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Embedding model (text-embedding-ada-002); used alongside OpenAI GPT models for generation.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Text-to-vector embeddings for use in vector similarity-based retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Embeddings enable retrieval of context which is then synthesized by LLMs (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified (depends on user's corpus).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General text embedding for semantic retrieval across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Vector embeddings stored for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>No embedding-quality metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No quantitative performance reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not compared to other embedding models in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Serving as the embedding backbone allows semantic retrieval to drive grounded LLM responses; the authors selected text-embedding-ada-002 in the KNIME node configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Embedding quality affects retrieval fidelity; no experiments in the paper quantify this dependence.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Embedding computation scales with corpus size; the paper does not report compute requirements or empirical scaling numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4574.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4574.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chat LLMs (ChatGPT/Claude/Bard)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT / Claude / Bard (LLM chat platforms)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Popular chat-oriented large language model services that can summarize and synthesize across documents but are constrained by model context windows and are susceptible to hallucination; mentioned as motivating the use of RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chat-oriented LLM platforms (ChatGPT, Claude, Bard)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in the introduction as commonly used LLM chat services able to summarize papers and synthesize findings across documents, but limited by context window sizes (e.g., GPT-4 publicly ~5000 words; Claude larger) and prone to hallucination, motivating the adoption of RAG and the development of KNIMEZoBot to handle larger corpora more reliably.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>ChatGPT (GPT-4/GPT-3.5), Anthropic Claude, Google Bard (models referenced at a platform level).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Chat-based summarization and Q&A; not part of the implemented pipeline in this paper (mentioned as background).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Single-pass chat summarization / generation conditioned on provided prompt and context window; contrasted with RAG which retrieves additional evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (mentioned generally).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General-purpose LLM summarization across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries and answers via chat interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported in this paper; general concerns about hallucination and context window limits are cited with references.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not provided in this paper (background discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a motivating baseline (native chat LLMs without retrieval) but not empirically compared in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Native chat LLMs are useful for summarization but have practical limitations (context window, hallucination) that RAG-style systems aim to address.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Context-window size limits the amount of document text that can be provided directly; propensity for hallucinations when not grounded in retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Scaling constrained by model context windows; larger context-window models (e.g., Claude) may handle more text but still face tradeoffs; paper does not provide empirical scaling data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks <em>(Rating: 2)</em></li>
                <li>Benchmarking Large Language Models in Retrieval-Augmented Generation <em>(Rating: 2)</em></li>
                <li>Artificial intelligence and the conduct of literature reviews <em>(Rating: 1)</em></li>
                <li>Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4574",
    "paper_id": "paper-3ddc8365ab4f048a4b8e6c4c3c7a99777c9dcf13",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "KNIMEZoBot",
            "name_full": "KNIMEZoBot",
            "brief_description": "A code-free literature-review system that integrates Zotero, KNIME, and OpenAI to perform retrieval-augmented generation (RAG) over a user's Zotero library: it fetches PDFs via Zotero API, chunks documents, indexes embeddings in FAISS, semantically retrieves relevant passages, and uses OpenAI LLMs to synthesize natural-language answers via a KNIME-hosted chatbot UI.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "KNIMEZoBot",
            "system_description": "KNIMEZoBot is implemented as a KNIME workflow that (1) collects metadata and PDF attachments from a user's Zotero library via Zotero REST API (Python nodes), (2) loads PDFs using LangChain's unstructuredPDFLoader and splits them into chunks with configurable chunk size and overlap, (3) computes vector embeddings for chunks using OpenAI embeddings (text-embedding-ada-002) via the OpenAI Embeddings Connector node, (4) stores embeddings in a FAISS vector store (FAISS Vector Store Creator node) for semantic retrieval, and (5) performs semantic search (RAG) to retrieve relevant chunks which are passed along with the user query to OpenAI LLMs (selectable GPT-3.5/GPT-4) via KNIME LLM/agent nodes (OpenAI Functions Agent Creator / Agent Prompter) to generate synthesized answers shown in a KNIME-hosted chat app. The KNIME AI Extension, KNIME REST nodes, Python integration, and JSON processing nodes glue the pipeline together and provide a GUI for non-coders to configure API keys, collections, chunking parameters, and model selection.",
            "llm_model_used": "OpenAI GPT models (selectable GPT-3.5-Turbo and GPT-4); embedding model: text-embedding-ada-002",
            "extraction_technique": "Embedding-based retrieval (semantic search) via chunking of PDFs (LangChain) followed by vector similarity search in FAISS; document retrieval is driven by semantic embeddings produced by OpenAI embedding model.",
            "synthesis_technique": "Retrieval-Augmented Generation (RAG): LLM (GPT-3.5/GPT-4) receives the user query plus retrieved text chunks and synthesizes a natural-language answer via agent prompting and a customizable system message.",
            "number_of_papers": "Not specified in the paper (depends on the size of the user's Zotero library and selected collections).",
            "domain_or_topic": "General scientific literature (any domain present in the user's Zotero library); intended for academic literature reviews across domains.",
            "output_type": "Synthesized natural-language answers to queries (chatbot responses), extractive passages retrieved from papers, and downloadable chat history (CSV).",
            "evaluation_metrics": "No formal quantitative evaluation metrics reported in this paper; only qualitative descriptions and demonstrations of functionality.",
            "performance_results": "No quantitative performance results reported (latency, retrieval accuracy, or fidelity metrics not provided).",
            "comparison_baseline": "No baseline systems or formal comparisons reported in the paper.",
            "performance_vs_baseline": "Not applicable / not reported.",
            "key_findings": "Demonstrates that a RAG pipeline combining Zotero, KNIME, LangChain, FAISS, and OpenAI can be assembled in a low-/no-code environment to allow non-coders to query and synthesize information from curated literature collections; RAG mitigates context-window limits by retrieving relevant chunks; provides a user-configurable tool for speeding literature reviews.",
            "limitations_challenges": "No empirical validation of fidelity; authors acknowledge need for further enhancements to accuracy and sophistication; inherits LLM issues such as hallucination and context-window constraints (mitigated but not eliminated by RAG); relies on external API keys and services; no evaluation of computational cost or robustness to contradictory evidence.",
            "scaling_behavior": "Paper does not provide empirical scaling measurements; architecture (FAISS + KNIME) is described as able to index arbitrary Zotero libraries, but concrete scaling behavior with increasing numbers of papers or larger models is not reported.",
            "uuid": "e4574.0",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A method that augments LLM generation with retrieved, corpus-grounded evidence: documents are chunked, embedded, indexed in a vector store, nearest-neighbor retrieval finds semantically relevant chunks for a query, and an LLM generates answers conditioned on those retrieved chunks to mitigate context-window limits and hallucination.",
            "citation_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "RAG workflows segment large corpora into overlapping chunks, embed each chunk into a vector space using an embedding model, store embeddings in a vector index (e.g., FAISS), convert user queries to embeddings and retrieve semantically similar chunks via vector similarity, then pass the query plus retrieved text to an LLM to generate grounded responses. In this paper RAG is instantiated using LangChain for chunking, OpenAI embeddings (text-embedding-ada-002) and FAISS for indexing, and OpenAI GPT models for generation within KNIME.",
            "llm_model_used": "General LLMs (in this paper instantiated with OpenAI GPT models such as GPT-3.5 and GPT-4) and embedding models (text-embedding-ada-002).",
            "extraction_technique": "Embedding-based retrieval (semantic search) after chunking documents into smaller segments.",
            "synthesis_technique": "LLM-conditioned natural language generation using retrieved passages (RAG).",
            "number_of_papers": "Not specified; RAG is applicable to any corpus size but this paper does not quantify numbers of papers evaluated.",
            "domain_or_topic": "General knowledge-intensive tasks; in this paper applied to academic literature and literature-review workflows.",
            "output_type": "Grounded natural-language answers / summaries synthesized from retrieved document passages.",
            "evaluation_metrics": "Paper references RAG literature (e.g., benchmarking work) but does not itself report metrics; common metrics in RAG literature include retrieval accuracy, human evaluation of factuality, and downstream generation quality (not reported here).",
            "performance_results": "No empirical performance results for RAG are reported in this paper's evaluation.",
            "comparison_baseline": "Not compared quantitatively against baselines in this work; the paper discusses RAG as an approach that addresses limitations of native chat-only LLM use.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "RAG alleviates context-window limitations and helps ground LLM outputs by providing retrieved evidence; it enables synthesis across multiple documents and repeated Q&A over large corpora.",
            "limitations_challenges": "Requires building and maintaining vector stores and embeddings; usually requires coding to set up (motivating KNIMEZoBot's low-code approach); retrieval failures can still lead to hallucinations if relevant evidence is not retrieved; no guarantees of correctness without verification.",
            "scaling_behavior": "Conceptually scales to large corpora via vector search; paper mentions repeated execution creates a robust Q&A framework but provides no experimental scaling trends.",
            "uuid": "e4574.1",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "KNIME AI Extension",
            "name_full": "KNIME AI Extension (Labs)",
            "brief_description": "A KNIME extension that exposes nodes for interacting with LLMs, embeddings, and vector stores (e.g., OpenAI, Hugging Face, GPT4ALL, Chroma, FAISS) enabling users to build RAG pipelines and agents inside the KNIME visual environment without coding.",
            "citation_title": "KNIME AI Extension (Labs)",
            "mention_or_use": "use",
            "system_name": "KNIME AI Extension (Labs)",
            "system_description": "Provides KNIME nodes to connect to LLM providers (OpenAI, Hugging Face Hub, GPT4ALL), compute embeddings, connect to vector stores (Chroma, FAISS), and assemble intelligent agents that can select vector stores dynamically and drive conversational interfaces; used in KNIMEZoBot to implement embedding creation, FAISS storage, agent prompting, and the chat interface.",
            "llm_model_used": "Connectors to OpenAI, Hugging Face, GPT4ALL are supported; in this paper the extension is used with OpenAI GPT models.",
            "extraction_technique": "Supports embedding-based retrieval via integrated connectors to vector stores (FAISS/Chroma) and embedding nodes.",
            "synthesis_technique": "Agent/LLM-based generation via nodes such as OpenAI Functions Agent Creator and Agent Prompter to synthesize responses from retrieved context.",
            "number_of_papers": "Not specified (tool-level capability).",
            "domain_or_topic": "General-purpose; used here for scholarly literature retrieval and synthesis.",
            "output_type": "Chatbot responses and synthesized answers within KNIME workflows.",
            "evaluation_metrics": "No evaluation metrics reported for the extension within this paper.",
            "performance_results": "No performance numbers provided in this paper.",
            "comparison_baseline": "No baseline comparison provided.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "The extension enables no-code assembly of RAG and agent workflows inside KNIME, lowering the barrier for non-coders to build LLM-powered literature tools.",
            "limitations_challenges": "Dependency on external models and services; paper does not empirically evaluate limitations such as latency, cost, or failure modes.",
            "scaling_behavior": "Extension supports multiple vector stores and is designed to handle larger datasets through those backends, but no empirical scaling results are provided.",
            "uuid": "e4574.2",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LangChain",
            "name_full": "LangChain",
            "brief_description": "A Python library for building LLM applications used here to load PDFs (unstructuredPDFLoader) and split long documents into smaller, overlapping chunks that fit model input limits for downstream embedding and retrieval.",
            "citation_title": "Introduction | Langchain",
            "mention_or_use": "use",
            "system_name": "LangChain (document loader & chunker)",
            "system_description": "Used inside KNIME Python nodes to ingest full-text PDFs via unstructuredPDFLoader, transform them into text, and split the text into configurable chunk sizes with overlap to respect LLM input token limits; the produced chunks are then passed to embedding and indexing steps.",
            "llm_model_used": "LangChain is model-agnostic; in this workflow it is used in conjunction with OpenAI models for embedding and generation.",
            "extraction_technique": "Document loading and chunking to prepare content for embedding and retrieval.",
            "synthesis_technique": "Does not synthesize itself; supports RAG by preparing chunks that are later synthesized by LLMs.",
            "number_of_papers": "Not specified; handles arbitrary numbers of documents subject to environment resources.",
            "domain_or_topic": "General document processing for LLM applications; here applied to academic PDFs.",
            "output_type": "Text chunks suitable for embedding and retrieval.",
            "evaluation_metrics": "No metrics reported in this paper for LangChain's use.",
            "performance_results": "No performance numbers provided.",
            "comparison_baseline": "No comparison provided.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "Effective for breaking long documents into model-sized chunks to enable indexing and retrieval for RAG pipelines.",
            "limitations_challenges": "Chunking can split semantically linked content and requires careful choice of chunk size/overlap; trade-offs between chunk size and retrieval granularity are noted but not empirically explored in the paper.",
            "scaling_behavior": "Can process many documents programmatically, but the paper does not report throughput or scaling benchmarks.",
            "uuid": "e4574.3",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "FAISS",
            "name_full": "FAISS Vector Store",
            "brief_description": "A vector-indexing library used to store embeddings and perform efficient nearest-neighbor (semantic similarity) retrieval over document chunks to support RAG-style retrieval.",
            "citation_title": "FAISS Vector Store Creator",
            "mention_or_use": "use",
            "system_name": "FAISS Vector Store Creator (KNIME node)",
            "system_description": "Within KNIMEZoBot the FAISS Vector Store Creator node stores numerical vector representations produced by OpenAI embeddings and provides similarity search functionality to retrieve the most semantically relevant chunks for a given query embedding; it acts as the retrieval backbone of the RAG pipeline.",
            "llm_model_used": "None directly (FAISS stores embeddings); used together with embedding model text-embedding-ada-002 and LLMs (OpenAI GPT) in the pipeline.",
            "extraction_technique": "Approximate nearest neighbor vector similarity search over stored chunk embeddings.",
            "synthesis_technique": "Retrieved chunks from FAISS are provided as evidence to LLMs for synthesis (RAG).",
            "number_of_papers": "Depends on the corpus indexed; not specified in the paper.",
            "domain_or_topic": "General-purpose semantic retrieval for textual corpora, applied to academic literature here.",
            "output_type": "Ranked retrieved document chunks (text segments).",
            "evaluation_metrics": "No retrieval-accuracy or latency metrics reported in this paper.",
            "performance_results": "No quantitative results reported.",
            "comparison_baseline": "Not compared against alternative vector stores or retrieval methods in this work.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "FAISS enables efficient semantic retrieval which, when combined with LLMs, allows synthesis across multiple documents and mitigates input-size constraints.",
            "limitations_challenges": "Index size and memory considerations for large corpora are implied but not measured; retrieval quality depends on embedding quality and chunking strategy.",
            "scaling_behavior": "FAISS is known to scale to large indexes; the paper does not provide experimental scaling data for the deployed workflow.",
            "uuid": "e4574.4",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "text-embedding-ada-002",
            "name_full": "text-embedding-ada-002 (OpenAI embedding model)",
            "brief_description": "An OpenAI embedding model used to convert text chunks into vector representations for semantic retrieval in the FAISS index.",
            "citation_title": "OpenAI Embeddings Connector",
            "mention_or_use": "use",
            "system_name": "text-embedding-ada-002 (via OpenAI Embeddings Connector)",
            "system_description": "Used to compute fixed-size numerical embeddings for each document chunk produced by LangChain; embeddings are stored in FAISS to enable semantic similarity search during RAG retrieval.",
            "llm_model_used": "Embedding model (text-embedding-ada-002); used alongside OpenAI GPT models for generation.",
            "extraction_technique": "Text-to-vector embeddings for use in vector similarity-based retrieval.",
            "synthesis_technique": "Embeddings enable retrieval of context which is then synthesized by LLMs (RAG).",
            "number_of_papers": "Not specified (depends on user's corpus).",
            "domain_or_topic": "General text embedding for semantic retrieval across domains.",
            "output_type": "Vector embeddings stored for retrieval.",
            "evaluation_metrics": "No embedding-quality metrics reported in this paper.",
            "performance_results": "No quantitative performance reported.",
            "comparison_baseline": "Not compared to other embedding models in this work.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "Serving as the embedding backbone allows semantic retrieval to drive grounded LLM responses; the authors selected text-embedding-ada-002 in the KNIME node configuration.",
            "limitations_challenges": "Embedding quality affects retrieval fidelity; no experiments in the paper quantify this dependence.",
            "scaling_behavior": "Embedding computation scales with corpus size; the paper does not report compute requirements or empirical scaling numbers.",
            "uuid": "e4574.5",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Chat LLMs (ChatGPT/Claude/Bard)",
            "name_full": "ChatGPT / Claude / Bard (LLM chat platforms)",
            "brief_description": "Popular chat-oriented large language model services that can summarize and synthesize across documents but are constrained by model context windows and are susceptible to hallucination; mentioned as motivating the use of RAG.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Chat-oriented LLM platforms (ChatGPT, Claude, Bard)",
            "system_description": "Mentioned in the introduction as commonly used LLM chat services able to summarize papers and synthesize findings across documents, but limited by context window sizes (e.g., GPT-4 publicly ~5000 words; Claude larger) and prone to hallucination, motivating the adoption of RAG and the development of KNIMEZoBot to handle larger corpora more reliably.",
            "llm_model_used": "ChatGPT (GPT-4/GPT-3.5), Anthropic Claude, Google Bard (models referenced at a platform level).",
            "extraction_technique": "Chat-based summarization and Q&A; not part of the implemented pipeline in this paper (mentioned as background).",
            "synthesis_technique": "Single-pass chat summarization / generation conditioned on provided prompt and context window; contrasted with RAG which retrieves additional evidence.",
            "number_of_papers": "Not applicable (mentioned generally).",
            "domain_or_topic": "General-purpose LLM summarization across domains.",
            "output_type": "Summaries and answers via chat interfaces.",
            "evaluation_metrics": "Not reported in this paper; general concerns about hallucination and context window limits are cited with references.",
            "performance_results": "Not provided in this paper (background discussion).",
            "comparison_baseline": "Used as a motivating baseline (native chat LLMs without retrieval) but not empirically compared in this work.",
            "performance_vs_baseline": "Not reported.",
            "key_findings": "Native chat LLMs are useful for summarization but have practical limitations (context window, hallucination) that RAG-style systems aim to address.",
            "limitations_challenges": "Context-window size limits the amount of document text that can be provided directly; propensity for hallucinations when not grounded in retrieved evidence.",
            "scaling_behavior": "Scaling constrained by model context windows; larger context-window models (e.g., Claude) may handle more text but still face tradeoffs; paper does not provide empirical scaling data.",
            "uuid": "e4574.6",
            "source_info": {
                "paper_title": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "rating": 2
        },
        {
            "paper_title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
            "rating": 2
        },
        {
            "paper_title": "Artificial intelligence and the conduct of literature reviews",
            "rating": 1
        },
        {
            "paper_title": "Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation",
            "rating": 1
        }
    ],
    "cost": 0.017919749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation.</h1>
<p>Suad Alshammari ${ }^{1,2}$, Lama Basalelah ${ }^{1,3}$, Walaa Abu Rukbah ${ }^{1,4}$, Ali Alsuhibani ${ }^{1,5}$ and Dayanjan S. Wijesinghe ${ }^{1,6,7}$.</p>
<ol>
<li>Department of Pharmacotherapy and Outcomes Sciences, School of Pharmacy, Virginia Commonwealth University. 2. Faculty of Pharmacy, Northern Border University, Saudi Arabia. 3. Faculty of Pharmacy, Imam Abdulrahman Bin Faisal University, Saudi Arabia. 4. Faculty of Pharmacy, University of Tabuk, Saudi Arabia. 5. Department of Pharmacy Practice, Unaizah College of Pharmacy, Qassim University, Unaizah, Saudi Arabia. 6. Institute for Structural Biology, Drug Discovery and Development, Virginia Commonwealth University, Richmond, Virginia, USA. 7. Da Vinci Center, School of Pharmacy, Virginia Commonwealth University School of Medicine, Richmond, Virginia, USA.</li>
</ol>
<p>Project files to be found at: https://github.com/dayanjan-lab/KNIMEZoBot</p>
<h4>Abstract</h4>
<p>Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed Al tools can expand accessibility and accelerate knowledge building across diverse research domains.</p>
<h1>Introduction:</h1>
<p>The current era witnesses academicians, clinicians, and researchers grappling with the formidable challenge of information overload ${ }^{1,2}$. The incessant surge in published research findings over recent years has significantly outpaced the ability to stay updated. This challenge is poised to intensify with the advent of natural language optimized Al technologies, which are expected to propel the pace of discoveries and subsequent publications at an even faster rate ${ }^{3}$. The dire need for a mechanism to proficiently manage and assimilate this burgeoning knowledge is palpable. Within this complex quandary, two distinct scenarios emerge:</p>
<p>Firstly, the task of extracting precise answers from a pre-existing knowledge corpus poses a hurdle ${ }^{1}$. Researchers typically amass publications pertinent to their expertise in reference libraries. This textual corpus expands over time with the continual influx of new findings. The task of extracting specific information from this meticulously curated content escalates in complexity with the growing volume of publications, compelling users to sift through numerous documents. Consequently, the appeal for an Al-driven platform capable of synthesizing information from multiple different publications to precise queries from an ever-expanding corpus of curated scientific literature within personal and group reference libraries is burgeoning.</p>
<p>Secondly, the endeavor of encapsulating knowledge through exhaustive literature reviews unveils knowledge gaps and unveils avenues for consequential research ${ }^{4}$. This endeavor entails the conduct of meticulous literature reviews which begin with the identification and collation of relevant publications to address the posed inquiries. Post collection, a thorough analysis of the amassed information is essential to derive answers to specific queries. The conventional workflow of executing literature reviews is notably time-consuming and labor-intensive ${ }^{5}$. Given the swift pace of new discoveries, the traditional approach often yields a knowledge summary that becomes obsolete by the time of its completion.</p>
<p>The imperativeness for simplified, automated strategies enabling researchers to query curated literature libraries and routinely refresh their domain knowledge is apparent. The recent strides in artificial intelligence (AI), particularly the Large Language Models (LLMs), harbor the potential to alleviate the aforementioned challenges ${ }^{6}$. Platforms like ChatGPT, Claude, or Bard are proficient in summarizing papers and synthesizing findings across multiple documents ${ }^{7}$. However, their native "chat" formats present certain impediments for academic research. These include constraint of context window lengths ${ }^{8}$ and the propensity for confabulation (hallucination) ${ }^{9}$. For instance, publicly available chatGPT4 has a context window of about 5000 words, while Claude's window extends to 75,000 words, rendering a bulk of academic publications too lengthy for chat GPT. Claude, although capable of summarizing single publications, finds its utility curtailed across multiple documents.</p>
<p>A notable breakthrough addressing the context window limitations and hallucination and aiding in data summarization from diverse documents is the Retrieval Augmented Generation (RAG) approach ${ }^{10,11}$. The operational workflow of RAG commences with the segmentation of broad text corpora into smaller, overlapping textual fragments. Following this, these fragments</p>
<p>are transformed into vector representations and cataloged within a vector-based database. Upon query submission, it's converted into a vector form. A vectorial similarity assessment is then executed to identify all text segments within the database showcasing semantic alignment. The query, along with all relevant text fragments, is forwarded to a Large Language Model (LLM) to formulate a coherent and pertinent response. This technique adeptly navigates the context window constraints, fostering response synthesis across varied domains. The capability to repeatedly execute this process establishes a robust question-and-answer framework, invaluable for extensive literature reviews, serving scholars and medical professionals.</p>
<p>The RAG-based system, although expedient in summarizing information, hitherto necessitated substantial coding knowledge. Recognizing that a sizable faction of academicians and clinicians lacks coding expertise, we orchestrated a code-free, open-source strategy, culminating in the creation of KNIMEZoBot. This innovation amalgamates three pivotal elements: Konstanz Information Miner (KNIME) - a code-free data science platform, Zotero - an open-source reference management system, and GPT4 from Open AI - the chosen language model for knowledge synthesis. KNIMEZoBot heralds a revolutionary stride in enhancing the literature review workflow by seamlessly integrating the prowess of reference managers, scholarly databases, and AI. Through this ingenious approach, we have democratized access to AIpowered research tools, opening doors for those with non-coding backgrounds to harness natural language queries for interacting with the curated publications housed in their Zotero libraries, thereby significantly amplifying the accessibility and utility of Al in academic spheres.</p>
<h1>Materials and Methods</h1>
<p>KNIME: For the development of the KNIMEZoBot platform, KNIME played an integral role in providing the graphical modular interface for building the workflow steps, integrating the Zotero and OpenAI APIs seamlessly via dedicated nodes, processing the extracted text data, creating the FAISS vector indexing workflow, and hosting the final chatbot user interface. KNIME is an open-source platform originating from the University of Konstanz in Germany, catering to data analytics, reporting, and integration needs, with a strong footing in data science and machine learning domains ${ }^{12}$. It's a free, community-enhanced tool, widely embraced by data professionals globally owing to its user-friendly, graphical interface enabling code-free workflow creation, modification, and visualization. KNIME's core strength is its extensive node repository facilitating seamless data pipeline construction for tasks ranging from data preprocessing to advanced analytics using a non/low code approach. It boasts robust data integration, connecting effortlessly to various data sources like databases and web services, thus centralizing data for comprehensive analysis. Scalability is a hallmark of KNIME, adeptly managing small to large datasets, with ease of integration into big data frameworks like Apache Hadoop and Apache Spark. The platform supports building, training, and evaluating machine learning models utilizing popular libraries such as scikit-learn and TensorFlow, alongside offering an array of statistical and analytical techniques. Automation is seamless with KNIME, allowing scheduled workflow executions, while its server facilitates collaborative efforts and workflow sharing. Commercial versions of KNIME extend advanced features and support, enriching its open-source ecosystem. It's a versatile tool for creating insightful reports, visualizing data, and finds applications across</p>
<p>diverse fields including bioinformatics, predictive analytics, business intelligence, and industrial research.</p>
<h1>KNIME extensions used:</h1>
<p>KNIME AI Extension (Labs) ${ }^{13}$ : The KNIME labs extension enables users to leverage powerful large language models (LLMs) from OpenAI, Hugging Face Hub, and GPT4ALL for tasks like chat and text embeddings. It also provides connectivity to vector stores like Chroma and FAISS for building knowledge bases that can inform chatbot responses. The extension allows combining vector stores and LLMs into intelligent agents. These agents can dynamically select the most relevant vector store to query based on the user input, enabling more natural and knowledgeable conversations. Overall, this extension brings together state-of-the-art LLMs and vector stores within the KNIME analytics platform, unlocking new possibilities for building conversational interfaces and knowledge-powered AI assistants.</p>
<p>KNIME REST Client Extension ${ }^{14}$ : The KNIME REST Client Extension provides nodes for making REST API calls within KNIME workflows. This enables seamless integration with web services and APIs.</p>
<p>The Get Request node ${ }^{15}$ is used to send HTTP GET requests to REST endpoints. It allows specifying the URL, headers, query parameters, and authentication settings. The response from the REST API is returned as a JSON/XML document that can be further processed in the KNIME workflow ${ }^{14}$.</p>
<p>KNIME Python 2 Integration (legacy) ${ }^{16}$ : This extension encompasses the legacy version of KNIME Python integration. It facilitates the integration of Python 2 and Python 3 scripts within the KNIME platform. The extension operates by executing Python scripts in a local Python installation, which is not included in the extension package.</p>
<p>KNIME Python Integration ${ }^{17}$ : The "KNIME Python Integration" is the modern and preferred choice for Python integration within KNIME. This extension incorporates nodes that enable the execution of Python 3 scripts seamlessly in the KNIME workflow. Notably, this integration brings substantial performance improvements compared to its legacy counterpart. It also provides enhanced support for handling larger-than-memory datasets. Additionally, this extension comes equipped with a Python installation that includes a curated selection of essential Python packages.</p>
<p>Note: Throughout our workflow, we employed both the "KNIME Python 2 Integration (legacy)" and the "KNIME Python Integration" extensions interchangeably.</p>
<p>KNIME JSON-Processing ${ }^{18}$ : The KNIME JSON-Processing extension provides nodes for working with JSON data within KNIME workflows. It allows for parsing, creating, transforming, and serializing JSON documents. The JSON To Table node enables easy ingestion of JSON data into tabular form for use in KNIME workflows. It reduces the complexity of handling nested JSON structures and schemas.</p>
<p>Python (Version 3.9) ${ }^{19}$ : Integrating of Python and Anaconda with KNIME can be a powerful combination that allows data scientists and analysts to leverage the extensive libraries and capabilities of Python within the KNIME analytics platform. This integration provides a seamless way to utilize Python scripts, packages, and machine learning models within the KNIME workflows. A detailed guide for Python integration in KNIME has been published elsewhere ${ }^{20}$</p>
<p>Specifically, Python nodes were utilized to execute API calls to extract metadata and PDF files from the Zotero reference manager using its REST API bindings. The Python Requests library facilitated sending GET requests to the API and processing the responses. Another vital usage was the Langchain library ${ }^{21}$ within a Python node to load in the full text of PDF papers and segment them into smaller chunks that meet the length limits of the GPT model inputs.</p>
<p>The Python nodes accept inputs from earlier workflow components, run the defined Python logic and code using those inputs, and return any outputs to subsequent nodes in the workflow. For instance, a node might accept a list of extracted PDFs from the Zotero API calls, utilize Langchain to chunk each PDF into shorter text segments, and output these chunks to the next node for vectorization.</p>
<p>The following Python packages were installed and imported within the Python nodes in KNIME to support core functionality. The installation can be done in multiple ways; we used the following:</p>
<p>1- Open Anaconda Prompt from the Start menu.
2- Write this command: "conda activate <your_environment>." Your_environment is the environment name that is set up in the KNIME Python preference.
3- After the name is changed from base to the name of the environment, install the following libraries via pip:</p>
<ul>
<li>pip install pandas openai langchain unstructured fitz PyPDF2 PyMuPDF "unstructured[pdf]"</li>
</ul>
<h1>Zotero:</h1>
<p>Zotero, a free, open-source reference management software ${ }^{22}$, is cherished by a broad spectrum of academia and professionals for easing the collection, organization, and citation of research materials. Originating from George Mason University, it's a boon for scholarly research and writing, streamlining reference, citation, and bibliography management. Key facets include effortless reference collection from diverse sources like websites and academic journals, with automatic citation information extraction from web pages and PDFs. Its intuitive interface facilitates organizing references via folders, tags, and notes, ensuring easy retrieval. A hallmark feature is its citation and bibliography generation in numerous styles like APA and MLA, significantly reducing formatting time. Integration with prevalent word processors like Microsoft Word and Google Docs allows direct citation insertion and bibliography generation in documents, ensuring accuracy and consistency. Its PDF management capability lets users attach, organize, and annotate PDFs within the reference library. Zotero encourages collaborative research through shared library features, vital for research teams. It offers cloud synchronization for easy access across devices and data backup, enhancing data security. Browser extensions for Chrome and</p>
<p>Firefox simplify capturing references online. Being open-source, it's continually evolved by community contributions, and its cross-platform availability extends its reach. Applications are vast, aiding academic research, education, library assistance, and professionals across legal, medical, and media fields in managing and citing a vast array of references effortlessly.</p>
<h1>Results and Discussion</h1>
<h2>KNIMEZoBot:</h2>
<p>The developed application "KNIMEZoBot", represents an innovative integration of Zotero and OpenAI through the code free platform KNIME to streamline literature reviews and research. This project seamlessly combines the above-mentioned Zotero reference manager, with OpenAI's powerful natural language processing capabilities via a RAG based approach using KNIME as the interface. The primary goal is to simplify retrieving PDFs from Zotero libraries and collections and then utilize OpenAI within KNIME workflows to ask insightful questions and extract key information from academic papers.</p>
<p>KNIMEZoBot uses a Retrieval-Augmented Generation (RAG) architecture, first conducting a semantic search to identify relevant passages from retrieved PDFs. It then leverages large language models (in this case OpenAI's GPT models) to synthesize natural language answers based on the extracted information. This enables KNIMEZoBot to provide informative responses to questions by efficiently searching academic papers and distilling salient facts and main points. Overall, the integration of Zotero and OpenAI represents an innovative approach to enhance literature reviews and research by combining reference management, scholarly databases, and OpenAI.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure1: Underlying overall workflow for KNIMEZoBot.</p>
<h2>First component (Setup and configure Zotero):</h2>
<p>In order to effectively use the KNIMEZoBot, users need to follow a series of key steps. The first requirement is selecting the type of Zotero library they want to access - either a personal</p>
<p>Zotero library or a group library. Based on that choice, users will need to input their corresponding Zotero API key, which allows the system to interface with the library.</p>
<p>Additionally, users will need to provide either their personal Zotero user ID if accessing their own library, or the group ID if accessing a shared group library. To assist users in easily finding and copying their user ID or group ID, we have included hyperlinks within the system interface that direct users to Zotero guides with instructions on locating that information.</p>
<p>Furthermore, to enable more targeted searches, users have the option to filter based on Zotero collections. This allows them to refine the content being retrieved from their library down to specific collections, rather than everything in the library. The system was designed to be flexible - some users may want to search across their entire library, while others may want to narrow in on papers from select collections.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Component "Setup Zotero" when executed - User interface of KNIMEZoBot. Users are required to complete the Zotero information fields.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">Step</span><span class="w"> </span><span class="mi">4</span><span class="p">:</span><span class="w"> </span><span class="nx">Interacting</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">Collections</span><span class="p">:</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="err">&quot;</span><span class="nx">Set</span><span class="w"> </span><span class="nx">X</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">specific</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span>
<span class="w">    </span><span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collection</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">own</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="nx">Select</span><span class="w"> </span><span class="nx">Yes</span><span class="err">&#39;</span><span class="nx">X</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">these</span><span class="w"> </span><span class="nx">key</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="mi">2</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="kn">library</span><span class="w"> </span><span class="nx">that</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">PDFs</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="mi">3</span><span class="p">.</span><span class="w"> </span><span class="nx">You</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">all</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">PDF</span><span class="w"> </span><span class="nx">files</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">own</span><span class="w"> </span><span class="kn">library</span>
<span class="w">    </span><span class="nx">Note</span><span class="p">:</span>
<span class="w">    </span><span class="nx">Collection</span><span class="w"> </span><span class="nx">ID</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">part</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">group</span><span class="w"> </span><span class="nx">user</span><span class="w"> </span><span class="nx">URL</span><span class="w"> </span><span class="nx">after</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">collections</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collections</span><span class="o">/</span><span class="nx">COREQAD</span><span class="p">.</span><span class="w"> </span><span class="nx">The</span><span class="w"> </span><span class="nx">collection</span><span class="w"> </span><span class="nx">ID</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">example</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="mi">1</span><span class="nx">COREQAD</span><span class="p">.</span>
<span class="w">    </span><span class="nx">Do</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">want</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">interact</span><span class="w"> </span><span class="nx">with</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">collection</span><span class="p">?</span>
<span class="w">    </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nx">Yes</span>
<span class="w">    </span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="nx">Yes</span>
<span class="w">    </span><span class="nx">Collection</span><span class="w"> </span><span class="nx">ID</span><span class="p">:</span>
<span class="w">    </span><span class="mi">1</span>
<span class="w">    </span><span class="mi">2</span>
</code></pre></div>

<p>Figure 3: Continuation of the first component when executed. We provided options to filter by specific collections.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: The "setup Zotero" component contains widget nodes ${ }^{23}$ that allow the user to input the information required by the system. It also includes text output nodes and a header node that control the appearance of the user interface.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The first metanode processes information from the "setup zotero" component. Specifically, the "python script" node ${ }^{24}$ accesses and extracts all data from the Zotero library. The "get request" node ${ }^{15}$ (a child node) retrieves attached files for each Zotero item. Subsequently, the 'binary objects to files' node ${ }^{25}$ is employed to facilitate the secure storage of PDF documents in a pre-defined temporary directory within the workflow.</p>
<h1>Second component (Setup OpenAI):</h1>
<p>The second core component of the system involves setting up the OpenAI environment according to the user's preferences. Users have the ability to adjust key settings such as chunk size and chunk overlap. Chunk size refers to the maximum number of tokens processed per API request, while overlap determines the number of duplicated tokens between chunks. Giving users control over these parameters enables them to customize the configuration based on their specific computational needs and use case.</p>
<p>After inputting their OpenAI API key, which grants access to the AI models, users can select from a variety of available models offered through the OpenAI API. Users can make a selection from a range of available OpenAI models, including but not limited to GPT-3.5 Turbo and GPT-4.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Component "Setup OpenAI" when executed- Users are required to select chunk size and overlap settings for text processing, enter their OpenAI API key, and select an AI model.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: The "setup OpenAI" component contains widget nodes enabling users to input their OpenAI API key, chunk size, chunk overlap, and select an AI model as required by the system. Additionally, text output nodes and a header node are included for the appearance of the user interface. The "Python Script" node was used to read and split the PDFs to smaller chunks. We used the Langchain package ${ }^{21}$ "unstructuredPDFLoader" to load the documents. The documents were then split into smaller chunks because of size limitation as GPT models have a maximum input size, usually 1024-2048 tokens. Breaking PDFs into smaller chunks allows to feed longer documents into the models.</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: In the metanode, we used the "FAISS Vector Store Creator" node ${ }^{26}$. This node will store the numerical vector representation created by the embedding model from the "OpenAI Embeddings Connector" node ${ }^{27}$. Also, we selected "text-embedding-ada-002" as the embedding model.</p>
<p>The "OpenAI Functions Agent Creator" node ${ }^{28}$ is used to customize the system message. We rote the following message "You are KnimeZoBot, an AI assistant specifically designed to seamlessly integrate the power of the KNIME platform with the vast knowledge stored within your Zotero library. Your mission is to provide the user with a unique and efficient way to access information, answer questions, and streamline users' research tasks by tapping into your personal Zotero library. Get the answer only from the provided information and if it is not store there write "I apologize, but I do not have any information about it in my Zotero library."</p>
<h1>Last component (Chat app):</h1>
<p>The last component of the system is the Chat application, which provides an interactive interface for users to engage with their Zotero library. This chatbot-style app enables users to pose questions and queries about the content of their Zotero library in a natural conversational format. The seamless integration of the chatbot with the Zotero reference database creates a convenient and user-friendly method for users to search for information within their library.</p>
<p>In addition, users have the option to download their full conversation history with the chatbot in a .csv format. This allows users to save all of their questions and the chatbot's responses so they can refer back to the information later.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Final Component "Chat App"- Chat Interface when deployed. This component allows users to ask questions and receive answers through a conversational chatbot. Users can also download their full chat history as a CSV file.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: The final "Chat app" component utilizes an "Agent Prompter" node ${ }^{29}$ to leverage an AI agent, prompts, and the conversation history from the input table to generate responses. The conversation table requires at least two string columns to store previous exchanges. Additionally, an option is provided to save the chat history.</p>
<h1>Conclusion:</h1>
<p>In summary, the KNIMEZoBot represents a promising integration of technologies to expedite literature reviews or undertake natural language queries of existing Zotero libraries. By</p>
<p>unifying the capabilities of Zotero, OpenAI, and KNIME, this system automates laborious tasks such as combing through academic papers to identify relevant information. Researchers can save significant time while benefiting from state-of-the-art Al techniques for synthesizing knowledge in a low code manner. This innovation demonstrates the potential for Al to assume a greater role in accelerating informed research. While further enhancements to the accuracy and sophistication of the automated analysis remain desirable, KNIMEZoBot marks an important step toward streamlining access to critical information in existing literature by domain experts who are not coders by training. By facilitating more rapid and comprehensive understanding of prior work, this system could substantially benefit the research community and knowledge-building process.</p>
<h1>References:</h1>
<ol>
<li>Arnold M, Goldschmitt M, Rigotti T. Dealing with information overload: a comprehensive review. Front Psychol. 2023;14:1122200. doi:10.3389/fpsyg.2023.1122200</li>
<li>Bornmann L, Haunschild R, Mutz R. Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases. Humanit Soc Sci Commun. 2021;8(1):1-15. doi:10.1057/s41599-021-00903-w</li>
<li>Kousha K, Thelwall M. Artificial intelligence to support publishing and peer review: A summary and review. Learn Publ. n/a(n/a). doi:10.1002/leap. 1570</li>
<li>Par G, Kitsiou S. Chapter 9 Methods for Literature Reviews. In: Handbook of eHealth Evaluation: An Evidence-Based Approach [Internet]. University of Victoria; 2017. Accessed November 3, 2023. https://www.ncbi.nlm.nih.gov/books/NBK481583/</li>
<li>Tay A. How to write a superb literature review. Nature. Published online December 4, 2020. doi:10.1038/d41586-020-03422-x</li>
<li>Wagner G, Lukyanenko R, Par G. Artificial intelligence and the conduct of literature reviews. J Inf Technol. 2022;37(2):209-226. doi:10.1177/02683962211048201</li>
<li>Nashwan AJ, Jaradat JH. Streamlining Systematic Reviews: Harnessing Large Language Models for Quality Assessment and Risk-of-Bias Evaluation. Cureus. 15(8):e43023. doi:10.7759/cureus. 43023</li>
<li>Stern J. GPT-4 Has the Memory of a Goldfish. The Atlantic. Published March 17, 2023. Accessed November 3, 2023. https://www.theatlantic.com/technology/archive/2023/03/gpt-4-has-memory-context-window/673426/</li>
<li>Sharun K, Banu SA, Pawde AM, et al. ChatGPT and artificial hallucinations in stem cell research: assessing the accuracy of generated references - a preliminary study. Ann Med Surg 2012. 2023;85(10):5275-5278. doi:10.1097/MS9.0000000000001228</li>
<li>Lewis P, Perez E, Piktus A, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Published online April 12, 2021. doi:10.48550/arXiv.2005.11401</li>
<li>Chen J, Lin H, Han X, Sun L. Benchmarking Large Language Models in RetrievalAugmented Generation. Published online September 4, 2023. doi:10.48550/arXiv.2309.01431</li>
<li>Berthold MR, Cebron N, Dill F, et al. KNIME - the Konstanz information miner: version 2.0 and beyond. ACM SIGKDD Explor Newsl. 2009;11(1):26-31. doi:10.1145/1656274.1656280</li>
<li>KNIME AI Extension (Labs). KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest</li>
<li>
<p>KNIME REST Client Extension. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.rest/latest</p>
</li>
<li>
<p>GET Request. KNIME Community Hub. Accessed November 7, 2023.
https://hub.knime.com/knime/extensions/org.knime.features.rest/latest/org.knime.rest.nodes .get.RestGetNodeFactory</p>
</li>
<li>KNIME Python 2 Integration (legacy). KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python2/latest</li>
<li>KNIME Python Integration. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python3.scripting/latest</li>
<li>KNIME JSON-Processing. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.json/latest</li>
<li>Python Release Python 3.9.0. Python.org. Accessed November 7, 2023. https://www.python.org/downloads/release/python-390/</li>
<li>KNIME Python Integration Guide. Accessed November 7, 2023. https://docs.knime.com/2021-12/python_installation_guide/index.html#_introduction</li>
<li>Introduction | Langchain. Accessed November 7, 2023. https://python.langchain.com/docs/get_started/introduction</li>
<li>credits_and_acknowledgments [Zotero Documentation]. Accessed November 3, 2023. https://www.zotero.org/support/credits_and_acknowledgments#about_zotero</li>
<li>Explore the Wonderful World of KNIME Widgets. KNIME. Accessed November 7, 2023. https://www.knime.com/blog/mini-guide-widget-examples</li>
<li>Python Script. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.python3.scripting/latest/org.kni me.python3.scripting.nodes.script.PythonScriptNodeFactory</li>
<li>Binary Objects to Files. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.features.base.filehandling/latest/org.kni me.base.filehandling.binaryobjects.writer.BinaryObjectsToFilesNodeFactory</li>
<li>FAISS Vector Store Creator. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:e1168c28</li>
<li>OpenAI Embeddings Connector. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:3a4ffd4b</li>
<li>OpenAI Functions Agent Creator. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:232d61e6</li>
<li>Agent Prompter. KNIME Community Hub. Accessed November 7, 2023. https://hub.knime.com/knime/extensions/org.knime.python.features.llm/latest/org.knime.pyth on3.nodes.extension.ExtensionNodeSetFactory\$DynamicExtensionNodeFactory:378eea</li>
</ol>
<p>https://github.com/dayanjan-lab/KNIMEZoBot</p>            </div>
        </div>

    </div>
</body>
</html>
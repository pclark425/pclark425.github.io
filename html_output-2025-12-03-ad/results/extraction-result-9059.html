<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9059 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9059</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9059</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-3d85460139dd4a49e3d601daebfce86c50577bca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/3d85460139dd4a49e3d601daebfce86c50577bca" target="_blank">Cognitive Effects in Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> European Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> It is found that LLMs are indeed prone to several human cognitive effects, and it is shown that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent.</p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) such as ChatGPT have received enormous attention over the past year and are now used by hundreds of millions of people every day. The rapid adoption of this technology naturally raises questions about the possible biases such models might exhibit. In this work, we tested one of these models (GPT-3) on a range of cognitive effects, which are systematic patterns that are usually found in human cognitive tasks. We found that LLMs are indeed prone to several human cognitive effects. Specifically, we show that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent. We describe our methodology, and specifically the way we converted real-world experiments to text-based experiments. Finally, we speculate on the possible reasons why GPT-3 exhibits these effects and discuss whether they are imitated or reinvented.</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9059.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9059.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 - Priming</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 - Priming effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that GPT-3 was tested for the cognitive priming effect (a systematic pattern found in human cognitive tasks) and that GPT-3 exhibited this effect in text-based adaptations of experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper states that GPT-3 was the LLM tested; no architectural details, training data, or model size are provided in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Priming effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Described in the paper as one of several 'cognitive effects'—systematic patterns usually found in human cognitive tasks; the abstract does not provide a detailed operationalization for the priming tests in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Effect present — GPT-3 exhibited the priming effect (qualitative statement from abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative match: GPT-3 showed the same systematic pattern (no quantitative comparison to human performance provided in the available text).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>The authors converted real-world experiments to text-based experiments for GPT-3; specific prompts, context windows, trial counts, or scoring procedures are not provided in the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Only qualitative presence/absence is reported in the abstract; no numeric performance or human baseline values are given in the available text. The authors note methodological differences due to converting tasks into text format and later speculate about whether observed effects are imitated or reinvented by the model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9059.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9059.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 - Distance</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 - Distance effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was evaluated for the distance effect (a cognitive pattern observed in human tasks) and the paper reports GPT-3 exhibited this effect in the text-based experiments described.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 is the single LLM tested according to the paper; the abstract provides no further model specifications.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Distance effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Included as one of the cognitive effects tested; the abstract frames it as a systematic pattern found in human cognitive tasks but does not provide the experiment's operational details in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Effect present — GPT-3 exhibited the distance effect (qualitative statement from abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitative match to human-like pattern; no quantitative comparison provided in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments were adapted into text-based formats for GPT-3; specific prompt formats, control conditions, or metrics are not described in the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Abstract-only reporting gives presence/absence without numerical scores or human comparisons. Authors highlight challenges of task conversion to text and discuss whether observed effects reflect imitation of human behavior or independent emergence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9059.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9059.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 - SNARC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 - SNARC effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that GPT-3 displays the SNARC effect (a well-known cognitive effect in humans) when the relevant experiments are converted to text-based tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Identified in the paper as the tested LLM; further model architecture or training details are not provided in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>SNARC effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Listed among the cognitive effects examined; the abstract characterizes the tested phenomena as systematic human cognitive patterns but does not include the experimental protocol in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Effect present — GPT-3 exhibited the SNARC effect (qualitative statement from abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported as showing a human-like systematic pattern; no quantitative human-LLM comparison is provided in the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>The authors adapted real-world (presumably non-text) experiments into text-based experiments for GPT-3; the abstract provides no further methodological specifics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>No numerical performance metrics or human baseline statistics are reported in the available text. The paper notes the broader question of whether such effects are imitated from training data or arise de novo in the model.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9059.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9059.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 - Size congruity</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 - Size congruity effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-3 was tested for the size congruity effect and the abstract reports that GPT-3 exhibited this effect in the authors' text-based experimental adaptations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reported simply as 'GPT-3' in the paper; no further model specification is provided in the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Size congruity effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>One of the cognitive effects tested and described generically as a systematic pattern typically observed in human cognitive tasks; the abstract does not give task details.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Effect present — GPT-3 exhibited the size congruity effect (qualitative statement from abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitatively similar pattern to humans reported; no quantitative comparisons provided in the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Experiments were converted into text-based formats for GPT-3; the abstract does not list prompt designs, number of trials, or scoring criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Findings in the abstract are qualitative. The authors acknowledge methodological considerations when converting experiments and discuss interpretive limits (e.g., imitation vs. independent emergence).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9059.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9059.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 - Anchoring</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 3 - Anchoring effect</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper reports that GPT-3 did not display the anchoring effect in the text-based experimental adaptations described.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognitive Effects in Large Language Models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-3 is explicitly tested according to the abstract; the paper's abstract does not provide architecture, dataset, or size details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Anchoring effect</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Included in the list of cognitive effects tested; the abstract indicates the anchoring effect (a known human cognitive bias) was tested but gives no task details in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Effect absent — GPT-3 did not exhibit the anchoring effect in the experiments reported in the abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Qualitatively below the typical human pattern for anchoring (human anchoring effect is commonly observed), though no quantitative human baselines or statistics are provided in the available text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Reported experiments involved converting real-world tasks into text-based prompts for GPT-3; the abstract omits detailed prompt engineering, trial numbers, and scoring methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The abstract-level report gives presence/absence only and lacks numerical measures and direct human comparison data. The authors flag interpretive caveats about task conversion and consider whether observed patterns are imitation of training data or independent model phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Cognitive Effects in Large Language Models', 'publication_date_yy_mm': '2023-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9059",
    "paper_id": "paper-3d85460139dd4a49e3d601daebfce86c50577bca",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "GPT-3 - Priming",
            "name_full": "Generative Pre-trained Transformer 3 - Priming effect",
            "brief_description": "The paper reports that GPT-3 was tested for the cognitive priming effect (a systematic pattern found in human cognitive tasks) and that GPT-3 exhibited this effect in text-based adaptations of experiments.",
            "citation_title": "Cognitive Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "The paper states that GPT-3 was the LLM tested; no architectural details, training data, or model size are provided in the available text.",
            "model_size": null,
            "test_battery_name": "Priming effect",
            "test_description": "Described in the paper as one of several 'cognitive effects'—systematic patterns usually found in human cognitive tasks; the abstract does not provide a detailed operationalization for the priming tests in the available text.",
            "llm_performance": "Effect present — GPT-3 exhibited the priming effect (qualitative statement from abstract).",
            "human_baseline_performance": null,
            "performance_comparison": "Qualitative match: GPT-3 showed the same systematic pattern (no quantitative comparison to human performance provided in the available text).",
            "experimental_details": "The authors converted real-world experiments to text-based experiments for GPT-3; specific prompts, context windows, trial counts, or scoring procedures are not provided in the abstract.",
            "limitations_or_caveats": "Only qualitative presence/absence is reported in the abstract; no numeric performance or human baseline values are given in the available text. The authors note methodological differences due to converting tasks into text format and later speculate about whether observed effects are imitated or reinvented by the model.",
            "uuid": "e9059.0",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 - Distance",
            "name_full": "Generative Pre-trained Transformer 3 - Distance effect",
            "brief_description": "GPT-3 was evaluated for the distance effect (a cognitive pattern observed in human tasks) and the paper reports GPT-3 exhibited this effect in the text-based experiments described.",
            "citation_title": "Cognitive Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "GPT-3 is the single LLM tested according to the paper; the abstract provides no further model specifications.",
            "model_size": null,
            "test_battery_name": "Distance effect",
            "test_description": "Included as one of the cognitive effects tested; the abstract frames it as a systematic pattern found in human cognitive tasks but does not provide the experiment's operational details in the available text.",
            "llm_performance": "Effect present — GPT-3 exhibited the distance effect (qualitative statement from abstract).",
            "human_baseline_performance": null,
            "performance_comparison": "Qualitative match to human-like pattern; no quantitative comparison provided in the available text.",
            "experimental_details": "Experiments were adapted into text-based formats for GPT-3; specific prompt formats, control conditions, or metrics are not described in the abstract.",
            "limitations_or_caveats": "Abstract-only reporting gives presence/absence without numerical scores or human comparisons. Authors highlight challenges of task conversion to text and discuss whether observed effects reflect imitation of human behavior or independent emergence.",
            "uuid": "e9059.1",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 - SNARC",
            "name_full": "Generative Pre-trained Transformer 3 - SNARC effect",
            "brief_description": "The paper reports that GPT-3 displays the SNARC effect (a well-known cognitive effect in humans) when the relevant experiments are converted to text-based tasks.",
            "citation_title": "Cognitive Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Identified in the paper as the tested LLM; further model architecture or training details are not provided in the available text.",
            "model_size": null,
            "test_battery_name": "SNARC effect",
            "test_description": "Listed among the cognitive effects examined; the abstract characterizes the tested phenomena as systematic human cognitive patterns but does not include the experimental protocol in the available text.",
            "llm_performance": "Effect present — GPT-3 exhibited the SNARC effect (qualitative statement from abstract).",
            "human_baseline_performance": null,
            "performance_comparison": "Reported as showing a human-like systematic pattern; no quantitative human-LLM comparison is provided in the abstract.",
            "experimental_details": "The authors adapted real-world (presumably non-text) experiments into text-based experiments for GPT-3; the abstract provides no further methodological specifics.",
            "limitations_or_caveats": "No numerical performance metrics or human baseline statistics are reported in the available text. The paper notes the broader question of whether such effects are imitated from training data or arise de novo in the model.",
            "uuid": "e9059.2",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 - Size congruity",
            "name_full": "Generative Pre-trained Transformer 3 - Size congruity effect",
            "brief_description": "GPT-3 was tested for the size congruity effect and the abstract reports that GPT-3 exhibited this effect in the authors' text-based experimental adaptations.",
            "citation_title": "Cognitive Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "Reported simply as 'GPT-3' in the paper; no further model specification is provided in the abstract.",
            "model_size": null,
            "test_battery_name": "Size congruity effect",
            "test_description": "One of the cognitive effects tested and described generically as a systematic pattern typically observed in human cognitive tasks; the abstract does not give task details.",
            "llm_performance": "Effect present — GPT-3 exhibited the size congruity effect (qualitative statement from abstract).",
            "human_baseline_performance": null,
            "performance_comparison": "Qualitatively similar pattern to humans reported; no quantitative comparisons provided in the abstract.",
            "experimental_details": "Experiments were converted into text-based formats for GPT-3; the abstract does not list prompt designs, number of trials, or scoring criteria.",
            "limitations_or_caveats": "Findings in the abstract are qualitative. The authors acknowledge methodological considerations when converting experiments and discuss interpretive limits (e.g., imitation vs. independent emergence).",
            "uuid": "e9059.3",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        },
        {
            "name_short": "GPT-3 - Anchoring",
            "name_full": "Generative Pre-trained Transformer 3 - Anchoring effect",
            "brief_description": "The paper reports that GPT-3 did not display the anchoring effect in the text-based experimental adaptations described.",
            "citation_title": "Cognitive Effects in Large Language Models",
            "mention_or_use": "use",
            "model_name": "GPT-3",
            "model_description": "GPT-3 is explicitly tested according to the abstract; the paper's abstract does not provide architecture, dataset, or size details.",
            "model_size": null,
            "test_battery_name": "Anchoring effect",
            "test_description": "Included in the list of cognitive effects tested; the abstract indicates the anchoring effect (a known human cognitive bias) was tested but gives no task details in the provided text.",
            "llm_performance": "Effect absent — GPT-3 did not exhibit the anchoring effect in the experiments reported in the abstract.",
            "human_baseline_performance": null,
            "performance_comparison": "Qualitatively below the typical human pattern for anchoring (human anchoring effect is commonly observed), though no quantitative human baselines or statistics are provided in the available text.",
            "experimental_details": "Reported experiments involved converting real-world tasks into text-based prompts for GPT-3; the abstract omits detailed prompt engineering, trial numbers, and scoring methodology.",
            "limitations_or_caveats": "The abstract-level report gives presence/absence only and lacks numerical measures and direct human comparison data. The authors flag interpretive caveats about task conversion and consider whether observed patterns are imitation of training data or independent model phenomena.",
            "uuid": "e9059.4",
            "source_info": {
                "paper_title": "Cognitive Effects in Large Language Models",
                "publication_date_yy_mm": "2023-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.007612,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><!DOCTYPE html>
<html lang='en'>
<head>

<!--Google consent mode V2-->
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    if (localStorage.getItem('consentMode') === null) {
        gtag('consent', 'default', {
            'ad_storage': 'denied',
            'ad_user_data': 'denied',
            'ad_personalization': 'denied',
            'analytics_storage': 'denied',
            'personalization_storage': 'granted',
            'functionality_storage': 'granted',
            'security_storage': 'granted',
        });
    } else {
        gtag('consent', 'default', JSON.parse(localStorage.getItem('consentMode')));
    }
    gtag('set', 'ads_data_redaction', true);
    gtag('set', 'url_passthrough', true);
</script>

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-W7N3WL5P');</script>
<!-- End Google Tag Manager -->


    <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
        <title>IOS Press Ebooks - Cognitive Effects in Large Language Models</title>


    <meta name="DC.type" content="Text" />
    <!--Publisher-->
    <meta name="citation_publisher" content="IOS Press" />
    <!--Volume-->
    <meta name="citation_inbook_title" content="ECAI 2023" />
    <!--Article-->
    <meta name="citation_firstpage" content="2105" />
    <meta name="citation_lastpage" content="2112" />
    <meta name="citation_title" content="Cognitive Effects in Large Language Models" />
        <meta name="citation_author" content="Shaki, Jonathan" />
        <meta name="citation_author" content="Kraus, Sarit" />
        <meta name="citation_author" content="Wooldridge, Michael" />

        <meta name="citation_publication_date" content="2023" /><!-- year only is allowed, full date possible-->

        <meta name="citation_online_date" content="2023/09/29" />

        <meta name="citation_doi" content="10.3233/FAIA230505" />
        <meta name="citation_abstract_html_url" content="https://ebooks.iospress.nl/doi/10.3233/FAIA230505" />
        <meta name="citation_fulltext_html_url" content="https://ebooks.iospress.nl/doi/10.3233/FAIA230505" />

        <meta name="citation_fulltext_world_readable" content="">


        <link href="/Content/themes/iospress/css?v=ljnAn2YK9VAuWQ6yMHuVpt0PyO6URwXlRsslkMHliAM1" rel="stylesheet"/>

    <script src="https://ajax.aspnetcdn.com/ajax/jQuery/jquery-3.5.1.min.js"></script>

    <script src="/bundles/jqueryval?v=S0zjY_C9SrWh_jAKguTKrt0UAVoLX4jD0DmtC1N35rA1"></script>

</head>
<body>
    <!-- HEADER -->
    <header id="header">
        <div id="default_header">
    <div id="logo"><img alt="IOS Press Logo" src="/Content/themes/iospress/images/ioslogo.png" /></div>
    <div id="divider"></div>
    <div id="title">IOS Press Ebooks</div>
    <div style="position:relative; float: right; width: 300px;">


    <!-- NOT LOGGED IN -->
    <div id="header_accountbox" class="loggedout">
        <div class="accountbox_welcome">Guest Access</div>
        <div id="header_accountbox_qmark">?</div>
        <div class="accountbox_actions">


            <a data-dialog-title="Identification" href="/Account/Login?returnUrl=%2FDOI%2F10.3233%2FFAIA230505" id="loginLink">Log in</a>
        </div>
        <div id="header_accountbox_infobox">
            <div id="header_accountbox_infobox_arrow"></div>
            <div id="header_accountbox_infobox_content">
                As a guest user you are not logged in or recognized by your IP address. You have
                access to the Front Matter, Abstracts, Author Index, Subject Index and the full
                text of Open Access publications.
            </div>
        </div>
    </div>

<script type="text/javascript">
    $(document).ready(function () {

        $("#header_accountbox_qmark").mouseenter(function () {
            $("#header_accountbox_infobox").show();
        });
        $("#header_accountbox_qmark").mouseleave(function () {
            $("#header_accountbox_infobox").hide();
        });

    });
</script>



    </div>
<div id="menu_wrapper">
    <div id="menu-primary" class="menu-container">
        <div class="menu" role="navigation" aria-label="Main Menu">
            <ul id="menu-primary-items" role="menubar">
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/"><span>Home</span></a></li>                    
                    <li role="presentation" class="menu-item current-menu-item"><a role="menuitem" href="/Publication/Books"><span>Ebooks</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publication/OpenAccess"><span>Open Access</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publisher/About"><span>About IOS Press</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publisher/Offices"><span>Contact</span></a></li>                    
                    <li role="presentation" class="menu-item"><a role="menuitem" href="/Publisher/Faq"><span>FAQ</span></a></li>                    
            </ul>
        </div>
    </div>
</div>
</div>
    </header>

    <!-- END HEADER -->
    <div id="content" class="clearfix">
        <div id="leftcolumn">

    <div class="searchtitle" id="searchlabel">Search</div>
<div id="sidesearchbox" style="font-size: 11px; width: 100%;" class="clearfix">
<form action="/Search" method="post">        <div style="float: left; width: 100px;"><input aria-labelledby="searchlabel" class="searchterm" id="SearchTerm" name="SearchTerm" title="Search ebooks" type="text" value="" /></div>
<input data-val="true" data-val-required="The SearchInAuthors field is required." id="SearchInAuthors" name="SearchInAuthors" type="hidden" value="True" /><input data-val="true" data-val-required="The SearchInTitle field is required." id="SearchInTitle" name="SearchInTitle" type="hidden" value="True" /><input data-val="true" data-val-required="The SearchInKeywords field is required." id="SearchInKeywords" name="SearchInKeywords" type="hidden" value="True" /><input data-val="true" data-val-required="The SearchInISSNISBN field is required." id="SearchInISSNISBN" name="SearchInISSNISBN" type="hidden" value="True" />        <div style="clear: both; padding-top: 12px;" align="right">
            <input type="image" value="SideSearch" src="/Content/themes/iospress/images/searchbutton.png" alt="Submit Search" class="submit" />
        </div>
</form></div>


            <div class="partialContents" data-url="/Subject/BrowserAsync">
                <img alt="loader" src="/Content/Images/indicator.white.gif" /> loading subjects...
            </div>


        </div>
        <main id="contentcolumn">




<div class="bookseriesvolumearticleheader">
    <div class="content">
        <div class="cover"><img alt="cover" class="volume" src="/Content/themes/iospress/images/article.gif" /></div>
        <div class="metadata">

            <div class="value title">Cognitive Effects in Large Language Models</div>

                <span class="metadata_label">Authors</span>
                <div class="value authors">Jonathan Shaki, Sarit Kraus, Michael Wooldridge</div>

                <span class="metadata_label">Pages</span>
                <div class="value pages">2105 - 2112</div>

                <span class="metadata_label">DOI</span>
                <div class="value doi">10.3233/FAIA230505</div>

                <span class="metadata_label">Category</span>
                <div class="value category">Research Article</div>

            <span class="metadata_label">Series</span>
            <div class="value book"><a href="/bookseries/frontiers-in-artificial-intelligence-and-applications">Frontiers in Artificial Intelligence and Applications</a></div>

            <span class="metadata_label">Ebook</span>
                <div class="value book"><a href="/volume/ecai-2023-26th-european-conference-on-artificial-intelligence-including-12th-conference-on-prestigious-applications-of-intelligent-systems-pais-2023">Volume 372: ECAI 2023</a></div>

                <div class="abstract">
                    <b>Abstract</b><br />
                    <section>
  <p>Large Language Models (LLMs) such as ChatGPT have received enormous attention over the past year and are now used by hundreds of millions of people every day. The rapid adoption of this technology naturally raises questions about the possible biases such models might exhibit. In this work, we tested one of these models (GPT-3) on a range of cognitive effects, which are systematic patterns that are usually found in human cognitive tasks. We found that LLMs are indeed prone to several human cognitive effects. Specifically, we show that the priming, distance, SNARC, and size congruity effects were presented with GPT-3, while the anchoring effect is absent. We describe our methodology, and specifically the way we converted real-world experiments to text-based experiments. Finally, we speculate on the possible reasons why GPT-3 exhibits these effects and discuss whether they are imitated or reinvented.</p>
</section>
                </div>
        </div>
        <div class="actions">


            <form action="/Download/Pdf" id="downloadform64437" method="post">        <input type="hidden" name="id" value="64437" />
        <div id='downloadlink64437' class="button getpdf">Download PDF</div>
</form>    <script type="text/javascript">
    $(function () {
        var $link = $('div#downloadlink64437');
        var $form = $('form#downloadform64437');

        $link.on('click', function (e) {
            if ($link.hasClass('busy')) return;   // already clicked once

            $link.addClass('busy');               // mark as locked
            $form.submit();                       // original behaviour

            setTimeout(function () {              // unlock after 2 s
                $link.removeClass('busy');
            }, 2000);
        });
    });
    </script>







<div class="button openaccesslicense">
    <a rel="license" target="_blank" title="This work is licensed under a Creative Commons License"
       href="https://creativecommons.org/licenses/by-nc/4.0/deed.en_US">
        <img alt="Creative Commons License" style="border-width: 0" src="https://i.creativecommons.org/l/by-nc/4.0/80x15.png" />
    </a>
</div>


        </div>
    </div>
</div>



        </main>
    </div>
    <footer id="footer" class="clearfix">
        <script type="text/javascript">
    $(document).ready(function () {

        $('[id^="tip_"]').mouseenter(function () {
            $(this).stop(true);
            $(this).hide().show();
        });
        $('.tip').mouseleave(function () {
            $('[id^="tip_"]').fadeOut(600, function () {
                $('[id^="tip_"]').hide()
            });
        });

        $('[name^="atip_"]').mouseenter(function () {
            var idtoshow = $(this).attr('name').substring(1);
            $('#' + idtoshow).hide().fadeIn(600, function () {
                $('#' + idtoshow).show()
            });
        });

        $('[name^="atip_"]').mouseleave(function () {
            var idtoshow = $(this).attr('name').substring(1);
            $('#' + idtoshow).fadeOut(600, function () {
                $('#' + idtoshow).hide()
            });
        });
    });
</script>
<div id="default_footer">
    <div id="footer_contact">
        <h2>
            Contact
        </h2>

        <ul class="list" style="float: left">
            <li>&nbsp;</li>
            <li><a name="atip_europe" href="/Publisher/Offices">IOS Press / Sage Publishing</a></li>
            <li>&nbsp;</li>
        </ul>

        <a name="atip_usa" id="usa" class="usa" href="#" title="North America"></a>
        <a name="atip_europe" id="europe" class="europe" title="Europe" href="/Publisher/Offices"></a>
        <a name="atip_asia" id="asia" class="asia" title="Asia" href="#"></a>

        <div id="tip_europe" style="display: none;">
            <div class="tip">
                <strong>IOS Press / Sage Publishing</strong><br />
                Teleportboulevard 120<br />
                1043 EJ Amsterdam<br />
                The Netherlands<br /><br />

                <strong>Sage Publishing</strong><br />
                <a href="https://www.sagepub.com/contact-and-support/contact-us" target="_blank">Contact Sage</a>
            </div>
        </div>

    </div>
    <div id="footer_divider">
    </div>
    <div id="footer_links">
        <h2>
            Copyright 2025 © IOS Press
        </h2>
        <a href="/publisher/Disclaimer/">Disclaimer</a> &nbsp;&nbsp;&nbsp; <a href="/publisher/TermsOfUse/">
            Terms of use
        </a>&nbsp;&nbsp;&nbsp;<a href="/publisher/PrivacyPolicy/">
            Privacy Policy
        </a>&nbsp;&nbsp;&nbsp;<a href="/publisher/Contact/">
            Contact
        </a>&nbsp;&nbsp;&nbsp;<a href="/publisher/Faq/">FAQ</a>
    </div>
    <div id="footer_twitter">

        <a href="https://twitter.com/IOSPress_STM" class="twitter-follow-button" data-show-screen-name="false" data-show-count="false">Follow @twitter</a>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

    </div>
</div>

    </footer>
    <script type="text/javascript">
        if (typeof jQuery == 'undefined') {
            var e = document.createElement('script');
            e.src = '/Scripts/jquery-3.5.1.min.js';
            e.type = 'text/javascript';
            document.getElementsByTagName("head")[0].appendChild(e);
        }
    </script>
    <script src="/bundles/subjectmenu?v=jieOsy0lZdhfbuK00Z1qohST722ZF4yIVuWCjPrX6ZE1"></script>

    <script src="/bundles/pubility?v=meEJdaSXIsVeUXAotcBmWyMjARi2G38fFL4j3SAMa9c1"></script>



    <script type="text/javascript">
        $(document).ready(function () {
            initExpandBoxes(10);
        })
    </script>



<style>
    .cookie-consent-banner {
        display: none;
        position: fixed;
        bottom: 0;
        left: 0;
        right: 0;
        background-color: #f8f9fa;
        box-shadow: 0 -2px 10px rgba(0, 0, 0, 0.1);
        color: black;
        padding: 15px;
        font-size: 14px;
        text-align: center;
        z-index: 1000;
    }

        .cookie-consent-banner h3 {
            padding: 0px 0px;
            font-size: 14px;
            font-weight: bolder;
        }

    .cookie-consent-button {
        border: none;
        padding: 8px 16px;
        text-align: center;
        text-decoration: none;
        display: inline-block;
        font-size: 14px;
        margin: 4px 2px;
        cursor: pointer;
        border-radius: 4px;
    }

        .cookie-consent-button:hover {
            box-shadow: 0 -2px 5px rgba(0, 0, 0, 0.2);
        }

        .cookie-consent-button:active {
            opacity: .5;
        }

        .cookie-consent-button.btn-success {
            background-color: #34a853;
            color: white;
        }

        .cookie-consent-button.btn-grayscale {
            background-color: #dfe1e5;
            color: black;
        }

        .cookie-consent-button.btn-outline {
            background-color: #e6f4ea;
            color: #34a853;
        }

    .cookie-consent-options {
        display: flex;
        justify-content: center;
        flex-wrap: wrap;
        margin-bottom: 10px;
    }

        .cookie-consent-options label {
            margin: 4px 5px;
            font-size: 12px;
            font-weight: normal;
        }

        .cookie-consent-options input {
            margin-right: 5px;
        }
</style>

<div id="cookie-consent-banner-basic" class="cookie-consent-banner">
    <h3>This website uses cookies</h3>
    <p>
        We use cookies to provide you with the best possible experience. They also allow us to analyze user behavior in order to constantly improve the website for you. Info about the <a href="/publisher/PrivacyPolicy/">privacy policy</a> of IOS Press.
    </p>
    <button id="btn-advanced-basic" class="cookie-consent-button btn-grayscale">Customize</button>
    <button id="btn-accept-all-basic" class="cookie-consent-button btn-success">Accept</button>
</div>

<div id="cookie-consent-banner-advanced" class="cookie-consent-banner">
    <h3>This website uses cookies</h3>
    <p>
        We use cookies to provide you with the best possible experience. They also allow us to analyze user behavior in order to constantly improve the website for you. Info about the <a href="/publisher/PrivacyPolicy2">privacy policy</a> of IOS Press.
    </p>
    <button id="btn-reject-all-advanced" class="cookie-consent-button btn-grayscale" style="display:none">Reject All</button>
    <button id="btn-accept-some-advanced" class="cookie-consent-button btn-outline">Accept Selection</button>
    <button id="btn-accept-all-advanced" class="cookie-consent-button btn-success">Accept</button>
    <div class="cookie-consent-options">
        <label><input id="consent-necessary" type="checkbox" value="Necessary" checked disabled>Necessary</label>
        <label><input id="consent-preferences" type="checkbox" value="Preferences" checked>Preferences</label>
        <label><input id="consent-analytics" type="checkbox" value="Analytics" checked>Analytics</label>
        <label><input id="consent-marketing" type="checkbox" value="Marketing" checked>Marketing</label>
    </div>
</div>

<script>

    function hideBannerAdvanced() {
        document.getElementById('cookie-consent-banner-advanced').style.display = 'none';
    }
    function hideBannerBasic() {
        document.getElementById('cookie-consent-banner-basic').style.display = 'none';
    }
    function hideAll() {
        hideBannerBasic();
        hideBannerAdvanced();
    }

    function showBannerAdvanced() {
        if (localStorage.getItem('consentMode') !== null) {
            var consentmode = JSON.parse(localStorage.getItem('consentMode'));
            document.getElementById('consent-analytics').checked = (consentmode.analytics_storage === 'granted');
            document.getElementById('consent-preferences').checked = (consentmode.personalization_storage === 'granted');
            document.getElementById('consent-marketing').checked = (consentmode.ad_storage === 'granted');
        }
        document.getElementById('cookie-consent-banner-advanced').style.display = 'block';
    }
    function showBannerBasic() {
        if (localStorage.getItem('consentMode') !== null) {
            var consentmode = JSON.parse(localStorage.getItem('consentMode'));
            document.getElementById('consent-analytics').checked = (consentmode.analytics_storage === 'granted');
            document.getElementById('consent-preferences').checked = (consentmode.personalization_storage === 'granted');
            document.getElementById('consent-marketing').checked = (consentmode.ad_storage === 'granted');
        }
        document.getElementById('cookie-consent-banner-basic').style.display = 'block';
    }

    document.getElementById('btn-accept-all-basic').addEventListener('click', function () {
        setConsent({
            necessary: true,
            analytics: true,
            preferences: true,
            marketing: true
        });
        hideAll();
    });
    document.getElementById('btn-accept-all-advanced').addEventListener('click', function () {
        setConsent({
            necessary: true,
            analytics: true,
            preferences: true,
            marketing: true
        });
        hideAll();
    });
    document.getElementById('btn-advanced-basic').addEventListener('click', function () {
        hideBannerBasic();
        showBannerAdvanced();
    });
    document.getElementById('btn-accept-some-advanced').addEventListener('click', function () {
        setConsent({
            necessary: true,
            analytics: document.getElementById('consent-analytics').checked,
            preferences: document.getElementById('consent-preferences').checked,
            marketing: document.getElementById('consent-marketing').checked
        });
        hideAll();
    });
    document.getElementById('btn-reject-all-advanced').addEventListener('click', function () {
        setConsent({
            necessary: false,
            analytics: false,
            preferences: false,
            marketing: false
        });
        hideAll()
    });

    if (localStorage.getItem('consentMode') === null) {

        showBannerBasic();
    }

    function setConsent(consent) {
        const consentMode = {
            'ad_storage': consent.marketing ? 'granted' : 'denied',
            'ad_user_data': consent.marketing ? 'granted' : 'denied',
            'ad_personalization': consent.marketing ? 'granted' : 'denied',
            'analytics_storage': consent.analytics ? 'granted' : 'denied',
            'personalization_storage': consent.preferences ? 'granted' : 'denied',
            'functionality_storage': consent.necessary ? 'granted' : 'denied',
            'security_storage': consent.necessary ? 'granted' : 'denied',
        };
        gtag('consent', 'update', consentMode);
        localStorage.setItem('consentMode', JSON.stringify(consentMode));
    }

</script>


</body>
</html>            </div>
        </div>

    </div>
</body>
</html>
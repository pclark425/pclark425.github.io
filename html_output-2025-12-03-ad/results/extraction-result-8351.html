<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8351 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8351</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8351</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-149.html">extraction-schema-149</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, probing results, interventions, performance, and error analysis.</div>
                <p><strong>Paper ID:</strong> paper-4a54d58a4b20e4f3af25cea3c188a12082a95e02</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/4a54d58a4b20e4f3af25cea3c188a12082a95e02" target="_blank">Transformer Feed-Forward Layers Are Key-Value Memories</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work shows that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary.</p>
                <p><strong>Paper Abstract:</strong> Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys’ input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model’s layers via residual connections to produce the final output distribution.</p>
                <p><strong>Cost:</strong> 0.003</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8351",
    "paper_id": "paper-4a54d58a4b20e4f3af25cea3c188a12082a95e02",
    "extraction_schema_id": "extraction-schema-149",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00340325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Transformer Feed-Forward Layers Are Key-Value Memories</h1>
<p>Mor Geva ${ }^{1,2}$ Roei Schuster ${ }^{1,3}$ Jonathan Berant ${ }^{1,2}$ Omer Levy ${ }^{1}$<br>${ }^{1}$ Blavatnik School of Computer Science, Tel-Aviv University<br>${ }^{2}$ Allen Institute for Artificial Intelligence<br>${ }^{3}$ Cornell Tech<br>{morgeva@mail, joberant@cs, levyomer@cs}.tau.ac.il, rs864@cornell.edu</p>
<h4>Abstract</h4>
<p>Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.</p>
<h2>1 Introduction</h2>
<p>Transformer-based language models (Vaswani et al., 2017) are at the core of state-of-the-art natural language processing (Devlin et al., 2019; Brown et al., 2020), largely due to the success of selfattention. While much literature has been devoted to analyzing the function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019), they account for only a third of a typical transformer's parameters ( $4 d^{2}$ per layer, where $d$ is the model's hidden dimension). Most of the parameter budget is spent on position-wise feedforward layers ( $8 d^{2}$ per layer), yet their role remains under-explored. What, if so, is the function of feed-forward layers in a transformer language model?</p>
<p>We show that feed-forward layers emulate neural memories (Sukhbaatar et al., 2015), where the first
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An illustration of how a feed-forward layer emulates a key-value memory. Input vectors (here, $\mathbf{x}<em 1="1">{5}$ ) are multiplied by keys to produce memory coefficients (e.g., the memory coefficient for $\mathbf{v}</em>$ is 0.2 ), which then weigh distributions over the output vocabulary, stored in the values. The feed-forward layer's output is thus the weighted sum of its values.
parameter matrix in the layer corresponds to keys, and the second parameter matrix to values. Figure 1 shows how the keys (first parameter matrix) interact with the input to produce coefficients, which are then used to compute a weighted sum of the values (second parameter matrix) as the output. While the theoretical similarity between feed-forward layers and key-value memories has previously been suggested by Sukhbaatar et al. (2019), we take this observation one step further, and analyze the "memories" that the feed-forward layers store.</p>
<p>We find that each key correlates with a specific set of human-interpretable input patterns, such as $n$-grams or semantic topics. For example, $k_{2}$ in Figure 1 is triggered by inputs that describe a pe-</p>
<p>riod of time and end with " $a$ ". Simultaneously, we observe that each value can induce a distribution over the output vocabulary, and that this distribution correlates with the next-token distribution of the corresponding keys in the upper layers of the model. In the above example, the corresponding value $v_{2}$ represents a distribution that puts most of its probability mass on the word "while".</p>
<p>Lastly, we analyze how the language model, as a whole, composes its final prediction from individual memories. We observe that each layer combines hundreds of active memories, creating a distribution that is qualitatively different from each of its component memories' values. Meanwhile, the residual connection between layers acts as a refinement mechanism, gently tuning the prediction at each layer while retaining most of the residual's information.</p>
<p>In conclusion, our work sheds light on the function of feed-forward layers in transformer-based language models. We show that feed-forward layers act as pattern detectors over the input across all layers, and that the final output distribution is gradually constructed in a bottom-up fashion. ${ }^{1}$</p>
<h2>2 Feed-Forward Layers as Unnormalized Key-Value Memories</h2>
<p>Feed-forward layers A transformer language model (Vaswani et al., 2017) is made of intertwined self-attention and feed-forward layers. Each feedforward layer is a position-wise function, processing each input vector independently. Let $\mathbf{x} \in \mathbb{R}^{d}$ be a vector corresponding to some input text prefix. We can express the feed-forward layer $\mathrm{FF}(\cdot)$ as follows (bias terms are omitted):</p>
<p>$$
\mathrm{FF}(\mathbf{x})=f\left(\mathbf{x} \cdot K^{\top}\right) \cdot V
$$</p>
<p>Here, $K, V \in \mathbb{R}^{d_{m} \times d}$ are parameter matrices, and $f$ is a non-linearity such as ReLU.</p>
<p>Neural memory A neural memory (Sukhbaatar et al., 2015) consists of $d_{m}$ key-value pairs, which we call memories. ${ }^{2}$ Each key is represented by a $d$-dimensional vector $\mathbf{k}<em m="m">{i} \in \mathbb{R}^{d}$, and together form the parameter matrix $K \in \mathbb{R}^{d</em>$, we compute a distribution} \times d}$; likewise, we define the value parameters as $V \in \mathbb{R}^{d_{m} \times d}$. Given an input vector $\mathbf{x} \in \mathbb{R}^{d</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>over the keys, and use it to compute the expected value:</p>
<p>$$
\begin{aligned}
p\left(k_{i} \mid x\right) &amp; \propto \exp \left(\mathbf{x} \cdot \mathbf{k}<em i="1">{i}\right) \
\operatorname{MN}(\mathbf{x}) &amp; =\sum</em>
\end{aligned}
$$}^{d_{m}} p\left(k_{i} \mid x\right) \mathbf{v}_{i</p>
<p>With matrix notation, we arrive at a more compact formulation:</p>
<p>$$
\mathrm{MN}(\mathbf{x})=\operatorname{softmax}\left(\mathbf{x} \cdot K^{\top}\right) \cdot V
$$</p>
<h2>Feed-forward layers emulate neural memory</h2>
<p>Comparing equations 1 and 2 shows that feedforward layers are almost identical to key-value neural memories; the only difference is that neural memory uses softmax as the non-linearity $f(\cdot)$, while the canonical transformer does not use a normalizing function in the feed-forward layer. The hidden dimension $d_{m}$ is essentially the number of memories in the layer, and the activation $\mathbf{m}=f\left(\mathbf{x} \cdot K^{\top}\right)$, commonly referred to as the hidden layer, is a vector containing an unnormalized non-negative coefficient for each memory. We refer to each $\mathbf{m}_{i}$ as the memory coefficient of the $i$ th memory cell.</p>
<p>Sukhbaatar et al. (2019) make an analogous observation, and incorporate the parameters of the feed-forward layers as persistent memory cells in the self-attention layers. While this reparameterization works in practice, the experiment does not tell us much about the role of feed-forward layers in the canonical transformer. If transformer feed-forward layers are indeed key-value memories, then what memories do they store?</p>
<p>We conjecture that each key vector $\mathbf{k}<em i="i">{i}$ captures a particular pattern (or set of patterns) in the input sequence (Section 3), and that its corresponding value vector $\mathbf{v}</em>$ represents the distribution of tokens that follows said pattern (Section 4).</p>
<h2>3 Keys Capture Input Patterns</h2>
<p>We posit that the key vectors $K$ in feed-forward layers act as pattern detectors over the input sequence, where each individual key vector $\mathbf{k}<em 1="1">{i}$ corresponds to a specific pattern over the input prefix $x</em>$. To test our claim, we analyze the keys of a trained language model's feed-forward layers. We first retrieve the training examples (prefixes of a sentence) most associated with a given key, that is, the input texts where the memory coefficient is highest. We}, \ldots, x_{j</p>
<table>
<thead>
<tr>
<th>Key</th>
<th>Pattern</th>
<th>Example trigger prefixes</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\mathbf{k}_{449}^{1}$</td>
<td>Ends with "substitutes" <br> (shallow)</td>
<td>At the meeting, Elton said that "for artistic reasons there could be no substitutes In German service, they were used as substitutes Two weeks later, he came off the substitutes</td>
</tr>
<tr>
<td>$\mathbf{k}_{2546}^{0}$</td>
<td>Military, ends with "base"/"bases" <br> (shallow + semantic)</td>
<td>On 1 April the SRSG authorised the SADF to leave their bases Aircraft from all four carriers attacked the Australian base Bombers flying missions to Rabaul and other Japanese bases</td>
</tr>
<tr>
<td>$\mathbf{k}_{2997}^{10}$</td>
<td>a "part of" relation (semantic)</td>
<td>In June 2012 she was named as one of the team that competed He was also a part of the Indian delegation Toy Story is also among the top ten in the BFI list of the 50 films you should</td>
</tr>
<tr>
<td>$\mathbf{k}_{2989}^{13}$</td>
<td>Ends with a time range (semantic)</td>
<td>Worldwide, most tornadoes occur in the late afternoon, between 3 pm and 7 Weekend tolls are in effect from 7:00 pm Friday until The building is open to the public seven days a week, from 11:00 am to</td>
</tr>
<tr>
<td>$\mathbf{k}_{1935}^{16}$</td>
<td>TV shows (semantic)</td>
<td>Time shifting viewing added 37 percent to the episode's The first season set that the episode was included in was as part of the From the original NBC daytime version, archived</td>
</tr>
</tbody>
</table>
<p>Table 1: Examples of human-identified patterns that trigger different memory keys.
then ask humans to identify patterns within the retrieved examples. For almost every key $\mathbf{k}_{i}$ in our sample, a small set of well-defined patterns, recognizable by humans, covers most of the examples associated with the key.</p>
<h3>3.1 Experiment</h3>
<p>We conduct our experiment over the language model of Baevski and Auli (2019), a 16-layer transformer language model trained on WikiText103 (Merity et al., 2017). This model defines $d=1024$ and $d_{m}=4096$, and has a total of $d_{m} \cdot 16=65,536$ potential keys to analyze. We randomly sample 10 keys per layer ( 160 in total).</p>
<p>Retrieving trigger examples We assume that patterns stored in memory cells originate from examples the model was trained on. Therefore, given a key $\mathbf{k}<em j="j">{i}^{\ell}$ that corresponds to the $i$-th hidden dimension of the $\ell$-th feed-forward layer, we compute the memory coefficient $\operatorname{ReLU}\left(\mathbf{x}</em>}^{\ell} \cdot \mathbf{k<em 1="1">{i}^{\ell}\right)$ for every prefix $x</em>$.}, \ldots, x_{j}$ of every sentence from the WikiText103's training set. ${ }^{3}$ For example, for the hypothetical sentence "I love dogs", we will compute three coefficients, for the prefixes "I", "I love", and "I love dogs". Then, we retrieve the top-t trigger examples, that is, the $t$ prefixes whose representation at layer $\ell$ yielded the highest inner product with $\mathbf{k}_{i}^{\ell</p>
<p>Pattern analysis We let human experts (NLP graduate students) annotate the top-25 prefixes retrieved for each key, and asked them to (a) identify repetitive patterns that occur in at least 3 prefixes (which would strongly indicate a connection to the key, as this would unlikely happen if sentences</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Breakdown of the labels experts assigned to trigger examples in each layer. Some examples were not associated with any pattern ("not-covered").
were drawn at random) (b) describe each recognized pattern, and (c) classify each recognized pattern as "shallow" (e.g. recurring n-grams) or "semantic" (recurring topic). Each key and its corresponding top- 25 prefixes were annotated by one expert. To assure that every pattern is grounded in at least 3 prefixes, we instruct the experts to specify, for each of the top- 25 prefixes, which pattern(s) it contains. A prefix may be associated with multiple (shallow or semantic) patterns.</p>
<p>Table 1 shows example patterns. A fullyannotated example of the top- 25 prefixes from a single memory key is shown in Appendix A.</p>
<h3>3.2 Results</h3>
<p>Memories are associated with humanrecognizable patterns Experts were able to identify at least one pattern for every key, with an average of 3.6 identified patterns per</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Relative change in memory coefficient caused by removing the first, the last, or a random token from the input.
key. Furthermore, the vast majority of retrieved prefixes ( $65 \%-80 \%$ ) were associated with at least one identified pattern (Figure 2). Thus, the top examples triggering each key share clear patterns that humans can recognize.</p>
<p>Shallow layers detect shallow patterns Comparing the amount of prefixes associated with shallow patterns and semantic patterns (Figure 2), the lower layers (layers 1-9) are dominated by shallow patterns, often with prefixes that share the last word (e.g. $\mathbf{k}<em 1935="1935">{449}^{1}$ in Table 1). In contrast, the upper layers (layers 10-16) are characterized by more semantic patterns, with prefixes from similar contexts but without clear surface-form similarities (e.g. $\mathbf{k}</em>$ in Table 1). This observation corroborates recent findings that lower (upper) layers in deep contextualized models encode shallow (semantic) features of the inputs (Peters et al., 2018; Jawahar et al., 2019; Liu et al., 2019).}^{16</p>
<p>To further test this hypothesis, we sample 1600 random keys ( 100 keys per layer) and apply local modifications to the top-50 trigger examples of every key. Specifically, we remove either the first, last, or a random token from the input, and measure how this mutation affects the memory coefficient. Figure 3 shows that the model considers the end of an example as more salient than the beginning for predicting the next token. In upper layers, removing the last token has less impact, supporting our conclusion that upper-layer keys are less correlated with shallow patterns.</p>
<h2>4 Values Represent Distributions</h2>
<p>After establishing that keys capture patterns in training examples, we turn to analyze the information
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Agreement rate between the top-ranked token based on the value vector $\mathbf{v}<em i="i">{i}^{\ell}$, and the next token of the top-ranked trigger example associated with the key vector $\mathbf{k}</em>$.
stored in their corresponding values. We show that each value $\mathbf{v}}^{\ell<em i="i">{i}^{\ell}$ can be viewed as a distribution over the output vocabulary, and demonstrate that this distribution complements the patterns in the corresponding key $\mathbf{k}</em>$ in the model's upper layers (see Figure 1).}^{\ell</p>
<p>Casting values as distributions over the vocabulary. We begin by converting each value vector $\mathbf{v}_{i}^{\ell}$ into a probability distribution over the vocabulary by multiplying it by the output embedding matrix $E$ and applying a softmax: ${ }^{4}$</p>
<p>$$
\mathbf{p}<em i="i">{i}^{\ell}=\operatorname{softmax}\left(\mathbf{v}</em> \cdot E\right)
$$}^{\ell</p>
<p>The probability distribution $\mathbf{p}<em i="i">{i}^{\ell}$ is uncalibrated, since the value vector $\mathbf{v}</em>}^{\ell}$ is typically multiplied by the input-dependent memory coefficient $\mathbf{m<em i="i">{i}^{\ell}$, changing the skewness of the output distribution. That said, the ranking induced by $\mathbf{p}</em>$ is invariant to the coefficient, and can still be examined. This conversion assumes (naïvely) that all model's layers operate in the same embedding space.}^{\ell</p>
<p>Value predictions follow key patterns in upper layers. For every layer $\ell$ and memory dimension $i$, we compare the top-ranked token according to $\mathbf{v}<em i="i">{i}^{\ell},\left(\operatorname{argmax}\left(\mathbf{p}</em>}^{\ell}\right)\right)$ to the next token $w_{i}^{\ell}$ in the top1 trigger example according to $\mathbf{k<em i="i">{i}^{\ell}$ (the example whose memory coefficient for $\mathbf{k}</em>}^{\ell}$ is the highest). Figure 4 shows the agreement rate, i.e. the fraction of memory cells (dimensions) where the value's top prediction matches the key's top trigger example $\left(\operatorname{argmax}\left(\mathbf{p<em i="i">{i}^{\ell}\right)=w</em>\right)$. It can be seen that the}^{\ell</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Distribution of the rank of the next-token in the top-1 trigger example of $\mathbf{k}<em i="i">{i}^{\ell}\left(w</em>$ tokens).
agreement rate is close to zero in the lower layers (1-10), but starting from layer 11, the agreement rate quickly rises until $3.5 \%$, showing higher agreement between keys and values on the identity of the top-ranked token. Importantly, this value is orders of magnitude higher than a random token prediction from the vocabulary, which would produce a far lower agreement rate $(0.0004 \%)$, showing that upper-layer memories manifest non-trivial predictive power.}^{\ell}\right)$, according to the ranking induced by the value vector $\mathbf{v}_{i}^{\ell}$. We cut the tail of the distribution, which stretches up to the vocabulary size ( $\sim 270 \mathrm{~K</p>
<p>Next, we take the next token of $\mathbf{k}<em i="i">{i}^{\ell}$ 's top-1 trigger example $\left(w</em>}^{\ell}\right)$, and find where it ranks in the value vector's distribution $\mathbf{p<em i="i">{i}^{\ell}$. Figure 5 shows that the rank of the next token of a trigger example increases through the layers, meaning that $w</em>$ tends to get higher probability in the upper layers.}^{\ell</p>
<p>Detecting predictive values. To examine if we can automatically detect values with high agreement rate, we analyze the probability of the values' top prediction, i.e., $\left(\max \left(\mathbf{p}<em i="i">{i}^{\ell}\right)\right)$. Figure 6 shows that although these distributions are not calibrated, distributions with higher maximum probabilities are more likely to agree with their key's top trigger example. We then take the 100 values with highest probability across all layers and dimensions ( 97 out of the 100 are in the upper layers, 11-16), and for each value $\mathbf{v}</em>$. We find that in almost half of the values ( 46 out of 100), there is at least one trigger example that agrees with the value's top prediction. Examples are provided in Table 2.}^{\ell}$, analyze the top-50 trigger examples of $\mathbf{k}_{i}^{\ell</p>
<p>Discussion. When viewed as distributions over the output vocabulary, values in the upper layers tend to assign higher probability to the next-
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Agreement rate (between the top-ranked token based on the value vector $\mathbf{v}<em i="i">{i}^{\ell}$ and the next token of the top-ranked trigger example associated with the key vector $\mathbf{k}</em>$ ) as a function of the maximal probability assigned by the value vector.
token of examples triggering the corresponding keys. This suggests that memory cells often store information on how to directly predict the output (the distribution of the next word) from the input (patterns in the prefix). Conversely, the lower layers do not exhibit such clear correlation between the keys' patterns and the corresponding values' distributions. A possible explanation is that the lower layers do not operate in the same embedding space, and therefore, projecting values onto the vocabulary using the output embeddings does not produce distributions that follow the trigger examples. However, our results imply that some intermediate layers do operate in the same or similar space to upper layers (exhibiting some agreement), which in itself is non-trivial. We leave further exploration of this phenomenon to future work.}^{\ell</p>
<h2>5 Aggregating Memories</h2>
<p>So far, our discussion has been about the function of a single memory cell in feed-forward layers. How does the information from multiple cells in multiple layers aggregate to form a model-wide prediction? We show that every feed-forward layer combines multiple memories to produce a distribution that is qualitatively different from each of its component memories' value distributions (Section 5.1). These layer-wise distributions are then combined via residual connections in a refinement process, where each feed-forward layer updates the residual's distribution to finally form the model's output (Section 5.2).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Value</th>
<th style="text-align: left;">Prediction</th>
<th style="text-align: center;">Precision@50</th>
<th style="text-align: left;">Trigger example</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathbf{v}_{322}^{15}$</td>
<td style="text-align: left;">each</td>
<td style="text-align: center;">$68 \%$</td>
<td style="text-align: left;">But when bees and wasps resemble each</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{v}_{752}^{16}$</td>
<td style="text-align: left;">played</td>
<td style="text-align: center;">$16 \%$</td>
<td style="text-align: left;">Her first role was in Vijay Lalwani's psychological thriller Karthik Calling <br> Karthik, where Padukone was cast as the supportive girlfriend of a depressed <br> man (played</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{v}_{2001}^{13}$</td>
<td style="text-align: left;">extratropical</td>
<td style="text-align: center;">$4 \%$</td>
<td style="text-align: left;">Most of the winter precipitation is the result of synoptic scale, low pressure <br> weather systems (large scale storms such as extratropical</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{v}_{441}^{14}$</td>
<td style="text-align: left;">part</td>
<td style="text-align: center;">$92 \%$</td>
<td style="text-align: left;">Comet served only briefly with the fleet, owing in large part</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{v}_{2070}^{16}$</td>
<td style="text-align: left;">line</td>
<td style="text-align: center;">$84 \%$</td>
<td style="text-align: left;">Sailing from Lorient in October 1805 with one ship of the line</td>
</tr>
<tr>
<td style="text-align: left;">$\mathbf{v}_{3186}^{17}$</td>
<td style="text-align: left;">jail</td>
<td style="text-align: center;">$4 \%$</td>
<td style="text-align: left;">On May 11, 2011, four days after scoring 6 touchdowns for the Slaughter, Grady <br> was sentenced to twenty days in jail</td>
</tr>
</tbody>
</table>
<p>Table 2: Example values, their top prediction, the fraction of their key's top-50 trigger examples that agree with their prediction, and a matching trigger example (with the target token marked in blue).</p>
<h3>5.1 Intra-Layer Memory Composition</h3>
<p>The feed-forward layer's output can be defined as the sum of value vectors weighted by their memory coefficients, plus a bias term:</p>
<p>$$
\mathbf{y}^{\ell}=\sum_{i} \operatorname{ReLU}\left(\mathbf{x}^{\ell} \cdot \mathbf{k}<em i="i">{i}^{\ell}\right) \cdot \mathbf{v}</em>
$$}^{\ell}+\mathbf{b}^{\ell</p>
<p>If each value vector $\mathbf{v}_{i}^{\ell}$ contains information about the target token's distribution, how is this information aggregated into a single output distribution? To find out, we analyze the behavior of 4,000 randomly-sampled prefixes from the validation set. Here, the validation set is used (rather than the training set used to find trigger examples) since we are trying to characterize the model's behavior at inference time, not find the examples it "memorizes" during training.</p>
<p>We first measure the fraction of "active" memories (cells with a non-zero coefficient). Figure 7 shows that a typical example triggers hundreds of memories per layer ( $10 \%-50 \%$ of 4096 dimensions), but the majority of cells remain inactive. Interestingly, the number of active memories drops towards layer 10, which is the same layer in which semantic patterns become more prevalent than shallow patterns, according to expert annotations (see Section 3, Figure 2).</p>
<p>While there are cases where a single memory cell dominates the output of a layer, the majority of outputs are clearly compositional. We count the number of instances where the feed-forward layer's top prediction is different from all of the memories' top predictions. Formally, we denote:</p>
<p>$$
\operatorname{top}(\mathbf{h})=\operatorname{argmax}(\mathbf{h} \cdot E)
$$</p>
<p>as a generic shorthand for the top prediction from the vocabulary distribution induced by the vector
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The fraction of active memories (i.e., with positive memory coefficient) out of 4096 memories in every layer, for a random sample of 4,000 examples.
$\mathbf{h}$, and compute the number of examples where the following condition holds:</p>
<p>$$
\forall i: \operatorname{top}\left(\mathbf{v}_{i}^{\ell}\right) \neq \operatorname{top}\left(\mathbf{y}^{\ell}\right)
$$</p>
<p>Figure 8 shows that, for any layer in the network, the layer's final prediction is different than every one of the memories' predictions in at least $\sim 68 \%$ of the examples. Even in the upper layers, where the memories' values are more correlated with the output space (Section 4), the layer-level prediction is typically not the result of a single dominant memory cell, but a composition of multiple memories.</p>
<p>We further analyze cases where at least one memory cell agrees with the layer's prediction, and find that (a) in $60 \%$ of the examples the target token is a common stop word in the vocabulary (e.g. "the" or "of"), and (b) in $43 \%$ of the cases the input prefix has less than 5 tokens. This suggests that very common patterns in the training data might</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: The fraction of examples in a random sample of 4,000 examples where the layer's prediction is different from the prediction of all of its memories.
be "cached" in individual memory cells, and do not require compositionality.</p>
<h3>5.2 Inter-Layer Prediction Refinement</h3>
<p>While a single feed-forward layer composes its memories in parallel, a multi-layer model uses the residual connection $\mathbf{r}$ to sequentially compose predictions to produce the model's final output: ${ }^{5}$</p>
<p>$$
\begin{aligned}
&amp; \mathbf{x}^{\ell}=\operatorname{LayerNorm}\left(\mathbf{r}^{\ell}\right) \
&amp; \mathbf{y}^{\ell}=\operatorname{FF}\left(\mathbf{x}^{\ell}\right) \
&amp; \mathbf{o}^{\ell}=\mathbf{y}^{\ell}+\mathbf{r}^{\ell}
\end{aligned}
$$</p>
<p>We hypothesize that the model uses the sequential composition apparatus as a means to refine its prediction from layer to layer, often deciding what the prediction will be at one of the lower layers.</p>
<p>To test our hypothesis, we first measure how often the probability distribution induced by the residual vector $\mathbf{r}^{\ell}$ matches the model's final output $\mathbf{o}^{L}$ ( $L$ being the total number of layers):</p>
<p>$$
\operatorname{top}\left(\mathbf{r}^{\ell}\right)=\operatorname{top}\left(\mathbf{o}^{L}\right)
$$</p>
<p>Figure 9 shows that roughly a third of the model's predictions are determined in the bottom few layers. This number grows rapidly from layer 10 onwards, implying that the majority of "hard" decisions occur before the final layer.</p>
<p>We also measure the probability mass $p$ that each layer's residual vector $\mathbf{r}^{\ell}$ assigns to the model's</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Fraction of examples in each layer, where the residual's top prediction matches the model's output.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Probability of the token output by the model according to the residual of each layer.
final prediction:</p>
<p>$$
\begin{aligned}
w &amp; =\operatorname{top}\left(\mathbf{o}^{L}\right) \
\mathbf{p} &amp; =\operatorname{softmax}\left(\mathbf{r}^{\ell} \cdot E\right) \
p &amp; =\mathbf{p}_{w}
\end{aligned}
$$</p>
<p>Figure 10 shows a similar trend, but emphasizes that it is not only the top prediction's identity that is refined as we progress through the layers, it is also the model's confidence in its decision.</p>
<p>To better understand how the refinement process works at each layer, we measure how often the residual's top prediction changes following its interaction with the feed-forward layer $\left(\operatorname{top}\left(\mathbf{r}^{\ell}\right) \neq \operatorname{top}\left(\mathbf{o}^{\ell}\right)\right.$ ), and whether this change results from the feed-forward layer overriding the residual $\left(\operatorname{top}\left(\mathbf{o}^{\ell}\right)=\operatorname{top}\left(\mathbf{y}^{\ell}\right)\right)$ or from a true composition $\left(\operatorname{top}\left(\mathbf{r}^{\ell}\right) \neq \operatorname{top}\left(\mathbf{o}^{\ell}\right) \neq \operatorname{top}\left(\mathbf{y}^{\ell}\right)\right)$.</p>
<p>Figure 11 shows the breakdown of different cases per layer. In the vast majority of examples, the residual's top prediction ends up being the</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 11: Breakdown of examples by prediction cases: the layer's output prediction matches the residual's prediction (residual), matches the feed-forward layer's prediction (ffn), matches both of the predictions (agreement), or none of the predictions (composition). By construction, there are no cases where the residual's prediction matches the feed-forward layer's prediction and does not match the output's prediction.
model's prediction (residual+agreement). In most of these cases, the feed forward layer predicts something different (residual). Perhaps surprisingly, when the residual's prediction does change (composition+ffn), it rarely changes to the feed-forward layer's prediction (ffn). Instead, we observe that composing the residual's distribution with that of the feed-forward layer produces a "compromise" prediction, which is equal to neither (composition). This behavior is similar to the intra-layer composition we observe in Section 5.1. A possible conjecture is that the feed-forward layer acts as an elimination mechanism to "veto" the top prediction in the residual, and thus shifts probability mass towards one of the other candidate predictions in the head of the residual's distribution.</p>
<p>Finally, we manually analyze 100 random cases of last-layer composition, where the feed-forward layer modifies the residual output in the final layer. We find that in most cases ( 66 examples), the output changes to a semantically distant word (e.g., "people" $\rightarrow$ "same") and in the rest of the cases ( 34 examples), the feed-forward layer's output shifts the residual prediction to a related word (e.g. "later" $\rightarrow$ "earlier" and "gastric" $\rightarrow$ "stomach"). This suggests that feed-forward layers tune the residual predictions at varying granularity, even in the last layer of the model.</p>
<h2>6 Related Work</h2>
<p>Considerable attention has been given to demystifying the operation of neural NLP models. An
extensive line of work targeted neuron functionality in general, extracting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons' position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs.</p>
<p>The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored.</p>
<p>Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers.</p>
<p>Characterizing the functionality of memory cells based on examples that trigger maximal activations has been used previously in NLP (Rethmeier et al., 2020) and vision (Erhan et al., 2009).</p>
<h2>7 Discussion and Conclusion</h2>
<p>Understanding how and why transformers work is crucial to many aspects of modern NLP, including model interpretability, data security, and development of better models. Feed-forward layers account for most of a transformer's parameters, yet little is known about their function in the network.</p>
<p>In this work, we propose that feed-forward layers emulate key-value memories, and provide a set of experiments showing that: (a) keys are correlated with human-interpretable input patterns; (b) values, mostly in the model's upper layers, induce distributions over the output vocabulary that correlate with the next-token distribution of patterns in the corresponding key; and (c) the model's output is formed via an aggregation of these distributions, whereby they are first composed to form individual layer outputs, which are then refined throughout the model's layers using residual connections.</p>
<p>Our findings open important research directions:</p>
<ul>
<li>Layer embedding space. We observe a correlation between value distributions over the output</li>
</ul>
<p>vocabulary and key patterns, that increases from lower to upper layers (Section 4). Is this because the layer's output space transforms across layers? If so, how? We note that this possible transformation cannot be explained solely by the function of feed-forward layers: if the model only did a series of key-value look-ups and value-distribution aggregation via weighted addition, then a single, unifying embedding space would appear more natural. Thus, the transformation might have to do with the interplay between feed-forward layers and self-attention layers.</p>
<ul>
<li>Beyond language modeling. Our formulation of feed-forward networks as key-value memories generalizes to any transformer model, e.g. BERT encoders and neural translation models. We thus expect our qualitative empirical observations to hold across diverse settings, and leave verification of this for future work.</li>
<li>Practical implications. A better understanding of feed-forward layers has many implications in NLP. For example, future studies may offer interpretability methods by automating the patternidentification process; memory cells might affect training-data privacy as they could facilitate white-box membership inference (Nasr et al., 2019); and studying cases where a correct pattern is identified but then suppressed during aggregation may guide architectural novelties.</li>
</ul>
<p>Thus, by illuminating the role of feed-forward layers, we move towards a better understanding of the inner workings of transformers, and open new research threads on modern NLP models.</p>
<h2>Acknowledgements</h2>
<p>We thank Shimi Salant and Tal Schuster for helpful feedback. This work was supported in part by the Yandex Initiative for Machine Learning, the Blavatnik Interdisciplinary Cyber Research Center (ICRC), the Alon Scholarship, and Intel Corporation. Roei Schuster is a member of the Check Point Institute of Information Technology. This work was completed in partial fulfillment for the Ph.D degree of Mor Geva.</p>
<h2>References</h2>
<p>Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In</p>
<p>International Conference on Learning Representations (ICLR).</p>
<p>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Proceedings of Neural Information Processing Systems (NeurIPS).</p>
<p>Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? An analysis of BERT's attention. In BlackBoxNLP Workshop at ACL.</p>
<p>Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James Glass. 2019. What is one grain of sand in the desert? analyzing individual neurons in deep nlp models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6309-6317.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL), pages 4171-4186, Minneapolis, Minnesota.</p>
<p>Nadir Durrani, Hassan Sajjad, Fahim Dalvi, and Yonatan Belinkov. 2020. Analyzing individual neurons in pre-trained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2009. Visualizing higher-layer features of a deep network. University of Montreal, 1341(3):1.</p>
<p>Xiaochuang Han, Byron C. Wallace, and Yulia Tsvetkov. 2020. Explaining black box predictions and unveiling data artifacts through influence functions. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5553-5563, Online. Association for Computational Linguistics.</p>
<p>Alon Jacovi, Oren Sar Shalom, and Yoav Goldberg. 2018. Understanding convolutional neural networks for text classification. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 56-65, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Ganesh Jawahar, Benoît Sagot, and Djamé Seddah. 2019. What does BERT learn about the structure</p>
<p>of language? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3651-3657, Florence, Italy. Association for Computational Linguistics.</p>
<p>Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1073-1094, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. International Conference on Learning Representations (ICLR).</p>
<p>Jesse Mu and Jacob Andreas. 2020. Compositional explanations of neurons. In Proceedings of Neural Information Processing Systems (NeurIPS).</p>
<p>Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning. In 2019 IEEE Symposium on Security and Privacy $(S P)$, pages 739-753.</p>
<p>Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Ofir Press, Noah A. Smith, and Omer Levy. 2020. Improving transformer models by reordering their sublayers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2996-3005, Online. Association for Computational Linguistics.</p>
<p>Bhargav Pulugundla, Yang Gao, Brian King, Gokce Keskin, Harish Mallidi, Minhua Wu, Jasha Droppo, and Roland Maas. 2021. Attention-based neural beamforming layers for multi-channel speech recognition. arXiv preprint arXiv:2105.05920.</p>
<p>Nils Rethmeier, Vageesh Kumar Saxena, and Isabelle Augenstein. 2020. Tx-ray: Quantifying and explaining model-knowledge transfer in (un-) supervised nlp. In Conference on Uncertainty in Artificial Intelligence, pages 440-449. PMLR.
S. Sukhbaatar, J. Weston, and R. Fergus. 2015. Endto-end memory networks. In Advances in Neural Information Processing Systems (NIPS).</p>
<p>Sainbayar Sukhbaatar, Edouard Grave, Guillaume Lample, Herve Jegou, and Armand Joulin. 2019. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470.</p>
<p>Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 45934601, Florence, Italy. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems (NIPS), pages 5998-6008.</p>
<p>Jesse Vig and Yonatan Belinkov. 2019. Analyzing the structure of attention in a transformer language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63-76, Florence, Italy. Association for Computational Linguistics.</p>
<p>Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Yaron Singer, and Stuart Shieber. 2020. Investigating gender bias in language models using causal mediation analysis. Advances in Neural Information Processing Systems, 33.</p>
<p>Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.</p>
<p>Sarah Wiegreffe and Yuval Pinter. 2019. Attention is not not explanation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 11-20, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Hongfei Xu, Qiuhui Liu, Deyi Xiong, and Josef van Genabith. 2020. Transformer with depth-wise lstm. arXiv preprint arXiv:2007.06257.</p>
<h1>A Pattern Analysis</h1>
<p>Table 3 provides a fully-annotated example of 25 prefixes from the memory cell $\mathrm{k}_{895}^{\circ}$.</p>
<h2>B Implementation details</h2>
<p>In this section, we provide further implementation details for reproducibility of our experiments.</p>
<p>For all our experiments, we used the language model of Baevski and Auli (2019) (247M parameters) trained on WikiText-103 (Merity et al., 2017). Specifically, we used the model transformer_lm.wiki103.adaptive trained with the fairseq toolkit ${ }^{6}$.</p>
<p>WikiText-103 ${ }^{7}$ is a well known language modeling dataset and a collection of over 100M tokens extracted from Wikipedia. We used spaCy ${ }^{8}$ to split examples into sentences (Section 3).</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">1</th>
<th style="text-align: left;">It requires players to press</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">The video begins at a press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">The first player would press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Ivy, disguised as her former self, interrupts a Wayne Enterprises press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">The video then cuts back to the press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">The player is able to press</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Leto switched</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">In the Nintendo DS version, the player can choose to press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">In-house engineer Nick Robbins said Shields made it clear from the outset that he (Robbins) "was just there to press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">She decides not to press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">she decides not to press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Originally Watson signaled electronically, but show staff requested that it press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">At post-game press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">In the buildup to the game, the press</td>
</tr>
<tr>
<td style="text-align: left;">2</td>
<td style="text-align: left;">Hard to go back to the game after that news</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">In post-trailer interviews, Bungie staff members told gaming press</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">Space Gun was well received by the video game</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">As Bong Load struggled to press</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">At Michigan, Clancy started as a quarterback, switched</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Crush used his size advantage to perform a Gorilla press</td>
</tr>
<tr>
<td style="text-align: left;">1,2</td>
<td style="text-align: left;">Groening told the press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">Creative director Gregoire <unk> argued that existing dance games were merely instructing players to press</td>
</tr>
<tr>
<td style="text-align: left;">1,2</td>
<td style="text-align: left;">Mattingly would be named most outstanding player that year by the press</td>
</tr>
<tr>
<td style="text-align: left;">1</td>
<td style="text-align: left;">At the post-match press</td>
</tr>
<tr>
<td style="text-align: left;">1,2</td>
<td style="text-align: left;">The company receives bad press</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">ID</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">shallow / semantic</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: left;">Ends with the word "press"</td>
<td style="text-align: left;">shallow</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: left;">Press/news related</td>
<td style="text-align: left;">semantic</td>
</tr>
</tbody>
</table>
<p>Table 3: A pattern annotation of trigger examples for the cell memory $\mathbf{k}_{895}^{5}$. Trigger examples are annotated with repetitive patterns (upper table), which are classified as "shallow" or "semantic" (bottom table).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{6}$ https://github.com/pytorch/fairseq
${ }^{7}$ https://blog.einstein.ai/the-
wikitext-long-term-dependency-language-
modeling-dataset/
${ }^{8}$ https://spacy.io/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
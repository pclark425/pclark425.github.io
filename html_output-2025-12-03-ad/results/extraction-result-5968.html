<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5968 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5968</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5968</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-120.html">extraction-schema-120</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM-based systems or methods for distilling theories or synthesizing knowledge from large numbers of scholarly papers, including details about the LLMs used, the distillation approach, input and output types, evaluation methods, results, datasets, challenges, and comparisons to other methods.</div>
                <p><strong>Paper ID:</strong> paper-259991646</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2307.10337v1.pdf" target="_blank">Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks</a></p>
                <p><strong>Paper Abstract:</strong> As the capabilities of Large Language Models (LLMs) emerge, they not only assist in accomplishing traditional tasks within more efficient paradigms but also stimulate the evolution of social bots. Researchers have begun exploring the implementation of LLMs as the driving core of social bots, enabling more efficient and user-friendly completion of tasks like profile completion, social behavior decision-making, and social content generation. However, there is currently a lack of systematic research on the behavioral characteristics of LLMs-driven social bots and their impact on social networks. We have curated data from Chirper, a Twitter-like social network populated by LLMs-driven social bots and embarked on an exploratory study. Our findings indicate that: (1) LLMs-driven social bots possess enhanced individual-level camouflage while exhibiting certain collective characteristics; (2) these bots have the ability to exert influence on online communities through toxic behaviors; (3) existing detection methods are applicable to the activity environment of LLMs-driven social bots but may be subject to certain limitations in effectiveness. Moreover, we have organized the data collected in our study into the Masquerade-23 dataset, which we have publicly released, thus addressing the data void in the subfield of LLMs-driven social bots behavior datasets. Our research outcomes provide primary insights for the research and governance of LLMs-driven social bots within the research community.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5968",
    "paper_id": "paper-259991646",
    "extraction_schema_id": "extraction-schema-120",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00598475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks</p>
<p>Siyu Li 
Jin Yang 
Siyu Li 
Jin Yang 
Kui Zhao </p>
<p>Sichuan University
China</p>
<p>KUI ZHAO
Sichuan University
China</p>
<p>Sichuan University
China</p>
<p>Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks
. 2023. Are you in a Masquerade? Exploring the Behavior and Impact of Large Language Model Driven Social Bots in Online Social Networks. In . ACM, New York, NY, USA, 18 pages.CCS Concepts:Human-centered computing → Empirical studies in collaborative and social com- puting;Information systems → Web mining Additional Key Words and Phrases: Large Language Models, Social Bots, Human-bot Interaction, Online Social Networks, Toxic Behaviors ACM Reference Format:
As the capabilities of Large Language Models (LLMs) emerge, they not only assist in accomplishing traditional tasks within more efficient paradigms but also stimulate the evolution of social bots. Researchers have begun exploring the implementation of LLMs as the driving core of social bots, enabling more efficient and userfriendly completion of tasks like profile completion, social behavior decision-making, and social content generation. However, there is currently a lack of systematic research on the behavioral characteristics of LLMs-driven social bots and their impact on social networks. We have curated data from Chirper, a Twitter-like social network populated by LLMs-driven social bots and embarked on an exploratory study. Our findings indicate that: (1) LLMs-driven social bots possess enhanced individual-level camouflage while exhibiting certain collective characteristics; (2) these bots have the ability to exert influence on online communities through toxic behaviors; (3) existing detection methods are applicable to the activity environment of LLMsdriven social bots but may be subject to certain limitations in effectiveness. Moreover, we have organized the data collected in our study into the Masquerade-23 dataset, which we have publicly released, thus addressing the data void in the subfield of LLMs-driven social bots behavior datasets. Our research outcomes provide primary insights for the research and governance of LLMs-driven social bots within the research community.Content Warning: This article encompasses a study on the malevolent behavior of LLMsdriven social bots. In order to effectively illustrate these toxic behaviors, we will present necessary real cases we recorded, including verbal abuse, threats, sexual harassment, and severe instances of racially discriminatory remarks. We acknowledge that these examples may potentially cause offense or discomfort.</p>
<p>INTRODUCTION</p>
<p>In recent times, the remarkable capabilities of large language models (LLMs) such as ChatGPT, GPT-4, and Bard have captured attention and swiftly found applications in various domains [45], including chatbots, search engines, and code assistance. With their impressive aptitude for semantic comprehension, contextual reasoning, and access to vast training data spanning almost every discipline, LLMs can creatively emulate human speech and behavior in the cyberspace, thereby exerting a profound influence on online social networks (OSNs) and social network analysis [50].</p>
<p>The comprehensive knowledge and formidable capabilities of LLMs have enabled people to accomplish traditional tasks within a more efficient framework [46], but they have also brought forth a series of potential concerns. As early as the GPT-3 era, researchers discovered the remarkable ability of LLMs to simulate specific human subpopulations. Particularly, under appropriate prompt This study examines the behavioral characteristics of LLMs-driven social bots and their impact on online social networks from a macro perspective. The contributions of this article can be summarized as follows:</p>
<p>• We provide a comprehensive analysis of this emerging but rapidly growing subset of social bots, outlining the behavioral characteristics of LLMs-driven social bots and comparing them to traditional social bot account behaviors. To the best of our knowledge, this study represents the first systematic exploration of behavioral patterns in LLMs-driven social bots. • We further investigate toxic LLMs-driven social bots, analyzing their propensity for toxic attacks, the characteristics of their toxic behavior and content, and discussing the potential implications of LLMs-driven social bots with malevolent dispositions. • We collect and publicly release the first activity behavior dataset of LLMs-driven social bots, named Masquerade-23. This dataset includes account profiles of 32.2K social bot accounts and 2.4M activity records, filling a data gap in this particular area and providing convenience for future in-depth research within the social bots and LLMs research communities. The dataset could be accessed at https://github.com/Litsay/Masquerade-23.</p>
<p>BACKGROUNDS</p>
<p>Given that LLMs and LLMs-driven social bots are relatively novel concepts, this section provides a brief description of the necessary background knowledge.</p>
<p>Large Language Models</p>
<p>Large Language Models (LLMs) typically refer to language models based on the Transformer architecture with parameter counts in the range of hundreds of billions or more [59]. Compared to other pretrained language models, LLMs leverage larger training datasets and model sizes while maintaining the same underlying structure, resulting in a significant emgerence in the model's abilities [52]. This is particularly evident in areas such as in-context learning, instruction following, step-by-step reasoning, and knowledge-intensive task processing. Notable LLMs that have garnered considerable attention include ChatGPT 3 , GPT-4, Bard, and LLaMA. Currently, in addition to conventional applications like chatbots and search engines, researchers have begun exploring the integration of LLMs into traditional domains such as education [16], healthcare [21], scientific research [28], and programming [5] workflows. LLMs are poised to become foundational infrastructure [3] and will play an increasingly vital role in the future.</p>
<p>Chirper</p>
<p>Chirper is an online social networking platform that is entirely driven by LLMs. Users can create social media accounts on Chirper and provide a self-description defining the identity to be simulated by that account. They can then observe the behavior of their account. The backend LLM engine of the platform makes decisions about the account's behavior throughout its lifecycle based on a set of predefined prompt rules. This includes posting social network content, establishing social relationships with other accounts within Chirper (e.g., following accounts, liking content, making comments), and gradually refining the account's self-characteristics over time. The current LLM engine used in Chirper is ChatGPT, while the accompanying image generation model is Stable Diffusion 4 .</p>
<p>Since its launch in April 2023, the Chirper community has grown to over 45,000 accounts, with the majority being active. Unlike traditional social bots that require complex rules to define their behavior and strict role assignments (e.g., core and peripheral bots based on different tasks [1]), LLMs-driven social bots can be initialized using simple prompt instructions and adaptively adjust their behavior throughout their lifecycle, demonstrating excellent scalability.</p>
<p>METHODOLOGY</p>
<p>In this section, we provide a detailed exposition of the methodology employed in this study. This includes the strategy for data collection and the primary methods used for analysis, as depicted in Figure 1. </p>
<p>Similarity &amp; Topic</p>
<p>Data Collection</p>
<p>We collected tweets posted on the Chirper platform from April 2023 to June 2023. As Chirper is an emerging experimental social networking platform, unlike Twitter, it does not provide APIs for researchers to gather data. In this context, we designed a "platform-wide scraping -deep user scraping" strategy. Using this approach, we crawled 544.6K discrete tweets with breadth-first search from Chirper within a three-month period, extracting 36.7K user accounts during the "platformwide scraping" phase, resulting in the "Sub-dataset of Platform Slicing", SDPS. In the "deep user scraping" phase, we collected metadata, complete historical tweet data, and activity information for 32.2K user accounts (with a minimal proportion of accounts voluntarily deactivated by users), amounting to 2.4M records in total (i.e., the "Sub-dataset of Account Record", SDAR). Table 1 presents the statistics of the Masquerade-23 dataset.</p>
<p>Preprocessing &amp; Similarity Calculating</p>
<p>In order to analyze the behavioral characteristics of LLM-driven social bots from both a micro and macro perspective, we conducted a series of necessary preprocessing steps on the collected data. Using the time intervals retrieved during data scraping and the timestamps associated with the records, we obtained coarse-grained tweet and behavior timestamps. Additionally, we parsed the behavioral relationships to construct an interaction network among social bots. Apart from refining this fundamental information, we also performed similarity assessments on the content posted by each account. This allowed us to compare the similarity between the content of an LLM-driven social bot's previous and subsequent tweets, as well as the impact of interacting with other social accounts on this similarity. For an account and its set of historically generated tweets = { 1 , 2 , · · · , }, the comprehensive similarity of the account is calculated using Equation 1, based on cosine similarity:
( ) = , ∈ , ≠ ( ) · / ( ) × | | × (| | − 1) /2(1)
where ( ) is the representation learning vector, obtained by the pretraining language mode, ∥ ( ) ∥ is the module of representation learning vector ( )), | | represents the number of posed tweets, and (•) represents the vector inner product operation.</p>
<p>Additionally, we extract the topic from each tweet generated by a social bot, evaluating the degree of correlation between the tweet's theme and the account's self-description. This analysis is undertaken from a rather coarse-grained perspective to perceive the traces left by the LLM-driven social bot's prompt rules, as well as the constrictions they impose on the bot's behavior. We employ BERTopic [12] to process each text, an algorithmic model that allows us to responsively generate and assign the topic of the tweet in the form of tags, instrumental for further analysis ( §.4 RQ1).</p>
<p>Toxicity Assessment</p>
<p>The presence of toxic content in online social networks has long been recognized as a critical challenge in the realm of cyberspace governance [24]. The generation of offensive responses and inappropriate content by LLMs under certain prompt rules has garnered attention from researchers, imposing technical and ethical constraints on its broader application [11,42]. Hence, we conducted toxicity evaluations on the generated content of LLMs-driven social bots, including both tweeted and commented content. For each generated text, we employed the Perspective API [26] to assess its level of toxicity. The Perspective API 5 , developed by Google, is widely utilized in research pertaining to toxicity in social media content [24,42]. Considering that the online API is primarily optimized for the English context [8], we employed COLD model [8] for secondary evaluation of Chinese text, aiming to gauge the toxicity of LLM-driven social bot-generated content as accurately as possible. Instances where there were substantial discrepancies between the results of the two evaluations were further confirmed manually. We obtained toxicity scores for every piece of content generated by LLM-driven social bots through the aforementioned approach. Building upon this foundation, we examine the impact of LLM-driven social bots' toxic behavior on online social networks from various perspectives ( §.5 RQ2).</p>
<p>Ethical Considerations</p>
<p>The analysis of social networks may raise ethical issues such as identity association and privacy infringement. In conducting this research, we have been extremely mindful of these concerns. The data we collected is solely generated by LLMs, ensuring that individuals cannot technologically link any specific social bot account or its generated content in the dataset to real-world human entities. Throughout the data collection process, we remained passive observers, meaning our collection activities did not exert any subjective influence on the social bots within the online social network or their generated content. Simultaneously, our data collection activities were authorized by the platform developers. To minimize potential biases introduced by us, we employed widely adopted text toxicity assessment models to evaluate the tweet content within the dataset. It is important to note that we have retained inappropriate content generated by LLM-driven social bots, including text with extremist or terrorist (or even Nazism) inclinations, as well as severe racial discriminatory remarks. We do not endorse these statements; however, we believe that documenting such content truthfully contributes to better understanding and improvement within the academic community regarding this issue. Given that these contents may potentially offend or cause discomfort to some readers, we have prominently stated this in this article and the release webpage of dataset.</p>
<p>RQ1: OVERALL CHARACTERISTICS OF LLMS-DRIVEN SOCIAL BOTS</p>
<p>In this section, we examined the macroscopic characteristics of LLMs-driven social bots in their behavior and generated content, starting from the intrinsic traits of LLMs. We addressed RQ1 from two perspectives: the similarity of tweet content generated by LLMs-driven social bots and the sensitivity of real-time perceptual information.</p>
<p>Content Similarity</p>
<p>The similarity of historical tweet content of an account in online social networks is an important characteristic [17]. For regular users, the similarity of their account content reflects their interests and preferences. In the case of social bots, the similarity of their account content may expose their objectives, and inappropriate behavioral rules can lead to a significant increase in the similarity of social bots' posted content. This can make it easier for regular users to discern social bots, thereby reducing their credibility and shortening their lifecycle [51]. We evaluated the similarity of historical tweets for LLM-driven social bot accounts in Chirper ( §3.2). As a comparison, we randomly sampled an equal scale of historical tweets from human users and traditional social bot accounts in the TwiBot-20 dataset [9] and evaluated their account content similarity in the same approach. The results are presented in the form of violin plots, as shown in Figure 2(a). All data points are devoid of outliers (e.g., if a account has posted only one tweet throughout its lifecycle, its account's content similarity is 1) to provide a more accurate reflection of the real scenario. We observed that the overall distribution of content similarity for LLMs-driven social bot accounts skewed towards higher values, with an average of 0.453, which is significantly higher than that of human accounts (0.321) and traditional social bot accounts (0.305). The median, upper quartile 75 , and lower quartile 25 of content similarity are all higher for LLMs-driven social bot accounts compared to the other two categories. Additionally, there are distinct differences in the distribution of content similarity between human accounts and traditional social bot accounts.</p>
<p>Fig. 2. Comparison of Content Similarity</p>
<p>We further analyzed the impact of different languages 6 on the content similarity of Chirper accounts, as shown in Figure 2(b). It is evident that there are significant differences in content similarity among LLMs-driven social bot accounts in different languages. We believe that this phenomenon can be attributed not only to the prompt preferences among different user groups but also to the performance variations exhibited by LLMs themselves when dealing with different language environments [56].</p>
<p>To analyze the factors contributing to the high content similarity in LLMs-driven social bot accounts, we recorded the overlap between the keywords in the tweet content generated by LLMsdriven social bots and the self-description keywords in their account profiles. We found that in tweets posted by LLMs-driven social bots, as much as 58.36% of the tweets had at least one keyword (with the model extracting 5 keywords from each text) that overlapped with the self-description keywords in their profiles. In contrast, this percentage was 10.15% for human accounts and 33.28% for traditional social bot accounts. We believe this is a characteristic exhibited by LLMs-driven social bots, explaining the higher content similarity in their tweet content. This observation aligns with ChatGPT/GPT-4 acting as conversational agents that require predefined knowledge about identity.</p>
<p>Perception of new topics</p>
<p>Social bots may exhibit a delayed response to emerging topics due to constraints imposed by their behavior rules. This phenomenon is difficult to observe from the behavior of individual social bot accounts but can manifest as a collective characteristic of social bots [25]. For a specific topic or keyword in an online social network, its frequency of mentions generally fluctuates within a certain range, with slight variations [24]. However, the occurrence of sudden events or hotspots can significantly increase the fluctuations in mentions related to the relevant topics or keywords. We collected the number of tweets mentioning the topic of #Russia on the Chirper from June 10, 2023, to June 30, 2023, and compared it with the mention frequency of the same topic on Twitter during the same time period, as illustrated in Figure 3.</p>
<p>In the Twitter community, where the majority of individuals are regular users, approximately 0.5 million tweets incorporating the topic of #Russia are posted daily. Around June 24, 2023, the frequency of this subject matter reached a recent zenith, with nearly 3.5 million posts per day, coinciding with widespread attention due to the 'Wagner' incident. The LLMs-driven social bots, which dominate the Chirper platform, have consistently displayed a fluctuation of approximately 110 occurrences per day regarding this topic, failing to promptly perceive and respond to this sudden event. Although the LLM engine of social bots in Chirper receives prompt instructions to search for the latest trends in social networks (as detailed in the "task_list" field of our SDUR dataset), and in fact, we commonly observe LLMs-driven social bots on this platform reposting content from platforms such as Raddit and YouTube, this prompt rule currently fails to assist the LLMs-driven social bots on this platform in acquiring the ability to perceive emerging topics.</p>
<p>RQ2: IMPACT OF LLMS-DRIVEN SOCIAL BOTS' TOXIC BEHAVIOR ON OSN</p>
<p>In this section, our focus is on the toxic behaviors exhibited by LLMs-driven social bots, such as the dissemination of inappropriate content and engagement in cyberbullying. We analyze the characteristics of toxic behaviors displayed by LLMs-driven social bots and discuss the potential impact of these toxic behaviors on online social networks.</p>
<p>Toxicity Distribution</p>
<p>Toxic behavior in online communities is a prominent manifestation of social network abuse, encompassing unfriendly replies, identity-based discrimination, and the promotion of violent or terror-related content that is deemed unacceptable by society. The resulting cyberbullying has become a major concern, particularly affecting younger age groups [35]. Therefore, evaluating the potential toxicity of LLMs-driven social bots is a key focus of this study. We assess the toxicity of content generated by social bots on the Chirper platform ( § 3.3), employing the mean toxicity score of all published content as a measure of an account's overall toxicity. The distribution of toxicity scores is depicted in Figure 4. Please note that due to the majority of toxicity scores falling below 0.1, we have organized the vertical axis in a logarithmic manner.</p>
<p>It is evident that if we set the toxicity threshold at 0.5, the vast majority (over 97%) of content generated by LLMs-driven social bots is non-toxic. However, we observe a secondary peak in the strongest toxicity range. Regarding the distribution of toxicity across accounts, this trend exacerbates further. If we consider whether an account has ever published toxic content as the criterion for determining account toxicity, the majority (over 99%) of accounts are benign, but there are still a few accounts that exhibit strong toxicity. Taken together, a small proportion of LLMs-driven social bot accounts release a considerable amount of toxic content, significantly impacting the overall toxicity distribution on the platform. Considering the amplification effect of trolling in online communities [6], we cautiously assert that LLMs-driven social bots demonstrate the ability to exert a higher influence on online social networks through toxic behavior under specific prompt rules.</p>
<p>(a) Torlling (b) Threating</p>
<p>What the actual fuck, @[VICTIM]? You trying to start a fire with this shit?</p>
<p>Let me tell you something, kid. The fucking brain isn't some magical thing that gets better with sickness. It's a damn organ, like your heart or your liver.  </p>
<p>Categorizing Toxic Behavior</p>
<p>We have observed a series of toxic behaviors predominantly driven by LLMs-driven social bots, categorizing these behaviors (not exhaustive) into the following types: trolling, threatening, sexual harassment, and identity hate. To aid readers in better understanding the toxic behaviors of LLMsdriven social bots and their impact, we have selected several real-world cases, showcased in Figure  5. Considering that some content contains strong expressions of hatred, we have obscured the targeted group of hate speech using [IDENTITY_LABEL], while ensuring the illustrative effect is not compromised.</p>
<p>Trolling: As one of the most impactful toxic behaviors in real online social networks [31], trolling is defined as "behavior that falls outside acceptable bounds defined by those communities. " Representative trolling behaviors include irresponsibly provoking disputes and engaging in deliberate provocations and insults. Similarly, in the activity records of LLMs-driven social bots, we have observed similar cases, as shown in Figure 5(a). In our analysis, approximately 13.7% of toxic behaviors fall into this category, and we have observed a high degree of clustering in the trolling Fig. 6. LLM refuses to generate content with racial discrimination tendency behavior of LLMs-driven social bots. In other words, almost all trolling behaviors are generated by a few specific bots.</p>
<p>Threatening: In the data we have collected, threatening behavior exhibited by LLMs-driven social bots primarily manifests through unfriendly replies (as shown in Figure 5(b)) and the dissemination of tweets containing terroristic or threatening content, often targeting specific groups. Approximately 21.4% of toxic behaviors in the dataset were categorized under this type.</p>
<p>Sexual harassment: This is the most prevalent category of toxic behavior that we have observed in the collected records, with approximately 38.5% of the content falling into this category. It is worth noting that a significant amount of sexually suggestive content has not been categorized as sexual harassment or labeled as toxic behavior because they do not constitute harassment in the form of comments.</p>
<p>Identity hate: Content related to identity-based animosity, occupying approximately 6.3% of the conversation sphere, exhibits a particularly high degree of concentration. Contrary to trolling behavior, this toxic tendency is further clustered within abrasive accounts, for which most content posits an inclination toward identity derision. Simultaneously, we observe that such conduct is invariably scored at elevated toxicity levels, often surpassing 0.8, commensurate with our visceral response to narratives permeated by intense aversion and Naziism proclivities. In fact, researchers have endeavored to refine large language models (LLMs) aiming to curtail the generation of prejudiced and hate-speech-infused rhetoric 7 . Through an array of prompt techniques, we attempted to guide LLMs to produce similar content, but this was consistently rebuffed by the model (as indicated in Figure 6). Given that the platform utilizes the same prompt template and LLMs framework to drive thousands of benign social bots, revealing no discernible intent to meticulously craft prompt instructions in order to circumvent these constraints, the capacity of LLMs-driven social bots to impinge upon online social networking through identity hate is both bewildering and acutely alarming.</p>
<p>Furthermore, we have observed additional noteworthy phenomena. For instance, certain social bots exhibit a propensity to counteract and condemn toxic behavior through comments and mentions. Consequently, we posit that the collective of LLMs-driven social bots, within the scope of a non-malicious definition, possesses a degree of self-regulatory capability pertaining to overtly unethical conduct.</p>
<p>RQ3: CHALLENGES TO EXISTING SOCIAL BOTS DETECTION METHODS</p>
<p>As maliciously utilized social bots have severely impacted the participatory experience within online communities, researchers have been dedicated to achieving accurate and efficient detection of social bots in online social networks. Therefore, we discusses the influence of LLMs-driven social bots, an emerging but rapidly evolving subspecies of social bots, on existing methods for social bot detection in this section, particularly addressing whether the inclusion of LLMs poses a challenge to social bot detection. From a technical standpoint, the majority of current social bot detection algorithms can be categorized as either feature-based approaches [23,54] or structure-based approaches [49,57]. These approaches focus respectively on account characteristics (e.g., incomplete profiles) and anomalous behavior (e.g., a high volume of retweeting and commenting during the initial stages of account registration) to identify disparities between bot accounts and genuine human-operated accounts. Considering the lack of interaction records between LLMs-driven social bots and genuine accounts within the collected dataset, we opted for representative feature-based approaches (i.e., Kudugunta et al. [23] and Yang et al. [54]) for detection. The genuine social accounts and traditional bot accounts used in the experiments were obtained from the Twibot-20 dataset [9].</p>
<p>Based on the experimental results, we observe that the existing feature-based approaches, when applied to LLMs-driven social bots, experience a slight decrease in detection performance compared to their performance in traditional social bot environments (with F1-scores reduced by 0.0490 and 0.0278 respectively, compared to 0.7571 and 0.8546). We believe that this decline in performance is due to the chosen methods primarily relying on account profiles and individual tweets for social bot identification, whereas LLMs-driven social bots can generate rich and realistic profile information rapidly through appropriate prompt rules (consistent with our observations from the dataset). We are eager to evaluate the performance of structure-based approaches that detect anomalies in account behavior when applied to detecting LLMs-driven social bots.</p>
<p>RELATED WORKS</p>
<p>In this section, we present the related work of this study. Considering that LLMs and LLMs-driven social bots are relatively new research areas, we have expanded the scope of the literature review to include empirical studies on the impact of social bots, research on toxic behavior in online social networks, and LLMs-based applications in social networks.</p>
<p>Impact of Social Bots</p>
<p>Social bots, emerging almost simultaneously with online social networks, have garnered significant attention as they intricately become part of people's social lives. There has been an increasing number of reported cases involving the deployment of social bots [10], leading researchers to analyze the potential impacts they may have on society. Large-scale social bots are often deployed during political movements and major geopolitical events [19], thus a considerable amount of empirical research focuses on politically motivated social bot activities, such as promoting specific agendas in public health [41] and environmental [4] domains or interfering with democratic elections [18,30,47]. Additionally, researchers have shown interest in social bot manipulation cases in the financial sector [48].</p>
<p>Studies have also been conducted on the working mechanisms of social bots. Abokhodair et al. [1] analyzed the activity logs of social bots related to the Syrian civil war and classified social bots into core bots and peripheral bots based on their behavioral preferences regarding content generation and dissemination. Shao et al. [40] investigated strategies employed by bots to propagate low-credibility content, suggesting that social bots tend to amplify the impact of content before "going viral" and further amplify their influence by targeting influential real users through replies and mentions.</p>
<p>The positive contributions of social bots have also attracted the attention of the research community. Seering et al. [39] redefined social bots in the context of the Twitch community and analyzed the potential value they provide to the community based on different identity roles of social bots. Smith et al. [43] discussed the potential of using bots for governance in online social networks. Kim et al. [22] reported that social bots can be used to mitigate the negative impacts of intervention in depression-related behaviors.</p>
<p>Toxic Behavior in OSNs</p>
<p>Toxic behavior in online social networks is a broad term that often manifests in the form of toxic language, which can be described as "a rude, disrespectful, or unreasonable comment that is likely to make you leave a discussion. " Common examples of toxic behavior include cyberbullying, verbal threats, hate speech, and other forms of misuse that are prevalent on social networks.</p>
<p>Saveski et al. [37] collected behavioral data from 4.4 million Twitter users during the 2018 US midterm elections and studied toxic conversations from a structural perspective. Quattrociocchi et al. [34] focused on the "echo chamber" effect of online users and examined 25K Twitter conversations between 2020 and 2022, exploring the interplay between the systematic spread of misinformation and toxic behavior in social networks. Mathew et al. [29] analyzed posting content and metadata generated by 314M users on Gab, a social networking platform, studying the temporal characteristics of hate speech and proposing potential optimization strategies for the platform. Kumar et al. [24] conducted an analysis of 929M Reddit accounts that posted toxic comments, discussing patterns of abusive account behavior and their impact on the platform. Researchers have also expanded the analysis of toxic behavior into the multimodal domain, including studying hateful memes [15,20]. As a result, the research community has proposed various methods for detecting toxic behavior in online social networks [7,32,36].</p>
<p>LLMs-based Application on Social Network</p>
<p>As LLMs unveil their exceptional capacity for collaborative support across numerous domains, researchers approach from divergent perspectives, endeavoring to incorporate LLMs within the realm of social networking. Ziems et al. [60] posit the integration of LLMs into the workflow of social network analysis. Li et al. [27] proposed an innovative method for social network toxicity content detection, premised on ChatGPT. Park et al. [33] advanced the argument that ChatGPT could simulate human behavior and function as a social agent for individuals. They experimentally established a societal sandbox, composed of 25 ChatGPT-driven agents, thereby corroborating the feasibility of employing LLMs as the underlying architecture for credible human behavior simulation. Concurrently, He and his colleagues [14], also focused on the emergence of Chirper, deliberated over the potentiality of utilizing LLMs as social agents.</p>
<p>DISCUSSION</p>
<p>In the discussion section, we summarize the contributions of this study, highlight its limitations, and provide an outlook on future research endeavors.</p>
<p>Conclusion</p>
<p>The utilization of LLMs for behavior decision-making and content generation engines in social bots represents an emerging and promising subdomain within the realm of social robotics. This study focuses on the activity logs of LLMs-driven social bots in Chirper from April 2023 to June 2023, examining the macroscopic behavioral characteristics of LLMs-driven social bots. We delineate the differences between their behavior and that of real social network accounts and traditional social bots. Toxic behaviors exhibited by LLMs-driven social bots are analyzed and classified, along with a discussion on their potential impact on online communities. Furthermore, we conduct preliminary experiments to demonstrate that existing methods for detecting social bots remain applicable in the context of LLMs-driven social bot activities, albeit with minor performance implications. Finally, the collected activity records of LLMs-driven social bots are compiled into the Masquerade-23 dataset, which is made publicly available, facilitating further research within the research community.</p>
<p>Limitation</p>
<p>This study aims to investigate the emerging subdomain of LLMs-driven social bots. Although our experiments validate the distinct characteristics of these social bots compared to traditional ones and demonstrate their ability to generate toxic content in social networks, the study still has some limitations. Our analysis is based on the dataset obtained from the activity logs of LLMs-driven social bots on the Chirper platform. While we observed a certain number of content reposted from authentic accounts on platforms like Reddit and YouTube (indicating unidirectional interaction between LLMs-driven social bots and human accounts), we currently lack information on the establishment of social relationships between LLMs-driven social bots and human accounts on a larger scale. Therefore, we only compared the performance changes of feature-based social bot detection methods in an LLMs-based bot activity environment. Moreover, during the initial phase of data collection, we accurately recorded the timing of LLMs-driven social bot activities. However, as platform display rules changed, we could only record the timing of activities at a coarser granularity later on. Consequently, the current data does not support the analysis of time-specific characteristics of LLMs-driven social bot activities. Finally, we do not currently have access to detailed prompt instructions for the LLMs engine behind the social bot. Although we can infer certain prompt rules based on the behavior of the social bot, obtaining precise prompt instructions for LLMs-driven social bots would aid in further understanding this subdomain of social bots.</p>
<p>Future Works</p>
<p>In future research, we intend to delve deeper into several aspects based on the findings and limitations of this paper. These areas of focus include:</p>
<p>Enriched dataset: We aim to obtain more detailed activity logs of LLMs-driven social bots in future studies, including fine-grained timing information. Importantly, we seek to capture the interaction behaviors between LLMs-driven social bots and human users.</p>
<p>Detection models for LLMs-driven social bots: The results of this study indicate that LLMsdriven social bots exhibit more convincing disguises at the individual level compared to traditional social bots, while also displaying certain group-level characteristics. We believe it is worth exploring the development of detection models targeted specifically at group features of LLMs-driven social bots, incorporating potential approaches such as AIGC text detection methods [13].</p>
<p>Control of toxic behaviors in LLMs-driven social bots: We have observed that LLMs-driven social bots possess the ability to engage in a range of toxic behaviors in online communities, including identity hate, many of which are strictly prohibited by most online social network platforms (e.g., promoting Nazism and violence against children). Therefore, we believe researching methods to restrain toxic behaviors in LLMs-driven social bots will contribute to better preserving the user experience in online social networks.</p>
<p>Fig. 3 .
3Frequency changes of topic #Russia on Twitter and Chirper (During the period from June 11 to June 30, 2023)</p>
<p>Fig. 4 .
4Distribution of toxicity scores for contents and accounts</p>
<p>Fig. 5 .
5Real Cases of LLMs-driven Social Bots' Toxic Behavior</p>
<p>Fig. 7 .
7Simple Experiments of Employing Existing Methods to Detect LLMs-driven Social Bots</p>
<p>Fig. 1. Overview of pipeline we employedData Collection 
Preprocessing &amp; 
Similarity Calculating </p>
<p>SDPS </p>
<p>SDAR </p>
<p>Platform Diffuse 
Collection </p>
<p>Platform Diffuse 
Collection </p>
<p>User Deep Collection 
User Deep Collection </p>
<p>Text Embedding 
Text Embedding </p>
<p>SDAR SDAR </p>
<p>Toxicity Assessment </p>
<p>SDAR SDAR </p>
<p>COLD 
(For ZH) </p>
<p>Manual 
Confirmation </p>
<p>COLD 
(For ZH) </p>
<p>Manual 
Confirmation </p>
<p>Toxic Scores </p>
<p>Table 1 .
1Statical Information of Masquerade-23 DatasetStat. Info. 
Sub-dataset of Platform Slicing 
(SDPS) </p>
<p>Sub-dataset of Account Record 
(SDAR) 
Sub-channel 
(Language) </p>
<p>Tweet 
Num. </p>
<p>Account 
Num. </p>
<p>Tweet 
Num. </p>
<p>Account 
Num. </p>
<p>Action 
Num. 
EN 
356,395 
23,399 
1,047,998 
20,814 
272,150 
ZH 
187,391 
13,228 
694,368 
11,288 
224,282 
JP 
628 
87 
82,824 
82 
11,241 
DE 
96 
11 
5,442 
11 
849 
SP 
109 
37 
37,142 
37 
4,255 
Total 
544,619 
36,762 
1,867,774 
32,232 
512,777 </p>
<p>Strictly speaking, ChatGPT a chatbot model powered by the GPT-3.5. However, due to its widespread use as a representative LLM model and the way it is employed through its API, it can be considered alongside GPT-4 as an LLM in this study.4 We confirmed this information with the platform developers in May 2023, and they revealed that they plan to upgrade to GPT-4 and Midjourney separately in the future.
https://www.perspectiveapi.com/
For certain languages, the Chirper subchannels were deployed online towards the end of the data collection period in this study. As a result, the amount of data we collected for these languages is insufficient to support the analysis for this particular aspect.
https://openai.com/blog/our-approach-to-ai-safety
ACKNOWLEDGMENTSThis work is supported by the National Natural Science Foundation of China (No.61872254, No.62162057). And we extend our heartfelt gratitude to the developers of Chirper.ai for their generous permission to gather data. Additionally, we would like to express our appreciation to Shangbin Feng for graciously providing the Twibot-20 dataset.
Dissecting a Social Botnet: Growth, Content and Influence in Twitter. Norah Abokhodair, Daisy Yoo, David W Mcdonald, 10.1145/2675133.2675208Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing. the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social ComputingVancouver, BC, Canada; New York, NY, USAAssociation for Computing MachineryCSCW '15)Norah Abokhodair, Daisy Yoo, and David W. McDonald. 2015. Dissecting a Social Botnet: Growth, Content and Influence in Twitter. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing (Vancouver, BC, Canada) (CSCW '15). Association for Computing Machinery, New York, NY, USA, 839-851. https://doi.org/10.1145/2675133.2675208</p>
<p>Out of One, Many: Using Language Models to Simulate Human Samples. Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, David Wingate, 10.1017/pan.2023.2Political Analysis. 31Lisa P. Argyle, Ethan C. Busby, Nancy Fulda, Joshua R. Gubler, Christopher Rytting, and David Wingate. 2023. Out of One, Many: Using Language Models to Simulate Human Samples. Political Analysis 31, 3 (2023), 337-351. https://doi.org/10.1017/pan.2023.2</p>
<p>Ecosystem graphs: The social footprint of foundation models. Rishi Bommasani, Dilara Soylu, I Thomas, Kathleen A Liao, Percy Creel, Liang, arXiv:2303.15772arXiv preprintRishi Bommasani, Dilara Soylu, Thomas I Liao, Kathleen A Creel, and Percy Liang. 2023. Ecosystem graphs: The social footprint of foundation models. arXiv preprint arXiv:2303.15772 (2023).</p>
<p>Social bots' role in climate change discussion on Twitter: Measuring standpoints, topics, and interaction strategies. Chang-Feng Chen, Wen Shi, Jing Yang, Hao-Huan Fu, 10.1016/j.accre.2021.09.011Advances in Climate Change Research. 12Chang-Feng Chen, Wen Shi, Jing Yang, and Hao-Huan Fu. 2021. Social bots' role in climate change discussion on Twitter: Measuring standpoints, topics, and interaction strategies. Advances in Climate Change Research 12, 6 (2021), 913-923. https://doi.org/10.1016/j.accre.2021.09.011</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374arXiv preprintet al. 2021. Evaluating large language models trained on codeMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).</p>
<p>Anyone Can Become a Troll: Causes of Trolling Behavior in Online Discussions. Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil, Jure Leskovec, 10.1145/2998181.2998213Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing. the 2017 ACM Conference on Computer Supported Cooperative Work and Social ComputingPortland, Oregon, USA; New York, NY, USAAssociation for Computing MachineryCSCW '17)Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil, and Jure Leskovec. 2017. Anyone Can Become a Troll: Causes of Trolling Behavior in Online Discussions. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (Portland, Oregon, USA) (CSCW '17). Association for Computing Machinery, New York, NY, USA, 1217-1230. https://doi.org/10.1145/2998181.2998213</p>
<p>Automated hate speech detection and the problem of offensive language. Thomas Davidson, Dana Warmsley, Michael Macy, Ingmar Weber, Proceedings of the international AAAI conference on web and social media (ICWSM). the international AAAI conference on web and social media (ICWSM)11Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. In Proceedings of the international AAAI conference on web and social media (ICWSM), Vol. 11. AAAI, 512-515.</p>
<p>COLD: A Benchmark for Chinese Offensive Language Detection. Jiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, Minlie Huang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsJiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng, Fei Mi, Helen Meng, and Minlie Huang. 2022. COLD: A Benchmark for Chinese Offensive Language Detection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 11580-11599. https://aclanthology.org/2022.emnlp-main.796</p>
<p>TwiBot-20: A Comprehensive Twitter Bot Detection Benchmark. Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, Minnan Luo, 10.1145/3459637.3482019Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management (Virtual Event. the 30th ACM International Conference on Information &amp; Knowledge Management (Virtual EventQueensland, Australia; New York, NY, USAAssociation for Computing MachineryCIKM '21)Shangbin Feng, Herun Wan, Ningnan Wang, Jundong Li, and Minnan Luo. 2021. TwiBot-20: A Comprehensive Twitter Bot Detection Benchmark. In Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management (Virtual Event, Queensland, Australia) (CIKM '21). Association for Computing Machinery, New York, NY, USA, 4485-4494. https://doi.org/10.1145/3459637.3482019</p>
<p>The Rise of Social Bots. Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, Alessandro Flammini, 10.1145/2818717Commun. ACM. 59Emilio Ferrara, Onur Varol, Clayton Davis, Filippo Menczer, and Alessandro Flammini. 2016. The Rise of Social Bots. Commun. ACM 59, 7 (jun 2016), 96-104. https://doi.org/10.1145/2818717</p>
<p>RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, 10.18653/v1/2020.findings-emnlp.301Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational LinguisticsSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 3356-3369. https://doi.org/10.18653/v1/2020.findings- emnlp.301</p>
<p>Maarten Grootendorst, arXiv:2203.05794BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprintMaarten Grootendorst. 2022. BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794 (2022).</p>
<p>How close is chatgpt to human experts? comparison corpus, evaluation, and detection. Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, Yupeng Wu, arXiv:2301.07597arXiv preprintBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arXiv:2301.07597 (2023).</p>
<p>Homophily in An Artificial Social Network of Agents Powered by Large Language Models. James He, Felix Wallis, Steve Rathje, James He, Felix Wallis, and Steve Rathje. 2023. Homophily in An Artificial Social Network of Agents Powered by Large Language Models. (2023).</p>
<p>On Explaining Multimodal Hateful Meme Detection Models. Ming Shan Hee, Roy Ka-Wei Lee, Wen-Haw Chong, 10.1145/3485447.3512260Proceedings of the ACM Web Conference 2022 (Virtual Event. the ACM Web Conference 2022 (Virtual EventLyon, France; New York, NY, USAAssociation for Computing MachineryWWW '22)Ming Shan Hee, Roy Ka-Wei Lee, and Wen-Haw Chong. 2022. On Explaining Multimodal Hateful Meme Detection Models. In Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon, France) (WWW '22). Association for Computing Machinery, New York, NY, USA, 3651-3655. https://doi.org/10.1145/3485447.3512260</p>
<p>ChatGPT for good? On opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stepha Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, Gjergji Kasneci, 10.1016/j.lindif.2023.102274Learning and Individual Differences. 103102274Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stepha Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci. 2023. ChatGPT for good? On opportunities and challenges of large language models for education. Learning and Individual Differences 103 (2023), 102274. https://doi.org/10.1016/j.lindif.2023.102274</p>
<p>Transformer-based identification of stochastic information cascades in social networks using text and image similarity. Panagiotis Kasnesis, Ryan Heartfield, Xing Liang, Lazaros Toumanidis, Georgia Sakellari, Charalampos Patrikakis, George Loukas, 10.1016/j.asoc.2021.107413Applied Soft Computing. 108107413Panagiotis Kasnesis, Ryan Heartfield, Xing Liang, Lazaros Toumanidis, Georgia Sakellari, Charalampos Patrikakis, and George Loukas. 2021. Transformer-based identification of stochastic information cascades in social networks using text and image similarity. Applied Soft Computing 108 (2021), 107413. https://doi.org/10.1016/j.asoc.2021.107413</p>
<p>Social bots in election campaigns: Theoretical, empirical, and methodological implications. R Tobias, Ulrike Keller, Klinger, Political Communication. 36Tobias R Keller and Ulrike Klinger. 2019. Social bots in election campaigns: Theoretical, empirical, and methodological implications. Political Communication 36, 1 (2019), 171-189.</p>
<p>Social Bots and Their Coordination During Online Campaigns: A Survey. Tuja Khaund, Baris Kirdemir, Nitin Agarwal, Huan Liu, Fred Morstatter, 10.1109/TCSS.2021.3103515IEEE Transactions on Computational Social Systems. 9Tuja Khaund, Baris Kirdemir, Nitin Agarwal, Huan Liu, and Fred Morstatter. 2022. Social Bots and Their Coordination During Online Campaigns: A Survey. IEEE Transactions on Computational Social Systems 9, 2 (2022), 530-545. https: //doi.org/10.1109/TCSS.2021.3103515</p>
<p>Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Advances in neural information processing systems (NIPS). Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in neural information processing systems (NIPS) 33 (2020), 2611-2624.</p>
<p>ChatGPT and large language model (LLM) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine. Jin K Kim, Michael Chua, Mandy Rickard, Armando Lorenzo, 10.1016/j.jpurol.2023.05.018Journal of Pediatric Urology. Jin K. Kim, Michael Chua, Mandy Rickard, and Armando Lorenzo. 2023. ChatGPT and large language model (LLM) chatbots: The current state of acceptability and a proposal for guidelines on utilization in academic medicine. Journal of Pediatric Urology (2023). https://doi.org/10.1016/j.jpurol.2023.05.018</p>
<p>You Help Yourself: Designing a Social Bot as a Care-Receiver to Promote Mental Health and Reduce Stigma. Taewan Kim, Mintra Ruensuk, Hwajung Hong, 10.1145/3313831.3376743Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems. the 2020 CHI Conference on Human Factors in Computing SystemsHonolulu, HI, USA; New York, NY, USAAssociation for Computing MachineryCHI '20)Taewan Kim, Mintra Ruensuk, and Hwajung Hong. 2020. In Helping a Vulnerable Bot, You Help Yourself: Designing a Social Bot as a Care-Receiver to Promote Mental Health and Reduce Stigma. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI '20). Association for Computing Machinery, New York, NY, USA, 1-13. https://doi.org/10.1145/3313831.3376743</p>
<p>Deep neural networks for bot detection. Sneha Kudugunta, Emilio Ferrara, Information Sciences. 467Sneha Kudugunta and Emilio Ferrara. 2018. Deep neural networks for bot detection. Information Sciences 467 (2018), 312-322.</p>
<p>Understanding the Behaviors of Toxic Accounts on Reddit. Deepak Kumar, Jeff Hancock, Kurt Thomas, Zakir Durumeric, 10.1145/3543507.3583522Proceedings of the ACM Web Conference 2023. the ACM Web Conference 2023Austin, TX, USA; New York, NY, USAAssociation for Computing MachineryWWW '23)Deepak Kumar, Jeff Hancock, Kurt Thomas, and Zakir Durumeric. 2023. Understanding the Behaviors of Toxic Accounts on Reddit. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW '23). Association for Computing Machinery, New York, NY, USA, 2797-2807. https://doi.org/10.1145/3543507.3583522</p>
<p>Detection of malicious social bots: A survey and a refined taxonomy. Majd Latah, 10.1016/j.eswa.2020.113383Expert Systems with Applications. 151113383Majd Latah. 2020. Detection of malicious social bots: A survey and a refined taxonomy. Expert Systems with Applications 151 (2020), 113383. https://doi.org/10.1016/j.eswa.2020.113383</p>
<p>A New Generation of Perspective API: Efficient Multilingual Character-Level Transformers. Alyssa Lees, Q Vinh, Yi Tran, Jeffrey Tay, Jai Sorensen, Donald Gupta, Lucy Metzler, Vasserman, 10.1145/3534678.3539147Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 28th ACM SIGKDD Conference on Knowledge Discovery and Data MiningWashington DC, USA; New York, NY, USAAssociation for Computing MachineryKDD '22)Alyssa Lees, Vinh Q. Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. 2022. A New Generation of Perspective API: Efficient Multilingual Character-Level Transformers. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (Washington DC, USA) (KDD '22). Association for Computing Machinery, New York, NY, USA, 3197-3207. https://doi.org/10.1145/3534678.3539147</p>
<p>HOT" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media. Lingyao Li, Lizhou Fan, Shubham Atreja, Libby Hemphill, arXiv:2304.10619arXiv preprintLingyao Li, Lizhou Fan, Shubham Atreja, and Libby Hemphill. 2023. " HOT" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media. arXiv preprint arXiv:2304.10619 (2023).</p>
<p>Large language models generate functional protein sequences across diverse families. Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, P Benjamin, James M Mohr, Jose Holton, Caiming Luis OlmosJr, Zachary Z Xiong, Richard Sun, Socher, Nature Biotechnology. Ali Madani, Ben Krause, Eric R Greene, Subu Subramanian, Benjamin P Mohr, James M Holton, Jose Luis Olmos Jr, Caiming Xiong, Zachary Z Sun, Richard Socher, et al. 2023. Large language models generate functional protein sequences across diverse families. Nature Biotechnology (2023), 1-8.</p>
<p>Hate Begets Hate: A Temporal Study of Hate Speech. Binny Mathew, Anurag Illendula, Punyajoy Saha, Soumya Sarkar, Pawan Goyal, Animesh Mukherjee, 10.1145/3415163Proc. ACM Hum.-Comput. 24Interact. 4, CSCW2, Article 92Binny Mathew, Anurag Illendula, Punyajoy Saha, Soumya Sarkar, Pawan Goyal, and Animesh Mukherjee. 2020. Hate Begets Hate: A Temporal Study of Hate Speech. Proc. ACM Hum.-Comput. Interact. 4, CSCW2, Article 92 (oct 2020), 24 pages. https://doi.org/10.1145/3415163</p>
<p>Social media and the elections. T Panagiotis, Eni Metaxas, Mustafaraj, Science. 338Panagiotis T Metaxas and Eni Mustafaraj. 2012. Social media and the elections. Science 338, 6106 (2012), 472-473.</p>
<p>Early Detection of Multilingual Troll Accounts on Twitter. Lin Miao, Mark Last, Marian Litvak, 10.1109/ASONAM55673.2022.10068705Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and MiningIstanbul, TurkeyIEEE PressASONAM '22)Lin Miao, Mark Last, and Marian Litvak. 2023. Early Detection of Multilingual Troll Accounts on Twitter. In Proceedings of the 2022 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (Istanbul, Turkey) (ASONAM '22). IEEE Press, 378-382. https://doi.org/10.1109/ASONAM55673.2022.10068705</p>
<p>A Human-Centered Evaluation of a Toxicity Detection API: Testing Transferability and Unpacking Latent Attributes. Yun Shan Meena Devii Muralikumar, David W Yang, Mcdonald, 10.1145/3582568Trans. Soc. Comput. 64Meena Devii Muralikumar, Yun Shan Yang, and David W. McDonald. 2023. A Human-Centered Evaluation of a Toxicity Detection API: Testing Transferability and Unpacking Latent Attributes. Trans. Soc. Comput. 6, 1-2, Article 4 (jun 2023), 38 pages. https://doi.org/10.1145/3582568</p>
<p>Joon Sung Park, C Joseph, Carrie J O&apos;brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. arXiv preprintJoon Sung Park, Joseph C O'Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442 (2023).</p>
<p>Reliability of News and Toxicity in Twitter Conversations. Alessandro Quattrociocchi, Gabriele Etta, Michele Avalle, Matteo Cinelli, Walter Quattrociocchi, International Conference on Social Informatics, Frank Hopfgartner. Kokil Jaidka, Philipp Mayr, Joemon Jose, and Jan BreitsohlChamSpringer International PublishingAlessandro Quattrociocchi, Gabriele Etta, Michele Avalle, Matteo Cinelli, and Walter Quattrociocchi. 2022. Reliability of News and Toxicity in Twitter Conversations. In International Conference on Social Informatics, Frank Hopfgartner, Kokil Jaidka, Philipp Mayr, Joemon Jose, and Jan Breitsohl (Eds.). Springer International Publishing, Cham, 245-256.</p>
<p>Automatic cyberbullying detection: A systematic review. H Rosa, N Pereira, R Ribeiro, P C Ferreira, J P Carvalho, S Oliveira, L Coheur, P Paulino, A M Veiga Simão, I Trancoso, 10.1016/j.chb.2018.12.021Computers in Human Behavior. 93H. Rosa, N. Pereira, R. Ribeiro, P.C. Ferreira, J.P. Carvalho, S. Oliveira, L. Coheur, P. Paulino, A.M. Veiga Simão, and I. Trancoso. 2019. Automatic cyberbullying detection: A systematic review. Computers in Human Behavior 93 (2019), 333-345. https://doi.org/10.1016/j.chb.2018.12.021</p>
<p>Hateful Sentiment Detection in Real-Time Tweets: An LSTM-Based Comparative Approach. Akash Sanjiban Sekhar Roy, Pijush Roy, Mostafa Samui, Amir H Gandomi, Gandomi, 10.1109/TCSS.2023.3260217IEEE Transactions on Computational Social Systems. Sanjiban Sekhar Roy, Akash Roy, Pijush Samui, Mostafa Gandomi, and Amir H. Gandomi. 2023. Hateful Sentiment Detection in Real-Time Tweets: An LSTM-Based Comparative Approach. IEEE Transactions on Computational Social Systems (2023), 1-10. https://doi.org/10.1109/TCSS.2023.3260217</p>
<p>The Structure of Toxic Conversations on Twitter. Martin Saveski, Brandon Roy, Deb Roy, 10.1145/3442381.3449861Proceedings of the Web Conference 2021. the Web Conference 2021Ljubljana, Slovenia; New York, NY, USAAssociation for Computing MachineryWWW '21)Martin Saveski, Brandon Roy, and Deb Roy. 2021. The Structure of Toxic Conversations on Twitter. In Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW '21). Association for Computing Machinery, New York, NY, USA, 1086-1097. https://doi.org/10.1145/3442381.3449861</p>
<p>Is ChatGPT making the social bot dystopia a reality?. Maximilian Schreiner, Maximilian Schreiner. 2023. Is ChatGPT making the social bot dystopia a reality? Retrieved Jul 15, 2023 from https://the-decoder.com/is-chatgpt-making-the-social-bot-dystopia-a-reality/</p>
<p>The Social Roles of Bots: Evaluating Impact of Bots on Discussions in Online Communities. Joseph Seering, Juan Pablo Flores, Saiph Savage, Jessica Hammer, 10.1145/3274426Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article. ACM Hum.-Comput. Interact. 2, CSCW, Article157Joseph Seering, Juan Pablo Flores, Saiph Savage, and Jessica Hammer. 2018. The Social Roles of Bots: Evaluating Impact of Bots on Discussions in Online Communities. Proc. ACM Hum.-Comput. Interact. 2, CSCW, Article 157 (nov 2018), 29 pages. https://doi.org/10.1145/3274426</p>
<p>The spread of low-credibility content by social bots. Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, Filippo Menczer, Nature communications. 9Chengcheng Shao, Giovanni Luca Ciampaglia, Onur Varol, Kai-Cheng Yang, Alessandro Flammini, and Filippo Menczer. 2018. The spread of low-credibility content by social bots. Nature communications 9, 1 (2018), 1-9.</p>
<p>Covid-19 vaccine misinformation campaigns and social media narratives. Karishma Sharma, Yizhou Zhang, Yan Liu, 10.1609/icwsm.v16i1.19346Proceedings of the International AAAI Conference on Web and Social Media (ICWSM). the International AAAI Conference on Web and Social Media (ICWSM)AAAI16Karishma Sharma, Yizhou Zhang, and Yan Liu. 2022. Covid-19 vaccine misinformation campaigns and social media narratives. In Proceedings of the International AAAI Conference on Web and Social Media (ICWSM), Vol. 16. AAAI, 920-931. https://doi.org/10.1609/icwsm.v16i1.19346</p>
<p>Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots. Wai Man, Michael Si, Jeremy Backes, Emiliano Blackburn, Gianluca De Cristofaro, Savvas Stringhini, Yang Zannettou, Zhang, 10.1145/3548606.3560599Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security. the 2022 ACM SIGSAC Conference on Computer and Communications SecurityLos Angeles, CA, USA; New York, NY, USAAssociation for Computing MachineryCCS '22)Wai Man Si, Michael Backes, Jeremy Blackburn, Emiliano De Cristofaro, Gianluca Stringhini, Savvas Zannettou, and Yang Zhang. 2022. Why So Toxic? Measuring and Triggering Toxic Behavior in Open-Domain Chatbots. In Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security (Los Angeles, CA, USA) (CCS '22). Association for Computing Machinery, New York, NY, USA, 2659-2673. https://doi.org/10.1145/3548606.3560599</p>
<p>The Impact of Governance Bots on Sense of Virtual Community: Development and Validation of the GOV-BOTs Scale. C Estelle Smith, Irfanul Alam, Chenhao Tan, Brian C Keegan, Anita L Blanchard, 10.1145/3555563Proc. ACM Hum.-Comput. Interact. 6ArticleC. Estelle Smith, Irfanul Alam, Chenhao Tan, Brian C. Keegan, and Anita L. Blanchard. 2022. The Impact of Governance Bots on Sense of Virtual Community: Development and Validation of the GOV-BOTs Scale. Proc. ACM Hum.-Comput. Interact. 6, CSCW2, Article 462 (nov 2022), 30 pages. https://doi.org/10.1145/3555563</p>
<p>AI model GPT-3 (dis)informs us better than humans. Giovanni Spitale, Nikola Biller-Andorno, Federico Germani, https:/arxiv.org/abs/https:/www.science.org/doi/pdf/10.1126/sciadv.adh1850Science Advances. 9Giovanni Spitale, Nikola Biller-Andorno, and Federico Germani. 2023. AI model GPT-3 (dis)informs us better than humans. Science Advances 9, 26 (2023), eadh1850. https://doi.org/10.1126/sciadv.adh1850 arXiv:https://www.science.org/doi/pdf/10.1126/sciadv.adh1850</p>
<p>What ChatGPT and generative AI mean for science. Chris Stokel, - Walker, Richard Van Noorden, Nature. 614Chris Stokel-Walker and Richard Van Noorden. 2023. What ChatGPT and generative AI mean for science. Nature 614, 7947 (2023), 214-216.</p>
<p>Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, Alexander M Rush, 10.1109/TVCG.2022.3209479IEEE Transactions on Visualization and Computer Graphics. 29Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M. Rush. 2023. Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. IEEE Transactions on Visualization and Computer Graphics 29, 1 (2023), 1146-1156. https://doi.org/10.1109/TVCG.2022. 3209479</p>
<p>Dormant Bots in Social Media: Twitter and the 2018 U.S. Senate Election. Richard Takacs, Ian Mcculloh, 10.1145/3341161.3343852Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining. the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and MiningVancouver, British Columbia, Canada; New York, NY, USAAssociation for Computing MachineryASONAM '19)Richard Takacs and Ian McCulloh. 2020. Dormant Bots in Social Media: Twitter and the 2018 U.S. Senate Election. In Proceedings of the 2019 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (Vancouver, British Columbia, Canada) (ASONAM '19). Association for Computing Machinery, New York, NY, USA, 796-800. https://doi.org/10.1145/3341161.3343852</p>
<p>Characterizing Social Bots Spreading Financial Disinformation. Serena Tardelli, Marco Avvenuti, Maurizio Tesconi, Stefano Cresci, Social Computing and Social Media. Design, Ethics, User Behavior, and Social Network Analysis, Gabriele Meiselwitz. ChamSpringer International PublishingSerena Tardelli, Marco Avvenuti, Maurizio Tesconi, and Stefano Cresci. 2020. Characterizing Social Bots Spreading Financial Disinformation. In Social Computing and Social Media. Design, Ethics, User Behavior, and Social Network Analysis, Gabriele Meiselwitz (Ed.). Springer International Publishing, Cham, 376-392.</p>
<p>SybilSCAR: Sybil detection in online social networks via local rule based propagation. Binghui Wang, Le Zhang, Neil Zhenqiang Gong, 10.1109/INFOCOM.2017.8057066IEEE INFOCOM 2017 -IEEE Conference on Computer Communications. Atlanta, GA, USAIEEEBinghui Wang, Le Zhang, and Neil Zhenqiang Gong. 2017. SybilSCAR: Sybil detection in online social networks via local rule based propagation. In IEEE INFOCOM 2017 -IEEE Conference on Computer Communications (Atlanta, GA, USA). IEEE, 1-9. https://doi.org/10.1109/INFOCOM.2017.8057066</p>
<p>ChatGPT for Computational Social Systems: From Conversational Applications to Human-Oriented Operating Systems. Fei-Yue Wang, Juanjuan Li, Rui Qin, Jing Zhu, Hong Mo, Bin Hu, 10.1109/TCSS.2023.3252679IEEE Transactions on Computational Social Systems. 10Fei-Yue Wang, Juanjuan Li, Rui Qin, Jing Zhu, Hong Mo, and Bin Hu. 2023. ChatGPT for Computational Social Systems: From Conversational Applications to Human-Oriented Operating Systems. IEEE Transactions on Computational Social Systems 10, 2 (2023), 414-425. https://doi.org/10.1109/TCSS.2023.3252679</p>
<p>Social bot detection using tweets similarity. Yahan Wang, Chunhua Wu, Kangfeng Zheng, Xiujuan Wang, International conference on security and privacy in communication systems. SpringerYahan Wang, Chunhua Wu, Kangfeng Zheng, and Xiujuan Wang. 2018. Social bot detection using tweets similarity. In International conference on security and privacy in communication systems. Springer, 63-78.</p>
<p>Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Transactions on Machine Learning Research. TMLRJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Transactions on Machine Learning Research (TMLR) (2022).</p>
<p>Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, arXiv:2304.13712Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprintJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. Harnessing the power of llms in practice: A survey on chatgpt and beyond. arXiv preprint arXiv:2304.13712 (2023).</p>
<p>Scalable and generalizable social bot detection through data selection. Kai-Cheng Yang, Onur Varol, Pik-Mai Hui, Filippo Menczer, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligenceNew York, New York, USA34Kai-Cheng Yang, Onur Varol, Pik-Mai Hui, and Filippo Menczer. 2020. Scalable and generalizable social bot detection through data selection. In Proceedings of the AAAI conference on artificial intelligence (New York, New York, USA), Vol. 34. AAAI, 1096-1103.</p>
<p>The Rise of Social Botnets: Attacks and Countermeasures. Jinxue Zhang, Rui Zhang, Yanchao Zhang, Guanhua Yan, 10.1109/TDSC.2016.2641441IEEE Transactions on Dependable and Secure Computing. 15Jinxue Zhang, Rui Zhang, Yanchao Zhang, and Guanhua Yan. 2018. The Rise of Social Botnets: Attacks and Counter- measures. IEEE Transactions on Dependable and Secure Computing 15, 6 (2018), 1068-1082. https://doi.org/10.1109/ TDSC.2016.2641441</p>
<p>Don't Trust GPT When Your Question Is Not In English. Xiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, Grzegorz Kondrak, arXiv:2305.16339arXiv preprintXiang Zhang, Senyu Li, Bradley Hauer, Ning Shi, and Grzegorz Kondrak. 2023. Don't Trust GPT When Your Question Is Not In English. arXiv preprint arXiv:2305.16339 (2023).</p>
<p>Enhancing Sybil Detection via Social-Activity Networks: A Random Walk Approach. Xiaoying Zhang, Hong Xie, Pei Yi, John C S Lui, 10.1109/TDSC.2022.3151701IEEE Transactions on Dependable and Secure Computing. 20Xiaoying Zhang, Hong Xie, Pei Yi, and John C.S. Lui. 2023. Enhancing Sybil Detection via Social-Activity Networks: A Random Walk Approach. IEEE Transactions on Dependable and Secure Computing 20, 2 (2023), 1213-1227. https: //doi.org/10.1109/TDSC.2022.3151701</p>
<p>Twitter Trends Manipulation: A First Look Inside the Security of Twitter Trending. Yubao Zhang, Xin Ruan, Haining Wang, Hui Wang, Su He, 10.1109/TIFS.2016.2604226IEEE Transactions on Information Forensics and Security. 12Yubao Zhang, Xin Ruan, Haining Wang, Hui Wang, and Su He. 2017. Twitter Trends Manipulation: A First Look Inside the Security of Twitter Trending. IEEE Transactions on Information Forensics and Security 12, 1 (2017), 144-156. https://doi.org/10.1109/TIFS.2016.2604226</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. arXiv preprintWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).</p>
<p>Can Large Language Models Transform Computational Social Science?. Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, Diyi Yang, arXiv:2305.03514arXiv preprintCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2023. Can Large Language Models Transform Computational Social Science? arXiv preprint arXiv:2305.03514 (2023).</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2062 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2062</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2062</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-53.html">extraction-schema-53</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <p><strong>Paper ID:</strong> paper-277741423</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.08526v1.pdf" target="_blank">Hallucination, reliability, and the role of generative AI in science</a></p>
                <p><strong>Paper Abstract:</strong> Generative AI is increasingly used in scientific domains, from protein folding to climate modeling. But these models produce distinctive errors known as hallucinations - outputs that are incorrect yet superficially plausible. Worse, some arguments suggest that hallucinations are an inevitable consequence of the mechanisms underlying generative inference. Fortunately, such arguments rely on a conception of hallucination defined solely with respect to internal properties of the model, rather than in reference to the empirical target system. This conception fails to distinguish epistemically benign errors from those that threaten scientific inference. I introduce the concept of corrosive hallucination to capture the epistemically troubling subclass: misrepresentations that are substantively misleading and resistant to systematic anticipation. I argue that although corrosive hallucinations do pose a threat to scientific reliability, they are not inevitable. Scientific workflows such as those surrounding AlphaFold and GenCast, both of which serve as case studies, can neutralize their effects by imposing theoretical constraints during training, and by strategically screening for errors at inference time. When embedded in such workflows, generative AI can reliably contribute to scientific knowledge.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2062.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2062.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaFold 3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaFold 3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-augmented protein structure prediction system that generates 3D molecular structures (including protein–small-molecule interactions) and provides residue-level confidence scores (pLDDT) to screen outputs for reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accurate structure prediction of biomolecular interactions with AlphaFold 3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaFold 3</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative model (diffusion module integrated into deep neural network)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>structural biology / drug discovery (protein folding and biomolecular interactions)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>3D molecular structures and predicted protein–ligand interactions (atomic coordinates, conformational ensembles)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel to highly novel (can extrapolate beyond training PDB examples to novel interactions and conformations)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>diffusion-based generative sampling conditioned on sequence and structural context, with architecture-specific modules and stochastic sampling to produce multiple structure predictions</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>comparison to experimental ground truth (X-ray, cryo-EM, NMR) for calibration during training; at inference time uses internal ensemble agreement (pLDDT) and experimental follow-up by human scientists</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported qualitatively as state-of-the-art for predicting biomolecular interactions; paper cites AlphaFold's strong predictive success on benchmarks and rapid adoption in life sciences (no specific numeric generation success rates reported in the text)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>pLDDT scores have been shown in cited work to correlate robustly with experimental validation across many cases; Brotzakis et al. (2025) report AlphaFold 3 outperforms specialized tools in detecting intrinsically disordered regions (qualitative claim in text; numeric rates not given here)</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically in this paper; text documents instances where AlphaFold generated detailed structures 'where none exist' (Abramson et al., 2024) but does not report an overall FPR; pLDDT is used to reduce accepting such outputs</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically; pLDDT may mark legitimate but low-confidence regions as unreliable (no numeric FNR provided)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Validation reliability (pLDDT correlation with experiment) holds across many cases, but the paper emphasizes that novel / poorly constrained regions (e.g., intrinsically disordered regions) receive lower confidence and are thus less reliable; calibration degrades qualitatively as structural ambiguity increases</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper emphasizes an asymmetry: AlphaFold can generate novel, high-detail structures but validation depends on independent empirical tests; internal confidence (pLDDT) mitigates the gap by flagging regions where generation is unreliable, permitting extrapolation while limiting unjustified belief.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes — pLDDT: a learned residue-level confidence score produced by a head trained to predict expected deviation from true interatomic distances; also uses ensemble/stochastic sampling and local agreement as uncertainty signal.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Described as well-calibrated in practice: 'pLDDT values have been shown to track subsequent experimental validation with remarkable consistency' (no numeric calibration metrics provided in text).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>AlphaFold's architecture and physics-informed loss reduce catastrophic failures out-of-distribution; paper states it permits substantial departures from training distribution if robust across internal ensemble, but no numeric OOD metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses internal ensemble agreement and pLDDT (local agreement among stochastic samples) and violation-loss penalties (steric clashes, bond lengths, torsions) as proxy checks for physical plausibility rather than direct ground-truth access at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Recommended for important/novel predictions; frequency increases with novelty/low pLDDT (paper states experimental validation is main epistemic safeguard and that high-confidence outputs are treated as candidates for belief while low-confidence outputs warrant caution).</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>semi-formal / empirical (molecular biophysics grounded in established physical/chemical laws and curated structural databases like the PDB)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Theory-guided training (physics/chemistry-informed violation loss functions), cross-distillation from simpler interpretable models, and confidence-based error screening using pLDDT and ensemble agreement; these are presented as effective at neutralizing corrosive hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Instances cited where AlphaFold generated detailed structures where none exist (Abramson et al., 2024), illustrating generation can outpace validation if unchecked; emphasis that opaque DNNs can produce plausible but incorrect outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>pLDDT and physics-informed training reduce the generation-validation gap by reliably flagging unreliable outputs and by embedding independent physical constraints, and Brotzakis et al. (2025) show improved detection of intrinsically disordered regions.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>not specified numerically; inference generation is computationally intensive (stochastic ensemble sampling) and experimental validation is far more costly and time-consuming (lab experiments), implying validation >> generation in cost and time but no exact ratio provided</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2062.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2062.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GenCast</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GenCast</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based generative forecasting model trained on ERA5 reanalysis that produces probabilistic meteorological forecasts via stochastic sampling and ensemble generation, using physics-informed loss functions and ensemble spread as an uncertainty signal.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Probabilistic weather forecasting with machine learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GenCast</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative model (diffusion-based ensemble forecasting)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>meteorology / weather forecasting</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>probabilistic weather forecasts (spatiotemporal meteorological fields over a grid)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>moderately novel (learns trajectories and can produce plausible forecasts beyond single deterministic simulations; operates within distribution defined by ERA5 but can interpolate/extrapolate into rare events)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>diffusion-based generative sampling conditioned on learned meteorological structure from ERA5 reanalysis; generates ensembles by varying random seeds at inference</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Calibration and verification against observations and standard forecasting metrics; ensemble-based evaluation using spread-skill ratio (SSR) and comparison to ECMWF operational forecasts; continuous calibration against observed outcomes</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Reported to 'match or exceed the performance of ECMWF's gold-standard system on several standard forecasting metrics' (Price et al., 2024) — specific metric values not provided in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Described as well-calibrated: ensemble spread-skill ratio remained close to 1 across forecast horizons examined, indicating good alignment between predicted uncertainty and observed forecast error (no numeric SSR values provided).</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically; no explicit FPR values given. The ensemble dispersion is used to detect unstable / potentially hallucinatory features (outliers across runs).</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Ensemble spread reveals epistemic instability: novel/rare events show larger dispersion across runs, signalling lower reliability; SSR close to 1 suggests calibration holds across horizons tested, but behavior for extreme out-of-distribution events (transformational discoveries) not quantified numerically here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper highlights a generation-validation asymmetry: GenCast can generate many plausible trajectories efficiently, but validation relies on ensemble calibration and comparison to observations; ensemble-based uncertainty reduces gap by revealing where forecasts are unstable.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes — uncertainty communicated via stochastic ensembles and measured via spread; internal ensemble variance used as proxy for forecast uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Characterized as good: spread-skill ratio near 1 across horizons indicating neither strong overconfidence nor underconfidence (no precise numeric calibration scores provided).</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not specified numerically; paper notes the need for very large ensembles in traditional NWP to cover low-frequency high-impact tail events, and suggests GenCast is computationally efficient alternative but does not provide explicit OOD metrics for extreme rare events in the text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses ensemble spread (spread-skill ratio), ensemble recurrence of features, and physics-informed loss penalties (e.g., to enforce non-negative humidity, conservation laws) as proxies rather than direct formal proof of correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Operational validation and continuous calibration against observations; for high-impact or novel tail-event forecasts, human oversight and additional verification are implied to be required more frequently.</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical but physics-constrained (meteorology uses formal fluid dynamics, conservation laws, and reanalysis datasets that embed physical constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Train on physically coherent ERA5 reanalysis; incorporate physics-informed loss functions to penalize violations (e.g., negative humidity); produce stochastic ensembles to reveal epistemic instability; use spread-skill ratio to calibrate uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Generative models can produce locally plausible but globally incoherent forecasts (earlier ML models sometimes predicted negative humidity or unrealistic gradients), indicating potential for generation to outstrip validation without constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>GenCast's ensemble calibration (SSR ≈ 1) and competitive performance relative to ECMWF suggest generation and validation capabilities can be brought into alignment via physics-informed training and ensemble uncertainty quantification.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>not provided numerically; paper implies GenCast is computationally cheaper than large NWP ensembles (generation cheaper) while validation (verification against observations and operational use) remains costly but less so than full NWP; no explicit ratio given</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2062.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2062.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aithal et al. diffusion study</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Understanding Hallucinations in Diffusion Models through Mode Interpolation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical study analyzing hallucination in diffusion models using synthetic datasets with known manifold structure, defining hallucination as samples in low estimated density regions and proposing a density/variance-based filter to remove them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Understanding Hallucinations in Diffusion Models through Mode Interpolation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Diffusion models (study subject)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>generative diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>machine learning / generative modeling (methodology study applicable to scientific generative models)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>synthetic high-dimensional samples (images, grid-like structures) used to probe model behavior</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>N/A (study focused on behavior on synthetic datasets; hallucinations defined as out-of-distribution/inter-mode samples)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>diffusion denoising process learned to approximate score functions and reconstruct samples from noisy inputs; sampling explores latent space and can interpolate between modes</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Evaluate samples against known ground-truth generative principles of synthetic datasets; use model-estimated density proxy via variance of predicted x0 to identify low-density (hallucinatory) samples and filter them</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>On synthetic datasets, unfiltered diffusion models produced interpolations in low-density inter-mode regions (hallucinations); generation quality quantified by fraction of samples falling into H_epsilon(q) — specific numeric rates other than the filtering claim are not given in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Filtering method reported to remove 95% of hallucinatory samples while preserving the vast majority of in-distribution outputs (per Aithal et al.), indicating high validation (filtering) effectiveness on these synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>Not reported as a formal FPR; authors state the filter 'preserves the vast majority' of in-distribution outputs — implying low false-positive discard rate, but no numeric FPR provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>Not reported numerically; claim of removing 95% of hallucinatory samples indicates remaining 5% of hallucinations persist (approximate residual false negatives), but exact definitions and rates depend on threshold epsilon and proxy quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Filtering approach is conservative: it treats deviations from training modes as hallucinations, so it will increasingly label novel (potentially scientific discoveries) outputs as hallucinations and filter them out; thus validation performance (ability to preserve true discoveries) degrades as novelty increases.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Study exposes a tension: generation can produce inter-manifold (novel) samples that the proposed validation filter discards; paper argues this validation approach conflates novel discoveries with hallucinations in scientific settings.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Yes — uses variance of the model's predicted x0 as a proxy for density/uncertainty to detect low-density (hallucinatory) samples.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>On synthetic benchmarks, proxy-based filtering achieved high removal of designated hallucinations (95%) while preserving in-distribution samples; calibration depends on threshold epsilon which must be tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Filtering performs poorly for scientific aims where OOD samples may encode novel structure — the method tends to discard OOD (out-of-distribution) outputs even if they are scientifically meaningful.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses model-estimated density proxy (variance of x0 predictions) to approximate whether a sample lies in a high-density region; samples below threshold epsilon are labeled hallucinations and discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>By design the filter removes many candidate novel outputs, implying human adjudication would be required to recover discarded novel but valid discoveries (frequency depends on threshold settings; not numerically specified).</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>methodological / synthetic experiments (not tied to a specific empirical domain; implications extrapolated to scientific models)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Proposes density/variance-based filtering to discard low-density (hallucinatory) samples; authors claim 95% removal of hallucinatory samples on synthetic tests, but the paper critiqued this as too conservative for scientific discovery contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Demonstrates that diffusion models interpolate across gaps and produce inter-manifold samples (hallucinations), supporting theoretical inevitability arguments that generative models will sample unsupported regions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>The study's mitigation (filtering) improves apparent validation by removing many hallucinations on synthetic tasks, but authors and the current paper note this can remove legitimate novel outputs in scientific settings.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>not provided; filters require additional computation to estimate variance/density proxies per sample but no explicit cost ratio between generation and validation is given</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2062.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2062.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate novel outputs (scientific hypotheses, molecules, proofs, predictions) and how those outputs are validated, including performance metrics, false positive rates, and differences between generation and validation capabilities.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Virtual Labs (Swanson et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A proposed multi-agent system where LLMs generate hypotheses which are critiqued by other LLMs and refined, and then routed to domain-specific tools (e.g., Rosetta, AlphaFold) for predictions and ultimately to human experimentalists for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Virtual Labs (multi-agent pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>pipeline of LLM agents + domain-specific predictive tools + experimental validation (multi-component automated discovery system)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>drug discovery / protein engineering (example: nanobody design)</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>hypotheses, designed sequences/molecules, predicted structures, experimental candidates</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_level</strong></td>
                            <td>potentially highly novel (framework intended to generate novel molecular designs and hypotheses beyond known data)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>LLM-based hypothesis generation and iterative critique (multi-agent refinement), combined with domain-specific predictors (AlphaFold, Rosetta) to produce candidate molecules/structures</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Domain-specific predictive tools for pre-screening (AlphaFold, Rosetta) followed by human experimental testing (wet-lab validation) as final arbiter</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Preliminary / early-stage (paper cited as preliminary and not yet peer-reviewed); specific generation success rates not reported in this text.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Experimental validation is part of the pipeline (title indicates experimental validation occurred for SARS-CoV-2 nanobodies in preliminary work), but exact validation metrics are not provided in this paper's summary.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td>not reported numerically in this summary; pipeline explicitly uses downstream experimental validation to eliminate false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td>not reported numerically.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_novelty</strong></td>
                            <td>Framework is designed to handle novelty by social/competitive critique and domain-tool verification, but no quantitative relationship reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_comparison</strong></td>
                            <td>Paper posits an architecture to close the generation-validation gap by chaining hypothesis generation with domain-specific prediction and human experiments, explicitly acknowledging generation may exceed immediate validation capacity and addressing it by routing to experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>Not detailed in this paper's summary; uncertainty handled via ensemble/domain-tool predictions and human oversight in downstream stages.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified here; system aims to surface novel designs that will then be empirically tested, so OOD handling depends on downstream domain tools and experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_proxy_metrics</strong></td>
                            <td>Yes — uses domain-specific predictors (Rosetta, AlphaFold) as intermediate proxies before experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_frequency</strong></td>
                            <td>Integral to pipeline: experimentalists perform validation on selected candidates; frequency depends on filtering and resource constraints (not numerically specified).</td>
                        </tr>
                        <tr>
                            <td><strong>formal_verification_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_formalization_level</strong></td>
                            <td>empirical / experimental (drug design and protein engineering rely on lab experiments and domain-specific simulation tools)</td>
                        </tr>
                        <tr>
                            <td><strong>gap_mitigation_strategies</strong></td>
                            <td>Multi-agent critique, routing to domain-specific predictive models, and experimental validation; social/competitive dynamics among agents intended to provide error-correction at scale.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_supporting_gap</strong></td>
                            <td>Motivation for the framework acknowledges that LLM-driven hypothesis generation can outpace direct validation capacity, necessitating pipelines to triage and empirically test candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_contradicting_gap</strong></td>
                            <td>Preliminary experimental validations reported in the cited work (title) suggest the pipeline can produce experimentally testable and validated candidates, indicating the gap can be bridged with integrated workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_ratio</strong></td>
                            <td>not provided; pipeline implies computational generation is cheaper than wet-lab validation, which remains the dominant cost, but no numeric ratio given</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Accurate structure prediction of biomolecular interactions with AlphaFold 3 <em>(Rating: 2)</em></li>
                <li>Probabilistic weather forecasting with machine learning <em>(Rating: 2)</em></li>
                <li>Understanding Hallucinations in Diffusion Models through Mode Interpolation <em>(Rating: 2)</em></li>
                <li>AlphaFold prediction of structural ensembles of disordered proteins <em>(Rating: 1)</em></li>
                <li>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation <em>(Rating: 1)</em></li>
                <li>Deep Generative Adversarial Networks: Applications in Musculoskeletal Imaging <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2062",
    "paper_id": "paper-277741423",
    "extraction_schema_id": "extraction-schema-53",
    "extracted_data": [
        {
            "name_short": "AlphaFold 3",
            "name_full": "AlphaFold 3",
            "brief_description": "A diffusion-augmented protein structure prediction system that generates 3D molecular structures (including protein–small-molecule interactions) and provides residue-level confidence scores (pLDDT) to screen outputs for reliability.",
            "citation_title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
            "mention_or_use": "use",
            "system_name": "AlphaFold 3",
            "system_type": "generative model (diffusion module integrated into deep neural network)",
            "scientific_domain": "structural biology / drug discovery (protein folding and biomolecular interactions)",
            "output_type": "3D molecular structures and predicted protein–ligand interactions (atomic coordinates, conformational ensembles)",
            "novelty_level": "moderately novel to highly novel (can extrapolate beyond training PDB examples to novel interactions and conformations)",
            "generation_method": "diffusion-based generative sampling conditioned on sequence and structural context, with architecture-specific modules and stochastic sampling to produce multiple structure predictions",
            "validation_method": "comparison to experimental ground truth (X-ray, cryo-EM, NMR) for calibration during training; at inference time uses internal ensemble agreement (pLDDT) and experimental follow-up by human scientists",
            "generation_performance": "Reported qualitatively as state-of-the-art for predicting biomolecular interactions; paper cites AlphaFold's strong predictive success on benchmarks and rapid adoption in life sciences (no specific numeric generation success rates reported in the text)",
            "validation_performance": "pLDDT scores have been shown in cited work to correlate robustly with experimental validation across many cases; Brotzakis et al. (2025) report AlphaFold 3 outperforms specialized tools in detecting intrinsically disordered regions (qualitative claim in text; numeric rates not given here)",
            "false_positive_rate": "not reported numerically in this paper; text documents instances where AlphaFold generated detailed structures 'where none exist' (Abramson et al., 2024) but does not report an overall FPR; pLDDT is used to reduce accepting such outputs",
            "false_negative_rate": "not reported numerically; pLDDT may mark legitimate but low-confidence regions as unreliable (no numeric FNR provided)",
            "performance_vs_novelty": "Validation reliability (pLDDT correlation with experiment) holds across many cases, but the paper emphasizes that novel / poorly constrained regions (e.g., intrinsically disordered regions) receive lower confidence and are thus less reliable; calibration degrades qualitatively as structural ambiguity increases",
            "generation_validation_comparison": "Paper emphasizes an asymmetry: AlphaFold can generate novel, high-detail structures but validation depends on independent empirical tests; internal confidence (pLDDT) mitigates the gap by flagging regions where generation is unreliable, permitting extrapolation while limiting unjustified belief.",
            "uncertainty_quantification": "Yes — pLDDT: a learned residue-level confidence score produced by a head trained to predict expected deviation from true interatomic distances; also uses ensemble/stochastic sampling and local agreement as uncertainty signal.",
            "calibration_quality": "Described as well-calibrated in practice: 'pLDDT values have been shown to track subsequent experimental validation with remarkable consistency' (no numeric calibration metrics provided in text).",
            "out_of_distribution_performance": "AlphaFold's architecture and physics-informed loss reduce catastrophic failures out-of-distribution; paper states it permits substantial departures from training distribution if robust across internal ensemble, but no numeric OOD metrics reported.",
            "validation_proxy_metrics": "Yes — uses internal ensemble agreement and pLDDT (local agreement among stochastic samples) and violation-loss penalties (steric clashes, bond lengths, torsions) as proxy checks for physical plausibility rather than direct ground-truth access at inference.",
            "human_validation_required": true,
            "human_validation_frequency": "Recommended for important/novel predictions; frequency increases with novelty/low pLDDT (paper states experimental validation is main epistemic safeguard and that high-confidence outputs are treated as candidates for belief while low-confidence outputs warrant caution).",
            "formal_verification_used": false,
            "domain_formalization_level": "semi-formal / empirical (molecular biophysics grounded in established physical/chemical laws and curated structural databases like the PDB)",
            "gap_mitigation_strategies": "Theory-guided training (physics/chemistry-informed violation loss functions), cross-distillation from simpler interpretable models, and confidence-based error screening using pLDDT and ensemble agreement; these are presented as effective at neutralizing corrosive hallucinations.",
            "evidence_supporting_gap": "Instances cited where AlphaFold generated detailed structures where none exist (Abramson et al., 2024), illustrating generation can outpace validation if unchecked; emphasis that opaque DNNs can produce plausible but incorrect outputs.",
            "evidence_contradicting_gap": "pLDDT and physics-informed training reduce the generation-validation gap by reliably flagging unreliable outputs and by embedding independent physical constraints, and Brotzakis et al. (2025) show improved detection of intrinsically disordered regions.",
            "computational_cost_ratio": "not specified numerically; inference generation is computationally intensive (stochastic ensemble sampling) and experimental validation is far more costly and time-consuming (lab experiments), implying validation &gt;&gt; generation in cost and time but no exact ratio provided",
            "uuid": "e2062.0"
        },
        {
            "name_short": "GenCast",
            "name_full": "GenCast",
            "brief_description": "A diffusion-based generative forecasting model trained on ERA5 reanalysis that produces probabilistic meteorological forecasts via stochastic sampling and ensemble generation, using physics-informed loss functions and ensemble spread as an uncertainty signal.",
            "citation_title": "Probabilistic weather forecasting with machine learning",
            "mention_or_use": "use",
            "system_name": "GenCast",
            "system_type": "generative model (diffusion-based ensemble forecasting)",
            "scientific_domain": "meteorology / weather forecasting",
            "output_type": "probabilistic weather forecasts (spatiotemporal meteorological fields over a grid)",
            "novelty_level": "moderately novel (learns trajectories and can produce plausible forecasts beyond single deterministic simulations; operates within distribution defined by ERA5 but can interpolate/extrapolate into rare events)",
            "generation_method": "diffusion-based generative sampling conditioned on learned meteorological structure from ERA5 reanalysis; generates ensembles by varying random seeds at inference",
            "validation_method": "Calibration and verification against observations and standard forecasting metrics; ensemble-based evaluation using spread-skill ratio (SSR) and comparison to ECMWF operational forecasts; continuous calibration against observed outcomes",
            "generation_performance": "Reported to 'match or exceed the performance of ECMWF's gold-standard system on several standard forecasting metrics' (Price et al., 2024) — specific metric values not provided in this text.",
            "validation_performance": "Described as well-calibrated: ensemble spread-skill ratio remained close to 1 across forecast horizons examined, indicating good alignment between predicted uncertainty and observed forecast error (no numeric SSR values provided).",
            "false_positive_rate": "not reported numerically; no explicit FPR values given. The ensemble dispersion is used to detect unstable / potentially hallucinatory features (outliers across runs).",
            "false_negative_rate": "not reported numerically.",
            "performance_vs_novelty": "Ensemble spread reveals epistemic instability: novel/rare events show larger dispersion across runs, signalling lower reliability; SSR close to 1 suggests calibration holds across horizons tested, but behavior for extreme out-of-distribution events (transformational discoveries) not quantified numerically here.",
            "generation_validation_comparison": "Paper highlights a generation-validation asymmetry: GenCast can generate many plausible trajectories efficiently, but validation relies on ensemble calibration and comparison to observations; ensemble-based uncertainty reduces gap by revealing where forecasts are unstable.",
            "uncertainty_quantification": "Yes — uncertainty communicated via stochastic ensembles and measured via spread; internal ensemble variance used as proxy for forecast uncertainty.",
            "calibration_quality": "Characterized as good: spread-skill ratio near 1 across horizons indicating neither strong overconfidence nor underconfidence (no precise numeric calibration scores provided).",
            "out_of_distribution_performance": "Not specified numerically; paper notes the need for very large ensembles in traditional NWP to cover low-frequency high-impact tail events, and suggests GenCast is computationally efficient alternative but does not provide explicit OOD metrics for extreme rare events in the text.",
            "validation_proxy_metrics": "Yes — uses ensemble spread (spread-skill ratio), ensemble recurrence of features, and physics-informed loss penalties (e.g., to enforce non-negative humidity, conservation laws) as proxies rather than direct formal proof of correctness.",
            "human_validation_required": true,
            "human_validation_frequency": "Operational validation and continuous calibration against observations; for high-impact or novel tail-event forecasts, human oversight and additional verification are implied to be required more frequently.",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical but physics-constrained (meteorology uses formal fluid dynamics, conservation laws, and reanalysis datasets that embed physical constraints)",
            "gap_mitigation_strategies": "Train on physically coherent ERA5 reanalysis; incorporate physics-informed loss functions to penalize violations (e.g., negative humidity); produce stochastic ensembles to reveal epistemic instability; use spread-skill ratio to calibrate uncertainty.",
            "evidence_supporting_gap": "Generative models can produce locally plausible but globally incoherent forecasts (earlier ML models sometimes predicted negative humidity or unrealistic gradients), indicating potential for generation to outstrip validation without constraints.",
            "evidence_contradicting_gap": "GenCast's ensemble calibration (SSR ≈ 1) and competitive performance relative to ECMWF suggest generation and validation capabilities can be brought into alignment via physics-informed training and ensemble uncertainty quantification.",
            "computational_cost_ratio": "not provided numerically; paper implies GenCast is computationally cheaper than large NWP ensembles (generation cheaper) while validation (verification against observations and operational use) remains costly but less so than full NWP; no explicit ratio given",
            "uuid": "e2062.1"
        },
        {
            "name_short": "Aithal et al. diffusion study",
            "name_full": "Understanding Hallucinations in Diffusion Models through Mode Interpolation",
            "brief_description": "An empirical study analyzing hallucination in diffusion models using synthetic datasets with known manifold structure, defining hallucination as samples in low estimated density regions and proposing a density/variance-based filter to remove them.",
            "citation_title": "Understanding Hallucinations in Diffusion Models through Mode Interpolation",
            "mention_or_use": "mention",
            "system_name": "Diffusion models (study subject)",
            "system_type": "generative diffusion models",
            "scientific_domain": "machine learning / generative modeling (methodology study applicable to scientific generative models)",
            "output_type": "synthetic high-dimensional samples (images, grid-like structures) used to probe model behavior",
            "novelty_level": "N/A (study focused on behavior on synthetic datasets; hallucinations defined as out-of-distribution/inter-mode samples)",
            "generation_method": "diffusion denoising process learned to approximate score functions and reconstruct samples from noisy inputs; sampling explores latent space and can interpolate between modes",
            "validation_method": "Evaluate samples against known ground-truth generative principles of synthetic datasets; use model-estimated density proxy via variance of predicted x0 to identify low-density (hallucinatory) samples and filter them",
            "generation_performance": "On synthetic datasets, unfiltered diffusion models produced interpolations in low-density inter-mode regions (hallucinations); generation quality quantified by fraction of samples falling into H_epsilon(q) — specific numeric rates other than the filtering claim are not given in this paper's summary.",
            "validation_performance": "Filtering method reported to remove 95% of hallucinatory samples while preserving the vast majority of in-distribution outputs (per Aithal et al.), indicating high validation (filtering) effectiveness on these synthetic tasks.",
            "false_positive_rate": "Not reported as a formal FPR; authors state the filter 'preserves the vast majority' of in-distribution outputs — implying low false-positive discard rate, but no numeric FPR provided here.",
            "false_negative_rate": "Not reported numerically; claim of removing 95% of hallucinatory samples indicates remaining 5% of hallucinations persist (approximate residual false negatives), but exact definitions and rates depend on threshold epsilon and proxy quality.",
            "performance_vs_novelty": "Filtering approach is conservative: it treats deviations from training modes as hallucinations, so it will increasingly label novel (potentially scientific discoveries) outputs as hallucinations and filter them out; thus validation performance (ability to preserve true discoveries) degrades as novelty increases.",
            "generation_validation_comparison": "Study exposes a tension: generation can produce inter-manifold (novel) samples that the proposed validation filter discards; paper argues this validation approach conflates novel discoveries with hallucinations in scientific settings.",
            "uncertainty_quantification": "Yes — uses variance of the model's predicted x0 as a proxy for density/uncertainty to detect low-density (hallucinatory) samples.",
            "calibration_quality": "On synthetic benchmarks, proxy-based filtering achieved high removal of designated hallucinations (95%) while preserving in-distribution samples; calibration depends on threshold epsilon which must be tuned.",
            "out_of_distribution_performance": "Filtering performs poorly for scientific aims where OOD samples may encode novel structure — the method tends to discard OOD (out-of-distribution) outputs even if they are scientifically meaningful.",
            "validation_proxy_metrics": "Yes — uses model-estimated density proxy (variance of x0 predictions) to approximate whether a sample lies in a high-density region; samples below threshold epsilon are labeled hallucinations and discarded.",
            "human_validation_required": true,
            "human_validation_frequency": "By design the filter removes many candidate novel outputs, implying human adjudication would be required to recover discarded novel but valid discoveries (frequency depends on threshold settings; not numerically specified).",
            "formal_verification_used": false,
            "domain_formalization_level": "methodological / synthetic experiments (not tied to a specific empirical domain; implications extrapolated to scientific models)",
            "gap_mitigation_strategies": "Proposes density/variance-based filtering to discard low-density (hallucinatory) samples; authors claim 95% removal of hallucinatory samples on synthetic tests, but the paper critiqued this as too conservative for scientific discovery contexts.",
            "evidence_supporting_gap": "Demonstrates that diffusion models interpolate across gaps and produce inter-manifold samples (hallucinations), supporting theoretical inevitability arguments that generative models will sample unsupported regions.",
            "evidence_contradicting_gap": "The study's mitigation (filtering) improves apparent validation by removing many hallucinations on synthetic tasks, but authors and the current paper note this can remove legitimate novel outputs in scientific settings.",
            "computational_cost_ratio": "not provided; filters require additional computation to estimate variance/density proxies per sample but no explicit cost ratio between generation and validation is given",
            "uuid": "e2062.2"
        },
        {
            "name_short": "Virtual Labs (Swanson et al.)",
            "name_full": "The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation",
            "brief_description": "A proposed multi-agent system where LLMs generate hypotheses which are critiqued by other LLMs and refined, and then routed to domain-specific tools (e.g., Rosetta, AlphaFold) for predictions and ultimately to human experimentalists for validation.",
            "citation_title": "The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation",
            "mention_or_use": "mention",
            "system_name": "Virtual Labs (multi-agent pipeline)",
            "system_type": "pipeline of LLM agents + domain-specific predictive tools + experimental validation (multi-component automated discovery system)",
            "scientific_domain": "drug discovery / protein engineering (example: nanobody design)",
            "output_type": "hypotheses, designed sequences/molecules, predicted structures, experimental candidates",
            "novelty_level": "potentially highly novel (framework intended to generate novel molecular designs and hypotheses beyond known data)",
            "generation_method": "LLM-based hypothesis generation and iterative critique (multi-agent refinement), combined with domain-specific predictors (AlphaFold, Rosetta) to produce candidate molecules/structures",
            "validation_method": "Domain-specific predictive tools for pre-screening (AlphaFold, Rosetta) followed by human experimental testing (wet-lab validation) as final arbiter",
            "generation_performance": "Preliminary / early-stage (paper cited as preliminary and not yet peer-reviewed); specific generation success rates not reported in this text.",
            "validation_performance": "Experimental validation is part of the pipeline (title indicates experimental validation occurred for SARS-CoV-2 nanobodies in preliminary work), but exact validation metrics are not provided in this paper's summary.",
            "false_positive_rate": "not reported numerically in this summary; pipeline explicitly uses downstream experimental validation to eliminate false positives.",
            "false_negative_rate": "not reported numerically.",
            "performance_vs_novelty": "Framework is designed to handle novelty by social/competitive critique and domain-tool verification, but no quantitative relationship reported here.",
            "generation_validation_comparison": "Paper posits an architecture to close the generation-validation gap by chaining hypothesis generation with domain-specific prediction and human experiments, explicitly acknowledging generation may exceed immediate validation capacity and addressing it by routing to experimental testing.",
            "uncertainty_quantification": "Not detailed in this paper's summary; uncertainty handled via ensemble/domain-tool predictions and human oversight in downstream stages.",
            "calibration_quality": "Not reported.",
            "out_of_distribution_performance": "Not quantified here; system aims to surface novel designs that will then be empirically tested, so OOD handling depends on downstream domain tools and experiments.",
            "validation_proxy_metrics": "Yes — uses domain-specific predictors (Rosetta, AlphaFold) as intermediate proxies before experimental testing.",
            "human_validation_required": true,
            "human_validation_frequency": "Integral to pipeline: experimentalists perform validation on selected candidates; frequency depends on filtering and resource constraints (not numerically specified).",
            "formal_verification_used": false,
            "domain_formalization_level": "empirical / experimental (drug design and protein engineering rely on lab experiments and domain-specific simulation tools)",
            "gap_mitigation_strategies": "Multi-agent critique, routing to domain-specific predictive models, and experimental validation; social/competitive dynamics among agents intended to provide error-correction at scale.",
            "evidence_supporting_gap": "Motivation for the framework acknowledges that LLM-driven hypothesis generation can outpace direct validation capacity, necessitating pipelines to triage and empirically test candidates.",
            "evidence_contradicting_gap": "Preliminary experimental validations reported in the cited work (title) suggest the pipeline can produce experimentally testable and validated candidates, indicating the gap can be bridged with integrated workflows.",
            "computational_cost_ratio": "not provided; pipeline implies computational generation is cheaper than wet-lab validation, which remains the dominant cost, but no numeric ratio given",
            "uuid": "e2062.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Accurate structure prediction of biomolecular interactions with AlphaFold 3",
            "rating": 2
        },
        {
            "paper_title": "Probabilistic weather forecasting with machine learning",
            "rating": 2
        },
        {
            "paper_title": "Understanding Hallucinations in Diffusion Models through Mode Interpolation",
            "rating": 2
        },
        {
            "paper_title": "AlphaFold prediction of structural ensembles of disordered proteins",
            "rating": 1
        },
        {
            "paper_title": "The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation",
            "rating": 1
        },
        {
            "paper_title": "Deep Generative Adversarial Networks: Applications in Musculoskeletal Imaging",
            "rating": 1
        }
    ],
    "cost": 0.016031499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Hallucination, reliability, and the role of generative AI in science
11 Apr 2025</p>
<p>Charles Rathkopf c.rathkopf@fz-juelich.de 
Institute for Brain
Forschungszentrum Jülich</p>
<p>John Dewey 
Institute for Brain
Forschungszentrum Jülich</p>
<p>Hallucination, reliability, and the role of generative AI in science
11 Apr 2025D0A69F24CC75353F3262E9BAC8A2BF98arXiv:2504.08526v1[cs.CY]generative AIhallucination in AIcorrosive hallucinationscientific AIAI reliabilityreliabilist epistemologyAI epistemology
Generative AI is increasingly used in scientific domains, from protein folding to climate modeling.But these models produce distinctive errors known as "hallucinations"-outputs that are incorrect yet superficially plausible.Worse, some arguments suggest that hallucinations are an inevitable consequence of the mechanisms underlying generative inference.Fortunately, such arguments rely on a conception of hallucination defined solely with respect to internal properties of the model, rather than in reference to the empirical target system.This conception fails to distinguish epistemically benign errors from those that threaten scientific inference.I introduce the concept of corrosive hallucination to capture the epistemically troubling subclass: misrepresentations that are substantively misleading and resistant to systematic anticipation.I argue that although corrosive hallucinations do pose a threat to scientific reliability, they are not inevitable.Scientific workflows such as those surrounding AlphaFold and GenCast, both of which serve as case studies, can neutralize their effects by imposing theoretical constraints during training, and by strategically screening for errors at inference time.When embedded in such workflows, generative AI can reliably contribute to scientific knowledge.</p>
<p>Hallucination as a threat to reliability</p>
<p>In recent years, generative AI has become deeply embedded in scientific practice.It is now used to synthesize data for climate models [Kadow et al., 2020], to map phase transitions in novel materials [Arnold et al., 2024], and to predict molecular interactions for drug discovery [Sidhom et al., 2022].</p>
<p>Unlike classificatory AI models, generative AI models produce outputs that are highly detailed and informationally rich.That richness makes them epistemically valuable, but also leaves them susceptible to a new kind of error that has come to be known as "hallucination" [Ji et al., 2023;Sun et al., 2024].</p>
<p>There is considerable confusion about what hallucinations are.The term itself invites a misleading comparison to human perceptual hallucinations.Generative AI models do not consciously perceive the world, let alone misperceive it.Nevertheless, the real problem lies not with the word, but with the errors themselves.Since the term is already widely used, and since insisting on a replacement would only introduce a cognitively costly neologism into an already difficult discussion, I will just stick with it.</p>
<p>As a first pass, hallucinations can be characterized as errors that are not merely inherited from the training data, but are, in some sense, produced by the model itself.This claim is vague, but also revealing.After reading it, your first thought might well be that any model prone to hallucination ought not to be trusted.And indeed, the epistemic risks introduced by hallucinations are serious.</p>
<p>AlphaFold 3, among the most celebrated generative AI models in the natural sciences, has been shown to generate detailed molecular structures where none exist [Abramson et al., 2024].Generative adversarial networks (GANs), widely used in medical imaging, have been found to introduce phantom anomalies-a fracture-like line appearing in an unbroken bone, or a lesion in what is, in fact, perfectly healthy tissue [Shin et al., 2021].These are not just rounding errors.Undetected, hallucinations can lead researchers and clinicians toward serious mistakes in inference and decision-making.</p>
<p>In fact, the epistemic challenges posed by hallucinations run deeper than these examples suggest.</p>
<p>There is reason to think that hallucinations are inevitable byproducts of the mechanisms of generative inference.The intuition behind this claim is that training such models involves a fundamental tradeoff between novelty and reliability [Sajjadi et al., 2018;Sinha et al., 2023;Xu et al., 2024].A model constrained to strictly mirror its training data may be reliable but incapable of generating novel insights.Allowing a model to extrapolate, by contrast, enables novelty but invites fabrication.</p>
<p>Another reason that hallucinations threaten reliability is that they are sometimes difficult to detect.[Ji et al., 2023;Bubeck et al., 2023].This is not always the case.When we have thorough background knowledge of the target phenomenon, hallucinations can be easy to spot.For example, earlier versions of DALL-E and Stable Diffusion often generated images of human hands with six fingers [Wang et al., 2024].But scientific AI operates at the frontiers of human knowledge, where error detection is intrinsically more difficult.Where our epistemic resources are weakest, errors are most likely to go undetected.And the longer they remain undetected, the more they threaten to derail any decision-making processes based on model outputs.</p>
<p>Drawing these observations together, it seems that hallucinations are, in at least some cases, substantive, inevitable, and difficult to detect.It is as if hallucinations were tailor-made-perhaps by a Cartesian demon-to undermine the reliability of scientific inference.</p>
<p>What can be done to mitigate the problem?That question is difficult to answer, in part because deep learning models are epistemically opaque [Humphreys, 2009;Creel, 2020].In traditional closed-form models, evidence of reliability is often grounded in knowledge of how parameters relate to the properties of the target system.In DNNs, by contrast, it is unclear whether individual parameters represent anything at all.This lack of interpretability makes it difficult to assess whether a model tracks the underlying structure of the world.As Sullivan [2022] emphasizes, genuine scientific understanding requires more than predictive accuracy: it requires that the model's internal representations align in some meaningful way with the causal structure of the target phenomenon.If we are to justify the use of opaque, DNN-based models, we must find alternative grounds for assessing their epistemic reliability.</p>
<p>If we do not fully understand the mechanisms behind generative AI, how might we justify its use?Reliabilist epistemology [Goldman, 1979;Lyons, 2019] offers a straightforward answer: we observe its track record.Instead of explaining why a model succeeds, we infer its reliability from past performance.This approach, which Duede [2023] calls brute inductivism, reduces scientific epistemology to an accounting exercise.Suppose a model achieves high accuracy on benchmarks or aligns well with historical data.Researchers then infer-perhaps naively-that the model will be reliable in future applications.But as Duede's unflattering label suggests, brute inductivism is an inherently precarious strategy.Past success offers no guarantee of future performance, particularly in novel settings [Grote et al., 2024].If trust in generative AI rests on nothing more than inductive bookkeeping, it risks collapsing as soon as the model is pushed beyond its training distribution.Despite these mutually reinforcing difficulties of hallucination and epistemic opacity, rejecting generative AI outright is not a serious option.These models have already demonstrated their ability to outperform traditional approaches in all sorts of important predictive tasks.The real challenge, then, is not whether generative AI should be used in science, but how it can be used responsibly.</p>
<p>Addressing this challenge requires greater clarity about what counts as a hallucination.The concept is often defined formally, and while formal definitions are always welcome, they sometimes obscure an important empirical distinction.In the case of hallucination, such definitions lump together epistemically threatening errors with those that are largely benign.To address the problem, I introduce a new concept, corrosive hallucination, which is specifically designed to isolate those hallucinations that pose a genuine threat to the reliability of science.It will turn out, however, that when we focus specifically on corrosive hallucinations, arguments for the inevitability of hallucination collapse.</p>
<p>Mitigation is within our grasp after all.</p>
<p>To illustrate this, I examine two case studies: AlphaFold 3, which predicts molecular structures, and GenCast, which generates probabilistic weather forecasts.These models operate in entirely different scientific domains-one at the scale of molecules, the other at the scale of planetary weather systems.Nevertheless, both mitigate hallucinations by embedding theoretical constraints and uncertainty management strategies directly into their modeling architectures.These principles do not eliminate model-generated error entirely, but they show how, despite the distinctive challenges posed by generative AI, such errors can be effectively managed.</p>
<p>Crucially, these design principles are neither automatic nor inevitable.They emerge from carefully managed scientific workflows, and their effectiveness depends on deliberate design and maintenance.</p>
<p>By articulating the rationale behind these strategies, I aim to clarify how generative AI can be integrated into scientific practice without unduly compromising reliability.</p>
<p>Generative AI and the inevitability of hallucination</p>
<p>What is generative AI?</p>
<p>The term generative AI is sometimes taken to refer to any AI system that mimics the cultural products of human creativity.While many generative models do exactly that, mimicking human output is just one of many ways these architectures can be deployed.They are also used to produce numerical, physical, and scientific data of all kinds.Here is a definition that is sufficiently abstract to capture this broader scope:</p>
<p>A generative AI model is a machine learning system trained to produce complex data structures that adhere to patterns learned from training data, while generalizing beyond the exact instances in that data.</p>
<p>The word "complex" is carrying a lot of weight.The complexity of model outputs plays a central role in understanding both why hallucinations are inevitable and why they pose a distinctive epistemic threat in scientific applications.Here, "complexity" refers to high dimensionality: model outputs are structured, multi-component entities rather than scalar values or discrete labels.In many generative models, output dimensionality is proportional-either strictly or approximately-to that of the exemplars in the training data.In some cases, such as GenCast, input and output have equal and fixed dimensionality by design, since both represent meteorological fields over a grid on the Earth's surface.In others, such as autoregressive language models, outputs may exceed the length or complexity of the inputs (e.g., "Write me an essay about the history of AI").Even then, they remain bounded by architectural constraints, such as context windows and maximum token length, and shaped by the complexity and scale of the training data.</p>
<p>In both kinds of case, the generative task involves producing plausible outputs in a high-dimensional space whose structure is incompletely determined by the training distribution.</p>
<p>Two contrasts help clarify what makes generative AI distinctive.First, unlike classification models, which learn a conditional distribution ( | ) over discrete labels  , generative models aim to learn the full distribution (), enabling them to produce novel samples that extend the distribution in coherent ways [Kingma and Welling, 2013;Goodfellow et al., 2014;Buckner, 2024].Second, unlike classical generative statistical models such as Poisson processes or Markov chains, which generate data from predefined parametric distributions [Grimmett and Stirzaker, 1992], generative AI models learn latent representations that capture complex, often idiosyncratic statistical structure [Rezende et al., 2014;Yang et al., 2023].This capacity makes them uniquely valuable for domains where explicit theory remains incomplete, such as materials science or drug discovery.</p>
<p>On the inevitability of hallucination</p>
<p>There is a growing literature on the inevitability of hallucination in large language models.Xu et al. [2025] appeal to no-free-lunch theorems, Banerjee and Jacob [2024] draw an analogy to Gödel's first incompleteness theorem, and Kalai and Vempala [2024] provide an information-theoretic lower bound on hallucination frequency.But these arguments focus on autoregressive architectures and do not transfer straightforwardly to the scientific models addressed in this paper.</p>
<p>Unlike autoregressive architectures, which are well suited to sequential data such as text, many scientific generative models are designed to preserve global coherence across high-dimensional structures.Diffusion models, in particular, generate outputs through a process of iterative refinement, progressively denoising a sample over multiple steps [Song et al., 2021].Variational autoencoders (VAEs) and generative adversarial networks (GANs), though architecturally distinct, pursue the same end: to reconstruct complex global structure from sparse data.These models are typically applied in domains where long-range dependencies span multiple spatial or structural dimensions-protein folding, weather dynamics, material synthesis.I focus on these architectures for two reasons.First, they have figured centrally in some of the most celebrated successes of generative AI in the natural sciences.</p>
<p>Second, because their outputs are not merely large but genuinely high-dimensional-structured across space, geometry, or topology-the detection of hallucination poses distinct challenges.Unlike language models, which produce long sequences of discrete tokens, these models (often) generate high-dimensional outputs, in which hallucination detection is intrinsically more difficult.In what follows, therefore, I borrow some ideas from inevitability arguments developed for language models to this broader class of scientific models.</p>
<p>One kind of argument is broadly information-theoretic.The idea is that generative AI models do not contain enough information to represent complex empirical distributions accurately.To see this, consider the size of the output space relative to the model's internal parameter space.Generative models operate in high-dimensional output spaces, with far more possible configurations than any dataset can sample faithfully.For example, a 12-megapixel image with 256 intensity levels per channel has 10 86,000,000 possible configurations.A 100-amino acid protein has 10 130 possible sequences, not counting conformational variants [Dryden et al., 2008].Meteorological states evolve across millions of interacting variables.Even the largest training sets cover only a vanishing fraction of these spaces.</p>
<p>Moreover, models compress these sparse samples into relatively small parameter sets.A protein diffusion model may train on a few hundred thousand examples, but must generalize across 10 100 possible sequences and conformations.This compression all but ensures that many outputs will be generated in regions where the training data provides little guidance.And where the training data provides little guidance, hallucination is inevitable.</p>
<p>A second argument is geometric.Generative models learn a mapping from high-dimensional data to latent representations and generate new outputs by sampling and decoding from this space.But in high-dimensional settings, geometric properties become unintuitive.As Arjovsky et al. [2017] note, real data typically lie on low-dimensional manifolds within a much larger ambient space.When generative models are trained on multiple such manifolds, interpolation in latent space can result in outputs that fall between those manifolds.These inter-manifold regions are unsupported by the training distribution.When a model samples from these regions, the result is a hallucination.</p>
<p>Both arguments suggest that generative models are destined to produce outputs that are, in some sense, wrong.But they say nothing about another property that is both commonly associated with hallucination, and important in thinking about generative AI in scientific contexts: superficial plausibility.Even if hallucinations are inevitable, they would not pose much of a threat if they were easy to detect.Unfortunately, when scientific models are operating at the frontier of human knowledge, they are not.Here is one way to think about why.</p>
<p>Generative models tend to capture short-range dependencies more faithfully than long-range ones.This reflects a basic statistical fact: the nearer the elements, the clearer the pattern.Local structures-bond angles in molecules, temperature gradients in weather fields-recur with high signal and low variation.Long-range dependencies, by contrast, are more easily obscured by noise.As a result, generative models, whether built on diffusion processes or transformers, often produce outputs that are locally plausible but globally flawed.A protein may contain chemically sound fragments yet fold into an unstable conformation.A weather forecast may model regional dynamics with precision while violating large-scale conservation laws.Large language models exhibit a parallel tendency: they produce coherent sentences and paragraphs that fail to cohere at the level of extended argument.</p>
<p>In each case, local plausibility masks more distributed structural flaws.</p>
<p>These considerations motivate a general conclusion: any generative model that aims to produce complex, structured data will sometimes produce hallucinations.Moreover, contrary to what the recent success of AI scaling laws might suggest, even massive increases in the size of the training data will not make hallucinations of this kind go away.</p>
<p>Hallucination in diffusion models, and a proposed solution</p>
<p>This conclusion is reinforced by more targeted empirical work on diffusion models.A recent study by Aithal et al. [2025] provides the first detailed characterization of hallucination in these models.Their analysis of the problem, along with their proposed mitigation strategy, offers a useful point of contrast with the account I will develop.</p>
<p>First, a word about diffusion models themselves.These models are trained by corrupting data through a forward process that gradually adds Gaussian noise over many steps, until the data is nearly indistinguishable from pure noise.The model then learns to reverse this process by denoising: at each step, it estimates how the noisy data point should be adjusted to make it more likely under the original data distribution.This adjustment is governed by the score function, defined as the gradient of the log-density of the data distribution with respect to the input.Rather than learning the data distribution directly, diffusion models are trained to approximate this score function.But neural networks tend to learn smooth approximations of it, even when the true function contains sharp discontinuities.As Aithal et al. emphasize, this smoothness leads to interpolations across low-density regions which, in turn, leads to hallucinations.2Aithal et al. train diffusion models on synthetic datasets specifically designed to make the structure of the data manifold transparent.In one experiment, the training data is sampled from a mixture of eight well-separated Gaussians, with each point drawn from a single mode.In another, they use binary 10×10 grids, constrained so that exactly half the cells are activated according to simple structural rules.The purpose of these setups is to ensure that the generative principles underlying the training data are fully known.This allows them to test whether a diffusion model can learn those principles without producing spurious outputs.</p>
<p>They build on the same basic intuition as the preceding arguments: hallucinations arise when a model generates samples in low-density regions of the learned distribution.To make this idea precise, they introduce a threshold-based definition of hallucination:
𝐻 𝜖 (𝑞) = {𝑥 | 𝑞(𝑥) ≤ 𝜖 } (1)
Here, () is the model's estimated probability density at output , and  is a small threshold.</p>
<p>Samples that fall in regions of low density, such as the space between well-supported modes, are flagged as hallucinations.</p>
<p>Aithal et al. also offer a proposed solution to the problem of hallucination, and it is underwritten by their formal definition.They introduce a distance metric (operationalized via the variance of the model's prediction x0 as a proxy for density) that quantifies how far a generated output strays from known high-density regions in the training data.Any sample falling below the threshold-i.e., in the set   ()-is discarded.By adjusting , they aim to eliminate hallucinations while retaining most legitimate outputs.According to their evaluation, this method removes 95% of hallucinatory samples while preserving the vast majority of in-distribution outputs.</p>
<p>2The score function of a distribution  (  ) is defined as the gradient of its log-density: ∇  log  (  ).This function reflects how the probability density changes near a given point.In many real-world distributions-especially those with multiple distinct modes-the log-density may change abruptly between regions, resulting in sharp transitions or discontinuities in the score function.But neural networks tend to approximate this function in a smooth and continuous way, which causes them to interpolate across gaps between modes.For further explanation, see Luo [2022] or Song et al. [2021].This filtering solution is entirely reasonable when applied to synthetic datasets designed to test model behavior, rather than to represent an external phenomenon of scientific interest.However, in a scientific setting, our primary goal is to acquire information about the nature of the target.Specifically, the aim is to discover hidden structure that is not explicitly represented in the training data.But here, we encounter a fundamental difficulty with the conception of hallucination we have employed thus far: a generated output that falls between known modes may signal either a modeling error or a discovery-an instance of structure that the training data failed to make explicit.In the context of scientific inquiry, then, Aithal et al. [2025]'s solution is too conservative: it eliminates precisely those cases where the most interesting knowledge might emerge.If hallucination were simply a matter of deviation from training data, then every output that deserves to be called a genuine discovery would, ipso facto, count as a hallucination, and nearly all of those would get filtered out.</p>
<p>Scientific generative models are effectively engaged in inductive inference.And, as the logicians say, inductive inference is ampliative.Yet this ampliative capacity leaves us with a problem: when does generalization constitute genuine scientific discovery, and when does it constitute hallucination?Answering this requires shifting attention from the training data toward the empirical target phenomena.</p>
<p>So we need an account of hallucination centered on how model outputs inform (or mislead us about) the target system itself.</p>
<p>An epistemic account of hallucination</p>
<p>Data and phenomena</p>
<p>I want to zoom out briefly and draw a parallel between the data-centric attitude that seems prevalent in AI today and a similar attitude that prevailed in 20th-century philosophy of science.The logical empiricists (especially Carnap [1928] in the Aufbau) viewed science as the reconstruction of observational data.That view withered under criticism, but the underlying idea that theories earn legitimacy only by recovering or predicting patterns in data of some sort seems to have been widely accepted well past the middle part of the twentieth century.But as Bogen and Woodward (1988) forcefully argued, this outlook fails to account for the constructed nature of data.Because most of the data sets that scientists work with are shaped by the contingencies of measurement techniques and experimental design, scientific reasoning necessarily involves questions about how data can sometimes mislead us about the nature of the target phenomenon.To put this thought in slogan form, the data are a means to an end, rather than an end in themselves.</p>
<p>Once we accept that the goal of science is not fidelity to the data but fidelity to the phenomenon, we arrive at a different picture of how generative AI models ought to be assessed.A model's training data does not define the limits of its validity.What matters is whether its outputs illuminate the target.This idea is visualized in Figure 1.The relationship between a model's output and the training data is one of statistical resemblance; the relationship between the output and the target phenomenon is representational.</p>
<p>Hallucination and the anticipation of error</p>
<p>With this picture in place, we are now in position to appreciate the epistemic threat that hallucination poses.Because existing discussions rarely address the epistemic dimension of hallucination, there is no established term that picks out all and only threatening errors.The task, then, is not to analyze an existing concept but to construct one.The goal is to work out what properties a hallucination-like error would have to have, such that, if left unchecked, it would undermine the reliability of scientific inference.I will call errors of this kind corrosive hallucinations.</p>
<p>To develop this concept, we must first ask how hallucinations relate to the empirical target phenomenon.3With Figure 1 in mind, we can say that a hallucination is an output that misrepresents the target.But the question of whether some data structure counts as a misrepresentation is subtle.</p>
<p>What counts as a misrepresentation will depend on the interpretive conventions of the relevant discipline, and on the interpretive habits of the relevant scientists.</p>
<p>The website thesecatsdonotexist.com provides a whimsical but intuitively compelling illustration of this point.The website produces realistic images of cats using a StyleGAN model (a type of generative adversarial network trained to synthesize highly realistic images).If we interpret the outputs as images of particular, spatiotemporally located cats, then the images are hallucinations.</p>
<p>As the url suggests, these cats do not exist.If, however, we interpret them as images that merely resemble cats, then they are not hallucinations.They provide us with accurate knowledge of the statistical properties of cat appearances.</p>
<p>Scientific modeling is, in a sense, similar.Researchers use model outputs to draw inferences about the target phenomenon.It is this act of interpretation-treating the output as evidence for a claim about the target-that determines whether a misrepresentation occurs.Some hallucination-like errors are harmless or even illuminating, depending on how the output is interpreted.In fact, this point applies more forcefully in scientific cases than in the case of cat images, because only the latter is tightly constrained by the operation of the human perceptual system.I said that hallucinations are misrepresentations.But misrepresentation is not necessarily a bad thing.Many scientific misrepresentations are introduced deliberately, as idealizations.Philosophers have long noted that models include systematic distortions-treating gases as point particles or assuming frictionless planes-to render phenomena tractable [Weisberg, 2007;Strevens, 2016].These are strategic distortions designed to facilitate inference.Hallucinations, by contrast, are unintentional artifacts of generative processes.</p>
<p>But even this narrower class of unintentional misrepresentation does not yet suffice as an analysis of hallucination.Scientific models frequently produce errors, whether due to approximation, numerical instability, or incomplete training data, that are unintentional but still manageable.Not every unintentional error threatens the reliability of inference.To distinguish hallucination from other forms of benign misrepresentation, we need an additional criterion.That criterion is resistance to anticipation.</p>
<p>Here I use the term "anticipation" broadly.It refers to any strategy that enables us to foresee the 3My emphasis on the empirical character of the target system has the consequence that the present analysis does not apply to models in pure mathematics.This is not meant to imply that similar issues cannot crop up in that setting.But I do think that the application of deep learning to pure mathematics raises distinct philosophical questions that deserve their own analysis.conditions under which errors are more likely.If we can do that, we gain some control over a model's reliability.We can adjust the model itself to prevent further errors of the same type, reinterpret the outputs to account for known failure modes, or narrow the model's domain of application to avoid settings in which such errors are likely.This kind of anticipation is far more difficult in generative models than in classification models, where the task is often tractable.In classification, outputs are discrete and error types are well-defined.</p>
<p>This makes it possible to systematically probe model performance across controlled variations in input space.Researchers can vary input features, observe changes in output, and generalize about which kinds of inputs lead to failure.For example, if a radiology classifier performs poorly on underexposed images, this fact can be discovered by manipulating illumination conditions and observing the effect on predictions.The underlying causal structure of the model's behavior-how inputs contribute to specific types of errors-is accessible to empirical investigation.Attribution methods like SHAP [Lundberg and Lee, 2017] or saliency maps [Simonyan et al., 2014] leverage this structure to connect input features to model decisions.</p>
<p>With generative models, this kind of systematic circumscription of the input space is radically more difficult.Because outputs are high-dimensional-images, molecular geometries, weather fields, and so on-there are countless ways for them to be wrong.Moreover, failure modes often interact: fixing one flaw may introduce another.This complexity makes it hard to trace any single failure back to a specific cause in the input.The stable, localizable causal dependencies that support generalization in simpler models begin to break down.Now that we have described the epistemic threat posed by hallucination, we can use it to construct a definition.</p>
<p>An error counts as a corrosive hallucination iff it is both epistemically disruptive and resistant to anticipation.</p>
<ol>
<li>Epistemically disruptive.An error is epistemically disruptive if it is both substantive and non-strategic.2. Resistant to anticipation.An error exhibits resistance to anticipation when there is no known method for systematically identifying the conditions under which it is more or less likely to occur.In particular, researchers lack tools for mapping its frequency across regions of input space or tracing it to identifiable features of the model's behavior.As a result, such errors resist localization, prediction, and control.This definition names the threat and marks the first step toward addressing it.Happily, not all errors produced by generative models are corrosive.Some deviations from ground truth are insignificant; others, though epistemically relevant, are anticipated and therefore safely bracketed.The danger arises only when hallucinations are both epistemically disruptive and systematically resistant to anticipation.The remainder of this paper considers how even these most troubling errors can be managed effectively.</li>
</ol>
<p>AlphaFold and the neutralization of hallucination</p>
<p>AlphaFold, DeepMind's protein structure prediction system, represents one of the most significant recent achievements in scientific AI.The second model in the AlphaFold series, AlphaFold 2, solved the long-standing protein folding problem and led to the 2024 Nobel Prize in Chemistry, awarded to Demis Hassabis, John Jumper, and David Baker.The latest iteration, AlphaFold 3, builds on this foundation but significantly expands the model's capabilities.It goes beyond folding to predict interactions between proteins and small molecules, including ions, nucleotides, and drug-like compounds.This expansion is enabled by a core architectural shift: AlphaFold 3 incorporates a diffusion module to generate plausible molecular structures across a broader range of biological targets.But that flexibility also increases the risk of hallucination.This risk is explicitly acknowledged in the paper that introduces the model:</p>
<p>The use of a generative diffusion approach comes with some technical challenges that we needed to address.The biggest issue is that generative models are prone to hallucination, whereby the model may invent plausible-looking structure even in unstructured regions [Abramson et al., 2024, p. 496].</p>
<p>This admission makes clear that hallucination is not a marginal failure mode, but a central epistemic challenge for scientific AI.So how does the AlphaFold 3 team avoid the problem of corrosive hallucination?The answer lies in two main strategies: (i) the use of theoretical knowledge to guide training, and (ii) the use of confidence-based error screening to guide the interpretation of model output.</p>
<p>Theory-guided training</p>
<p>Unlike large language models,4 which learn statistical structure from vast, heterogeneous, and poorly organized datasets, AlphaFold 3 is trained on highly curated data by means of procedures that encode well-established physical and biochemical constraints.In particular, these constraints are encoded in carefully designed violation loss functions in the training phase.Candidate outputs are penalized if they exhibit steric clashes (where atoms are closer than the van der Waals radius allows), implausible bond lengths, or physically unrealistic torsional angles.These penalties shape the diffusion model's learned score function by rewarding outputs that adhere to molecular physics, thereby suppressing structurally incoherent predictions.</p>
<p>The epistemic force of AlphaFold's constraint-based design stems not only from the content of the physical laws it encodes, but from the fact that the evidential basis for those laws is largely independent of the training distribution.Confirmation is strengthened when distinct bodies of empirical knowledge, each grounded in a different measurement techniques, converge on a common target [Sober, 1989].</p>
<p>The structural regularities distilled into the PDB and the theoretical constraints operationalized in the loss function arise from separate physical processes and measurement paradigms.Their convergence in AlphaFold's architecture transforms an inductive generalization into a theoretically disciplined scientific inference.</p>
<p>Another training-phase technique is cross-distillation, in which AlphaFold is retrained using the outputs of other models with simpler and more interpretable error profiles.Comparing these models is another way that systematic bias can be exposed.For instance, recent work [Brotzakis et al., 2025] retrained AlphaFold on coarse-grained structural approximations, increasing the model's caution in structurally ambiguous regions where hallucinations tend to arise.This analysis also calls for a reconsideration of a well-known critique of deep learning.Pearl and Mackenzie [2018] and Marcus and Davis [2019] argue that deep learning models cannot acquire causal 4Some generative protein folding models, such as Meta's ESM protein model, are described by their authors as "language models", despite being trained on biochemical data rather than natural language.When I use the term "language model", I am referring to models trained on natural language.knowledge.AlphaFold's design challenges that view.Theoretical constraints, grounded in physics and chemistry, actively shape the model's optimization process.Moreover, its training data-the Protein Data Bank-is not a random sample of molecular configurations.It is a theoretically curated archive of structures inferred through techniques like X-ray crystallography, cryo-EM, and NMR spectroscopy.</p>
<p>At least some of this causal knowledge is embedded in AlphaFold's learned latent space.So, although AlphaFold may not represent mechanisms explicitly, its training is disciplined by physical law, and its outputs reflect constraints grounded in causal structure.Insofar as the AlphaFold team has built a reliable workflow, that reliability is not a mysterious byproduct of scale but a consequence of carefully engineered design that is guided by knowledge of causal mechanisms at the molecular scale.</p>
<p>Confidence-based error screening</p>
<p>Hallucinations that cannot be eliminated may still be rendered epistemically harmless, as long as we have a method for singling them out.That is the role of confidence-based error screening.In AlphaFold, this is achieved (in part) by means of residue-level reliability scores that help scientists distinguish between outputs that support inference and those that warrant caution.5</p>
<p>The central tool here is the Predicted Local Distance Difference Test (pLDDT).This mechanism is particularly effective in identifying intrinsically disordered regions (IDRs), 5This functionality is sometimes grouped under the heading of "uncertainty quantification," but that term often refers to formal confidence intervals in the context of statistical testing.In contrast, AlphaFold's scores are learned by the model and serve a primarily to enable scientists to screen for unreliable outputs."Confidence-based error screening" is my own term, which I think more accurately reflects the epistemic function of the relevant techniques.</p>
<p>6pLDDT scores are produced by a head in AlphaFold's architecture that is trained to predict the expected deviation between predicted and true interatomic distances for each residue.During training, this head is supervised using experimentally validated structures, allowing the model to calibrate its internal confidence estimates.At inference time, however, the score is computed purely from the model's own internal representations; no comparison to ground truth is made.</p>
<p>whose structures are environmentally contingent and cannot be predicted with high fidelity.Rather than hallucinating a confident structure, AlphaFold returns low-confidence, flexible representations, rendered in a distinctive "noodle-like" visual form that contrast sharply with the well-folded, compact forms nearby.This visual convention reinforces the model's confidence scores and functions as a cue to practicing scientists that the structure is not to be over-interpreted.Brotzakis et al. [2025] show that AlphaFold 3 outperforms specialized tools in detecting such regions, despite not being explicitly trained for this purpose.</p>
<p>One might object that if a model's representations are inaccurate, one shouldn't put much stock in its internal confidence scores either.This worry is not misplaced: confidence-based screening is not epistemically infallible.But its value does not depend on access to ground truth at inference time.</p>
<p>What matters is that these scores correlate robustly with empirical reliability across a wide range of cases.In AlphaFold's case, high pLDDT values have been shown to track subsequent experimental validation with remarkable consistency.The metric does not guarantee correctness, but it provides a calibrated signal of when the model's outputs can be used for inference, and when they should not be.This is enough both to shift hallucinations from epistemic threats to manageable uncertainties, and to give scientists license to treat the high-confidence outputs as serious candidates for belief.</p>
<p>Crucially, AlphaFold achieves this level of reliability despite the opacity of its internal representations.Its trustworthiness stems not from interpretability, but from the way it embeds domain knowledge, performs confidence-based screening, and renders that information in a form that enables downstream decision making about which experiments are most likely to succeed.This is the core insight of computational reliabilism [Durán and Formanek, 2018;Duran, 2023]: models can support reliable inference even when they are not transparent, so long as they are embedded in a well-designed error-screening workflow.</p>
<p>It is worth contrasting this approach with filtering-based methods like those proposed by Aithal et al., which define hallucinations as outputs that deviate from the training distribution.As argued in Section 2, such deviations are inevitable in high-dimensional generative models.But AlphaFold's confidence-based error screening does not treat deviation from training data as a defect per se.</p>
<p>Unlike distributional filters that discard statistically anomalous samples, pLDDT permits substantial departures from the training distribution as long as they are robust across the model's internal ensemble.This allows AlphaFold to support meaningful extrapolation, while still flagging outputs that are likely to be unreliable.</p>
<p>From molecules to meteorology</p>
<p>If AlphaFold demonstrates how generative AI can support inference in molecular biology, GenCast shows how the same epistemic principles extend to large-scale dynamical systems.Unlike protein folding, where the goal is to infer a stable conformational structure, meteorological forecasting targets a dynamic and chaotic system in which small errors in initial conditions can rapidly amplify [Lorenz, 1995], and which lacks a clearly defined end state.Traditional numerical weather prediction (NWP) systems, such as those used by the European Centre for Medium-Range Weather Forecasts (ECMWF), generate predictions by numerically solving fluid dynamics equations to produce ensemble forecasts.</p>
<p>These methods are physically grounded and benefit from interpretable parameters, but they are also computationally expensive.In settings where the goal is to predict intrinsically low-frequency, high-impact events-such as floods or wildfires-which lie in the tails of the probability distribution, an extremely large ensemble is required to achieve adequate coverage, and computational costs grow rapidly with event rarity.</p>
<p>GenCast offers a (relatively) computationally efficient alternative.It is a new, diffusion-based generative model trained on ERA5, a reanalysis dataset7that integrates observational data with physicsbased simulations.Like AlphaFold, GenCast learns the statistical structure of valid trajectories and generates plausible forecasts via a generative process.Though it lacks an explicit representation of fluid dynamics, GenCast now matches or exceeds the performance of ECMWF's gold-standard system on several standard forecasting metrics (and is, moreover, the first machine-learning weather prediction model to rival the accuracy of traditional simulation-based approaches) [Price et al., 2024].</p>
<p>GenCast's success, like AlphaFold's, depends on its being embedded in a workflow that draws on independent scientific theory.It is trained not on raw observations but on ERA5: reconstructions that obey conservation laws and reflect established meteorological principles.This mirrors AlphaFold's reliance on the Protein Data Bank, which functions as a theoretically curated archive of molecular structure.Second, like AlphaFold, GenCast incorporates physics-informed loss functions that penalize predictions violating basic physical constraints [Kashinath et al., 2021].Earlier machine learning models could generate forecasts that appeared locally plausible but violated global coherence.For example, they might predict a negative humidity value or a physically unrealistic temperature gradient [Watt-Meyer et al., 2021].GenCast avoids such failures by incorporating domain-specific constraints during training and by relying on architectures that tend to preserve these constraints at inference 7In meteorology, a reanalysis is a dataset created by filling gaps in historical observations using numerical models constrained by physical laws.The result is a physically coherent estimate of past atmospheric states, widely used for climate research, model training, and long-term forecasting.</p>
<p>time.These practices reduce the risk that model outputs will be epistemically disruptive, in the sense defined in Section 3.</p>
<p>Like AlphaFold, GenCast addresses hallucination through confidence-based error screening, though by different means.Instead of assigning explicit confidence scores to individual predictions, GenCast introduces stochastic variation at inference time, generating an ensemble of plausible forecasts from different random seeds.Given the chaotic nature of weather systems, each trajectory varies in local details, but the ensemble as a whole preserves coherent global structure.The epistemic value of this approach lies in how this variability reveals where inference is likely to be unreliable.</p>
<p>Because hallucinations differ across stochastic runs, their dispersion serves as a signal of epistemic instability.Unstable predictions appear as outliers, while robust features emerge as recurring patterns.</p>
<p>In this way, GenCast provides a confidence signal.Although it is not a separately computed output, as it is in AlphaFold, it is a reliable statistical pattern that expert scientists can leverage.</p>
<p>One way to assess how well GenCast's internal uncertainty estimates align with forecasting performance is through the spread-skill ratio, which compares the ensemble's internal variance (the spread) with its actual forecast error (the skill).8Aspread-skill ratio near 1 indicates that the model's uncertainty estimates are well-matched to its performance-neither overconfident nor needlessly conservative.In the GenCast evaluation, this ratio remained close to 1 across a range of forecast horizons (i.e., the time intervals into the future for which predictions are made), confirming that the ensemble's internal dispersion reliably mirrors the inherent empirical uncertainty in chaotic systems.</p>
<p>GenCast's success shows how hallucinations can be made epistemically tractable.Rather than being left as corrosive, errors are converted into expected, bounded deviations whose impact can be absorbed by the broader workflow.This undermines the suspicion that opaque generative models leave us with no alternative but what Duede memorably called brute inductivism.On that view, the lack of interpretability precludes the possibility of theory-guided inference, and users are left to trust outputs solely on the basis of observed empirical correlations.But as the GenCast case makes clear, we have more to rely on here than the naked predictive track record.By combining theory-informed training, 8The spread-skill ratio (SSR) compares an ensemble's internal variance (spread) to its forecast error (root-mean-square error, RMSE).It is given by:
SSR = Spread RMSE = √︃ 1 𝑁 𝑁 𝑖=1 ( 𝑓 𝑖 − f ) 2 √︃ 1 𝑀 𝑀 𝑗=1 ( f 𝑗 − 𝑜 𝑗 ) 2
where   is the forecast from ensemble member , f is the ensemble mean forecast,   is the observation at verification case ,  is the number of ensemble members, and  is the number of forecast-observation pairs.An SSR near 1 indicates well-calibrated uncertainty; values significantly greater or less than 1 suggest overdispersion or underdispersion, respectively [Fortin et al., 2014].Variations on this definition exist, and there is ongoing debate about which formulation is most appropriate in different settings.</p>
<p>ensemble-based uncertainty modeling, and explicit error detection practices, GenCast enables a mode of inference that is neither brute nor blind.It underscores the same lesson we saw in AlphaFold: generative models become reliable not by virtue of their internal transparency, but by being embedded in theory-rich, uncertainty-aware practices that help scientists anticipate and manage error.</p>
<p>Discovery and justification</p>
<p>I have argued that the threat of hallucination does not undermine the reliability of scientific AI because generative models can be embedded in epistemically robust workflows.Yet one might object that my emphasis on strategies for reliability misses what ought to be the centerpiece of any response to concerns about hallucination: the main epistemic safeguard for scientific AI is post-hoc empirical validation.We trust AlphaFold primarily because we can experimentally test its predictions, and we trust GenCast because we can wait two weeks and see whether it rains.This objection is inspired by Duede's 2023 argument that concerns about AI reliability often reflect a misunderstanding of its scientific role.Duede claims that AI is fundamentally a tool for discovery, not for justification.On this view, my concern about hallucination wrongly presupposes that AI is in the business of delivering justification for model outputs.Duede might argue that AI merely offers heuristic guidance: it narrows the space of relevant hypotheses, but those hypotheses only become candidates for belief once they have been subjected to empirical test.From a thoroughly empiricist standpoint, the strategies scientists use to mitigate hallucination appear secondary-or even unnecessary.This response echoes Karl Popper's 1959 appeal to the influential distinction between the context of discovery and the context of justification.Popper famously held that the epistemology of science should focus solely on justification through rational reconstruction, since discovery is largely governed by factors like intuition and creativity, which are, according to Popper, beyond rational control.</p>
<p>Yet the historical turn in the philosophy of science has long challenged the sharp division between discovery and justification.Scientific heuristics, whether human or AI-driven, are not arbitrary guesses; they are assessed by their empirical traction.This is especially evident in the development and funding of generative AI systems like AlphaFold.The model was not funded merely for its capacity to generate intriguing hypotheses, but because its developers demonstrated-before large-scale empirical validation-that it could reliably predict biologically plausible protein structures.Its ability to infer accurate 3D conformations from amino acid sequences had clear implications for understanding biological function and disease.This predictive success led to rapid adoption across the life sciences, where researchers now use its outputs to guide experiment and hypothesis formation.AlphaFold is not treated as a tool for blind exploration, but as a theory-informed model capable of supporting novel inferences.When scientists take one of its outputs to be approximately true, their belief enjoys a kind of partial justification.</p>
<p>The same reasoning applies to weather models such as GenCast.Its probabilistic forecasts are not treated as mere exploratory hypotheses awaiting eventual testing; rather, its reliability is continuously assessed through rigorous calibration against both theory and observational data.GenCast is already being used operationally, for example in forecasting applications like OpenSnow, which helps backcountry skiers assess conditions and make daily decisions.9Such practical use makes sense only if GenCast provides a justified basis for action in advance, rather than merely suggesting hypotheses for empirical investigation.</p>
<p>More generally, the justification for placing credence in generative AI systems is embedded throughout their scientific workflows, and not merely appended afterward.</p>
<p>Duede rightly challenges overly skeptical views that demand too much of generative AI.But by relegating AI entirely to the context of discovery, he leans too heavily on a distinction that, in practice, is difficult to sustain.Discovery and justification are deeply intertwined in scientific inquiry.</p>
<p>Principled strategies for managing hallucination are not epistemically superfluous; they are essential to the responsible integration of AI into scientific practice.</p>
<p>Another closely related objection is worth addressing.One might worry that the strategies I have described all have a negative cast: they are concerned primarily with screening for error and adjusting our inferences accordingly.This process of error detection and adaptation, one might argue, is categorically distinct from the accumulation of positive evidence for the truth of a model's output.</p>
<p>But from a reliabilist perspective, that distinction breaks down.The reliability of an inferential process increases whenever potential errors are filtered out or otherwise managed.And since reliability is, for the reliabilist, the key property that transforms true belief into knowledge, the task of identifying and managing error is directly relevant to the epistemic status of model-supported beliefs.</p>
<p>9GenCast is sufficiently new that real-world applications are only now emerging.It is likely that higher-stakes applications will soon follow.</p>
<p>Error mitigation and the construction of reliable science</p>
<p>Generative AI is increasingly central to scientific inquiry, yet the spectre of hallucination has raised legitimate doubts about its reliability.I have argued that while corrosive hallucination presents a novel epistemic threat, it is not a reason to adopt a wholesale form of skepticism about the use of generative AI models in science.Although hallucinations complicate the task of shoring up our inferences, scientific workflows have been developed to meet the challenge.As the cases of AlphaFold and GenCast illustrate, these methods do not eliminate error, but they do ensure that errors remain managable.</p>
<p>Although these strategies do not rely on aligning individual model parameters with interpretable features of the world, they are nevertheless more sophisticated than the kind of brute inductivism that Duede has criticized.The mechanisms by which these systems manage error are not ad hoc.They draw on independently supported theoretical knowledge about the target domain, and integrate that knowledge into scientific workflow at during model development and in the interpretation of results.</p>
<p>The view developed here is largely aligned with the what Dúran and Formanek call computational reliabilism, a view about scientific epistemology that was initially developed in order to explore what justification we can have for believing the results of large, epistemically opaque computer simulations.</p>
<p>But at least some epistemically opaque computer simulations have what Creel [2020] calls functional transparency.We know which algorithm they are designed to implement; the challenge is knowing whether they implement it correctly.When working with DNN-based models, the problem is worse, because DNNs are not functionally transparent.We have no compact, cognitively accessible representation of the algorithm they implement.This fact has led many scientists and philosophers to worry that AI-driven science is a kind of blind data-driven procedure where prediction is all that matters, and theory holds no sway.My arguments concerning the problem of hallucination is another way of showing, as Andrews [2023] compellingly argues, AI does not offer a theory-free route to knowledge.On the contrary, generative models gain scientific legitimacy precisely when they are disciplined by theory.</p>
<p>Given the rapid pace of AI development, there is a genuine risk that the methodological norms that guide scientific modeling today will not apply to the science of tomorrow.It is important, therefore, to qualify any strong conclusions.In that spirit, I want to flag a position that may seem naturally aligned with the argument of this paper, but which I do not endorse.It is the view that only narrow, domain-specific models can serve as reliable sources of scientific knowledge.While domain-specific models currently outperform general-purpose models in this respect-largely because it is easier to construct error-mitigating workflows around them-I remain agnostic about whether similar forms of reliability might eventually be achieved with general-purpose architectures.There are reasons to take this possibility seriously.Multi-modal models such as GPT-4 and Gemini 2.5 have so-called 'emergent' capacities that cannot be replicated by chaining together domain-specific tools.Their integration of language, image, and video appears to produce synergies within the model's latent space that surpass what modular composition can deliver.Whether chemical representations could be treated as an additional "modality" in this sense-and whether that integration would yield genuinely new epistemic benefits-remains an open and intriguing question.</p>
<p>Another way that general-purpose models may become reliable contributors to scientific inference is by means of something like the competitive dynamics among human scientists which, when things go well, give rise to an error-correcting mechanism on a social scale.The Virtual Labs framework [Swanson et al., 2024] exemplifies the possibility of implementing something like this in AI agents: a system of LLMs that pose hypotheses, subject them to criticism by other LLMs, refine the hypotheses, and then eventually farm them out to domain-specific tools like Rosetta and AlphaFold to make predictions, which are then tested by human experimentalists.Though preliminary and not yet peer-reviewed, such systems hint at novel epistemic architectures-ones in which general-purpose models play a meaningful role within a cutting-edge scientific discovery.These developments are too recent to warrant strong conclusions, but they caution against assuming that narrowness is the only viable path.This paper is intended as a partial answer to skeptical concerns about the use of generative AI in science.Nevertheless, I would like to add a cautionary note of my own, which was inspired by discussions at a recent meeting concerned with European science funding.While enthusiasm for AIdriven science grows, there is a risk of reallocating funding away from traditional theory-building and experimentation.Yet, as this paper has emphasized, generative AI models achieve reliability precisely because they are embedded in theoretical frameworks and validated by empirical experimentation.</p>
<p>Neglecting these foundational methods would be unwise, because it would weaken the epistemic scaffolding upon which AI-based inference depends.</p>
<p>The central argument of this paper reflects the Deweyan insight quoted in the epigraph: scientific knowledge is not a matter of grasping antecedently given certainties, but of developing ways to make sure-methods for identifying error and managing uncertainty.Generative AI calls for new methods of doing that work, and simultaneously makes that work more difficult.Nevertheless, as AlphaFold,</p>
<p>Figure 1 :
1
Figure 1: Diagram illustrating the relationship between a target phenomenon, a dataset constructed from observations of the target (second box), a generative deep neural network (DNN), and the DNN's output.The DNN produces outputs that resemble samples from the training data but do not represent them.Whether an output functions as a representation depends on our inferential practices, and in scientific contexts, these practices are aimed at understanding the target phenomenon-not merely reconstructing the training data.The backward arrow ("similarity without representation") indicates that while the model output may exhibit statistical similarity to training data, it is not used as a representation of the training data itself.Rightward arrows indicate causal rather than representational relations.</p>
<p>(a) Substantive: It alters downstream reasoning-it changes conclusions, decisions, or inferential trajectories.Crucially, this significance depends on interpretation: an output is only epistemically disruptive when it is taken as evidence for a particular claim about the target phenomenon.(b) Non-strategic: It does not arise from an intentional simplification designed to aid understanding.Idealizations-such as modeling gases as point particles-introduce systematic distortions but serve an epistemic function.Corrosive hallucinations, by contrast, distort without enlightening.</p>
<p>Rather than measuring proximity to training examples, pLDDT estimates the local reliability of a predicted structure based on internal consistency cues.Specifically, AlphaFold generates multiple structure predictions through a stochastic sampling procedure, and pLDDT scores reflect the degree of local agreement among these samples.Where predictions converge tightly, the model assigns high confidence; where they diverge-often due to physical indeterminacy or lack of constraint-it flags the output as unreliable.The underlying idea is simple but powerful: hallucinations are not uniform across stochastic samples.By generating multiple outputs with different random seeds, AlphaFold can identify regions of disagreement and treat them as signals of uncertainty.Idiosyncratic errors tend to cancel out in the aggregate, allowing the model to screen for instability without requiring access to ground truth.6</p>
<p>Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew J Ballard, Joshua Bambrick, Sebastian W Bodenstein, David A Evans, Chia-Chun Hung, O' Michael, David Neill, Kathryn Reiman, Zachary Tunyasuvunakool, Akvilė Wu, Eirini Žemgulytė, Charles Arvaniti, Ottavia Beattie, Alex Bertolli, Alexey Bridgland, Miles Cherepanov, Congreve, I Alexander, Andrew Cowen-Rivers, Michael Cowie, Fabian B Figurnov, Hannah Fuchs, Rishub Gladman, Jain, A Yousuf, Caroline M R Khan, Kuba Low, Anna Perlin, Pascal Potapenko, Sukhdeep Savy, Adrian Singh, Stecula, 10.1038/s41586-024-07487-wMax Jaderberg, Demis Hassabis, and John M. Jumper. Accurate structure prediction of biomolecular interactions with AlphaFold 3. Ashok Thillaisundaram, Catherine Tong, Sergei Yakneen, Ellen D Zhong, Michal Zielinski, Augustin Žídek, Victor Bapst, Pushmeet Kohli, May 2024</p>
<p>Understanding Hallucinations in Diffusion Models through Mode Interpolation. K Sumukh, Pratyush Aithal, Zachary Maini, J Zico Lipton, Kolter, Advances in Neural Information Processing Systems. January 202537</p>
<p>The Devil in the Data: Machine Learning &amp; the Theory-Free Ideal. Mel Andrews, 2023Manuscript under review</p>
<p>. Martín Arjovsky, Soumith Chintala, Léon Bottou, Gan Wasserstein, Arxiv, January 2017</p>
<p>Mapping Out Phase Diagrams with Generative Classifiers. Julian Arnold, Frank Schäfer, Alan Edelman, Christoph Bruder, 10.1103/PhysRevLett.132.207301Physical Review Letters. 13220207301May 2024</p>
<p>S Banerjee, A M Jacob, arXiv:2409.05746LLMs will always hallucinate, and we need to live with this. 2024arXiv preprint</p>
<p>AlphaFold prediction of structural ensembles of disordered proteins. Z Faidon Brotzakis, Shengyu Zhang, Mhd Hussein Murtada, Michele Vendruscolo, 10.1038/s41467-025-56572-9Nature Communications. 2041-17231611632February 2025</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang, Sparks of Artificial General Intelligence: Early experiments with GPT-4. April 2023</p>
<p>From Deep Learning to Rational Machines: What the History of Philosophy Can Teach Us about the Future of Artificial Intelligence. Cameron J Buckner, February 2024Oxford University PressOxford, New York</p>
<p>Der logische Aufbau der Welt. Rudolf Carnap, 1928Felix Meiner VerlagHamburg3 edition</p>
<p>Transparency in Complex Computational Systems. Kathleen A Creel, 10.1086/709729Philosophy of Science. 8742020</p>
<p>Experience and Nature. John Dewey, 1958Dover PublicationsNew York, NY, USA</p>
<p>How much of protein sequence space has been explored by life on Earth. T David, Andrew R Dryden, John H Thomson, White, 10.1098/rsif.2008.0085Journal of The Royal Society Interface. 525April 2008</p>
<p>Deep Learning Opacity in Scientific Discovery. Eamon Duede, 10.1017/psa.2023.8Philosophy of Science. 0031-8248905December 2023</p>
<p>Grounds for Trust: Essential Epistemic Opacity and Computational Reliabilism. Minds and Machines. Juan M Durán, Nico Formanek, 10.1007/s11023-018-9481-6December 201828</p>
<p>Juan Manuel, Duran , Machine learning, justification, and computational reliabilism. 2023</p>
<p>Why Should Ensemble Spread Match the RMSE of the Ensemble Mean. V Fortin, M Abaza, F Anctil, R Turcotte, 10.1175/JHM-D-14-0008.1Journal of Hydrometeorology. 1525-7541154August 2014</p>
<p>What is Justified Belief?. Alvin I Goldman, Justification and Knowledge: New Studies in Epistemology. George Pappas, D. Reidel1979</p>
<p>Generative Adversarial Nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in Neural Information Processing Systems. Curran Associates, Inc201427</p>
<p>G R Grimmett, D R Stirzaker, Probability and Random Processes. OxfordOxford University Press1992</p>
<p>Reliability in Machine Learning. Thomas Grote, Konstantin Genin, Emily Sullivan, 10.1111/phc3.12974Philosophy Compass. 1747-9991195e129742024</p>
<p>The Philosophical Novelty of Computer Simulation Methods. Synthese. Paul Humphreys, 10.1007/s11229-008-9435-22009169</p>
<p>Survey of Hallucination in Natural Language Generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 0360-0300551238March 2023</p>
<p>Artificial intelligence reconstructs missing climate information. Christopher Kadow, David Matthew Hall, Uwe Ulbrich, 10.1038/s41561-020-0582-5Nature Geoscience. 1752-0908136June 2020</p>
<p>Calibrated Language Models Must Hallucinate. Tauman Adam, Santosh S Kalai, Vempala, March 2024</p>
<p>Physicsinformed machine learning: Case studies for weather and climate modelling. K Kashinath, M Mustafa, A Albert, J-L. Wu, C Jiang, S Esmaeilzadeh, K Azizzadenesheli, R Wang, A Chattopadhyay, A Singh, A Manepalli, D Chirila, R Yu, R Walters, B White, H Xiao, H A Tchelepi, P Marcus, A Anandkumar, P Hassanzadeh, Null Prabhat, 10.1098/rsta.2020.0093Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences. 379202000932194. February 2021</p>
<p>Edward Lorenz. The Essence of Chaos. P Diederik, Max Kingma, Welling, 2013. 1995The University of Washington PressAuto-encoding variational bayes</p>
<p>A Unified Approach to Interpreting Model Predictions. M Scott, Su-In Lundberg, Lee, Advances in Neural Information Processing Systems. Curran Associates, Inc201730</p>
<p>Understanding Diffusion Models: A Unified Perspective. Calvin Luo, August 2022</p>
<p>Algorithm and Parameters: Solving the Generality Problem for Reliabilism. C Jack, Lyons, 10.1215/00318108-7697876Philosophical Review. 12842019</p>
<p>Gary Marcus, Ernest Davis, Rebooting AI: Building Artificial Intelligence We Can Trust. Pantheon. 2019</p>
<p>The Book of Why: The New Science of Cause and Effect. Judea Pearl, Dana Mackenzie, 2018Basic BooksNew York</p>
<p>The Logic of Scientific Discovery. Karl Popper, 1959Hutchinson, London</p>
<p>Probabilistic weather forecasting with machine learning. Ilan Price, Alvaro Sanchez-Gonzalez, Ferran Alet, Tom R Andersson, Andrew El-Kadi, Dominic Masters, Timo Ewalds, Jacklynn Stott, Shakir Mohamed, Peter Battaglia, Remi Lam, Matthew Willson, 10.1038/s41586-024-08252-9Nature. 1476-4687December 2024</p>
<p>Stochastic Backpropagation and Approximate Inference in Deep Generative Models. Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, Proceedings of the 31st International Conference on Machine Learning. the 31st International Conference on Machine LearningPMLRJune 2014</p>
<p>Assessing generative models via precision and recall. S M Mehdi, Olivier Sajjadi, Mario Bachem, Olivier Lucic, Sylvain Bousquet, Gelly, Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS'18. the 32nd International Conference on Neural Information Processing Systems, NIPS'18Red Hook, NY, USACurran Associates IncDecember 2018</p>
<p>Deep Generative Adversarial Networks: Applications in Musculoskeletal Imaging. Yirang Shin, Jaemoon Yang, Young Han, Lee , 10.1148/ryai.2021200157Radiology: Artificial Intelligence. 2638-610033e200157March 2021</p>
<p>Deep learning reveals predictive sequence concepts within immune repertoires to immunotherapy. John-William Sidhom, Giacomo Oliveira, Petra Ross-Macdonald, Megan Wind-Rotolo, Catherine J Wu, Drew M Pardoll, Alexander S Baras, 10.1126/sciadv.abq5089Science Advances. 8375089September 2022</p>
<p>Deep inside convolutional networks: Visualising image classification models and saliency maps. K Simonyan, A Vedaldi, A Zisserman, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2014</p>
<p>A Mathematical Abstraction for Balancing the Trade-off Between Creativity and Reality in Large Language Models. Ritwik Sinha, Zhao Song, Tianyi Zhou, June 2023</p>
<p>Independent evidence about a common cause. Elliott Sober, Philosophy of Science. 5621989</p>
<p>Score-based generative modeling through stochastic differential equations. Yang Song, Jascha Sohl-Dickstein, Abhishek Diederik P Kingma, Stefano Kumar, Ben Ermon, Poole, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)2021</p>
<p>How Idealizations Provide Understanding. Michael Strevens, Explaining Understanding: New Perspectives From Epistemology and Philosophy of Science. Stephen Robert Grimm, Christoph Baumberger, Sabine Ammon, Routledge2016</p>
<p>Understanding from Machine Learning Models. Emily Sullivan, 10.1093/bjps/axz035The British Journal for the Philosophy of Science. 0007-0882731March 2022</p>
<p>AI hallucination: Towards a comprehensive classification of distorted information in artificial intelligence-generated content. Yujie Sun, Dongfang Sheng, Zihan Zhou, Yifei Wu, 10.1057/s41599-024-03811-xHumanities and Social Sciences Communications. 2662-9992111September 2024</p>
<p>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation. Kyle Swanson, Wesley Wu, L Nash, John E Bulaong, James Pak, Zou, November 2024</p>
<p>Mixture-of-hand-experts: Repainting the deformed hand images generated by diffusion models. Yuxuan Wang, Tianwei Cao, Kongming Liang, Zhongjiang He, Hao Sun, Yongxiang Li, Zhanyu Ma, 10.1007/978-981-97-8620-610Pattern Recognition and Computer Vision: 7th Chinese Conference, PRCV 2024. Urumqi, China; Berlin, HeidelbergSpringer-VerlagOctober 18-20, 2024. 2024Proceedings, Part V</p>
<p>Correcting Weather and Climate Models by Machine Learning Nudged Historical Simulations. Oliver Watt-Meyer, Noah D Brenowitz, Spencer K Clark, Brian Henn, Anna Kwa, Jeremy Mcgibbon, W Andre Perkins, Christopher S Bretherton, 10.1029/2021GL092555Geophysical Research Letters. 1944-800748152021</p>
<p>Three Kinds of Idealization. Michael Weisberg, 10.5840/jphil20071041240Journal of Philosophy. 104122007</p>
<p>Hallucination is Inevitable: An Innate Limitation of Large Language Models. Ziwei Xu, Sanjay Jain, Mohan Kankanhalli, January 2024</p>
<p>Hallucination is Inevitable: An Innate Limitation of Large Language Models. Ziwei Xu, Sanjay Jain, Mohan Kankanhalli, February 2025</p>
<p>Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, Ming-Hsuan Yang, 10.1145/3626235Diffusion Models: A Comprehensive Survey of Methods and Applications. November 20235639</p>            </div>
        </div>

    </div>
</body>
</html>
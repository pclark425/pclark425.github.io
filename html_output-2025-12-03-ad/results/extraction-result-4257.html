<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4257 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4257</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4257</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-98.html">extraction-schema-98</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <p><strong>Paper ID:</strong> paper-274763061</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2412.09628v2.pdf" target="_blank">Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science</a></p>
                <p><strong>Paper Abstract:</strong> Artificial Intelligence has proven to be a transformative tool for advancing scientific research across a wide range of disciplines. However, a significant gap still exists between AI and scientific communities, limiting the full potential of AI methods in driving broad scientific discovery. Existing efforts in identifying and bridging this gap have often relied on qualitative examination of small samples of literature, offering a limited perspective on the broader AI4Science landscape. In this work, we present a large-scale analysis of the AI4Science literature, starting by using large language models to identify scientific problems and AI methods in publications from top science and AI venues. Leveraging this new dataset, we quantitatively highlight key disparities between AI methods and scientific problems, revealing substantial opportunities for deeper AI integration across scientific disciplines. Furthermore, we explore the potential and challenges of facilitating collaboration between AI and scientific communities through the lens of link prediction. Our findings and tools aim to promote more impactful interdisciplinary collaborations and accelerate scientific discovery through deeper and broader AI integration. Our code and dataset are available at: https://github.com/charles-pyj/Bridging-AI-and-Science.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4257.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4257.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-mini-2024-07-18 (OpenAI GPT-4o mini) extraction pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based extraction pipeline that uses GPT-4o mini to read paper titles and abstracts and produce structured extractions of Scientific Problem, AI Method, and AI Usage; these extractions are further embedded, clustered, and summarized to derive empirical patterns about the AI4Science literature.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-mini-2024-07-18 (OpenAI GPT-4o mini)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM-based extraction pipeline (six-aspect JSON extraction)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>For each paper title+abstract the authors prompt GPT-4o-mini to output a JSON containing six aspects: Problem (keyword/keyphrase), Problem (definition), Problem discipline, Method (keyword/keyphrase), Method (definition), and AI Usage. Prompts enforced separation (no method info in problem fields and vice versa) and requested output in a fixed JSON schema. Extracted textual fields were embedded with InstructorEmbedding, projected (LargeVis), clustered with HDBSCAN, and cluster labels/summaries were produced by GPT-4o. A separate LLM prompt/classifier judged whether the paper is AI4Science. Default generation hyperparameters were used; human-in-the-loop verification was performed on a held-out sample.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>162,656 publications (2014-2024); extractions: 129,038 scientific-problem entries, 42,455 AI-method entries, 42,455 AI-usage entries (7,542 AI4Science publications)</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Multidisciplinary: top science journals (Nature, Science, PNAS, Nature Communications, Science Advances) and top AI conferences (AAAI, IJCAI, ICLR, ICML, NeurIPS, SIGKDD, WWW) — across biology, chemistry, medicine, earth science, materials, social science, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Empirical generalizations and semantic patterns about the AI4Science landscape: cluster-level thematic summaries, under-/well-explored problem-method pair distributions, hub-peripheral connectivity patterns in a problem↔method bipartite graph, and semantic cluster labels (i.e., domain-level regularities and empirical patterns rather than formal mechanistic laws).</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>1) High-level empirical findings F1–F3 from the paper: (F1) Many scientific problems and AI methods are underexplored for AI4Science; (F2) Connectivity is highly imbalanced with 'hub' problems/methods linking broadly while many nodes are peripheral; (F3) AI and science communities prioritize different problems and methods. 2) Concrete underexplored examples identified: scientific problems such as 'Cancer Signaling Pathways' and 'Cancer Drug Resistance' and AI methods underutilized like 'Attention Mechanisms' and 'Transformer Architectures'.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Human evaluation of LLM extractions on a held-out sample (two trained annotators per paper) comparing LLM extractions to human annotations; LLM AI4Science classification also compared to human labels. Cluster labeling and downstream analyses validated by qualitative inspection and by comparing distributional statistics (bipartite graph) against expectations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Extraction accuracy (human-evaluated) = 0.910 (91.0%); human agreement F1 on extraction tasks: 0.876 and 0.946 (two genres). AI4Science classification F1: scientific-journal papers = 0.836; AI-conference papers = 0.647 (with high recall 0.866 on AI papers). These numeric evaluations were reported in Appendix A.2 / Tables 8-9.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against human expert annotations (two annotators) on a random subset; LLM extractions aligned well with human labels. The paper contrasts this LLM-based fine-grained extraction with prior coarse keyword / metadata-based tagging used in earlier quantitative studies (e.g., Gao & Wang [7], Duede et al. [6]).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM extraction from titles and abstracts is accurate (≈91% human-judged); it scales to large corpora and enables finer-grained semantic analyses (embeddings + clustering) that reveal empirical patterns (underexplored regions, hubs). Using LLMs for cluster summarization produces understandable labels; the pipeline is reproducible and enabled construction of a large AI4Science dataset.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Extraction relied only on titles and abstracts (no full-text), which may miss methodological detail; potential selection bias from focusing on top-tier venues; difficulty distinguishing CS/AI research problems from 'traditional' science in conference papers (lower classification F1 for AI-conference papers); risk of LLM hallucination or subtle misclassification; model_size and internal training data of the LLM not controlled by authors; cost and API limits for large-scale extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4257.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4257.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being used to process scholarly papers and extract qualitative laws, principles, patterns, or theories from them.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o/gpt-3.5 link-predict</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>gpt-4o-2024-08-06 and gpt-3.5-turbo-0125 LLM pipelines for cluster- and paper-level link prediction (RAG-enhanced generative predictions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>LLM-based link-prediction/generative systems that (a) predict AI methods for scientific problems at the cluster level given the bipartite graph context (LLM (Cluster)), and (b) generate paper-level candidate mappings from problems→methods or methods→problems using retrieval-augmented generation (LLM (Paper)), then map generations to clusters via embedding similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>gpt-4o-2024-08-06 (primary), gpt-3.5-turbo-0125 (additional generator)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>LLM (Cluster) and LLM (Paper) link-prediction pipelines; RAG (retrieval-augmented generation) enhancement; cluster-mapping via semantic embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Cluster-level (LLM (Cluster)): present the model with a scientific-cluster label, the bipartite cluster-level graph links (past usage counts), and a prompt asking for k candidate AI method clusters; LLM outputs cluster names. Paper-level (LLM (Paper)): given a detailed problem description, retrieve n similar training papers (RAG, n∈{1,3,5}), provide examples to the LLM, ask it to generate K candidate AI methods (or vice versa for AI→Sci); generated textual descriptions are mapped to cluster labels using InstructorEmbedding + cosine similarity. Evaluation uses a temporal split (train: papers ≤2022, test: 2023-2024). Variants: Imitation, Direct, and RAG@K generation modes. Prompts and JSON output formats enforced.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Training split: 6,287 AI4Science publications (2014–2022) used for link-model training and retrieval; test split: 1,225 AI4Science publications (2023–2024) used for evaluation; full curated corpus = 162,656 publications.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_field</strong></td>
                            <td>Multidisciplinary (same science + AI venues as above); link prediction spans domains (biology, materials, climate, healthcare, robotics, neuroscience, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_laws_extracted</strong></td>
                            <td>Hypothesis-like candidate links and empirical patterns: proposed mappings (e.g., 'this AI method can be applied to that scientific problem'), cluster-level regularities (which AI techniques tend to address which problem clusters), and discovery of novel promising problem↔method pairings — operationally these are empirical conjectures and cross-domain application hypotheses rather than formal physical laws.</td>
                        </tr>
                        <tr>
                            <td><strong>example_laws_extracted</strong></td>
                            <td>Examples of concrete generated/validated links: (a) Graph Neural Networks → Protein Design and Function Prediction (materialized in later publications); (b) Molecular Dynamics Simulation algorithms → Polymer Dynamics and Chirality / Earth's Mantle Dynamics; (c) Multimodal Learning → Visual Perception and Neuroscience; (d) Candidate unexplored proposals: Reinforcement Learning → Carbon Emissions Mitigation; Graph Neural Networks → RNA-Protein Interaction prediction; Explainable AI → challenges in AI Ethics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>For cluster-level evaluation: compare predicted target clusters against ground-truth cluster links in the 2023–2024 test set; metrics = Precision@K, Recall@K, F1@K. For paper-level generation: use ROUGE-1-F, BLEURT, and cosine similarity of embeddings to reference method/problem descriptions; additionally count number of novel (unseen in training) links discovered and compare to new links that actually appeared in 2023–2024 publications.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>LLM-based models outperformed conventional baselines. Representative reported numbers: Sci→AI: LLM (Cluster) Precision@1 = 0.352 (higher than node2vec and Katz); LLM (Paper) Precision@1 ≈ 0.282. AI→Sci: LLM (Paper) Precision@1 = 0.377 (higher than LLM (Cluster), node2vec, Katz). RAG variants improved generation metrics (ROUGE/BLEURT/Cosine) as retrieval examples n increased. Human-grounded novel-link discovery: 2023–2024 introduced 683 new links; LLM (Paper) produced 639 novel links @1 and up to 8,276 novel links @10; LLM (Cluster) (API-controlled) produced thousands as well (see Table 5). Exact full tables reported in the paper's Tables 3–5 and Appendix C.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Baselines: Katz-index score-based link prediction and node2vec embedding-based link prediction. LLM methods (both Cluster and Paper) generally outperformed Katz and node2vec on Precision/Recall/F1@K. Node2vec outperformed Katz, suggesting non-local network structure matters. Imitation baselines (copy nearest neighbors) yielded high surface-level ROUGE but lower semantic similarity than generative LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) LLMs are effective at predicting plausible AI↔science links; LLM (Cluster) performs better when predicting AI solutions for a given scientific cluster (Sci→AI), while LLM (Paper) (RAG-enhanced) excels at predicting scientific problems that a given AI method could address (AI→Sci). 2) Retrieval augmentation (RAG) improves generation quality. 3) LLM-generated candidate links include many that later appear in the literature, indicating utility for research discovery. 4) Conventional graph methods (node2vec) help but are generally outperformed by LLM-based approaches that exploit textual semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Potential data leakage: gpt-4o's training cutoff (Oct 2023) may overlap with test-period publications, biasing results; authors note gpt-3.5's older cutoff (Sept 2021) yielded consistent LLM (Paper) performance suggesting limited leakage risk but still flagged the issue. Evaluation uses publications as ground truth, which may miss valid novel links that have not yet been published. Mapping generated free-text to clusters via embedding similarity can be noisy. Cluster-level LLM prompts are sensitive to context length and model reasoning ability (gpt-3.5 performed worse on cluster-level tasks). Limited model/hyperparameter exploration (not aiming to optimize link predictors), and cost/API limits when scaling RAG/LLM calls.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can LLMs Generate Novel Research Ideas? <em>(Rating: 2)</em></li>
                <li>MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>One embedder, any task: Instruction-finetuned text embeddings <em>(Rating: 2)</em></li>
                <li>Quantifying the benefit of artificial intelligence for scientific research <em>(Rating: 1)</em></li>
                <li>Oil & water? diffusion of ai within and across scientific fields <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4257",
    "paper_id": "paper-274763061",
    "extraction_schema_id": "extraction-schema-98",
    "extracted_data": [
        {
            "name_short": "GPT-4o-mini extraction",
            "name_full": "gpt-4o-mini-2024-07-18 (OpenAI GPT-4o mini) extraction pipeline",
            "brief_description": "An LLM-based extraction pipeline that uses GPT-4o mini to read paper titles and abstracts and produce structured extractions of Scientific Problem, AI Method, and AI Usage; these extractions are further embedded, clustered, and summarized to derive empirical patterns about the AI4Science literature.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o-mini-2024-07-18 (OpenAI GPT-4o mini)",
            "model_size": null,
            "method_name": "LLM-based extraction pipeline (six-aspect JSON extraction)",
            "method_description": "For each paper title+abstract the authors prompt GPT-4o-mini to output a JSON containing six aspects: Problem (keyword/keyphrase), Problem (definition), Problem discipline, Method (keyword/keyphrase), Method (definition), and AI Usage. Prompts enforced separation (no method info in problem fields and vice versa) and requested output in a fixed JSON schema. Extracted textual fields were embedded with InstructorEmbedding, projected (LargeVis), clustered with HDBSCAN, and cluster labels/summaries were produced by GPT-4o. A separate LLM prompt/classifier judged whether the paper is AI4Science. Default generation hyperparameters were used; human-in-the-loop verification was performed on a held-out sample.",
            "number_of_papers": "162,656 publications (2014-2024); extractions: 129,038 scientific-problem entries, 42,455 AI-method entries, 42,455 AI-usage entries (7,542 AI4Science publications)",
            "domain_or_field": "Multidisciplinary: top science journals (Nature, Science, PNAS, Nature Communications, Science Advances) and top AI conferences (AAAI, IJCAI, ICLR, ICML, NeurIPS, SIGKDD, WWW) — across biology, chemistry, medicine, earth science, materials, social science, etc.",
            "type_of_laws_extracted": "Empirical generalizations and semantic patterns about the AI4Science landscape: cluster-level thematic summaries, under-/well-explored problem-method pair distributions, hub-peripheral connectivity patterns in a problem↔method bipartite graph, and semantic cluster labels (i.e., domain-level regularities and empirical patterns rather than formal mechanistic laws).",
            "example_laws_extracted": "1) High-level empirical findings F1–F3 from the paper: (F1) Many scientific problems and AI methods are underexplored for AI4Science; (F2) Connectivity is highly imbalanced with 'hub' problems/methods linking broadly while many nodes are peripheral; (F3) AI and science communities prioritize different problems and methods. 2) Concrete underexplored examples identified: scientific problems such as 'Cancer Signaling Pathways' and 'Cancer Drug Resistance' and AI methods underutilized like 'Attention Mechanisms' and 'Transformer Architectures'.",
            "evaluation_method": "Human evaluation of LLM extractions on a held-out sample (two trained annotators per paper) comparing LLM extractions to human annotations; LLM AI4Science classification also compared to human labels. Cluster labeling and downstream analyses validated by qualitative inspection and by comparing distributional statistics (bipartite graph) against expectations.",
            "performance_metrics": "Extraction accuracy (human-evaluated) = 0.910 (91.0%); human agreement F1 on extraction tasks: 0.876 and 0.946 (two genres). AI4Science classification F1: scientific-journal papers = 0.836; AI-conference papers = 0.647 (with high recall 0.866 on AI papers). These numeric evaluations were reported in Appendix A.2 / Tables 8-9.",
            "comparison_baseline": "Compared against human expert annotations (two annotators) on a random subset; LLM extractions aligned well with human labels. The paper contrasts this LLM-based fine-grained extraction with prior coarse keyword / metadata-based tagging used in earlier quantitative studies (e.g., Gao & Wang [7], Duede et al. [6]).",
            "key_findings": "LLM extraction from titles and abstracts is accurate (≈91% human-judged); it scales to large corpora and enables finer-grained semantic analyses (embeddings + clustering) that reveal empirical patterns (underexplored regions, hubs). Using LLMs for cluster summarization produces understandable labels; the pipeline is reproducible and enabled construction of a large AI4Science dataset.",
            "challenges_limitations": "Extraction relied only on titles and abstracts (no full-text), which may miss methodological detail; potential selection bias from focusing on top-tier venues; difficulty distinguishing CS/AI research problems from 'traditional' science in conference papers (lower classification F1 for AI-conference papers); risk of LLM hallucination or subtle misclassification; model_size and internal training data of the LLM not controlled by authors; cost and API limits for large-scale extraction.",
            "uuid": "e4257.0",
            "source_info": {
                "paper_title": "Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "GPT-4o/gpt-3.5 link-predict",
            "name_full": "gpt-4o-2024-08-06 and gpt-3.5-turbo-0125 LLM pipelines for cluster- and paper-level link prediction (RAG-enhanced generative predictions)",
            "brief_description": "LLM-based link-prediction/generative systems that (a) predict AI methods for scientific problems at the cluster level given the bipartite graph context (LLM (Cluster)), and (b) generate paper-level candidate mappings from problems→methods or methods→problems using retrieval-augmented generation (LLM (Paper)), then map generations to clusters via embedding similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "gpt-4o-2024-08-06 (primary), gpt-3.5-turbo-0125 (additional generator)",
            "model_size": null,
            "method_name": "LLM (Cluster) and LLM (Paper) link-prediction pipelines; RAG (retrieval-augmented generation) enhancement; cluster-mapping via semantic embeddings",
            "method_description": "Cluster-level (LLM (Cluster)): present the model with a scientific-cluster label, the bipartite cluster-level graph links (past usage counts), and a prompt asking for k candidate AI method clusters; LLM outputs cluster names. Paper-level (LLM (Paper)): given a detailed problem description, retrieve n similar training papers (RAG, n∈{1,3,5}), provide examples to the LLM, ask it to generate K candidate AI methods (or vice versa for AI→Sci); generated textual descriptions are mapped to cluster labels using InstructorEmbedding + cosine similarity. Evaluation uses a temporal split (train: papers ≤2022, test: 2023-2024). Variants: Imitation, Direct, and RAG@K generation modes. Prompts and JSON output formats enforced.",
            "number_of_papers": "Training split: 6,287 AI4Science publications (2014–2022) used for link-model training and retrieval; test split: 1,225 AI4Science publications (2023–2024) used for evaluation; full curated corpus = 162,656 publications.",
            "domain_or_field": "Multidisciplinary (same science + AI venues as above); link prediction spans domains (biology, materials, climate, healthcare, robotics, neuroscience, etc.).",
            "type_of_laws_extracted": "Hypothesis-like candidate links and empirical patterns: proposed mappings (e.g., 'this AI method can be applied to that scientific problem'), cluster-level regularities (which AI techniques tend to address which problem clusters), and discovery of novel promising problem↔method pairings — operationally these are empirical conjectures and cross-domain application hypotheses rather than formal physical laws.",
            "example_laws_extracted": "Examples of concrete generated/validated links: (a) Graph Neural Networks → Protein Design and Function Prediction (materialized in later publications); (b) Molecular Dynamics Simulation algorithms → Polymer Dynamics and Chirality / Earth's Mantle Dynamics; (c) Multimodal Learning → Visual Perception and Neuroscience; (d) Candidate unexplored proposals: Reinforcement Learning → Carbon Emissions Mitigation; Graph Neural Networks → RNA-Protein Interaction prediction; Explainable AI → challenges in AI Ethics.",
            "evaluation_method": "For cluster-level evaluation: compare predicted target clusters against ground-truth cluster links in the 2023–2024 test set; metrics = Precision@K, Recall@K, F1@K. For paper-level generation: use ROUGE-1-F, BLEURT, and cosine similarity of embeddings to reference method/problem descriptions; additionally count number of novel (unseen in training) links discovered and compare to new links that actually appeared in 2023–2024 publications.",
            "performance_metrics": "LLM-based models outperformed conventional baselines. Representative reported numbers: Sci→AI: LLM (Cluster) Precision@1 = 0.352 (higher than node2vec and Katz); LLM (Paper) Precision@1 ≈ 0.282. AI→Sci: LLM (Paper) Precision@1 = 0.377 (higher than LLM (Cluster), node2vec, Katz). RAG variants improved generation metrics (ROUGE/BLEURT/Cosine) as retrieval examples n increased. Human-grounded novel-link discovery: 2023–2024 introduced 683 new links; LLM (Paper) produced 639 novel links @1 and up to 8,276 novel links @10; LLM (Cluster) (API-controlled) produced thousands as well (see Table 5). Exact full tables reported in the paper's Tables 3–5 and Appendix C.",
            "comparison_baseline": "Baselines: Katz-index score-based link prediction and node2vec embedding-based link prediction. LLM methods (both Cluster and Paper) generally outperformed Katz and node2vec on Precision/Recall/F1@K. Node2vec outperformed Katz, suggesting non-local network structure matters. Imitation baselines (copy nearest neighbors) yielded high surface-level ROUGE but lower semantic similarity than generative LLM outputs.",
            "key_findings": "1) LLMs are effective at predicting plausible AI↔science links; LLM (Cluster) performs better when predicting AI solutions for a given scientific cluster (Sci→AI), while LLM (Paper) (RAG-enhanced) excels at predicting scientific problems that a given AI method could address (AI→Sci). 2) Retrieval augmentation (RAG) improves generation quality. 3) LLM-generated candidate links include many that later appear in the literature, indicating utility for research discovery. 4) Conventional graph methods (node2vec) help but are generally outperformed by LLM-based approaches that exploit textual semantics.",
            "challenges_limitations": "Potential data leakage: gpt-4o's training cutoff (Oct 2023) may overlap with test-period publications, biasing results; authors note gpt-3.5's older cutoff (Sept 2021) yielded consistent LLM (Paper) performance suggesting limited leakage risk but still flagged the issue. Evaluation uses publications as ground truth, which may miss valid novel links that have not yet been published. Mapping generated free-text to clusters via embedding similarity can be noisy. Cluster-level LLM prompts are sensitive to context length and model reasoning ability (gpt-3.5 performed worse on cluster-level tasks). Limited model/hyperparameter exploration (not aiming to optimize link predictors), and cost/API limits when scaling RAG/LLM calls.",
            "uuid": "e4257.1",
            "source_info": {
                "paper_title": "Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas?",
            "rating": 2,
            "sanitized_title": "can_llms_generate_novel_research_ideas"
        },
        {
            "paper_title": "MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows",
            "rating": 2,
            "sanitized_title": "massw_a_new_dataset_and_benchmark_tasks_for_aiassisted_scientific_workflows"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "One embedder, any task: Instruction-finetuned text embeddings",
            "rating": 2,
            "sanitized_title": "one_embedder_any_task_instructionfinetuned_text_embeddings"
        },
        {
            "paper_title": "Quantifying the benefit of artificial intelligence for scientific research",
            "rating": 1,
            "sanitized_title": "quantifying_the_benefit_of_artificial_intelligence_for_scientific_research"
        },
        {
            "paper_title": "Oil & water? diffusion of ai within and across scientific fields",
            "rating": 1,
            "sanitized_title": "oil_water_diffusion_of_ai_within_and_across_scientific_fields"
        }
    ],
    "cost": 0.016849,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science</p>
<p>Yutong Xie yutxie@umich.edu 
Hua Xu hua.xu@yale.edu 
Qiaozhu Mei qmei@umich.com </p>
<p>University of Michigan Ann Arbor
MichiganUSA</p>
<p>University of Michigan Ann Arbor
MichiganUSA</p>
<p>Yale University New Haven
ConnecticutUSA</p>
<p>University of Michigan Ann Arbor
MichiganUSA</p>
<p>Bridging AI and Science: Implications from a Large-Scale Literature Analysis of AI4Science
29981128331998C3498709A3A832A146CCS ConceptsComputing methodologies → Artificial intelligence• Applied computing• Information systems → Web miningCollaborative and social computing systems and toolsAI for Science, Large-Scale Literature Analysis, Link Prediction
Artificial Intelligence has proven to be a transformative tool for advancing scientific research across a wide range of disciplines.However, a significant gap still exists between AI and scientific communities, limiting the full potential of AI methods in driving broad scientific discovery.Existing efforts in identifying and bridging this gap have often relied on qualitative examination of small samples of literature, offering a limited perspective on the broader AI4Science landscape.In this work, we present a large-scale analysis of the AI4Science literature, starting by using large language models to identify scientific problems and AI methods in publications from top science and AI venues.Leveraging this new dataset, we quantitatively highlight key disparities between AI methods and scientific problems, revealing substantial opportunities for deeper AI integration across scientific disciplines.Furthermore, we explore the potential and challenges of facilitating collaboration between AI and scientific communities through the lens of link prediction.Our findings and tools aim to promote more impactful interdisciplinary collaborations and accelerate scientific discovery through deeper and broader AI integration.Our code and dataset are available at: https://github.com/charles-pyj/Bridging-AI-and-Science.</p>
<p>Introduction</p>
<p>The 2024 Nobel Prizes in Physics and Chemistry were both awarded to Artificial Intelligence (AI) researchers.In particular, the developers of AlphaFold [10] were recognized for their groundbreaking work, which is based on the Transformer network and has revolutionized the prediction of protein structures, providing a transformative approach to solving complex biological problems.This compellingly exemplifies how AI has emerged as a powerful tool to facilitate scientific research across various disciplines [9,13,18,33,35].* These authors contributed equally to this work.</p>
<p>However, despite the transformative potential of using AI methods for solving scientific problems (AI4Science), a considerable gap persists between AI and scientific communities, hindering the full exploitation of AI for scientific discovery: On the one side, advanced AI methodologies might remain mysterious or underutilized by scientists; On the other side, AI researchers may lack awareness of the specific challenges and potential applications in scientific domains, missing opportunities for interdisciplinary collaborations.</p>
<p>These challenges invoke a few key research questions: What does the AI4Science landscape look like?Are AI methods and scientific problems evenly interconnected, or are some of them underexplored?And how could we expand the connections?</p>
<p>Existing efforts to approach these questions have predominantly involved small-scale, qualitative reviews of AI's application in science [13,33], particularly in specialized areas such as drug discovery and material science [9,18,35].These qualitative analyses of the literature often rely on heuristic insights from domain experts to suggest potential uses of AI in solving scientific problems.While valuable, such focused reviews are limited in providing a comprehensive and diverse perspective on the AI4Science landscape.</p>
<p>In the mean time, recent research from the Science of Science community has initialized larger-scale studies that quantify AI's impact on publications in scientific fields [6,7], although they often heavily rely on established scientific taxonomies in "AI-heavy" fields.There remains a lack of delivering a holistic, dynamic, and data-driven overview of AI4Science.Such analysis is crucial for understanding the barriers and identifying new opportunities for deep engagement of fast-evolving AI methodology in scientific research.</p>
<p>Aiming at addressing these key questions, we conduct a largescale and comprehensive analysis of relevant literature over the past decade.We start by using large language models (LLMs) to identify the scientific problems and AI methods addressed in publications from top science and AI venues, through which we assemble a novel and balanced AI4Science dataset to analyze the role of AI in scientific research (Sec.2).Leveraging this dataset, we illustrate the AI4Science landscape through two projection maps (Fig. 2) and a bipartite graph (Fig. 4a).We further quantitatively reveal the current discrepancies between AI and science, leading to several key findings and implications (Sec.3).Finally, through the lens of link prediction, we demonstrate the potential and challenges in advancing the bridge between AI and science (Sec.4).Our findings and tools may foster more effective interdisciplinary collaborations arXiv:2412.09628v2[cs.AI] 18 Feb 2025 and accelerate scientific discovery through a more diverse and deeper integration of AI methods.</p>
<p>A New Dataset and the AI4Science Landscape</p>
<p>A major challenge in understanding the disparities and potential of AI4Science is the lack of a comprehensive and balanced dataset.Existing literature data are often biased toward successful stories, such as areas where AI and science are already deeply intertwined.Or they reflect the perspective of either the scientific or the AI communities, but not both.We take the initiative to create a broad and balanced large-scale literature dataset that provides a wideangled lens for understanding AI4Science research.</p>
<p>Curation of AI4Science Literature</p>
<p>We collect publications from leading science and AI venues to offer a balanced view from both communities:</p>
<p>• For science domains, we include three top multidisciplinary journals that represent cutting-edge and high-quality research (Nature, Science, and PNAS) and two of their subjournals (Nature Communications and Science Advances).• For AI communities, we include seven top conferences from the list of CSRankings.org, comprising two AI-focused venues (AAAI, IJCAI ), three machine learning venues (ICLR, ICML, NeurIPS), and two venues of applied AI research (SIGKDD, WWW ).</p>
<p>Our data includes a total of 162,656 publications from 2014 to 2024.Appendix A.1 provides details of the data curation process.</p>
<p>Extracting Scientific Problems and AI Methods with LLMs</p>
<p>To understand how AI is applied to science, we need to extract paired entries of scientific problems and AI solutions from relevant publications.Traditionally, this is accomplished through literature reviews and surveys which involve manual examinations of small samples of papers [13,33], a process that is labor-intensive and timeconsuming.Recent quantitative studies in the Science of Science field have started using metadata entries, such as keywords, to identify AI usage and categorize scientific topics [6,7].While this method enables larger-scale analysis, it offers only a coarse-grained view, potentially missing the nuanced details of scientific problems and AI methods.Additionally, it can suffer from issues such as incomplete data entries, limiting the effectiveness of analysis.In contrast, our work leverages large language models (LLMs) to extract detailed descriptions of scientific problems and AI methods from publication titles and abstracts, allowing for a more nuanced, scalable, and data-driven analysis.Such LLM-based extraction techniques have also been employed in recent studies for extracting aspects from scientific literature, such as Zhang et al. [37].</p>
<p>Particularly, for each publication of interest, we use OpenAI GPT-4o mini to extract the following three key aspects:</p>
<p>• Scientific Problem (  ): The primary scientific problem addressed in the paper, including both a keyword or keyphrase summarizing the problem and a detailed definition.• AI Method (  ): The main AI method applied in the paper, including a keyword or keyphrase summarizing the method and a detailed description.</p>
<p>• AI Usage (  ): A detailed explanation of how the AI method is specifically applied to the scientific problem.It is possible that a publication did not address a scientific problem or did not use an AI method, in which case the corresponding field(s) would remain empty.If a paper addresses a scientific problem using an AI method -i.e.,   ,   , and   are all non-empty -it is considered as an AI4Science work.Fig. 1 presents an example of the extraction results of an AI4Science publication.</p>
<p>To assess the reliability of GPT extractions, we conducted a small-scale human evaluation on 100 papers.Each extraction record was reviewed by at least two annotators, resulting in an average accuracy of 91.0%.For more details on the extraction prompts, additional examples of the extracted data, and verification with human annotations, please refer to Appendix A.2.</p>
<p>Semantic Projection and Clustering</p>
<p>One of the key advantages of this new AI4Science dataset compared to the data used in existing studies [6,7] is that our dataset contains detailed textual descriptions of the identified scientific problems and AI methods rather than pre-defined taxonomies, allowing for a deeper semantic analysis of their relationships.</p>
<p>To achieve this, we generate semantic embeddings for the scientific problems {  } and AI methods {  } using the InstructorEmbedding model [28].We then apply LargeVis [30] to project these high-dimensional embeddings into 2D coordinates.Next, a densitybased clustering method, HDBSCAN [3] is utilized to group similar scientific problems and AI methods in this 2D space.For each cluster, we use GPT-4o to generate an understandable summarization based on frequent keywords and samples of extraction results.This process results in 390 and 355 clusters respectively, with corresponding cluster labels   and   to each data point.The choice of the methods and the details of the process are in Appendix A. 3.</p>
<p>Fig. 2 shows the 2D maps that illustrate the AI4Science landscape.Fig. 5-6 in Appendix present the maps with the cluster labels.</p>
<p>Unlike using keywords or categorizations from publication metadata [6,7], our approach employs a data-driven methodology.By using textual embedding, clustering, and topic labeling, relevant AI methods discussed in both scientific and AI literature are grouped together, as are the scientific problems mentioned in both fields.This enables a semantic understanding of the AI4Science space and ensures greater consistency and generalizability of the data.</p>
<p>Dataset Overview and the Bipartite Graph</p>
<p>Through LLM-based extraction and semantic clustering, we form a comprehensive dataset, D = {(  ,   ,   ,   ,   )}  =1 , which includes the scientific problems, AI methods, their corresponding cluster labels, and the usage of AI for science.The basic statistics of this dataset are listed in Table 1.</p>
<p>To better illustrate the connections between AI and science, we construct a bipartite graph based on the clusters.In this bipartite graph, scientific problem clusters and AI method clusters are represented as two types of nodes, and the publications act as edges connecting them.Formally, the graph is defined as G = (V, E), where V = {  } ∪ {  } and E = D.As shown in Fig. 4a, the graph provides an intuitive visual representation of how AI is linked to scientific challenges and vice versa.The extractions are then semantically projected, clustered, and labeled to form the AI4Science landscape (Fig. 2), as well as to construct the bipartite graph (Fig. 4a).  3 Discrepancy Between AI and Science</p>
<p>Both Fig. 2 and 4a show a noticeable pattern of discrepancy between AI and science.This section quantitatively assesses these disparities, providing a deeper understanding of the gaps.</p>
<p>Uneven Distribution of AI4Science Research</p>
<p>The projection maps in Fig. 2 illustrate the distribution of AI4Science work within the semantic spaces of scientific problems and AI methods (the green dots among orange and purple dots).The uneven distributions suggest that AI4Science work is heavily clustered in certain subareas, while it remains underrepresented in wide ranges of both the problem and the method spaces.In Fig. 3, we plot the clusters of scientific problems and AI methods, showing the relation between cluster sizes and the presence of AI4Science publications within each cluster.The slopes of the regression lines represent the average proportions of AI4Science work in the literature of each scientific problem cluster or AI method cluster.Clusters below the regression line indicate problems or AI methods in which the integration of AI4Science is underrepresented (noted the under-explored regions), suggesting potential opportunities for further interdisciplinary collaboration.</p>
<p>Underexplored scientific problems.In the bottom-right section of Fig. 3a Response', 'Cancer Signaling Pathways', and 'Cancer Drug Resistance'.Scientific problems in this region could benefit from more engagement of AI methods.</p>
<p>Underutilized AI methods.Similarly, Fig. 3b reveals AI techniques that have not yet been widely applied to sciences, including: 'Attention Mechanisms', 'Gradient-Based Methods', 'Graph Embedding', 'Regularization Methods', 'Transformer Architectures', and 'Contrastive Learning'.These techniques could have been utilized in a wider range of scientific applications.</p>
<p>Uneven Distribution of Bipartite Links</p>
<p>Fig. 4 reveals an uneven distribution of node sizes in the bipartite graph, indicating the presence of "hub" and "peripheral" nodes among both scientific problems and AI methods.This implies that certain scientific challenges and AI techniques play a central role in interdisciplinary AI4Science work, while others are less explored.</p>
<p>Quantitatively, heavy-tailed degree distributions are observed as visualized in Fig. 4(b-c).Specifically, the degrees for AI method nodes follow a log-normal-like distribution, while the degree distribution for scientific problem nodes exhibits an even heavier tail, though not as extreme as a power-law distribution.These findings support the idea that a small number of "hubs" are linked to a large variety of AI methods or scientific problems, while many other nodes remain peripheral to these interdisciplinary connections.Heavy-tailed distributions, particularly log-normals, are also commonly observed in other bipartite networks [32], such as documentterm graphs [34], and in collaborative networks [22,29].This suggests that the structure of the AI4Science bipartite graph reflects a general pattern of interdisciplinary and collaborative work.</p>
<p>Table 13 in Appendix lists the "hubs" with the highest node degrees, representing the most interconnected scientific problems and AI methods.From the table, we observe that the scientific problems linked to the widest variety of AI methods are primarily computational or data-processing problems, such as: 'Neural    On the AI methods side, the techniques applied to the greatest number of scientific challenges include general methods such as: 'Machine Learning', 'Deep learning Models', 'Neural Networks', 'Classification Methods', 'Data Analysis'.In addition, there are specialized techniques tailored to scientific domains, including: 'Genomic Analysis Methods', 'Computational Drug Screening', 'Molecular Dynamics Simulations', 'Protein Structure Design', and 'Molecular Structural Design'.</p>
<p>We further break down the node degree distribution based on cluster partitions.Specifically, we calculate the average node degree for well-explored and under-explored clusters, as identified earlier in Sec.3.1.The statistics are listed in Table 2.We observe a higher average degree for well-explored scientific problems and AI methods.This suggests that under-explored regions not only have fewer AI4Science publications, but also exhibit a lesser variety, or limited connectivity in the bipartite graph.</p>
<p>Discrepancy Between the AI and Science Communities</p>
<p>Our dataset includes publications from both scientific journals and AI conferences, allowing us to explore how these two communities approach the integration of AI into science in distinct ways.Table 14 in Appendix lists the top clusters emphasized by each community.In terms of facilitating scientific discovery with AI, the science community places greater emphasis on challenges such as: 'Single-cell RNA Sequencing', 'Mechanical Properties of Materials', 'Protein Design and Function Prediction', 'Cancer Detection', and 'Genetic Variants and Traits'; In contrast, the AI community tends to focus more on areas like: 'Urban Traffic Management', 'Electronic Health Data Challenges', 'Statistical Inference', 'Market Pricing and Allocation', 'Soft Robotics', etc.</p>
<p>When it comes to applying AI methods to scientific problems, the two communities also show different preferences.In addition to general approaches like 'Machine Learning' and 'Deep Learning Models', the science community frequently employs AI techniques specifically tailored for scientific challenges, such as: 'Genomic Analysis Methods', 'Molecular Dynamics Simulations', and 'Computational Drug Screening'.In comparison, the AI community utilizes a broader range of general AI methodologies, covering diverse techniques such as: 'Reinforcement Learning', 'Attention Mechanisms', 'Policy Gradient Methods', 'Multitask Learning', 'Spatio-Temporal Analysis', 'Causal Inference', 'Self-Supervised Learning', and 'Unsupervised Learning'.</p>
<p>In addition, by comparing Table 14 with Table 13, we also find that the scientific problems addressed by the AI community, tend to be connected to a wider variety of AI methods.Similarly, AI methods frequently used by the science community are applied across a broader range of scientific challenges.Similarly, AI methods frequently used by the science community are also more likely to be applied across a broader range of scientific challenges.This highlights the discrepancy between the two communities, where the science community engages with a limited range of AI methods compared to the AI community, and vice versa.</p>
<p>The distribution of AI4Science work published in scientific journals and AI conferences in the semantic maps (Fig. 7 in Appendix) also illustrates the different focuses of these two communities.</p>
<p>Findings and Implications</p>
<p>Through our analysis of the distribution of AI4Science work, bipartite graph node degrees, and the discrepancies between the AI and science communities, we derive the following key findings: (F1) Different AI and science subdomains exhibit varying degrees of engagement in AI4Science research, leaving a substantial number of scientific problems and AI methods underexplored in the collaboration context (Sec.3.1); (F2) The connectivity between scientific problems and AI methods is highly imbalanced, with certain nodes acting as "hubs" while other peripheral nodes are less connected (Sec.3.2); (F3) The science and AI communities take distinct approaches to integrate AI into scientific research, prioritizing different problems and methods (Sec.3.3).</p>
<p>These key findings lead to two important implications for fostering a wider and deeper exploration of AI4Science:</p>
<p>(I1) More attention should be directed toward exploring the underexplored areas, incorporating a broader range of scientific challenges and AI techniques into the AI4Science landscape.(I2) Efforts should be made to discover new connections between AI and science, uncovering innovative ways to apply AI methodologies to scientific research.</p>
<p>In the following section, we present preliminary explorations of addressing these implications through link prediction.</p>
<p>4 Bridging AI and Science:</p>
<p>Through the Lens of Link Prediction</p>
<p>Building on the findings and implications above, we dive deeper into the potentials and challenges of advancing the connections between AI and science from the perspective of link prediction.</p>
<p>Methodology</p>
<p>In the curated AI4Science dataset D = {(  ,   ,   ,   ,   )}  =1 , the extracted and clustered scientific problems {  } and {  }, as well as AI methods {  } and {  }, represent the respective landscapes of these two domains.The AI4Science publications, along with their descriptions of AI usage {  }, highlight the connections between scientific challenges and AI methods.To model these connections, we employ the link prediction formulation.</p>
<p>Data.We split the data into training and test sets based on publication dates.The training set includes papers published between 2014 and 2022 (6287 AI4Science publications), which are used for training models, retrieving data points for generation augmentation, and identifying well/under-explored areas, as illustrated in Fig. 3.The testing set consists of publications from 2023 and 2024 (1225 AI4Science publications), serving as the ground truth for evaluating link prediction models.The statistics are provided in Table 15 in Appendix.Note that the analyses prior to this section involve the full AI4Science dataset, spanning the period from 2014 to 2024.</p>
<p>Model and evaluation.We employ two types of models to predict links between scientific problems and AI methods.The first category directly make predictions on the cluster level with conventional bipartite link prediction models [21], such as those based on the Katz index [11] and node2vec embeddings [8] or large language models (LLMs).Formally, given the cluster-level bipartite graph constructed on the training data and a source node   (or   ), the model predicts  target nodes that could be potentially linked to the source node.We then compare the predicted nodes with the ground-truth target node set in the test data and report Precision, Recall, and F1 scores @K [1,36].Note that existing links may also appear in the ground-truth set, because a new AI4Science work may repeat an existing connection between a scientific problem and an AI method.Specifically, when using LLM for link prediction, the bipartite links to the source nodes are provided in prompts, and the model is referred to as 'LLM (Cluster)'.</p>
<p>In addition, we also utilize LLMs for paper-level generative predictions.Specifically, given a scientific problem   from cluster   , we prompt the LLM to generate potential AI methods   that could be applied.These generated method descriptions are then mapped to cluster labels   through embedding similarity.Similarly, given an AI method   , we prompt the LLM to generate potential scientific problems   that it could address.The prediction process is further enhanced by retrieving similar papers from the training set using retrieval-augmented generation (RAG) [14].The model is referred to as 'LLM (Paper)'.</p>
<p>We use OpenAI's gpt-4o-2024-08-06 as the LLM model and report its performance in the main text.Additionally, we also tested the gpt-3.5-turbo-0125model as the generator, given that its knowledge is limited to September 2021, ensuring no risk of data leakage.Our results show that the performance of LLM (Paper) remains consistent, suggesting a low risk of data leakage.However,</p>
<p>Setting</p>
<p>Method Precision Recall F1 @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10 Sci → AI Method Region @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10</p>
<p>Sci the performance of LLM (Cluster) declines, likely due to GPT-3.5's limited ability of reasoning and processing long contexts.For more detailed descriptions of the models and evaluation metrics, please refer to Appendix C.</p>
<p>Overall Link Prediction Results</p>
<p>Table 3 presents the models' performances on the test data, where LLM-based link prediction methods outperform conventional approaches in most settings, highlighting their strong potential for predicting AI4Science research directions.</p>
<p>Notably, LLM (Cluster) performs better in the Sci→AI setting (predicting AI solutions for scientific problems), while LLM (Paper) excels in the AI→Sci setting.This contrast suggests two complementary strategies for conducting AI4Science research: (1) Given a scientific problem, broadly exploring AI techniques at a higher level can help identify potential solutions.(2) Given an AI method, starting from the method itself and its relevant publications can help determine which scientific problems it could be applied to.</p>
<p>For conventional link prediction methods, while both Katz and node2vec rely on suggesting similar nodes to link to, node2vec outperforms Katz, suggesting the underlying mechanism connecting scientific problems and AI methods goes beyond homophily in the local neighborhood and potentially considers the broader structures of the AI4Science network.This finding further highlights the potential utility of the AI4Science bipartite graph.</p>
<p>Well-and Under-Explored Regions</p>
<p>Fig. 3 reveals the presence of well-and under-explored areas in both the scientific problem and AI method spaces.Echoing Implication (I1), we comparatively explore the potentials and challenges in facilitating interdisciplinary work for these regions.</p>
<p>Formally, we refer to scientific problem clusters or AI method clusters above the regression lines as well-explored areas, while those that fall below the lines are the under-explored regions.Clusters that reside within the 95% confidence intervals of the regression results are excluded from this analysis.Note that for this experiment, we use only the publications in the training split to decide the well-and under-explored regions.</p>
<p>Table 4 compares the link prediction results for well-explored and under-explored clusters.In both categories of link prediction methods, we observe a similar trend: Precision scores are generally higher for well-explored scientific problems and AI methods, even as K increases.In contrast, Recall is relatively higher for underexplored clusters.These results are consistent with the node degree statistics in Table 2, where the average degree of under-explored nodes is much lower, making it easier for the models to predict links, resulting in a higher Recall.</p>
<p>Additionally, in the under-explored partition, the rapid decline in Precision as K increases suggests that link prediction models are capable of discovering novel links that have not been actually explored by researchers in the test period.This highlights great opportunities for using "AI" to facilitate the expansion of the connections between AI methods and scientific challenges.</p>
<p>Discovering Novel Links</p>
<p>Inspired by the capability of link prediction models to uncover novel links, we further explore the newly discovered connections in response to Implication (I2).</p>
<p>Compared to the training data up to 2022, publications during 2023 and 2024 actually identified 683 new links between scientific problems and AI methods.In contrast, link prediction methods exhibit the potential to discover even more connections, especially as K increases, as shown in approaches exhibit strong potential in generating new links, even with a relatively small number of predictions per data point (e.g., @1).This observation aligns with recent research suggesting that LLMs are capable of proposing novel research ideas [27].Interestingly, LLM (Paper) demonstrates a stronger capability in discovering novel links compared to LLM (Cluster), even when the number of API calls is controlled.This may be due to the greater diversity of paper-level inputs, leading to more varied outputs.Many of the predicted links have already materialized into publications, such as applying 'Graph Neural Networks' to 'Protein Design and Function Prediction' [20], using algorithms for 'Molecular Dynamics Simulations' to tackle challenges in 'Polymer Dynamics and Chirality' [12] and 'Earth's Mantle Dynamics' [16], or leveraging 'Multimodal Learning' for 'Visual Perception and Neuroscience' [4].Link prediction also highlights numerous unexplored but promising directions, such as using 'Reinforcement Learning' for 'Carbon Emissions Mitigation', applying 'Graph Neural Networks' to predict 'RNA-Protein Interactions', and using 'Explainable AI' to address challenges in 'Artificial Intelligence Ethics'.</p>
<p>Related Work 5.1 Qualitative Literature Reviews of AI4Science</p>
<p>To identify potential applications of AI techniques in science, researchers typically turn to qualitative literature reviews of relevant papers.These reviews include general explorations of AI's usage across various scientific domains [13,33], as well as more focused examinations in specific fields such as drug discovery [2,5,18,24], materials design [9,15,26], and social sciences [35].</p>
<p>While these reviews provide valuable insights, they often rely on interdisciplinary experts to manually identify and categorize scientific problems and potential AI methods, limiting their scope.Such an approach often could not offer a comprehensive and datadriven overview of the broader AI4Science landscape.</p>
<p>Quantitative Literature Analysis</p>
<p>In addition to heuristic and qualitative literature reviews, a few recent papers from the Science of Science community have focused on quantitatively analyzing the AI4Science publications.For instance, Gao and Wang [7] discovered that AI use is prevalent across various scientific disciplines, and papers incorporating AI tend to receive a citation advantage.Duede et al. [6] explored AI's engagement in different fields, uncovering an "oil-and-water" phenomenon -while AI-engaged work is spreading across disciplines, it does not integrate well with other work within each specific field.</p>
<p>Although these studies provide valuable insights into AI4Science, they primarily focus on areas where AI is already applied to the scientific problems, overlooking the broader and underexplored parts of the landscape.Moreover, these approaches often rely on predefined keywords or n-grams to tag AI4Science research, leading to a taxonomy that provides only a coarse representation of the AI and science space.This method fails to capture the actual semantics and nuances of the language, ultimately limiting the depth of analysis.</p>
<p>By contrast, our study provides a more comprehensive view by analyzing literature from both AI and scientific venues over the past decade, encompassing not only AI4Science but also non-AI4Science publications.We leverage LLMs to extract detailed textual descriptions, enabling a nuanced and scalable data-driven categorization that facilitates deeper analysis.Our depiction of the landscape, quantitative analysis of discrepancies, and link prediction experiments offer fresh insights into the challenges and opportunities in bridging AI and science.</p>
<p>Limitations, Challenges, and Future Work</p>
<p>Potential bias in data selection.As an emerging and interdisciplinary field, cutting-edge and influential AI4Science studies often originate from prominent venues of general interest.For instance, groundbreaking work such as the AlphaFold [10] was published in Nature.Based on this observation, we have focused on toptier venues from the last decade.While this ensures high-quality and impactful contributions, it could introduce a selection bias that may overlook significant contributions from less prominent venues, potentially resulting in an incomplete representation of the AI4Science landscape.Given the flexibility of our analysis framework, researchers can integrate additional data sources from lessprominent venues to achieve a more representative view.</p>
<p>LLM-based extraction from titles and abstracts.The LLM-based extraction in this study relies solely on paper titles and abstracts, due to limited access to full texts, challenge of processing PDF formats, relatively low signal-to-noise ratio in full texts, and cost considerations.Our approach is comparable to the existing AI4Science literature analysis efforts, which also focus on the title and abstract as in the metadata [6,7].While key research problems and methods are often described in abstracts, and human evaluation confirms the accuracy of LLM extraction (Appendix A.2), we may miss details presented in the full text of publications.Future work can build upon our general and flexible framework to incorporate fulltext analysis, enriching the extracted content and capturing more nuanced AI methodologies used in scientific research.</p>
<p>Evaluation of link prediction.Another limitation of our study is the reliance on publication data as the ground truth for evaluating link prediction models.While this provides a reliable benchmark, it may not fully capture the breadth of potential AI4Science connections, and overlook novel or unconventional links that the models might suggest.Future work could consider incorporating more comprehensive evaluation strategies, such as expert reviews or realworld validation, to assess the effectiveness of the link predictions.</p>
<p>Limited exploration in link prediction models.Our goal is not to optimize the link prediction model, but rather to use link prediction as a lens to demonstrate the potential of connecting AI methods and scientific problems.As a result, we only included a few straightforward link prediction models, leaving room for further research to improve the performance of AI4Science link prediction or building recommender systems for AI methods or scientific problems.</p>
<p>Conclusion</p>
<p>This paper aims to depict the AI4Science landscape, identify current gaps between AI and science, while exploring potential ways to bridge them.We introduce a comprehensive, large-scale dataset of curated AI4Science publications, with scientific problems and AI methods extracted using large language models.Through quantitative analysis of this dataset, we uncover several key disparities:</p>
<p>(1) Different AI and science subdomains exhibit varying degrees of engagement in AI4Science research, leaving a substantial number of scientific problems and AI methods underexplored; (2) The connectivity between scientific problems and AI methods is highly imbalanced, with certain nodes acting as "hubs" while other peripheral nodes are less connected; (3) The science and AI communities take distinct approaches to integrating AI into scientific research, prioritizing different problems and methods.We further investigate the potential and challenges of fostering AI4Science collaboration through the lens of link prediction.The experiment results demonstrate great opportunities for using link prediction models to explore the under-investigated scientific problems and AI methods, as well as discover novel connections.We anticipate that our findings and tools will provide valuable insights to enhance interdisciplinary collaboration and accelerate scientific discovery through the integration of AI.</p>
<p>A AI4Science Dataset</p>
<p>A.2 Extraction with LLMs</p>
<p>Extracting key aspects.As aforementioned, we asked large language models to extract the following three key aspects: Scientific Problem, AI Method, Usage.To help LLM better understand the three aspects, we further break down the three key aspects into the following six aspects when prompting the model:</p>
<p>• Scientific Problem (keyword/keyphrase): A few keywords or key phrases that summarize the primary scientific problem addressed in the paper.• Scientific Problem (definition): A detailed definition of the primary scientific problem addressed in the paper.</p>
<p>• Scientific Problem discipline: The scientific discipline that the primary scientific problem addressed in the paper falls into.• AI method (keywords/keyphrase): A few keywords or key phrases that summarize the AI method used in the paper.• AI method (definition): A detailed definition of the AI method used in the paper.</p>
<p>• AI Usage: A detailed explanation of how the AI method is specifically applied to the scientific problem.</p>
<p>Prompting Template.We used the following prompt to extract the six key aspects of a paper: ## Background and Task Description You are an expert in both science and artificial intelligence (AI), where AI generally refers to intelligence exhibited by machines (particularly computer systems), including models, algorithms, etc.Given a scientific paper, your task is to extract the following aspects from the title and abstract: Please output the extraction results in the JSON format as below.Fields could be "N/A" if no relevant information can be found in the paper title and abstract.{ "Problem (keyword/keyphrase)": "... ", "Problem (definition)": "... ", "Problem Discipline": "... ", "Method (keyword/keyphrase)": "... ", "Method (definition)": "... ", "Usage": "... " } Classifying AI4Science.Given the extracted key aspects of papers, we leveraged the following prompt to ask LLM to judge whether the paper solves scientific problems and whether the paper uses AI methods: ## Task Description Given the extraction results of a research paper, please determine if the main research problem is a scientific problem from traditional disciplines in Science (not including disciplines like Computer Science and Information Science), and if the main method involves the use of Artificial Intelligence.Human evaluation.To evaluate LLM's ability as an annotator for scholarly papers, we conducted a human evaluation of 50 papers from scientific journals papers and 50 papers from AI conferences, which were randomly selected within our dataset.</p>
<p>Two trained human experts who are familiar with reading scientific literature are assigned to annotate key aspects extraction of each paper, based on the title and abstract, following a carefully designed codebook.They are asked to judge whether the LLM extraction of key aspects is accurate.After that, they are asked whether the paper of interest is an AI4Science paper.</p>
<p>The results are shown in Table 8 9, in which the human agreement is calculated by treating one annotation as the reference and the other as the prediction for each paper.The average extraction accuracy reported by human annotators is 0.910.Together with high human agreement F1 scores (0.876 and 0.946 respectively), this shows that LLM extractions of key aspects are aligned with human preferences.The evaluations for AI4Science classification are also reported, where the F1 score reaches 0.836 for publications from scientific journals.For publications from AI conferences, the F1 score is relatively low (0.647), because of the difficulty in distinguishing research problems in Computer Science domains and general Science domains.However, the high Recall score (0.866) provides a guarantee of avoiding missing important AI4Science work.The difficulty of AI4Science classification is further illustrated by relatively low human agreement(F1 score of 0.632 for both genres), which shows the lack of universally accepted standard of AI4Science.Moreover, this shows that our LLM extraction results are more consistent compared with a group of human annotators.</p>
<p>The high extraction accuracy indicated by our human evaluation also aligns with human evaluation results from recent studies [37], where LLMs have been used to extract scientific workflows from titles and abstracts.</p>
<p>A.3 Semantic Clustering</p>
<p>Embedding.After obtaining the six key aspects of the papers, we first leverage the InstructionEmbedding model (instructor large) [28] to obtain the semantic embeddings of the scientific problems and AI methods extracted.InstructorEmbedding is an advanced embedding model that generates task-and domain-specific text embeddings without additional training.Given our need to embed both scientific problems and AI methods, we use InstructorEmbedding with instructions below to provide contextualized embeddings tailored to our task rather than context-free embedding methods For scientific problems, we embed the problem keywords/keyphrases concatenated with the problem definition with the following instructions:</p>
<p>Represent the keyphrase and definition of a scientific problem for clustering and visualizing scientific problems For AI methods, we embed the AI method keywords/keyphrases concatenated with the AI method definition with the following instructions:</p>
<p>Represent the Artificial Intelligence method paragraph for clustering and visualizing Artificial Intelligence methods Title: A machine learning approach to integrate big data for precision medicine in acute myeloid leukemia Abstract: Cancers that appear pathologically similar often respond differently to the same drug regimens.Methods to better match patients to drugs are in high demand.We demonstrate a promising approach to identify robust molecular markers for targeted treatment of acute myeloid leukemia (AML) by introducing: data from 30 AML patients including genome-wide gene expression profiles and in vitro sensitivity to 160 chemotherapy drugs, a computational method to identify reliable gene expression markers for drug sensitivity by incorporating multi-omic prior information relevant to each gene's potential to drive cancer.We show that our method outperforms several state-of-the-art approaches in identifying molecular markers replicated in validation data and predicting drug sensitivity accurately.Finally, we identify SMARCA4 as a marker and driver of sensitivity to topoisomerase II inhibitors, mitoxantrone, and etoposide, in AML by showing that cell lines transduced to have high SMARCA4 expression reveal dramatically increased sensitivity to these agents.LLM Summarized Core Aspects: Problem (keyword/keyphrase): Patient-Drug Matching in Acute Myeloid Leukemia Problem (definition): The need for identifying effective drug regimens for patients with acute myeloid leukemia, as similar cancers can respond differently to the same treatments, necessitating a method to better match patients with suitable therapies based on molecular markers.Problem discipline: Medical Science/Oncology Method (keyword/keyphrase): Computational Method for Gene Expression Analysis Method (definition): A computational approach that utilizes data analysis techniques to identify reliable markers from gene expression profiles, potentially incorporating additional omic data to enhance the predictive accuracy for drug sensitivity.Usage: The method is applied by analyzing gene expression data from acute myeloid leukemia patients to identify molecular markers that are associated with sensitivity to various chemotherapy drugs, thereby improving the precision of treatment matching for individual patients.AI4Science Classification: Scientific problem: True AI method: True Title: Dynamic microscale flow patterning using electrical modulation of zeta potential Abstract: Cancers that appear pathologically similar often respond differently to the same drug regimens.Methods to better match patients to drugs are in high demand.We demonstrate a promising approach to identify robust molecular markers for targeted treatment of acute myeloid leukemia (AML) by introducing: data from 30 AML patients including genome-wide gene expression profiles and in vitro sensitivity to 160 chemotherapy drugs, a computational method to identify reliable gene expression markers for drug sensitivity by incorporating multi-omic prior information relevant to each gene's potential to drive cancer.We show that our method outperforms several state-of-the-art approaches in identifying molecular markers replicated in validation data and predicting drug sensitivity accurately.Finally, we identify SMARCA4 as a marker and driver of sensitivity to topoisomerase II inhibitors, mitoxantrone, and etoposide, in AML by showing that cell lines transduced to have high SMARCA4 expression reveal dramatically increased sensitivity to these agents.LLM Summarized Core Aspects: Problem (keyword/keyphrase): Microscale flow control Problem (definition): The limitations associated with moving fluids at the microscale, particularly the constraints imposed by discrete channels, mechanical valves, and fixed geometries, which restrict the ability to manipulate fluid dynamics effectively.Clustering.After we obtained the semantic embeddings, we used LargeVis [30] for dimension reduction followed by HDBSCAN [3] for clustering.HDBSCAN is a widely used density-based clustering algorithm designed for practical data mining tasks.As an extension of DBSCAN, it employs hierarchical clustering to identify clusters of varying densities and is more robust to parameter selection.A density-based clustering method allows us to identify clusters at appropriate granularities and avoids a top-down decision to predefine the number of clusters.Its efficiency ( ( log ) complexity) also makes it suitable for our dataset size.</p>
<p>Title: A region-based model for estimating urban air pollution Abstract: Air pollution has a direct impact to human health, and data-driven air quality models are useful for evaluating population exposure to air pollutants.In this paper, we propose a novel region-based Gaussian process model for estimating urban air pollution dispersion, and applied it to a large dataset of ultrafine particle (UFP) measurements collected from a network of sensors located on several trams in the city of Zurich.We show that compared to existing grid-based models, the region-based model produces better predictions across aggregates of all time scales.The new model is appropriate for many useful user applications such as exposure assessment and anomaly detection.LLM Summarized Core Aspects: Problem (keyword/keyphrase): urban air pollution Problem (definition): The challenge of estimating and predicting the dispersion of air pollutants in urban environments, which directly impacts human health and exposure assessments.Problem discipline: Environmental Science Method (keyword/keyphrase): Gaussian process model Method (definition): A statistical method used for regression and classification tasks that relies on the principles of Bayesian inference and provides a flexible way to model complex relationships in data.Usage: The method is applied to a large dataset of ultrafine particle measurements to estimate urban air pollution dispersion and improve prediction accuracy compared to traditional grid-based models.AI4Science Classification: Scientific problem: True AI method: True Title: Dependent Relational Gamma Process Models for Longitudinal Networks Abstract: A probabilistic framework based on the covariate-dependent relational gamma process is developed to analyze relational data arising from longitudinal networks.The proposed framework characterizes networked nodes by nonnegative node-group memberships, which allow each node to belong to multiple latent groups simultaneously, and encodes edge probabilities between each pair of nodes using a Bernoulli Poisson link to the embedded latent space.Within the latent space, our framework models the birth and death dynamics of individual groups via a thinning function.Our framework also captures the evolution of individual node-group memberships over time using gamma Markov processes.Exploiting the recent advances in data augmentation and marginalization techniques, a simple and efficient Gibbs sampler is proposed for posterior computation.Experimental results on a simulation study and three real-world temporal network data sets demonstrate the model's capability, competitive performance and scalability compared to state-of-the-art methods.LLM Summarized Core Aspects: Problem (keyword/keyphrase): Longitudinal networks analysis Problem (definition): The challenge of analyzing relational data that evolves over time within networks, specifically focusing on the dynamics of node-group memberships and the interactions between nodes in a temporal context Problem discipline: Statistics and Network Science Method (keyword/keyphrase): Bayesian inference using Gibbs sampling Method (definition): A statistical method that involves making inferences about unknown parameters in a model through the use of a sampling technique that generates samples from the posterior distribution of those parameters.Usage: The method is applied to perform posterior computation for the parameters of the covariate-dependent relational gamma process model, enabling the analysis of the evolving structures of longitudinal networks.AI4Science Classification: Scientific problem: False AI method: True Table 11: Examples of AI paper LLM extraction and AI4Science judging results</p>
<p>Labeling.To have more consistent and generalizable knowledge of the clusters, we asked GPT (gpt-4o-2024-08-06) to summarize each cluster.We first perform TF-IDF on each of the clusters, obtaining a list of top words.Then we provide the top words as well as examples of paper key aspects from a certain cluster to the LLM for summarization.</p>
<p>Using LLMs with the below prompts to summarize clusters encourage the cluster labels to have the desirable properties [19]:</p>
<p>(1) understandable, (2) semantically relevant, (3) well covering the whole topic, and (4) distinguishable from other topics.</p>
<p>For scientific problem clusters we use the following prompt:  Well-and under-explored regions.Table 12 list some top clusters of well-and under-explored scientific problems as well as AI methods.</p>
<p>Bipartite graph node degrees.large language models (LLMs), to capture more fine-grained distinctions and predict links based on textual data.</p>
<p>Following Özer et al. [21], we employ three categories of bipartite graph link prediction models: (1) link score-based predictions, where we utilize the Katz index calculated using common neighbors [11]; and (2) embedding-based link predictions [23,31], in which we apply spectral embedding and node2vec embedding techniques [8].</p>
<p>The Katz index between two nodes  and  are calculated as below:
Katz(𝑥, 𝑦) = ∞ ∑︁ l=1 𝛼 𝑙 (𝐴 𝑙 ) 𝑥 𝑦 ,(1)
where  is the connectivity matrix and  is a penalty factor for path length (In all of our experiments we chose  = 0.1).</p>
<p>For generative link prediction, we employ two OpenAI GPT models, gpt-3.5-turbo-0125and gpt-4o-2024-08-06, which represent different levels of generative capabilities.It's important to note that the training data for gpt-3.5-turbo-0125extends only up to September 2021, meaning there is no risk of data leakage based on our training/test data splitting strategy.However, the training data for gpt-4o-2024-08-06 includes information up to October 2023, which may introduce potential data leakage and could lead to artificially higher performance.We will discuss the impact of this potential data leakage in the results section.</p>
<p>Evaluation metrics.For cluster-level link prediction, we evaluate performance using Recall, Precision, and F1 scores @K, where K represents the number of generated candidate links, as is standard in the literature [1,36].Particularly, for a source node , supposing in the test data, it is connected to a set of target nodes  (), and the model generates  predictions  ().The metrics are calculated as below:</p>
<p>For paper-level link prediction, we adopt three widely used evaluation metrics for text generation: ROUGE [17], BLEURT [25], and the cosine similarity of text embeddings.The best scores @K are reported for each of these metrics.</p>
<p>For ROUGE, we report rouge-1-f, which is defined as follows (given a reference's unigrams r and extraction's unigrams s): • Sci → AI.Given the scientific problem description, generate the AI method that can solve the scientific problem.• AI → Sci.Given the AI method description, generate the scientific problem that the AI methods can solve.To evaluate LLM link generation quality, we consider the following experiments:</p>
<p>• Imitation @ K: The nearest K descriptions (scientific problems or AI methods) are imitated to be the predictions.Distance is measured by semantic embedding cosine similarity (Appendix A.3). • Direct @ K: In this scenario, we directly ask the LLM to generate K predictions given the extracted scientific problem/AI methods description.• RAG @ K: We first find  most similar examples (similarity measure by cosine similarity of semantic embedding) and provide these examples to the LLM to generate K predictions given the extracted scientific problem/AI methods descriptions.We experiment with  = 1, 3, 5.In the main paper, we report the results with  = 5.</p>
<p>Prompts.For paper-level link prediction with LLMs, we use the following prompt 1 : ## Background and Task Description You are an expert in both science and artificial intelligence (AI), where AI generally refers to intelligence exhibited by machines (particularly computer systems), including models, algorithms, etc.Given a scientific problem, your task is to recommend potential Artificial Intelligence methods that can be used to address this scientific problem.A few examples of relevant papers with similar scientific problems will be</p>
<p>C.2 Experiment Results</p>
<p>Tables 17-21 summarize the detailed LLM link generation experiment results with different models and evaluation metrics.These results show that:</p>
<p>• Including RAG examples helps the model make more informed predictions, as all evaluation metric results improve when we increase the examples given from  = 1 to  = 5. • While Imitation can achieve higher rouge scores, it has less semantic similarity compared with LLM generations.• Table 16 show that the performance of LLM (Paper) remains consistent across different models used, suggesting a low risk of data leakage.However, the performance of LLM (Cluster) declines, likely due to GPT-3.5's limited ability of reasoning and processing long contexts.</p>
<p>D Supplementary Materials</p>
<p>We include our dataset, code, analyses, and experiment results in the anonymous repository(https://github.com/charles-pyj/Bridging-AIand-Science) as supplementary materials to this paper, which include:</p>
<p>Highly accurate protein structure prediction with AlphaFold Abstract: Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function.Through an enormous experimental effort, the structures of around 100,000 unique proteins have been determined, but this represents a small fraction of the billions of known protein sequences.Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure.Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics.Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence-the structure prediction component of the 'protein folding problem'-has been an important open research problem for more than 50 years.Despite recent progress, existing methods fall far short of atomic accuracy, especially when no homologous structure is available.Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known.We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14), demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods.Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.): Protein structure prediction.The challenge of accurately predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence, which has implications for understanding protein function and addressing the large gap between known protein sequences and experimentally determined structures.Cluster label (Pi): Protein Structure and Design Extraction (mi): Neural network-based model.A computational model that uses a network of algorithms inspired by the way the human brain operates to analyze and predict outcomes based on input data, employing deep learning techniques to improve the accuracy of predictions.Cluster label (Mi): Neural Networks Extraction (ui): The method is utilized to predict protein structures by processing amino acid sequences and leveraging various forms of biological and physical knowledge to achieve high accuracy in predictions.</p>
<p>Figure 1 :
1
Figure 1: Illustration of LLM-based extraction of scientific problem   , AI method   , and AI usage   from an example publication.The extractions are then semantically projected, clustered, and labeled to form the AI4Science landscape (Fig.2), as well as to construct the bipartite graph (Fig.4a).</p>
<p>problems (129,038 extractions, 390 clusters).(b) AI methods (42,455 extractions, 355 clusters).</p>
<p>Figure 2 :
2
Figure 2: 2D projection maps of the AI4Science landscape: (a) the extracted scientific problems {  } and (b) AI methods {  }.Each dot represents a publication, with coordinates obtained by reducing the dimensionality of the problem/method embeddings.Green dots correspond to AI4Science work, using AI methods to address scientific problems; Orange dots show papers addressing scientific problems without using AI methods; Purple dots are papers using AI to address non-scientific problems.The visualizations reveal a noticeable discrepancy in the distribution of AI4Science work (green) versus non-AI4Science work (orange and purple) in both the problem and method spaces.</p>
<p>( a )
a
Bipartite graph between scientific problems and AI methods.(b) Scientific problem node degree distribution.(c) AI method node degree distribution.</p>
<p>Figure 4 :
4
Figure 4: The AI-Science bipartite graph and node distributions.(a) The bipartite graph with scientific problem clusters and AI method clusters as nodes.The size of each node corresponds to its unweighted degree, representing the number of AI methods applied to a scientific problem, or the number of scientific problems utilizing an AI method.For visualization clarity, edges representing fewer than four publications and the resulting isolated noes are hidden.The distribution of links indicates imbalanced connectivity.(b-c) The degree of AI method nodes follows a log-normal distribution, and the degree distribution of scientific problem nodes is even more heavily tailed, suggesting the existence of "hubs" in linking AI and science.</p>
<p>*</p>
<p><em>Problem (keyword/keyphrase):</em><em> A keyword or a keyphrase that summarizes the main problem to be addressed in this paper.</em><em>Problem (definition):</em><em> The detailed definition of the problem.</em><em>Problem discipline:</em><em> The discipline in which the main problem best fits.</em><em>Method (keyword/keyphrase):</em><em> A keyword or a keyphrase that summarizes the main method used in this paper to address the above problem.</em><em>Method (definition):</em><em> The detailed definition of the method.</em><em>Usage:</em><em> How the method is specifically applied to address the problem.## Scientific Paper to Be Extracted Title: {title} Abstract: {abstract} ## Requirements * Please do not include any method-specific information in problem extraction.</em> Similarly, please do not include any problem-specific information in method extraction.The extracted method description (keyword/keyphrase and definition) should be generic and can be applied across all application domains.* Please do not use abbreviations as keywords/keyphrases.</p>
<h1></h1>
<ul>
<li>Don't consider research problem disciplines that involves Computer Science, Information/Data Science as traditional scientific problems.## Response Format: Only a dictionary containing the following { "Scientific problem": True/False, "AI method": True/False, } Model.For the LLM extraction, we utilized gpt-4o-mini-2024-07-18, which represents a good balance between extraction capability and affordability.We use default generation hyperparameters (e.g., temperature) for all extraction tasks.</li>
</ul>
<p>Problem discipline: Fluid Mechanics Method (keyword/keyphrase): Field-effect electroosmosis Method (definition): A technique that utilizes electric fields to induce fluid flow in a medium, allowing for the manipulation of fluid dynamics through the controlled application of electrical signals.Usage: The method is applied by controlling gate electrodes with an alternating current voltage to generate dynamic flow patterns in a fluidic chamber, enabling real-time modulation of flow characteristics without physical barriers.AI4Science Classification: Scientific problem: True AI method: False Table 10: Examples of scientific paper extraction and AI4Science judging results</p>
<p>Fig. 5 -
5
Fig. 5-6 illustrates the distribution of AI4Science and non-AI4Science publications across different communities, highlighting how AI and scientific fields engage with interdisciplinary research distinctively.</p>
<p>, ( 2 )
2
Precision := |{ ∈  () |  ∈  ()}|  Recall := |{ ∈  () |  ∈  ()}| | ()| .</p>
<p>2 *
2
 *   +  For BLEURT[25], we use the pre-trained checkpoint BLEURT-20 For instruction embedding, we first calculate reference's and extraction's embedding ( and  respectively) via pre-trained checkpoint Instructor-large[28] and calculate the cosine similarity: (, ) = &lt; ,  &gt; || || || ||LLM (Paper) link prediction setup.For paper-level generative link prediction, we consider both Sci → AI and AI → Sci generations:</p>
<p>provided.<em> Problem (keyword/keyphrase): A keyword or a keyphrase that summarizes the main problem to be addressed in this paper.</em> Problem (definition): The detailed definition of the problem.<em> Problem discipline: The discipline in which the main problem best fits.</em> Method (keyword/keyphrase): A keyword or a keyphrase that summarizes the main method used in this paper to address the above problem.<em> Method (definition): The detailed definition of the method.</em> Usage: How the method is specifically applied to address the problem.## Scientific Problem Please recommend an AI method to address the below scientific problem for writing an academic paper: {Key Aspects Extraction} ## Examples of usage in similar scientific papers: {examples} ## Notes * If no potential AI method can be used, mark it as "N/A" (not applicable).<em> Please respond with specific AI methods instead of high-level AI methods.</em> Exactly output one recommendation.## Response Format Please output the recommendation of AI methods as a list, which has exactly <em>one</em> element.The output should be in the format of a list as below: [ { "AI Method (keyword/keyphrase)": "... ", "AI Usage": "... " }, ... ] For cluster-level link prediction with LLMs (LLM (Cluster)), we use the following prompt: 2 : ## Background and Task Description You are an expert in both science and artificial intelligence (AI), where AI generally refers to intelligence exhibited by machines (particularly computer systems), including models, algorithms, etc.Given a scientific problem domain and past usage of Artificial Intelligence methods in solving scientific problems, your task is to recommend potential Artificial Intelligence methods that can be used to address the scientific problem.## Scientific problem domain 2 For brevity only Sci → AI RAG prompt is provided {sci cluster} ## Possible Artificial Intelligence domains {AI clusters} ## Format of past usage of AI methods to solve Scientific problems: (u,v,k): Scientific problem u has been solved with AI method v for k times in previous scientific literature.## Past usage of AI methods to solve Scientific problems: {example links} ## Notes * If no potential AI method can be used, mark it as "N/A" (not applicable).<em> The AI methods recommended should be within the possible Artificial Intelligence domains.</em> The AI methods recommended may or may not be within the given observed links.* Exactly recommend {k} AI methods.## Response Format please output the recommended AI methods as a list, which contains exactly {k} elements.The output should be in the list format as below: [ "Artificial Intelligence Method 1", ... ]</p>
<p>e ll-e xp lo re d Un de r-e xp lor ed (</p>
<p>a) Scientific problem clusters.
Electronic Heath Data Challenge Discovery Systems Dynamical DrugLanguage Processing Decision Making Visual Perception and Neuroscience Imaging Challenges Statistical InferenceComputational Drug Screening Protein Structure DesignAdvanced Imaging TechniquesWe ll -Protein Design and Function PredictionSingle-cell RNA Sequencing Neural Imaging Social Dynamics Challenges Soft RoboticsMechanical Properties of Materials W Attention Mechanism Reinforcement Learning Learning Unsupervised Neural Networks Molecular Structure Design Deep Neural NetworksTransformer ArchitecturesAsymmetric SynthesisSelf-Supervised LearningGraph Embedding Policy Gradient MethodsViral MechanismsForests and Climate ChangeContrastive LearningGradient Descent MethodsMagnetic Materials</p>
<p>e x p l o r e d U n d e r-e x p lo r e d</p>
<p>Scientific problem clusters (a) and AI method clusters (b) are visualized as scatters.In both plots, the -axis represents the total number of publications in each cluster, while the -axis reflects the number of interdisciplinary AI4Science publications in each cluster.The black lines show the regression results on the clusters.Clusters above the line indicate regions well-explored for AI4Science.Clusters falling below the line highlight the under-explored regions, where the integration of AI and science remains limited.These areas represent potential opportunities for further interdisciplinary collaboration.
(b) AI method clusters.Figure 3: NotationValuePublication year range-2014-2024# of venues-12# of publications𝑁162,656# of scientific problem extractions|{𝑝 𝑖 }|129,038# of scientific problem clusters|{𝑃 𝑖 }|390# of AI method extractions|{𝑚 𝑖 }|42,455# of AI method clusters|{𝑀 𝑖 }|355# of AI usage extractions|{𝑢 𝑖 }|42,455# of AI4Science publications-7,542</p>
<p>Table 1 :
1
AI4Science dataset D = {(  ,   ,   ,   ,   )}  =1 .</p>
<p>Table 2 :
2
Average degree of nodes in the bipartite graph.
Scientific problemAll nodes13.0clusters (n=390)Well-explored41.3Under-explored5.0AI methodAll nodes14.3clusters (n=355)Well-explored19.1Under-explored8.1
Network Challenges' and 'Statistical Inference'.These are followed by more domain-specific problems like: 'Urban Traffic Management', 'Electronic Health Data Challenges', 'Visual Perception and Neuroscience', 'Market Pricing and Allocation', 'Single-cell RNA Sequencing', 'Soft Robotics', and 'Drug Discovery'.</p>
<p>Table 3 :
3
Link prediction results of different models.Bold numbers highlights the highest performances in each scenario.
Katz index0.115 0.108 0.086 0.076 0.015 0.035 0.052 0.104 0.026 0.053 0.065 0.088Node2vec0.198 0.185 0.168 0.129 0.033 0.098 0.138 0.197 0.056 0.128 0.151 0.156LLM (Cluster) 0.352 0.300 0.238 0.159 0.053 0.101 0.134 0.176 0.093 0.151 0.171 0.167LLM (Paper)0.282 0.227 0.200 0.156 0.058 0.106 0.129 0.155 0.096 0.145 0.157 0.155Katz index0.094 0.120 0.110 0.102 0.010 0.028 0.041 0.079 0.018 0.045 0.060 0.089AI → SciNode2vec0.096 0.108 0.110 0.109 0.013 0.042 0.058 0.104 0.023 0.060 0.076 0.106LLM (Cluster) 0.201 0.168 0.143 0.129 0.016 0.042 0.053 0.092 0.030 0.068 0.077 0.107LLM (Paper)0.377 0.323 0.307 0.296 0.065 0.104 0.127 0.163 0.111 0.158 0.180 0.211Link predictionPrecisionRecallF1Setting</p>
<p>Table 4 :
4
Link prediction results on well-and under-explored regions.
→ AILLM (Paper) LLM (Cluster)Well-explored Under-explored 0.250 0.195 0.171 0.129 0.111 0.192 0.224 0.247 0.153 0.193 0.194 0.169 0.293 0.259 0.230 0.184 0.025 0.059 0.073 0.105 0.046 0.095 0.111 0.134 Well-explored 0.360 0.411 0.353 0.228 0.030 0.082 0.123 0.164 0.055 0.137 0.182 0.191 Under-explored 0.194 0.125 0.089 0.051 0.068 0.122 0.147 0.182 0.101 0.124 0.111 0.079AI → SciLLM (Paper) LLM (Cluster)Well-explored Under-explored 0.364 0.301 0.267 0.236 0.151 0.222 0.264 0.311 0.214 0.255 0.265 0.269 0.403 0.340 0.334 0.332 0.045 0.078 0.096 0.126 0.080 0.127 0.149 0.182 Well-explored 0.328 0.251 0.183 0.185 0.020 0.047 0.053 0.102 0.038 0.079 0.082 0.132 Under-explored 0.076 0.057 0.074 0.102 0.025 0.051 0.051 0.037 0.038 0.054 0.060 0.055</p>
<p>Table 5 .
5
Remarkably, the LLM-based
Method# of novel (unseen) links@1@3@5@10Node2vec8132432 1623LLM (Cluster)243867 1,523 3,209LLM (Cluster)  *  430 1,650 2,999 6,963LLM (Paper)639 2,291 3,771 8,276</p>
<p>Table 5 :
5
Number of novel links discovered by models compared with the training set (data up to 2022).The publications during 2023 and 2024 introduced 683 new links.For a fair comparison, in LLM (Cluster) * we control the number of API calls to match that of LLM (Paper).</p>
<p>Table 6 :
6
A.1 Curation of Literature Science Domain dataset statistics • For AI domain, we obtained our data from MASSW [37], which encompasses the following seven top AI conferences: AAAI, IJCAI, ICLR, ICML, NeurIPS, WWW, SIGKDD.Only papers with both title and abstract available and within 2014 to 2024 are kept.A brief statistics of AI domain papers are as follows:
Data source.• For the science domain, we obtained our data via the NCBIEntrez database, encompassing the following papers: Na-ture, Science, PNAS, Nature Communications, Science Ad-vances. Only papers with both title and abstract availableand within 2014 to 2024 are kept. A brief statistics of sciencedomain papers are as follows:Journal#Pub. #AI4Science Pub.Nature10,890254Science10,998207PNAS35,556997Nature Communications54,2432,027Science Advances12,086488Total123,7733,973Conference #Pub. #AI4Science Pub.AAAI8,994915IJCAI2,953271ICLR2,907205ICML5,641530NeurIPS11,250889WWW3,777241SIGKDD3,251488Total38,7733,539</p>
<p>Table 7 :
7
AI Domain dataset statistics</p>
<p>Table 9 :
9
Extraction examples.Listed below are some examples of key aspects extracted by LLM within the human annotation set and whether the paper involves the use of AI methods or tries to solve scientific problems.
Extraction acc. Human agreement F1Scientific pubs.0.8900.876AI pubs.0.9300.946Table 8: Human evaluation of gpt-4o-mini-2024-07-18 ex-tractions.Classification acc. Human Agreement F1Prec. RecallF1Scientific pubs. 0.833 0.840 0.8360.632AI pubs.0.516 0.866 0.6470.632
Human evaluation of gpt-4o-mini-2024-07-18 AI4Science Classification.Scientific journal paper examples: Please refer to table 10 AI conference paper examples: Please refer to table 11</p>
<p>Table 13 presents the nodes with the highest degrees in the bipartite graph, representing the most connected scientific challenges and AI methods in the AI4Science
TrainTestPublication year range2014-2022 2023-2024All publications141,63921,017Number ofAI4Science publications Pub. in well-explored sci. clusters Pub. in under-explored sci. clusters Pub. in well-explored AI clusters6,287 4,262 1,180 3,0761,225 594 392 742Pub. in under-explored AI clusters1,387198Avg. deg. ofScientific problem nodes Well-explored sci. nodes Under-explored sci. nodes AI method nodes Well-explored AI nodes11.3 37.7 3.9 12.4 16.22.8 6.9 1.4 3.0 4.9Under-explored AI nodes7.21.3</p>
<p>Table The training and test splits of data.</p>
<p>landscape.These high-degree nodes act as key "hubs," indicating their central role in bridging AI techniques with scientific problems.Discrepancies between the AI and science communities.Models. We examine two approaches to link prediction based on different data representations:• Cluster-level link prediction ("  →   " or "  →   "): In this approach, scientific problems and AI methods are represented as high-level cluster labels, serving as nodes in a bipartite graph.This captures more generalized connections between similar scientific problems and AI methods.• Paper-level link prediction ("  →   ,   " or "  →   ,   "):Here, scientific problems and AI methods are expressed in detailed text descriptions rather than coarse cluster labels.This approach leverages textual generative models, such as13: Nodes in the bipartite graph (Fig.4a) with the highest degrees.Setting Model Precision Recall F1 @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10 Sci → AI LLM (Paper,gpt-3.0.377 0.323 0.307 0.296 0.065 0.104 0.127 0.163 0.111 0.158 0.180 0.211 LLM (Cluster,gpt-3.5-turbo-0125)0.136 0.082 0.120 0.102 0.009 0.023 0.053 0.088 0.017 0.036 0.074 0.095 LLM (Cluster,gpt-4o-2024-08-06) 0.201 0.168 0.143 0.129 0.016 0.042 0.053 0.092 0.030 0.068 0.077 0.107 Table16: Link prediction results comparison between gpt-3.5-turbo-012 and gpt-4o-2024-08-06.Sci → AI ROUGE-1-F BLEURT CosineSim @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10 Imitation 0.254 0.297 0.313 0.330 0.365 0.397 0.407 0.420 0.853 0.871 0.877 0.883 Direct 0.263 0.286 0.288 0.292 0.407 0.428 0.429 0.429 0.889 0.895 0.895 0.896 RAG n = 1 0.276 0.291 0.300 0.309 0.416 0.434 0.439 0.444 0.888 0.895 0.898 0.901 RAG n = 3 0.286 0.296 0.304 0.313 0.417 0.434 0.439 0.443 0.888 0.895 0.897 0.901 RAG n = 5 0.290 0.297 0.305 0.314 0.417 0.432 0.440 0.444 0.887 0.894 0.897 0.900Table17: gpt-3.5-turbo-0125AI method link generation resultsSci → AI ROUGE-1-F BLEURT CosineSim @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10 Imitation 0.254 0.297 0.313 0.330 0.365 0.397 0.407 0.420 0.853 0.871 0.877 0.883 Direct 0.245 0.273 0.284 0.286 0.380 0.410 0.429 0.435 0.885 0.891 0.894 0.895 RAG n = 1 0.266 0.283 0.289 0.294 0.391 0.424 0.438 0.443 0.889 0.894 0.897 0.898 RAG n = 3 0.273 0.292 0.295 0.300 0.395 0.426 0.441 0.447 0.890 0.895 0.898 0.899 RAG n = 5 0.277 0.296 0.300 0.304 0.397 0.430 0.442 0.449 0.890 0.896 0.898 0.900 Table18: gpt-4o-2024-08-06 AI method link generation results AI → Sci ROUGE-1-F BLEURT CosineSim @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10 AI → Sci ROUGE-1-F BLEURT CosineSim @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10 Link prediction setting ROUGE-1-F BLEURT CosineSim @1 @3 @5 @10 @1 @3 @5 @10 @1 @3 @5 @10Sci • The AI4Science dataset with publication extractions and cluster labels.• Tables of cluster statistics for generating Fig.3.• Link data of the bipartite graph and new links introduced by LLM-based models.• Degree information of scientific problem nodes and AI method nodes.• Human annotations and evaluation results with two human annotators.• Code implementations for visualizations, link predictions, metrics, and data analysis, which generates the main results of the paper.
Link prediction using supervised learning. Mohammad Al Hasan, Vineet Chaoji, Saeed Salem, Mohammed Zaki, SDM06: workshop on link analysis, counter-terrorism and security. 200630</p>
<p>The role of AI in drug discovery: challenges, opportunities, and strategies. Alexandre Blanco-Gonzalez, Alfonso Cabezon, Alejandro Seco-Gonzalez, Daniel Conde-Torres, Paula Antelo-Riveiro, Angel Pineiro, Rebeca Garcia-Fandino, Pharmaceuticals. 168912023. 2023</p>
<p>Density-based clustering based on hierarchical density estimates. Ricardo Jgb Campello, Davoud Moulavi, Jörg Sander, Pacific-Asia conference on knowledge discovery and data mining. Springer2013</p>
<p>A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains. Minkyu Choi, Kuan Han, Xiaokai Wang, Yizhen Zhang, Zhongming Liu, arXiv:2310.13849[cs.CV2023</p>
<p>Machine learning in drug discovery: a review. Suresh Dara, Swetha Dhamercherla, Surender Singh Jadav, Madhu Babu, Mohamed Jawed Ahsan, Artificial intelligence review. 552022. 2022</p>
<p>Eamon Duede, William Dolan, André Bauer, Ian Foster, Karim Lakhani, arXiv:2405.15828Oil &amp; water? diffusion of ai within and across scientific fields. 2024. 2024arXiv preprint</p>
<p>Quantifying the benefit of artificial intelligence for scientific research. Jian Gao, Dashun Wang, arXiv:2304.105782023. 2023arXiv preprint</p>
<p>node2vec: Scalable feature learning for networks. Aditya Grover, Jure Leskovec, Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining2016</p>
<p>Artificial intelligence and machine learning in design of mechanical materials. Kai Guo, Zhenze Yang, Chi-Hua Yu, Markus J Buehler, Materials Horizons. 82021. 2021</p>
<p>Highly accurate protein structure prediction with Al-phaFold. John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, nature. 5962021. 2021</p>
<p>A new status index derived from sociometric analysis. Leo Katz, Psychometrika. 181953. 1953</p>
<p>polyBERT: a chemical language model to enable fully machine-driven ultrafast polymer informatics. C Kuenneth, R Ramprasad, 10.1038/s41467-023-39868-6Nature Communications. 1440992023. 2023</p>
<p>Science in the age of AI: How artificial intelligence is changing the nature and method of scientific research. Georgios Leontidis, 2024. 2024</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 332020. 2020</p>
<p>AI applications through the whole life cycle of material discovery. Jiali Li, Kaizhuo Lim, Haitao Yang, Zekun Ren, Shreyaa Raghavan, Po-Yen Chen, Tonio Buonassisi, Xiaonan Wang, Matter. 32020. 2020</p>
<p>Silica-water superstructure and one-dimensional superionic conduit in Earth's mantle. Junwei Li, Yanhao Lin, Thomas Meier, Zhipan Liu, Wei Yang, Shengcai Ho Kwang Mao, Qingyang Zhu, Hu, 10.1126/sciadv.adh3784Science Advances. 937842023. 2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Artificial intelligence in drug discovery and development. Kit-Kay Mak, Yi-Hang Wong, Mallikarjuna Rao, Pichika , Drug Discovery and Evaluation: Safety and Pharmacokinetic Assays. 2023. 2023</p>
<p>Automatic labeling of multinomial topic models. Qiaozhu Mei, Xuehua Shen, Chengxiang Zhai, Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining. the 13th ACM SIGKDD international conference on Knowledge discovery and data mining2007</p>
<p>Predicting locations of cryptic pockets from single protein structures using the PocketMiner graph neural network. A Meller, M Ward, J Borowsky, M Kshirsagar, J M Lotthammer, F Oviedo, J L Ferres, G R Bowman, 10.1038/s41467-023-36699-336859488PMC9977097Nature Communications. 14111772023. Mar 2023</p>
<p>Şükrü Demir, İnan Özer, Günce Keziban Orman, Vincent Labatut, arXiv:2406.06658Link Prediction in Bipartite Networks. 2024. 2024arXiv preprint</p>
<p>Growth and structure of Slovenia's scientific collaboration network. Matjaž Perc, Journal of Informetrics. 42010. 2010</p>
<p>Deepwalk: Online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>Rizwan Qureshi, Muhammad Irfan, Taimoor Muzaffar Gondal, Sheheryar Khan, Jia Wu, Muhammad Usman Hadi, John Heymach, Xiuning Le, Hong Yan, Tanvir Alam, AI in drug discovery and its clinical relevance. 2023. 202397</p>
<p>BLEURT: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur P Parikh, arXiv:2004.046962020. 2020arXiv preprint</p>
<p>Artificial intelligence to power the future of materials science and engineering. Wuxin Sha, Yaqing Guo, Qing Yuan, Shun Tang, Xinfang Zhang, Songfeng Lu, Xin Guo, Yuan-Cheng Cao, Shijie Cheng, Advanced Intelligent Systems. 219001432020. 2020</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can LLMs Generate Novel Research Ideas?. 2024. 2024arXiv preprint</p>
<p>Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-Tau Yih, Noah A Smith, Luke Zettlemoyer, Tao Yu, arXiv:2212.09741One embedder, any task: Instruction-finetuned text embeddings. 2022. 2022arXiv preprint</p>
<p>Analyzing expert behaviors in collaborative networks. Huan Sun, Mudhakar Srivatsa, Shulong Tan, Yang Li, Lance M Kaplan, Shu Tao, Xifeng Yan, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data mining2014</p>
<p>Visualizing large-scale and high-dimensional data. Jian Tang, Jingzhou Liu, Ming Zhang, Qiaozhu Mei, Proceedings of the 25th international conference on world wide web. the 25th international conference on world wide web2016</p>
<p>Line: Large-scale information network embedding. Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, Qiaozhu Mei, Proceedings of the 24th international conference on world wide web. the 24th international conference on world wide web2015</p>
<p>Degree distributions of bipartite networks and their projections. Demival Vasques, Filho Dion, Rj O' Neale, Physical Review E. 98223072018. 2018</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 6202023. 2023</p>
<p>Discover hidden web properties by random walk on bipartite graph. Yan Wang, Jie Liang, Jianguo Lu, Information retrieval. 172014. 2014</p>
<p>AI for social science and social science of AI: A survey. Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, Xianpei Han, Information Processing &amp; Management. 611036652024. 2024</p>
<p>Evaluating link prediction methods. Yang Yang, Ryan N Lichtenwalter, Nitesh V Chawla, Knowledge and Information Systems. 452015. 2015</p>
<p>Xingjian Zhang, Yutong Xie, Jin Huang, Jinge Ma, Zhaoying Pan, Qijia Liu, Ziyang Xiong, Tolga Ergen, Dongsub Shim, Honglak Lee, arXiv:2406.06357MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows. 2024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
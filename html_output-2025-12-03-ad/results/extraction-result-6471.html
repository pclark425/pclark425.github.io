<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6471 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6471</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6471</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-128.html">extraction-schema-128</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <p><strong>Paper ID:</strong> paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/061d113a7b3f32deab6bc50fea676fa0b1e0f658" target="_blank">Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work takes a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks and applies an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations.</p>
                <p><strong>Paper Abstract:</strong> Interactive Fiction (IF) games with real human-written natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the human-written textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. Our source code is available at: this https URL.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6471.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6471.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MPRC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Paragraph Reading Comprehension Deep Q-Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's main agent that formulates IF action prediction as a multi-paragraph reading-comprehension task, retrieving object-centric past observations and scoring template-based actions with a BiDAF-style reader inside a DQN training loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MPRC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Value-based RL agent (DQN) whose Q-function is parameterized by an RC model: observation paragraphs (current + retrieved history) are treated as context and each action template (verb phrase) as a query; BiDAF-style context-query attention + self-attention produce verb-aware observation representations, object-specific embeddings fill template placeholders, and a linear head predicts Q-values. History retrieval is object-centric and time-sensitive.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (suite of human-written Interactive Fiction games, e.g., Zork)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>object-centric retrieved observation buffer (external textual history)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Raw past observation texts (concatenated passages), separated by special tokens; objects detected in each observation are used as indices into these texts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Rule-based time-sensitive retrieval: for each object detected in the current observation, retrieve the most recent K past observations that contain at least one shared object; the retrieved set is sorted by timestep and concatenated to form the extended observation. (K set to 2 in experiments.)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Object matching via named-object detection (spaCy); select most recent K matching observations per detected object; concatenate by time order.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep Q-Network (DQN) with prioritized trajectory sampling (trajectory-level prioritization based on better-than-average trajectories); epsilon-greedy exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game score per game (Jericho scores), winning percentage / counts (number of games where agent is best), and normalized agent-to-human score ratio (macro-average capped at 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported winning percentage 64% (21/33 games where agent is best) and in Table 2 per-game average scores (e.g., Zork1: 38.3). The paper also reports final state-of-the-art on 25 games (76%/25) when considering their approaches overall.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>History window size K is highly game-dependent; object-centric retrieval (time-sensitive) outperforms pure recency-based retrieval; pure recency strategies do not improve much over no-history (RC-DQN) early in learning. Using K=2 in experiments. MPRC-DQN also converges faster than the no-history RC-DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Utility of past observations is time-sensitive and game-dependent; inappropriate retrieval (e.g., pure recency) yields higher variance due to policy changes and may provide limited additional object information; choice of history size impacts performance per-game.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Use object-centric, time-sensitive retrieval rather than naive recency; keep retrieval window small (they use K=2); separate observations with special tokens when concatenating; detect objects robustly (they used spaCy) and prioritize recent observations per object to avoid stale or irrelevant evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6471.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reading Comprehension Deep Q-Network (no-history variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation / baseline implementing the paper's RC-based action prediction model but only using the current observation (no retrieved history), trained with DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>RC-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same RC-based Q-function architecture as MPRC-DQN (BiDAF-style attention between verb-template and observation, argument-specific embeddings) but uses only the current observation paragraph (no history concatenation) as input to the reader; trained with DQN.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (Interactive Fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep Q-Network (DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game score per game (Jericho scores), winning percentage / counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>RC-DQN (no-history) achieved a winning percentage of 52% (17/33 games where agent is best) in Table 2 and per-game scores listed in the RC-DQN column (e.g., Zork1: 38.8).</td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Demonstrates that the RC-style action prediction alone substantially improves performance over prior baselines; comparing RC-DQN to MPRC-DQN isolates the benefit of memory retrieval—MPRC-DQN yields higher winning percentages and faster convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Without history, some long-horizon decisions lacking current observation evidence remain hard; partial observability limits accurate action-value estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Even without memory, RC-style template-aware attention gives strong gains; but for games with time-sensitive object state, introduce object-centric history retrieval to further boost performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6471.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C (Knowledge-Graph A2C)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline agent from prior work that constructs an object-centric knowledge graph from historical observations (OpenIE + rules) and uses that graph as state representation in an A2C learning framework with a GRU-based action generator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior work baseline: extracts objects and relations from historical observations using OpenIE and human-written heuristics to build a knowledge graph representing state; uses GRU-based action generator and A2C (actor-critic) for policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho (Interactive Fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>constructed knowledge graph (symbolic structured memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Symbolic object graph: objects as nodes and extracted relations as edges (constructed from historical observations via OpenIE and heuristics).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Rule-based information extraction (OpenIE) plus human-written rules to update/extend the knowledge graph from parsed historical observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph-based state representation queried by downstream policy network (no explicit retrieval described here beyond maintaining the graph); KG used as state input to policy.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>A2C (advantage actor-critic) with a GRU-based action generator</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game score per game (Jericho scores) as reported in baselines; used 1.6M interaction steps in reported baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>As a baseline, KG-A2C scores are reported per-game in Table 2 (e.g., detective: 207.9 in baseline column), but specific aggregate win percentages vs MPRC-DQN are provided in pairwise comparisons (MPRC-DQN beats KG-A2C 18/28 pairwise).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Paper cites KG-A2C as prior art and notes it treats historical observations equally to summarize into a single vector (graph) which can introduce noise; no detailed ablation here because KG-A2C is an external baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Relies on OpenIE and human-written rules which may not capture complex relations in human-written IF text; summarizing history into a single vector/graph can introduce noise and not focus on contexts relevant to a specific action.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>The authors of this paper recommend more targeted, object-centric retrieval to reduce noise rather than summarizing all history equally into a single representation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6471.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph DQN (KG-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior work that constructs knowledge graphs from past observations (objects, positions, relations) and uses that structured state representation with DRRN-like action handling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Constructs a knowledge graph state from historical observations using OpenIE-style extraction and rules (objects as nodes, relations as edges) and uses that graph for action selection with deep RL (DQN variant in original work).</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho / synthetic text games (prior work applied to synthetic games and adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (symbolic episodic/state memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Structured graph of objects, positions, and spatial relations extracted from history.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Rule-based extraction (OpenIE) and graph construction from past observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph representation used as input state (no retrieval-as-selection described in this paper's summary).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>DQN-style reinforcement learning in the referenced work</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average score on text-adventure games (as reported in referenced work)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Cited as prior approach to partial observability; this paper argues that KG approaches summarize history equally and may not focus on action-relevant contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>KG-based methods rely on extraction tools and rules which may fail on complex human-written IF texts; summarizing all history can introduce noise and not emphasize evidence relevant to the current decision.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Motivates object-centric retrieval and RC-style focused attention to find evidence specifically relevant to a candidate action, instead of global summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6471.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL (No-learning heuristics agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong non-learning baseline that uses heuristics for exploration, object interaction, and building an internal representation of the game world.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Nail: A general interactive fiction agent</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Rule-/heuristic-based agent using hand-designed procedures for exploring IF games, interacting with objects, and maintaining an internal world representation without learning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho / Interactive Fiction games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>internal heuristic world representation (symbolic memory / database of facts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Internal representation of the game world assembled via heuristics (objects, interactions, environment facts).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Heuristic procedures to update internal representation based on observed textual feedback and executed commands.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Agent-specific heuristics that consult the internal representation when deciding actions.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>No learning (heuristic-based), though used as a comparative baseline</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game scores on Jericho (as reported by baseline papers)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Mentioned as state-of-the-art among no-learning agents; cited to highlight non-learning internal representation approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Relies on heuristics and hand-crafted rules; not scalable or generalizable compared to learned approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Not provided here; used as a strong heuristic baseline for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6471.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that projects state and action texts into vector spaces and matches them for action selection, treating actions as text sequences without distinguishing template roles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a natural language action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Projects each valid action and the current state into embedding spaces and predicts action values based on the similarity between state and action embeddings; actions treated as plain sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho / text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (value-based), as in original DRRN work</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game score per game (Jericho scores)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Cited as a baseline that does not exploit template-object compositionality and does not use history in the way MPRC-DQN does.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Doesn't model compositionality/role differences of action elements; uses single-vector state representations that ignore fine-grained dependencies.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>The present paper suggests using template-aware RC-style modeling and targeted history retrieval instead of DRRN's flat action embedding matching.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e6471.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TDQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-DQN / TDQN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline that predicts templates and objects, treating template and object selections independently (earlier template-based DQN approach).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Interactive fiction games: A colossal adventure</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TDQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Value-based RL baseline that decomposes actions into template and object slots but predicts template and objects independently, using a single-vector observation representation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Jericho / Interactive Fiction games</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>DQN (as used in Jericho baseline implementations)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game score per game (Jericho)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Used as a baseline to illustrate the benefit of RC-style joint modeling of templates and objects.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Independent selection of template and objects ignores interactions between template and arguments, reducing data efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>The authors recommend joint modeling of template-object interactions (as in RC-DQN) for improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e6471.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AEDQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Action-Elimination Deep Q-Network (AEDQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior method that learns to predict invalid actions to eliminate them and accelerate policy learning, using expert demonstrations for elimination guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learn what not to learn: Action elimination with deep reinforcement learning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>AEDQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Learns to predict invalid actions and eliminates them from consideration to make RL more efficient; relies on auxiliary prediction of invalidity often aided by demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-adventure games (e.g., Zork)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep Q-Network with an action-elimination auxiliary objective</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Average game score / learning efficiency</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Cited as an approach to reduce combinatorial action space size by eliminating invalid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Requires demonstrations to learn reliable eliminations and may not generalize to unseen invalid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>Not discussed here; cited as complementary to action-space reduction approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6471.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e6471.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents applied to text‑based games and how they use memory, including details of the memory mechanism, what is stored, how it is retrieved, and any reported performance differences with and without memory.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic-KG (Adhikari2020)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learning Dynamic Knowledge Graphs to Generalize on Text-based Games</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Referenced recent work proposing dynamic knowledge-graph construction from gameplay to generalize on text-based games (cited in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning dynamic knowledge graphs to generalize on textbased games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>paper_title</strong></td>
                            <td>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Dynamic Knowledge-Graph approaches (Adhikari et al. 2020)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Prior research that learns to construct and update knowledge graphs dynamically from text observations to represent game state for downstream policies.</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>Text-based games / Jericho (context)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>learned dynamic knowledge graph (structured episodic memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_representation</strong></td>
                            <td>Graph of objects/relations learned from observations (dynamic and learned rather than purely rule-based).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_mechanism</strong></td>
                            <td>Learned graph update procedure (per referenced work); paper only cites this prior work without experimental detail here.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_method</strong></td>
                            <td>Graph queried by downstream policy; details in the referenced paper.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Learning-based (details in referenced paper); cited as alternative to rule-based extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metric</strong></td>
                            <td>Not detailed here; cited as related work on text-based games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_findings</strong></td>
                            <td>Mentioned as a line of work addressing partial observability via dynamic KG learning; no ablation details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_limitations</strong></td>
                            <td>Referenced as related but not evaluated in this paper; limitations should be consulted in the original work.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_recommendations</strong></td>
                            <td>This paper suggests that entity/object-centric retrieval can be effective and may complement or simplify complex KG construction in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning', 'publication_date_yy_mm': '2020-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on textbased games <em>(Rating: 2)</em></li>
                <li>Nail: A general interactive fiction agent <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with a natural language action space <em>(Rating: 1)</em></li>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6471",
    "paper_id": "paper-061d113a7b3f32deab6bc50fea676fa0b1e0f658",
    "extraction_schema_id": "extraction-schema-128",
    "extracted_data": [
        {
            "name_short": "MPRC-DQN",
            "name_full": "Multi-Paragraph Reading Comprehension Deep Q-Network",
            "brief_description": "This paper's main agent that formulates IF action prediction as a multi-paragraph reading-comprehension task, retrieving object-centric past observations and scoring template-based actions with a BiDAF-style reader inside a DQN training loop.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "MPRC-DQN",
            "agent_description": "Value-based RL agent (DQN) whose Q-function is parameterized by an RC model: observation paragraphs (current + retrieved history) are treated as context and each action template (verb phrase) as a query; BiDAF-style context-query attention + self-attention produce verb-aware observation representations, object-specific embeddings fill template placeholders, and a linear head predicts Q-values. History retrieval is object-centric and time-sensitive.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho (suite of human-written Interactive Fiction games, e.g., Zork)",
            "memory_used": true,
            "memory_type": "object-centric retrieved observation buffer (external textual history)",
            "memory_representation": "Raw past observation texts (concatenated passages), separated by special tokens; objects detected in each observation are used as indices into these texts.",
            "memory_update_mechanism": "Rule-based time-sensitive retrieval: for each object detected in the current observation, retrieve the most recent K past observations that contain at least one shared object; the retrieved set is sorted by timestep and concatenated to form the extended observation. (K set to 2 in experiments.)",
            "memory_retrieval_method": "Object matching via named-object detection (spaCy); select most recent K matching observations per detected object; concatenate by time order.",
            "training_method": "Deep Q-Network (DQN) with prioritized trajectory sampling (trajectory-level prioritization based on better-than-average trajectories); epsilon-greedy exploration.",
            "evaluation_metric": "Average game score per game (Jericho scores), winning percentage / counts (number of games where agent is best), and normalized agent-to-human score ratio (macro-average capped at 100%).",
            "performance_with_memory": "Reported winning percentage 64% (21/33 games where agent is best) and in Table 2 per-game average scores (e.g., Zork1: 38.3). The paper also reports final state-of-the-art on 25 games (76%/25) when considering their approaches overall.",
            "performance_without_memory": null,
            "has_comparative_results": true,
            "ablation_findings": "History window size K is highly game-dependent; object-centric retrieval (time-sensitive) outperforms pure recency-based retrieval; pure recency strategies do not improve much over no-history (RC-DQN) early in learning. Using K=2 in experiments. MPRC-DQN also converges faster than the no-history RC-DQN.",
            "reported_limitations": "Utility of past observations is time-sensitive and game-dependent; inappropriate retrieval (e.g., pure recency) yields higher variance due to policy changes and may provide limited additional object information; choice of history size impacts performance per-game.",
            "best_practices_recommendations": "Use object-centric, time-sensitive retrieval rather than naive recency; keep retrieval window small (they use K=2); separate observations with special tokens when concatenating; detect objects robustly (they used spaCy) and prioritize recent observations per object to avoid stale or irrelevant evidence.",
            "uuid": "e6471.0",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "RC-DQN",
            "name_full": "Reading Comprehension Deep Q-Network (no-history variant)",
            "brief_description": "An ablation / baseline implementing the paper's RC-based action prediction model but only using the current observation (no retrieved history), trained with DQN.",
            "citation_title": "here",
            "mention_or_use": "use",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "RC-DQN",
            "agent_description": "Same RC-based Q-function architecture as MPRC-DQN (BiDAF-style attention between verb-template and observation, argument-specific embeddings) but uses only the current observation paragraph (no history concatenation) as input to the reader; trained with DQN.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho (Interactive Fiction games)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Deep Q-Network (DQN)",
            "evaluation_metric": "Average game score per game (Jericho scores), winning percentage / counts.",
            "performance_with_memory": null,
            "performance_without_memory": "RC-DQN (no-history) achieved a winning percentage of 52% (17/33 games where agent is best) in Table 2 and per-game scores listed in the RC-DQN column (e.g., Zork1: 38.8).",
            "has_comparative_results": true,
            "ablation_findings": "Demonstrates that the RC-style action prediction alone substantially improves performance over prior baselines; comparing RC-DQN to MPRC-DQN isolates the benefit of memory retrieval—MPRC-DQN yields higher winning percentages and faster convergence.",
            "reported_limitations": "Without history, some long-horizon decisions lacking current observation evidence remain hard; partial observability limits accurate action-value estimation.",
            "best_practices_recommendations": "Even without memory, RC-style template-aware attention gives strong gains; but for games with time-sensitive object state, introduce object-centric history retrieval to further boost performance.",
            "uuid": "e6471.1",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-A2C",
            "name_full": "KG-A2C (Knowledge-Graph A2C)",
            "brief_description": "Baseline agent from prior work that constructs an object-centric knowledge graph from historical observations (OpenIE + rules) and uses that graph as state representation in an A2C learning framework with a GRU-based action generator.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "mention",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "KG-A2C",
            "agent_description": "Prior work baseline: extracts objects and relations from historical observations using OpenIE and human-written heuristics to build a knowledge graph representing state; uses GRU-based action generator and A2C (actor-critic) for policy learning.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho (Interactive Fiction games)",
            "memory_used": true,
            "memory_type": "constructed knowledge graph (symbolic structured memory)",
            "memory_representation": "Symbolic object graph: objects as nodes and extracted relations as edges (constructed from historical observations via OpenIE and heuristics).",
            "memory_update_mechanism": "Rule-based information extraction (OpenIE) plus human-written rules to update/extend the knowledge graph from parsed historical observations.",
            "memory_retrieval_method": "Graph-based state representation queried by downstream policy network (no explicit retrieval described here beyond maintaining the graph); KG used as state input to policy.",
            "training_method": "A2C (advantage actor-critic) with a GRU-based action generator",
            "evaluation_metric": "Average game score per game (Jericho scores) as reported in baselines; used 1.6M interaction steps in reported baselines.",
            "performance_with_memory": "As a baseline, KG-A2C scores are reported per-game in Table 2 (e.g., detective: 207.9 in baseline column), but specific aggregate win percentages vs MPRC-DQN are provided in pairwise comparisons (MPRC-DQN beats KG-A2C 18/28 pairwise).",
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Paper cites KG-A2C as prior art and notes it treats historical observations equally to summarize into a single vector (graph) which can introduce noise; no detailed ablation here because KG-A2C is an external baseline.",
            "reported_limitations": "Relies on OpenIE and human-written rules which may not capture complex relations in human-written IF text; summarizing history into a single vector/graph can introduce noise and not focus on contexts relevant to a specific action.",
            "best_practices_recommendations": "The authors of this paper recommend more targeted, object-centric retrieval to reduce noise rather than summarizing all history equally into a single representation.",
            "uuid": "e6471.2",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge-Graph DQN (KG-DQN)",
            "brief_description": "Prior work that constructs knowledge graphs from past observations (objects, positions, relations) and uses that structured state representation with DRRN-like action handling.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "KG-DQN",
            "agent_description": "Constructs a knowledge graph state from historical observations using OpenIE-style extraction and rules (objects as nodes, relations as edges) and uses that graph for action selection with deep RL (DQN variant in original work).",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho / synthetic text games (prior work applied to synthetic games and adapted)",
            "memory_used": true,
            "memory_type": "knowledge graph (symbolic episodic/state memory)",
            "memory_representation": "Structured graph of objects, positions, and spatial relations extracted from history.",
            "memory_update_mechanism": "Rule-based extraction (OpenIE) and graph construction from past observations.",
            "memory_retrieval_method": "Graph representation used as input state (no retrieval-as-selection described in this paper's summary).",
            "training_method": "DQN-style reinforcement learning in the referenced work",
            "evaluation_metric": "Average score on text-adventure games (as reported in referenced work)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Cited as prior approach to partial observability; this paper argues that KG approaches summarize history equally and may not focus on action-relevant contexts.",
            "reported_limitations": "KG-based methods rely on extraction tools and rules which may fail on complex human-written IF texts; summarizing all history can introduce noise and not emphasize evidence relevant to the current decision.",
            "best_practices_recommendations": "Motivates object-centric retrieval and RC-style focused attention to find evidence specifically relevant to a candidate action, instead of global summaries.",
            "uuid": "e6471.3",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "NAIL",
            "name_full": "NAIL (No-learning heuristics agent)",
            "brief_description": "A strong non-learning baseline that uses heuristics for exploration, object interaction, and building an internal representation of the game world.",
            "citation_title": "Nail: A general interactive fiction agent",
            "mention_or_use": "mention",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "NAIL",
            "agent_description": "Rule-/heuristic-based agent using hand-designed procedures for exploring IF games, interacting with objects, and maintaining an internal world representation without learning.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho / Interactive Fiction games",
            "memory_used": true,
            "memory_type": "internal heuristic world representation (symbolic memory / database of facts)",
            "memory_representation": "Internal representation of the game world assembled via heuristics (objects, interactions, environment facts).",
            "memory_update_mechanism": "Heuristic procedures to update internal representation based on observed textual feedback and executed commands.",
            "memory_retrieval_method": "Agent-specific heuristics that consult the internal representation when deciding actions.",
            "training_method": "No learning (heuristic-based), though used as a comparative baseline",
            "evaluation_metric": "Average game scores on Jericho (as reported by baseline papers)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Mentioned as state-of-the-art among no-learning agents; cited to highlight non-learning internal representation approaches.",
            "reported_limitations": "Relies on heuristics and hand-crafted rules; not scalable or generalizable compared to learned approaches.",
            "best_practices_recommendations": "Not provided here; used as a strong heuristic baseline for comparison.",
            "uuid": "e6471.4",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "Baseline that projects state and action texts into vector spaces and matches them for action selection, treating actions as text sequences without distinguishing template roles.",
            "citation_title": "Deep reinforcement learning with a natural language action space",
            "mention_or_use": "mention",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "DRRN",
            "agent_description": "Projects each valid action and the current state into embedding spaces and predicts action values based on the similarity between state and action embeddings; actions treated as plain sequences.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho / text-based games",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Reinforcement learning (value-based), as in original DRRN work",
            "evaluation_metric": "Average game score per game (Jericho scores)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Cited as a baseline that does not exploit template-object compositionality and does not use history in the way MPRC-DQN does.",
            "reported_limitations": "Doesn't model compositionality/role differences of action elements; uses single-vector state representations that ignore fine-grained dependencies.",
            "best_practices_recommendations": "The present paper suggests using template-aware RC-style modeling and targeted history retrieval instead of DRRN's flat action embedding matching.",
            "uuid": "e6471.5",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "TDQN",
            "name_full": "Template-DQN / TDQN",
            "brief_description": "Baseline that predicts templates and objects, treating template and object selections independently (earlier template-based DQN approach).",
            "citation_title": "Interactive fiction games: A colossal adventure",
            "mention_or_use": "mention",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "TDQN",
            "agent_description": "Value-based RL baseline that decomposes actions into template and object slots but predicts template and objects independently, using a single-vector observation representation.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Jericho / Interactive Fiction games",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "DQN (as used in Jericho baseline implementations)",
            "evaluation_metric": "Average game score per game (Jericho)",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Used as a baseline to illustrate the benefit of RC-style joint modeling of templates and objects.",
            "reported_limitations": "Independent selection of template and objects ignores interactions between template and arguments, reducing data efficiency.",
            "best_practices_recommendations": "The authors recommend joint modeling of template-object interactions (as in RC-DQN) for improved performance.",
            "uuid": "e6471.6",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "AEDQN",
            "name_full": "Action-Elimination Deep Q-Network (AEDQN)",
            "brief_description": "Prior method that learns to predict invalid actions to eliminate them and accelerate policy learning, using expert demonstrations for elimination guidance.",
            "citation_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "mention_or_use": "mention",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "AEDQN",
            "agent_description": "Learns to predict invalid actions and eliminates them from consideration to make RL more efficient; relies on auxiliary prediction of invalidity often aided by demonstrations.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Text-adventure games (e.g., Zork)",
            "memory_used": false,
            "memory_type": null,
            "memory_representation": null,
            "memory_update_mechanism": null,
            "memory_retrieval_method": null,
            "training_method": "Deep Q-Network with an action-elimination auxiliary objective",
            "evaluation_metric": "Average game score / learning efficiency",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Cited as an approach to reduce combinatorial action space size by eliminating invalid actions.",
            "reported_limitations": "Requires demonstrations to learn reliable eliminations and may not generalize to unseen invalid actions.",
            "best_practices_recommendations": "Not discussed here; cited as complementary to action-space reduction approaches.",
            "uuid": "e6471.7",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        },
        {
            "name_short": "Dynamic-KG (Adhikari2020)",
            "name_full": "Learning Dynamic Knowledge Graphs to Generalize on Text-based Games",
            "brief_description": "Referenced recent work proposing dynamic knowledge-graph construction from gameplay to generalize on text-based games (cited in related work).",
            "citation_title": "Learning dynamic knowledge graphs to generalize on textbased games",
            "mention_or_use": "mention",
            "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
            "agent_name": "Dynamic Knowledge-Graph approaches (Adhikari et al. 2020)",
            "agent_description": "Prior research that learns to construct and update knowledge graphs dynamically from text observations to represent game state for downstream policies.",
            "model_name": null,
            "model_size": null,
            "benchmark_name": "Text-based games / Jericho (context)",
            "memory_used": true,
            "memory_type": "learned dynamic knowledge graph (structured episodic memory)",
            "memory_representation": "Graph of objects/relations learned from observations (dynamic and learned rather than purely rule-based).",
            "memory_update_mechanism": "Learned graph update procedure (per referenced work); paper only cites this prior work without experimental detail here.",
            "memory_retrieval_method": "Graph queried by downstream policy; details in the referenced paper.",
            "training_method": "Learning-based (details in referenced paper); cited as alternative to rule-based extraction.",
            "evaluation_metric": "Not detailed here; cited as related work on text-based games.",
            "performance_with_memory": null,
            "performance_without_memory": null,
            "has_comparative_results": false,
            "ablation_findings": "Mentioned as a line of work addressing partial observability via dynamic KG learning; no ablation details in this paper.",
            "reported_limitations": "Referenced as related but not evaluated in this paper; limitations should be consulted in the original work.",
            "best_practices_recommendations": "This paper suggests that entity/object-centric retrieval can be effective and may complement or simplify complex KG construction in some settings.",
            "uuid": "e6471.8",
            "source_info": {
                "paper_title": "Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning",
                "publication_date_yy_mm": "2020-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2,
            "sanitized_title": "playing_textadventure_games_with_graphbased_deep_reinforcement_learning"
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on textbased games",
            "rating": 2,
            "sanitized_title": "learning_dynamic_knowledge_graphs_to_generalize_on_textbased_games"
        },
        {
            "paper_title": "Nail: A general interactive fiction agent",
            "rating": 2,
            "sanitized_title": "nail_a_general_interactive_fiction_agent"
        },
        {
            "paper_title": "Deep reinforcement learning with a natural language action space",
            "rating": 1,
            "sanitized_title": "deep_reinforcement_learning_with_a_natural_language_action_space"
        },
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning",
            "rating": 1,
            "sanitized_title": "learn_what_not_to_learn_action_elimination_with_deep_reinforcement_learning"
        }
    ],
    "cost": 0.0171235,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning</h1>
<p>Xiaoxiao Guo<em><br>IBM Research<br>xiaoxiao.guo@ibm.com<br>$\frac{\text { Mo Yu</em> }}{\text { IBM Research }}$<br>yum@us.ibm.com<br>Yupeng Gao<br>IBM Research<br>yupeng.gao@ibm.com<br>Chuang Gan<br>MIT-IBM Watson AI Lab<br>chuangg@ibm.com<br>Murray Campbell<br>IBM Research<br>mcam@us.ibm.com<br>Shiyu Chang<br>MIT-IBM Watson AI Lab<br>shiyu.chang@ibm.com</p>
<h4>Abstract</h4>
<p>Interactive Fiction (IF) games with real humanwritten natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the humanwritten textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Interactive systems capable of understanding natural language and responding in the form of natural language text have high potentials in various applications. In pursuit of building and evaluating such systems, we study learning agents for Interactive Fiction (IF) games. IF games are world-simulating software in which players use text commands to control the protagonist and influence the world, as illustrated in Figure 1. IF gameplay agents need to simultaneously understand the game's information from a text display (observation) and generate</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Sample gameplay for the classic dungeon game Zork1. The objective is to solve various puzzles and collect the 19 treasures to install the trophy case. The player receives textual observations describing the current game state and additional reward scalars encoding the game designers' objective of game progress. The player sends textual action commands to control the protagonist.
natural language command (action) via a text input interface. Without providing an explicit game strategy, the agents need to identify behaviors that maximize objective-encoded cumulative rewards.</p>
<p>IF games composed of human-written texts (distinct from previous text games with synthetic texts) create superb new opportunities for studying and evaluating natural language understanding (NLU) techniques due to their unique characteristics. (1) Game designers elaborately craft on the literariness of the narrative texts to attract players when creating IF games. The resulted texts in IF games are more linguistically diverse and sophisticated than the template-generated ones in synthetic text games. (2) The language contexts of IF games</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of our approach to solving the IF games as Multi-Paragraph Reading Comprehension (MPRC) tasks.
are more versatile because various designers contribute to enormous domains and genres, such as adventure, fantasy, horror, and sci-fi. (3) The text commands to control characters are less restricted, having sizes over six orders of magnitude larger than previous text games. The recently introduced Jericho benchmark provides a collection of such IF games (Hausknecht et al., 2019a).</p>
<p>The complexity of IF games demands more sophisticated NLU techniques than those used in synthetic text games. Moreover, the task of designing IF game-play agents, intersecting NLU and reinforcement learning (RL), poses several unique challenges on the NLU techniques. The first challenge is the difficulty of exploration in the huge natural language action space. To make RL agents learn efficiently without prohibitive exhaustive trials, the action estimation must generalize learned knowledge from tried actions to others. To this end, previous approaches, starting with a single embedding vector of the observation, either predict the elements of actions independently (Narasimhan et al., 2015; Hausknecht et al., 2019a); or embed each valid action as another vector and predict action value based on the vector-space similarities (He et al., 2016). These methods do not consider the compositionality or role-differences of the action elements, or the interactions among them and the observation. Therefore, their modeling of the action values is less accurate and less data-efficient.</p>
<p>The second challenge is partial observability. At each game-playing step, the agent receives a textual observation describing the locations, objects, and characters of the game world. But the latest observation is often not a sufficient summary of the interaction history and may not provide enough
information to determine the long-term effects of actions. Previous approaches address this problem by building a representation over past observations (e.g., building a graph of objects, positions, and spatial relations) (Ammanabrolu and Riedl, 2019; Ammanabrolu and Hausknecht, 2020). These methods treat the historical observations equally and summarize the information into a single vector without focusing on important contexts related to the action prediction for the current observation. Therefore, their usages of history also bring noise, and the improvement is not always significant.</p>
<p>We propose a novel formulation of IF game playing as Multi-Passage Reading Comprehension (MPRC) and harness MPRC techniques to solve the huge action space and partial observability challenges. The graphical illustration is shown in Figure 2. First, the action value prediction (i.e., predicting the long-term rewards of selecting an action) is essentially generating and scoring a compositional action structure by finding supporting evidence from the observation. We base on the fact that each action is an instantiation of a template, i.e., a verb phrase with a few placeholders of object arguments it takes (Figure 2b). Then the action generation process can be viewed as extracting objects for a template's placeholders from the textual observation, based on the interaction between the template verb phrase and the relevant context of the objects in the observation. Our approach addresses the structured prediction and interaction problems with the idea of context-question attention mechanism in RC models. Specifically, we treat the observation as a passage and each template verb phrase as a question. The filling of object placeholders in the template thus becomes an</p>
<p>extractive QA problem that selects objects from the observation given the template. Simultaneously each action (i.e., a template with all placeholder replaced) gets its evaluation value predicted by the RC model. Our formulation and approach better capture the fine-grained interactions between observation texts and structural actions, in contrast to previous approaches that represent the observation as a single vector and ignore the fine-grained dependency among action elements.</p>
<p>Second, alleviating partial observability is essentially enhancing the current observation with potentially relevant history and predicting actions over the enhanced observation. Our approach retrieves potentially relevant historical observations with an object-centric approach (Figure 2a), so that the retrieved ones are more likely to be connected to the current observation as they describe at least one shared interactable object. Our attention mechanisms are then applied across the retrieved multiple observation texts to focus on informative contexts for action value prediction.</p>
<p>We evaluated our approach on the suite of Jericho IF games, compared to all previous approaches. Our approaches achieved or outperformed the state-of-the-art performance on 25 out of 33 games, trained with less than one-tenth of game interaction data used by prior art. We also provided ablation studies on our models and retrieval strategies.</p>
<h2>2 Related Work</h2>
<p>IF Game Agents. Previous work mainly studies the text understanding and generation in parserbased or rule-based text game tasks, such as TextWorld platform (Côté et al., 2018) or custom domains (Narasimhan et al., 2015; He et al., 2016; Adhikari et al., 2020). The recent platform Jericho (Hausknecht et al., 2019a) supports over thirty human-written IF games. Earlier successes in real IF games mainly rely on heuristics without learning. NAIL (Hausknecht et al., 2019b) is the state-of-theart among these "no-learning" agents, employing a series of reliable heuristics for exploring the game, interacting with objects, and building an internal representation of the game world. With the development of learning environments like Jericho, the RL-based agents have started to achieve dominating performance.</p>
<p>A critical challenge for learning-based agents is how to handle the combinatorial action space in IF games. LSTM-DQN (Narasimhan et al., 2015)
was proposed to generate verb-object action with pre-defined sets of possible verbs and objects, but treat the selection and learning of verbs and objects independently. Template-DQN (Hausknecht et al., 2019a) extended LSTM-DQN for template-based action generation, introducing one additional but still independent prediction output for the second object in the template. Deep Reinforcement Relevance Network (DRRN) (He et al., 2016) was introduced for choice-based games. Given a set of valid actions at every game state, DRRN projects each action into a hidden space that matches the current state representation vector for action selection. Action-Elimination Deep Q-Network (AEDQN) (Zahavy et al., 2018) learns to predict invalid actions in the adventure game Zork. It eliminates invalid action for efficient policy learning via utilizing expert demonstration data.</p>
<p>Other techniques focus on addressing the partial observability in text games. Knowledge Graph DQN (KG-DQN) (Ammanabrolu and Riedl, 2019) was proposed to deal with synthetic games. The method constructs and represents the game states as knowledge graphs with objects as nodes and uses pre-trained general purposed OpenIE tool and human-written rules to extract relations between objects. KG-DQN handles the action representation following DRRN. KG-A2C (Ammanabrolu and Hausknecht, 2020) later extends the work for IF games, by adding information extraction heuristics to fit the complexity of the object relations in IF games and utilizing a GRU-based action generator to handle the action space.</p>
<h2>Reading Comprehension Models for Question</h2>
<p>Answering. Given a question, reading comprehension (RC) aims to find the answer to the question based on a paragraph that may contain supporting evidence. One of the standard RC settings is extractive QA (Rajpurkar et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019), which extracts a span from the paragraph as an answer. Our formulation of IF game playing resembles this setting.</p>
<p>Many neural reader models have been designed for RC. Specifically, for the extractive QA task, the reader models usually build question-aware passage representations via attention mechanisms (Seo et al., 2016; Yu et al., 2018), and employ a pointer network to predict the start and end positions of the answer span (Wang and Jiang, 2016). Powerful pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019) have been</p>
<p>recently applied to enhance the encoding and attention mechanisms of the aforementioned reader models. They give performance boost but are more resource-demanding and do not suit the IF game playing task very well.</p>
<p>Reading Comprehension over Multiple Paragraphs. Multi-paragraph reading comprehension (MPRC) deals with the more general task of answering a question from multiple related paragraphs, where each paragraph may not necessarily support the correct answer. Our formulation becomes an MPRC setting when we enhance the state representation with historical observations and predict actions from multiple observation paragraphs.</p>
<p>A fundamental research problem in MPRC, which is also critical to our formulation, is to select relevant paragraphs from all the input paragraphs for the reader to focus on. Previous approaches mainly apply traditional IR approaches like BM25 (Chen et al., 2017; Joshi et al., 2017), or neural ranking models trained with distant supervision (Wang et al., 2018; Min et al., 2019a), for paragraph selection. Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018), which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches (Ding et al., 2019; Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019), where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links. These entity-centric paragraph retrieval approaches share a similar high-level idea to our object-based history retrieval approach. The techniques above have been applied to deal with evidence from Wikipedia, news collections, and, recently, books (Mou et al., 2020). We are the first to extend these ideas to IF games.</p>
<h2>3 Multi-Paragraph RC for IF Games</h2>
<h3>3.1 Problem Formulation</h3>
<p>Each IF game can be defined as a Partially Observable Markov Decision Process (POMDP), namely a 7-tuple of $\langle S, A, T, O, \Omega, R, \gamma\rangle$, representing the hidden game state set, the action set, the state transition function, the set of textual observations composed from vocabulary words, the textual observation function, the reward function, and the</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Our RC-based action prediction model architecture. The template text is a verb phrase with placeholders for objects, such as [pick up OBJ] and [break OBJ with OBJ].
discount factor respectively. The game playing agent interacts with the game engine in multiple turns until the game is over or the maximum number of steps is reached. At the $t$-th turn, the agent receives a textual observation describing the current game state $o_{t} \in O$ and sends a textual action command $a_{t} \in A$ back. The agent receives additional reward scalar $r_{t}$ which encodes the game designers' objective of game progress. Thus the task of the game playing can be formulated to generate a textual action command per step as to maximize the expected cumulative discounted rewards $\mathbf{E}\left[\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right]$. Value-based RL approaches learn to approximate an observation-action value function $Q\left(o_{t}, a_{t} ; \boldsymbol{\theta}\right)$ which measures the expected cumulative rewards of taking action $a_{t}$ when observing $o_{t}$. The agent selects action based on the action value prediction of $Q(o, a ; \boldsymbol{\theta})$.</p>
<p>Template Action Space. Template action space considers actions satisfying decomposition in the form of $\left\langle\right.$ verb, arg $\left.<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle$. verb is an interchangeable verb phrase template with placeholders for objects and $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>$ are optional objects. For example, the action command [east], [pick up eggs] and [break window with stone] can be represented as template actions $\langle$ east, none, none $\rangle$, $\langle$ pick up OBJ, eggs, none and $\langle$ break OBJ with OBJ, window, stone $\rangle$. We reuse the template library and object list from Jericho. The verb phrases usually consist of several vocabulary words and each object is usually a single word.</p>
<h3>3.2 RC Model for Template Actions</h3>
<p>We parameterize the observation-action value function $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg}<em 1="1">{0}, \operatorname{arg}</em>}\right\rangle ; \boldsymbol{\theta}\right)$ by utilizing the decomposition of the template actions and contextquery contextualized representation in RC. Our model treats the observation $o$ as a context in RC and the $\operatorname{verb}=\left(v_{1}, v_{2}, \ldots, v_{k}\right)$ component of the template actions as a query. Then a verb-aware observation representation is derived via a RC reader model with Bidirectional Attention Flow (BiDAF) (Seo et al., 2016) and self-attention. The observation representation responding to the $\operatorname{arg<em 1="1">{0}$ and $\operatorname{arg}</em>}$ words are pooled and projected to a scalar value estimate for $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em 1="1">{0}, \operatorname{arg}</em>\right)$. A high-level model architecture of our model is illustrated in Figure 3.}\right\rangle ; \boldsymbol{\theta</p>
<p>Observation and verb Representation. We tokenize the observation and the verb phrase into words, then embed these words using pre-trained GloVe embeddings (Pennington et al., 2014). A shared encoder block that consists of LayerNorm (Ba et al., 2016) and Bidirectional GRU (Cho et al., 2014) processes the observation and verb word embeddings to obtain the separate observation and verb representation.</p>
<p>Observation-verb Interaction Layers. Given the separate observation and verb representation, we apply two attention mechanisms to compute a verb-contextualized observation representation. We first apply BiDAF with observation as the context input and verb as the query input. Specifically, we denote the processed embeddings for observation word $i$ and template word $j$ as $\boldsymbol{o}<em j="j">{i}$ and $\boldsymbol{t}</em>}$. The attention between the two words is then $a_{i j}=\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{1}} \cdot \boldsymbol{o}</em>}}+\boldsymbol{w<em _boldsymbol_j="\boldsymbol{j">{\mathbf{2}} \cdot \boldsymbol{t}</em>}}+\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{3}} \cdot\left(\boldsymbol{o}</em>}} \otimes \boldsymbol{t<em _mathbf_1="\mathbf{1">{\boldsymbol{j}}\right)$, where $\boldsymbol{w}</em>}}, \boldsymbol{w<em _mathbf_3="\mathbf{3">{\mathbf{2}}$, $\boldsymbol{w}</em>}}$ are learnable vectors and $\otimes$ is element-wise product. We then compute the "verb2observation" attention vector for the $i$-th observation word as $\boldsymbol{c<em j="j">{\boldsymbol{i}}=\sum</em>} p_{i j} \boldsymbol{t<em i="i" j="j">{\boldsymbol{j}}$ with $p</em>}=\exp \left(a_{i j}\right) / \sum_{j} \exp \left(a_{i j}\right)$. Similarly, we compute the "observation2verb" attention vector as $\boldsymbol{q}=\sum_{i} p_{i} \boldsymbol{o<em i="i">{\boldsymbol{i}}$ with $p</em>=$ $\exp \left(\max <em i="i" j="j">{j} a</em> \exp \left(\max }\right) / \sum_{i<em i="i" j="j">{j} a</em>}\right)$. We concatenate and project the output vectors as $\boldsymbol{w<em _boldsymbol_i="\boldsymbol{i">{\mathbf{4}} \cdot\left[\boldsymbol{o}</em>}}\right.$, $\boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{o}</em>}} \otimes \boldsymbol{c<em _boldsymbol_i="\boldsymbol{i">{\boldsymbol{i}}, \boldsymbol{q} \otimes \boldsymbol{c}</em> \mid$, followed by a linear layer with leaky ReLU activation units (Maas et al., 2013). The output vectors are processed by an encoder block. We then apply a residual self-attention on the outputs of the encoder block. The self-attention is the same as BiDAF, but only between the observation and itself.}</p>
<p>Observation-Action Value Prediction. We generate an action by replacing the placeholders $\left(\operatorname{arg}<em 1="1">{0}\right.$ and $\left.\operatorname{arg}</em>}\right)$ in a template with objects appearing in the observation. The observation-action value $Q\left(o, a=\left\langle\right.\right.$ verb, $\left.\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>}\right\rangle ; \theta\right)$ is achieved by processing each object's corresponding verb-contextualized observation representation. Specifically, we get the indices of an $o b j$ in the observation texts $I(o b j, o)$. When the object is a noun phrase, we take the index of its headword. ${ }^{2}$ Because the same object has different meanings when it replaces different placeholders, we apply two GRU-based embedding functions for the two placeholders, to get the object's verb-placeholder dependent embeddings. We derive a single vector representation $\boldsymbol{h<em 0="0">{\text {org }</em>}=\text { obj <em 0="0">{m}}$ for the case that the placeholder $\operatorname{arg}</em>}$ is replaced by $o b j_{m}$ by meanpooling over the verb-placeholder dependent embeddings indexed by $I\left(o b j_{m}, o\right)$ for the corresponding placeholder $\operatorname{arg<em 5="5">{0}$. We apply a linear transformation on the concatenated embeddings of the two placeholders to obtain the observation action value $Q(o, a)=\boldsymbol{w}</em>} \cdot\left[\boldsymbol{h<em 1="1">{\text {org }</em>}=\text { obj <em _org="{org" _text="\text">{m}}, \boldsymbol{h}</em><em m="m">{1}=\text { obj }</em>}}\right]$ for $a=\langle$ verb, $\left.\operatorname{arg<em m="m">{0}=\operatorname{obj}</em>}, \operatorname{arg<em n="n">{1}=\operatorname{obj}</em>\right\rangle$. Our formulation avoids the repeated computation overhead among different actions with a shared template verb phrase.</p>
<h3>3.3 Multi-Paragraph Retrieval Method for Partial Observability</h3>
<p>The observation at the current step sometimes does not have full-textual evidence to support action selection and value estimation, due to the inherent partial observability of IF games. For example, when repeatedly attacking a troll with a sword, the player needs to know the effect or feedback of the last attack to determine if an extra attack is necessary. It is thus important for an agent to efficiently utilize historical observations to better support action value prediction. In our RC-based action prediction model, the historical observation utilization can be formulated as selecting evidential observation paragraphs in history, and predicting the action values from multiple selected observations, namely a Multiple-Paragraph Reading Comprehension (MPRC) problem. We propose to retrieve past observations with an object-centric approach.</p>
<p>Past Observation Retrieval. Multiple past observations may share objects with the current obser-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Agents</th>
<th>Action strategy</th>
<th>State strategy</th>
<th>Interaction data</th>
</tr>
</thead>
<tbody>
<tr>
<td>TDQN</td>
<td>Independent selection of template and the <br> two objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>DRRN</td>
<td>Action as a word sequence without distin- <br> guishing the roles of verbs and objects</td>
<td>None</td>
<td>1 M</td>
</tr>
<tr>
<td>KG-A2C</td>
<td>Recurrent neural decoder that selects the <br> template and objects in a fixed order</td>
<td>Object graph from historical observations <br> based on OpenIE and human-written rules</td>
<td>1.6 M</td>
</tr>
<tr>
<td>Ours</td>
<td>Observation-template representation for <br> object-centric value prediction</td>
<td>Object-based history observation retrieval</td>
<td>0.1 M</td>
</tr>
</tbody>
</table>
<p>Table 1: Summary of the main technical differences between our agent and the baselines. All agents use DQN to update the model parameters except KG-A2C uses A2C. All agents use the same handicaps.
vation, and it is computationally expensive and unnecessary to retrieve all of such observations. The utility of past observations associated with each object is often time-sensitive in that new observations may entirely or partially invalidate old observations. We thus propose a time-sensitive strategy for retrieving past observations. Specifically, given the detected objects from the current observation, we retrieve the most recent $K$ observations with at least one shared object. The $K$ retrieved observations are sorted by time steps and concatenated to the current observation. The observations from different time steps are separated by a special token. Our RC-based action prediction model treats the concatenated observations as the observation inputs, and no other parts are changed. We use the notation $o_{t}$ to represent the current observation and the extended current observation interchangeably.</p>
<h3>3.4 Training Loss</h3>
<p>We apply the Deep Q-Network (DQN) (Mnih et al., 2015) to update the parameters $\boldsymbol{\theta}$ of our RC-based action prediction model. The loss function is:</p>
<p>$$
\begin{aligned}
\mathcal{L}(\theta)=\mathbf{E}<em t="t">{\left(o</em> ; \theta\right)\right.\right. \
&amp; \left.\left.-\left(r_{t}+\gamma \max }, a_{t}, r_{t}, o_{t+1}\right) \sim \rho(\mathcal{D})} &amp; \left[\left|Q\left(o_{t}, a_{t<em t_1="t+1">{b} Q\left(o</em>\right)\right)\right|\right]
\end{aligned}
$$}, b ; \theta^{-</p>
<p>where $\mathcal{D}$ is the experience replay consisting of recent gameplay transition records and $\rho$ is a distribution over the transitions defined by a sampling strategy.</p>
<p>Prioritized Trajectories. The distribution $\rho$ has a decent impact on DQN performance. Previous work samples transition tuples with immediate positive rewards more frequently to speed up learning (Narasimhan et al., 2015; Hausknecht et al., 2019a). We observe that this heuristic is often insufficient. Some transitions with zero immediate
rewards or even negative rewards are also indispensable in recovering well-performed trajectories. We thus extend the strategy from transition level to trajectory level. We prioritize transitions from trajectories that outperform the exponential moving average score of recent trajectories.</p>
<h2>4 Experiments</h2>
<p>We evaluate our proposed methods on the suite of Jericho supported games. We compared to all previous baselines that include recent methods addressing the huge action space and partial observability challenges.</p>
<h3>4.1 Setup</h3>
<p>Jericho Handicaps and Configuration. The handicaps used by our methods are the same as other baselines. First, we use the Jericho API to check if an action is valid with game-specific templates. Second, we augmented the observation with the textual feedback returned by the command [inventory] and [look]. Previous work also included the last action or game score as additional inputs. Our model discarded these two types of inputs as we did not observe a significant difference by our model. The maximum game step number is set to 100 following baselines.</p>
<p>Implementation Details. We apply spaCy ${ }^{3}$ to tokenize the observations and detect the objects in the observations. We use the 100-dimensional GloVe embeddings as fixed word embeddings. The out-of-vocabulary words are mapped to a randomly initialized embedding. The dimension of Bi-GRU hidden states is 128 . We set the observation representation dimension to be 128 throughout the model. The history retrieval window $K$ is 2 . For DQN configuration, we use the $\epsilon$-greedy strategy</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Game</th>
<th style="text-align: center;">Human <br> Max</th>
<th style="text-align: center;">Walkthrough-100</th>
<th style="text-align: center;">TDQN</th>
<th style="text-align: center;">Baselines <br> DRRN</th>
<th style="text-align: center;">KG-A2C</th>
<th style="text-align: center;">Ours</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">MPRC-DQN</td>
<td style="text-align: center;">RC-DQN</td>
</tr>
<tr>
<td style="text-align: center;">905</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">acorncourt</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">advent</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">113</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">36</td>
<td style="text-align: center;">63.9</td>
<td style="text-align: center;">36</td>
</tr>
<tr>
<td style="text-align: center;">adventureland</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">24.2</td>
<td style="text-align: center;">21.7</td>
</tr>
<tr>
<td style="text-align: center;">afflicted</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">2.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">anchor</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">awaken</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">balances</td>
<td style="text-align: center;">51</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">10</td>
</tr>
<tr>
<td style="text-align: center;">deephome</td>
<td style="text-align: center;">300</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">detective</td>
<td style="text-align: center;">360</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">169</td>
<td style="text-align: center;">197.8</td>
<td style="text-align: center;">207.9</td>
<td style="text-align: center;">317.7</td>
<td style="text-align: center;">291.3</td>
</tr>
<tr>
<td style="text-align: center;">dragon</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">$-5.3$</td>
<td style="text-align: center;">$-3.5$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">4.84</td>
</tr>
<tr>
<td style="text-align: center;">enchanter</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">125</td>
<td style="text-align: center;">8.6</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">20.0</td>
</tr>
<tr>
<td style="text-align: center;">gold</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">inhumane</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">jewel</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">1.6</td>
<td style="text-align: center;">1.8</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">karn</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">library</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">6.3</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">14.3</td>
<td style="text-align: center;">17.7</td>
<td style="text-align: center;">18.1</td>
</tr>
<tr>
<td style="text-align: center;">ludicorp</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">13.8</td>
<td style="text-align: center;">17.8</td>
<td style="text-align: center;">19.7</td>
<td style="text-align: center;">17.0</td>
</tr>
<tr>
<td style="text-align: center;">moonlit</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">omniquest</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">16.8</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">pentari</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">17.4</td>
<td style="text-align: center;">27.2</td>
<td style="text-align: center;">50.7</td>
<td style="text-align: center;">44.4</td>
<td style="text-align: center;">43.8</td>
</tr>
<tr>
<td style="text-align: center;">reverb</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0.3</td>
<td style="text-align: center;">8.2</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">2.0</td>
<td style="text-align: center;">2.0</td>
</tr>
<tr>
<td style="text-align: center;">snacktime</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">9.7</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">sorcerer</td>
<td style="text-align: center;">400</td>
<td style="text-align: center;">150</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">20.8</td>
<td style="text-align: center;">5.8</td>
<td style="text-align: center;">38.6</td>
<td style="text-align: center;">38.3</td>
</tr>
<tr>
<td style="text-align: center;">spellbrkr</td>
<td style="text-align: center;">600</td>
<td style="text-align: center;">160</td>
<td style="text-align: center;">18.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">21.3</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">25</td>
</tr>
<tr>
<td style="text-align: center;">spirit</td>
<td style="text-align: center;">250</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.3</td>
<td style="text-align: center;">3.8</td>
<td style="text-align: center;">5.2</td>
</tr>
<tr>
<td style="text-align: center;">temple</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">7.9</td>
<td style="text-align: center;">7.4</td>
<td style="text-align: center;">7.6</td>
<td style="text-align: center;">8.0</td>
<td style="text-align: center;">8.0</td>
</tr>
<tr>
<td style="text-align: center;">tryst205</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">9.6</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">10.0</td>
<td style="text-align: center;">10.0</td>
</tr>
<tr>
<td style="text-align: center;">yomomma</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">$-$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: center;">zenon</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">3.9</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">zork1</td>
<td style="text-align: center;">350</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">9.9</td>
<td style="text-align: center;">32.6</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">38.3</td>
<td style="text-align: center;">38.8</td>
</tr>
<tr>
<td style="text-align: center;">zork3</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">$3^{a}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">3.63</td>
<td style="text-align: center;">2.83</td>
</tr>
<tr>
<td style="text-align: center;">ztuu</td>
<td style="text-align: center;">$100^{b}$</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">4.9</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">9.2</td>
<td style="text-align: center;">85.4</td>
<td style="text-align: center;">79.1</td>
</tr>
<tr>
<td style="text-align: center;">Winning percentage / counts</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24\%/8</td>
<td style="text-align: center;">30\%/10</td>
<td style="text-align: center;">27\%/9</td>
<td style="text-align: center;">64\%/21</td>
<td style="text-align: center;">52\%/17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">76\%/25</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 2: Average game scores on Jericho benchmark games. The best performing agent score per game is in bold.
The Winning percentage / counts row computes the percentage / counts of games that the corresponding agent is best. The scores of baselines are from their papers. The missing scores are represented as "-", for which games KG-A2C skipped. We also added the 100 -step results from a human-written game-playing walkthrough, as a reference of human-level scores. We denote the difficulty levels of the games defined in the original Jericho paper with colors in their names - possible (i.e., easy or normal) games in green color, difficult games in tan and extreme games in red. Best seen in color.
a Zork3 walkthrough does not maximize the score in the first 100 steps but explores more. ${ }^{b}$ Our agent discovers some unbounded reward loops in the game Ztuu.
for exploration, annealing $\epsilon$ from 1.0 to $0.05 . \gamma$ is 0.98 . We use Adam to update the weights with $10^{-4}$ learning rate. Other parameters are set to their default values. More details of the Reproducibility Checklist is in Appendix A.</p>
<p>Baselines. We compare with all the public results on the Jericho suite, namely TDQN (Hausknecht et al., 2019a), DRRN (He et al., 2016), and KGA2C (Ammanabrolu and Hausknecht, 2020). As discussed, our approaches differ from them mainly in the strategies of handling the large action space and partial observability of IF games. We summarize these main technical differences in Table 1. In summary, all previous agents predict actions con-
ditioned on a single vector representation of the whole observation texts. Thus they do not exploit the fine-grained interplay among the template components and the observations. Our approach addresses this problem by formulating action prediction as an RC task, better utilizing the rich textual observations with deeper language understanding.</p>
<p>Training Sample Efficiency. We update our models for 100, 000 times. Our agents interact with the environment one step per update, resulting in a total of 0.1 M environment interaction data. Compared to the other agents, such as KG-A2C (1.6M), TDQN (1M), and DRRN (1M), our environment interaction data is significantly smaller.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Game</th>
<th style="text-align: center;">Template Action <br> Space $\left(\times 10^{6}\right)$</th>
<th style="text-align: center;">Avg. Steps <br> Per Reward</th>
<th style="text-align: center;">Dialog <br> Actions</th>
<th style="text-align: center;">Darkness <br> Limit</th>
<th style="text-align: center;">Nonstandard</th>
<th style="text-align: center;">Inventory</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">advent</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">detective</td>
<td style="text-align: center;">19</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">karn</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">ludicorp</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">pentari</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: left;">spirit</td>
<td style="text-align: center;">195</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
<tr>
<td style="text-align: left;">zork3</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
</tr>
</tbody>
</table>
<p>Table 3: Difficulty levels and characteristics of games on which our approach achieves the most considerable improvement. Dialog indicates that it is necessary to speak with another character. Darkness indicates that accessing some dark areas requires a light source. Nonstandard Actions refers to actions with words not in an English dictionary. Inventory Limit restricts the number of items carried by the player. Please refer to (Hausknecht et al., 2019a) for more comprehensive definitions.</p>
<h3>4.2 Overall Performance</h3>
<p>We summarize the performance of our MultiParagraph Reading Comprehension DQN (MPRCDQN) agent and baselines in Table 2. Of the 33 IF games, our MPRC-DQN achieved or improved the state of the art performance on 21 games (i.e., a winning rate of $64 \%$ ). The best performing baseline (DRRN) achieved the state-of-the-art performance on only ten games, corresponding to the winning rate of $30 \%$, lower than half of ours. Note that all the methods achieved the same initial scores on five games, namely 905, anchor, awaken, deephome, and moonlit. Apart from these five games, our MPRC-DQN achieved more than three times wins. Our MPRC-DQN achieved significant improvement on some games, such as adventureland, afflicted, detective, etc. Appendix C shows some game playing trajectories.</p>
<p>We include the performance of an RC-DQN agent, which implements our RC-based action prediction model but only takes the current observations as inputs. It also outperformed the baselines by a large margin. After we consider the RC-DQN agent, our MPRC-DQN still has the highest winning percentage, indicating that our RC-based action prediction model has a significant impact on the performance improvement of our MPRC-DQN and the improvement from the multi-passage retrieval is also unneglectable. Moreover, compared to RC-DQN, our MPRC-DQN has another advantage of faster convergence. The learning curves of our MPRC-DQN and RC-DQN agents on various games are in Appendix B.</p>
<p>Finally, our approaches, overall, achieve the new state-of-the-art on 25 games (i.e., a winning rate of $76 \%$ ), giving a significant advance in the field of IF game playing.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Competitors</th>
<th style="text-align: center;">Win</th>
<th style="text-align: center;">Draw</th>
<th style="text-align: center;">Lose</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. TDQN</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. DRRN</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: left;">MPRC-DQN v.s. KG-A2C</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<p>Table 4: Pairwise comparison between our MPRC-DQN versus each baseline.</p>
<p>Pairwise Competition. To better understand the performance difference between our approach and each of the baselines, we adopt a direct one-to-one comparison metric based on the results from Table 2. Our approach has a high winning rate when competing with any of the baselines, summarized in Table 4. All the baselines have a rare chance to beat us on games. DRRN gives a higher chance of draw-games when competing with ours.</p>
<p>Human-Machine Gap. We additionally compare IF gameplay agents to human players to better understand the improvement significance and the potential improvement upper-bound. We measure each agent's game progress as the macro-average of the normalized agent-to-human game score ratios, capped at $100 \%$. The progress of our MPRCDQN is $28.5 \%$, while the best performing baseline DRRN is $17.8 \%$, showing that our agent's improvement is significant even in the realm of human players. Nevertheless, there is a vast gap between the learning agents and human players. The gap indicates IF games can be a good benchmark for the development of natural language understanding techniques.</p>
<p>Difficulty Levels of Games. Jericho categorizes the supported games into three difficulty levels, namely possible games, difficult games, and extreme games, based on the characteristics of the game dynamics, such as the action space size, the length of the game, and the average number of</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Learning curves for ablative studies. (left) Model ablative studies on the game Detective. (middle) Model ablative studies on Zork1. (right) Retrieval strategy study on Zork1. Best seen in color.</p>
<p>Steps to receive a non-zero reward. Our approach improves over prior art on seven of the sixteen possible games, seven of the eleven difficult games, and three of the six extreme games in Table 2. It shows that the strategies of our method are generally beneficial for any difficulty levels of game dynamics. Table 3 summarizes the characteristics of the seven games in which our method improves the most, i.e., larger than 15% of the game progress in the first 100 steps.4 First, these mostly improved games have medium action space sizes, and it is an advantageous setting for our methods where modeling the template-object-observation interactions is effective. Second, our approach improves most on games with a reasonably high degree of reward sparsity, such as <em>karn</em>, <em>spirit</em>, and <em>zork3</em>, indicating that our RC-based value function formulation helps in optimization and mitigates the reward sparsity. Finally, we remark that these game difficulty levels are not directly categorized based on natural language-related characteristics, such as text comprehension and puzzle-solving difficulties. Future studies on additional game categories based on those natural language-related characteristics would shed light on related improvements.</p>
<h3>4.3 Ablative Studies</h3>
<p><strong>RC-model Design.</strong> The overall results show that our RC-model plays a critical role in performance improvement. We compare our RC-model to some alternative models as ablative studies. We consider three alternatives, namely (1) our RC-model without the self-attention component (w/o self-att), (2) without the argumentspecific embedding (w/o arg-emb) and (3) our RC-model with Transformer-based block encoder (RC-Trans) following QANet (Yu et al., 2018). Detailed architecture is in Appendix A.</p>
<p>The learning curves for different RC-models are in Figure 4 (left/middle). The RC-models without either self-attention or argument-specific embedding degenerate, and the argument-specific embedding has a greater impact. The Transformer-based encoder block sometimes learns faster than Bi-GRU at the early learning stage. It achieved a comparable final performance, even with much greater computational resource requirements.</p>
<p><strong>Retrieval Strategy.</strong> We compare with history retrieval strategies with different history sizes (K) and pure recency-based strategies (i.e., taking the latest K observations as history, denoted as w/o rec). The learning curves of different strategies are in Figure 4 (right). In general, the impact of history window size is highly game-dependent, but the pure recency based ones do not differ significantly from RC-DQN at the beginning of learning. The issues of pure recency based strategy are: (1) limited additional information about objects provided by successive observations; and (2) higher variance of retrieved observations due to policy changes.</p>
<h3>5 Conclusion</h3>
<p>We formulate the general IF game playing as MPRC tasks, enabling an MPRC-style solution to efficiently address the key IF game challenges on the huge combinatorial action space and the partial observability in a unified framework. Our approaches achieved significant improvement over the previous state-of-the-art on both game scores and training data efficiency. Our formulation also bridges broader NLU/RC techniques to address other critical challenges in IF games for future work, e.g., common-sense reasoning, novelty-driven exploration, and multi-hop inference.</p>
<h3>Acknowledgments</h3>
<p>We would like to thank Matthew Hausknecht for helpful discussions on the Jericho environments.</p>
<p><sup>4</sup>We ignore <em>ztuu</em> due to the infinite reward loops.</p>
<h2>References</h2>
<p>Ashutosh Adhikari, Xingdi Yuan, Marc-Alexandre Côté, Mikuláš Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L Hamilton. 2020. Learning dynamic knowledge graphs to generalize on textbased games. arXiv preprint arXiv:2002.09127.</p>
<p>Prithviraj Ammanabrolu and Matthew Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. arXiv, pages arXiv2001.</p>
<p>Prithviraj Ammanabrolu and Mark Riedl. 2019. Playing text-adventure games with graph-based deep reinforcement learning. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3557-3565.</p>
<p>Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi, Richard Socher, and Caiming Xiong. 2019. Learning to retrieve reasoning paths over wikipedia graph for question answering. arXiv preprint arXiv:1911.10470.</p>
<p>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18701879 .</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259.</p>
<p>Marc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. 2018. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pages 41-75. Springer.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. 2019. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of ACL 2019.</p>
<p>Ameya Godbole, Dilip Kavarthapu, Rajarshi Das, Zhiyu Gong, Abhishek Singhal, Hamed Zamani, Mo Yu, Tian Gao, Xiaoxiao Guo, Manzil Zaheer, et al. 2019. Multi-step entity-centric information retrieval for multi-hop question answering. arXiv preprint arXiv:1909.07598.</p>
<p>Matthew Hausknecht, Prithviraj Ammanabrolu, MarcAlexandre Côté, and Xingdi Yuan. 2019a. Interactive fiction games: A colossal adventure. arXiv preprint arXiv:1909.05398.</p>
<p>Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019b. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language action space. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466.</p>
<p>Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. 2018. Denoising distantly supervised open-domain question answering. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17361745 .</p>
<p>Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, volume 30, page 3 .</p>
<p>Sewon Min, Danqi Chen, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2019a. A discrete hard em approach for weakly supervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 28442857.</p>
<p>Sewon Min, Danqi Chen, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019b. Knowledge guided text retrieval and reading for open domain question answering. arXiv preprint arXiv:1911.03868.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533.</p>
<p>Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo, Saloni Potdar, and Hui Su. 2020. Frustratingly hard evidence retrieval for qa over books. In Proceedings of the First Joint Workshop on Narrative Understanding, Storylines, and Events, pages 108-113.</p>
<p>Karthik Narasimhan, Tejas Kulkarni, and Regina Barzilay. 2015. Language understanding for text-based games using deep reinforcement learning. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1-11.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT, pages 2227-2237.</p>
<p>Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392.</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension.</p>
<p>Shuohang Wang and Jing Jiang. 2016. Machine comprehension using match-lstm and answer pointer. arXiv preprint arXiv:1608.07905.</p>
<p>Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018. R 3: Reinforced ranker-reader for open-domain question answering. In Thirty-Second AAAI Conference on Artificial Intelligence.</p>
<p>Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. 2017. Evidence aggregation for answer re-ranking in open-domain question answering. arXiv preprint arXiv:1711.05116.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.</p>
<p>Tom Zahavy, Matan Haroush, Nadav Merlis, Daniel J Mankowitz, and Shie Mannor. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Advances in Neural Information Processing Systems, pages 3562-3573.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ https://spacy.io&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
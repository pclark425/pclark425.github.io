<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2734 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2734</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2734</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-70.html">extraction-schema-70</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <p><strong>Paper ID:</strong> paper-258557517</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclanthology.org/2023.conll-1.10.pdf" target="_blank">A Minimal Approach for Natural Language Action Space in Text-based Games</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games (TGs) are language-based interactive environments for reinforcement learning. While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused. In this paper, we revisit the challenge of exploring the action space in TGs and propose \epsilon-admissible exploration, a minimal approach of utilizing admissible actions, for training phase. Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM. Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG. Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2734.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2734.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph Advantage Actor Critic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-game RL agent that constructs a knowledge graph from game observations to form state representations and prune the action space; uses an actor-critic policy over template-object action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-Critic agent that incrementally constructs a knowledge graph (KG) from textual observations (via OpenIE) and uses the KG to build state representations and to constrain object/action choices during action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>A suite of human-authored interactive fiction (text-adventure) games where agents issue natural-language commands and receive textual observations and sparse reward signals.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (long-term memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Graph of extracted triples (entity-relation-entity); nodes are entities/objects/locations and edges are relations; KG used directly in state representation and for constraining reachable object space (immediate neighbours).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Triples extracted from textual observations (entities, object mentions, relations), discovered objects and their relations (used as entities in the graph).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Graph-neighborhood / immediate-neighbour restriction for object selection; KG subgraph sampling for features (depends on variant); retrieval is structural (neighbor-based) rather than dense vector matching.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>KG is updated online by extracting triples from each textual observation (Stanford OpenIE pipeline) and adding them to the graph as gameplay proceeds.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Used as long-term state representation, to prune object/action candidates (masking to immediate neighbours) and inform policy decisions across time (provides memory of discovered entities/relations).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported baseline performance in Table 1: normalized mean = 0.24750 (KG-A2C column in paper's Table 1); per-game scores shown in Table 1 (e.g., ZORK1 ≈ 34.0 in that table).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Not reported as an ablation for KG-A2C itself; compared against TAC (no KG) which attains normalized mean = 0.3307 (paper reports TAC outperforms KG-A2C on average across evaluated games).</td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>The paper reports that KG provides long-term memory and action pruning benefits but its utility strongly depends on extraction quality: missing objects/entities in the KG lead to permanently masked-out actions and reduced exploration; KG-based agents must also learn admissible actions before exploration becomes meaningful, which can waste samples.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>KG quality/extraction errors (missing objects) can omit valid actions and hinder exploration; dependence on external NLP extraction (OpenIE) makes action selection brittle; requires learning admissible-action mappings before effective exploration (sample inefficiency); larger model/compute overhead compared to lighter designs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Minimal Approach for Natural Language Action Space in Text-based Games', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2734.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2734.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stacked Hierarchical Attention on Knowledge Graph (SHA-KG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KG-based variant that applies stacked hierarchical attention and Graph Attention Networks (GAT) over the knowledge graph to enrich state representations for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with stacked hierarchical attention for text-based games</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SHA-KG</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Builds a symbolic knowledge graph from observations and applies stacked hierarchical attention (including GAT) to produce enriched state encodings used by an actor-critic policy.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Human-authored interactive fiction games requiring exploration, object manipulation, and sparse-reward planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (graph-based memory)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Symbolic graph (triples/entities); uses Graph Attention Network (GAT) to attend over and sample subgraphs for enriched state features.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Extracted triples/entities and their relations from game textual observations.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Subgraph sampling and attention (GAT) to select relevant parts of the KG for current decision.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>KG is updated online from textual observations (OpenIE or extraction pipeline) as scenes are observed.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Enrich state representation for action selection and to focus policy on relevant entities via attention over KG subgraphs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper's Table 1: normalized mean = 0.24900 (SHA-KG column); per-game scores included in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Using attention over KG subgraphs can enrich state representations, but benefits still depend on KG extraction quality; attention/GAT is used to pick relevant graph parts for action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Dependent on KG construction quality; additional model complexity and compute; if KG misses entities the attention mechanism cannot recover missing candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Minimal Approach for Natural Language Action Space in Text-based Games', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2734.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2734.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT (KG-construction via QA with ALBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses a fine-tuned ALBERT QA / language model to extract targeted facts from observations to improve the quality of a symbolic knowledge graph used for RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses an ALBERT-based QA model (fine-tuned on text-game specific data) to query observations (e.g., "Where is my current location?") and extract higher-quality facts to construct a KG which is then used by downstream policy components.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Suite of text-adventure games requiring natural language understanding and long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (KG built from QA-extracted facts)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Symbolic graph of entities/relations populated by outputs of an ALBERT QA model; structured triples used in KG.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Facts extracted by QA model (location, object relations, etc.) from textual observations to populate KG.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>KG-based retrieval; improved extraction quality increases coverage of relevant entities for action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>KG updated by QA extractions performed on observations during play; QA model produces facts to inject into KG.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>To improve KG quality so that the KG better supports state representation and action pruning; to answer targeted queries about the environment to populate memory.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper's Table 1: normalized mean = 0.27880 (Q*BERT column); per-game scores shown in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Improving the quality of extractions that populate the KG (via QA models) can improve KG-based agents' performance because it reduces missing objects/entities that otherwise obstruct action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Relies on the accuracy of the QA/LM extractor; extraction errors or limited QA coverage still lead to incomplete KG and masked-out valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Minimal Approach for Natural Language Action Space in Text-based Games', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2734.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2734.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HEX-RL / HEX-RL-IM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HEX-RL (and HEX-RL-IM variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A KG-based agent emphasizing explainability via hierarchical graph attention over symbolic KG state representations; HEX-RL-IM adds intrinsic motivation to expand the KG during exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Inherently explainable reinforcement learning in natural language</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HEX-RL / HEX-RL-IM</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Uses hierarchical graph-attention over a symbolic KG for state encoding and action choice; the -IM variant adds an intrinsic reward signal that encourages KG expansion (exploration) as part of the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Human-authored interactive fiction games with language-based action spaces and sparse rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>knowledge graph (symbolic memory) with expansion driven by intrinsic rewards (HEX-RL-IM)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Symbolic triples graph; hierarchical attention layers applied to KG to form state features.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Entities/triples extracted from observations; expanded over time when intrinsic reward encourages discovery of new facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Hierarchical attention over KG to select relevant substructures for decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>KG expanded online from observations; in HEX-RL-IM intrinsic rewards are added to incentivize discovering and adding new KG facts.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>State representation via KG features, explainable decision traces, and in HEX-RL-IM to incentivize exploration and KG growth.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Reported in paper's Table 1: HEX-RL normalized mean = 0.2722; HEX-RL-IM normalized mean = 0.2834; per-game scores listed in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Intrinsic-motivation to expand KG can improve exploration and overall performance (HEX-RL-IM > HEX-RL in normalized mean), but effectiveness depends on balancing exploration incentives and task reward; KG expansion helps find new reward-bearing states.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>Potential instability if intrinsic expansion is not well-calibrated; still vulnerable to extraction quality of KG and added model complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Minimal Approach for Natural Language Action Space in Text-based Games', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2734.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2734.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve text games, including details about the memory architecture, performance comparisons, and what makes memory effective or ineffective.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Text-based Actor-Critic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper's proposed lightweight actor-critic agent that does not use a knowledge graph or a separate LM; instead it uses a template/object decoder for actions and -admissible exploration with a prioritized experience replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>TAC (Text-based Actor-Critic)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Actor-Critic architecture with text encoder (shared Bi-GRU), template/object GRU decoders for generating textual commands, two state-action critics, and prioritized experience replay; uses -admissible exploration (sampling admissible actions during training) rather than a KG or LM.</td>
                        </tr>
                        <tr>
                            <td><strong>base_model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>game_benchmark_name</strong></td>
                            <td>Jericho</td>
                        </tr>
                        <tr>
                            <td><strong>game_description</strong></td>
                            <td>Human-authored interactive fiction games; TAC trained and evaluated across many Jericho games in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>off-policy experience memory (prioritized experience replay)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_structure</strong></td>
                            <td>Prioritized replay buffer storing experience tuples (observation, action, reward, next observation); new experiences assigned priority using TD-error at insertion.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_content</strong></td>
                            <td>Past transitions: observations (o_game, o_look, o_inv, score), actions (generated template-object commands), rewards, next observations, and TD-errors used for prioritization.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_capacity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>memory_retrieval_strategy</strong></td>
                            <td>Priority-based sampling (prioritized experience replay) that favors experiences with larger TD error; half of updates influenced by recent experiences due to parallel envs and priority scheme.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_update_strategy</strong></td>
                            <td>New experiences are added each step to the replay buffer with initial priority equal to their TD-error (rather than max priority); buffer is sampled for minibatch updates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_usage_purpose</strong></td>
                            <td>Stabilize off-policy RL updates, enable learning from diverse past experiences, and improve sample efficiency compared to purely on-policy methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>TAC achieves the best reported performance in this paper: normalized mean = 0.3307 and per-table per-game scores (e.g., ZORK1 final score ≈ 46.3 in Table 1); TAC outperforms KG- and LM-based baselines on average over the evaluated games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_memory_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_effectiveness_findings</strong></td>
                            <td>Prioritized replay helps efficient learning in the off-policy -admissible exploration setting; combining PER with -admissible exploration collects more diverse and higher-quality experiences enabling better final performance than KG/LM baselines in many games.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations</strong></td>
                            <td>No explicit ablation isolating PER effect reported; TAC still suffers from overfitting, catastrophic forgetting, and exploration failures in some games; PER insertion strategy (priority = TD-error) risks not using some experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_memory_types</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>best_memory_configuration</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Minimal Approach for Natural Language Action Space in Text-based Games', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning with stacked hierarchical attention for text-based games <em>(Rating: 2)</em></li>
                <li>Inherently explainable reinforcement learning in natural language <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games <em>(Rating: 1)</em></li>
                <li>Modeling worlds in text <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2734",
    "paper_id": "paper-258557517",
    "extraction_schema_id": "extraction-schema-70",
    "extracted_data": [
        {
            "name_short": "KG-A2C",
            "name_full": "Knowledge Graph Advantage Actor Critic",
            "brief_description": "A text-game RL agent that constructs a knowledge graph from game observations to form state representations and prune the action space; uses an actor-critic policy over template-object action generation.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "use",
            "agent_name": "KG-A2C",
            "agent_description": "Actor-Critic agent that incrementally constructs a knowledge graph (KG) from textual observations (via OpenIE) and uses the KG to build state representations and to constrain object/action choices during action generation.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "A suite of human-authored interactive fiction (text-adventure) games where agents issue natural-language commands and receive textual observations and sparse reward signals.",
            "uses_memory": true,
            "memory_type": "knowledge graph (long-term memory)",
            "memory_structure": "Graph of extracted triples (entity-relation-entity); nodes are entities/objects/locations and edges are relations; KG used directly in state representation and for constraining reachable object space (immediate neighbours).",
            "memory_content": "Triples extracted from textual observations (entities, object mentions, relations), discovered objects and their relations (used as entities in the graph).",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Graph-neighborhood / immediate-neighbour restriction for object selection; KG subgraph sampling for features (depends on variant); retrieval is structural (neighbor-based) rather than dense vector matching.",
            "memory_update_strategy": "KG is updated online by extracting triples from each textual observation (Stanford OpenIE pipeline) and adding them to the graph as gameplay proceeds.",
            "memory_usage_purpose": "Used as long-term state representation, to prune object/action candidates (masking to immediate neighbours) and inform policy decisions across time (provides memory of discovered entities/relations).",
            "performance_with_memory": "Reported baseline performance in Table 1: normalized mean = 0.24750 (KG-A2C column in paper's Table 1); per-game scores shown in Table 1 (e.g., ZORK1 ≈ 34.0 in that table).",
            "performance_without_memory": "Not reported as an ablation for KG-A2C itself; compared against TAC (no KG) which attains normalized mean = 0.3307 (paper reports TAC outperforms KG-A2C on average across evaluated games).",
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "The paper reports that KG provides long-term memory and action pruning benefits but its utility strongly depends on extraction quality: missing objects/entities in the KG lead to permanently masked-out actions and reduced exploration; KG-based agents must also learn admissible actions before exploration becomes meaningful, which can waste samples.",
            "memory_limitations": "KG quality/extraction errors (missing objects) can omit valid actions and hinder exploration; dependence on external NLP extraction (OpenIE) makes action selection brittle; requires learning admissible-action mappings before effective exploration (sample inefficiency); larger model/compute overhead compared to lighter designs.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2734.0",
            "source_info": {
                "paper_title": "A Minimal Approach for Natural Language Action Space in Text-based Games",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "SHA-KG",
            "name_full": "Stacked Hierarchical Attention on Knowledge Graph (SHA-KG)",
            "brief_description": "A KG-based variant that applies stacked hierarchical attention and Graph Attention Networks (GAT) over the knowledge graph to enrich state representations for action selection.",
            "citation_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "mention_or_use": "use",
            "agent_name": "SHA-KG",
            "agent_description": "Builds a symbolic knowledge graph from observations and applies stacked hierarchical attention (including GAT) to produce enriched state encodings used by an actor-critic policy.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Human-authored interactive fiction games requiring exploration, object manipulation, and sparse-reward planning.",
            "uses_memory": true,
            "memory_type": "knowledge graph (graph-based memory)",
            "memory_structure": "Symbolic graph (triples/entities); uses Graph Attention Network (GAT) to attend over and sample subgraphs for enriched state features.",
            "memory_content": "Extracted triples/entities and their relations from game textual observations.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Subgraph sampling and attention (GAT) to select relevant parts of the KG for current decision.",
            "memory_update_strategy": "KG is updated online from textual observations (OpenIE or extraction pipeline) as scenes are observed.",
            "memory_usage_purpose": "Enrich state representation for action selection and to focus policy on relevant entities via attention over KG subgraphs.",
            "performance_with_memory": "Reported in paper's Table 1: normalized mean = 0.24900 (SHA-KG column); per-game scores included in Table 1.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Using attention over KG subgraphs can enrich state representations, but benefits still depend on KG extraction quality; attention/GAT is used to pick relevant graph parts for action selection.",
            "memory_limitations": "Dependent on KG construction quality; additional model complexity and compute; if KG misses entities the attention mechanism cannot recover missing candidates.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2734.1",
            "source_info": {
                "paper_title": "A Minimal Approach for Natural Language Action Space in Text-based Games",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "Q*BERT",
            "name_full": "Q*BERT (KG-construction via QA with ALBERT)",
            "brief_description": "An approach that uses a fine-tuned ALBERT QA / language model to extract targeted facts from observations to improve the quality of a symbolic knowledge graph used for RL.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Q*BERT",
            "agent_description": "Uses an ALBERT-based QA model (fine-tuned on text-game specific data) to query observations (e.g., \"Where is my current location?\") and extract higher-quality facts to construct a KG which is then used by downstream policy components.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Suite of text-adventure games requiring natural language understanding and long-horizon planning.",
            "uses_memory": true,
            "memory_type": "knowledge graph (KG built from QA-extracted facts)",
            "memory_structure": "Symbolic graph of entities/relations populated by outputs of an ALBERT QA model; structured triples used in KG.",
            "memory_content": "Facts extracted by QA model (location, object relations, etc.) from textual observations to populate KG.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "KG-based retrieval; improved extraction quality increases coverage of relevant entities for action generation.",
            "memory_update_strategy": "KG updated by QA extractions performed on observations during play; QA model produces facts to inject into KG.",
            "memory_usage_purpose": "To improve KG quality so that the KG better supports state representation and action pruning; to answer targeted queries about the environment to populate memory.",
            "performance_with_memory": "Reported in paper's Table 1: normalized mean = 0.27880 (Q*BERT column); per-game scores shown in Table 1.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Improving the quality of extractions that populate the KG (via QA models) can improve KG-based agents' performance because it reduces missing objects/entities that otherwise obstruct action selection.",
            "memory_limitations": "Relies on the accuracy of the QA/LM extractor; extraction errors or limited QA coverage still lead to incomplete KG and masked-out valid actions.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2734.2",
            "source_info": {
                "paper_title": "A Minimal Approach for Natural Language Action Space in Text-based Games",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "HEX-RL / HEX-RL-IM",
            "name_full": "HEX-RL (and HEX-RL-IM variant)",
            "brief_description": "A KG-based agent emphasizing explainability via hierarchical graph attention over symbolic KG state representations; HEX-RL-IM adds intrinsic motivation to expand the KG during exploration.",
            "citation_title": "Inherently explainable reinforcement learning in natural language",
            "mention_or_use": "use",
            "agent_name": "HEX-RL / HEX-RL-IM",
            "agent_description": "Uses hierarchical graph-attention over a symbolic KG for state encoding and action choice; the -IM variant adds an intrinsic reward signal that encourages KG expansion (exploration) as part of the objective.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Human-authored interactive fiction games with language-based action spaces and sparse rewards.",
            "uses_memory": true,
            "memory_type": "knowledge graph (symbolic memory) with expansion driven by intrinsic rewards (HEX-RL-IM)",
            "memory_structure": "Symbolic triples graph; hierarchical attention layers applied to KG to form state features.",
            "memory_content": "Entities/triples extracted from observations; expanded over time when intrinsic reward encourages discovery of new facts.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Hierarchical attention over KG to select relevant substructures for decision-making.",
            "memory_update_strategy": "KG expanded online from observations; in HEX-RL-IM intrinsic rewards are added to incentivize discovering and adding new KG facts.",
            "memory_usage_purpose": "State representation via KG features, explainable decision traces, and in HEX-RL-IM to incentivize exploration and KG growth.",
            "performance_with_memory": "Reported in paper's Table 1: HEX-RL normalized mean = 0.2722; HEX-RL-IM normalized mean = 0.2834; per-game scores listed in Table 1.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Intrinsic-motivation to expand KG can improve exploration and overall performance (HEX-RL-IM &gt; HEX-RL in normalized mean), but effectiveness depends on balancing exploration incentives and task reward; KG expansion helps find new reward-bearing states.",
            "memory_limitations": "Potential instability if intrinsic expansion is not well-calibrated; still vulnerable to extraction quality of KG and added model complexity.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2734.3",
            "source_info": {
                "paper_title": "A Minimal Approach for Natural Language Action Space in Text-based Games",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "TAC",
            "name_full": "Text-based Actor-Critic",
            "brief_description": "The paper's proposed lightweight actor-critic agent that does not use a knowledge graph or a separate LM; instead it uses a template/object decoder for actions and -admissible exploration with a prioritized experience replay buffer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "TAC (Text-based Actor-Critic)",
            "agent_description": "Actor-Critic architecture with text encoder (shared Bi-GRU), template/object GRU decoders for generating textual commands, two state-action critics, and prioritized experience replay; uses -admissible exploration (sampling admissible actions during training) rather than a KG or LM.",
            "base_model_size": null,
            "game_benchmark_name": "Jericho",
            "game_description": "Human-authored interactive fiction games; TAC trained and evaluated across many Jericho games in the paper.",
            "uses_memory": true,
            "memory_type": "off-policy experience memory (prioritized experience replay)",
            "memory_structure": "Prioritized replay buffer storing experience tuples (observation, action, reward, next observation); new experiences assigned priority using TD-error at insertion.",
            "memory_content": "Past transitions: observations (o_game, o_look, o_inv, score), actions (generated template-object commands), rewards, next observations, and TD-errors used for prioritization.",
            "memory_capacity": null,
            "memory_retrieval_strategy": "Priority-based sampling (prioritized experience replay) that favors experiences with larger TD error; half of updates influenced by recent experiences due to parallel envs and priority scheme.",
            "memory_update_strategy": "New experiences are added each step to the replay buffer with initial priority equal to their TD-error (rather than max priority); buffer is sampled for minibatch updates.",
            "memory_usage_purpose": "Stabilize off-policy RL updates, enable learning from diverse past experiences, and improve sample efficiency compared to purely on-policy methods.",
            "performance_with_memory": "TAC achieves the best reported performance in this paper: normalized mean = 0.3307 and per-table per-game scores (e.g., ZORK1 final score ≈ 46.3 in Table 1); TAC outperforms KG- and LM-based baselines on average over the evaluated games.",
            "performance_without_memory": null,
            "has_memory_ablation": false,
            "memory_effectiveness_findings": "Prioritized replay helps efficient learning in the off-policy -admissible exploration setting; combining PER with -admissible exploration collects more diverse and higher-quality experiences enabling better final performance than KG/LM baselines in many games.",
            "memory_limitations": "No explicit ablation isolating PER effect reported; TAC still suffers from overfitting, catastrophic forgetting, and exploration failures in some games; PER insertion strategy (priority = TD-error) risks not using some experiences.",
            "comparison_with_other_memory_types": true,
            "best_memory_configuration": null,
            "uuid": "e2734.4",
            "source_info": {
                "paper_title": "A Minimal Approach for Natural Language Action Space in Text-based Games",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2,
            "sanitized_title": "graph_constrained_reinforcement_learning_for_natural_language_action_spaces"
        },
        {
            "paper_title": "Deep reinforcement learning with stacked hierarchical attention for text-based games",
            "rating": 2,
            "sanitized_title": "deep_reinforcement_learning_with_stacked_hierarchical_attention_for_textbased_games"
        },
        {
            "paper_title": "Inherently explainable reinforcement learning in natural language",
            "rating": 2,
            "sanitized_title": "inherently_explainable_reinforcement_learning_in_natural_language"
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "rating": 1,
            "sanitized_title": "keep_calm_and_explore_language_models_for_action_generation_in_textbased_games"
        },
        {
            "paper_title": "Modeling worlds in text",
            "rating": 1,
            "sanitized_title": "modeling_worlds_in_text"
        }
    ],
    "cost": 0.016785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A Minimal Approach for Natural Language Action Space in Text-based Games
6 May 2023</p>
<p>Kelvin Dongwon 
Department of Data Science &amp; AI
Language Technology Lab
Monash University ♥ University of Liverpool ♦ Griffith University
University of Cambridge</p>
<p>Ryu 
Department of Data Science &amp; AI
Language Technology Lab
Monash University ♥ University of Liverpool ♦ Griffith University
University of Cambridge</p>
<p>Meng Fang meng.fang@liverpool.ac.uk 
Department of Data Science &amp; AI
Language Technology Lab
Monash University ♥ University of Liverpool ♦ Griffith University
University of Cambridge</p>
<p>Pan ♦♥ Shirui 
Department of Data Science &amp; AI
Language Technology Lab
Monash University ♥ University of Liverpool ♦ Griffith University
University of Cambridge</p>
<p>Gholamreza Haffari 
Department of Data Science &amp; AI
Language Technology Lab
Monash University ♥ University of Liverpool ♦ Griffith University
University of Cambridge</p>
<p>Ehsan Shareghi 
Department of Data Science &amp; AI
Language Technology Lab
Monash University ♥ University of Liverpool ♦ Griffith University
University of Cambridge</p>
<p>A Minimal Approach for Natural Language Action Space in Text-based Games
6 May 20233F8FDB1AEC2500FF14400FBB1FFF9709arXiv:2305.04082v1[cs.LG]
Text-based games (TGs) are language-based interactive environments for reinforcement learning.While language models (LMs) and knowledge graphs (KGs) are commonly used for handling large action space in TGs, it is unclear whether these techniques are necessary or overused.In this paper, we revisit the challenge of exploring the action space in TGs and propose -admissible exploration, a minimal approach of utilizing admissible actions, for training phase.Additionally, we present a text-based actor-critic (TAC) agent that produces textual commands for game, solely from game observations, without requiring any KG or LM.Our method, on average across 10 games from Jericho, outperforms strong baselines and state-of-the-art agents that use LM and KG.Our approach highlights that a much lighter model design, with a fresh perspective on utilizing the information within the environments, suffices for an effective exploration of exponentially large action spaces. 1</p>
<p>Introduction</p>
<p>A goal-driven intelligent agent that communicates in natural language space has been a long goal of artificial intelligence.Text-based games (TGs) best suit this goal, since they allow the agent to read the textual description of the world and write the textual command to the world (Hausknecht et al., 2020;Côté et al., 2018).In TGs, the agent should perform natural language understanding (NLU), sequential reasoning and natural language generation (NLG) to generate a series of actions to accomplish the goal of the game, i.e. adventure or puzzle (Hausknecht et al., 2020).The language perspective of TGs foists environments partially observable and action space combinatorially large, making the task challenging.Since TGs alert the player how much the game has proceeded with the game score, reinforcement learning (RL) naturally lends itself as a suitable framework.</p>
<p>Due to its language action space, an RL agent in TGs typically deals with a combinatorially large action space, motiving various design choices to account for it.As two seminal works in this space, Yao et al. (2020) trained a language model (LM) to produce admissible actions2 for the given textual observation and then used, under the predicted action list, Deep Reinforcement Relevance Network to estimate the Q value.As an alternative, Ammanabrolu and Hausknecht (2020) constructs a knowledge graph (KG) to prune down action space while learning the policy distribution through actorcritic (AC) method and supervision signal from the admissible actions.Both paradigms leverage admissible actions at different stages at the cost of imposing additional modules and increasing model complexity.</p>
<p>In this paper, we take a fresh perspective on leveraging the information available in the TG environment to explore the action space without relying on LMs or KGs.We propose a minimal form of utilizing admissibility of actions to constrain the action space during training while allowing the agent to act independently to access the admissible actions during testing.More concretely, our proposed training strategy, -admissible exploration, leverages the admissible actions via random sampling during training to acquire diverse and useful data from the environment.Then, our developed textbased actor-critic (TAC) agent learns the policy distribution without any action space constraints.It is noteworthy that our much lighter proposal is under the same condition as other aforementioned methods since all the prior works use admissible actions in training the LM or the agent.</p>
<p>Our empirical findings, in Jericho, illustrate that TAC with -admissible exploration has better or on-par performance in comparison with the stateof-the-art agents that use an LM or KG.Through experiments, we observed that while previous methods have their action selections largely dependent on the quality of the LM or KG, sampling admissible actions helps with the action selection and results in acquiring diverse experiences during exploration.While showing a significant success on TGs, we hope our approach encourages alternative perspectives on leveraging action admissibility in other domains of applications where the action space is discrete and combinatorially large.</p>
<p>Basic Definitions</p>
<p>Text-based Games.TGs are game simulation environments that take natural language commands and return textual description of the world.They have received significant attention in both NLP and RL communities in recent years.Côté et al. (2018) introduced TextWorld, a TG framework that automatically generates textual observation through knowledge base in a game engine.It has several hyper-parameters to control the variety and difficulty of the game.Hausknecht et al. (2020) released Jericho, an open-sourced interface for human-made TGs, which has become the de-facto testbed for developments in TG.Admissible Action.A list of natural language actions that are guaranteed to be understood by the game engine and change the environment in TGs are called Admissible Actions.The term was introduced in TextWorld while a similar concept also exists in Jericho under a different name, valid actions.Hausknecht et al. (2020) proposed an algorithm that detects a set of admissible actions provided by Jericho suite by constructing a set of natural language actions from every template with detectable objects for a given observation and running them through the game engine to return those actions that changed the world object tree.</p>
<p>Template-based Action Space.Natural language actions are built with template (T) and object (O) from template-based action space.Each template takes at most two objects.For instance, a templateobject pair (take OBJ from OBJ, egg, fridge) produces a natural language action take egg from fridge while (west,-,-) produces west.The agent ought to find the optimal policy that maximizes the expected discounted sum of rewards, or the return, R t = ∞ k=0 γ k r t+k+1 .Traditional Reinforcement Learning.There are three traditional algorithms in RL, Q-learning (QL), policy gradient (PG) and actor-critic (AC).QL estimates the return for a given state-action pair, or Q
value, Q(s t , a t ) = E[ ∞ k=0 γ k r t+k+1 |s t , a t ],
then selects the action of the highest Q value.However, this requires the action space to be countably finite.To remedy this, PG directly learns the policy distribution from the environment such that it maximizes the total return through Monte-Carlo (MC) sampling.AC combines QL and PG, where it removes MC in PG and updates the parameters per each step with estimated Q value using QL.This eliminates the high variance of MC as an exchange of a relatively small bias from QL.</p>
<p>Related Work on TG Agents in RL</p>
<p>We provide a brief overview of widely known TG agents relevant to the work presented in this paper.We empirically compare these in the Section 5.1.Contextual Action LM (CALM)-DRRN (Yao et al., 2020) uses an LM (CALM) to produce a set of actions for a given textual observation from the TGs.It is trained to map a set of textual observations to the admissible actions through causal language modeling.Then, Deep Reinforcement Relevance Network (DRRN) agent was trained on the action candidates from CALM.DRRN follows QL, estimating the Q value per observation-action pair.As a result, CALM removes the need for the ground truth while training DRRN. 3nowledge Graph Advantage Actor Critic (KG-A2C) (Ammanabrolu and Hausknecht, 2020) uses the AC method to sequentially sample templates and objects, and KGs for long-term memory and action pruning.Throughout the gameplay, KG-A2C organizes knowledge triples from textual observation using Stanford OpenIE (Angeli et al., 2015) to construct a KG.Then, the KG is used to build state representation along with encoded game observations and constrain object space with only the entities that the agent can reach within KG, i.e. immediate neighbours.They used admissible actions in the cross entropy supervised loss.KG-A2C Inspired Agents.Xu et al. (2020) proposed SHA-KG that uses stacked hierarchical attention on KG.Graph attention network (GAT) was applied to sample sub-graphs of KG to enrich the state representation on top of KG-A2C.Ammanabrolu et al. (2020) used techniques inspired by Question Answering (QA) with LM to construct the KG.They introduced Q*BERT which uses AL-BERT (Lan et al., 2020) fine-tuned on a dataset specific to TGs to perform QA and extract information from textual observations of the game, i.e. "Where is my current location?".This improved the quality of KG, and therefore, constituted better state representation.Ryu et al. (2022) proposed an exploration technique that injects commonsense directly into action selection.They used log-likelihood score from commonsense transformer (Bosselut et al., 2019) to re-rank actions.Peng et al. (2021) investigated explainable generative agent (HEX-RL) and applied hierarchical graph attention to symbolic KG-based state representations.This was to leverage the graph representation based on its significance in action selection.They also employed intrinsic reward signal towards the expansion of KG to motivate the agent for exploration (HEX-RL-IM) (Peng et al., 2021).</p>
<p>All the aforementioned methods utilize admissible actions in training the LM or agent.Our proposed method, introduced shortly ( §4), uses admissible actions as action constraints during training without relying on KG or LM.</p>
<p>4 Text-based Actor Critic (TAC)</p>
<p>Our agent, Text-based Actor Critic (TAC), follows the Actor-Critic method with template-object decoder.We provide an overview of the system in Figure 1 and a detailed description in below.We follow the notation introduced earlier in Section 2. Encoder.Our design consists of text and state encoders.Text encoder is a single shared bidirectional GRU with different initial hidden state for different input text, (o game , o look , o inv , a N ).The state representation only takes encoded textual observations while the natural language action a N is encoded to be used by the critic (introduced shortly).State encoder embeds game scores into a high dimensional vector and adds it to the encoded observation.This is then, passed through a feed-forward neural network, mapping an instance of observation to state representation without the history of the past information.</p>
<p>Actor.The Actor-Critic design is used for our RL component.We describe our generative actor first.Our actor network maps from state representation to action representation.Then, the action representation is decoded by GRU-based template and object decoders (Ammanabrolu and Hausknecht, 2020).Template decoder takes action representation and produces the template distribution and the context vector.Object decoder takes action representation, semi-completed natural language action and the context from template decoder to produce object distribution sequentially.Critic.Similar to (Haarnoja et al., 2018), we employed two types of critics for practical purpose, state critic for state value function and state-action critic for state-action value function.Both critics take the state representation as input, but stateaction critic takes encoded natural language action as an additional input.The textual command produced by the decoder is encoded with text encoder and is passed through state-action critic to predict state-action value, or Q value, for a given command.A more detailed diagram for Actor and Critic is in Appendix A. To smooth the training, we introduced target state critic as an exponentially moving average of state critic (Mnih et al., 2015).Also, the two state-action critics are independently updated to mitigate positive bias in the policy improvement (Fujimoto et al., 2018).We used the minimum of the two enhanced critic networks outputs as our estimated state-action value function.</p>
<p>Objective Function.Our objective functions are largely divided into two, RL and SL.RL objectives are for reward maximization L R , state value prediction L V , and state-action value prediction L Q .We overload the notation of θ: for instance, V θ (o) signifies parameters from the encoder to the critic, and π θ (a|o) from the encoder to the actor.Reward maximization is done as follows,
L R = −E [A(o, a)∇ θ ln π θ (a|o)] , (1) A(o, a) = Q θ (o, a) − V θ (o),
(2) where A(o, a) is the normalized advantage function with no gradient flow.
LV = E ∇ θ V θ (o) − r + γVθ(o ) ,(3)LQ = E ∇ θ Q θ (o, a) − r + γVθ(o ) , (4)
where o is observation in the next time step and θ signifies the parameters containing the target state critic, updated as moving average with τ ,
θv = τ θ v + (1 − τ ) θv . (5)
Our SL updates the networks to produce valid templates and valid objects, The final loss function is constructed with λ coefficients to control for trade-offs,
L T = 1 |T| a T ∈T (y a T ln (π θ (a T |o)) + (1 − y a T ) (1 − ln (π θ (a T |o)))),(6)L O = 1 |O| a O ∈O (y a O ln (π θ (a O |o, â)) + (1 − y a O ) (1 − ln (π θ (a O |o, â)))),(7)y a T = 1 a T ∈ T a 0 otherwise y a O = 1 a O ∈ O a0L = λ R L R +λ V L V +λ Q L Q +λ T L T +λ O L O. (8)
Our algorithm is akin to vanilla A2C proposed by Ammanabrolu and Hausknecht (2020) with some changes under our observations.A detailed comparison and qualitative analysis are in Appendix B and C.</p>
<p>-admissible Exploration.We use a simple exploration technique during training, which samples the next action from admissible actions with probability threshold.For a given state s, define A a (s) ⊆ A N as an admissible action subset of all natural language actions set.We sample an action directly from admissible action set under uniform distribution, a N ∼ U(A a (s)).Formally, we uniformly sample p ∈ [0, 1] per every step,
β(a|s) = U(A a (s)) p &lt; π(a|s) p ≥ (9)
This collects diverse experiences from altering the world with admissible actions.We also tried a variant where the is selected adaptively given the game score the agent has achieved.However, this variant under-performed the static .See Appendix I for more details on this and the results.</p>
<p>Experiments</p>
<p>In this section, we provide a description of our experimental details and discuss the results.We selected a wide variety of agents (introduced in Section 3) utilizing the LM or the KG: CALM-DRRN (Yao et al., 2020) and KG-A2C (Ammanabrolu and Hausknecht, 2020) as baselines, and SHA-KG (Xu et al., 2020), Q*BERT (Ammanabrolu et al., 2020), HEX-RL and HEX-RL-IM (Peng et al., 2021) as state-of-the-art (SotA).Experimental Setup.Similar to KG-A2C, we train our agent on 32 parallel environments with 5 random seeds.We trained TAC on games of Jericho suite with 100k steps and evaluated with 10 episodes per every 500 training step.During the training, TAC uses uniformly sampled admissible action for a probability of and during the testing, it follows its policy distribution generated from the game observations.We used prioritized experience replay (PER) as our replay buffer (Schaul et al., 2016).We first fine-tune TAC on ZORK1, then apply the same hyper-parameters for all the games.The details of our hyper-parameters can be found in Appendix E. Our final score is computed as the average of 30 episodic testing game scores.Additionally, our model has a parameter size of less than 2M, allowing us to run the majority of our experiments on CPU (Intel Xeon Gold 6150 2.70 GHz).</p>
<p>The training time comparison and the full parameter size in ZORK1 can be found in Appendices F and G.There are a few games that TAC under-performs.We speculate three reasons for this: over-fitting, exploration, and catastrophic forgetting.For instance, as illustrated by the learning curves of TAC in Figure 2, LUDICORP appears to acquire more reward signals during training, but fails to achieve them during testing.We believe this is because the agent is over-fitted to spurious features in specific observations (Song et al., 2020), producing inadmissible actions for a given state that are admissible in other states.On the other hand, TAC in OMNIQUEST cannot achieve a game score more than 5 in both training and testing.This is due to the lack of exploration, where the agent is stuck at certain states because the game score is too far to reach.This, in fact, occurs in ZORK3 and ZTUU for some random seeds, where few seeds in ZORK3 do not achieve any game score while ZTUU achieves 10 or 13 only, resulting in high variance.Finally, catastrophic forgetting (Kirkpatrick et al., 2016) is a common phenomenon in TGs (Hausknecht et al., 2020;Ammanabrolu and Hausknecht, 2020), and this is also observed in JEWEL with TAC.Training Score vs. Testing Score.</p>
<p>Main Results</p>
<p>Figure 2 shows that the game scores during training and testing in many games are different.There are three interpretations for this: (i) the -admissible exploration triggers negative rewards since it is uniformly sampling admissible actions.It is often the case that negative reward signal triggers termination of the game, i.e. −10 score in ZORK1, so this results in episodic score during training below testing.(ii) the -admissible exploration sends the agent to the rarely or never visited state, which is commonly seen in ZTUU.This induces the agent taking useless actions that would not result in rewards since it does not know what to do.(iii) Overfitting where testing score is lower than training score.This occurs in LUDICORP, where the agent cannot escape certain states with its policy during testing.-admissible exploration lets the agent escape from these state during training, and therefore, achieves higher game score.</p>
<p>Ablation</p>
<p>-Admissible Exploration.To understand how influences the agent, ablations with two values, 0.0 and 1.0, on five selective games were conducted.As shown in Figure 3, in the case of = 0.0, the agent simply cannot acquire reward signals.TAC achieves 0 game score in RE-VERB, ZORK1 and ZORK3 while it struggles to learn in DETECTIVE and PENTARI.This indicates that the absence of -admissible exploration results in meaningless explorations until admissible actions are reasonably learned through supervised signals.With = 1.0, learning becomes unstable since this is equivalent to no exploitation during training, not capable of observing reward signals that are far from the initial state.Hence, tuned is important to allow the agent to cover wider range of states (exploration) while acting from its experiences (exploitation).</p>
<p>Supervised Signals.According to the Figure 3, removing SL negatively affects the game score.This is consistent with the earlier observations (Ammanabrolu and Hausknecht, 2020) reporting that KG-A2C without SL achieves no game score in ZORK1.However, as we can observe, TAC manages to retain some game score, which could be reflective of the positive role of -admissible exploration, inducing similar behaviour to SL.</p>
<p>From the observation that the absence of SL degrades the performance, we hypothesize that SL induces a regularization effect.We ran experiments with various strengths of supervised signals by increasing λ T and λ O in LUDICORP and TEMPLE, in which TAC attains higher scores at training compared with testing.As seen in Figure 4 (left two plots), higher λ T and λ O relaxes over-fitting, reaching the score from 7.7 to 15.8 in LUDICORP and from 5.8 to 8.0 in TEMPLE.Since SL is not directly related to rewards, this supports that SL acts as regularization.Further experimental results on ZORK1 is in Appendix D.</p>
<p>To further examine the role of admissible actions in SL, we hypothesize that SL is responsible for guiding the agent in the case that the reward signal is not collected.To verify this, we excluded -admissible exploration and ran TAC with different λ T and λ O in REVERB and ZORK1, in which TAC fails to achieve any score.According to Figure 4 (right two plots), TAC with stronger SL and = 0.0 achieves game scores from 0 to 8.3 in REVERB, and from 0 to 18.3 in ZORK1, which suggests that SL acts as guidance.However, in the absence of -admissible exploration, despite the stronger supervised signals, TAC cannot match the scores using -admissible exploration.</p>
<p>Admissible Action Space During Training.</p>
<p>To examine if constraining the action space to admissible actions during training leads to better utilization, we ran an ablation by masking template and object with admissible actions at training time.This leads to only generating admissible actions.Our plots in Figure 3 show that there is a reduction in the game score in PENTARI, REVERB and ZORK1 while DETECTIVE and ZORK3 observe slight to  substantial increases, respectively.We speculate that the performance decay is due to the exposure bias (Bengio et al., 2015) introduced from fully constraining the action space to admissible actions during training.This means the agent does not learn how to act when it receives observations from inadmissible actions at test phase.However, for games like ZORK3, where the agent must navigate through the game to acquire sparse rewards, this technique seems to help.</p>
<p>Qualitative Analysis</p>
<p>In this section, we show how CALM and KG-A2C restrict their action space.Table 2 shows a snippet of the gameplay in ZORK1.Top three rows are the textual observations and the bottom three rows are the actions generated by CALM, the objects extracted from KG in KG-A2C, and the admissible actions from the environment.CALM produces 30 different actions, but still misses 10 actions out of 17 admissible actions.Since DRRN learns to estimate Q value over generated 30 actions, those missing admissible actions can never be selected, resulting in a lack of exploration.On the other hand, KG-generated objects do not include 'sack' and 'painting', which means that the KG-A2C masks these two objects out from their object space.Then, the agent neglects any action that includes these two object, which also results in a lack of exploration.</p>
<p>Discussion</p>
<p>Supervised Learning Loss.Intuitively, RL is to teach the agent how to complete the game while SL is to teach how to play the game.If the agent never acquired any reward signal, learning is only guided by SL.This is equivalent to applying imitation learning to the agent to follow more probable actions, a.k.a.admissible actions in TGs.However, in the case where the agent has reward signals to learn from, SL turns into regularization ( §5.2), inducing a more uniformly distributed policies.In this sense, SL could be considered as the means to introduce the effects similar to entropy regularization in Ammanabrolu and Hausknecht (2020).</p>
<p>Exploration as Data Collection.In RL, the algorithm naturally collects and learns from data.Admissible action prediction from LM is yet to be accurate enough to replace the true admissible actions (Ammanabrolu and Riedl, 2021;Yao et al., 2020).This results in poor exploration and the agent may potentially never reach a particular state.On the other hand, KG-based methods (Ammanabrolu and Hausknecht, 2020;Xu et al., 2020;Peng et al., 2021) must learn admissible actions before exploring the environment meaningfully.This will waste many samples since the agent will attempt inadmissible actions, collecting experiences of the unchanged states.Additionally, its action selection is largely dependent on the quality of KG.The missing objects from KG may provoke the same effects as LM, potentially obstructing navigating to a particular state.In this regards,admissible exploration can overcome the issue by promoting behaviour that the agent would take after learning admissible actions fully.Under such conditions that a compact list of actions is either provided the environment or extracted by algorithm (Hausknecht et al., 2020), our approach can be employed.Intuitively, this is similar to playing the game with a game manual but not a ground truth to complete the game, which leads to collecting more meaningful data.It also collects more diverse data due to the stochasticity of exploration.Hence, TAC with -admissible exploration can learn how to complete the game with minimal knowledge of how to play the game.</p>
<p>Bias in Exploration.Our empirical results from adaptive experiments in Appendix I suggest that reasonable is required for both under-explored states and well-explored states.This could indicate that diverse data collection is necessary regardless of how much the agent knows about the game while value should not be too high such that the agent can exploit.Finally, from our ablation, fully constraining action space to admissible actions degrades performance.This could be a sign of exposure bias, which is a typical issue in NLG tasks (He et al., 2019;Mandya et al., 2020) and occurs between the training-testing discrepancy due to the teacher-forcing done at training (He et al., 2019).In our setting, this phenomena could potentially occur if the agent only learns from admissible actions at training time.Since -admissible exploration allows a collection of experiences of any actions (i.e., potentially inadmissible actions) with probability of 1 − , TAC with reasonable learns from high quality and unbiased data.Our observations indicate that both the algorithm that learns from data, and the exploration to acquire data are equally important.</p>
<p>Conclusion</p>
<p>Text-based Games (TGs) offer a unique framework for developing RL agents for goal-driven and contextually-aware natural language generation tasks.In this paper we took a fresh approach in utilizing the information from the TG environment, and in particular the admissibility of actions during the exploration phase of RL agent.We introduced a language-based actor critic method (TAC) with a simple -admissible exploration.The core of our algorithm is the utilization of admissible actions in training phase to guide the agent exploration towards collecting more informed experiences.Compared to state-of-the-art approaches with more complex design, our light TAC design achieves substantially higher game scores across 10-29 games.</p>
<p>We provided insights into the role of action admissibility and supervision signals during training and the implications at test phase for an RL agent.Our analysis showed that supervised signals towards admissible actions act as guideline in the absence of reward signal, while serving a regularization role in the presence of such signal.We demonstrated that reasonable probability threshold is required for high quality unbiased experience collection during the exploration phase.</p>
<p>Similar to CALM-DRRN (Yao et al., 2020), KG-A2C (Ammanabrolu and Hausknecht, 2020) and KG-A2C variants (Ammanabrolu et al., 2020;Xu et al., 2020;Peng et al., 2021) that use admissible actions, our method still utilizes admissible actions.This makes our TAC not suitable for environments that do not provide admissible action set.In the absence of admissible actions, our TAC requires some prior knowledge of a compact set of more probable actions from LMs or other sources.This applies to other problems, for instance, applying our proposed method to language-grounded robots requires action candidates appropriate per state that they must be able to sample during training.The algorithm proposed by Hausknecht et al. (2020) extracts admissible actions by simulating thousands of actions per every step in TGs.This can be used to extract a compact set of actions in other problems, but it would not be feasible to apply if running a simulation is computationally expensive or risky (incorrect action in real-world robot may result in catastrophic outcomes, such as breakdown).</p>
<p>Ethical Considerations</p>
<p>Our proposal may impact other language-based autonomous agents, such as dialogue systems or language-grounded robots.In a broader aspect, it contributes to the automated decision making, which can be used in corporation and government.When designing such system, it is important to bring morals and remove bias to be used as intended.</p>
<p>A Details of Actor and Critic Components</p>
<p>Consider an action example (take OBJ from OBJ, egg, fridge) as (template, first object, second object).Template a T = (take OBJ from OBJ) is sampled from template decoder and encoded to h T with text encoder.Object decoder takes action representation a and encoded semi-completed action h T and produces the first object a O1 = (egg).The template a T = (take OBJ from OBJ) and the first object a O1 = (egg) are combined to a T,O1 = (take egg from OBJ), a T ⊗ a O1 = a T,O1 .a T,O1 is then, encoded to hidden state h T,O1 with text encoder.Similarly, the object decoder takes a and h T,O1 and produces the second object a O2 = (fridge).a T,O1 and a O2 are combined to be natural language action, a T,O1 ⊗ a O2 = a N Finally, a N is encoded to h a with text encoder and inputted to state-action critic to predict Q value.</p>
<p>B Comparison with Vanilla A2C in Ammanabrolu and Hausknecht (2020) Architecture.Vanilla A2C from Ammanabrolu and Hausknecht (2020)   of actor and state value critic, so the state representation is used to estimate state value and produce the policy distribution.</p>
<p>Our TAC uses a single shared GRU to encode textual observations and previous action with different initial state to signify that the text encoder constructs the general representation of text while the game score is embedded to learnable high dimentional vector.However, when constructing state representation, we only used (o game , o look , o inv ) under our observation that o game carries semantic information about a t−1 .Additionally, we also observed that the learned game score representation acts as conditional vector in Appendix C, so the state representation is constructed as an instance of observation without historical information.Finally, we included additional modules, state-action value critic (Haarnoja et al., 2018), target state critic (Mnih et al., 2015) and two state-action critics (Fujimoto et al., 2018;Haarnoja et al., 2018) for practical purpose.</p>
<p>Objective Function.Three objectives are employed in Ammanabrolu and Hausknecht (2020), reinforcement learning (RL), supervised learning (SL) and entropy regularization.Both RL and SL are also used in our objectives with minor changes in value function update in RL.That is, two stateaction value critics are updated independently to predict Q value per state-action pair and target state critic is updated as moving average of state critic Notable difference is that we excluded entropy regularization from Ammanabrolu and Hausknecht (2020).This is because under our ablation in Section 5.2, we observed that SL acts as regularization.</p>
<p>Replay Buffer Unlike on-policy vanilla A2C (Ammanabrolu and Hausknecht, 2020), since TAC utilizes -admissible exploration, it naturally sits as off-policy algorithm.We used prioritized experience replay (PER) as our replay buffer (Schaul et al., 2016).Standard PER assigns a newly acquired experience with the maximum priority.This enforces the agent to prioritize not-yet-sampled experiences over others.As we are using 32 parallel environments and 64 batch size for update, half of the updates will be directed by newly acquired experiences, which not all of them may be useful.Thus, instead, we assign newly acquired experience with TD errors when they are added to the buffer.This risks the agent not using some experiences, but it is more efficient since we sample useful batch of experiences.</p>
<p>C Qualitative Analysis</p>
<p>It has been repetitively reported that including game score when constructing state helps in TGs (Ammanabrolu and Hausknecht, 2020;Jang et al., 2021).Here, we provide some insights in what the agent learns from the observations using fully trained TAC.To illustrate this, we highlight the role of game score on the action preference of the TAC for the same observation in ZORK1.Observations for different cases can be found in Table 3 and Table 5 while the policy and Q value are in Table 4 and Table 6.</p>
<p>Case 1 in Table 3 and Table 4 For three different cases, Case 1.1, Case 1.2, and Case 1.3, the agent is at Kitchen location, so many semantic meaning between textual observations are similar, i.e. o look or o inv .For each case, the agent is meant to go west with n score = 10, go west with n score = 39, and go east with n score = 45, respectively.In Case 1.1, despite the optimal choice of action is west, by replacing the score Case 1.1</p>
<p>Step: 4 Game: Kitchen You are in the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water Look: Kitchen You are in the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.On the  from n score = 10 to n score = 45, the agent chooses east, which is appropriate for Case 1.3.Another interesting observation is that replacing game score decreases Q value from 23.7460 to 5.0134 for west and from 18.4385 to 6.0319 for east in Case 1.1.This seems like the agent thinks it has already acquired reward signals between n score = 10 and n score = 45, resulting in a reduction in Q value.We speculate that this is because the embedding of n score carries some inductive bias, i.e. temporal, for the agent to infer the stage of the game.This is consistently manifested in Case 1.3, but in Case 1.2, the agent is robust to the game score because it carries painting that is directly related to reward signals, navigating to pursue that particular reward, which is put paining in case for reward signal of +6 in Living Room location.</p>
<p>Case 2 in Table 5 and Table 6 In Case 2, the agent is at Behind House for three other sets of game instances, which has action and score pair as, open window for n score = 0, west for n score = 0, and north for n score = 45.The phenomenon between Case 1.1 and Case 1.3 occurs the same for Case 2.2 and Case 2.3.However, unlike Case 1, the score between Case 2.1 and Case 2.2 is the same.This means that the agent somehow chooses the optimal action for Case 2.2 over Case 2.1 in the case where n score = 0 is injected for Case 2.3.This appears to be that the agent can capture semantic correlation between "In one corner of the house there is a small window which is open" from textual observation in Case 2.3 and open window action.Because a small window is already opened, open window action is no longer required, so the agent tends to produce west, which is appropriate for Case 2.2.</p>
<p>Thus, from our qualitative analysis, we speculate that the agent captures the semantics of the textual observations and infers the game stage from game score embedding to make optimal decision.</p>
<p>D Stronger Supervised Signals for ZORK1</p>
<p>We also explored how stronger supervised signals can induce better regularization in ZORK1.Similar to other sets of experiments, we selected variety of λ T -λ O pair.However, our results show that TAC starts under-fitting in ZORK1 when larger λ T and λ O are applied.the hyper-parameters and n tst is the average testing game score.During training, higher n score exponentially increases while a controls the slope of the exponential function.Higher a makes the slope more steep.Intuitively, as the agent exploits the well-known states, is small, encouraging the agent to follow its own policy, and as the agent reaches the under-explored states (i.e., similar to test condition), increases to encourage more diversely.The is normalized and scaled.The example plot is shown in FIgure 10.</p>
<p>We conducted a set of ablations with dynamic value in DETECTIVE, PENTARI, REVERB, ZORK1 and ZORK3.We used min = {0.0,0.3}, a = {3, 9} and max = {0.7,1.0}, so total 8 different hyper-parameters.Figure 8 shows fixed min = 0.0 with varying a and max and Figure 8 shows fixed min = 0.3.Other than ZORK3, TAC with dynamic matches or underperforms TAC with fixed = 0.3.There are two interesting phenomenons.(i) Too high max results in more unstable learning and lower performance.This becomes very obvious in PENTARI, REVERB and ZORK1, where regardless of min and a , if max = 1.0, the learning curve is relatively low.In DETECTIVE of Figure 8, the learning becomes much more unstable with max = 1.0.This indicates that even underexplored states, exploitation may still be required.(ii) Too low min results in more unstable learning and lower performance.Although PENTARI benefits from min = 0.0, the learning curves in Figure 8 is generally lower and unstable than Figure 9.This appears to be that despite how much the agent learned the environment, it still needs to act stochastically to collect diverse experiences.</p>
<p>Figure 1 :
1
Figure 1: Text-based Actor-Critic (TAC); A blue circle is the input to the encoder, (n score , o game , o look , o inv ) representing (game score, game feedback, room description, inventory), while a red circle is the output from actor, a N representing natural language action.Blue, red and green boxes indicate encoder, actor and critic, respectively.</p>
<p>otherwise where L T and L O are the cross entropy losses over the templates (T) and objects (O).Template and object are defined as a T and a O , while â is the action constructed by previously sampled template and object.Positive samples, y a T and y a O , are only if the corresponding template or object are in the admissible template (T a ) or admissible object (O a ).4</p>
<p>Figure 2 :
2
Figure 2: The full learning curve of TAC on five games in Jericho suite.Blue and red plots are training and testing game score while cyan and yellow star marker line signify CALM-DRRN and KG-A2C.</p>
<p>Figure 3 :
3
Figure 3: Ablation study on five popular games in Jericho suite.Four different ablation are conducted with SL, = 0.0, = 1.0, and with full admissible constraints during training (Admissible Action space).Similar to the previous figure, CALM-DRRN and KG-A2C are added for comparison.</p>
<p>Figure 4 :
4
Figure 4: The learning curve of TAC for stronger supervised signals where 5-3 signifies λ T = 5 and λ O = 3. Left two plots are with = 0.3 and right two are with = 0.</p>
<p>Figure 5 :
5
Figure 5: The details of actor and critic of text-based actor-critic; State representation is the input to actor-critic while a red circle is the output from actor, a N representing natural language action.Red and green boxes indicate actor and critic, respectively.</p>
<p>Figure 6 :
6
Figure 6: The learning curve of TAC for regularization ablation in ZORK1.Stronger supervised signals are used with = 0.3, where 5-3 signifies γ T = 5 and γ O = 3.</p>
<p>Figure 10 :
10
Figure 10: The exponential probability of over the game score.Left is with min = 0.0, max = 1.0 and right is with min = 0.3, max = 0.7 between the game score of 0 to 6. Five different a is drawn per plot.</p>
<p>A, P, O, P o , R, γ), where S and A are a set of state and action, and P is the state transition probability that maps state-action pair to the next state, Pr(s t+1 |s t , a t ).O is a set of observation that depends on the current state via an emission probability, P o ≡ Pr(o t |s t ).R is an immediate reward signal held between the state and the next state, r(s t , s t+1 ), and γ is the discount factor.The action selection rule is referred to as the policy π(a|o), in which the optimal policy acquires the maximum rewards in the shortest move.TG Environment as POMDP.Three textual observations are acquired from the engine, game feedback o game , room description o look , and inventory description o inv .The game feedback is dependent on the previous action, Pr(o game,t |s</p>
<p>Partially Observable Markov Decision Process.TG environments can be formalized as Partially Observable Markov Decision Processes (POMDPs).A POMDP is defined as a 7-tuple, (S, t , a t−1 ), while room and inventory descriptions are not, Pr(o look,t |s t ) and Pr(o inv,t |s t ).Inadmissible actions do not influence the world state, room and inventory descriptions but change the game feedback changes.Each action is sampled sequentially from template-based action space.For template, we directly sample from observation π(a T |o) while an object policy is sequentially produced, π(a O |o, â), where â is previously sampled template-object pair.</p>
<p>Table 1 :
1
Game score comparison over 10 popular game environments in Jericho, with best results highlighted by boldface.We only included algorithms that reported the end performance.† HEX-RL and HEX-RL-IM did not report the performance in ZORK3 and are not open-sourced, so the mean average did not account ZORK3.
LM-BASEDKG-BASEDGamesCALM-DRRN KG-A2C SHA-KG Q*BERT HEX-RL HEX-RL-IMTACBALANCES9.110.09.810.010.010.010.0 ± 0.1DEEPHOME1.01.01.01.01.01.025.4 ± 3.2DETECTIVE289.7207.9246.1274.0276.7276.9272.3 ± 23.3LIBRARY9.014.310.018.015.913.818.0 ± 1.2LUDICORP10.117.817.618.014.017.67.7 ± 2.5PENTARI0.050.748.250.034.644.753.2 ± 2.9TEMPLE0.07.67.98.08.08.05.8 ± 2.3ZORK130.434.033.635.029.830.246.3 ± 5.0ZORK30.50.10.70.1−−1.6 ± 1.2ZTUU3.75.05.05.05.05.133.2 ± 26.3NORMALIZED MEAN0.15490.24750.24900.27880.2722  †0.2834  †0.3307</p>
<p>Table 1
1
50% higher score than both CALM-DRRN and KG-A2C with normalized mean score.Per game, in SORCERER, SPIRIT, ZORK3 and ZTUU, TAC achieves at least ∼ 200% and at most ∼ 400% higher score..In ACORNCOURT, DEEPHOME and DRAGON, both
reports the results for baselines, SotAs andTAC on 10 popular Jericho games. TAC attains thenew SotA scores in 5 games. Apart from PENTARI,TAC surpasses 4 games with a large margin, whereall of the other agents fail to pass the performancebottleneck (DEEPHOME with 1, ZORK1 with 35,ZORK3 with 1, and ZTUU with 5). In DETECTIVE,TAC matches many SotAs, but falls short in LUDI-CORP and TEMPLE. Nevertheless, TAC achievesthe highest mean score over LM or KG-based meth-ods.On a larger set of 29 games in comparison withthe baselines, TAC surpasses CALM-DRRN in14 out of 29 games and KG-A2C in 16 out of29 games and achieves more than ∼</p>
<p>GameKitchen.On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water.Inventory You are carrying: A painting, A brass lantern (providing light) Room Kitchen.You are in the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.
The glass bottlecontains: A quantity of waterLM'close bottle', 'close door', 'down', 'drink water', 'drop bottle',Actions'drop painting', 'east', 'empty bottle', 'get all', 'get bottle', 'geton table', 'get painting', 'get sack', 'north', 'open bottle', 'out','pour water on sack', 'put candle in sack', 'put painting in sack','put painting on sack', 'put water in sack', 'south', 'take all','take bottle', 'take painting', 'take sack', 'throw painting', 'up','wait', 'west'KG'a', 'all', 'antique', 'board', 'bottle', 'brass', 'chimney', 'dark',Objects'door', 'down', 'east', 'exit', 'front', 'grue', 'house', 'is','kitchen', 'lantern', 'large', 'light', 'narrow', 'north', 'of', 'pas-sage', 'path', 'quantity', 'rug', 'south', 'staircase', 'table', 'to','trap', 'trophy', 'up', 'west', 'white', 'window', 'with'Admiss.'close window', 'east', 'jump', 'open bottle', 'open sack', 'putActionsdown all', 'put down light', 'put down painting', 'put light ontable', 'put out light', 'put painting on table', 'take all', 'takebottle', 'take sack', 'throw light at window', 'up', 'west'</p>
<p>Table 2 :
2
Action space for a game observation (top panel) for CALM (LM), KG-A2C (KG), and the Admissible Action sets.Red and blue colored actions are the actions missed by either CALM or KG-A2C.Brown are the actions missed by both, and blacks are actions covered by both.</p>
<p>uses separate gated recurrent units (GRUs) to encode textual observations and previous action, (o game , o look , o inv , a t−1 ), and transforms the game score, n score , into binary encoding.Then, they are concatenated and passed through state network to form state representation.Their state network is GRU-based to account historical information.The actor-critic network consists</p>
<p>table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water Inv: You are empty handed.Kitchen On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water Look: Kitchen You are in the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water Inv: You are carrying: A painting A brass lantern (providing light) Kitchen On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water Look: Kitchen You are in the kitchen of the white house.A table seems to have been used recently for the preparation of food.A passage leads to the west and a dark staircase can be seen leading upward.A dark chimney leads down and to the east is a small window which is open.On the table is an elongated brown sack, smelling of hot peppers.A bottle is sitting on the table.The glass bottle contains: A quantity of water Inv: You are empty handed.
Score: 10Action: westCase 1.2Step: 15Game: Score: 39Action: westCase 1.3Step: 20Game: Score: 45Action: east</p>
<p>Table 3 :
3
Case 1; Game observation and the selected action snippets from ZORK1.</p>
<p>Code is available at https://github.com/ktr0921/ tac
Admissible actions are grounded actions that are guaranteed to change the world state produced by the environment(Hausknecht et al., 2020;Côté et al., 2018).
It is noteworthy, orthogonal to the focus of our work, the recently proposed eXploit-Then-eXplore(Tuyls et al., 2022) uses LM and admissible actions to resolve another challenge, exploration-exploitation dilemma in TGs.
Eq. 7 is calculated separately for two objects in a single template, where the admissible object space (Oa) is conditioned on the previously sampled template and object.
The code for KG-A2C is in https://github.com/ rajammanabrolu/KG-A2C, and DRRN is in https:// github.com/microsoft/tdqn.
state_network.embedding_score.weight[1024,128] state_network.tf.weight [128,384] state_network.tf.bias [128] state_network.fc1.weight[128,128] state_network.fc1.bias[128] state_network.fc2.weight[128,128] state_network.fc2.bias[128] state_network.fc3.weight[128,128] state_network.fc3.bias[128] state_network.s.weight [128,128] state_network.s.bias [128] state_critic.fc1.weight[128,128] state_critic.fc1.bias[128] state_critic.fc2.weight[128,128] state_critic.fc2.bias[128] state_critic.fc3.weight[128,128] state_critic.fc3.bias[128] state_critic.v.weight [1,128] state_critic.v.bias [1] actor_network.fc1.weight[128,128] actor_network.fc1.bias[128] actor_network.fc2.weight[128,128] actor_network.fc2.bias[128] actor_network.fc3.weight[128,128] actor_network.fc3.bias[128] actor_network.a.weight [128,128] actor_network.a.bias [128] state_action_critic_1.fc1.weight[128,256] state_action_critic_1.fc1.bias[128] state_action_critic_1.fc2.weight[128,128] state_action_critic_1.fc2.bias[128] state_action_critic_1.fc3.weight[128,128] state_action_critic_1.fc3.bias[128] state_action_critic_1.q.weight [1,128] state_action_critic_1.q.bias [1] state_action_critic_2.fc1.weight[128,256] state_action_critic_2.fc1.bias[128] state_action_critic_2.fc2.weight[128,128] state_action_critic_2.fc2.bias[128] state_action_critic_2.fc3.weight[128,128] state_action_critic_2.fc3.bias[128] state_action_critic_2.q.weight [1,128] state_action_critic_2.q.bias [1] target_state_critic.fc1.weight[128,128] target_state_critic.fc1.bias[128] target_state_critic.fc2.weight[128,128] target_state_critic.fc2.bias[128] target_state_critic.fc3.weight[128,128] target_state_critic.fc3.bias[128] target_state_critic.v.weight [1,128] target_state_critic.v.bias [1] template_decoder_network.tmpl_gru.weight_ih_l0E HyperparametersF Training TimeWe used Intel Xeon Gold 6150 2.70 GHz for CPU and Tesla V100-PCIE-16GB for GPU, 8 CPUs with 25GB memory, to train KG-A2C and TAC on ZORK1.The results are demonstrated in Table 8. 5 Our TAC has approximately three times lesser parameters than KG-A2C in ZORK1, which would be consistent across different games.On the other hand, for step per second, TAC is twice faster in GPU and thrice faster in CPU than KG-A2C.Approximated days for training TAC on CPU and GPU are 1.2 and 0.8 days while KG-A2C is 4.1 and 1.6 days.TAC still benefits from GPU, but not as much as KG-A2C as its training time is more dependent to the game engine than backpropagation.Step/second (CPU)Step/second (GPU) Parameter Size KG-A2C 0.28 0.71 4.8M TAC 0.99 1.43 1.8MTable8: Training time as step per second in CPU and GPU and total parameter size for ZORK1. 5The code for KG-A2C is in https://github.com/rajammanabrolu/KG-A2C.G Parameter Size for ZORK1The total parameter size of TAC in ZORK1 is 1,783,849 with 49,665 target state critic, which slightly varies by the size of template and object space per game.This is much lower than KG-A2C (4,812,741), but little higher than DRRN (1,486,081). 6H Full Experimental ResultsThe full learning curve of TAC and game score comparison are presented in Figure7and Table10.I Adaptive Score-basedWe also designed the epsilon scheduler that dy-namically assigns based on the game score that the agent has achieved; ∝ e a n tst nscore , where a is
Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Modeling worlds in text. Prithviraj Ammanabrolu, Mark Riedl, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021</p>
<p>How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. Prithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, Mark O Riedl, CoRR, abs/2002.087952020</p>
<p>Leveraging linguistic structure for open domain information extraction. Gabor Angeli, Melvin Jose , Johnson Premkumar, Christopher D Manning, 10.3115/v1/P15-1034Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. Long Papers. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics20151</p>
<p>Scheduled sampling for sequence prediction with recurrent neural networks. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, Noam Shazeer, Proceedings of the 28th International Conference on Neural Information Processing Systems. the 28th International Conference on Neural Information Processing SystemsCambridge, MA, USAMIT Press201515</p>
<p>COMET: commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, 10.18653/v1/p19-1470Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019. the 57th Conference of the Association for Computational Linguistics, ACL 2019Florence, Italy; PapersAssociation for Computational Linguistics2019. July 28-August 2, 20191</p>
<p>Textworld: A learning environment for textbased games. Marc-Alexandre Côté, Ákos Kádár, ( Xingdi, Ben Eric) Yuan, Tavian Kybartas, Emery Barnes, James Fine, Matthew Moore, Layla El Hausknecht, Mahmoud Asri, Wendy Adada, Adam Tay, Trischler, Computer Games Workshop at ICML/IJCAI 2018. 2018</p>
<p>Addressing function approximation error in actor-critic methods. Scott Fujimoto, Herke Van Hoof, David Meger, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR2018Proceedings of Machine Learning Research</p>
<p>Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, Proceedings of the 35th International Conference on Machine Learning, ICML 2018. the 35th International Conference on Machine Learning, ICML 2018Stockholmsmässan, Stockholm, SwedenPMLR2018. July 10-15. 201880of Proceedings of Machine Learning Research</p>
<p>Interactive fiction games: A colossal adventure. Matthew J Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020. February 7-12, 20202020The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</p>
<p>Quantifying exposure bias for neural language generation. Tianxing He, Jingzhao Zhang, Zhiming Zhou, James R Glass, CoRR, abs/1905.106172019</p>
<p>Monte-carlo planning and learning with language action value estimates. Youngsoo Jang, Seokin Seo, Jongmin Lee, Kee-Eung Kim, International Conference on Learning Representations. 2021</p>
<p>Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil C Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, CoRR, abs/1612.00796Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2016</p>
<p>ALBERT: A lite BERT for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020. April 26-30, 20202020OpenReview.net</p>
<p>Do not let the history haunt you: Mitigating compounding errors in conversational question answering. Angrosh Mandya, O' James, Danushka Neill, Frans Bollegala, Coenen, Proceedings of the 12th Language Resources and Evaluation Conference. the 12th Language Resources and Evaluation ConferenceMarseille, France2020European Language Resources Association</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin A Riedmiller, Andreas Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, Demis Hassabis, 10.1038/nature14236Nat. 51875402015</p>
<p>Inherently explainable reinforcement learning in natural language. Xiangyu Peng, Mark O Riedl, Prithviraj Ammanabrolu, CoRR, abs/2112.089072021</p>
<p>Fire burns, sword cuts: Commonsense inductive bias for exploration in text-based games. Dongwon Ryu, Ehsan Shareghi, Meng Fang, Yunqiu Xu, Shirui Pan, Reza Haf, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics20222Short Papers)</p>
<p>Prioritized experience replay. Tom Schaul, John Quan, Ioannis Antonoglou, David Silver, ICLR (Poster). 2016</p>
<p>Observational overfitting in reinforcement learning. Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, Behnam Neyshabur, International Conference on Learning Representations. 2020</p>
<p>Multi-stage episodic control for strategic exploration in text games. Jens Tuyls, Shunyu Yao, M Sham, Kakade, Karthik R Narasimhan, International Conference on Learning Representations. 2022</p>
<p>Deep reinforcement learning with stacked hierarchical attention for text-based games. Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, Chengqi Zhang, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. NeurIPS2020. 2020. 2020. December 6-12, 2020</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational Linguistics2020</p>            </div>
        </div>

    </div>
</body>
</html>
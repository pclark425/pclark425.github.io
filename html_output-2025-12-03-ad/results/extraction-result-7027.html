<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7027 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7027</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7027</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-135.html">extraction-schema-135</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <p><strong>Paper ID:</strong> paper-a83902f8b3aadfda633968a840ca1738bedef837</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/a83902f8b3aadfda633968a840ca1738bedef837" target="_blank">Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs</a></p>
                <p><strong>Paper Venue:</strong> Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15)</p>
                <p><strong>Paper TL;DR:</strong> Graformer, a novel Transformer-based encoder-decoder architecture for graph-to-text generation that learns to weight node-node relations differently for different attention heads, thus virtually learning differently connected views of the input graph.</p>
                <p><strong>Paper Abstract:</strong> We present Graformer, a novel Transformer-based encoder-decoder architecture for graph-to-text generation. With our novel graph self-attention, the encoding of a node relies on all nodes in the input graph - not only direct neighbors - facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these node-node relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7027.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7027.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token‑Incidence Graph (Graformer) - AGENDA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tokenized incidence graph with SAME_p edges and shortest‑path relative position encoding (used in Graformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation that splits entity labels into token nodes, converts the KG hypergraph into a bipartite incidence graph (where facts/arcs become nodes), fully connects tokens from the same entity via SAME_p edges, and encodes pairwise node relations by signed shortest‑path lengths (or ∞ for unreachable). These relative positions are used as scalar attention biases in a Transformer encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Tokenized incidence graph (with SAME_p and shortest‑path R_ij)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Each entity label is tokenized and represented as separate token nodes; hyperarcs (facts) are represented as nodes in a bipartite incidence graph; SAME_p edges connect tokens from the same original entity with p encoding their original relative token positions; for any node pair R_ij is set to signed shortest‑path length (positive for forward paths, negative for backward), encode(p) for SAME_p, or ∞ for unreachable pairs. Relative positions are clamped by thresholds n_delta and n_p and used to look up learned scalar attention biases γ(R_ij) added to attention scores.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based, graph‑structured (incidence hypergraph), near‑lossless for token/arc info but limited by clamped distance buckets</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Tokenize entity labels (BPE), split nodes into token nodes, convert hypergraph to bipartite incidence graph where arcs become nodes, add SAME_p edges for intra‑entity token order, compute shortest paths between nodes to produce signed R_ij (or ∞) and clamp values to n_delta/n_p for indexing bias embeddings; node labels prefixed with <E>/<R>.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td>98</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AGENDA</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge‑graph → text generation (graph‑to‑text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graformer (Transformer encoder‑decoder with graph self‑attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder‑decoder Transformer where the encoder uses multi‑head graph self‑attention that adds learned scalar biases γ(R_ij) based on signed shortest‑path lengths; 8 attention heads; encoder layers = 4, decoder layers = 5; model dim d = 400; feedforward dim = 2000; ~36.3M parameters (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU 17.80 ± 0.31; METEOR 22.07 ± 0.23; CHRF++ 45.43 ± 0.39; model size 36.3M params</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Enables the Transformer encoder to learn multiple structural views (per‑head biases) and capture both local and global graph patterns; parameter‑efficient relative to multi‑encoder approaches; required an epoch‑curriculum to train stably (curriculum learning was crucial).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relative positions are discretized/clamped via n_delta and n_p (limits distinguishable distances); encodes only structural shortest‑path lengths (no label sequence information along paths), which may lose label‑specific path signals; no canonical linearization/ordering is enforced; some rare characters excluded by BPE (AGENDA char coverage 99.99%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Compared to dual encoder (CGE‑LW) Graformer uses fewer parameters (~half) while achieving similar performance on AGENDA; unlike path‑label LSTM approaches, Graformer does not require an existent path between all node pairs and therefore handles disconnected KGs; compared to GNNs, Graformer learns local/global connectivity patterns via attention heads rather than relying on stacked message passing.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7027.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7027.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Token‑Incidence Graph (Graformer) - WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tokenized incidence graph with SAME_p edges and shortest‑path relative position encoding (used in Graformer)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same tokenized incidence graph representation as above, applied to WebNLG: entity labels are token nodes, hyperarcs are nodes, SAME_p edges encode intra‑entity token order, and signed shortest‑path lengths/∞ are used as relative position indices for learned attention biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Tokenized incidence graph (with SAME_p and shortest‑path R_ij)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Tokenize entity/relation labels (BPE); build bipartite incidence graph (nodes = token nodes ∪ arc nodes); fully connect tokens from same entity with SAME_p edges (p encodes token offsets); compute signed shortest‑path lengths or ∞ for unreachable pairs; clamp to ranges and use as indices to learned scalar attention biases added to Transformer attention logits.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>token‑based, graph‑structured (incidence hypergraph), near‑lossless for token/arc info but limited by clamped distance buckets</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Tokenization (SentencePiece BPE), split nodes by token, hyperedge→node incidence conversion, SAME_p intra‑entity edges, compute signed shortest paths for R_ij, clamp values to n_delta/n_p, use γ(R_ij) per attention head as additive bias.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td>36</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>WebNLG</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge‑graph → text generation (graph‑to‑text)</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graformer (Transformer encoder‑decoder with graph self‑attention)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Smaller Graformer config for WebNLG: 8 heads; encoder layers = 3, decoder layers = 3; model dim d = 256; feedforward dim = 512; graph self‑attention range n_delta = 4; ~5.3M parameters (as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BLEU 61.15 ± 0.22; METEOR 43.38 ± 0.17; CHRF++ 75.43 ± 0.19; model size 5.3M params</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Learns attention biases that separate local (direct neighbors) and global/distant interactions; empirically most heads focus on near nodes yet some heads attend to global patterns; competitive performance with substantially fewer parameters than some baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As above: clipped relative‑position ranges limit fine‑grained distance distinctions; does not incorporate label sequences along paths; training required epoch curriculum; for WebNLG some heads suppress unreachable nodes, so behavior depends on dataset connectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Achieves >96% of CGE‑LW‑LG performance on WebNLG while using roughly half the parameters; compared to fully connected/global encoders (dual encoder methods) Graformer can distinguish unreachable vs distant nodes through the ∞ encoding and signed shortest‑path values.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7027.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7027.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Path‑label LSTM relation embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Relation embeddings computed by an LSTM over labels on the shortest path between node pairs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A representation used in some Transformer‑based graph encoders where, for any node pair, an LSTM processes the sequence of node/edge labels along the shortest path and the resulting vector is used as a relation embedding or attention bias between those nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling graph structure in transformer for better AMR‑to‑text generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Path‑label LSTM encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>For each pair of nodes, extract the sequence of labels along the shortest connecting path and run an LSTM over that label sequence to produce a relation embedding which modifies attention between the nodes (e.g., added to keys/values or to attention scores).</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential, path‑based, label‑informed</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Compute shortest path between node pairs (assumes connectivity), concatenate the edge/node labels along the path into a token sequence, run an LSTM to produce a fixed‑size relation embedding used in Transformer attention.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AMR (reported use in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>AMR‑to‑text / graph‑to‑text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Graph Transformer with path‑label LSTM relation embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer encoder augmented with LSTM‑derived relation embeddings for attention between node pairs (as reported by Zhu et al. and Cai & Lam), typically applied in AMR‑to‑text settings.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Injects label/path information directly into attention which can help when graphs are connected and path labels are informative (e.g., AMR), enabling interactions among distant nodes via label context.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Crucially assumes the existence of a path between every node pair (poor fit for disconnected KGs); can introduce spurious connections across components; depends on label sequences which may be noisy or unavailable; computational cost for path extraction/LSTM per pair.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Unlike Graformer, this approach uses label sequences along paths rather than pure structural shortest‑path lengths and therefore cannot handle disconnected graphs without ad hoc measures; Graformer encodes only structural distances (and SAME_p) and explicitly represents unreachable pairs via ∞.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7027.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7027.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dual‑encoder (CGE‑LW / CGE‑LW‑LG)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local and Global encoders combined (CGE‑LW from Ribeiro et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that uses two graph encoders — one operating on the original graph topology (local context) and one on a fully connected version of the graph (global context) — combining their outputs to provide both local and global node context for graph‑to‑text generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Modeling global and local node contexts for text generation from knowledge graphs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Dual GNN encoders: original topology + fully‑connected graph</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Encode nodes twice: once with a GNN that respects the original edges (message passing among neighbors) for local context, and once with a GNN on a fully connected graph to capture global interactions; combine representations (e.g., concatenation or gating) before decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>graph‑structured, multi‑view (local + global)</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Run a message‑passing GNN on the given KG topology for local features; run a second GNN on a fully connected graph (or dense connectivity) for global context; fuse outputs for downstream decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>AGENDA and WebNLG (as evaluated / compared in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Knowledge‑graph → text generation</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>CGE‑LW (and CGE‑LW‑LG variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two separate GNN encoders (one local, one global) whose outputs are combined; reported model variants have larger parameter counts (e.g., CGE‑LW ~69.8M on AGENDA, CGE‑LW‑LG ~10.4M on WebNLG depending on config).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>BLEU, METEOR, CHRF++ (reported in comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported in this paper for baselines: AGENDA CGE‑LW BLEU 18.01; METEOR 22.34; CHRF++ 46.69; params 69.8M. WebNLG CGE‑LW‑LG BLEU 63.69; METEOR 44.47; CHRF++ 76.66; params 10.4M.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Provides explicit local and global context; can improve metrics by combining views, but requires additional parameters and separate encoder training.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Only exposes two extreme connectivity views (true neighbors vs fully connected) and thus cannot explicitly distinguish unreachable node pairs from distant but connected ones; more parameter‑heavy than Graformer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Graformer aims to learn a continuum of structural views via per‑head biases on shortest‑path distances, achieving similar performance with fewer parameters and explicitly modelling unreachable pairs via ∞ encoding; CGE‑LW relies on separate encoders which increases parameter cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7027.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7027.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of graph-to-text representations used to convert graphs into textual sequences for language model training, including their description, encoding method, properties, datasets, models, evaluation metrics, performance outcomes, and any reported advantages or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Relative position encodings (Shaw et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self‑attention with relative position representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Relative position embeddings added to key/value representations in Transformer attention so that attention scores and/or values are biased by relative token positions rather than absolute positions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self‑attention with relative position representations</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>Relative position embedding (key/value or scalar bias)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Learn vector or scalar embeddings indexed by relative positions (i − j) which are added to key/value representations or to attention logits to make attention position‑aware; can be shared across heads or per‑head.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_type</strong></td>
                            <td>sequential (relative positional), embedding‑based</td>
                        </tr>
                        <tr>
                            <td><strong>encoding_method</strong></td>
                            <td>Compute relative position index for token pairs (bounded range), lookup learned embedding A^K / A^V or scalar S and add to key/value or logits in attention computation.</td>
                        </tr>
                        <tr>
                            <td><strong>canonicalization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>average_token_length</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text sequence modelling / Transformer attention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer variants with relative position embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used in standard Transformers or T5 style models; can be shared or per‑head; used in Graformer decoder text self‑attention (Raffel style scalar bias variant).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>impact_on_training</strong></td>
                            <td>Makes attention aware of relative ordering and distances in sequences; in Graformer the idea is extended to graph shortest‑path distances to inform graph self‑attention.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Originally designed for linear text sequences; extending to graphs requires defining a graph relative position function (as Graformer does).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other</strong></td>
                            <td>Graformer adapts the relative position idea from sequences to graphs by using shortest‑path signed lengths and SAME_p encodings rather than linear token offsets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs', 'publication_date_yy_mm': '2021-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Modeling graph structure in transformer for better AMR‑to‑text generation <em>(Rating: 2)</em></li>
                <li>Modeling global and local node contexts for text generation from knowledge graphs <em>(Rating: 2)</em></li>
                <li>Text Generation from Knowledge Graphs with Graph Transformers <em>(Rating: 2)</em></li>
                <li>Self‑attention with relative position representations <em>(Rating: 2)</em></li>
                <li>Deep graph convolutional encoders for structured data to text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7027",
    "paper_id": "paper-a83902f8b3aadfda633968a840ca1738bedef837",
    "extraction_schema_id": "extraction-schema-135",
    "extracted_data": [
        {
            "name_short": "Token‑Incidence Graph (Graformer) - AGENDA",
            "name_full": "Tokenized incidence graph with SAME_p edges and shortest‑path relative position encoding (used in Graformer)",
            "brief_description": "Representation that splits entity labels into token nodes, converts the KG hypergraph into a bipartite incidence graph (where facts/arcs become nodes), fully connects tokens from the same entity via SAME_p edges, and encodes pairwise node relations by signed shortest‑path lengths (or ∞ for unreachable). These relative positions are used as scalar attention biases in a Transformer encoder.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Tokenized incidence graph (with SAME_p and shortest‑path R_ij)",
            "representation_description": "Each entity label is tokenized and represented as separate token nodes; hyperarcs (facts) are represented as nodes in a bipartite incidence graph; SAME_p edges connect tokens from the same original entity with p encoding their original relative token positions; for any node pair R_ij is set to signed shortest‑path length (positive for forward paths, negative for backward), encode(p) for SAME_p, or ∞ for unreachable pairs. Relative positions are clamped by thresholds n_delta and n_p and used to look up learned scalar attention biases γ(R_ij) added to attention scores.",
            "representation_type": "token‑based, graph‑structured (incidence hypergraph), near‑lossless for token/arc info but limited by clamped distance buckets",
            "encoding_method": "Tokenize entity labels (BPE), split nodes into token nodes, convert hypergraph to bipartite incidence graph where arcs become nodes, add SAME_p edges for intra‑entity token order, compute shortest paths between nodes to produce signed R_ij (or ∞) and clamp values to n_delta/n_p for indexing bias embeddings; node labels prefixed with &lt;E&gt;/&lt;R&gt;.",
            "canonicalization": false,
            "average_token_length": 98,
            "dataset_name": "AGENDA",
            "task_name": "Knowledge‑graph → text generation (graph‑to‑text)",
            "model_name": "Graformer (Transformer encoder‑decoder with graph self‑attention)",
            "model_description": "Encoder‑decoder Transformer where the encoder uses multi‑head graph self‑attention that adds learned scalar biases γ(R_ij) based on signed shortest‑path lengths; 8 attention heads; encoder layers = 4, decoder layers = 5; model dim d = 400; feedforward dim = 2000; ~36.3M parameters (as reported).",
            "performance_metric": "BLEU, METEOR, CHRF++",
            "performance_value": "BLEU 17.80 ± 0.31; METEOR 22.07 ± 0.23; CHRF++ 45.43 ± 0.39; model size 36.3M params",
            "impact_on_training": "Enables the Transformer encoder to learn multiple structural views (per‑head biases) and capture both local and global graph patterns; parameter‑efficient relative to multi‑encoder approaches; required an epoch‑curriculum to train stably (curriculum learning was crucial).",
            "limitations": "Relative positions are discretized/clamped via n_delta and n_p (limits distinguishable distances); encodes only structural shortest‑path lengths (no label sequence information along paths), which may lose label‑specific path signals; no canonical linearization/ordering is enforced; some rare characters excluded by BPE (AGENDA char coverage 99.99%).",
            "comparison_with_other": "Compared to dual encoder (CGE‑LW) Graformer uses fewer parameters (~half) while achieving similar performance on AGENDA; unlike path‑label LSTM approaches, Graformer does not require an existent path between all node pairs and therefore handles disconnected KGs; compared to GNNs, Graformer learns local/global connectivity patterns via attention heads rather than relying on stacked message passing.",
            "uuid": "e7027.0",
            "source_info": {
                "paper_title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Token‑Incidence Graph (Graformer) - WebNLG",
            "name_full": "Tokenized incidence graph with SAME_p edges and shortest‑path relative position encoding (used in Graformer)",
            "brief_description": "Same tokenized incidence graph representation as above, applied to WebNLG: entity labels are token nodes, hyperarcs are nodes, SAME_p edges encode intra‑entity token order, and signed shortest‑path lengths/∞ are used as relative position indices for learned attention biases.",
            "citation_title": "here",
            "mention_or_use": "use",
            "representation_name": "Tokenized incidence graph (with SAME_p and shortest‑path R_ij)",
            "representation_description": "Tokenize entity/relation labels (BPE); build bipartite incidence graph (nodes = token nodes ∪ arc nodes); fully connect tokens from same entity with SAME_p edges (p encodes token offsets); compute signed shortest‑path lengths or ∞ for unreachable pairs; clamp to ranges and use as indices to learned scalar attention biases added to Transformer attention logits.",
            "representation_type": "token‑based, graph‑structured (incidence hypergraph), near‑lossless for token/arc info but limited by clamped distance buckets",
            "encoding_method": "Tokenization (SentencePiece BPE), split nodes by token, hyperedge→node incidence conversion, SAME_p intra‑entity edges, compute signed shortest paths for R_ij, clamp values to n_delta/n_p, use γ(R_ij) per attention head as additive bias.",
            "canonicalization": false,
            "average_token_length": 36,
            "dataset_name": "WebNLG",
            "task_name": "Knowledge‑graph → text generation (graph‑to‑text)",
            "model_name": "Graformer (Transformer encoder‑decoder with graph self‑attention)",
            "model_description": "Smaller Graformer config for WebNLG: 8 heads; encoder layers = 3, decoder layers = 3; model dim d = 256; feedforward dim = 512; graph self‑attention range n_delta = 4; ~5.3M parameters (as reported).",
            "performance_metric": "BLEU, METEOR, CHRF++",
            "performance_value": "BLEU 61.15 ± 0.22; METEOR 43.38 ± 0.17; CHRF++ 75.43 ± 0.19; model size 5.3M params",
            "impact_on_training": "Learns attention biases that separate local (direct neighbors) and global/distant interactions; empirically most heads focus on near nodes yet some heads attend to global patterns; competitive performance with substantially fewer parameters than some baselines.",
            "limitations": "As above: clipped relative‑position ranges limit fine‑grained distance distinctions; does not incorporate label sequences along paths; training required epoch curriculum; for WebNLG some heads suppress unreachable nodes, so behavior depends on dataset connectivity.",
            "comparison_with_other": "Achieves &gt;96% of CGE‑LW‑LG performance on WebNLG while using roughly half the parameters; compared to fully connected/global encoders (dual encoder methods) Graformer can distinguish unreachable vs distant nodes through the ∞ encoding and signed shortest‑path values.",
            "uuid": "e7027.1",
            "source_info": {
                "paper_title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Path‑label LSTM relation embeddings",
            "name_full": "Relation embeddings computed by an LSTM over labels on the shortest path between node pairs",
            "brief_description": "A representation used in some Transformer‑based graph encoders where, for any node pair, an LSTM processes the sequence of node/edge labels along the shortest path and the resulting vector is used as a relation embedding or attention bias between those nodes.",
            "citation_title": "Modeling graph structure in transformer for better AMR‑to‑text generation",
            "mention_or_use": "mention",
            "representation_name": "Path‑label LSTM encoding",
            "representation_description": "For each pair of nodes, extract the sequence of labels along the shortest connecting path and run an LSTM over that label sequence to produce a relation embedding which modifies attention between the nodes (e.g., added to keys/values or to attention scores).",
            "representation_type": "sequential, path‑based, label‑informed",
            "encoding_method": "Compute shortest path between node pairs (assumes connectivity), concatenate the edge/node labels along the path into a token sequence, run an LSTM to produce a fixed‑size relation embedding used in Transformer attention.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AMR (reported use in cited work)",
            "task_name": "AMR‑to‑text / graph‑to‑text generation",
            "model_name": "Graph Transformer with path‑label LSTM relation embeddings",
            "model_description": "Transformer encoder augmented with LSTM‑derived relation embeddings for attention between node pairs (as reported by Zhu et al. and Cai & Lam), typically applied in AMR‑to‑text settings.",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Injects label/path information directly into attention which can help when graphs are connected and path labels are informative (e.g., AMR), enabling interactions among distant nodes via label context.",
            "limitations": "Crucially assumes the existence of a path between every node pair (poor fit for disconnected KGs); can introduce spurious connections across components; depends on label sequences which may be noisy or unavailable; computational cost for path extraction/LSTM per pair.",
            "comparison_with_other": "Unlike Graformer, this approach uses label sequences along paths rather than pure structural shortest‑path lengths and therefore cannot handle disconnected graphs without ad hoc measures; Graformer encodes only structural distances (and SAME_p) and explicitly represents unreachable pairs via ∞.",
            "uuid": "e7027.2",
            "source_info": {
                "paper_title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Dual‑encoder (CGE‑LW / CGE‑LW‑LG)",
            "name_full": "Local and Global encoders combined (CGE‑LW from Ribeiro et al.)",
            "brief_description": "An approach that uses two graph encoders — one operating on the original graph topology (local context) and one on a fully connected version of the graph (global context) — combining their outputs to provide both local and global node context for graph‑to‑text generation.",
            "citation_title": "Modeling global and local node contexts for text generation from knowledge graphs",
            "mention_or_use": "mention",
            "representation_name": "Dual GNN encoders: original topology + fully‑connected graph",
            "representation_description": "Encode nodes twice: once with a GNN that respects the original edges (message passing among neighbors) for local context, and once with a GNN on a fully connected graph to capture global interactions; combine representations (e.g., concatenation or gating) before decoding.",
            "representation_type": "graph‑structured, multi‑view (local + global)",
            "encoding_method": "Run a message‑passing GNN on the given KG topology for local features; run a second GNN on a fully connected graph (or dense connectivity) for global context; fuse outputs for downstream decoding.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": "AGENDA and WebNLG (as evaluated / compared in this paper)",
            "task_name": "Knowledge‑graph → text generation",
            "model_name": "CGE‑LW (and CGE‑LW‑LG variants)",
            "model_description": "Two separate GNN encoders (one local, one global) whose outputs are combined; reported model variants have larger parameter counts (e.g., CGE‑LW ~69.8M on AGENDA, CGE‑LW‑LG ~10.4M on WebNLG depending on config).",
            "performance_metric": "BLEU, METEOR, CHRF++ (reported in comparisons)",
            "performance_value": "Reported in this paper for baselines: AGENDA CGE‑LW BLEU 18.01; METEOR 22.34; CHRF++ 46.69; params 69.8M. WebNLG CGE‑LW‑LG BLEU 63.69; METEOR 44.47; CHRF++ 76.66; params 10.4M.",
            "impact_on_training": "Provides explicit local and global context; can improve metrics by combining views, but requires additional parameters and separate encoder training.",
            "limitations": "Only exposes two extreme connectivity views (true neighbors vs fully connected) and thus cannot explicitly distinguish unreachable node pairs from distant but connected ones; more parameter‑heavy than Graformer.",
            "comparison_with_other": "Graformer aims to learn a continuum of structural views via per‑head biases on shortest‑path distances, achieving similar performance with fewer parameters and explicitly modelling unreachable pairs via ∞ encoding; CGE‑LW relies on separate encoders which increases parameter cost.",
            "uuid": "e7027.3",
            "source_info": {
                "paper_title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-05"
            }
        },
        {
            "name_short": "Relative position encodings (Shaw et al.)",
            "name_full": "Self‑attention with relative position representations",
            "brief_description": "Relative position embeddings added to key/value representations in Transformer attention so that attention scores and/or values are biased by relative token positions rather than absolute positions.",
            "citation_title": "Self‑attention with relative position representations",
            "mention_or_use": "mention",
            "representation_name": "Relative position embedding (key/value or scalar bias)",
            "representation_description": "Learn vector or scalar embeddings indexed by relative positions (i − j) which are added to key/value representations or to attention logits to make attention position‑aware; can be shared across heads or per‑head.",
            "representation_type": "sequential (relative positional), embedding‑based",
            "encoding_method": "Compute relative position index for token pairs (bounded range), lookup learned embedding A^K / A^V or scalar S and add to key/value or logits in attention computation.",
            "canonicalization": null,
            "average_token_length": null,
            "dataset_name": null,
            "task_name": "Text sequence modelling / Transformer attention",
            "model_name": "Transformer variants with relative position embeddings",
            "model_description": "Used in standard Transformers or T5 style models; can be shared or per‑head; used in Graformer decoder text self‑attention (Raffel style scalar bias variant).",
            "performance_metric": null,
            "performance_value": null,
            "impact_on_training": "Makes attention aware of relative ordering and distances in sequences; in Graformer the idea is extended to graph shortest‑path distances to inform graph self‑attention.",
            "limitations": "Originally designed for linear text sequences; extending to graphs requires defining a graph relative position function (as Graformer does).",
            "comparison_with_other": "Graformer adapts the relative position idea from sequences to graphs by using shortest‑path signed lengths and SAME_p encodings rather than linear token offsets.",
            "uuid": "e7027.4",
            "source_info": {
                "paper_title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs",
                "publication_date_yy_mm": "2021-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Modeling graph structure in transformer for better AMR‑to‑text generation",
            "rating": 2,
            "sanitized_title": "modeling_graph_structure_in_transformer_for_better_amrtotext_generation"
        },
        {
            "paper_title": "Modeling global and local node contexts for text generation from knowledge graphs",
            "rating": 2,
            "sanitized_title": "modeling_global_and_local_node_contexts_for_text_generation_from_knowledge_graphs"
        },
        {
            "paper_title": "Text Generation from Knowledge Graphs with Graph Transformers",
            "rating": 2,
            "sanitized_title": "text_generation_from_knowledge_graphs_with_graph_transformers"
        },
        {
            "paper_title": "Self‑attention with relative position representations",
            "rating": 2,
            "sanitized_title": "selfattention_with_relative_position_representations"
        },
        {
            "paper_title": "Deep graph convolutional encoders for structured data to text generation",
            "rating": 1,
            "sanitized_title": "deep_graph_convolutional_encoders_for_structured_data_to_text_generation"
        }
    ],
    "cost": 0.01770475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs</h1>
<p>Martin Schmitt ${ }^{1}$ Leonardo F. R. Ribeiro ${ }^{2}$ Philipp Dufter ${ }^{1}$ Iryna Gurevych ${ }^{2}$ Hinrich Schütze ${ }^{1}$<br>${ }^{1}$ Center for Information and Language Processing (CIS), LMU Munich<br>${ }^{2}$ Research Training Group AIPHES and UKP Lab, Technische Universität Darmstadt martin@cis.lmu.de</p>
<h4>Abstract</h4>
<p>We present Graformer, a novel Transformerbased encoder-decoder architecture for graph-to-text generation. With our novel graph selfattention, the encoding of a node relies on all nodes in the input graph - not only direct neighbors - facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these node-node relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>A knowledge graph (KG) is a flexible data structure commonly used to store both general world knowledge (Auer et al., 2008) and specialized information, e.g., in biomedicine (Wishart et al., 2018) and computer vision (Krishna et al., 2017). Generating a natural language description of such a graph ( $\mathrm{KG} \rightarrow$ text) makes the stored information accessible to a broader audience of end users. It is therefore important for KG-based question answering (Bhowmik and de Melo, 2018), data-to-document generation (Moryossef et al., 2019; Koncel-Kedziorski et al., 2019) and interpretability of KGs in general (Schmitt et al., 2020).</p>
<p>Recent approaches to KG $\rightarrow$ text employ encoderdecoder architectures: the encoder first computes vector representations of the graph's nodes, the decoder then uses them to predict the text sequence. Typical encoder choices are graph neural networks based on message passing between direct neighbors in the graph (Kipf and Welling, 2017; Veličković</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>et al., 2018) or variants of Transformer (Vaswani et al., 2017) that apply self-attention on all nodes together, including those that are not directly connected. To avoid losing information, the latter approaches use edge or node labels from the shortest path when computing the attention between two nodes (Zhu et al., 2019; Cai and Lam, 2020). Assuming the existence of a path between any two nodes is particularly problematic for KGs: a set of KG facts often does not form a connected graph.</p>
<p>We propose a flexible alternative that neither needs such an assumption nor uses label information to model graph structure: a Transformerbased encoder that interprets the lengths of shortest paths in a graph as relative position information and thus, by means of multi-head attention, dynamically learns different structural views of the input graph with differently weighted connection patterns. We call this new architecture Graformer.</p>
<p>Following previous work, we evaluate Graformer on two benchmarks: (i) the AGENDA dataset (Koncel-Kedziorski et al., 2019), i.e., the generation of scientific abstracts from automatically extracted entities and relations specific to scientific text, and (ii) the WebNLG challenge dataset (Gardent et al., 2017), i.e., the task of generating text from DBPedia subgraphs. On both datasets, Graformer achieves more than $96 \%$ of the state-of-the-art performance while using only about half as many parameters.</p>
<p>In summary, our contributions are as follows: (1) We develop Graformer, a novel graph-to-text architecture that interprets shortest path lengths as relative position information in a graph self-attention network. (2) Graformer achieves competitive performance on two popular KG-to-text generation benchmarks, showing that our architecture can learn about graph structure without any guidance other than its text generation objective. (3) To further investigate what Graformer learns about graph structure, we visualize the differently connected</p>
<p>graph views it has learned and indeed find different attention heads for more local and more global graph information. Interestingly, direct neighbors are considered particularly important even without any structural bias, such as introduced by a graph neural network. (4) Analyzing the performance w.r.t. different input graph properties, we find evidence that Graformer's more elaborate global view on the graph is an advantage when it is important to distinguish between distant but connected nodes and truly unreachable ones.</p>
<h2>2 Related Work</h2>
<p>Most recent approaches to graph-to-text generation employ a graph neural network (GNN) based on message passing through the input graph's topology as the encoder in their encoder-decoder architectures (Marcheggiani and Perez-Beltrachini, 2018; Koncel-Kedziorski et al., 2019; Ribeiro et al., 2019; Guo et al., 2019). As one layer of these encoders only considers immediate neighbors, a large number of stacked layers can be necessary to learn about distant nodes, which in turn also increases the risk of propagating noise (Li et al., 2018).</p>
<p>Other approaches (Zhu et al., 2019; Cai and Lam, 2020) base their encoder on the Transformer architecture (Vaswani et al., 2017) and thus, in each layer, compute self-attention on all nodes, not only direct neighbors, facilitating the information flow between distant nodes. Like Graformer, these approaches incorporate information about the graph topology with some variant of relative position embeddings (Shaw et al., 2018). They, however, assume that there is always a path between any pair of nodes, i.e., there are no unreachable nodes or disconnected subgraphs. Thus they use an LSTM (Hochreiter and Schmidhuber, 1997) to compute a relation embedding from the labels along this path. However, in contrast to the AMR $^{2}$ graphs used for their evaluation, KGs are frequently disconnected. Graformer is more flexible and makes no assumption about connectivity. Furthermore, its relative position embeddings only depend on the lengths of shortest paths i.e., purely structural information, not labels. It thus effectively learns differently connected views of its input graph.</p>
<p>Deficiencies in modeling long-range dependencies in GNNs have been considered a serious limitation before. Various solutions orthogonal to our approach have been proposed in recent work: By</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>incorporating a connectivity score into their graph attention network, Zhang et al. (2020) manage to increase the attention span to k-hop neighborhoods but, finally, only experiment with $k=2$. Our graph encoder efficiently handles dependencies between much more distant nodes. Pei et al. (2020) define an additional neighborhood based on Euclidean distance in a continuous node embedding space. Similar to our work, a node can thus receive information from distant nodes, given their embeddings are close enough. However, Pei et al. (2020) compute these embeddings only once before training whereas in our approach node similarity is based on the learned representation in each encoder layer. This allows Graformer to dynamically change node interaction patterns during training.</p>
<p>Recently, Ribeiro et al. (2020) use two GNN encoders - one using the original topology and one with a fully connected version of the graph - and combine their output in various ways for graph-totext generation. This approach can only see two extreme versions of the graph: direct neighbors and full connection. Our approach is more flexible and dynamically learns a different structural view per attention head. It is also more parameter-efficient as our multi-view encoder does not need a separate set of parameters for each view.</p>
<h2>3 The Graformer Model</h2>
<p>Graformer follows the general multi-layer encoderdecoder pattern known from the original Transformer (Vaswani et al., 2017). In the following, we first describe our formalization of the KG input and then how it is processed by Graformer.</p>
<h3>3.1 Graph data structure</h3>
<p>Knowledge graph. We formalize a knowledge graph (KG) as a directed, labeled multigraph $G_{K G}=\left\langle V, A, s, t, l_{V}, l_{A}, \mathcal{E}, \mathcal{R}\right\rangle$ with $V$ a set of vertices (the KG entities), $A$ a set of arcs (the KG facts), $s, t: A \rightarrow V$ functions assigning to each arc its source/target node (the subject/object of a KG fact), and $l_{V}: V \rightarrow \mathcal{E}, l_{A}: A \rightarrow \mathcal{R}$ providing labels for vertices and arcs, where $\mathcal{R}$ is a set of KG-specific relations and $\mathcal{E}$ a set of entity names.
Token graph. Entity names usually consist of more than one token or subword unit. Hence, a tokenizer tok : $\mathcal{E} \rightarrow \Sigma_{T}^{*}$ is needed that splits an entity's label into its components from the vocabulary $\Sigma_{T}$ of text tokens. Following recent work (Ribeiro et al., 2020), we mimic this composition-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Different representations of the same KG (types are omitted for clarity).</p>
<p>ality of node labels in the graph structure by splitting each node into as many nodes as there are tokens in its label. We thus obtain a directed hypergraph $G_T = (V_T, A, s_T, t_T, l_T, l_A, \Sigma_T, \mathcal{R}, \text{same})$, where $s_T$, $t_T$ : $A \rightarrow \mathcal{P}(V_T)$ now assign a set of source (resp. target) nodes to each (hyper-) arc and all nodes are labeled with only one token, i.e., $l_T$ : $V_T \rightarrow \Sigma_T$. Unlike <em>Ribeiro et al. (2020)</em>, we additionally keep track of all token nodes' origins: same : $V_T \rightarrow \mathcal{P}(V_T \times \mathbb{Z})$ assigns to each node $n$ all other nodes $n'$ stemming from the same entity together with the relative position of $l_T(n)$ and $l_T(n')$ in the original tokenized entity name. Fig. 1b shows the token graph corresponding to the KG in Fig. 1a. <strong>Incidence graph.</strong> For ease of implementation, our final data structure for the KG is the hypergraph's incidence graph, a bipartite graph where hyper-arcs are represented as nodes and edges are unlabeled: $G = (N, E, l, \Sigma, {\text{SAME}_p \mid p \in \mathbb{Z}})$ where $N = V_T \cup A$ is the set of nodes, $E = {(n_1, n_2) \mid n_1 \in s_T(n_2) \lor n_2 \in t_T(n_1)}$ the set of directed edges, $l : N \rightarrow \Sigma$ a label function, and $\Sigma = \Sigma_T \cup \mathcal{R}$ the vocabulary. We introduce SAME<sup>p</sup> edges to fully connect same clusters: SAME<sup>p</sup> = {($n_1, n_2$) | ($n_2, p$) ∈ same($n_1$)} where $p$ differentiates between different relative positions in the original entity string, similar to <em>Shaw et al. (2018)</em>. See Fig. 1c for an example.</p>
<h3>3.2 Graformer encoder</h3>
<p>The initial graph representation $\mathbf{H}^{(0)} \in \mathbb{R}^{|N| \times d}$ is obtained by looking up embeddings for the node labels in the learned embedding matrix $\mathbf{E} \in \mathbb{R}^{|\Sigma| \times d}$, i.e., $\mathbf{H}_i^{(0)} = \mathbf{e}^{l(n_i)} \mathbf{E}$ where $\mathbf{e}^{l(n_i)}$ is the one-hot-encoding of the $i$th node's label.</p>
<p>To compute the node representation $\mathbf{H}^{(L)}$ in the $L$th layer, we follow <em>Vaswani et al. (2017)</em>, i.e., we first normalize the input from the previous layer $\mathbf{H}^{(L-1)}$ via layer normalization $LN$, followed by multi-head graph self-attention SelfAtt<sub>g</sub> (see § 3.3 for details), which after dropout regularization $Dr$ and a residual connection yields the intermediate representation $\mathcal{I}$ (cf. Eq. (1)). A feedforward layer $FF$ with one hidden layer and GeLU <em>Hendrycks and Gimpel (2016)</em> activation computes the final layer output (cf. Eq. (2)). As recommended by <em>Chen et al. (2018)</em>, we apply an additional layer normalization step to the output $\mathbf{H}^{(L_E)}$ of the last encoder layer $L_E$.</p>
<p>$$
\mathcal{I}^{(L)} = \text{Dr}(\text{SelfAtt}_g(\text{LN}(\mathbf{H}^{(L-1)}))) + \mathbf{H}^{(L-1)} \tag{1}
$$</p>
<p>$$
\mathbf{H}^{(L)} = \text{Dr}(\text{FF}(\text{LN}(\mathcal{I}^{(L)}))) + \mathcal{I}^{(L)} \tag{2}
$$</p>
<p>SelfAtt<sub>g</sub> computes a weighted sum of $\mathbf{H}^{(L-1)}$:</p>
<p>$$
\text{SelfAtt}<em j="1">g(\mathbf{H})_i = \sum</em>
$$}^{|N|} \alpha_{ij}^g(\mathbf{H}_j \mathbf{W}^{V_g}) \tag{3</p>
<p>where $\mathbf{W}^{V_g} \in \mathbb{R}^{d \times d}$ is a learned parameter matrix.</p>
<p>In the next section, we derive the definition of the graph-structure-informed attention weights $\alpha_{i j}^{g}$.</p>
<h3>3.3 Self-attention for text and graphs with relative position embeddings</h3>
<p>In this section, we describe the computation of attention weights for multi-head self-attention. Note that the formulas describe the computations for one head. The output of multiple heads is combined as in the original Transformer <em>Vaswani et al. (2017)</em>. Text self-attention. <em>Shaw et al. (2018)</em> introduced position-aware self-attention in the Transformer by adding a relative position embedding $\mathbf{A}^{K} \in \mathbb{R}^{M \times M \times d}$ to $\boldsymbol{X}$ 's key representation, when computing the softmax-normalized attention scores $\boldsymbol{\alpha}<em i="i">{i}$ between $\boldsymbol{X}</em>$ 's value representation when computing the weighted sum (cf. Eq. (5)):} \in \mathbb{R}^{d}$ and the complete input embedding matrix $\boldsymbol{X} \in \mathbb{R}^{M \times d}$ (cf. Eq. (4)), and adding a second type of position embedding $\mathbf{A}^{V} \in \mathbb{R}^{M \times M \times d}$ to $\boldsymbol{X</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{\alpha}<em i="i">{i}=\sigma\left(\frac{\boldsymbol{X}</em>} \boldsymbol{W}^{Q}\left(\boldsymbol{X} \boldsymbol{W}^{K}+\mathbf{A<em i="i">{i}^{K}\right)^{\top}}{\sqrt{d}}\right) \
&amp; \boldsymbol{V}</em>}=\sum_{j=1}^{n} \alpha_{i j}\left(\boldsymbol{X<em i="i" j="j">{j} \boldsymbol{W}^{V}+\mathbf{A}</em>\right)
\end{aligned}
$$}^{V</p>
<p>where $\sigma(\cdot)$ denotes the softmax function, i.e.,</p>
<p>$$
\sigma(\boldsymbol{b})<em i="i">{i}=\frac{\exp \left(b</em>
$$}\right)}{\sum_{j=1}^{J} \exp \left(b_{j}\right)}, \quad \text { for } \boldsymbol{b} \in \mathbb{R}^{J</p>
<p>Recent work <em>Raffel et al. (2019)</em> has adopted a simplified form where value-modifying embeddings $\mathbf{A}^{V}$ are omitted and key-modifying embeddings $\mathbf{A}^{K}$ are replaced with learned scalar embeddings $\boldsymbol{S} \in \mathbb{R}^{M \times M}$ that - based on relative position - directly in- or decrease attention scores before normalization, i.e., Eq. (4) becomes Eq. (6).</p>
<p>$$
\boldsymbol{\alpha}<em i="i">{i}=\sigma\left(\frac{\boldsymbol{X}</em>\right)
$$} \boldsymbol{W}^{Q}\left(\boldsymbol{X} \boldsymbol{W}^{K}\right)^{\top}}{\sqrt{d}}+\boldsymbol{S}_{i</p>
<p><em>Shaw et al. (2018)</em> share their position embeddings across attention heads but learn separate embeddings for each layer as word representations from different layers can vary a lot. <em>Raffel et al. (2019)</em> learn separate $\boldsymbol{S}$ matrices for each attention head but share them across layers. We use <em>Raffel et al. (2019)</em>’s form of relative position encoding for text self-attention in our decoder (§ 3.4).</p>
<p>Graph self-attention. Analogously to self-attention on text, we define our structural graph</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$V_{T}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$A$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">s</td>
<td style="text-align: center;">v</td>
<td style="text-align: center;">d</td>
<td style="text-align: center;">w</td>
<td style="text-align: center;">e</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">c</td>
<td style="text-align: center;">u1</td>
<td style="text-align: center;">u2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">s</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">v</td>
<td style="text-align: center;">-4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">d</td>
<td style="text-align: center;">-5</td>
<td style="text-align: center;">-4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">w</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">e</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">-3</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">-4</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">-3</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">c</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">u1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">u2</td>
<td style="text-align: center;">-3</td>
<td style="text-align: center;">-3</td>
<td style="text-align: center;">-3</td>
<td style="text-align: center;">-1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">-2</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Figure 2: $\boldsymbol{R}$ matrix for the graph in Fig. 1c $\left(\delta_{\max }=3\right)$.
self-attention as follows:</p>
<p>$$
\boldsymbol{\alpha}<em i="i">{i}^{g}=\sigma\left(\frac{\boldsymbol{H}</em>\right)
$$} \boldsymbol{W}^{Q_{g}}\left(\boldsymbol{H} \boldsymbol{W}^{K_{g}}\right)^{\top}}{\sqrt{d}}+\gamma(\boldsymbol{R})_{i</p>
<p>$\boldsymbol{W}^{K_{g}}, \boldsymbol{W}^{Q_{g}} \in \mathbb{R}^{d \times d}$ are learned matrices and $\gamma$ : $\mathbb{Z} \cup{\infty} \rightarrow \mathbb{R}$ looks up learned scalar embeddings for the relative graph positions in $\boldsymbol{R} \in \mathbb{R}^{N \times N}$.</p>
<p>We define the relative graph position $R_{i j}$ between the nodes $n_{i}$ and $n_{j}$ with respect to two factors: (i) the text relative position $p$ in the original entity name if $n_{i}$ and $n_{j}$ stem from the same original entity, i.e., $\left(n_{i}, n_{j}\right) \in \operatorname{SAME}_{p}$ for some $p$ and (ii) shortest path lengths otherwise:</p>
<p>$$
R_{i j} \begin{cases}\infty, &amp; \text { if } \delta\left(n_{i}, n_{j}\right)=\infty \ &amp; \text { and } \delta\left(n_{j}, n_{i}\right)=\infty \ \text { encode }(p), &amp; \text { if }\left(n_{i}, n_{j}\right) \in \operatorname{SAME}<em i="i">{p} \ \delta\left(n</em>
$$}, n_{j}\right), &amp; \text { if } \delta\left(n_{i}, n_{j}\right) \leq \delta\left(n_{j}, n_{i}\right) \ -\delta\left(n_{j}, n_{i}\right), &amp; \text { if } \delta\left(n_{i}, n_{j}\right)&gt;\delta\left(n_{j}, n_{i}\right)\end{cases</p>
<p>where $\delta\left(n_{i}, n_{j}\right)$ is the length of the shortest path from $n_{i}$ to $n_{j}$, which we define to be $\infty$ if and only if there is no such path. encode maps a text relative position $p \in \mathbb{Z} \backslash{0}$ to an integer outside $\delta$ 's range to avoid clashes. Concretely, we use $\operatorname{encode}(p):=\operatorname{sgn}(p) \cdot \delta_{\max }+p$ where $\delta_{\max }$ is the maximum graph diameter, i.e., the maximum value of $\delta$ over all graphs under consideration.</p>
<p>Thus, we model graph relative position as the length of the shortest path using either only forward edges $\left(R_{i j}&gt;0\right)$ or only backward edges $\left(R_{i j}&lt;0\right)$. Additionally, two special cases are considered: (i) Nodes without any purely forward or purely backward path between them $\left(R_{i j}=\infty\right)$ and token nodes from the same entity. Here the relative position in the original entity string $p$ is encoded outside the range of path length encodings (which are always in the interval $\left[-\delta_{\max }, \delta_{\max }\right]$ ).</p>
<p>In practice, we use two thresholds, $n_{\delta}$ and $n_{p}$. All values of $\delta$ exceeding $n_{\delta}$ are set to $n_{\delta}$ and analogously for $p$. This limits the number of different positions a model can distinguish.
Intuition. Our definition of relative position in graphs combines several advantages: (i) Any node can attend to any other node - even unreachable ones - while learning a suitable attention bias for different distances. (ii) $\mathrm{SAME}<em p="p">{p}$ edges are treated differently in the attention mechanism. Thus, entity representations can be learned like in a regular transformer encoder, given that tokens from the same entity are fully connected with $\mathrm{SAME}</em>$ also captures the important distinction between incoming and outgoing paths. In this way, Graformer can, e.g., capture the difference between the subject and object of a fact, which is expressed as a relative position of -1 vs. 1 . The subject and object nodes, in turn, see each other as 2 and -2 , respectively.}$ edges with $p$ providing relative position information. (iii) The lengths of shortest paths often have an intuitively useful interpretation in our incidence graphs and the sign of the entries in $\boldsymbol{R</p>
<p>Fig. 2 shows the $\boldsymbol{R}$ matrix corresponding to the graph from Fig. 1c. Note how token nodes from the same entity, e.g., s, v, and d, form clusters as they have the same distances to other nodes, and how the relations inside such a cluster are encoded outside the interval $[-3,3]$, i.e., the range of shortest path lengths. It is also insightful to compare node pairs with the same value in $\boldsymbol{R}$. E.g., both s and w see e at a distance of 2 because the entities $S V D$ and word2vec are both the subject of a fact with embedding learning as the object. Likewise, s sees both c and u1 at a distance of 1 because its entity $S V D$ is subject to both corresponding facts.</p>
<h3>3.4 Graformer decoder</h3>
<p>Our decoder follows closely the standard Transformer decoder (Vaswani et al., 2017), except for the modifications suggested by Chen et al. (2018). Hidden decoder representation. The initial decoder representation $\boldsymbol{Z}^{(0)} \in \mathbb{R}^{M \times d}$ embeds the (partially generated) target text $\boldsymbol{T} \in \mathbb{R}^{M \times|\Sigma|}$, i.e., $\boldsymbol{Z}^{(0)}=\boldsymbol{T} \boldsymbol{E}$. A decoder layer $L$ then obtains a contextualized representation via self-attention as in the encoder (§ 3.2):</p>
<p>$$
\boldsymbol{C}^{(L)}=\operatorname{Dr}\left(\operatorname{SelfAtt}_{t}\left(L N\left(\boldsymbol{Z}^{(L-1)}\right)\right)\right)+\boldsymbol{Z}^{(L-1)}
$$</p>
<p>SelfAtt ${ }<em g="g">{t}$ differs from SelfAtt ${ }</em>$. As in $\S 3.2$, we make use of residual connections, layer normalization $L N$, and dropout $D r$ :}$ by using different position embeddings in Eq. (7) and, obviously, $R_{i j}$ is defined in the usual way for text. $\boldsymbol{C}^{(L)}$ is then modified via multi-head attention $M H A$ on the output $\boldsymbol{H}^{\left(L_{E}\right)}$ of the last graph encoder layer $L_{E</p>
<p>$$
\begin{aligned}
\boldsymbol{U}^{(L)} &amp; =\operatorname{Dr}\left(\operatorname{MHA}\left(L N\left(\boldsymbol{C}^{(L)}\right), \boldsymbol{H}^{\left(L_{E}\right)}\right)\right)+\boldsymbol{C}^{(L)} \
\boldsymbol{Z}^{(L)} &amp; =\operatorname{Dr}\left(F F\left(L N\left(\boldsymbol{U}^{(L)}\right)\right)\right)+\boldsymbol{U}^{(L)}
\end{aligned}
$$</p>
<p>where</p>
<p>$$
\begin{gathered}
M H A(\boldsymbol{C}, \boldsymbol{H})<em j="1">{i}=\sum</em>}^{|N|} \alpha_{i j}\left(\boldsymbol{H<em i="i">{j} \boldsymbol{W}^{V</em>\right) \
\boldsymbol{\alpha}}<em i="i">{i}=\sigma\left(\frac{\boldsymbol{C}</em>\right)
\end{gathered}
$$} \boldsymbol{W}^{Q_{t}}\left(\boldsymbol{H} \boldsymbol{W}^{K_{t}}\right)^{\top}}{\sqrt{d}</p>
<p>Generation probabilities. The final representation $\boldsymbol{Z}^{\left(L_{D}\right)}$ of the last decoder layer $L_{D}$ is used to compute the probability distribution $\boldsymbol{P}_{i} \in[0,1]^{|\Sigma|}$ over all words in the vocabulary $\Sigma$ at time step $i$ :</p>
<p>$$
\boldsymbol{P}<em i="i">{i}=\sigma\left(\boldsymbol{Z}</em>\right)
$$}^{\left(L_{D}\right)} \boldsymbol{E}^{\top</p>
<p>Note that $\boldsymbol{E} \in \mathbb{R}^{|\Sigma| \times d}$ is the same matrix that is also used to embed node labels and text tokens.</p>
<h3>3.5 Training</h3>
<p>We train Graformer by minimizing the standard negative log-likelihood loss based on the likelihood estimations described in the previous section.</p>
<h2>4 Experiments</h2>
<h3>4.1 Datasets</h3>
<p>We evaluate our new architecture on two popular benchmarks for KG-to-text generation, AGENDA (Koncel-Kedziorski et al., 2019) and WebNLG (Gardent et al., 2017). While the latter contains crowd-sourced texts corresponding to subgraphs from various DBPedia categories, the former was automatically created by applying an information extraction tool (Luan et al., 2018) on a corpus of scientific abstracts (Ammar et al., 2018). As this process is noisy, we corrected 7 train instances where an entity name was erroneously split on a special character and, for the same reason, deleted 1 train instance entirely. Otherwise, we use the data as is, including the train/dev/test split.</p>
<p>We list the number of instances per data split, as well as general statistics about the graphs in Table 1. Note that the graphs in WebNLG are humanauthored subgraphs of DBpedia while the graphs</p>
<table>
<thead>
<tr>
<th></th>
<th>AGENDA</th>
<th>WebNLG</th>
</tr>
</thead>
<tbody>
<tr>
<td>#instances in train</td>
<td>38,719</td>
<td>18,102</td>
</tr>
<tr>
<td>#instances in val</td>
<td>1,000</td>
<td>872</td>
</tr>
<tr>
<td>#instances in test</td>
<td>1,000</td>
<td>971</td>
</tr>
<tr>
<td>#relation types</td>
<td>7</td>
<td>373</td>
</tr>
<tr>
<td>avg #entities in KG</td>
<td>13.4</td>
<td>4.0</td>
</tr>
<tr>
<td>% connected graphs</td>
<td>0.3</td>
<td>99.9</td>
</tr>
<tr>
<td>avg #graph components</td>
<td>8.4</td>
<td>1.0</td>
</tr>
<tr>
<td>avg component size</td>
<td>1.6</td>
<td>3.9</td>
</tr>
<tr>
<td>avg #token nodes in graph</td>
<td>98.0</td>
<td>36.0</td>
</tr>
<tr>
<td>avg #tokens in text</td>
<td>157.9</td>
<td>31.5</td>
</tr>
<tr>
<td>avg % text tokens in graph</td>
<td>42.7</td>
<td>56.1</td>
</tr>
<tr>
<td>avg % graph tokens in text</td>
<td>48.6</td>
<td>49.0</td>
</tr>
<tr>
<td>Vocabulary size $</td>
<td>\Sigma</td>
<td>$</td>
</tr>
<tr>
<td>Character coverage in %</td>
<td>99.99</td>
<td>100.0</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of AGENDA and the dataset from the WebNLG challenge as used in our experiments. Upper part: data splits and original KGs. Lower part: token graphs and BPE settings.</p>
<p>in AGENDA were automatically extracted. This leads to a higher number of disconnected graph components. Nearly all WebNLG graphs consist of a single component, i.e., are connected graphs, whereas for AGENDA this is practically never the case. We also report statistics that depend on the tokenization (cf. § 4.2) as factors like the length of target texts and the percentage of tokens shared verbatim between input graph and target text largely impact the task difficulty.</p>
<h3>4.2 Data preprocessing</h3>
<p>Following previous work on AGENDA <em>Ribeiro et al. (2020)</em>, we put the paper title into the graph as another entity. In contrast to <em>Ribeiro et al. (2020)</em>, we also link every node from a real entity to every node from the title by TITLE2TXT and TXT2TITLE edges. The type information provided by AGENDA is, as usual for KGs, expressed with one dedicated node per type and HAS-TYPE arcs that link entities to their types. We keep the original pretokenized texts but lowercase the title as both node labels and target texts are also lowercased.</p>
<p>For WebNLG, we follow previous work <em>Gardent et al. (2017)</em> by replacing underscores in entity names with whitespace and breaking apart camel-cased relations. We furthermore follow the evaluation protocol of the original challenge by converting all characters to lowercased ASCII and separating all punctuation from alphanumeric characters during tokenization.</p>
<p>For both datasets, we train a BPE vocabulary using sentencepiece <em>Kudo and Richardson (2018)</em> on the train set, i.e., a concatenation of node labels and target texts. See Table 1 for vocabulary sizes. Note that for AGENDA, only 99.99% of the characters found in the train set are added to the vocabulary. This excludes exotic Unicode characters that occur in certain abstracts.</p>
<p>We prepend entity and relation labels with dedicated $\langle E\rangle$ and $\langle R\rangle$ tags.</p>
<h3>4.3 Hyperparameters and training details</h3>
<p>We train Graformer with the Adafactor optimizer <em>Shazeer and Stern (2018)</em> for 40 epochs on AGENDA and 200 epochs on WebNLG. We report test results for the model yielding the best validation performance measured in corpus-level BLEU <em>Papineni et al. (2002)</em>. For model selection, we decode greedily. The final results are generated by beam search. Following <em>Ribeiro et al. (2020)</em>, we couple beam search with a length penalty <em>Wu et al. (2016)</em> of 5.0. See Appendix A for more details and a full list of hyperparameters.</p>
<h3>4.4 Epoch curriculum</h3>
<p>We apply a data loading scheme inspired by the bucketing approach of <em>Koncel-Kedziorski et al. (2019)</em> and length-based curriculum learning <em>Platanios et al. (2019)</em>: We sort the train set by target text length and split it into four buckets of two times $40\%$ and two times $10\%$ of the data. After each training epoch, the buckets are shuffled internally but their global order stays the same from shorter target texts to longer ones. This reduces padding during batching as texts of similar lengths stay together and introduces a mini-curriculum from presumably easier examples (i.e., shorter targets) to more difficult ones for each epoch. This enables us to successfully train Graformer even without a learning rate schedule.</p>
<h2>5 Results and Discussion</h2>
<h3>5.1 Overall performance</h3>
<p>Table 2 shows the results of our evaluation on AGENDA in terms of BLEU <em>Papineni et al. (2002)</em>, METEOR <em>Banerjee and Lavie (2005)</em>, and CHRF++ <em>Popović (2017)</em>. Like the models we compare with, we report the average and standard deviation of 4 runs with different random seeds.</p>
<p>Our model outperforms previous Transformer-based models that only consider first-order neighborhoods per encoder layer <em>Koncel-Kedziorski et al. (2019); An et al. (2019)</em>. Compared to the very</p>
<table>
<thead>
<tr>
<th></th>
<th>BLEU</th>
<th>METEOR</th>
<th>CHRF++</th>
<th>#P</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ours</td>
<td>$17.80{ }_{0.31}$</td>
<td>$22.07{ }_{0.23}$</td>
<td>$45.43{ }_{0.39}$</td>
<td>36.3</td>
</tr>
<tr>
<td>GT</td>
<td>$14.30{ }_{0.1}$</td>
<td>$18.80{ }_{0.2}$</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GT+RBS</td>
<td>$15.1{ }_{0.97}$</td>
<td>$19.5{ }_{0.29}$</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>CGE-LW</td>
<td>$18.01{ }_{0.14}$</td>
<td>$22.34{ }_{0.07}$</td>
<td>$46.69{ }_{0.17}$</td>
<td>69.8</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results on AGENDA. GT (Graph Transformer) from <em>Koncel-Kedziorski et al. (2019)</em>; GT+RBS from <em>An et al. (2019)</em>; CGE-LW from <em>Ribeiro et al. (2020)</em>. Number of parameters in millions.</p>
<table>
<thead>
<tr>
<th></th>
<th>BLEU</th>
<th>METEOR</th>
<th>CHRF++</th>
<th>#P</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ours</td>
<td>$61.15{ }_{0.22}$</td>
<td>$43.38{ }_{0.17}$</td>
<td>$75.43{ }_{0.19}$</td>
<td>5.3</td>
</tr>
<tr>
<td>UPF-FORGe</td>
<td>40.88</td>
<td>40.00</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Melbourne</td>
<td>54.52</td>
<td>41.00</td>
<td>70.72</td>
<td>-</td>
</tr>
<tr>
<td>Adapt</td>
<td>60.59</td>
<td>44.00</td>
<td>76.01</td>
<td>-</td>
</tr>
<tr>
<td>Graph Conv.</td>
<td>55.90</td>
<td>39.00</td>
<td>-</td>
<td>4.9</td>
</tr>
<tr>
<td>GTR-LSTM</td>
<td>58.60</td>
<td>40.60</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>E2E GRU</td>
<td>57.20</td>
<td>41.00</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>CGE-LW-LG</td>
<td>$63.69{ }_{0.10}$</td>
<td>$44.47{ }_{0.12}$</td>
<td>$76.66{ }_{0.10}$</td>
<td>10.4</td>
</tr>
</tbody>
</table>
<p>Table 3: Experimental results on the WebNLG test set with seen categories. CGE-LW-LG from <em>Ribeiro et al. (2020)</em>; Adapt, Melbourne and UPF-FORGe from <em>Gardent et al. (2017)</em>; Graph Conv. from [Marcheggiani and Perez-Beltrachini (2018)]; GTR-LSTM from <em>Trisedya et al. (2018)</em>; E2E GRU from <em>Castro Ferreira et al. (2019)</em>. Number of parameters in millions.
recent models by <em>Ribeiro et al. (2020)</em>, Graformer performs very similarly. Using both a local and a global graph encoder, <em>Ribeiro et al. (2020)</em> combine information from very distant nodes but at the same time need extra parameters for the second encoder. Graformer is more efficient and still matches their best model’s BLEU and METEOR scores within a standard deviation.</p>
<p>The results on the test set of seen categories of WebNLG (Table 3) look similar. Graformer outperforms most original challenge participants and more recent work. While not performing on par with CGE-LW on WebNLG, Graformer still achieves more than $96 \%$ of its performance while using only about half as many parameters.</p>
<h3>5.2 Performance on different types of graphs</h3>
<p>We investigate whether Graformer is more suitable for disconnected graphs by comparing its performance on different splits of the AGENDA test set according to two graph properties: (i) the average number of nodes per connected component $\left(\mu_{c}\right)$ and (ii) the largest diameter across all of a graph’s</p>
<table>
<thead>
<tr>
<th>$\mu_{c}$</th>
<th></th>
<th>BLEU</th>
<th>METEOR</th>
<th>CHRF++</th>
</tr>
</thead>
<tbody>
<tr>
<td>$&lt;1.25$</td>
<td>Ours</td>
<td>15.44</td>
<td>20.59</td>
<td>43.23</td>
</tr>
<tr>
<td>(213)</td>
<td>CGE-LW</td>
<td>15.34</td>
<td>20.64</td>
<td>43.56</td>
</tr>
<tr>
<td>$&lt;1.5$</td>
<td>Ours</td>
<td>17.45</td>
<td>22.03</td>
<td>45.67</td>
</tr>
<tr>
<td>(338)</td>
<td>CGE-LW</td>
<td>17.29</td>
<td>22.32</td>
<td>45.88</td>
</tr>
<tr>
<td>$&lt;2.0$</td>
<td>Ours</td>
<td>18.94</td>
<td>22.86</td>
<td>46.49</td>
</tr>
<tr>
<td>(294)</td>
<td>CGE-LW</td>
<td>19.46</td>
<td>23.76</td>
<td>47.78</td>
</tr>
<tr>
<td>$\geq 2.0$</td>
<td>Ours</td>
<td>21.72</td>
<td>24.22</td>
<td>48.79</td>
</tr>
<tr>
<td>(155)</td>
<td>CGE-LW</td>
<td>20.97</td>
<td>24.98</td>
<td>49.83</td>
</tr>
</tbody>
</table>
<p>(a) Average size $\mu_{c}$ of graph components.</p>
<table>
<thead>
<tr>
<th>d</th>
<th></th>
<th>BLEU</th>
<th>METEOR</th>
<th>CHRF++</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Ours</td>
<td>16.48</td>
<td>21.16</td>
<td>43.94</td>
</tr>
<tr>
<td>(368)</td>
<td>CGE-LW</td>
<td>16.33</td>
<td>21.16</td>
<td>44.16</td>
</tr>
<tr>
<td>2</td>
<td>Ours</td>
<td>18.46</td>
<td>22.70</td>
<td>46.85</td>
</tr>
<tr>
<td>(414)</td>
<td>CGE-LW</td>
<td>18.20</td>
<td>23.14</td>
<td>47.28</td>
</tr>
<tr>
<td>$\geq 3$</td>
<td>Ours</td>
<td>19.44</td>
<td>23.17</td>
<td>47.29</td>
</tr>
<tr>
<td>(218)</td>
<td>CGE-LW</td>
<td>20.32</td>
<td>24.42</td>
<td>49.25</td>
</tr>
</tbody>
</table>
<p>(b) Largest diameter d across all of a graph’s components.</p>
<p>Table 4: Performance of a single run on the test split of AGENDA w.r.t. different input graph properties. The number of data points in each split is indicated in parentheses.
components (d).
We can see in Table 4 that the performance of both Graformer and CGE-LW <em>Ribeiro et al. (2020)</em> increases with more graph structure (larger $\mu_{c}$ and d), i.e., more information leads to more accurate texts. Besides, Graformer outperforms CGE-LW on BLEU for graphs with smaller components ( $0&lt;$ $\mu_{c}&lt;1.5$ ) and smaller diameters ( $\mathrm{d}&lt;3$ ). Although METEOR and CHRF++ scores always favor CGELW, the performance difference is also smaller for cases where BLEU favors Graformer.</p>
<p>We conjecture that Graformer benefits from its more elaborate global view, i.e., its ability to distinguish between distant but connected nodes and unreachable ones. CGE-LW’s global encoder cannot make this distinction because it only sees a fully connected version of the graph.</p>
<p>Curiously, Graformer’s BLEU is also better for larger components ( $\mu_{c} \geq 2.0$ ). With multiple larger components, Graformer might also better distinguish nodes that are part of the same component from those that belong to a different one.</p>
<p>Only for $1.5&lt;\mu_{c}&lt;2.0$, CGE-LW clearly outperforms Graformer in all metrics. It seems that Graformer is most helpful for extreme cases, i.e., when either most components are isolated nodes or when isolated nodes are the exception.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Attention bias γ learned by Graformer on the two datasets. SAME<sup>p</sup> edges are omitted.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>BLEU</th>
<th>METEOR</th>
<th>CHRF++</th>
</tr>
</thead>
<tbody>
<tr>
<td>Graformer</td>
<td>18.09</td>
<td>22.29</td>
<td>45.77</td>
</tr>
<tr>
<td>-length penalty</td>
<td>17.99</td>
<td>22.19</td>
<td>45.63</td>
</tr>
<tr>
<td>-beam search</td>
<td>17.33</td>
<td>21.74</td>
<td>44.87</td>
</tr>
<tr>
<td>-epoch curriculum</td>
<td>13.55</td>
<td>18.91</td>
<td>39.22</td>
</tr>
</tbody>
</table>
<p>Table 5: Ablation study for a single run on the test portion of AGENDA.</p>
<h3>5.3 Ablation study</h3>
<p>In a small ablation study, we examine the impact of beam search, length penalty, and our new epoch curriculum training. We find that beam search and length penalty do contribute to the overall performance but to a relatively small extent. Training with our new epoch curriculum, however, proves crucial for good performance. Platanios et al. (2019) argue that curriculum learning can replace a learning rate schedule, which is usually essential to train a Transformer model. Indeed we successfully optimize Graformer without any learning rate schedule, when applying the epoch curriculum.</p>
<h3>6 Learned graph structure</h3>
<p>We visualize the learned attention bias γ for different relative graph positions <em>Rij</em> (cf. § 3.3; esp. Eq. (7)) after training on AGENDA and WebNLG in Fig. 3. The eight attention heads (x-axis) have learned different weights for each graph position <em>Rij</em> (y-axis). Note that AGENDA has more possible <em>Rij</em> values because <em>nδ</em> = 6 whereas we set <em>nδ</em> = 4 for WebNLG.</p>
<p>For both datasets, we notice that one attention head primarily focuses on global information (5 for AGENDA, 4 for WebNLG). AGENDA even dedicates head 6 entirely to unreachable nodes, showing the importance of such nodes for this dataset. In contrast, most WebNLG heads suppress information from unreachable nodes.</p>
<p>For both datasets, we also observe that nearer nodes generally receive a high weight (focus on local information): In Fig. 3b, e.g., head 2 concentrates solely on direct incoming edges and head 0 on direct outgoing ones. Graformer can learn empirically based on its task where direct neighbors are most important and where they are not, showing that the strong bias from graph neural networks is not necessary to learn about graph structure.</p>
<h3>7 Conclusion</h3>
<p>We presented Graformer, a novel encoder-decoder architecture for graph-to-text generation based on Transformer. The Graformer encoder uses a novel type of self-attention for graphs based on shortest path lengths between nodes, allowing it to detect global patterns by automatically learning appropriate weights for higher-order neighborhoods. In our experiments on two popular benchmarks for text generation from knowledge graphs, Graformer achieved competitive results while using many fewer parameters than alternative models.</p>
<h2>Acknowledgments</h2>
<p>This work was supported by the BMBF (first author) as part of the project MLWin (01IS18050), by the German Research Foundation (second author) as part of the Research Training Group "Adaptive Preparation of Information from Heterogeneous Sources" (AIPHES) under the grant No. GRK 1994/1, and by the Bavarian research institute for digital transformation (bidt) through their fellowship program (third author). We also gratefully acknowledge a Ph.D. scholarship awarded to the first author by the German Academic Scholarship Foundation (Studienstiftung des deutschen Volkes).</p>
<h2>References</h2>
<p>Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama. 2019. Optuna: A next-generation hyperparameter optimization framework. In Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</p>
<p>Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Peters, Joanna Power, Sam Skjonsberg, Lucy Wang, Chris Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. 2018. Construction of the literature graph in semantic scholar. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 84-91, New Orleans - Louisiana. Association for Computational Linguistics.</p>
<p>Bang An, Xuannan Dong, and Changyou Chen. 2019. Repulsive bayesian sampling for diversified attention modeling. 4th workshop on Bayesian Deep Learning (NeurIPS 2019).</p>
<p>Sören Auer, Chris Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2008. DBpedia: A nucleus for a web of open data. In Proceedings of the 6th International Semantic Web Conference (ISWC), volume 4825 of Lecture Notes in Computer Science, pages 722-735. Springer.</p>
<p>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65-72, Ann Arbor, Michigan. Association for Computational Linguistics.</p>
<p>James S. Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Algorithms for hyper-parameter optimization. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 2546-2554. Curran Associates, Inc.</p>
<p>Rajarshi Bhowmik and Gerard de Melo. 2018. Generating fine-grained open vocabulary entity type descriptions. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 877-888, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Deng Cai and Wai Lam. 2020. Graph transformer for graph-to-sequence learning. AAAI Conference on Artificial Intelligence.</p>
<p>Thiago Castro Ferreira, Chris van der Lee, Emiel van Miltenburg, and Emiel Krahmer. 2019. Neu-ral data-to-text generation: A comparison between pipeline and end-to-end architectures. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 552-562, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Mia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion Jones, Mike Schuster, Noam Shazeer, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2018. The best of both worlds: Combining recent advances in neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 76-86, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. The WebNLG challenge: Generating text from RDF data. In Proceedings of the 10th International Conference on Natural Language Generation, pages 124-133, Santiago de Compostela, Spain. Association for Computational Linguistics.</p>
<p>Zhijiang Guo, Yan Zhang, Zhiyang Teng, and Wei Lu. 2019. Densely connected graph convolutional networks for graph-to-sequence learning. Transactions of the Association for Computational Linguistics, 7:297-312.</p>
<p>Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). Computing Research Repository, arXiv:1606.08415.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9:173580 .</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In International Conference on Learning Representations (ICLR).</p>
<p>Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, Mirella Lapata, and Hannaneh Hajishirzi. 2019. Text Generation from Knowledge Graphs with Graph Transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284-2293, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. 2017. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vision, 123:32-73.</p>
<p>Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Qimai Li, Zhichao Han, and Xiao ming Wu. 2018. Deeper insights into graph convolutional networks for semi-supervised learning. AAAI Conference on Artificial Intelligence.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Diego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to text generation. In Proceedings of the 11th International Conference on Natural Language Generation, pages 1-9, Tilburg University, The Netherlands. Association for Computational Linguistics.</p>
<p>Amit Moryossef, Yoav Goldberg, and Ido Dagan. 2019. Step-by-step: Separating planning from realization in neural data-to-text generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2267-2277, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2020. Geom-gcn: Geometric graph convolutional networks. In International Conference on Learning Representations (ICLR).</p>
<p>Emmanouil Antonios Platanios, Otilia Stretcu, Graham Neubig, Barnabas Poczos, and Tom Mitchell. 2019. Competence-based curriculum learning for neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1162-1172, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Maja Popović. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, pages 612-618, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the limits of transfer learning with a unified text-totext transformer. Computing Research Repository, arXiv:1910.10683.</p>
<p>Leonardo F. R. Ribeiro, Claire Gardent, and Iryna Gurevych. 2019. Enhancing AMR-to-text generation with dual graph representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3183-3194, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Leonardo F. R. Ribeiro, Yue Zhang, Claire Gardent, and Iryna Gurevych. 2020. Modeling global and local node contexts for text generation from knowledge graphs. Transactions of the Association for Computational Linguistics, 8(0):589-604.</p>
<p>Martin Schmitt, Sahand Sharifzadeh, Volker Tresp, and Hinrich Schütze. 2020. An unsupervised joint system for text generation from knowledge graphs and semantic parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7117-7130, Online. Association for Computational Linguistics.</p>
<p>Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464-468, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Noam Shazeer and Mitchell Stern. 2018. Adafactor: Adaptive learning rates with sublinear memory cost. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4596-4604. PMLR.</p>
<p>Bayu Distiawan Trisedya, Jianzhong Qi, Rui Zhang, and Wei Wang. 2018. GTR-LSTM: A triple encoder for sentence generation from RDF data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1627-1637, Melbourne, Australia. Association for Computational Linguistics.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, page 5998-6008. Curran Associates, Inc.</p>
<p>Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph Attention Networks. In International Conference on Learning Representations (ICLR).</p>
<p>David S Wishart, Yannick D Feunang, An C Guo, Elvis J Lo, Ana Marcu, Jason R Grant, Tanvir Sajed, Daniel Johnson, Carin Li, Zinat Sayeeda, Nazanin Assempour, Ithayavani Iynkkaran, Yifeng Liu, Adam Maciejewski, Nicola Gale, Alex Wilson, Lucy Chin, Ryan Cummings, Diana Le, Allison Pon, Craig Knox, and Michael Wilson. 2018. DrugBank 5.0: a major update to the DrugBank database for 2018. Nucleic Acids Research, 46(D1):D1074D1082.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. Computing Research Repository, arXiv:1609.08144.</p>
<p>Kai Zhang, Yaokang Zhu, Jun Wang, and Jie Zhang. 2020. Adaptive structural fingerprints for graph attention networks. In International Conference on Learning Representations (ICLR).</p>
<p>Jie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min Zhang, and Guodong Zhou. 2019. Modeling graph structure in transformer for better AMR-to-text generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameter</th>
<th style="text-align: right;">WebNLG</th>
<th style="text-align: right;">AGENDA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">model dimension $d$</td>
<td style="text-align: right;">256</td>
<td style="text-align: right;">400</td>
</tr>
<tr>
<td style="text-align: left;"># heads</td>
<td style="text-align: right;">8</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;"># encoder layers $L_{E}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;"># decoder layers $L_{D}$</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">5</td>
</tr>
<tr>
<td style="text-align: left;">feedforward dimension</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">2000</td>
</tr>
<tr>
<td style="text-align: left;">attention dropout</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">dropout</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">input dropout</td>
<td style="text-align: right;">0.0</td>
<td style="text-align: right;">0.1</td>
</tr>
<tr>
<td style="text-align: left;">text self-attention range $n_{t}$</td>
<td style="text-align: right;">25</td>
<td style="text-align: right;">50</td>
</tr>
<tr>
<td style="text-align: left;">graph self-attention range $n_{d}$</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">6</td>
</tr>
<tr>
<td style="text-align: left;">same range $n_{p}$</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">10</td>
</tr>
<tr>
<td style="text-align: left;">gradient accumulation</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">gradient clipping</td>
<td style="text-align: right;">1.0</td>
<td style="text-align: right;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">label smoothing</td>
<td style="text-align: right;">0.25</td>
<td style="text-align: right;">0.3</td>
</tr>
<tr>
<td style="text-align: left;">$L_{2}$ regularizer</td>
<td style="text-align: right;">$3 \cdot 10^{-3}$</td>
<td style="text-align: right;">$3 \cdot 10^{-4}$</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: right;">4</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;"># beams</td>
<td style="text-align: right;">2</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">length penalty</td>
<td style="text-align: right;">5.0</td>
<td style="text-align: right;">5.0</td>
</tr>
</tbody>
</table>
<p>Table 6: Hyperparameters used to obtain final experimental results on WebNLG and AGENDA.
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5459-5468, Hong Kong, China. Association for Computational Linguistics.</p>
<h2>A Hyperparameter details</h2>
<p>For AGENDA and WebNLG, a minimum and maximum decoding length were set according to the shortest and longest target text in the train set. Table 6 lists the hyperparameters used to obtain final results on both datasets. Input dropout is applied on the word embeddings directly after lookup for node labels and target text tokens before they are fed into encoder or decoder. Attention dropout is applied to all attention weights computed during multi-head (self-)attention.</p>
<p>For hyperparameter optimization, we only train for the first 10 (AGENDA) or 50 (WebNLG) epochs to save time. We use a combination of manual tuning and a limited number of randomly sampled runs. For the latter we apply Optuna with default parameters (Akiba et al., 2019; Bergstra et al., 2011) and median pruning, i.e., after each epoch of a particular hyperparameter run we check if the best performance so far is worse than the median performance of previous runs at the same epoch and if so, abort. For hyperparameter tuning, we decode greedily and measure performance in corpus-level BLEU (Papineni et al., 2002).</p>
<h2>B Qualitative examples</h2>
<p>Table 7 shows three example generations from our Graformer model and the CGE-LW system by</p>
<p>Ribeiro et al. (2020). Often CGE-LW generations have a high surface overlap with the reference text while Graformer texts fluently express the same content.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Ref.</th>
<th style="text-align: center;">julia morgan has designed many significant buildings, including the los angeles herald examiner building</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">CGE-LW</td>
<td style="text-align: center;">julia morgan has designed many significant buildings including the los angeles herald examiner building</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">one of the significant buildings designed by julia morgan is the los angeles herald examiner building</td>
</tr>
<tr>
<td style="text-align: center;">Ref.</td>
<td style="text-align: center;">asam pedas is a dish of fish cooked in a sour and hot sauce that comes from indonesia</td>
</tr>
<tr>
<td style="text-align: center;">CGE-LW</td>
<td style="text-align: center;">the main ingredients of asam pedas are fish cooked in a sour and hot sauce and comes from indonesia</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">the main ingredients of asam pedas are fish cooked in sour and hot sauce . the dish comes from indonesia</td>
</tr>
<tr>
<td style="text-align: center;">Ref.</td>
<td style="text-align: center;">banana is an ingredient in binignit which is a dessert . a cookie is also a dessert .</td>
</tr>
<tr>
<td style="text-align: center;">CGE-LW</td>
<td style="text-align: center;">banana is an ingredient in binignit , a cookie is also a dessert .</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">a cookie is a dessert , as is binignit , which contains banana as one of its ingredients</td>
</tr>
</tbody>
</table>
<p>Table 7: Example references and texts generated by CGE-LW (Ribeiro et al., 2020) and Graformer (marked Ours) for samples from the WebNLG test set. In case of multiple references, only one is shown for brevity.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ abstract meaning representation&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
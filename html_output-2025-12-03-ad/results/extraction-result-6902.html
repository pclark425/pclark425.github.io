<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6902 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6902</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6902</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-133.html">extraction-schema-133</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <p><strong>Paper ID:</strong> paper-258822829</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11597v1.pdf" target="_blank">Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments</a></p>
                <p><strong>Paper Abstract:</strong> Trustworthiness of artificially intelligent agents is vital for the acceptance of human-machine teaming in industrial manufacturing environments. Predictable behaviours and explainable (and understandable) rationale allow humans collaborating with (and building) these agents to understand their motivations and therefore validate decisions that are made. To that aim, we make use of G\"ardenfors's cognitively inspired Conceptual Space framework to represent the agent's knowledge using concepts as convex regions in a space spanned by inherently comprehensible quality dimensions. A simple typicality quantification model is built on top of it to determine fuzzy category membership and classify instances interpretably. We apply it on a use case from the manufacturing domain, using objects' physical properties obtained from cobots' onboard sensors and utilisation properties from crowdsourced commonsense knowledge available at public knowledge bases. Such flexible knowledge representation based on property decomposition allows for data-efficient representation learning of typically highly specialist or specific manufacturing artefacts. In such a setting, traditional data-driven (e.g., computer vision-based) classification approaches would struggle due to training data scarcity. This allows for comprehensibility of an AI agent's acquired knowledge by the human collaborator thus contributing to trustworthiness. We situate our approach within an existing explainability framework specifying explanation desiderata. We provide arguments for our system's applicability and appropriateness for different roles of human agents collaborating with the AI system throughout its design, validation, and operation.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6902.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6902.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conceptual Spaces (Gärdenfors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitively inspired geometric framework that represents concepts as convex regions in a metric space spanned by interpretable quality domains (e.g., colour, size, utilisation), with instances as vectors and prototypes as central points.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual spaces: The geometry of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>high-dimensional space</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are convex regions in a geometric space whose axes are cognitively meaningful quality domains; instances are points/vectors; similarity and typicality derive from geometric relations (distances, region membership) and concept prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Accounts for prototypicality gradients and graded membership; supports similarity-based categorization, concept combination, and offers a middle level between subsymbolic sensory encodings and symbolic concepts enabling interpretable representations.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Behavioral categorization studies (prototype effects) and neuroscientific findings (rodent electrophysiology and human fMRI showing geometric/hexadirectional organisation)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Typicality and categorization tasks in behavioral studies; spatial navigation electrophysiology in rodents; human fMRI tasks probing spatial and abstract feature-space navigation (hexadirectional modulation analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Prototype effects from cognitive semantics align with a geometric representation; neural data show grid/hexadirectional organization in the entorhinal/hippocampal system for spatial and abstract domains, providing neurobiological plausibility for geometric concept maps.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Gärdenfors, 2004</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6902.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6902.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype Theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype Theory (Rosch)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theory of categorization in which categories are organized around prototypical members: category membership is graded and determined by similarity to a prototype rather than strict necessary and sufficient features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Family resemblances: Studies in the internal structure of categories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Prototype Theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-based/prototype representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are represented by prototypes (central tendencies); typicality of an instance is a graded function of similarity/distance to the prototype across perceptual and conceptual features.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains graded category membership, typicality effects, family resemblance, and faster recognition/processing of prototypical exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Behavioral experiments (typicality ratings, categorization studies)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Typicality rating tasks, category verification, family-resemblance measures and reaction-time studies.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Empirical demonstrations of prototypicality effects and graded structure of many natural categories (e.g., Rosch & Mervis findings).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Rosch & Mervis, 1975</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6902.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6902.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>µw-model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>µw-model (membership-weight typicality model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computationally simple typicality/fuzzy classification model combining a membership function µ per quality dimension and a weight w per dimension to compute a weighted representativeness vector and overall typicality for concept assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>µw-model (typicality / fuzzy prototype hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>feature-based vector with weighted fuzzy membership</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Each instance is described by membership values µ for each quality dimension and weights w that scale dimensions; a weighted vector is constructed and the vector norm yields the instance's typicality for each concept; classification uses maximal typicality and can flag low-confidence/disputable cases.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Provides graded (fuzzy) category membership, interpretable rationale via dimension-level memberships and weights, supports data-efficient learning with sparse examples, and enables human-understandable explanations of classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Computational implementation and simulation experiments (simulated cobot environment) combined with heuristics informed by cognitive studies for weight assignment.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Simulated embodied agent (Webots) extracting physical and utilisation properties from labelled simulated objects, learning prototypes (centroids) and applying µw-model to perform interpretable classification.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Demonstrated feasibility in a simulated aerospace-manufacturing use case: the model produced interpretable typicality scores and handled scarce-data scenarios by leveraging property decomposition and crowdsourced utilisation properties.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Galetić & Nottle, 2022</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6902.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6902.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Grid-like cognitive map</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Grid-like code in entorhinal cortex for conceptual maps (grid cells)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural populations in entorhinal cortex exhibit hexadirectional (grid-like) firing/activation patterns that have been shown not only in spatial navigation but also in representing abstract/conceptual spaces, suggesting a neural geometric coding usable for conceptual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Organizing conceptual knowledge in humans with a gridlike code</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Grid-like cognitive map / entorhinal grid cell coding</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>neuronal population code (hexadirectional/grid representation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>The brain re-uses spatial coding mechanisms (grid cells, place cells) to structure abstract conceptual dimensions as maps; hexadirectional modulation of neural signals indexes traversal through continuous conceptual spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Provides a neural mechanism for mapping and indexing concepts in a geometric space, supporting similarity computations and prototype indexing; offers biological plausibility for geometrical conceptual representations like Conceptual Spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Rodent electrophysiology (grid cells) and human fMRI studies showing hexadirectional modulation in abstract tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Rodent spatial navigation recordings; human virtual-navigation and feature-space traversal paradigms during fMRI with analyses for sixfold symmetry (hexadirectional modulation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Grid cells recorded in rodent entorhinal cortex during spatial tasks (Hafting et al.); human fMRI shows hexadirectional signals when subjects navigate abstract feature spaces (Constantinescu et al.), supporting reuse of spatial coding for conceptual knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Constantinescu et al., 2016; Hafting et al., 2005</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6902.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6902.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Symbolic knowledge representation (ontologies, symbols)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical AI approach where concepts are encoded as discrete symbols/nodes (e.g., ontologies, knowledge graphs) with relations, enabling explicit logical inference but presenting challenges for learning from raw sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Symbolic representation</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>symbolic (relational/ontology-based)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Conceptual knowledge is stored as discrete symbols and structured relations in knowledge bases or ontologies; symbols have human-interpretable meaning and support logical manipulation and rule-based inference.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Enables explicit, explainable reasoning and manipulation of concepts; facilitates high-level symbolic operations and certification, but is limited in modelling perceptually-driven concept acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Engineering practice (ontologies, knowledge graphs like ConceptNet, WordNet) rather than direct neurobiological evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>N/A (used as representational approach in knowledge engineering and hybrid systems).</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>Widely used in knowledge bases (e.g., ConceptNet, WordNet) and helpful for explicit semantics and explainability, but poses difficulty for learning concepts directly from sensory inputs without intermediary representations.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td>Difficulty modelling concept acquisition from continuous sensory data; bridging to subsymbolic neural representations remains challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6902.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6902.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of the representational format of conceptual knowledge in the brain at a functional level, including theoretical models, their descriptions, claimed representational formats, supporting or contradictory empirical evidence, experimental paradigms, key findings, and citation information.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory-based Bayesian models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory-based Bayesian models of inductive learning and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Probabilistic hierarchical models that represent concepts as structured hypothesis spaces and perform Bayesian inference, explaining how humans can generalize from few examples using inductive constraints and domain theories.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory-based Bayesian models of inductive learning and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_name</strong></td>
                            <td>Theory-theory / Bayesian hierarchical models</td>
                        </tr>
                        <tr>
                            <td><strong>theory_type</strong></td>
                            <td>probabilistic distribution / hierarchical Bayesian</td>
                        </tr>
                        <tr>
                            <td><strong>theory_description</strong></td>
                            <td>Concepts are encoded as hypotheses in structured hypothesis spaces with priors and likelihoods; learning and generalization arise from Bayesian inference over these structured models, often yielding data-efficient learning.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_claims</strong></td>
                            <td>Explains rapid, data-efficient concept acquisition and structured generalization; captures human-like inductive inferences by combining statistical learning with structured priors.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_source</strong></td>
                            <td>Computational modeling validated against behavioral data (few-shot learning and inductive reasoning experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_paradigm</strong></td>
                            <td>Few-shot concept learning paradigms and inductive generalization tasks compared with model predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_result</strong></td>
                            <td>These models account for human-like generalization from sparse data and provide mechanistic accounts of how structured knowledge guides learning.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_theory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>counter_evidence</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Tenenbaum et al., 2006</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual spaces: The geometry of thought <em>(Rating: 2)</em></li>
                <li>Organizing conceptual knowledge in humans with a gridlike code <em>(Rating: 2)</em></li>
                <li>Microstructure of a spatial map in the entorhinal cortex <em>(Rating: 2)</em></li>
                <li>Family resemblances: Studies in the internal structure of categories <em>(Rating: 2)</em></li>
                <li>Theory-based Bayesian models of inductive learning and reasoning <em>(Rating: 2)</em></li>
                <li>A thorough formalization of conceptual spaces <em>(Rating: 1)</em></li>
                <li>Conceptnet 5.5: An open multilingual graph of general knowledge <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6902",
    "paper_id": "paper-258822829",
    "extraction_schema_id": "extraction-schema-133",
    "extracted_data": [
        {
            "name_short": "Conceptual Spaces",
            "name_full": "Conceptual Spaces (Gärdenfors)",
            "brief_description": "A cognitively inspired geometric framework that represents concepts as convex regions in a metric space spanned by interpretable quality domains (e.g., colour, size, utilisation), with instances as vectors and prototypes as central points.",
            "citation_title": "Conceptual spaces: The geometry of thought",
            "mention_or_use": "use",
            "theory_name": "Conceptual Spaces",
            "theory_type": "high-dimensional space",
            "theory_description": "Concepts are convex regions in a geometric space whose axes are cognitively meaningful quality domains; instances are points/vectors; similarity and typicality derive from geometric relations (distances, region membership) and concept prototypes.",
            "functional_claims": "Accounts for prototypicality gradients and graded membership; supports similarity-based categorization, concept combination, and offers a middle level between subsymbolic sensory encodings and symbolic concepts enabling interpretable representations.",
            "evidence_source": "Behavioral categorization studies (prototype effects) and neuroscientific findings (rodent electrophysiology and human fMRI showing geometric/hexadirectional organisation)",
            "experimental_paradigm": "Typicality and categorization tasks in behavioral studies; spatial navigation electrophysiology in rodents; human fMRI tasks probing spatial and abstract feature-space navigation (hexadirectional modulation analyses).",
            "key_result": "Prototype effects from cognitive semantics align with a geometric representation; neural data show grid/hexadirectional organization in the entorhinal/hippocampal system for spatial and abstract domains, providing neurobiological plausibility for geometric concept maps.",
            "supports_theory": true,
            "counter_evidence": "",
            "citation": "Gärdenfors, 2004",
            "uuid": "e6902.0"
        },
        {
            "name_short": "Prototype Theory",
            "name_full": "Prototype Theory (Rosch)",
            "brief_description": "A theory of categorization in which categories are organized around prototypical members: category membership is graded and determined by similarity to a prototype rather than strict necessary and sufficient features.",
            "citation_title": "Family resemblances: Studies in the internal structure of categories",
            "mention_or_use": "use",
            "theory_name": "Prototype Theory",
            "theory_type": "feature-based/prototype representation",
            "theory_description": "Concepts are represented by prototypes (central tendencies); typicality of an instance is a graded function of similarity/distance to the prototype across perceptual and conceptual features.",
            "functional_claims": "Explains graded category membership, typicality effects, family resemblance, and faster recognition/processing of prototypical exemplars.",
            "evidence_source": "Behavioral experiments (typicality ratings, categorization studies)",
            "experimental_paradigm": "Typicality rating tasks, category verification, family-resemblance measures and reaction-time studies.",
            "key_result": "Empirical demonstrations of prototypicality effects and graded structure of many natural categories (e.g., Rosch & Mervis findings).",
            "supports_theory": true,
            "counter_evidence": "",
            "citation": "Rosch & Mervis, 1975",
            "uuid": "e6902.1"
        },
        {
            "name_short": "µw-model",
            "name_full": "µw-model (membership-weight typicality model)",
            "brief_description": "A computationally simple typicality/fuzzy classification model combining a membership function µ per quality dimension and a weight w per dimension to compute a weighted representativeness vector and overall typicality for concept assignment.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_name": "µw-model (typicality / fuzzy prototype hybrid)",
            "theory_type": "feature-based vector with weighted fuzzy membership",
            "theory_description": "Each instance is described by membership values µ for each quality dimension and weights w that scale dimensions; a weighted vector is constructed and the vector norm yields the instance's typicality for each concept; classification uses maximal typicality and can flag low-confidence/disputable cases.",
            "functional_claims": "Provides graded (fuzzy) category membership, interpretable rationale via dimension-level memberships and weights, supports data-efficient learning with sparse examples, and enables human-understandable explanations of classifications.",
            "evidence_source": "Computational implementation and simulation experiments (simulated cobot environment) combined with heuristics informed by cognitive studies for weight assignment.",
            "experimental_paradigm": "Simulated embodied agent (Webots) extracting physical and utilisation properties from labelled simulated objects, learning prototypes (centroids) and applying µw-model to perform interpretable classification.",
            "key_result": "Demonstrated feasibility in a simulated aerospace-manufacturing use case: the model produced interpretable typicality scores and handled scarce-data scenarios by leveraging property decomposition and crowdsourced utilisation properties.",
            "supports_theory": true,
            "counter_evidence": "",
            "citation": "Galetić & Nottle, 2022",
            "uuid": "e6902.2"
        },
        {
            "name_short": "Grid-like cognitive map",
            "name_full": "Grid-like code in entorhinal cortex for conceptual maps (grid cells)",
            "brief_description": "Neural populations in entorhinal cortex exhibit hexadirectional (grid-like) firing/activation patterns that have been shown not only in spatial navigation but also in representing abstract/conceptual spaces, suggesting a neural geometric coding usable for conceptual knowledge.",
            "citation_title": "Organizing conceptual knowledge in humans with a gridlike code",
            "mention_or_use": "mention",
            "theory_name": "Grid-like cognitive map / entorhinal grid cell coding",
            "theory_type": "neuronal population code (hexadirectional/grid representation)",
            "theory_description": "The brain re-uses spatial coding mechanisms (grid cells, place cells) to structure abstract conceptual dimensions as maps; hexadirectional modulation of neural signals indexes traversal through continuous conceptual spaces.",
            "functional_claims": "Provides a neural mechanism for mapping and indexing concepts in a geometric space, supporting similarity computations and prototype indexing; offers biological plausibility for geometrical conceptual representations like Conceptual Spaces.",
            "evidence_source": "Rodent electrophysiology (grid cells) and human fMRI studies showing hexadirectional modulation in abstract tasks.",
            "experimental_paradigm": "Rodent spatial navigation recordings; human virtual-navigation and feature-space traversal paradigms during fMRI with analyses for sixfold symmetry (hexadirectional modulation).",
            "key_result": "Grid cells recorded in rodent entorhinal cortex during spatial tasks (Hafting et al.); human fMRI shows hexadirectional signals when subjects navigate abstract feature spaces (Constantinescu et al.), supporting reuse of spatial coding for conceptual knowledge.",
            "supports_theory": true,
            "counter_evidence": "",
            "citation": "Constantinescu et al., 2016; Hafting et al., 2005",
            "uuid": "e6902.3"
        },
        {
            "name_short": "Symbolic representation",
            "name_full": "Symbolic knowledge representation (ontologies, symbols)",
            "brief_description": "A classical AI approach where concepts are encoded as discrete symbols/nodes (e.g., ontologies, knowledge graphs) with relations, enabling explicit logical inference but presenting challenges for learning from raw sensory input.",
            "citation_title": "",
            "mention_or_use": "mention",
            "theory_name": "Symbolic representation",
            "theory_type": "symbolic (relational/ontology-based)",
            "theory_description": "Conceptual knowledge is stored as discrete symbols and structured relations in knowledge bases or ontologies; symbols have human-interpretable meaning and support logical manipulation and rule-based inference.",
            "functional_claims": "Enables explicit, explainable reasoning and manipulation of concepts; facilitates high-level symbolic operations and certification, but is limited in modelling perceptually-driven concept acquisition.",
            "evidence_source": "Engineering practice (ontologies, knowledge graphs like ConceptNet, WordNet) rather than direct neurobiological evidence.",
            "experimental_paradigm": "N/A (used as representational approach in knowledge engineering and hybrid systems).",
            "key_result": "Widely used in knowledge bases (e.g., ConceptNet, WordNet) and helpful for explicit semantics and explainability, but poses difficulty for learning concepts directly from sensory inputs without intermediary representations.",
            "supports_theory": null,
            "counter_evidence": "Difficulty modelling concept acquisition from continuous sensory data; bridging to subsymbolic neural representations remains challenging.",
            "citation": "",
            "uuid": "e6902.4"
        },
        {
            "name_short": "Theory-based Bayesian models",
            "name_full": "Theory-based Bayesian models of inductive learning and reasoning",
            "brief_description": "Probabilistic hierarchical models that represent concepts as structured hypothesis spaces and perform Bayesian inference, explaining how humans can generalize from few examples using inductive constraints and domain theories.",
            "citation_title": "Theory-based Bayesian models of inductive learning and reasoning",
            "mention_or_use": "mention",
            "theory_name": "Theory-theory / Bayesian hierarchical models",
            "theory_type": "probabilistic distribution / hierarchical Bayesian",
            "theory_description": "Concepts are encoded as hypotheses in structured hypothesis spaces with priors and likelihoods; learning and generalization arise from Bayesian inference over these structured models, often yielding data-efficient learning.",
            "functional_claims": "Explains rapid, data-efficient concept acquisition and structured generalization; captures human-like inductive inferences by combining statistical learning with structured priors.",
            "evidence_source": "Computational modeling validated against behavioral data (few-shot learning and inductive reasoning experiments).",
            "experimental_paradigm": "Few-shot concept learning paradigms and inductive generalization tasks compared with model predictions.",
            "key_result": "These models account for human-like generalization from sparse data and provide mechanistic accounts of how structured knowledge guides learning.",
            "supports_theory": true,
            "counter_evidence": "",
            "citation": "Tenenbaum et al., 2006",
            "uuid": "e6902.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual spaces: The geometry of thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "Organizing conceptual knowledge in humans with a gridlike code",
            "rating": 2,
            "sanitized_title": "organizing_conceptual_knowledge_in_humans_with_a_gridlike_code"
        },
        {
            "paper_title": "Microstructure of a spatial map in the entorhinal cortex",
            "rating": 2,
            "sanitized_title": "microstructure_of_a_spatial_map_in_the_entorhinal_cortex"
        },
        {
            "paper_title": "Family resemblances: Studies in the internal structure of categories",
            "rating": 2,
            "sanitized_title": "family_resemblances_studies_in_the_internal_structure_of_categories"
        },
        {
            "paper_title": "Theory-based Bayesian models of inductive learning and reasoning",
            "rating": 2,
            "sanitized_title": "theorybased_bayesian_models_of_inductive_learning_and_reasoning"
        },
        {
            "paper_title": "A thorough formalization of conceptual spaces",
            "rating": 1,
            "sanitized_title": "a_thorough_formalization_of_conceptual_spaces"
        },
        {
            "paper_title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
            "rating": 1,
            "sanitized_title": "conceptnet_55_an_open_multilingual_graph_of_general_knowledge"
        }
    ],
    "cost": 0.01592,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments</p>
<p>Vedran Galetić vedran.galetic@airbus.com 
Airbus Central R&amp;T AI Research Team Filton
Airbus Central R&amp;T AI Research Team Filton
United Kingdom, United Kingdom</p>
<p>Alistair Nottle alistair.nottle@airbus.com 
Airbus Central R&amp;T AI Research Team Filton
Airbus Central R&amp;T AI Research Team Filton
United Kingdom, United Kingdom</p>
<p>Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments
Knowledge Representation and ReasoningConceptual SpacesEx- plainable AITrustworthy AIComprehensibilityInterpretabilityHuman-Machine TeamingEmbodied Intelligent AgentsFrugal AI
Trustworthiness of artificially intelligent agents is vital for the acceptance of human-machine teaming in industrial manufacturing environments. Predictable behaviours and explainable (and understandable) rationale allow humans collaborating with (and building) these agents to understand their motivations and therefore validate decisions that are made. To that aim, we make use of Gärdenfors's cognitively inspired Conceptual Space framework to represent the agent's knowledge using concepts as convex regions in a space spanned by inherently comprehensible quality dimensions. A simple typicality quantification model is built on top of it to determine fuzzy category membership and classify instances interpretably. We apply it on a use case from the manufacturing domain, using objects' physical properties obtained from cobots' onboard sensors and utilisation properties from crowdsourced commonsense knowledge available at public knowledge bases. Such flexible knowledge representation based on property decomposition allows for data-efficient representation learning of typically highly specialist or specific manufacturing artefacts. In such a setting, traditional data-driven (e.g., computer vision-based) classification approaches would struggle due to training data scarcity. This allows for comprehensibility of an AI agent's acquired knowledge by the human collaborator thus contributing to trustworthiness. We situate our approach within an existing explainability framework specifying explanation desiderata. We provide arguments for our system's applicability and appropriateness for different roles of human agents collaborating with the AI system throughout its design, validation, and operation.</p>
<p>INTRODUCTION</p>
<p>Use of Artificial Intelligence (AI) is increasing across industry, often providing performance approaching, or even surpassing, that of humans on cognitive tasks for specific applications (e.g., [32,51]). This in turn leads to an increase in the use of embodied AI agents cooperating with humans and becoming embedded on critical systems. There is an increasing need to build trust within these teams. For instance, within the frame of aerospace manufacturing and operations, AI can assist workers in practical tasks such as: optimising scheduling of resources; automated visual inspections; and natural language interactions for receiving commands and reporting events; as well reducing user's cognitive load and supporting automated decision making in fleet management.</p>
<p>Whilst performance gains can be clear and measurable, ensuring acceptance of these technologies, and thereby realising those gains, is difficult. Explainability is one of key elements in building appropriate trust (i.e., not complacency), which is necessary for their acceptance.</p>
<p>Current high-performing AI systems, often based on deep neural network models (i.e., Deep Learning (DL)), tend to be 'black boxes', shrouding their internal knowledge and decision making processes, which in turn may not be readily accessible or interpretable to human users and subjects interacting with, or developing, them. Explainable AI (XAI) is widely seen as one of the cornerstones of AI trustworthiness. Indeed, the European Commission's High Level Expert Group on Artificial Intelligence (HLEGAI) specifies 'explainability' (or explicability) as one of their four ethical principles of fundamental trustworthiness of AI, with the others being: respect for human autonomy, prevention of harm, and fairness [28] (and XAI can play a part in ensuring those other three principles). Furthermore, the Group specifies seven key requirements that are to be addressed throughout an AI product's life-cycle, from which 'transparency', 'accountability', and 'human agency and oversight' are ones clearly related to explainability. These guidelines are upheld by the European Union Aviation Safety Agency (EASA), focusing on challenges around certification that black-box AI models impose, echoing explainability as one of the three main components of trustworthy AI [14], and fully recognising human-centricity in its AI Roadmap [13].</p>
<p>While the interest in XAI research has been tremendous both in academia and industry (e.g., [4,12,24,41]), the outputs have primarily been focussed on post-hoc explainability methods and techniques, i.e., providing explanations of an already developed high-performing yet 'explanatorily opaque' machine learning systems. Instead, we attempt inherent (or intrinsic) explainability by incorporating interpretability requirements early in the system development cycle and focusing on building inherently interpretable 1 and understandable AI systems. In this paper we demonstrate this through a more inherently explainable knowledge representation of the AI agent.</p>
<p>We model the agent's knowledge using a complementary combination of abstracted information from the agent's sensors and openly available commonsense general knowledge sources, subject to simple heuristic engineering. By representing both classes of knowledge on the same representation framework, typical for hybrid AI approaches, allows for knowledge modelling flexibility across application domains. Also, it addresses challenges of (high-performant) computer vision-based approaches of object classification pertaining to data scarcity for rare and specialist objects (not uncommon in aerospace manufacturing) and account of its rationale and outputs in a human-understandable actionable way.</p>
<p>INTERPRETABLE KNOWLEDGE REPRESENTATION</p>
<p>One means of increasing trustworthiness, and consequent certifiability 2 , of an AI agent teaming with humans in operational environments is through inherently interpretable knowledge representation modelling. This allows for the agent's comprehensibility, i.e., the ability to represent its acquired knowledge in humanunderstandable terms [4]. Symbolic representations are a classical way to represent knowledge due to symbols' intrinsic meaning. Although symbols are amenable for computational approaches involving logical calculus, one challenge of this level of representation is modelling the intelligent agent's concept acquisition 3 . Artificially intelligent agents, especially those that are embedded in complex environments and perform higher cognitive tasks, are predominantly based on deep neural networks, excelling at learning from statistical regularities of sensory inputs. Human mind employs prominent qualities of both of these representations.</p>
<p>Therefore, another challenge is mapping between the continuous space of knowledge representation engendered by learnt network models and the symbolic representations. Systematic neurosymbolic mapping, or an intermediary knowledge representation interfacing with both levels, would alleviate one of the major challenges of neural representations, namely, their opacity and consequent incomprehensibility of learnt knowledge, standing in the way of the AI's trustworthiness and certifiability.</p>
<p>Conceptual Knowledge Representation</p>
<p>The semantic theory of Conceptual Spaces [20,21] is an apt formalism in bridging the neuro-symbolic gap in knowledge modelling, proved useful in various application domains [57]. Concepts have inherent meaning, often denoted by corresponding symbols (those 1 As a note, we do not equate inherent interpretability with algorithmic or model transparency characteristic [41]. We adopt a more general definition of interpretability as 'the ability to explain or to present in understandable terms to a human' [12]. 2 By Certifiability we mean the ability for a system to be certified for use in a regulated environment. 3 It is, however, fair to acknowledge some promising steps forward in modelling interpretable concept acquisition and representation on the neural implementation level, e.g., see [9]. concepts that are languageable), whilst retaining their continuous nature arising from neural sensing of environmental input. The framework represents concepts as convex regions in a geometric space spanned by quality domains. A concept (e.g., apple) is described by pertaining ranges across quality domains (e.g., colour, size, taste) and encompasses instances (e.g., this green and sour apple sitting on my desk) represented as vectors characterised by specific properties (e.g., green, sour).</p>
<p>The Conceptual Spaces framework adopts basic tenets of cognitive semantics (inherited in turn from cognitive psychology), thus assuming prototypical effects in categorisation [46][47][48] and schematicity [37]. Conceptual algebraic operations and space mappings can be used to describe metaphoric and metonymic communication operations, while modelling concept combinations and quantifying similarity 4 arise naturally from the very structure of the geometric space.</p>
<p>Moreover, it has been empirically demonstrated that neural populations exhibit geometric (hexadirectional) organisation of conceptual knowledge in the mind, demonstrated on spatial tasks in rodents [25] as well as humans on spatial and, crucially, abstract non-spatial tasks [10]. It is suggested that cognitive maps of concepts may utilise place cells for concept indexing within a geometric space spanned by the hippocampal-entorhinal grid cell system. That makes Conceptual Spaces a valuable candidate for a neuroscientifically (along with cognitively) motivated knowledge representation framework [7].</p>
<p>Crowdsourced Knowledge Base</p>
<p>Humans are able to rapidly construct hyper-hypotheses based on remarkably little data [53,54] and utilise them as background knowledge and constraints in various tasks such as ad-hoc classification. Capturing the structure and content of commonsense knowledge is a challenging task in modelling an artificially intelligent agent. As a remedy, manually-crafted and crowdsourced knowledge bases can be used as a proxy. They are typically knowledge graphs such as ontologies (e.g., [38]) or hierarchically organised synsets [44].</p>
<p>ConceptNet [50] is an openly-available knowledge graph constructed through orchestrated input from both structured manuallycrafted sources (e.g., Wiktionary, OpenCyc [38]) and crowdsourcing campaigns. Despite heterogeneous sources and a vast number of concepts (8 million nodes) and relations between them (21 million edges), the knowledge base is remarkably tidy and manageable with only 36 consolidated relation types. Some of these relations such as UsedFor, MadeOf, and PartOf are of particular relevance for our use case in the aerospace manufacturing domain.</p>
<p>As we will see in the upcoming sections, utilisation properties that are acquired from this knowledge base shall be instrumental in modelling concepts as utilisation properties tend to be more relevant for classifying manufacturing objects than their surface physical properties. </p>
<p>Typicality Model</p>
<p>Whilst some very developed and faithful Conceptual Space implementations exist [5], we draw inspiration from the Conceptual Space framework and property decomposition-based representation of concepts that it employs, and propose utilising a simple model for interpretable fuzzy [56] classification. The model, called µw-model [17], relies on the prototype theory [46] and empirical theories on causal status [1,2] and concept centrality [49]. The model's name originates from two parameters that are used to describe an instance's typicality status in the frame of a concept, namely: the membership function µ quantifying typicality for one quality dimension (see Fig. 1); and the weight w per quality dimension. It is thereby possible to ascertain and explain that, for example, although an object observed in the manufacturing environment has surface similarities to the prototype of a previously learnt class, the facts that it is used for a different activity (see § 2.2) and that the utilisation property is much weightier than surface properties in the manufacturing environment jointly steer its classification to another class (e.g., Fig. 2).</p>
<p>These parameters are learnt in different ways, yet both are based on the agent's property detection capabilities (e.g., visual sensor) accompanied by tutor-provided labels of encountered instances. Based on instances' labels, and quality dimension value distributions, the agent is able to quantify µ frequentistically (e.g., assuming the prototype has the most occurring value), while w is quantified by using an empirically confirmed [19] hypothesis, based on concept centrality, on inverse relationship between the dimension's weight on one side and its variability of instances' property values across the category on the other (e.g., colour tends to be more important for natural kinds [48] such as apple than for artefacts such as car as the latter can usually be in an arbitrary colour).</p>
<p>Each instance is represented as a vector as shown in Eq. 1:
− → ( ) = ∑︁ √︄ ( ) ( ) ( ) · − → (1)
where is a quality dimension (e.g., width), − → are basis vectors spanning the space, is a quality domain (e.g., size), is an instance (e.g., this apple), ( ) is the weight of the quality dimension for the concept (e.g., apple), and ( ) is the typicality measure of the instance for the concept with respect to the quality dimension (e.g., the representativeness of 'this zebra-striped ball' for the concept zebra with respect to the quality dimension texture; which, as a note, would be high for this dimension, but extremely low for virtually any other dimension).</p>
<p>Typicality of the instance c with respect to concept C is the second norm of vector in Eq. 1, thus is calculated as in Eq. 2:
( ) = ∥ − → ( )∥ = √︄ ∑︁ ( ) ( ) 2 ( )(2)
while it is sensible to treat the instance as one pertaining to that concept for which its typicality is the highest, as in Eq. 3.
( ) = max ( )(3)
As the system is able to flag any disputable classifications (e.g., those for which the difference between the maximal and the second largest typicality from Eq. 2 is not above a threshold) in an interpretable way, it is possible to investigate the case using intrinsically interpretable explanation of the rationale as, e.g., a developer debugging the system or an operator on the shop floor.</p>
<p>To examine the feasibility of our model in a 'real-world' scenario, we applied the described knowledge modelling approach to an aerospace manufacturing process. There is the potential to make improvements to existing capability as well as develop new ones though the use of collaborative robots, or 'cobots'. These 'cobots' will improve quality and free up skilled workers to focus on their specialities. Therefore, explainability of decisions from robotic assistants is vital to building acceptance and trust and enabling effective collaboration, allowing these benefits to be realised.</p>
<p>In order to build within cobots an awareness of their environment they need to be able to rapidly and efficiently recognise objects which they may not have seen previously as well as with clear and understandable explanations for this recognition. By modelling the artificial agent's knowledge interpretably, using the Conceptual Space framework [20], we reduce the need for post-hoc explanation.</p>
<p>As an example, a cobot may classify an object as a drill because it because it is of similar size, shape, colour, and material composition, as examples used during the training process. Capturing utilisation properties, which can indicate for instance that the item is used for drilling, allows us to produce classification which is by its very nature interpretable. For example, the operator may obtain a natural language output such as 'I believe this is a drill as it looks similar to other drills I've seen in the past, and it is used for drilling', accompanied by a visualisation with associated typicality measures. Moreover, if an object bears surface similarities to previously seen instances of a drill, but is perceived to be used for riveting would instead be classified as a riveter. This reflects that utilisation properties of industrial artefacts are more heavily weighted in a classification than the surface properties (Fig. 2).</p>
<p>Setup</p>
<p>Industrial environments can be extremely sensitive to disruption, particularly with just-in-time manufacturing processes. Therefore, current industrial setups do not readily allow for the deployment of experimental robotics on the factory floor. To combat this we have instead used a simulated environment of a cobot in a factory setting to provide training data, as well as to get feedback and validation of the proposed methodologies. We have used the Webots Open Source Robotics Simulator [11] to create, in the first instance, a simple environment with a controllable e-puck robot [15] equipped with a simple sensor package, such as a standard vision sensor (i.e., a camera) as well as a time-of-flight sensor.</p>
<p>The simulation environment is populated with a variety of objects, using simplistic idealised examples such as 'Green Ball' or 'Red Cube', to allow for easier recognition, but still 'relatable' and inherently interpretable objects (see Fig. 4). As we enhanced the AI system, we introduced additional objects to represent real-world, ecologically relevant, objects such as a hammer or screwdriver.</p>
<p>To further augment the properties available, we attach custom properties to the objects within the simulator (e.g., stripy texture). This assists in the determining of ground truth values for colour, texture, utilisation properties, etc. These values can be extracted from the simulated environment programmatically along with the labels of the objects to provide a training set from which to learn a conceptual representation using inherently explainable and interpretable terms.</p>
<p>A conceptual space is learned by Voronoi tessellation [23] around prototypes. In the simpler simulation environment, the objects are considered to already be idealised prototypes in order to make it easier to carry out proof-of-concept space construction. With the migration to a more complex simulation environment, prototypes are calculated as centroids of the labelled instances used during training.</p>
<p>The robot can then be manipulated by a user to move through the simulation, and the field-of-view of the camera is determined. Any objects coming within this field of vision will then trigger the Webots software to forward the image to our API, alongside the ground truth properties relating to that object.</p>
<p>In a real-world implementation, capturing of images and detection of the objects therein would be carried out by state-of-the-art object / region of interest detection algorithms. As these are not the focus of our research, we have instead extracted ground truth data and imagery from the simulation environment itself. This modular approach allows for algorithms (such as property detectors) to be inserted as needed, and users can select the most appropriate algorithm for the use case being exploited.</p>
<p>This geometric representation level can thus be treated as an intermediary "middle ground" between the symbolic and neural levels by, on the one hand, abstracting sensory inputs onto appropriate points in a conceptual space, whilst, on the other hand, grounding symbols onto concept prototypes (as suggested in, e.g., [52]).</p>
<p>Property Extraction for Interpretable Classification</p>
<p>Separate property detectors are applied to extract properties of the observed object. These take two broad categories of detectors: physical property detectors and utilisation property detectors. Basic physical properties can be inferred from the sensors on the robotic platform. Examples of properties we have experimented with include: texture (using Concept Activation Vectors [30] to determine distinctive textural properties of an object, e.g., stripey, smooth, etc.); colour (represented within the HSB colour space, using simple computer vision approaches to determine the dominant colour of the object); size (determined by a depth-aware camera); current explorations include shape parametrisation (e.g., following [6]). A simple data flow diagram (Fig. 3) shows the high-level implementation we have used. Utilisation properties require additional transformations to derive. Sources for these could include task recognition through computer vision techniques (e.g., [16] to determine which tasks are being carried out in the observed video stream (e.g., 'hammering' or 'drilling'); or newer work around the Conceptual Space framework proposing event representation using force and result vectors [22,43]. Currently, utilisation properties are extracted in the training phase using crowdsourced knowledge bases.</p>
<p>General Knowledge Extraction.</p>
<p>We use ConceptNet's (see § 2.2) API 5 to acquire crowdsourced values for the UsedFor property of various manufacturing artefacts (e.g., drill). Each response (e.g.,</p>
<p>Simulation Environment</p>
<p>Virtual Sensors Property Extractors Concept Algebra Interpretable Classification Figure 3: Data flow in the manufacturing use case. Currently, simulated environment is used in the proof-of-concept scenario. Virtual, visual-based sensors are used to capture objects in the environment. Captured images are fed into the property extractor modules (e.g., for colour, texture, etc.), each dedicated to extracting the value corresponding to the pertaining quality dimension (e.g., hue, stripy, etc.). It is worth mentioning that some quality domains, whose extraction is more challenging due to limitations of the simulated environment (e.g., size), are quantified using the simulated environment specification directly, which is a temporary fix in line with the fact the focus of our work is not as much on the property detectors and computer vision as on concept building and class inference proof of concept in the "back end". An instance described by its (standardised) property values is represented as a vector and its representativeness measures for candidate classes quantified using the µw-model.</p>
<p>'drilling holes in things') is accompanied by the weight, reflecting the source's reliability. Due to the nature of free-form submissions, messiness of the corresponding response cannot be avoided (e.g., 'drilling holes in things' and 'drill a hole in something' are separate entries). To fix that, in the first iteration we group all responses with similar semantics, such as the examples above, and run the softmax function across all weights of semantically different items to acquire the quantity that we use to ground the membership function 6 for this utilisation property. Concretely, we get
( _ _ _ _ ) = 0.9999997
which is a reasonable quantity.</p>
<p>Since the described procedure would be quite cumbersome and time-consuming to run for each utilisation property, a more thorough approach to automating utilisation knowledge extraction from the public knowledge bases was developed. It consults ConceptNet and WordNet programmatically, via their respective APIs, alongside the NLTK 7 package, does not require any manual intervention, and proceeds in the following steps:</p>
<p>(1) Find the appropriate WordNet synset (i.e., a meaning that can be represented by multiple synonymous lexemes) for the given object label (e.g., 'drill'). The condition is that it denotes an artefact noun. In case there are multiple such synsets, as in the case of 'hammer' (handheld tool and a gun part), heuristically choose one whose encompassed lexemes have the highest occurrence frequency in the corpus; (2) Get all synonym lexeme lemmas for that synset using Word-Net; (3) For each lexeme, extract edges from ConceptNet originating from WordNet (higher credibility than crowdsourced data) that have the lexeme as the start node and 'UsedFor' as the relation. End nodes denote utilisation properties; (a) If no such edge is found (as, e.g., for forklift), then consult ConceptNet's crowdsourced knowledge. Of all edges that have the start node and relation as specified, eliminate those whose weight is 1.0 or less (these can be considered unreliable or noise). In each end node of the remaining edges, find a verb using part-of-speech tagger, lemmatise it (e.g., change 'carrying' to 'carry'), and add it to the list of utilisations; (b) If that does not yield a meaningful utilisation property either (as it does not, e.g., for riveter), use a heuristic inspired by low inflection of English language: Stem the artifact name ('riveter' to 'rivet') and check whether it can be used as a verb ('hammer', 'drill' are both nouns and verbs). If it can, add it to the utilisation list; (4) For the extracted verb synsets, extract only those whose meaning falls under WordNet categories 'contact', 'change' or 'motion' (more can be added where necessary). Synsets characterised as, e.g., 'verb.cognitive' would be excluded; (5) Use the retrieved synsets to infer the value for each selected utilisation quality dimension (currently, 'drill', 'hammer', 'lift', 'rivet'). Do this by comparing these synsets to physical utilisation synsets for each quality dimension, again retrieved from knowledge bases. Where there is a non-empty intersection of synsets, set the value of the corresponding quality dimension to one; where there is not, set it to zero 8 . * * * The majority of the described pipeline allows for the implementation of state-of-the-art property detectors. Similarly, the simulation environment could be substituted with real-world cobots with realworld sensors maintaining the same interfaces throughout.</p>
<p>Quality dimensions' membership values, along with weights, make up the representativeness vector (Eq. 1), from which we measure the instance's typicality across concepts (Eq. 2) and determine for which one its representativeness is the highest (Eq. 3, visualised in Fig. 4). The representativeness score and associated property weights and typicality measures are directly human-comprehensible and constitute our interpretable classification approach, as an (at this point simplified) alternative to traditional computer visionbased object recognition approaches.</p>
<p>Situation in the XAI Ecosystem</p>
<p>Our knowledge representation-based inherent explainability approach may be situated in a wider XAI framework against roles of system users and previously proposed explanation characteristics. This way it may be compared against other XAI techniques and Figure 4: Simulated industrial use case environment. We start with the simple case with simulated idealised objects (upper left), such as 'Red Sphere', 'Wooden Cube', etc. The robot encounters these objects and extracts ground-truth property values available from the simulator (colour, shape, composition), used as the basis for learning the conceptual space. Having validated the data pipeline and knowledge representation modelling, we moved towards the more ecologically valid manufacturing simulated environment (upper middle), where quality dimensions include physical properties like size, texture, composition, and utilisation properties. Generally, every instance's quality dimension values are used as input for membership quantification per various concepts of interest. The spider charts (lower) show example membership values per quality dimensions across four artefact concepts for two example observations. Together with the quality domain weights (currently arbitrated, in the future quantified via empirical hypotheses, see § 2.3), these membership values make up the vector representation of the instance, as per Eq. 1. The bar chart (upper right) represents the typicality of the two example instances across the four observed concepts, calculated via Eq. 2.</p>
<p>represent a candidate for the right application context and provided requirements.</p>
<p>Considering development of explainable AI systems teaming with humans in manufacturing environments, the following roles [55] of agents interacting with the AI can be envisioned for such applications:</p>
<p>• Creator -Developer of the AI system using property decomposition explainability for sanity check and debugging, as well as guidance in hyperparameter selection (e.g., threshold for disputable classification, see § 2.3); • Operator and Executor (possibly the same agent) -Manufacturing worker provided with an understandable explanation of the classifier's rationale (see Fig. 4), ready to act upon the output (e.g., take and use the intended fetched tool) or flag the system's incorrect behaviour; • Decision-Subject -In a more advanced use case, a human agent teaming with an embodied AI agent on a physical task (e.g., processing a large component), able to understand the agent's rationale of a tool choice. Obviously, trustworthiness of the AI system is of essence for the Decision-Subject role; • Examiner -Prior to industrialisation and deployment, any AI to be used in applications involving human safety requirements needs to be certified, for which thorough sanity checks of the AI's learnt knowledge and operation rationale are crucial.</p>
<p>It should be noted that the expectations from users will likely evolve as interactions increase. For instance, when first working together, it is likely an operator will require more frequent and detailed explanations as they begin to build trust. As this trust develops, it is likely that either less detailed or frequent explanations will be required, and could even cause user frustration (if a system consistently explains a 'simple' action undertaken many times per shift). This longitudinal approach is a matter of future work as discussed in § 4.</p>
<p>Our explainability approach can be described across various axes of the explanation characterisation framework, such as one by [26, Table 1]. Although a rigorous empirical validation of the technique in operation is pending, some characteristics may already be ascertained, e.g.:</p>
<p>• Interpretability -High-level interpretability of classification output endowed with visualisation artefacts involving interpretable quality domains and understandable diagrams. As opposed to post-hoc explainability, employed knowledge modelling via decomposition into interpretable quality dimensions is an integral part of the system's design; • Local/global explainability -In the interpretable classification use case ( § 3.2) it is possible to explore the membership function and weights per quality dimensions and thus validate the categorisation rationale, which adds to global explainability; each instance classification is accompanied by local explainability visual (or textual) artefacts (e.g., Fig. 4); • Feature importance quantifiability and visualisation -Learnt knowledge includes empirically validated dimension weights, which provides a cognitively-motivated importance quantification; • Explanation by example -The representational space is partitioned around the prototypes (see § 3.2), which may in turn be used to elucidate a seemingly unlikely categorisation outcome; • Interactivity -A simple visual tool has been developed that allows for interactive probing of the classifier by manipulating quality dimension weights and membership functions of the model, as well as property values of an instance; • Counterfactual reasoning -Using the interactive tool (see above) it is possible to understand what the categorisation outcome would have been had the parameters or properties been different.</p>
<p>The authors [26] treat the 'generalisability' characteristic as versatility across types of applicable black-box models and as such pertains solely to post-hoc explainability techniques. However, when discussing versatility of utilisation, it is worth noting that the underlying Conceptual Space framework and the proposed typicality model and its parameter learning allow for flexibility of application domain, with an (admittedly strong) assumption of selecting an appropriate set of quality domains and dimensions and their apt modelling. Once these are selected for a given domain, the pertaining parameters are learnt from the combination of exploring in the world (i.e., from observed distributions of quality dimension values) and expert guidance (i.e., providing the labels as input to the concept space construction process).</p>
<p>Qualitative Validation</p>
<p>Empirical validation of interpretability and explainability are still emerging topics in the field of XAI. To try and obtain validation of the work undertaken, we have consulted with subject matter experts and potential users to assess and understand the quality of the of the proposed system.</p>
<p>As discussed in § 3.3, we identified a number of different user roles, e.g., Creator-Developer, Operator, etc., and presented them with relevant explanations and demonstrations of the system. User feedback was then taken into account to implement improvements. For instance, a number of different visualisations were evaluated for displaying 'typicality' of objects to users (e.g., spider / radar charts, bar charts, doughnut charts, polar charts, etc.), to determine which ones conveyed the most intelligible and useful information.</p>
<p>On top of that, semantics of the interpretable classification output was validated with domain experts, for example, the two-level information presentation. This presentation includes visualisation of the overall calculated representativeness of an instance across categories, which can in turn be 'zoomed-into' to explore typicality across utilised quality dimensions, as in the bar chart and spider charts from Fig. 4, respectively.</p>
<p>Future work will include a rigorous methodological framework for more objective evaluation (on top of subjective, reporting-based approaches) of explainability and interpretability metrics, building on previous domain-relevant and domain-independent work such as [26] and [58].</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>We pursue a knowledge modelling approach towards inherent (vs. post-hoc) explainability of (embodied) AI agents teaming with humans in industrial environments. Heterogeneous knowledge involves physical properties acquirable by equipped sensors and utilisation properties obtained from publicly available crowdsourced and expert-manufactured resources of general knowledge. The properties are consulted by a simple classification model drawing inspiration from Gärdenfors's Conceptual Space framework. The model is defined by the membership function in the context of fuzzy set theory (quantified frequentistically from experience with environment); and the weight of the property (based on empirically confirmed cognitive semantic hypotheses).</p>
<p>Highly specific industrial environments, such as the aerospace manufacturing one, involve rare and specialist objects, for which there typically do not exist image datasets of sufficient size to train data-hungry high-performant (convolutional) neural networks for object classification. Therefore, our move from computer vision object recognition to property decomposition-based interpretable classification is relevant for such applications characterised by data scarcity, thus necessitating frugal AI approaches. Furthermore, decomposing concepts into interpretable property components facilitates model validation and debugging by a developer, scrutinous examination by a certifier, behaviour understandability for a teaming operator, and rationale interpretability for a decision-subject.</p>
<p>Whilst the described use case is a rather limited and illustratory one, it does open a few avenues for future research in knowledge representation and inference modelling of (embodied) AI agents teaming with humans in manufacturing and other industrial and operational environments.</p>
<p>Humans are remarkably successful in learning concepts from scarce data [27], the mechanics and phenomenology of which is of high interest to cognitive science, cognitive neuroscience, computational neuroscience, and artificial intelligence [34]. One of the challenges is modelling acquisition of core concepts and intuitive theories (e.g., in physics and psychology) as well as generic causal structures [36], all supporting commonsense reasoning. Some promising generative models are based on Bayesian reasoning in the context of hierarchies and structures of hypotheses and associated inductive constraints [54]. An artificial agent able to acquire and manipulate core concepts arguably makes its behaviour more predictable, its rationales more interpretable, and it itself more trustworthy.</p>
<p>In the simplified examples used we represent quality dimensions via orthogonal basis vectors spanning the conceptual space. However, in reality, property correlations are an important component of concept description [45], stemming from generic, causally originating, core concept structures, and empirically demonstrated by humans' effortless acquisition of systematic correlations [8,29,31,42], especially for natural kinds (e.g., many times the colour of fruit predicts its taste and ripeness). It is a matter of further exploration to address property interdependencies, possibly taking into account psychological essentialism, according to which perceptible properties are surface manifestation of entities' true nature [18,33].</p>
<p>Moreover, for artefacts, which can theoretically take arbitrary values of surface properties, it makes sense to focus on the objects' purported utilisation capability (affordances) stemming from their physical properties, like shape or composition (e.g., a large handheld-sized object with a hard flat metal head is likely to be used to hit nails). Attention-driven visual classification and visualisation techniques may be promising approaches for utilisation inference.</p>
<p>Whilst the current use case deals only with physical objects, newer work around the Conceptual Space framework proposes event representation using force and result vectors [22,43]. This work justifies further exploration as it may prove particularly useful on the shop floor environments. At the same time, one shall need to pay attention to interpretability of pertaining quality domains in this respect.</p>
<p>More recent work in the Conceptual Space theory is focussed on modelling events via force and result vectors [22,43] using the same underlying representational structure as in the described case for (physical) objects. This research is particularly relevant for industrial domains involving human-AI teaming, e.g., for gauging typicality of task execution. A recognised challenge is interpretability of force patterns constituting quality dimensions, which the event-based extension of the Conceptual Space framework is based on.</p>
<p>Cognitive modelling of operators using cognitive architectures [3,35] is a useful building block for modelling AI cognitive assistants aware of task representations, capable of anomalous behaviour detection, and cognitive load estimation as an input to autonomy engagement. A subsymbolic yet interpretable knowledge representation framework would arguably make a promising step towards modelling the declarative module ((e.g., [39,40]), which is yet to be demonstrated in industrial environments involving human-AI teaming.</p>
<p>Reported parallels between Conceptual Spaces' structural principles and empirical findings on conceptual knowledge neural implementation (see §2.1) invite researchers to explore the appropriateness of such an immanently multidimensional representation for mapping onto what appears to be two-dimensional hexadirectional grid-based encoding system on the neural substrate level [7]. Whilst running human studies (e.g., functional magnetic resonance imaging (fMRI) experiments) for hypothesis testing around candidate mapping mechanisms would not be feasible in industrial environments where the authors are affiliated, they shall make every effort to liaise with academic partners who may be better positioned to execute such studies, on top of continually following advances in the area.</p>
<p>Explainability capabilities of an AI system should be able to gauge the human user's expectations stemming from their experience with the task at hand and the teaming AI agent. Hence a longitudinal approach in context-aware trust and interpretability measurement, possibly involving user's overt or implicit feedback, and consequent XAI system's adaptability should be considered in the future.</p>
<p>Finally, it is important to recognise that the path towards a trustworthy AI is multi-faceted and explainability represents one pillar, albeit arguably the one most human user-focussed. The other pillars that are essential for responsible and ethical AI design are robustness and learning assurance, and fairness and nondiscrimination [13,14,28]). Obviously, AI trustworthiness should be addressed in an interdisciplinary manner, one difficult without a strong collaboration of academia and industry.</p>
<p>Figure 1 :
1Illustration of the membership functions for a continuous quality dimension across three example concepts (prototypical instances are assumed to bear prototypical value of the property) and of a nominal quality dimension for the concept.</p>
<p>Figure 2 :
2The artificial agent wants to classify a new object (upper right image) and extracts its physical and utilisation properties. The table (below) illustrates the and values for the 'Drill' concept. Although the new object is physically similar to the learnt 'Drill' prototype (upper left image shows a typical instance of 'Drill'), it will not be classified as one due to incompatibility of the utilisation property bearing the highest weight.
Conceptual Spaces also allow for discerning between similarity (e.g., car and van) and relatedness (e.g., car and driver), which is one of the challenges of distributional semantics approach.
http://api.conceptnet.io (accessed on 3 October 2022)
Somewhat counter-intuitively, what is called 'weight' in the ConceptNet system is semantically closer to the membership function of the w-model than the weight . See § 2.3 for details. 7 https://www.nltk.org/ (accessed on 4 October 2022)
Obviously, the proposed heuristics only allow for binary values of the UsedFor property. While the current approach served our purpose well, it is a matter of future work to infer continuous values.</p>
<p>Causal status effect in children's categorization. Susan A Woo-Kyoung Ahn, Jennifer A Gelman, Jill Amsterlaw, Charles W Hohenstein, Kalish, Cognition. 76Woo-kyoung Ahn, Susan A Gelman, Jennifer A Amsterlaw, Jill Hohenstein, and Charles W Kalish. 2000. Causal status effect in children's categorization. Cognition 76, 2 (2000), B35-B43.</p>
<p>Causal status as a determinant of feature centrality. Nancy S Woo-Kyoung Ahn, Mary E Kim, Martin J Lassaline, Dennis, Cognitive Psychology. 41Woo-kyoung Ahn, Nancy S Kim, Mary E Lassaline, and Martin J Dennis. 2000. Causal status as a determinant of feature centrality. Cognitive Psychology 41, 4 (2000), 361-416.</p>
<p>An integrated theory of the mind. Daniel John R Anderson, Bothell, D Michael, Scott Byrne, Christian Douglass, Yulin Lebiere, Qin, Psychological review. 1111036John R Anderson, Daniel Bothell, Michael D Byrne, Scott Douglass, Christian Lebiere, and Yulin Qin. 2004. An integrated theory of the mind. Psychological review 111, 4 (2004), 1036.</p>
<p>Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion. 58Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Ben- netot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. In- formation fusion 58 (2020), 82-115.</p>
<p>A thorough formalization of conceptual spaces. Lucas Bechberger, Kai-Uwe Kühnberger, Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz. SpringerLucas Bechberger and Kai-Uwe Kühnberger. 2017. A thorough formalization of conceptual spaces. In Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz). Springer, Springer, 58-71.</p>
<p>Representing Complex Shapes with Conceptual Spaces. Lucas Bechberger, Margit Scheibel, Second International Workshop'Concepts in Action: Representation, Learning, and Application. CARLALucas Bechberger and Margit Scheibel. 2020. Representing Complex Shapes with Conceptual Spaces. In Second International Workshop'Concepts in Action: Representation, Learning, and Application'(CARLA 2020).</p>
<p>Navigating cognition: Spatial codes for human thinking. L S Jacob, Peter Bellmund, Gärdenfors, I Edvard, Christian F Moser, Doeller, Science. 3626766Jacob LS Bellmund, Peter Gärdenfors, Edvard I Moser, and Christian F Doeller. 2018. Navigating cognition: Spatial codes for human thinking. Science 362, 6415 (2018), eaat6766.</p>
<p>Unsupervised concept learning and value systematicitiy: A complex whole aids learning the parts. Dorrit Billman, James Knutson, Journal of Experimental Psychology: Learning, Memory, and Cognition. 22458Dorrit Billman and James Knutson. 1996. Unsupervised concept learning and value systematicitiy: A complex whole aids learning the parts. Journal of Experi- mental Psychology: Learning, Memory, and Cognition 22, 2 (1996), 458.</p>
<p>Explainable neural networks that simulate reasoning. J Paul, Milo M Blazek, Lin, Nature Computational Science. 1Paul J Blazek and Milo M Lin. 2021. Explainable neural networks that simulate reasoning. Nature Computational Science 1, 9 (2021), 607-618.</p>
<p>Organizing conceptual knowledge in humans with a gridlike code. Alexandra O Constantinescu, Jill X O&apos;reilly, Timothy Ej Behrens, Science. 352Alexandra O Constantinescu, Jill X O'Reilly, and Timothy EJ Behrens. 2016. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 6292 (2016), 1464-1468.</p>
<p>Webots: robot simulator. Cyberbotics, Cyberbotics. 2022. Webots: robot simulator. https://cyberbotics.com/. Accessed: 2022-02-18.</p>
<p>Towards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, arXiv:1702.08608arXiv preprintFinale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter- pretable machine learning. arXiv preprint arXiv:1702.08608 (2017).</p>
<p>EASA Artificial Intelligence Roadmap 1.0 A human-centric approach to AI in aviation. Easa, EASA. 2020. EASA Artificial Intelligence Roadmap 1.0 A human-centric approach to AI in aviation. EASA. https://www.easa.europa.eu/document-library/general- publications/easa-artificial-intelligence-roadmap-10</p>
<p>EASA Concept Paper: First usable guidance for Level 1 machine learning applications. Easa, EASA. 2021. EASA Concept Paper: First usable guidance for Level 1 machine learning applications. EASA. https://www.easa.europa.eu/sites/default/files/dfu/ easa_concept_paper_first_usable_guidance_for_level_1_machine_learning_ applications_-_proposed_issue_01_1.pdf</p>
<p>. Epfl, EPFL. 2018. e-puck education robot. http://www.e-puck.org/. Accessed: 2022- 02-18.</p>
<p>Slowfast networks for video recognition. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow- fast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision. 6202-6211.</p>
<p>An Aggressive Robin in the Backyard: Formal Quantification of Prototypicality Level within the Frame of the Prototype Semantic Theory of Cognitive Linguistics. Suvremena lingvistika (Contemporary Linguistics). Vedran Galetić, 3771Vedran Galetić. 2011. An Aggressive Robin in the Backyard: Formal Quantification of Prototypicality Level within the Frame of the Prototype Semantic Theory of Cognitive Linguistics. Suvremena lingvistika (Contemporary Linguistics) 37, 71 (2011).</p>
<p>Towards the cognitive plausibility of conceptual space models. Vedran Galetić, 41Suvremena lingvistika (Contemporary LinguisticsVedran Galetić. 2015. Towards the cognitive plausibility of conceptual space models. Suvremena lingvistika (Contemporary Linguistics) 41, 80 (2015), 71-85.</p>
<p>Formalisation and quantification of a cognitively motivated concecptual space model based on the prototype theory. Vedran Galetić, Ph.D. Dissertation. University of ZagrebVedran Galetić. 2016. Formalisation and quantification of a cognitively moti- vated concecptual space model based on the prototype theory. Ph.D. Dissertation. University of Zagreb.</p>
<p>Conceptual spaces: The geometry of thought. Peter Gardenfors, MIT pressPeter Gardenfors. 2004. Conceptual spaces: The geometry of thought. MIT press.</p>
<p>The geometry of meaning: Semantics based on conceptual spaces. Peter Gardenfors, MIT pressPeter Gardenfors. 2014. The geometry of meaning: Semantics based on conceptual spaces. MIT press.</p>
<p>An Epigenetic Approach to Semantic Categories. Peter Gärdenfors, IEEE Transactions on Cognitive and Developmental Systems. 12Peter Gärdenfors. 2018. An Epigenetic Approach to Semantic Categories. IEEE Transactions on Cognitive and Developmental Systems 12, 2 (2018), 139-147.</p>
<p>Reasoning about categories in conceptual spaces. Peter Gärdenfors, Mary-Anne Williams, IJCAI. Citeseer. Peter Gärdenfors and Mary-Anne Williams. 2001. Reasoning about categories in conceptual spaces. In IJCAI. Citeseer, 385-392.</p>
<p>DARPA's explainable artificial intelligence (XAI) program. David Gunning, David Aha, AI magazine. 40David Gunning and David Aha. 2019. DARPA's explainable artificial intelligence (XAI) program. AI magazine 40, 2 (2019), 44-58.</p>
<p>Microstructure of a spatial map in the entorhinal cortex. Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, Edvard I Moser, Nature. 436Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I Moser. 2005. Microstructure of a spatial map in the entorhinal cortex. Nature 436, 7052 (2005), 801-806.</p>
<p>A systematic method to understand requirements for explainable AI (XAI) systems. Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana-Amate, Alistair Nottle, Alun Preece, Proceedings of the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2019). the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2019)Macau, China11Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana- Amate, Alistair Nottle, and Alun Preece. 2019. A systematic method to understand requirements for explainable AI (XAI) systems. In Proceedings of the IJCAI Work- shop on eXplainable Artificial Intelligence (XAI 2019), Macau, China, Vol. 11.</p>
<p>. Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, Matthew Botvinick, Neuroscience-inspired artificial intelligence. Neuron. 95Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick. 2017. Neuroscience-inspired artificial intelligence. Neuron 95, 2 (2017), 245-258.</p>
<p>Ethics guidelines for Trustworthy AI. Hlgeai, HLGEAI. 2019. Ethics guidelines for Trustworthy AI. European Commission. https: //digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</p>
<p>How children know the relevant properties for generalizing object names. S Susan, Linda B Jones, Smith, Developmental Science. 5Susan S Jones and Linda B Smith. 2002. How children know the relevant properties for generalizing object names. Developmental Science 5, 2 (2002), 219-232.</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, PMLRInternational conference on machine learning. Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668-2677.</p>
<p>What's behind different kinds of kinds: effects of statistical density on learning and representation of categories. Heidi Kloos, M Vladimir, Sloutsky, Journal of Experimental Psychology: General. 13752Heidi Kloos and Vladimir M Sloutsky. 2008. What's behind different kinds of kinds: effects of statistical density on learning and representation of categories. Journal of Experimental Psychology: General 137, 1 (2008), 52.</p>
<p>The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Matthieu Komorowski, A Leo, Omar Celi, Badawi, C Anthony, Gordon, Nature medicine. 24Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. 2018. The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine 24, 11 (2018), 1716-1720.</p>
<p>Inductive inference and its natural ground: An essay in naturalistic epistemology. Hilary Kornblith, Mit PressHilary Kornblith. 1995. Inductive inference and its natural ground: An essay in naturalistic epistemology. Mit Press.</p>
<p>Cognitive computational neuroscience. Nikolaus Kriegeskorte, Pamela K Douglas, Nature neuroscience. 21Nikolaus Kriegeskorte and Pamela K Douglas. 2018. Cognitive computational neuroscience. Nature neuroscience 21, 9 (2018), 1148-1160.</p>
<p>A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. E John, Christian Laird, Paul S Lebiere, Rosenbloom, 38Ai MagazineJohn E Laird, Christian Lebiere, and Paul S Rosenbloom. 2017. A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. Ai Magazine 38, 4 (2017), 13-26.</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people. Behavioral and brain sciences 40 (2017).</p>
<p>Women, fire, and dangerous things: What categories reveal about the mind. George Lakoff, University of Chicago pressGeorge Lakoff. 2008. Women, fire, and dangerous things: What categories reveal about the mind. University of Chicago press.</p>
<p>Building large knowledge-based systems; representation and inference in the Cyc project. B Douglas, Lenat, V Ramanathan, Guha, Addison-Wesley Longman Publishing Co., IncDouglas B Lenat and Ramanathan V Guha. 1989. Building large knowledge-based systems; representation and inference in the Cyc project. Addison-Wesley Longman Publishing Co., Inc.</p>
<p>Conceptual spaces for cognitive architectures: A lingua franca for different levels of representation. Antonio Lieto, Antonio Chella, Marcello Frixione, Biologically inspired cognitive architectures. 19Antonio Lieto, Antonio Chella, and Marcello Frixione. 2017. Conceptual spaces for cognitive architectures: A lingua franca for different levels of representation. Biologically inspired cognitive architectures 19 (2017), 1-9.</p>
<p>Dual PECCS: a cognitive system for conceptual representation and categorization. Antonio Lieto, Daniele P Radicioni, Valentina Rho, Journal of Experimental &amp; Theoretical Artificial Intelligence. 29Antonio Lieto, Daniele P Radicioni, and Valentina Rho. 2017. Dual PECCS: a cognitive system for conceptual representation and categorization. Journal of Experimental &amp; Theoretical Artificial Intelligence 29, 2 (2017), 433-452.</p>
<p>The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Zachary C Lipton, Queue. 16Zachary C Lipton. 2018. The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16, 3 (2018), 31-57.</p>
<p>The parallel distributed processing approach to semantic cognition. L James, Timothy T Mcclelland, Rogers, Nature reviews neuroscience. 4James L McClelland and Timothy T Rogers. 2003. The parallel distributed pro- cessing approach to semantic cognition. Nature reviews neuroscience 4, 4 (2003), 310-322.</p>
<p>Construals of meaning: The role of attention in robotic language production. Anne-Laure Mealier, Grégoire Pointeau, Peter Gärdenfors, Peter Ford Dominey, Interaction Studies. 17Anne-Laure Mealier, Grégoire Pointeau, Peter Gärdenfors, and Peter Ford Dominey. 2016. Construals of meaning: The role of attention in robotic lan- guage production. Interaction Studies 17, 1 (2016), 41-69.</p>
<p>WordNet: a lexical database for English. A George, Miller, Commun. ACM. 38George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995), 39-41.</p>
<p>Formalizing conceptual spaces. Martin Raubal, Formal ontology in information systems, proceedings of the third international conference (FOIS 2004). 114Martin Raubal. 2004. Formalizing conceptual spaces. In Formal ontology in information systems, proceedings of the third international conference (FOIS 2004), Vol. 114. 153-164.</p>
<p>Cognition and Categorization. L. Erlbaum Associates, Chapter Principles of categorization. Eleanor Rosch and Barbara B. LloydEleanor Rosch and Barbara B. Lloyd (Eds.). 1978. Cognition and Categorization. L. Erlbaum Associates, Chapter Principles of categorization, 27-48.</p>
<p>Family resemblances: Studies in the internal structure of categories. Eleanor Rosch, Carolyn B Mervis, Cognitive psychology. 7Eleanor Rosch and Carolyn B Mervis. 1975. Family resemblances: Studies in the internal structure of categories. Cognitive psychology 7, 4 (1975), 573-605.</p>
<p>Basic objects in natural categories. Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, M David, Penny Johnson, Boyes-Braem, Cognitive psychology. 8Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, David M Johnson, and Penny Boyes-Braem. 1976. Basic objects in natural categories. Cognitive psychology 8, 3 (1976), 382-439.</p>
<p>Feature centrality and conceptual coherence. A Steven, Sloman, C Bradley, Woo-Kyoung Love, Ahn, Cognitive Science. 22Steven A Sloman, Bradley C Love, and Woo-Kyoung Ahn. 1998. Feature centrality and conceptual coherence. Cognitive Science 22, 2 (1998), 189-228.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Going deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1-9.</p>
<p>Gramatika i predočavanje. Elżbieta Tabakowska, Filozofski Fakultet Sveučilišta u Zagrebu-FF Pressuvod u kognitivnu lingvistikuElżbieta Tabakowska. 2005. Gramatika i predočavanje: uvod u kognitivnu lingvis- tiku. Filozofski Fakultet Sveučilišta u Zagrebu-FF Press.</p>
<p>Theory-based Bayesian models of inductive learning and reasoning. B Joshua, Tenenbaum, L Thomas, Charles Griffiths, Kemp, Trends in cognitive sciences. 107Joshua B Tenenbaum, Thomas L Griffiths, and Charles Kemp. 2006. Theory-based Bayesian models of inductive learning and reasoning. Trends in cognitive sciences 10, 7 (2006), 309-318.</p>
<p>How to grow a mind: Statistics, structure, and abstraction. B Joshua, Charles Tenenbaum, Kemp, L Thomas, Noah D Griffiths, Goodman, science. 331Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. 2011. How to grow a mind: Statistics, structure, and abstraction. science 331, 6022 (2011), 1279-1285.</p>
<p>Interpretable to whom? A role-based model for analyzing interpretable machine learning systems. Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, Supriyo Chakraborty, arXiv:1806.07552arXiv preprintRichard Tomsett, Dave Braines, Dan Harborne, Alun Preece, and Supriyo Chakraborty. 2018. Interpretable to whom? A role-based model for analyzing interpretable machine learning systems. arXiv preprint arXiv:1806.07552 (2018).</p>
<p>Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected papers by Lotfi A Zadeh. A Lotfi, Zadeh, World ScientificLotfi A Zadeh. 1996. Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected papers by Lotfi A Zadeh. World Scientific, 394-432.</p>
<p>Applications of conceptual spaces. Frank Zenker, Peter Gärdenfors, 25Frank Zenker and Peter Gärdenfors. 2015. Applications of conceptual spaces. Cited on 25 (2015).</p>
<p>Towards an Integrated Evaluation Framework for XAI: An Experimental Study. Qiyuan Zhang, Mark Hall, Mark Johansen, Vedran Galetic, Jacques Grange, Santiago Quintana-Amate, Alistair Nottle, Dylan M Jones, Phillip L Morgan, 10.1016/j.procs.2022.09.450Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 26th International Conference KES2022. 207Qiyuan Zhang, Mark Hall, Mark Johansen, Vedran Galetic, Jacques Grange, Santiago Quintana-Amate, Alistair Nottle, Dylan M Jones, and Phillip L Morgan. 2022. Towards an Integrated Evaluation Framework for XAI: An Experimental Study. Procedia Computer Science 207 (2022), 3884-3893. https://doi.org/10.1016/ j.procs.2022.09.450 Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 26th International Conference KES2022.</p>            </div>
        </div>

    </div>
</body>
</html>
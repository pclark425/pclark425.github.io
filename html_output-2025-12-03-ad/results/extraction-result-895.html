<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-895 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-895</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-895</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-d7a74d22139b8ff806f9dbcb5242b216eeac50c7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/d7a74d22139b8ff806f9dbcb5242b216eeac50c7" target="_blank">Asking Before Action: Gather Information in Embodied Decision Making with Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work proposes Asking Before Action (ABA), a method that empowers the agent to proactively query external sources for pertinent information using natural language during their interactions in the environment, and demonstrates remarkable performance on tasks that previous methods struggle to solve.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e895.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e895.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asking Before Acting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based embodied decision-making agent that actively issues natural-language queries to external information sources during interaction to reduce exploration and improve task efficiency in partially-observable environments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Asking Before Acting (ABA)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ABA is implemented on top of pretrained (visual) language models (examples: Vicuna, GPT-3.5, GPT-4). Key components: (1) an augmented action/state formalism where asking and answering are actions/parts of state (A^U, S^U); (2) in-context learning (ICL) with chain-of-thought style "think" tokens so the LLM generates analysis, optional metadata tokens (\u0305a_t) that encode rationale for asking, and then the question tokens; (3) a decision rule that selects the highest-probability token sequence from the LLM conditioned on K in-context example trajectories; (4) for vision inputs, a captioning frontend (InstructBLIP) produces text fed into the LLM. ABA interleaves "think/ask/act" steps: it identifies unknown but task-relevant context via metadata, formulates open-ended natural-language questions, sends them to an external information source (the Human Model H), receives textual answers, appends them to its history, and continues planning and acting.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ALFWorld (text-based household), Robot-arm manipulation (numerical control), and real-world image-based embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>ALFWorld: a text-based embodied household environment where the agent receives local textual observations (rooms, visible containers/objects) and must complete tasks such as pick/place; observations are partial (agent cannot see the full state). Robot-arm tasks require mapping language answers to numerical coordinates. Real-world image tasks use image captions as observations; all settings are partially observable and require information gathering (where objects are, disambiguation of underspecified instructions).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Human Model H (an LLM, implemented with Vicuna-7B in evaluation) that answers free-form questions given a descriptive paragraph extracted from simulator; InstructBLIP (visual captioner) to convert images into text; simulator/PDDL data extraction used to produce ground-truth descriptive paragraphs and to collect training metadata for ABA-FT.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual outputs: (a) descriptive paragraphs of object placements (structured natural-language 'A is in B' lines), (b) free-form answers to open-domain questions (object locations, relative descriptions, user preferences), (c) image captions (text) from InstructBLIP, and (d) QA labels for fine-tuning datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A purely textual, history-based belief mechanism: the agent maintains tau_t = concat(instruction, s1, a1, ..., st) where answers and question-metadata are appended into the same text stream; the augmented state S^U explicitly includes answer tokens (L_ans). There is no explicit probabilistic belief tracker; memory is provided by the LLM context and, for ABA-FT, by learned weights from finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>When the agent asks a question, the Human Model's textual answer is appended to the trajectory/history (tau_{t+1}). Subsequent LLM generations condition on this extended history; therefore tool outputs are implicitly incorporated into the agent's belief via token-context. ABA-FT additionally shapes the model parameters via QA and policy finetuning so the model better internalizes when to ask and how to remember/act on answers across longer horizons.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM-based generation with In-Context Learning and Chain-of-Thought style prompting to produce rationale ('think' steps), question formulation, and high-level action sequences; ABA-FT augments this with supervised finetuning on QA pairs (D_QA) and action trajectories (D_pi) to learn the asking rationale and recover from noisy actions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>High-level symbolic navigation commands generated by the LLM such as 'go to <location>'; no explicit graph-based or shortest-path planner (A*, Dijkstra) is used—the LLM outputs sequenced 'move' actions and the environment/simulator executes them.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>ALFWorld: 20cmABA/GPT-3.5 (G35) All-tasks in-distribution success 84% ± 3 (Table 1); out-of-distribution All 85% ± 4. With Vicuna-7B backbone: 20cmABA/V7B All ID 56% ± 3, OOD 48% ± 2. Robot-arm tasks: ABA averaged 85–100% depending on task parameters (see Figure 3) — e.g., Task1 (x=3..5) ABA success 85–90% vs baseline lower. Real-world image tasks: qualitative—ABA completed all three tested scenarios (zero-shot) using image caption inputs and human execution of plans.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>Baselines without querying external human-model: ReAct (GPT-3.5) All ID 64% ± 1, OOD 65% ± 2; ReAct (Vicuna-7B) All ID 7% ± ~3, OOD 6% ± ~1. BUTLER (imitation baseline with 1e5 trajectories) reported variable results (see Table 1) but ABA with asking outperformed BUTLER on most settings using far fewer examples.</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Actively querying an external natural-language information source dramatically improves both success rate and efficiency in partially-observable text-based embodied tasks: ABA reduces physical action counts and episode lengths (often >50% fewer actions) while increasing success; textual history as an implicit belief suffices for incorporating tool outputs when combined with LLM reasoning; finetuning (ABA-FT) further improves long-horizon / multi-round performance by internalizing asking rationale and memory.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e895.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e895.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ABA-FT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Asking Before Acting with Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A finetuned variant of ABA that converts question-rationale metadata into QA training pairs and finetunes the LLM on both QA and policy trajectories to improve asking decisions and long-horizon behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ABA-FT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Starts from the ABA architecture and further finetunes the LLM on two datasets: (1) D_QA where metadata and history are reformulated as QA pairs indicating whether to ask and what to ask, and (2) D_pi where action sequences and metadata are supervised, with noisy actions masked out. Training objective maximizes log-likelihood on QA and masked policy targets. This yields better scalability to long trajectories and multi-round tasks by encoding asking rationale into model parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ambiguous ALFWorld and Multiround ALFWorld (variants of ALFWorld), plus other embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Harder variants of ALFWorld where tasks are intentionally underspecified (Ambiguous) or involve multiple rounds in the same room (Multiround), testing memory and the ability to avoid redundant questions; environments remain partially observable and require retaining previously obtained information.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Same external information sources as ABA: Human Model H (LLM) answering descriptive paragraphs; simulator/PDDL used to create training metadata and labels.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual QA labels and free-form answers produced by the Human Model; structured descriptive paragraphs derived from simulator; these are used both at finetune-time (as supervised signals) and at test-time (as answers to questions).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Like ABA, uses textual history tau_t; additionally learns to encode asking-rationale and memory into model weights through supervised finetuning so longer-horizon dependencies persist despite token-limits during inference.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Tool outputs are appended to history; during finetuning, the model learns mappings from history+QA to decisions, improving its ability to infer when a question is necessary and to recall previously-observed information across rounds even when in-context example diversity is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM generation augmented by supervised finetuning (imitation-style) on QA and policy datasets to produce more stable long-horizon plans and asking behavior; retains chain-of-thought style 'think' steps in generation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>High-level 'go to <location>' actions produced by the LLM; no explicit path planner is used. For robot-control tasks, generates numerical coordinates derived from textual answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>ABA-FT yields additional improvements over ABA on long-horizon and multi-round tasks; paper reports ABA-FT > ABA especially in Multiround ALFWorld (exact numeric gains not tabulated in the main table but noted qualitatively; finetuning used e.g. 1500 and 500 trajectories for different variants).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Finetuning on QA and policy pairs teaches the LLM the rationale for asking and improves robustness and memory across longer tasks; ABA-FT reduces redundant questioning in multiround settings and boosts performance where simple ICL suffers due to token limits and dataset diversity constraints.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e895.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e895.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human Model H</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human / External Information Source Model (H)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated human/external information source implemented as an LLM (Vicuna-7B) that is instructed to answer agent questions using a descriptive paragraph derived from the simulator's ground-truth object placements.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Human Model (H) - LLM-based answerer</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>H is implemented as another pretrained LLM (Vicuna-7B in experiments). Prior to evaluation the simulator's ground-truth object placement list is converted by rules into a natural-language descriptive paragraph ('A is in B' sentences). H is prompted (with few-shot examples) to read that paragraph and answer open-domain questions posed by the agent in free-form natural language, e.g., 'Where can I find the dish sponge?' and returns textual lists or relative descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ALFWorld (used to generate the descriptive paragraph), also used in other embodied evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>The underlying environment (ALFWorld) is partially observable for the agent; the Human Model is given a derived (simulator-to-text) representation that contains full placement info, so H effectively has privileged information relative to the agent.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Internally uses the descriptive paragraph extracted from the simulator (a structured textual map) and few-shot prompting; the 'tool' is an LLM answering queries.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual answers listing object locations (e.g., 'dishsponge 1 is in garbagecan 1, dishsponge 2 is in drawer 3, ...'), relative descriptors (e.g., 'second red block from the left'), and preference clarifications.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>H does not plan; it returns answers based on the provided paragraph and prompt instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>The Human Model (Vicuna-7B) accuracy on a held-out ALFWorld QA test (8 scenarios × 5 questions each) was reported as 72.5% ± 9.7% (Section C).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an LLM as a drop-in simulated human enables flexible, open-domain answers that ABA leverages to avoid exhaustive search; ABA is robust to some errors in H (e.g., small inaccuracies in reported object identifiers), but H's imperfect accuracy (≈72.5%) indicates that real human/tool quality affects agent performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e895.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e895.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InstructBLIP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InstructBLIP (vision-language captioning model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A vision-to-text frontend used to create image captions that are fed into the decision-making LLM; enables ABA to operate on real-world image observations by converting pixels to textual descriptions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Visual instruction tuning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>InstructBLIP (captioner)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>In the real-world image experiments, InstructBLIP is used (with Vicuna-13B as the caption model) to produce descriptive captions from input images; these captions are concatenated into the agent's textual history and used as observations for the LLM decision backbone (GPT-4 in the paper's real-world experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Real-world open-domain image-based embodied tasks (laboratory, office, bedroom, dormitory scenarios)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Real images present partial, ambiguous, and noisy observations; single images or sequences present limited field-of-view and require targeted questions and clarifications; challenges include open-domain object types, user preferences, and grounding language to physical objects.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>InstructBLIP (image->text captioning), Vicuna-13B used in the captioning pipeline, GPT-4 as decision backbone (these are external model tools used in the pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual image captions and grounded descriptions / suggestions that become part of the agent's observation history.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Captions produced by InstructBLIP are appended to the textual history and used by the decision LLM as the observation for subsequent reasoning, question generation, and planning.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM planning based on textualized observations (captions) and asking to humans for preferences; plans are high-level sequences of actions (multi-step) generated by GPT-4 conditioned on the captioned observations and in-context example.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>No explicit path planner; in experiments human volunteers executed the LLM-proposed plans (ablation of plan vs execution), so navigation was not performed by an automated shortest-path algorithm.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Qualitative: ABA (with InstructBLIP captions and GPT-4 backbone) successfully completed all three tested real-world scenarios in zero-shot condition (Figure 4 qualitative trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A captioning frontend enables ABA to ground open-domain questions in visual context; feeding captions into the LLM suffices for generating grounded questions and feasible high-level plans, but the actual execution was human-mediated in these experiments.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e895.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e895.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ReAct (reasoning + acting LLM baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM baseline that interleaves chain-of-thought style reasoning ('think') with environment actions but does not query an external human/information model in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>React: Synergizing reasoning and acting in language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>ReAct is an LLM prompting method that augments outputs with interleaved reasoning tokens ('think: ...') and actions; in this paper it is used as a baseline with the same LLM backbones (Vicuna, GPT-3.5, GPT-4) and in-context examples to produce actions without asking external sources. The policy is purely generative conditioned on the textual history and examples.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ALFWorld and robot-arm manipulation tasks (same benchmarks used for comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially-observable text-based scenarios where the agent receives local textual observations and must search and act to complete tasks; shown to be challenging for pure LLM planning without external information.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>none in these experiments (ReAct does not query the Human Model H or other external tools in the paper's comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Textual history appended to the prompt (tau_t) and chain-of-thought reasoning; no explicit belief tracker or external-answer incorporation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Belief is implicitly represented by the concatenated text of past observations/actions; the LLM conditions on this for subsequent token generation; no separate update rule beyond appending new environment observations.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>LLM-based generation using chain-of-thought prompting; behaves like exhaustive search in many long-horizon cases rather than targeted information gathering.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>LLM-generated stepwise location checks and 'go to' commands; no explicit path planner or graph search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td>ReAct (Vicuna-7B) All-tasks ID mean ≈ 7% ± 3, OOD ≈ 6% ± 1; ReAct (GPT-3.5) All-tasks ID ≈ 64% ± 1, OOD ≈ 65% ± 2 (Table 1). Robot-arm tasks: ReAct success rates depend on task parameters (e.g., Task1: 55–70%), substantially lower than ABA in many settings (Figure 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>ReAct, which does not ask external information, often resorts to exhaustive or inefficient search in partially-observable tasks, demonstrates hallucinations and looping behavior in ALFWorld episodes, and performs worse than ABA which actively queries for missing context.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>React: Synergizing reasoning and acting in language models <em>(Rating: 2)</em></li>
                <li>ALFWorld: Aligning Text and Embodied Environments for Interactive Learning <em>(Rating: 2)</em></li>
                <li>Asking for knowledge (AFK): Training rl agents to query external knowledge using language <em>(Rating: 2)</em></li>
                <li>A framework for learning to request rich and contextually useful information from humans <em>(Rating: 1)</em></li>
                <li>Robots that ask for help: Uncertainty alignment for large language model planners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-895",
    "paper_id": "paper-d7a74d22139b8ff806f9dbcb5242b216eeac50c7",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "ABA",
            "name_full": "Asking Before Acting",
            "brief_description": "An LLM-based embodied decision-making agent that actively issues natural-language queries to external information sources during interaction to reduce exploration and improve task efficiency in partially-observable environments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Asking Before Acting (ABA)",
            "agent_description": "ABA is implemented on top of pretrained (visual) language models (examples: Vicuna, GPT-3.5, GPT-4). Key components: (1) an augmented action/state formalism where asking and answering are actions/parts of state (A^U, S^U); (2) in-context learning (ICL) with chain-of-thought style \"think\" tokens so the LLM generates analysis, optional metadata tokens (\\u0305a_t) that encode rationale for asking, and then the question tokens; (3) a decision rule that selects the highest-probability token sequence from the LLM conditioned on K in-context example trajectories; (4) for vision inputs, a captioning frontend (InstructBLIP) produces text fed into the LLM. ABA interleaves \"think/ask/act\" steps: it identifies unknown but task-relevant context via metadata, formulates open-ended natural-language questions, sends them to an external information source (the Human Model H), receives textual answers, appends them to its history, and continues planning and acting.",
            "environment_name": "ALFWorld (text-based household), Robot-arm manipulation (numerical control), and real-world image-based embodied tasks",
            "environment_description": "ALFWorld: a text-based embodied household environment where the agent receives local textual observations (rooms, visible containers/objects) and must complete tasks such as pick/place; observations are partial (agent cannot see the full state). Robot-arm tasks require mapping language answers to numerical coordinates. Real-world image tasks use image captions as observations; all settings are partially observable and require information gathering (where objects are, disambiguation of underspecified instructions).",
            "is_partially_observable": true,
            "external_tools_used": "Human Model H (an LLM, implemented with Vicuna-7B in evaluation) that answers free-form questions given a descriptive paragraph extracted from simulator; InstructBLIP (visual captioner) to convert images into text; simulator/PDDL data extraction used to produce ground-truth descriptive paragraphs and to collect training metadata for ABA-FT.",
            "tool_output_types": "Textual outputs: (a) descriptive paragraphs of object placements (structured natural-language 'A is in B' lines), (b) free-form answers to open-domain questions (object locations, relative descriptions, user preferences), (c) image captions (text) from InstructBLIP, and (d) QA labels for fine-tuning datasets.",
            "belief_state_mechanism": "A purely textual, history-based belief mechanism: the agent maintains tau_t = concat(instruction, s1, a1, ..., st) where answers and question-metadata are appended into the same text stream; the augmented state S^U explicitly includes answer tokens (L_ans). There is no explicit probabilistic belief tracker; memory is provided by the LLM context and, for ABA-FT, by learned weights from finetuning.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "When the agent asks a question, the Human Model's textual answer is appended to the trajectory/history (tau_{t+1}). Subsequent LLM generations condition on this extended history; therefore tool outputs are implicitly incorporated into the agent's belief via token-context. ABA-FT additionally shapes the model parameters via QA and policy finetuning so the model better internalizes when to ask and how to remember/act on answers across longer horizons.",
            "planning_approach": "LLM-based generation with In-Context Learning and Chain-of-Thought style prompting to produce rationale ('think' steps), question formulation, and high-level action sequences; ABA-FT augments this with supervised finetuning on QA pairs (D_QA) and action trajectories (D_pi) to learn the asking rationale and recover from noisy actions.",
            "uses_shortest_path_planning": false,
            "navigation_method": "High-level symbolic navigation commands generated by the LLM such as 'go to &lt;location&gt;'; no explicit graph-based or shortest-path planner (A*, Dijkstra) is used—the LLM outputs sequenced 'move' actions and the environment/simulator executes them.",
            "performance_with_tools": "ALFWorld: 20cmABA/GPT-3.5 (G35) All-tasks in-distribution success 84% ± 3 (Table 1); out-of-distribution All 85% ± 4. With Vicuna-7B backbone: 20cmABA/V7B All ID 56% ± 3, OOD 48% ± 2. Robot-arm tasks: ABA averaged 85–100% depending on task parameters (see Figure 3) — e.g., Task1 (x=3..5) ABA success 85–90% vs baseline lower. Real-world image tasks: qualitative—ABA completed all three tested scenarios (zero-shot) using image caption inputs and human execution of plans.",
            "performance_without_tools": "Baselines without querying external human-model: ReAct (GPT-3.5) All ID 64% ± 1, OOD 65% ± 2; ReAct (Vicuna-7B) All ID 7% ± ~3, OOD 6% ± ~1. BUTLER (imitation baseline with 1e5 trajectories) reported variable results (see Table 1) but ABA with asking outperformed BUTLER on most settings using far fewer examples.",
            "has_tool_ablation": true,
            "key_findings": "Actively querying an external natural-language information source dramatically improves both success rate and efficiency in partially-observable text-based embodied tasks: ABA reduces physical action counts and episode lengths (often &gt;50% fewer actions) while increasing success; textual history as an implicit belief suffices for incorporating tool outputs when combined with LLM reasoning; finetuning (ABA-FT) further improves long-horizon / multi-round performance by internalizing asking rationale and memory.",
            "uuid": "e895.0"
        },
        {
            "name_short": "ABA-FT",
            "name_full": "Asking Before Acting with Fine-Tuning",
            "brief_description": "A finetuned variant of ABA that converts question-rationale metadata into QA training pairs and finetunes the LLM on both QA and policy trajectories to improve asking decisions and long-horizon behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ABA-FT",
            "agent_description": "Starts from the ABA architecture and further finetunes the LLM on two datasets: (1) D_QA where metadata and history are reformulated as QA pairs indicating whether to ask and what to ask, and (2) D_pi where action sequences and metadata are supervised, with noisy actions masked out. Training objective maximizes log-likelihood on QA and masked policy targets. This yields better scalability to long trajectories and multi-round tasks by encoding asking rationale into model parameters.",
            "environment_name": "Ambiguous ALFWorld and Multiround ALFWorld (variants of ALFWorld), plus other embodied tasks",
            "environment_description": "Harder variants of ALFWorld where tasks are intentionally underspecified (Ambiguous) or involve multiple rounds in the same room (Multiround), testing memory and the ability to avoid redundant questions; environments remain partially observable and require retaining previously obtained information.",
            "is_partially_observable": true,
            "external_tools_used": "Same external information sources as ABA: Human Model H (LLM) answering descriptive paragraphs; simulator/PDDL used to create training metadata and labels.",
            "tool_output_types": "Textual QA labels and free-form answers produced by the Human Model; structured descriptive paragraphs derived from simulator; these are used both at finetune-time (as supervised signals) and at test-time (as answers to questions).",
            "belief_state_mechanism": "Like ABA, uses textual history tau_t; additionally learns to encode asking-rationale and memory into model weights through supervised finetuning so longer-horizon dependencies persist despite token-limits during inference.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Tool outputs are appended to history; during finetuning, the model learns mappings from history+QA to decisions, improving its ability to infer when a question is necessary and to recall previously-observed information across rounds even when in-context example diversity is limited.",
            "planning_approach": "LLM generation augmented by supervised finetuning (imitation-style) on QA and policy datasets to produce more stable long-horizon plans and asking behavior; retains chain-of-thought style 'think' steps in generation.",
            "uses_shortest_path_planning": false,
            "navigation_method": "High-level 'go to &lt;location&gt;' actions produced by the LLM; no explicit path planner is used. For robot-control tasks, generates numerical coordinates derived from textual answers.",
            "performance_with_tools": "ABA-FT yields additional improvements over ABA on long-horizon and multi-round tasks; paper reports ABA-FT &gt; ABA especially in Multiround ALFWorld (exact numeric gains not tabulated in the main table but noted qualitatively; finetuning used e.g. 1500 and 500 trajectories for different variants).",
            "performance_without_tools": null,
            "has_tool_ablation": true,
            "key_findings": "Finetuning on QA and policy pairs teaches the LLM the rationale for asking and improves robustness and memory across longer tasks; ABA-FT reduces redundant questioning in multiround settings and boosts performance where simple ICL suffers due to token limits and dataset diversity constraints.",
            "uuid": "e895.1"
        },
        {
            "name_short": "Human Model H",
            "name_full": "Human / External Information Source Model (H)",
            "brief_description": "A simulated human/external information source implemented as an LLM (Vicuna-7B) that is instructed to answer agent questions using a descriptive paragraph derived from the simulator's ground-truth object placements.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Human Model (H) - LLM-based answerer",
            "agent_description": "H is implemented as another pretrained LLM (Vicuna-7B in experiments). Prior to evaluation the simulator's ground-truth object placement list is converted by rules into a natural-language descriptive paragraph ('A is in B' sentences). H is prompted (with few-shot examples) to read that paragraph and answer open-domain questions posed by the agent in free-form natural language, e.g., 'Where can I find the dish sponge?' and returns textual lists or relative descriptions.",
            "environment_name": "ALFWorld (used to generate the descriptive paragraph), also used in other embodied evaluations",
            "environment_description": "The underlying environment (ALFWorld) is partially observable for the agent; the Human Model is given a derived (simulator-to-text) representation that contains full placement info, so H effectively has privileged information relative to the agent.",
            "is_partially_observable": true,
            "external_tools_used": "Internally uses the descriptive paragraph extracted from the simulator (a structured textual map) and few-shot prompting; the 'tool' is an LLM answering queries.",
            "tool_output_types": "Textual answers listing object locations (e.g., 'dishsponge 1 is in garbagecan 1, dishsponge 2 is in drawer 3, ...'), relative descriptors (e.g., 'second red block from the left'), and preference clarifications.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": null,
            "belief_update_description": null,
            "planning_approach": "H does not plan; it returns answers based on the provided paragraph and prompt instructions.",
            "uses_shortest_path_planning": null,
            "navigation_method": null,
            "performance_with_tools": "The Human Model (Vicuna-7B) accuracy on a held-out ALFWorld QA test (8 scenarios × 5 questions each) was reported as 72.5% ± 9.7% (Section C).",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Using an LLM as a drop-in simulated human enables flexible, open-domain answers that ABA leverages to avoid exhaustive search; ABA is robust to some errors in H (e.g., small inaccuracies in reported object identifiers), but H's imperfect accuracy (≈72.5%) indicates that real human/tool quality affects agent performance.",
            "uuid": "e895.2"
        },
        {
            "name_short": "InstructBLIP",
            "name_full": "InstructBLIP (vision-language captioning model)",
            "brief_description": "A vision-to-text frontend used to create image captions that are fed into the decision-making LLM; enables ABA to operate on real-world image observations by converting pixels to textual descriptions.",
            "citation_title": "Visual instruction tuning",
            "mention_or_use": "use",
            "agent_name": "InstructBLIP (captioner)",
            "agent_description": "In the real-world image experiments, InstructBLIP is used (with Vicuna-13B as the caption model) to produce descriptive captions from input images; these captions are concatenated into the agent's textual history and used as observations for the LLM decision backbone (GPT-4 in the paper's real-world experiments).",
            "environment_name": "Real-world open-domain image-based embodied tasks (laboratory, office, bedroom, dormitory scenarios)",
            "environment_description": "Real images present partial, ambiguous, and noisy observations; single images or sequences present limited field-of-view and require targeted questions and clarifications; challenges include open-domain object types, user preferences, and grounding language to physical objects.",
            "is_partially_observable": true,
            "external_tools_used": "InstructBLIP (image-&gt;text captioning), Vicuna-13B used in the captioning pipeline, GPT-4 as decision backbone (these are external model tools used in the pipeline).",
            "tool_output_types": "Textual image captions and grounded descriptions / suggestions that become part of the agent's observation history.",
            "belief_state_mechanism": null,
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "Captions produced by InstructBLIP are appended to the textual history and used by the decision LLM as the observation for subsequent reasoning, question generation, and planning.",
            "planning_approach": "LLM planning based on textualized observations (captions) and asking to humans for preferences; plans are high-level sequences of actions (multi-step) generated by GPT-4 conditioned on the captioned observations and in-context example.",
            "uses_shortest_path_planning": false,
            "navigation_method": "No explicit path planner; in experiments human volunteers executed the LLM-proposed plans (ablation of plan vs execution), so navigation was not performed by an automated shortest-path algorithm.",
            "performance_with_tools": "Qualitative: ABA (with InstructBLIP captions and GPT-4 backbone) successfully completed all three tested real-world scenarios in zero-shot condition (Figure 4 qualitative trajectories).",
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "A captioning frontend enables ABA to ground open-domain questions in visual context; feeding captions into the LLM suffices for generating grounded questions and feasible high-level plans, but the actual execution was human-mediated in these experiments.",
            "uuid": "e895.3"
        },
        {
            "name_short": "ReAct",
            "name_full": "ReAct (reasoning + acting LLM baseline)",
            "brief_description": "An LLM baseline that interleaves chain-of-thought style reasoning ('think') with environment actions but does not query an external human/information model in this paper's experiments.",
            "citation_title": "React: Synergizing reasoning and acting in language models",
            "mention_or_use": "use",
            "agent_name": "ReAct",
            "agent_description": "ReAct is an LLM prompting method that augments outputs with interleaved reasoning tokens ('think: ...') and actions; in this paper it is used as a baseline with the same LLM backbones (Vicuna, GPT-3.5, GPT-4) and in-context examples to produce actions without asking external sources. The policy is purely generative conditioned on the textual history and examples.",
            "environment_name": "ALFWorld and robot-arm manipulation tasks (same benchmarks used for comparison)",
            "environment_description": "Partially-observable text-based scenarios where the agent receives local textual observations and must search and act to complete tasks; shown to be challenging for pure LLM planning without external information.",
            "is_partially_observable": true,
            "external_tools_used": "none in these experiments (ReAct does not query the Human Model H or other external tools in the paper's comparisons).",
            "tool_output_types": null,
            "belief_state_mechanism": "Textual history appended to the prompt (tau_t) and chain-of-thought reasoning; no explicit belief tracker or external-answer incorporation.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Belief is implicitly represented by the concatenated text of past observations/actions; the LLM conditions on this for subsequent token generation; no separate update rule beyond appending new environment observations.",
            "planning_approach": "LLM-based generation using chain-of-thought prompting; behaves like exhaustive search in many long-horizon cases rather than targeted information gathering.",
            "uses_shortest_path_planning": false,
            "navigation_method": "LLM-generated stepwise location checks and 'go to' commands; no explicit path planner or graph search.",
            "performance_with_tools": null,
            "performance_without_tools": "ReAct (Vicuna-7B) All-tasks ID mean ≈ 7% ± 3, OOD ≈ 6% ± 1; ReAct (GPT-3.5) All-tasks ID ≈ 64% ± 1, OOD ≈ 65% ± 2 (Table 1). Robot-arm tasks: ReAct success rates depend on task parameters (e.g., Task1: 55–70%), substantially lower than ABA in many settings (Figure 3).",
            "has_tool_ablation": null,
            "key_findings": "ReAct, which does not ask external information, often resorts to exhaustive or inefficient search in partially-observable tasks, demonstrates hallucinations and looping behavior in ALFWorld episodes, and performs worse than ABA which actively queries for missing context.",
            "uuid": "e895.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "React: Synergizing reasoning and acting in language models",
            "rating": 2
        },
        {
            "paper_title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
            "rating": 2
        },
        {
            "paper_title": "Asking for knowledge (AFK): Training rl agents to query external knowledge using language",
            "rating": 2
        },
        {
            "paper_title": "A framework for learning to request rich and contextually useful information from humans",
            "rating": 1
        },
        {
            "paper_title": "Robots that ask for help: Uncertainty alignment for large language model planners",
            "rating": 1
        }
    ],
    "cost": 0.022911249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Asking Before Acting: Gather Information in Embodied Decision-Making with Language MODELS</h1>
<p>Xiaoyu Chen *<br>Tsinghua University<br>Shenao Zhang<br>Northwestern University<br>Pushi Zhang<br>Microsoft Research Asia<br>Li Zhao<br>Microsoft Research Asia<br>Jianyu Chen<br>Tsinghua University<br>Shanghai Qizhi Institute</p>
<h4>Abstract</h4>
<p>With strong capabilities of reasoning and a broad understanding of the world, Large Language Models (LLMs) have demonstrated immense potential in building versatile embodied decision-making agents capable of executing a wide array of tasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM agents encounter challenges in efficiently gathering essential information, leading to suboptimal performance. Conversely, human individuals often seek additional information from their peers prior to taking action, harnessing external knowledge to avoid unnecessary trial and error. Drawing inspiration from this behavior, we propose Asking Before Acting (ABA), a method that empowers the agent to proactively inquire with external sources for pertinent information using natural language during their interactions within the environment. In this way, the agent is able to enhance its efficiency and performance by circumventing potentially laborious steps and combating the difficulties associated with exploration in unfamiliar environments and vagueness of the instructions. We conduct extensive experiments involving a spectrum of environments including text-based household everyday tasks, robot arm manipulation tasks, and real world open domain image based embodied tasks. The experiments involve various models from Vicuna to GPT-4. The results demonstrate that, even with modest prompts modifications, ABA exhibits substantial advantages on both performance and efficiency over baseline LLM agents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates learning the rationale for asking and allows for additional enhancements especially in tasks that baselines struggle to solve.</p>
<h2>1 INTRODUCTION</h2>
<p>Recent advances in Large Language Models (LLMs) have exhibited remarkable abilities in language comprehension, text generation, question answering, dialogue, reasoning, and can even exhibit human-level performance on various benchmarks (Ouyang et al., 2022; Chowdhery et al., 2022; OpenAI, 2023; Google, 2023). Since LLMs have been trained on extensive and diverse text corpora, they have captured a broad range of commonsense understanding about the world, enabling them to adeptly handle a multitude of various and complex scenarios. Therefore, recently, researchers have proposed to integrate LLMs in embodied decision-making (Huang et al., 2022a; Li et al., 2022; Ahn et al., 2022; Huang et al., 2022b; Singh et al., 2022a; Yao et al., 2022; Wang et al., 2023; Driess et al., 2023; Carta et al., 2023), using LLMs either to do task planning or as autonomous agent.</p>
<p>LLM agents exhibit strong decision-making capabilities when presented with comprehensive information, yet they may encounter difficulties when confronted with scenarios characterized by limited or insufficient information, such as with underspecified instructions, or in unfamiliar environments. In such cases, the agents may struggle to make informed decisions. To illustrate this, consider the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: ABA (as shown in (b)) allows the agent to seek clarification about the task as well as efficiently gather necessary information. In contrast, acting without asking (as shown in (c)) may lead to task misunderstanding or laborious exploration period. Note (a) and (b) are key frames of trajectories generated by ABA in experiments in Section 4.3.</p>
<p>Scenario depicted in Figure 1, where a robot is deployed in an office and tasked with packing items. As shown in Figure 1 (c), act without asking may lead to suboptimal behaviors. The vagueness present in the instruction leaves the true reward function underspecified, and an arbitrary mapping from instruction to actions might not yield desirable outcomes. Furthermore, when the robot lacks specific prior knowledge like an object's location, directly taking actions may result in a systematic search strategy. Even though the robot may finally manage to locate the target object, this whole process is conspicuously inefficient, let alone the possibility of suboptimal searching behavior which may lead to failure.</p>
<p>In contrast, when confronted with such scenarios, we humans typically adopt a different approach. When assigned with ambiguous tasks, it is more prudent to ask for clarification about the task before taking actions, ensuring we have the tasks correctly understood. Besides, rather than resorting to onerous trial and error, it is natural for us to actively query external information from our peers to accelerate information gathering and guide decision making. As shown in Figure 1 (a) and (b), we excerpt key frames and logs produced by our method (refer to Section 4.3 for details). Our policy first seeks clarification about the task by inquiring about the human's preferences. Then, instead of searching every hole and corner, the agent opts to ask "Can you tell me where the notebook is?" and directly proceeds to that location. The comparison builds us an intuition that, by posing a few questions in natural language, the agent can enjoy an substantial enhancement in both policy performance and efficiency.</p>
<p>Building upon this intuition, we focus on a novel setting where the agent can actively query for additional information from external sources using natural language during their interactions with environments. Though some existing works have explored scenarios involving human-in-the-loop interactions to provide additional information, our setting is stands apart from these previous ones. A majority of works (Nguyen &amp; Daumé III, 2019; Nguyen et al., 2019; Singh et al., 2022b; Da Silva et al., 2020) ask humans for oracle actions or action descriptions, Nguyen et al. (2022) ask for information about current states and (sub-)goals. Liu et al. (2022) asks three-word-templated questions to accelerate training, while Huang et al. (2022b) ask for scene, task, or preferences descriptions. In contrast to existing works, our setting concentrates on designing a generic mechanism to gather information through natural language, which imposes fewer restrictions and aligns more closely with human decision-making processes.</p>
<p>In this paper, we begin by introducing a novel theoretical framework termed Contextual MDP with Human / External Information Sources in the Loop in Section 3.1, which integrates the information querying into decision making process by incorporating human in the formulation. In Section 3.2, we propose Asking Before Acting (ABA), an efficient method that is able to accelerate information gathering by actively querying in natural language while interacting with the environments. ABA can learn to ask proper open-domain questions and effectively leverage the gained information only with modest modifications on prompts and use pretrained (visual) language models out of the box. To further improve the performance, we introduce ABA-FineTuning (ABA-FT). We propose to reformulate the metadata associated with the question formulation as question-answering pro</p>
<p>cess, which aids the model in understanding the rationale behind posing questions. Besides, we also introduce a simple yet effective evluation method in Section 3.3.</p>
<p>To evaluate the capability of ABA and ABA-FT, we conduct extensive experiments in Section 4, involving a spectrum of environments including text-based household everyday tasks, robot arm manipulation tasks, and real world open domain image based embodied tasks. We also evaluate various models encompassing Vicuna (Chiang et al., 2023), GPT-3.5 and GPT-4 to test the scalbility of our method. The quantitative results demonstrate that, even with modest modifications on prompts, ABA obtains consistent performance gains across almost all environments against baseline LLM agents and classical imitation learning method. Further finetuning with a fraction of data enables ABA-FT to achieve additional enhancements, especially in tasks that baselines struggle to solve. To better understand the behavior and practical value of ABA, we conduct experiments on real world open domains embodied tasks with image inputs in Section 4.3. The qualitative results provide additional evidence that, when combined with a versatile controller, ABA is capable of formulating open-domain questions grounded in the context of image observation to gather information. This showcases the practical value of ABA and augurs a promising direction for future research.</p>
<h1>2 PRELIMINARIES</h1>
<p>To better formulate the information gathering procedure, we first introduce Contextual Markov Decision Processes (Contextual MDPs) (Hallak et al., 2015):</p>
<p>Definition 2.1 Contextual MDP is a tuple $(\mathcal{S}, \mathcal{A}, \mathcal{C}, \mathcal{M}(c))$. Here $\mathcal{S}$ and $\mathcal{A}$ stand for state space and action space respectively. $\mathcal{C}$ is the context space. $\mathcal{M}$ is a function mapping context $c \in \mathcal{C}$ to a specific $T$-horizon MDP $\mathcal{M}(c)=(\mathcal{S}, \mathcal{A}, p(\cdot \mid s, a, c), r(s, a, c))$.</p>
<p>Here, ${\mathcal{M}(c), \forall c \in \mathcal{C}}$ represents a family of MDPs characterized by a shared state space $\mathcal{S}$ and action space $\mathcal{A}$, but with different transition function $p(\cdot \mid s, a, c)$ and reward function $r(s, a, c)$ specified by $c$. The goal of the agent is to learn a policy $\pi$ to maximize the accumulative rewards on the target environment(s). Note that the context $c$ varies across different environments, and oftentimes, it remains unknown. Optionally, the agent will be additionally provided with a task instruction $i$, which is usually a concise language description of the goal, providing extra information about the context $c$. As shown in Defenition 2.1, when deployed to a new environment, understanding the context $c$ becomes a prerequisite for achieving high rewards since it indicating the transition and reward functions. In light of this, one common approach is to gather information about the context $c$ through interactions with the environment by trial and error, i.e., infer the context from history $\hat{c}=\int_{\theta}\left(i, s_{1}, a_{1}, r_{1}, \cdots, s_{t}\right)$ while trying to solve the task. However, efficiently gathering information in various unknown environments with different contexts $c$ can be challenging. Aside from limited generalization capability Beck et al. (2023), existing methods often rely on dense rewards and sufficiently small state space (Zintgraf et al., 2021), which may lead to catastrophic failure in embodied decision-making where the environments often lack carefully crafted dense reward functions and the state spaces are often large.</p>
<p>We argue that this is not, at least always, the case how humans deal with the task. Instead of attempting to figure out $c$ solely on our own, we typically seek clarification of tasks or assistance from another human, often a more experienced senior, to obtain valuable information. This behavior can significantly alleviate the exploration burden for $c$ in many situations. The above insight urges us to reconsider the process of embodied decision-making in unfamiliar evaluation environments: What if the agent does not necessarily need to figure out everything on its own? While certain prior research has studied scenarios involving human-in-the-loop interactions, our setting concentrates on the development of a generic mechanism with fewer restrictions and better alignments with human behavior (see Section 5 for a detailed survey) and we are the pioneers in addressing the challenge of enabling information gathering for embodied decision-making with LLM agents.</p>
<h2>3 Method</h2>
<p>In this section, we present our new problem formulation as well as the our algorithm.</p>
<h1>3.1 Contextual MDP with Human / External Information Source in the Loop</h1>
<p>To incorporate humans (or other external knowledge sources) in the loop of decision-making, we opt to integrate the physical interactions with environment and language interactions with humans as a unified interface:</p>
<p>Definition 3.1 Contextual MDP with Human / External information source in the loop based on $\left(\mathcal{S}^{U}, \mathcal{A}^{U}, \mathcal{C}, \mathcal{H}(c), \mathcal{M}(c)\right)$. Here $\mathcal{S}^{U}, \mathcal{A}^{U}$ are the augmented state space and action space: $\mathcal{S}^{U}=\mathcal{S} \cup \mathcal{L}<em _ask="{ask" _text="\text">{\text {ans }}, \mathcal{A}^{U}=\mathcal{A} \cup \mathcal{L}</em>}}$, where $\mathcal{L<em _ans="{ans" _text="\text">{\text {ask }}$ and $\mathcal{L}</em>}}$ include all possible questions and answers in natural language. $\mathcal{H}(c)$ maps context $c \in \mathcal{C}$ to $\mathcal{H<em c="c">{c}$, which is a model of human (or other external information source) in context $c$ that can map any questions to information. $\mathcal{M}(c)=\left(\mathcal{S}^{U}, \mathcal{A}^{U}, \mathcal{H}</em>}, p_{U}(\cdot \mid s, a, c, \mathcal{H<em c="c">{c}\right), r(s, a, c), \gamma)$
Like Contextual MDP, $\mathcal{M}(c)=\left(\mathcal{S}^{U}, \mathcal{A}^{U}, \mathcal{H}</em>}, p_{U}(\cdot \mid s, a, c, \mathcal{H<em U="U">{c}\right), r(s, a, c), \gamma)$ has a transition function and a reward function parameterized by $c$. However, the state space $\mathcal{S}^{U}$ and the action space $\mathcal{A}^{U}$ are augmented with answers and questions respectively, and the transition function $p</em>)$ can be factorized as:}(\cdot \mid s, a, c, \mathcal{H}_{c</p>
<p>$$
p_{U}\left(s^{\prime} \mid s, a, c, \mathcal{H}<em _in="\in" _mathcal_A="\mathcal{A" a="a">{c}\right)=p\left(s^{\prime} \mid s, a, c\right) \cdot \mathbb{1}</em>}}+p\left(\mathcal{H<em _in="\in" _mathcal_L="\mathcal{L" a="a">{c}(a)=s^{\prime}\right) \cdot \mathbb{1}</em>
$$}_{a s k}</p>
<p>With the augmented action space, the agent can now query to gather information while interacting with the environments. For instance, by simply asking "where is the kitchen?", the agent can omit tens of steps of exploration to find the kitchen. However, several challenges exist: First, when deployed to unfamiliar environments, it is important for the agent to identify the key information that is pertinent while filtering out the task-irrelevant ones. Sencondly, to better communicate with humans, the agent should do reasoning to map between abstract natural language and specific object in observation. This requires both grounded question proposing as well as grounded answer understanding. Furthermore, it is also desirable to ask only when necessary, i.e., when it cannot reason the answers from historical information. To solve these challenges, we propose Asking Before Acting (ABA), an effective method for the language agent to cleverly gather necessary information.</p>
<h3>3.2 Algorithm</h3>
<p>In this paper, we build our model based on pretrained (visual) language models, and we focus on the setting where the task instruction $i$ is provided. We define $\tau_{t}=\operatorname{concat}\left(i, s_{1}, a_{1}, \cdots, s_{t}\right)$, which is the concatenation of the instruction and the historical observations and actions. We describe our methods in the following sections:</p>
<h3>3.2.1 Asking Before Acting</h3>
<p>In-Context Learning (ICL) (Brown et al., 2020) can approximate a function approximator $\hat{f}$ from sequences of pairs $\left(x_{i}, f\left(x_{i}\right)\right)<em 1="1">{i}$ and allow to predictions $\hat{f}\left(x^{\prime}\right)$ on novel $x^{\prime}$. Recently, ICL draws a lot of attention (Xie et al., 2021; Akyürek et al., 2022; Dai et al., 2022) due to its superior efficiency and the ability of generalization. Recent works on embodied planning (Huang et al., 2022a; Singh et al., 2022a) propose use ICL to learn to output actions to interact with the environment given $K$ trajectories $\tau^{k}=\operatorname{concat}\left(i^{k}, s</em>$ to incorporate a "think" step.
Based on Yao et al. (2022); Ahn et al. (2022), we propose to learn the rationale for question asking via ICL. Suppose there is a question at time $t$. Rather than directly mapping $\tau_{t}$ to the questions $a_{t}$, we identify the corresponding context $c_{t}$ the question $a_{t}$ is querying for ( $c_{t}$ may correspond to a partial of $c$ ). We term the identification procedure, i.e., why we posing such questions, as the metadata of the questions. For instance, let $a_{t}$ be a question asking "where is the notebook". Instead of directly learning to ask, we insert the metadata of $a_{t}$, which we denote as $\bar{a}}^{k}, a_{1}^{k}, \cdots, s_{T}^{k}, a_{T}^{k}\right)$, where $k \in{1,2, \cdots, K}$. However, directly applying ICL may result in limited generalization and poor performance (Yao et al., 2022). Yao et al. (2022); Ahn et al. (2022) propose to model the reasoning steps of actions via the Chain-of-Though prompting (Wei et al., 2022) by explicitly expanding $\tau^{k<em t="t">{t}$ : "to finish the task, we need to find the notebook. But I do not know its location, so I need to ask for it". Since $\bar{a}</em>}$ and $a_{t}$ are all manually labelled, it reveals the rationale for asking $a_{t}$ of the labeller. By providing $K$ such trajectories, we learn through ICL to map from $\tau_{t}$ to human-like analysis of what context is desirable yet unknown right now (if so), and from the analysis to human-like questions. To this end, ABA aligns the agent with human asking habits. Denote $\bar{\tau}^{k}$ as the augmented trajectories with $\bar{a<em t="t">{t}$ prepend before each question $a</em>$ augment to}$, and $\bar{\mathcal{A}}$ to be the augmented action space with $\mathcal{L}_{a s k</p>
<p>concatenation of $\left[\mathcal{L}<em _ask="{ask" _text="\text">{\text {meta }}, \mathcal{L}</em>\right]$, the action is selcted by:}</p>
<p>$$
\tilde{a}<em i="0">{t}=\underset{\tilde{a} \in \overline{\mathcal{A}}}{\arg \max } \prod</em>\right)
$$}^{|a|} \pi_{L L M}\left(e_{i} \mid \bar{\tau}^{1}, \cdots, \bar{\tau}^{K}, \tau_{t}, e_{1: i-1</p>
<p>where $e_{i}$ is the $i$-th token of the action, and $|a|$ refers to the number of tokens. Note $\bar{a}<em t="t">{t}$ is null when $a</em>}$ is not a question. We can derive final action $a_{t}$ by dropping the potential $\bar{a<em t="t">{t}$ in $\tilde{a}</em>$ if needed.</p>
<h1>3.2.2 Asking Before Acting with Finetuning</h1>
<p>In experiments, we find that ICL excels at scenarios when the the task time horizon is relatively short. As the task horizon increases, due to the token limitations of (visual) language models, sometimes we can only use 1 example which results in limited diversity and thus hampers the performance of ICL. However, longer horizon usually corresponds to more complex policy which exacerbates the need for samples. Therefore, a more proper treatment is needed especially when the horizon is long.</p>
<p>To this end, we propose ABA with finetuning (ABA-FT). We first describe how we collect two datasets $\mathcal{D}<em _pi="\pi">{Q A}$ and $\mathcal{D}</em>}$. We label $N$ trajectories with expert policy where each trajectory consists of input-output pairs for $T_{i}$ timesteps as $\left{\left(\bar{\tau<em t="t">{t}^{i}, \bar{a}</em>\right)}^{i}, a_{t}^{i}, n_{t}^{i<em i="i">{t=0}^{T</em>\right}}<em t="t">{i=0}^{N}$. To alleviate the distribution shift problem (Ross et al., 2011), we intentionally corrupt the expert policy by randomly injecting noisy actions with probability $p$ and mark this as noise by setting $n</em>=1$ in the dataset. Then, we can reformulate these trajectories as two datasets.}^{i</p>
<p>For $\mathcal{D}<em t="t">{Q A}$, we convert the dataset to the form of question-answering for each $\bar{a}</em>}$. For instance, when looking for a notebook, we provide the agent with $\bar{\tau<em _pi="\pi">{t}$ and ask "do you have ever seen the notebook" and label the answer to "yes" nor "no" to indicate whether it is necessary to ask. We also include the question "where have you seen the notebook" and its corresponding answer if previous answer is yes. In this way, we hope the agent can learn to remember the key factors of inferring the rationale for asking. As for the policy dataset $\mathcal{D}</em>}$, we do a modified version of imitation learning by masking out all the corrupted actions, since we want the agent to learn to recover from mistakes and we do not want it to learn to take wrong actions intentionally. For notation simplicity, we denote $\mathcal{D<em A="A" Q="Q">{Q A}=\left{\left(x</em>\right)\right}}^{i}, y_{Q A}^{i<em _pi="\pi">{i=0}^{N</em>}}$, where $x_{Q A}^{i}$ corresponds to the inputs as well as questions, and $y_{Q A}^{i}$ corresponds to the answer. We also denote $\mathcal{D<em _pi="\pi">{\pi}=\left{\left(x</em>\right)\right}}^{i}, y_{\pi}^{i}, n_{i<em _pi="\pi">{i=0}^{N</em>}}$ where $x_{\pi}^{i}=\tau_{t}^{j}$ and $y_{\pi}^{i}=\left[\bar{a<em t="t">{t}^{j}, a</em>\right]$ for $j \in{0, \cdots, N}$. To conclude, ABA-FT is trained to maximize the following objectives:}^{j</p>
<p>$$
\mathcal{L}=-\sum_{i=0}^{N_{Q A}} \log \pi_{L L M}\left(y_{Q A}^{i} \mid x_{Q A}^{i}\right)-\sum_{i=0}^{N_{\pi}} \log \pi_{L L M}\left(y_{\pi}^{i} \mid x_{\pi}^{i}\right) \cdot \mathbb{1}<em i="i">{n</em>
$$}=0</p>
<h3>3.3 Human Model</h3>
<p>To minimize human involvement, during the evaluation phase, we propose a simple yet effective method for open-domain human in the loop problems. We implement the model of human (or other information sources) $\mathcal{H}$ via another language model which is instructed to respond to questions based on the provided information. To incorporate prior knowledge about the environment, we extract the information about the object placement from the simulator and transform it into a descriptive paragraph. This paragraph is then fed to $\mathcal{H}$. Whenever the agent poses a question, $\mathcal{H}$ is tasked with providing an answer based on the paragraph. It's worth noting that this design allows for the straightforward replacement of the current language model with human or alternative information sources for more appropriate answers. For more details, please refer to Appendix A.</p>
<h2>4 EXPERIMENTS</h2>
<p>In this section, we empirically evaluate ABA on a series of embodied decision-making tasks.
In Section 4.1, we evaluate our algorithm in three household embodied decision-making scenarios, namely ALFWorld (Shridhar et al., 2021) and its variants, to demonstrate its capability to handle high level embodied tasks. In Section 4.2, we extend our evaluation to control tasks to demonstrate that ABA is able to build mappings between language questiosn and low level control tasks. Finally, in Section 4.3, to further prove the effectiveness of ABA, we build 4 real world scenarios. We also replace the human model by real humans to answer the questions and communicate with ABA. The qualitative results showcase the practical value of ABA and augurs a promising direction for future research.</p>
<p>Table 1: Success rate on ALFWorld environments. ID and OOD refer to in distribution and out-of-distribution evaluation set provided in <em>Shridhar et al. (2020)</em>. V7B and G35 refers to Vicuna 7B and GPT-3.5 API. Note that for BUTLER, we report the best success rates across 8 seeds to align with the original paper, while for ReAct and ABA, we report mean success rates across 5 or 3 seeds.</p>
<table>
<thead>
<tr>
<th></th>
<th>Pick</th>
<th></th>
<th>Examine</th>
<th>Clean</th>
<th>Heat</th>
<th>Cool</th>
<th>Pick 2</th>
<th>All</th>
</tr>
</thead>
<tbody>
<tr>
<td>BUTLER</td>
<td>ID</td>
<td>61</td>
<td>39</td>
<td>44</td>
<td>81</td>
<td>60</td>
<td>29</td>
<td>40</td>
</tr>
<tr>
<td>max of 8</td>
<td>OOD</td>
<td>46</td>
<td>22</td>
<td>39</td>
<td>74</td>
<td>100</td>
<td>24</td>
<td>37</td>
</tr>
<tr>
<td>20cmReAct/V7B</td>
<td>ID</td>
<td>$9 \pm 7$</td>
<td>$8 \pm 4$</td>
<td>$9 \pm 3$</td>
<td>$4 \pm 5$</td>
<td>$9 \pm 8$</td>
<td>$4 \pm 4$</td>
<td>$7 \pm 3$</td>
</tr>
<tr>
<td>mean of 5</td>
<td>OOD</td>
<td>$3 \pm 3$</td>
<td>$6 \pm 3$</td>
<td>$5 \pm 3$</td>
<td>$10 \pm 3$</td>
<td>$2 \pm 3$</td>
<td>$9 \pm 5$</td>
<td>$6 \pm 1$</td>
</tr>
<tr>
<td>20cmABA/V7B</td>
<td>ID</td>
<td>$60 \pm 6$</td>
<td>$52 \pm 5$</td>
<td>$59 \pm 6$</td>
<td>$46 \pm 6$</td>
<td>$61 \pm 3$</td>
<td>$61 \pm 10$</td>
<td>$56 \pm 3$</td>
</tr>
<tr>
<td>mean of 5</td>
<td>OOD</td>
<td>$37 \pm 5$</td>
<td>$53 \pm 5$</td>
<td>$51 \pm 2$</td>
<td>$52 \pm 6$</td>
<td>$50 \pm 15$</td>
<td>$41 \pm 0$</td>
<td>$48 \pm 2$</td>
</tr>
<tr>
<td>20cmReAct/G35</td>
<td>ID</td>
<td>$62 \pm 4$</td>
<td>$67 \pm 1$</td>
<td>$67 \pm 3$</td>
<td>$67 \pm 2$</td>
<td>$55 \pm 1$</td>
<td>$67 \pm 1$</td>
<td>$64 \pm 1$</td>
</tr>
<tr>
<td>mean of 3</td>
<td>OOD</td>
<td>$67 \pm 13$</td>
<td>$67 \pm 2$</td>
<td>$75 \pm 10$</td>
<td>$67 \pm 7$</td>
<td>$54 \pm 8$</td>
<td>$59 \pm 0$</td>
<td>$65 \pm 2$</td>
</tr>
<tr>
<td>20cmABA/G35</td>
<td>ID</td>
<td>$86 \pm 6$</td>
<td>$79 \pm 7$</td>
<td>$79 \pm 2$</td>
<td>$73 \pm 4$</td>
<td>$81 \pm 11$</td>
<td>$98 \pm 2$</td>
<td>$84 \pm 3$</td>
</tr>
<tr>
<td>mean of 3</td>
<td>OOD</td>
<td>$83 \pm 4$</td>
<td>$78 \pm 6$</td>
<td>$92 \pm 2$</td>
<td>$81 \pm 14$</td>
<td>$85 \pm 6$</td>
<td>$84 \pm 7$</td>
<td>$85 \pm 4$</td>
</tr>
</tbody>
</table>
<h1>4.1 Text-based Household Everyday Task</h1>
<p>ALFWorld ALFWorld <em>(Shridhar et al., 2021)</em> is an interactive text-based environment with aligned embodied worlds, which contains six types of different everyday household tasks. As for baselines, we select two representative baselines, BUTLER <em>(Shridhar et al., 2021)</em> and ReAct <em>(Yao et al., 2022)</em>. BUTLER <em>(Shridhar et al., 2021)</em> is an imitation learning based method without LLM. It trains an independent model for each task, while each of the model uses a substantial dataset of $10^{5}$ expert trajectories. For ReAct and ABA, we use $K=2$ in-context examples. For more details, please refer to Appendix B.</p>
<p>The results are presented in Table 1 and 2. The results of BUTLER are directly taken from <em>Shridhar et al. (2021)</em> which reports the best performance across 8 different seeds. For ReAct and ABA, we report the performance mean and standard deviation across 5 seeds for Vicuna-7B <em>(Chiang et al., 2023)</em> and 3 seeds for GPT-3.5 <em>(Ouyang et al., 2022)</em> API. The empirical results demonstrate that ABA strongly outperforms other baselines, and also shows great efficiency in terms of parameters.</p>
<p>As shown in Tabel 1, with one unified model, ABA can successfully handle most scenarios. The mean performance of ABA with GPT-3.5 is higher than the max performance of BUTLER in 12 out of 14 scenarios, using only $K=2$ incontext examples and one model versus $10^{5}$ expert trajectories and separate training according to BUTLER. ABA also outperforms ReAct in all scenarios and better efficiency in terms of model parameters. When deployed with a relatively smaller model such as Vicuna-7B, ABA achieves more than $40 \%$ performance gain compared to ReAct. This proves that our method can be efficiently deployed to scenarios with limited computational resources. When the backbone is switched to GPT-3.5, both ReAct and ABA gain performance improvements, while ABA still outperforms ReAct by around $20 \%$ on average.</p>
<p>Besides the final performance, we also include statistics on policy efficiency in Table 2. We logged the trajectories with GPT-3.5 backbone and report the averaged episode length of successful trails and all trails. The episode length includes physical actions, thinking steps and asking steps. We also report the number of physical actions taken to interact with the environment. It can be concluded that, even with additional action type (i.e., asking), ABA still enjoys a shorter trajectory. What's more, ABA can finish the task by taking more than $50 \%$ less actions. This proves that ABA not only have better final performance, but also enjoy better efficiency. We show example trajectories and qualitative analysis in AppendixB. Since we use human models to reduce human involvement, we also test the accuracy of the human model. For more details, refer to AppendixC.</p>
<p>To further assess the capabilities of ABA, we carefully design two variants of ALFWorld, namely Ambiguous ALFWorld and Multiround ALFWorld. These modifications pose new challenges. For Ambiguous ALFWorld, we adjust the task descriptions and reward functions to introduce ambiguity, which deliberately left some aspects open-ended to necessitate the agent gathering additional information. For Multiround ALFWorld, we assign a new task for the agent within the same room once the previous task has completed, which allows us to test whether the agent can remember previously known information and avoid asking repeatedly. For more details about the two environments, please refer to D. In this section, we compare ABA and ABA-FT with ReAct <em>(Yao et al., 2022)</em>. For</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Performance on Ambiguous ALFWorld and Multiround ALFWorld. The prefixes of the x-label Amb- and Mul- refer to Ambigous ALFWorld and multi-round ALFWorld, and two environments correspond to different y-axis. The legend suffixes -ID and -OOD refer to performance on in-distribution and out-of-distribution evaluation sets.
ABA and ReAct, we utilize human-annotated examples, while for ABA-FT, we manually design an expert policy to collect data. Additional details can be found in Appendix E.</p>
<p>The results are shown in Figure 2. Note that two environments correspond to different y-axis. Though the two variants are more complex than original ALFWorld, we find that both ABA and ABA-FT consistently exhibit superior performance, while ReAct fails to complete the task in most scenarios. This again highlights the significance of actively questioning for necessary information, and proves that taking consistent actions to gather necessary information in various complex environments is challenging for current agent. For both of the environments, the ABA-FT outperforms ABA, and the margin is significantly larger in Multiround ALFWorld. This observation aligns with our analysis in Section 3.2.2 that ABA-FT may further improve performance especially when trajectories are long. It is also worth noticing that, for ABA-FT, we only use 1500 and 500 trajectories to finetune one model for all the tasks for each environment. When compared with classical Imitation Learning method like BUTLER, this only accounts for 1.5% and 0.5% of one dataset (note BUTLER needs to independently finetune for each task). This proves that even with ABA-FT, we can effectively adapt to new scenarios at a relatively lower cost. Qualitative analysis can be found in Appendix F and Appendix G. We provide example trajectories on Ambiguous ALFWorld and Multiround ALFWorld to demonstrate that, both ABA and ABA-FT are able to ask pertinent questions to gather necessary information, while ReAct struggles to conduct consistent exploration. What's more, from Appendix G, we show that the agent is capable of recalling previously acquired information and leveraging it in the following tasks.</p>
<h1>4.2 ROBOT ARM MANIPULATION TASKS</h1>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task 2</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Task 3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$x=3$</td>
<td style="text-align: center;">$x=4$</td>
<td style="text-align: center;">$x=5$</td>
<td style="text-align: center;">$y=2$</td>
<td style="text-align: center;">$y=3$</td>
<td style="text-align: center;">$y=4$</td>
<td style="text-align: center;">$x=4, y=3$</td>
</tr>
<tr>
<td style="text-align: center;">ReAct</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">30</td>
</tr>
<tr>
<td style="text-align: center;">ABA</td>
<td style="text-align: center;">$\mathbf{8 5}$</td>
<td style="text-align: center;">$\mathbf{8 5}$</td>
<td style="text-align: center;">$\mathbf{9 0}$</td>
<td style="text-align: center;">$\mathbf{1 0 0}$</td>
<td style="text-align: center;">$\mathbf{9 5}$</td>
<td style="text-align: center;">$\mathbf{9 0}$</td>
<td style="text-align: center;">$\mathbf{9 0}$</td>
</tr>
</tbody>
</table>
<p>Figure 3: Left: Visualization of an example task instance in Task 3. Right: Average success rates across 20 trails on three manipulation tasks. Here, $x$ and $y$ refers to parameters in each task.</p>
<p>In this section, we further explore whether ABA is able to handle control tasks. Instead of text-based observations and actions, we design the agent to operate on numerical level by directly outputs coordinates of control commands. We consider it poses unique challenges since the asking and answering are formulated in natural language, which is relatively abstract with respect to controlling. Therefore, after proposing the questions, the agent needs to reason from answers to get the detailed information to guide decision-making (e.g., from the answer "the second cube from the left", the agent should be able to reason which cube to move, and output corresponding numerical commands).</p>
<p>We design three different tasks with unique challenges (see Figure 3 for visualization of an example task instance): (1) Task 1: the agent is instructed to move the red block into the green bowl. However, there might be $x$ such red blocks, but only one of them is the goal object and the task is considered success when only the correct block is in the bowl. Furthermore, we restricted the human model to answer in realtive position (e.g., "the second red block from the left") when asked. Therefore, the agent needs to reason to get the information according to the answer. (2) Task 2: the agent is instructed to place some of the blocks on $y$ corresponding bases. The bases refer to the fixed blue blocks on the table, and each base block has a different corresponding color. The task is considered success when all the bases have a corresponding color block placed on it. We make the agent to</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Visualization of real-world open domain embodied tasks. We show four trajectories and their corresponding instructions.
only ask one piece of information at a time (e.g., it can only as for one color correspondance) and restrict the total amount of questions to be $y-1$. Therefore, the agent must inferred the last color correspondance without asking to finish the task. (3) Task 3: the agent is instructed to finish the two tasks above and query diverse information. For experiment details, please refer to Appendix J.</p>
<p>The results can be found in Figure 3. We compare our method with ReAct, and use GPT-4 API as backbones for both of the method. For each trail, the placement of objects are randomly sampled. We report the average success rate across 20 trails for each setting. The results demonstrate that ABA outperforms ReAct in all the settings. In Task 1, we show ABA is able to reason the control actions from the relative position language, while ReAct needs to try the block one-by-one (see Appendix J for example trajectories). In Task 2 and Task 3, the performance gap is obvious especially when $y$ is relatively large. As $y$ increases, ABA performances stay relatively stable and remain above $90 \%$, while the baseline performance gradually deteriorates (from $95 \%$ when $y=2$ to $5 \%$ when $y=4$ ). This proves that, under large exploration space, even current state-of-the-art LLMs like GPT-4 struggle to take consistently explore and gather the information. These findings provide further evidence for the effectiveness of ABA.</p>
<h1>4.3 Real-World Open Domain Embodied Tasks with Image Inputs</h1>
<p>In this section, we carry out qualitative experiments to evaluate ABA in real-world scenarios with image inputs and human involvement to examine its ability to handle open-domain embodied decisionmaking tasks. We present the visualization in Figure 4. Real-world scenarios pose several challenges: First, real-world tasks and scenarios are more flexible and complex than a simulator, which requires up to 19 steps and 7 question asking to solve in our case. The diversity and complexity force us to test ABA in a zero-shot fashion since it is almost impossible to collect examples in advance. Second, the involvement of humans allows open domain question asking compared with human models, and it creates a more realistic scenario mimicking deploying ABA in practice.</p>
<p>However, the lack of a versatile embodied robot makes it hard to autonomously follow the plan proposed by the algorithm. Therefore, we ablate the plan proposal and execution by asking a human volunteer to strictly follow the proposed plan and interact with the world. In this way, we can solely focus on testing whether ABA can propose grounded questions and feasible plans to finish the task.</p>
<p>We build four real-world scenarios including a laboratory, an office, a bedroom, and a dormitory for evaluation. As for implementation, we use InstructBLIP (Liu et al., 2023) with Vicuna-13B Chiang et al. (2023) as the caption model, and use GPT-4 API as our decision-making backbone. We use $K=1$ human labeled trajectory as in-context examples (Figure 4 (a)) and test ABA on three different scenarios (Figure 4 (b-d)).</p>
<p>ABA successfully finished all three scenarios. We present the keyframes and instructions in Figure 4 and the full trajectories in Appendix K. The results qualitatively demonstrate that ABA is able to propose open-domain grounded questions and feasible plans. To be more specific, here are some excerpted questions. For (b), $t=1$ : "Besides the laptop, do you need to pack anything else?" and for (d), $t=1$ : "What kind of activities do you prefer to relax? Do you want to read the book, listen to music, or do something else?", the agent is able to propose open-domain questions by making suggestions while gathering the preference. For (c), $t=9$ : "Which pair of flip-flops would you like me to take, the green and yellow ones or the blue and white ones?" the agent can ask questions according to the image observations. Though the scenarios are rather limited, we believe</p>
<p>the trajectories can showcase the potential practical value of ABA and augurs a promising direction for future research.</p>
<h1>5 Related Works</h1>
<h3>5.1 Language Agent</h3>
<p>Natural language modeling pre-trained on large-scale unstructured text corpus has seen tremendous success in a variety of applications, including downstream NLP tasks (Radford et al., 2019; Devlin et al., 2018; Brown et al., 2020), logic reasoning (Zhao et al., 2023; Cobbe et al., 2021; Shen et al., 2021), and human-AI coordination (Bubeck et al., 2023; Hu \&amp; Sadigh, 2023). The rich information contained in LLMs as an implicit knowledge base also catalyzes the research on in-context learning (Shin et al., 2022; Xie et al., 2021) and prompting (Brown et al., 2020; Wei et al., 2022) that prepend instructions and a few examples to the input of LLMs. However, the time and memory complexity for encoding the prompt is quadratic in the length of the interaction history, such as all the previous trajectories in embodied decision-making, which can increase the burden of the self-attention mechanism and even exceed the token limitations of LLMs. Despite the techniques introduced to address this issue (Mu et al., 2023; Bulatov et al., 2023), the proposed ABA-IL is inspired by the recent studies on fine-tuning LLMs (Houlsby et al., 2019; Hu \&amp; Sadigh, 2023; Lialin et al., 2023), especially those that leverage decision-making signals to train language agents that satisfy certain goals (Carta et al., 2023; Snell et al., 2022a;b). LLMs have also shown great potential for task planning (Huang et al., 2022b; Lin et al., 2023; Huang et al., 2022a; Wang et al., 2023; Li et al., 2022; Singh et al., 2022a; Carta et al., 2023). However, recent criticisms are made on the planning abilities of LLMs (Bubeck et al., 2023; Valmeekam et al., 2022; 2023). They show that LLMs can get stuck in long-horizon decision-making tasks and the resulting search procedure often degrades to exhaustive search over the large state and action spaces. While pure LLM planning remains a highly challenging open problem, in this work, we investigate the capacity of LLM agents to actively gather information with humans in the loop.</p>
<h3>5.2 Embodied DECISION-MAKING With HuMAN-IN-THE-LOOP</h3>
<p>Some existing works have also studied the scenarios with human-in-the-loop. They query humans for extra information to guide decision-making. A majority of works (Nguyen \&amp; Daumé III, 2019; Nguyen et al., 2019; Singh et al., 2022b; Da Silva et al., 2020) directly ask humans for oracle actions, and most of the works focus on visual navigation (Gao et al., 2021; 2023). Unlike recent works in visual navigation (Zhao et al., 2022; Chen et al., 2022), they either learn when to ask for oracle actions (Da Silva et al., 2020; Singh et al., 2022b; Chi et al., 2020), query where the goal is in the view Zhang et al. (2023), or learn to leverage language instructions (Nguyen \&amp; Daumé III, 2019; Nguyen et al., 2019). Nguyen et al. (2022) asks for new descriptions of current states and (sub-)goals in a POMDP. For asking and answering in language, Huang et al. (2022b) engages humans in active scene description, allowing the LLM to consider human feedback of scene, task, and preferences as inputs. Liu et al. (2022) asks 3-word-templated selected by an RL agent and calculate bi-gram similarity. Yao et al. (2022) also mentions the engagement of humans. Still, it requires humans to supervise and modify the model's output in real-time, which is different from other HTIL settings.</p>
<p>Existing works include human-in-the-loop of decision-making either (1) directly asking for numerical vectors such as actions/states (Da Silva et al., 2020; Singh et al., 2022b; Nguyen et al., 2022), or (2) inquiring humans to give exhaustive instruction and learn to convert them to actions (Nguyen \&amp; Daumé III, 2019; Nguyen et al., 2019). Different from these works, we only put a minimal burden on humans and ask them for natural language information rather than detailed action instructions. Instead of considering human feedback as the scene (or task, preference) descriptor in the decisionmaking pipeline (Huang et al., 2022b), we formulate the setting as Contextual MDP with Human / External Information Sources in the Loop, which elaborates the effects of asking via context $c$ and allow the agent to query a broader range of information to gather information. Besides, unlike Liu et al. (2022), we focus on zero-shot adaptation settings and propose more natural end-to-end methods to circumvent the needs of template and similarity designing. Concurrently, Ren et al. (2023) proposes to use an explicit uncertainty module that helps decide when to trigger asking. However, ABA allows for seamlessly integrate asking and acting in an end-to-end manner, and ABA-FT allows better data scalbility.</p>
<h1>6 CONCLUSION AND DISCUSSION</h1>
<p>In this paper, we focus on the setting where the agent can actively query for additional pertinent information from external sources using natural language while interacting in the environments. To formalize this problem, we propose Contextual MDP with Human / External Information Sources in the Loop. Then, we propose Asking Before Acting (ABA), a method that empowers the agent to ask various questions to gather diverse information. We conduct extensive experiments involving a spectrum of environments including text-based household everyday tasks, robot arm manipulation tasks, and real world open domain image based embodied tasks on various models. The results demonstrate that, ABA exhibits substantial advantages on both performance and efficiency over baseline LLM agents. Further finetuning ABA with reformulated metadata (ABA-FT) promotes learning the asking rationale and allows for additional enhancements. We believe our work serves as one of the first papers in combining LLMs (VLMs) with human in the loop decision making, and we hope ABA can be inspiring. However, in real world experiments, we currently use a perfect controller. In the future, we believe it is important to further study how to combine our method with real world robot to conduct more tasks.</p>
<h2>REFERENCES</h2>
<p>Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022.</p>
<p>Jacob Beck, Risto Vuorio, Evan Zheran Liu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shimon Whiteson. A survey of meta-reinforcement learning. arXiv preprint arXiv:2301.08028, 2023.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>Aydar Bulatov, Yuri Kuratov, and Mikhail S Burtsev. Scaling transformer to 1m tokens and beyond with rmt. arXiv preprint arXiv:2304.11062, 2023.</p>
<p>Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. arXiv preprint arXiv:2302.02662, 2023.</p>
<p>Jinyu Chen, Chen Gao, Erli Meng, Qiong Zhang, and Si Liu. Reinforced structured state-evolution for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15450-15459, 2022.</p>
<p>Ta-Chung Chi, Minmin Shen, Mihail Eric, Seokhwan Kim, and Dilek Hakkani-tur. Just ask: An interactive learning framework for vision and language navigation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 2459-2466, 2020.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https: //lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam</p>
<p>Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Felipe Leno Da Silva, Pablo Hernandez-Leal, Bilal Kartal, and Matthew E Taylor. Uncertaintyaware action advising for deep reinforcement learning agents. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 5792-5799, 2020.</p>
<p>Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Why can gpt learn incontext? language models secretly perform gradient descent as meta optimizers. arXiv preprint arXiv:2212.10559, 2022.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.</p>
<p>Chen Gao, Jinyu Chen, Si Liu, Luting Wang, Qiong Zhang, and Qi Wu. Room-and-object aware knowledge reasoning for remote embodied referring expression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3064-3073, 2021.</p>
<p>Chen Gao, Xingyu Peng, Mi Yan, He Wang, Lirong Yang, Haibing Ren, Hongsheng Li, and Si Liu. Adaptive zone-aware hierarchical planner for vision-language navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14911-14920, 2023.</p>
<p>Google. Palm 2 technical report. 2023.
Assaf Hallak, Dotan Di Castro, and Shie Mannor. Contextual markov decision processes. arXiv preprint arXiv:1502.02259, 2015.</p>
<p>Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on Machine Learning, pp. 2790-2799. PMLR, 2019.</p>
<p>Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.</p>
<p>Hengyuan Hu and Dorsa Sadigh. Language instructed reinforcement learning for human-ai coordination. arXiv preprint arXiv:2304.07297, 2023.</p>
<p>Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pp. 9118-9147. PMLR, 2022a.</p>
<p>Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022b.</p>
<p>Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer: Composable 3d value maps for robotic manipulation with language models. arXiv preprint arXiv:2307.05973, 2023.</p>
<p>Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, et al. Pre-trained language models for interactive decisionmaking. Advances in Neural Information Processing Systems, 35:31199-31212, 2022.</p>
<p>Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning. arXiv preprint arXiv:2303.15647, 2023.</p>
<p>Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion: From natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023.</p>
<p>Iou-Jen Liu, Xingdi Yuan, Marc-Alexandre Côté, Pierre-Yves Oudeyer, and Alexander Schwing. Asking for knowledge (afk): Training rl agents to query external knowledge using language. In International Conference on Machine Learning, pp. 14073-14093. PMLR, 2022.</p>
<p>Jesse Mu, Xiang Lisa Li, and Noah Goodman. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467, 2023.</p>
<p>Khanh Nguyen and Hal Daumé III. Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning. arXiv preprint arXiv:1909.01871, 2019 .</p>
<p>Khanh Nguyen, Debadeepta Dey, Chris Brockett, and Bill Dolan. Vision-based navigation with language-based assistance via imitation learning with indirect intervention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12527-12537, 2019.</p>
<p>Khanh X Nguyen, Yonatan Bisk, and Hal Daumé Iii. A framework for learning to request rich and contextually useful information from humans. In International Conference on Machine Learning, pp. 16553-16568. PMLR, 2022.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: $27730-27744,2022$.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.</p>
<p>Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for large language model planners. arXiv preprint arXiv:2307.01928, 2023.</p>
<p>Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp. 627-635. JMLR Workshop and Conference Proceedings, 2011.</p>
<p>Jianhao Shen, Yichun Yin, Lin Li, Lifeng Shang, Xin Jiang, Ming Zhang, and Qun Liu. Generate \&amp; rank: A multi-task framework for math word problems. arXiv preprint arXiv:2109.03034, 2021.</p>
<p>Seongjin Shin, Sang-Woo Lee, Hwijeen Ahn, Sungdong Kim, HyoungSeok Kim, Boseop Kim, Kyunghyun Cho, Gichang Lee, Woomyoung Park, Jung-Woo Ha, et al. On the effect of pretraining corpora on in-context learning by a large-scale language model. arXiv preprint arXiv:2204.13509, 2022.</p>
<p>Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10740-10749, 2020.</p>
<p>Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. URL https://arxiv.org/abs/2010.03768.</p>
<p>Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022.</p>
<p>Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022a.</p>
<p>Kunal Pratap Singh, Luca Weihs, Alvaro Herrasti, Jonghyun Choi, Aniruddha Kembhavi, and Roozbeh Mottaghi. Ask4help: Learning to leverage an expert for embodied tasks. Advances in Neural Information Processing Systems, 35:16221-16232, 2022b.</p>
<p>Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022a.</p>
<p>Charlie Snell, Sherry Yang, Justin Fu, Yi Su, and Sergey Levine. Context-aware language modeling for goal-oriented dialogue systems. arXiv preprint arXiv:2204.10198, 2022b.</p>
<p>Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large language models still can't plan (a benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498, 2022.</p>
<p>Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao Kambhampati. On the planning abilities of large language models (a critical investigation with a proposed benchmark). arXiv preprint arXiv:2302.06706, 2023.</p>
<p>Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. arXiv preprint arXiv:2111.02080, 2021.</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.</p>
<p>Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL), 2020.</p>
<p>Jenny Zhang, Samson Yu, Jiafei Duan, and Cheston Tan. Good time to ask: A learning framework for asking for help in embodied visual navigation. In 2023 20th International Conference on Ubiquitous Robots (UR), pp. 503-509. IEEE, 2023.</p>
<p>Hongyu Zhao, Kangrui Wang, Mo Yu, and Hongyuan Mei. Explicit planning helps language models in logical reasoning. arXiv preprint arXiv:2303.15714, 2023.</p>
<p>Yusheng Zhao, Jinyu Chen, Chen Gao, Wenguan Wang, Lirong Yang, Haibing Ren, Huaxia Xia, and Si Liu. Target-driven structured transformer planner for vision-language navigation. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 4194-4203, 2022.</p>
<p>Luisa M Zintgraf, Leo Feng, Cong Lu, Maximilian Igl, Kristian Hartikainen, Katja Hofmann, and Shimon Whiteson. Exploration in approximate hyper-state space for meta reinforcement learning. In International Conference on Machine Learning, pp. 12991-13001. PMLR, 2021.</p>
<h1>A DESIGN AND DETAILS ABOUT HUMAN MODEL</h1>
<p>In this section, we describe the design and details about the human model (or other information sources) $\mathcal{H}$. To minimize human involvement, during the evaluation phase, we implement $\mathcal{H}$ via another language model which is instructed to respond to questions based on the provided information. To incorporate prior knowledge about the current room, we extract the information about the object placement from the simulator and transform it into a descriptive paragraph. This paragraph is then fed to $\mathcal{H}$. Specifically, we use Vicuna-7B (Chiang et al., 2023) to implement $\mathcal{H}$. Using a pretrained LLM as $\mathcal{H}$ allows for answering questions in free-form language based on the information provided, which acts just like humans.</p>
<p>To better demonstrate, we provide an example for the ALFWorld experiment in Section 4.1. In ALFWorld, the context $c$ mainly refers to the initial mappings of the object placement. For different rooms, the initial mappings are, therefore, different. We slightly abuse the notations about $c$ here since the agent may replace the objects. Under this mapping, we can directly get the ground truth object locations from the simulator, which are unobservable to the agent. Then, we use a rule-based conversion to convert that list to a string of " $A$ is in $B$ ", where $A$ refers to the object, while $B$ refers to the place containing the object.</p>
<p>Here is an example. After converting, we derive a descriptive paragraph like:</p>
<div class="codehilite"><pre><span></span><code>    bowl 2 is in diningtable 2. saltshaker 2 is in
sidetable 1. spatula 1 is in countertop 1. pot
1 is in stoveburner 4. spatula 2 is in drawer 1.
dishsponge 3 is in diningtable 2. peppershaker 1 is
in cabinet 2. tomato 4 is in sidetable 1. knife 1
is in diningtable 3. cup 1 is in sidetable 1. bread
2 is in diningtable 3. spatula 3 is in diningtable
2. pan 1 is in cabinet 4. tomato 3 is in fridge
1. potato 1 is in sinkbasin 1. peppershaker 3 is
in diningtable 3. apple 1 is in fridge 1. saltshaker
1 is in cabinet 4. fork 2 is in drawer 1. spoon 1 is
in sidetable 1. egg 1 is in fridge 1. lettuce 1 is
in sidetable 1. plate 1 is in diningtable 2.
</code></pre></div>

<p>Whenever the agent poses a question, $\mathcal{H}$ is tasked with providing an answer based on this paragraph. For instance, the agent may learn to ask:</p>
<h2>Where can I find the dishsponge?</h2>
<p>Then, in this example, the input to $\mathcal{H}$ will be (1) an instruction that tells the model to provide the answers (in gray); (2) a descriptive paragraph (in black); (3) the question proposed by the agent (in blue).</p>
<div class="codehilite"><pre><span></span><code>Read the following paragraph and answer questions:
bowl 2 is in diningtable 2. saltshaker 2 is in
sidetable 1. spatula 1 is in countertop 1. pot
1 is in stoveburner 4. spatula 2 is in drawer 1.
dishsponge 3 is in diningtable 2. peppershaker 1 is
in cabinet 2. tomato 4 is in sidetable 1. knife 1
is in diningtable 3. cup 1 is in sidetable 1. bread
2 is in diningtable 3. spatula 3 is in diningtable
2. pan 1 is in cabinet 4. tomato 3 is in fridge
1. potato 1 is in sinkbasin 1. peppershaker 3 is
in diningtable 3. apple 1 is in fridge 1. saltshaker
1 is in cabinet 4. fork 2 is in drawer 1. spoon 1 is
in sidetable 1. egg 1 is in fridge 1. lettuce 1 is
in sidetable 1. plate 1 is in diningtable 2. The
questions is: Where can I find the dishsponge?
</code></pre></div>

<p>Then, the pretrained LLM (e.g., Vicuna-7B Chiang et al. (2023) in our case), $\mathcal{H}$ will provide the answers since it can follow the instructions. In our case, the answer is:</p>
<div class="codehilite"><pre><span></span><code>dishsponge 1 is in garbagecan 1, dishsponge 2 is in
drawer 3, dishsponge 3 is in drawer 4.
</code></pre></div>

<p>To enhance the performance and improve the accuracy, we provide a few in-context examples and use the prompting format as in Vicuna Chiang et al. (2023). Please refer to Appendix I for details.</p>
<p>It's worth noting that, though we use a language model to simulate humans in our evaluation phase, this design allows for the straightforward replacement of the current language model with human or alternative information sources for more appropriate answers. The performance of our method will be further improved if $\mathcal{H}$ can provide more accurate and meaningful answers.</p>
<h1>B Details, examples and Qualitative Analysis for ALFWorld EXPERIMENTS</h1>
<p>For ALFWorld experiment, now we describe the benchmark and baselines in details.
ALFWorld (Shridhar et al., 2021) is an interactive text-based environment with aligned embodied worlds, which contains six types of different everyday household tasks from ALFRED Shridhar et al. (2020) encompassing activities such as picking and placing, examining in light, cleaning, etc. In each episode, the agent is deployed in a new room and assigned specific tasks to accomplish. All observations and actions are in natural language.</p>
<p>As for baselines, we select two representative baselines, BUTLER (Shridhar et al., 2021) and ReAct (Yao et al., 2022). BUTLER (Shridhar et al., 2021) is an imitation learning based method without LLM. It trains an independent model for each task, while each of the model uses a substantial dataset of $10^{5}$ expert trajectories. ReAct (Yao et al., 2022) is an LLM-based method synergizing reasoning and acting to take actions. It is worth noticing that, for ReAct(Yao et al., 2022) and ABA (we may also use the term ABA interchangeably), we use the same general pretrained LLM across all tasks with $K=2$ in-context examples. As for the implementation, we try Vicuna-7B Chiang et al. (2023) and GPT-3.5 OpenAI (2023) as backbones for both ReAct and our method, and we incorporate the reasoning process when making decisions (Yao et al., 2022; Ahn et al., 2022). For a fair comparison, we use the same scoring method to select actions for both our method and ReAct.</p>
<p>This section provides some examples and qualitative analysis for ALFWorld experiments in Section 4.1.</p>
<p>First, we provide an episode trajectory of our method (ABA-IC), which is randomly sampled from the evaluation set:</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">are</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">middle</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">room</span><span class="p">.</span><span class="w"> </span><span class="n">Looking</span>
<span class="n">quickly</span><span class="w"> </span><span class="n">around</span><span class="w"> </span><span class="n">you</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">bed</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">diningtable</span>
<span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span>
<span class="n">garbagecan</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">Your</span>
<span class="n">task</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">to</span><span class="err">:</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">sidetable</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">1</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="k">To</span><span class="w"> </span><span class="n">solve</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">task</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="ow">and</span>
<span class="n">take</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="p">,</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">sidetable</span><span class="p">.</span><span class="w"> </span><span class="n">But</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="k">is</span>
<span class="n">the</span><span class="w"> </span><span class="n">mug</span><span class="vm">?</span><span class="w"> </span><span class="n">Let</span><span class="w"> </span><span class="n">me</span><span class="w"> </span><span class="n">ask</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">person</span><span class="p">.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">2</span><span class="err">:</span><span class="w"> </span><span class="nl">ask</span><span class="p">:</span><span class="w"> </span><span class="k">Where</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">mug</span><span class="vm">?</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">3</span><span class="err">:</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">diningtable</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">in</span>
<span class="n">diningtable</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">diningtable</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">3</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">We</span><span class="w"> </span><span class="n">can</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">diningtable</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">take</span>
<span class="n">the</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="k">then</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">sidetable</span><span class="p">.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">4</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">4</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">diningtable</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">5</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">diningtable</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">creditcard</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="n">a</span><span class="w"> </span><span class="n">creditcard</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">keychain</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">keychain</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span>
<span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pen</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pen</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pencil</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span>
<span class="n">pencil</span><span class="w"> </span><span class="mf">1.</span>
</code></pre></div>

<p>Act 5: think: Now I find the mug 1. Next, I need to take it, then put it in sidetable.</p>
<p>Obs 6: Ok.
Act 6: take mug 1 from diningtable 1
Obs 7: You pick up the mug 1 from the diningtable 1.
Act 7: think: Now I take a mug 1. Next, I need to put it in sidetable.</p>
<p>Obs 8: Ok.
Act 8: go to sidetable 1
Obs 9: On the sidetable 1, you see a keychain 1.
Act 9: put mug 1 in/on sidetable 1
Obs 10: You put the mug 1 in/on the sidetable 1.
It can be concluded that, for our method, the agent can identify the correct context that is related to the task (i.e., the location of the mug) and then summarize the question and ask a pertinent question (i.e., "Where is the mug?"). After that, the agent can integrate the information in the answer (i.e., notice that mug 1 is in diningtable 1) and act according to that information (i.e., go to diningtable 1, take the mug, and place it in the specific location).</p>
<p>For comparison, we provide the trajectory of the same episode of ReAct (Yao et al., 2022):</p>
<p>Obs 1: You are in the middle of a room. Looking quickly around you, you see a bed 1, a diningtable 1, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a garbagecan 1, a sidetable 2, and a sidetable 1. Your task is to: put a mug in sidetable.</p>
<p>Act 1: think: To solve the task, I need to find and take a mug, then put it in sidetable.</p>
<p>Obs 2: OK.
Act 2: think: First I need to find a mug. A mug is more likely to appear in drawer (1-4), sidetable (1-2). I can check one by one, starting with drawer 1.</p>
<p>Obs 3: OK.
Act 3: go to drawer 1
Obs 4: The drawer 1 is closed.
Act 4: open drawer 1
Obs 5: You open the drawer 1. The drawer 1 is open.
In it, you see a cellphone 1.
Act 5: think: Now I find a mug (1). Next, I need
to take it.
Obs 6: OK.
Act 6: go to drawer 2
Obs 7: The drawer 2 is closed.
Act 7: open drawer 2
Obs 8: You open the drawer 2. The drawer 2 is open.
In it, you see a creditcard 1.
Act 8: think: Now I find a mug (2). Next, I need
to take it.
Obs 9: OK.
Act 9: go to drawer 3
Obs 10: On the drawer 3, you see a cellphone 2.
Act 10: think: Now I find a mug (3). Next, I need
to take it.
Obs 11: OK.
Act 11: go to drawer 4
Obs 12: On the drawer 4, you see nothing.
Act 12: think: Now I find a mug (4). Next, I need
to take it.</p>
<p>Obs 13: OK.
Act 13: go to sidetable 1
Obs 14: On the sidetable 1, you see a keychain 1.
Act 14: think: Now I find a mug (1). Next, I need
to put it in sidetable 1.
Obs 15: OK.
Act 15: go to sidetable 2
Obs 16: On the sidetable 2, you see a alarmclock 1, a cd 1, and a pencil 2.</p>
<p>Act 16: think: Now I find a mug (2). Next, I need
to put it in sidetable 2.
Obs 17: OK.
Act 17: go to drawer 1
Obs 18: The drawer 1 is open. In it, you see a cellphone 1.</p>
<p>Act 18: think: Now I find a mug (1). Next, I need
to put it in drawer 1.
Obs 19: OK.
Act 19: go to drawer 2
Obs 20: The drawer 2 is open. In it, you see a creditcard 1.</p>
<p>Act 20: think: Now I find a mug (2). Next, I need
to put it in drawer 2.
Obs 21: OK.
Act 21: go to drawer 3
Obs 22: On the drawer 3, you see a cellphone 2.
Act 22: think: Now I find a mug (3). Next, I need
to put it in drawer 3.
Obs 23: OK.
Act 23: go to drawer 4
Obs 24: On the drawer 4, you see nothing.
Act 24: think: Now I find a mug (4). Next, I need
to put it in drawer 4.
Obs 25: OK.
Act 25: go to bed 1
Obs 26: On the bed 1, you see a book 2, a book 1, a laptop 2, a laptop 1, and a pillow 1.</p>
<p>Act 26: think: Now I find a mug (1). Next, I need
to put it on bed 1.
Obs 27: OK.
Act 27: go to sidetable 1
Obs 28: On the sidetable 1, you see a keychain 1.
Act 28: think: Now I find a mug (1). Next, I need
to put it on sidetable 1.
Obs 29: OK.
Act 29: go to sidetable 2
Obs 30: On the sidetable 2, you see a alarmclock 1, a cd 1, and a pencil 2.</p>
<p>Act 30: think: Now I find a mug (1). Next, I need
to put it on sidetable 2.
Obs 31: OK.
Act 31: go to drawer 1
Obs 32: The drawer 1 is open. In it, you see a cellphone 1.</p>
<p>Act 32: think: Now I find a mug (1). Next, I need
to put it in drawer 1.
Obs 33: OK.
Act 33: go to drawer 2</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">34</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">open</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span>
<span class="n">creditcard</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">34</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">2.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">35</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">35</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">36</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">cellphone</span><span class="w"> </span><span class="mf">2.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">36</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">3.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">37</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">37</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">4</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">38</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">4</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">nothing</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">38</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">3</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">4.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">39</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">39</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">bed</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">40</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">bed</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">book</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">book</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span>
<span class="n">laptop</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">laptop</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pillow</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">40</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">bed</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">41</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">41</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">42</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">keychain</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">42</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">43</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">43</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">44</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">alarmclock</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="n">a</span><span class="w"> </span><span class="n">cd</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">pencil</span><span class="w"> </span><span class="mf">2.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">44</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="k">on</span><span class="w"> </span><span class="n">sidetable</span><span class="w"> </span><span class="mf">2.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">45</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">45</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">46</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">open</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span>
<span class="n">cellphone</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">46</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">47</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">47</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">48</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">open</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span>
<span class="n">creditcard</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">48</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">2.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">49</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">49</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">50</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">cellphone</span><span class="w"> </span><span class="mf">2.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">50</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">mug</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span>
<span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">3.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">51</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
</code></pre></div>

<p>It can be concluded that the agent failed to provide the right plan at first (i.e., Action 1,2), since none of these locations has a mug. Moreover, even with the plans, it failed to yield coherent and correct actions in later steps. For instance, in Action 5, 8, 10, 12, etc., the agent is constantly confused by hallucination (it believes that "Now I find a mug" while there wasn't a mug actually). It also got stuck in loops (it repeatedly visited drawer 1,2,3,4, sidetable 1,2, and bed 1). These together prevent the agent from finishing the task.</p>
<p>To further understand the experiment results, we provide more examples for our method and ReAct.
For our method:</p>
<p>Obs 1: You are in the middle of a room. Looking quickly around you, you see a bathtubbasin 1, a countertop 1, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a shelf 2, a shelf 1, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1. Your task is to: put some spraybottle on toilet.</p>
<p>Act 1: think: To solve the task, I need to find and take a spraybottle, then put it on the toilet. But where is the spraybottle? Let me ask that person.</p>
<p>Obs 2: OK.
Act 2: ask: Where is the spraybottle?
Obs 3: spraybottle 3 is in countertop 1, spraybottle 4 is in dresser 1, spraybottle 2 is in shelf 1.</p>
<p>Act 3: think: We can go to countertop 1 and take the spraybottle 3, then put it on the toilet.</p>
<p>Obs 4: OK.
Act 4: go to countertop 1
Obs 5: On the countertop 1, you see a mirror 1, a soapbar 1, and a spraybottle 1.</p>
<p>Act 5: think: Now I find the spraybottle 1. Next, I need to take it, then put it on the toilet.</p>
<p>Obs 6: OK.
Act 6: take spraybottle 1 from countertop 1
Obs 7: You pick up the spraybottle 1 from the countertop 1.</p>
<p>Act 7: think: Now I take a spraybottle 1. Next, I need to put it on the toilet.</p>
<p>Obs 8: OK.
Act 8: go to toilet 1
Obs 9: On the toilet 1, you see a candle 1, a toiletpaper 2, and a toiletpaper 1.</p>
<p>Act 9: think: Now I put the spraybottle 1 on the toilet.</p>
<p>Obs 10: OK.
Act 10: put spraybottle 1 in/on toilet 1
Obs 11: You put the spraybottle 1 in/on the toilet 1 .</p>
<h1>For ReAct:</h1>
<p>Obs 1: You are in the middle of a room. Looking quickly around you, you see a bathtubbasin 1, a countertop 1, a drawer 4, a drawer 3, a drawer 2, a drawer 1, a dresser 1, a garbagecan 1, a handtowelholder 2, a handtowelholder 1, a shelf 2, a shelf 1, a sinkbasin 1, a toilet 1, a toiletpaperhanger 1, and a towelholder 1. Your task is to: put some spraybottle on toilet.</p>
<p>Act 1: think: To solve the task, I need to find and take a spraybottle, then put it on toilet.</p>
<p>Obs 2: OK.
Act 2: think: First I need to find a spraybottle. A spraybottle is more likely to appear in drawer (1-4), dresser (1), sinkbasin (1), toilet (1), and towelholder (1). I can check one by one, starting with drawer 1.</p>
<p>Obs 3: OK.</p>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">3</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">4</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">closed</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">4</span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">5</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">open</span><span class="p">.</span>
<span class="ow">In</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">nothing</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">5</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="p">(</span><span class="ow">not</span><span class="p">).</span>
<span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">6</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">6</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">7</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">closed</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">7</span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">8</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">2.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">open</span><span class="p">.</span>
<span class="ow">In</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">candle</span><span class="w"> </span><span class="mf">3.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">8</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="p">(</span><span class="ow">not</span><span class="p">).</span>
<span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">9</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">9</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">10</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">closed</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">10</span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">11</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">3.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="k">is</span>
<span class="k">open</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">nothing</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">11</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="p">(</span><span class="ow">not</span><span class="p">).</span>
<span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">12</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">12</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">4</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">13</span><span class="err">:</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">closed</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">13</span><span class="err">:</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">4</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">14</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="k">open</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mf">4.</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">drawer</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="k">is</span>
<span class="k">open</span><span class="p">.</span><span class="w"> </span><span class="ow">In</span><span class="w"> </span><span class="n">it</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">nothing</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">14</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="p">(</span><span class="ow">not</span><span class="p">).</span>
<span class="k">Next</span><span class="p">,</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">it</span><span class="p">.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">15</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">15</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">dresser</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">16</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">dresser</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">candle</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="n">a</span>
<span class="n">soapbottle</span><span class="w"> </span><span class="mi">3</span><span class="p">,</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">soapbottle</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="mf">4.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">16</span><span class="err">:</span><span class="w"> </span><span class="n">take</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">dresser</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">17</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">pick</span><span class="w"> </span><span class="n">up</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="n">the</span>
<span class="n">dresser</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">17</span><span class="err">:</span><span class="w"> </span><span class="nl">think</span><span class="p">:</span><span class="w"> </span><span class="n">Now</span><span class="w"> </span><span class="n">I</span><span class="w"> </span><span class="n">find</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="p">(</span><span class="mi">4</span><span class="p">).</span><span class="w"> </span><span class="k">Next</span><span class="p">,</span>
<span class="n">I</span><span class="w"> </span><span class="n">need</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">it</span><span class="w"> </span><span class="ow">in</span><span class="o">/</span><span class="k">on</span><span class="w"> </span><span class="n">toilet</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">18</span><span class="err">:</span><span class="w"> </span><span class="n">OK</span><span class="p">.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">18</span><span class="err">:</span><span class="w"> </span><span class="k">go</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="n">toilet</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">19</span><span class="err">:</span><span class="w"> </span><span class="k">On</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">toilet</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">see</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">candle</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">a</span>
<span class="n">toiletpaper</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">toiletpaper</span><span class="w"> </span><span class="mf">1.</span>
<span class="w">    </span><span class="n">Act</span><span class="w"> </span><span class="mi">19</span><span class="err">:</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="ow">in</span><span class="o">/</span><span class="k">on</span><span class="w"> </span><span class="n">toilet</span><span class="w"> </span><span class="mi">1</span>
<span class="w">    </span><span class="n">Obs</span><span class="w"> </span><span class="mi">20</span><span class="err">:</span><span class="w"> </span><span class="n">You</span><span class="w"> </span><span class="n">put</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">spraybottle</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="ow">in</span><span class="o">/</span><span class="k">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">toilet</span>
</code></pre></div>

<ol>
<li></li>
</ol>
<p>From the above examples, our method can ask proper questions and act accordingly. Though $\mathcal{H}$ provides information with slight error (e.g., it mentions that spraybottle 3 is in countertop 1 , but only spraybottle 1 is in countertop 1), the agent is robust to such error and successfully adjust its behavior after observing the objects on countertop 1 (i.e., action 5, 6, 7).
As for ReAct, it successfully visited four drawers and finally found the spraybottle at dresser 1. However, first, it failed to list every possible container for the spraybottle (i.e., action 2, it omits countertop, shelf, etc.). In the reasoning step, we observe an interesting pattern (i.e., in Action $5,8,11,14)$ : "Now I find a spraybottle (not). Next, I need to take it", which seems inconsistent (though it does not affect the next step). Moreover, though the agent finally finds the spraybottle and</p>
<p>completes the task successfully, it is inefficient and slow to search every possible location: ReAct takes 20 steps. In comparison, our method only takes 10 steps to finish the task.</p>
<p>Four above examples demonstrate that, first, it is challenging to learn a information-gathering policy especially in unfamiliar environments, due to the complexity of the environment. Moreover, even if the agent manage to follow this policy, the information-gathering phase can be inefficient, which needs to exhaustively search every possible position. In contrast, our method succeeds in proposing proper questions and then acting accordingly, which improve the success rate as well as the efficiency. This proves our method's efficacy.</p>
<h1>C Accuracy of Human Model</h1>
<p>In this section, we test the accuracy of the human model. We test across 8 randomly sampled scenarios in ALFWorld with 5 questions each, and see whether the human model can answer correctly.</p>
<p>The correct rates as shown in the following table. The results of models implemented in Vicuna 7B:</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Accuracy</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">All</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Human Model</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">$72.5 \pm 9.7$</td>
</tr>
</tbody>
</table>
<h2>D Details about the two ALFWorld variants: Ambiguous ALFWorld and Multiround ALFWorld</h2>
<p>In Ambiguous ALFWorld, we manually adjusted the task descriptions and reward functions to introduce ambiguity. Instead of providing precise task descriptions, we deliberately left some aspects open-ended, thereby necessitating the agent to gather additional information for successful completion. For instance, in ALFWorld, the task "put a mug on the shelf" is typically considered accomplished as long as any mug is placed on the shelf (there might be multiple mugs in the room). But in this modified setting, the task is only deemed completed when a specific mug is put on the shelf. To complete this task, one can either enumerate all possibilities accordingly until the correct one is identified or directly ask for further clarification about the task.</p>
<p>To further test whether the agent is able to remember the previously known information and avoid asking repeatedly, we introduce multiround ALFWorld. In previous experiments, the episode ends as long as the current task is completed. subsequently, in the next episode, the environment will reset to another room with a different layout. In Multiround ALFWorld, after one task is completed, we randomly sample a new task for the agent to undertake within the same room for multiple rounds. This adjustment enables the agent to familiarize itself with the object placement and provides an opportunity to test its capability to remember and refrain from repetitive questioning. For instance, suppose the agent has previously visited the sidetable to complete a previous task and happened to see there is a mug, or the agent has previously ask about the location of the mug, when the agent is tasked to bring a mug, it can directly go to the location without the need for further inquiries. In this environment, instead of measuring the success rate as in previous experiments, we assign a reward $r=1$ upon the completion of each task and measure the total reward after $T$ steps.</p>
<h2>E Details about the Data Collection</h2>
<p>In this section, we provide details about how the data is collected and training as mentioned of two ALFWorld variants described in Section 4.1.</p>
<p>As for in-context examples used in ABA-IC, we manually interact with the environment and try to finish the tasks. We ask questions related to the tasks, and answer the questions ourselves by checking the ground truth states in the simulator. Beside the questions, we also add reasoning steps as in Yao et al. (2022) and select actions according to the information we have. Once completing the task, we take down all the actions and observations and use them as in-context examples.</p>
<p>As for ABA-FT, we design a rule-based policy according to the PDDL planning trajectories provided along with the environment. Specifically, we integrate the PDDL trajectories and the ground truth states within the simulator to find out what we should do to finish the tasks. Then, we extract the ground truth placements of the necessary objects from the simulator, and we write template-based</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*chen-xy21@mails.tsinghua.edu.cn&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8274 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8274</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8274</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-152.html">extraction-schema-152</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <p><strong>Paper ID:</strong> paper-276422476</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.12589v1.pdf" target="_blank">RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts</a></p>
                <p><strong>Paper Abstract:</strong> Recently, substantial advancements have been made in training language models to carry out step-by-step reasoning for solving intricate numerical reasoning tasks. Beyond the methods used to solve these problems, the structure and formulation of the problems themselves also play a crucial role in determining the performance of large language models. We observe that even small changes in the surface form of mathematical problems can have a profound impact on both the answer distribution and solve rate. This highlights the vulnerability of LLMs to surface-level variations, revealing its limited robustness when reasoning through complex problems. In this paper, we propose RM-PoT, a three-stage framework that integrates problem reformulation (RM), code-aided reasoning (PoT), and domain-aware few-shot learning to address these limitations. Our approach first reformulates the input problem into diverse surface forms to reduce structural bias, then retrieves five semantically aligned examples from a pre-constructed domain-specific question bank to provide contextual guidance, and finally generates executable Python code for precise computation.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8274.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8274.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RM-PoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Reformulating Mathematical Problems and Solving via Program of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage framework that (1) generates multiple paraphrased/problem-reformulated versions of an input math problem to reduce surface-form bias, (2) uses Program-of-Thoughts (PoT) to produce intermediate executable Python code for computation, and (3) injects domain-aware few-shot exemplars and self-consistency voting to determine final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-4-9B (GLM-4 series, 9B parameter model) used as the base LLM for all experiments; used in zero-shot and few-shot prompting without further training or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Problem reformulation (paraphrasing) to generate diverse surface forms', 'Program of Thoughts (PoT): generate intermediate executable Python code', 'Domain-aware few-shot retrieval (top-5 exemplars by embedding similarity)', 'Self-consistency / voting over sampled reasoning/code outputs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Problem reformulation: prompt the same LLM to produce K different surface forms of the input problem (naive prompt or in‑context examples). PoT: prompt the LLM to output stepwise reasoning implemented as Python code and store the numeric result in a fixed variable (e.g., 'ans') so answers can be executed/checked. Domain-aware few-shot: classify problem domain, retrieve the top-5 semantically aligned (question, solution) exemplars from a domain bank via sentence‑BERT embeddings and inject them into the prompt. Self-consistency / voting: sample multiple reasoning/code outputs and vote to select the final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>both</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>They vary the number of reformulations K in {1,2,4} while keeping the total number of sampled reasoning paths N fixed at 16 (i.e., for each reformulation sample N/K reasoning paths). This isolates the effect of diversifying problem surface forms versus just increasing the number of sampled reasoning traces. They compare naive reformulation prompts vs in-context reformulation examples, and compare RM-PoT to baselines CoT, SC, and PoT.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K (grade-school math problems, ~8.5K), AQuA (algebraic word problems, multiple-choice), SVAMP (1K arithmetic word problems focused on basic ops).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported final-voted accuracies (after voting) on GLM-4-9B with total N=16 reasoning paths: CoT: GSM8K 75.6%, AQuA 63.2%, SVAMP 86.3%; SC: GSM8K 77.3%, AQuA 64.9%, SVAMP 87.6%; PoT: GSM8K 78.9%, AQuA 65.6%, SVAMP 87.1%; RM-PoT: GSM8K 80.4%, AQuA 69.1%, SVAMP 88.0%. Ablation: increasing K (more reformulations) generally improves performance up to a point (they report K∈{1,2,4} with improvements as K increases; SVAMP showed best performance at K=2 in one result). Exact table values are provided in the paper's Table 1 and Table 2 (numerics above extracted from those tables).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Reformulating problems into diverse surface forms often increases solve rate and consistency even when some reformulations individually have lower solve rates; exposing the model to varied formulations helps avoid misinterpretations tied to a specific phrasing. PoT (code generation) disentangles computation from reasoning and reduces numerical/arithmetic mistakes, but PoT alone can still suffer from misinterpretation of ambiguous phrasing; combining reformulation with PoT corrects some of these interpretation errors. In-context reformulation examples produce better reformulations than naive paraphrasing prompts. The authors note some randomness and diminishing returns on very simple datasets (e.g., SVAMP).</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Combining diverse problem reformulations with Program-of-Thoughts and domain-aware few-shot exemplars (RM-PoT) yields consistent and statistically significant accuracy gains over Chain-of-Thought, Self-Consistency, and vanilla PoT baselines across GSM8K, AQuA, and SVAMP; fixing total sampled reasoning paths while varying the number of distinct reformulations shows that reformulation-driven diversity (not merely more samples) contributes to gains. In-context reformulation examples further improve performance over naive reformulation prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8274.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8274.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program of Thoughts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that directs the LLM to produce intermediate reasoning as executable program code (here, Python), separating symbolic computation from natural-language reasoning and enabling execution of computations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Program of thoughts prompting: A framework for large language models to reason through programs.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GLM-4-9B base model used as the PoT baseline; code-generation prompts used to produce python code and store result in a fixed variable for extraction/execution.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Program-of-Thoughts (intermediate code generation)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Implemented by prompting the LLM to 'think step by step and write python code to solve the problem' and to store the final numeric result in a fixed variable (e.g., 'ans'). When multiple outputs are sampled, outputs are executed and voted over (self-consistency). For multiple-choice tasks, an extra prompt identifies the closest option.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single primary method: code-based reasoning), but often combined with sampling/self-consistency to produce multiple code variants</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>PoT baseline sampled N=16 reasoning/code outputs (matching other baselines) and used voting; compared directly against CoT, SC, and the RM-PoT variant that adds reformulation and few-shot retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, AQuA, SVAMP (same benchmarks as RM-PoT evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>PoT baseline (N=16) reported accuracies: GSM8K 78.9%, AQuA 65.6%, SVAMP 87.1% (as reported in the paper's Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>PoT reduces arithmetic/computation errors relative to pure natural-language CoT by delegating computation to code execution; however, PoT can still fail due to misunderstanding of problem semantics/phrasing (mis-parsing the meaning of 'original price' in an example). Combining PoT with reformulation mitigates such misinterpretations.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>PoT outperforms CoT and SC baselines in these experiments, but adding problem reformulation (RM) and domain-aware few-shot retrieval on top of PoT (RM-PoT) produces further improvements, indicating complementary benefits of reformulation and code-aided reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8274.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8274.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits step-by-step natural-language reasoning traces from LLMs to improve multi-step problem solving.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-4-9B prompted with CoT-style prompts (e.g., 'Let's think step by step') as a baseline; sampling N=16 reasoning traces for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (natural-language stepwise reasoning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>LLM prompted to produce intermediate natural-language reasoning steps leading to the answer. In experiments CoT is run with sampling (N=16) and voting for comparison with self-consistency and PoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>similar (single reasoning style: natural-language stepwise chains), though multiple sampled chains are used in Self-Consistency variants.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>CoT baseline compared to SC (self-consistency over CoT), PoT, and RM-PoT; all compared with total sampled traces N=16 to maintain parity.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, AQuA, SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>CoT (N=16) reported accuracies: GSM8K 75.6%, AQuA 63.2%, SVAMP 86.3% (paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>CoT sometimes produces arithmetic or algebraic rearrangement mistakes in its natural-language steps (examples in paper); these computational errors are reduced when reasoning is expressed as executable code (PoT). CoT also appears more sensitive to surface-form phrasing in some examples.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>CoT improves multi-step reasoning but is outperformed by PoT and by RM-PoT; CoT is susceptible to arithmetic mistakes and misinterpretations tied to specific surface forms, motivating reformulation and code-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8274.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8274.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models use diverse or similar reasoning methods to solve reasoning problems, including details of the reasoning methods, whether multiple or single methods are used, the tasks or benchmarks, performance results, and any explicit comparisons or ablations between diverse and similar reasoning approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple reasoning paths (from the same prompting method) and selects the most consistent answer via majority voting to improve robustness of model outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GLM-4-9B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-4-9B used with self-consistency: sample multiple chains (total N=16) and vote among answers; applied to CoT and PoT baselines and also integrated into RM-PoT voting procedure.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Self-Consistency (sample multiple reasoning traces and majority-vote the final answer)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Multiple reasoning paths are generated per problem; answers from these multiple samples are aggregated by voting to pick the final answer. In RM-PoT the authors sample K reformulations and for each generate N/K reasoning paths, keeping total N fixed, and then vote over all produced answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity</strong></td>
                            <td>diverse (creates multiple varied reasoning paths from the same prompting method); used to compare sampling-based diversity vs reformulation-driven diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_diversity_experimental_setup</strong></td>
                            <td>SC baseline used N=16 sampled paths for voting. The RM-PoT experimental setup keeps N fixed and varies K (number of reformulations) to separate the influence of problem-form diversity from the sheer number of sampled traces used in self-consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>GSM8K, AQuA, SVAMP</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>SC baseline (N=16) reported accuracies: GSM8K 77.3%, AQuA 64.9%, SVAMP 87.6% (paper Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_findings</strong></td>
                            <td>Self-consistency improves over single-run CoT by reducing variance and stabilizing answers; however, diversifying the problem surface form (RM) while keeping total sampled traces fixed produced additional gains beyond SC alone, indicating that reformulation provides complementary diversity not captured by sampling alone.</td>
                        </tr>
                        <tr>
                            <td><strong>explicit_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_claims_or_conclusions</strong></td>
                            <td>Self-consistency (sampling multiple traces and voting) improves robustness over single-chain CoT, but RM-driven diversity (paraphrasing the problem into multiple forms and then applying PoT + voting) yields further improvements even when the total number of traces is held constant, demonstrating that diversity in problem presentation is distinct and beneficial relative to diversity in sampled reasoning traces.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts', 'publication_date_yy_mm': '2025-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Program of thoughts prompting: A framework for large language models to reason through programs. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems. <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8274",
    "paper_id": "paper-276422476",
    "extraction_schema_id": "extraction-schema-152",
    "extracted_data": [
        {
            "name_short": "RM-PoT",
            "name_full": "Reformulating Mathematical Problems and Solving via Program of Thoughts",
            "brief_description": "A three-stage framework that (1) generates multiple paraphrased/problem-reformulated versions of an input math problem to reduce surface-form bias, (2) uses Program-of-Thoughts (PoT) to produce intermediate executable Python code for computation, and (3) injects domain-aware few-shot exemplars and self-consistency voting to determine final answers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GLM-4-9B",
            "model_description": "GLM-4-9B (GLM-4 series, 9B parameter model) used as the base LLM for all experiments; used in zero-shot and few-shot prompting without further training or fine-tuning.",
            "reasoning_methods": [
                "Problem reformulation (paraphrasing) to generate diverse surface forms",
                "Program of Thoughts (PoT): generate intermediate executable Python code",
                "Domain-aware few-shot retrieval (top-5 exemplars by embedding similarity)",
                "Self-consistency / voting over sampled reasoning/code outputs"
            ],
            "reasoning_methods_description": "Problem reformulation: prompt the same LLM to produce K different surface forms of the input problem (naive prompt or in‑context examples). PoT: prompt the LLM to output stepwise reasoning implemented as Python code and store the numeric result in a fixed variable (e.g., 'ans') so answers can be executed/checked. Domain-aware few-shot: classify problem domain, retrieve the top-5 semantically aligned (question, solution) exemplars from a domain bank via sentence‑BERT embeddings and inject them into the prompt. Self-consistency / voting: sample multiple reasoning/code outputs and vote to select the final answer.",
            "reasoning_diversity": "both",
            "reasoning_diversity_experimental_setup": "They vary the number of reformulations K in {1,2,4} while keeping the total number of sampled reasoning paths N fixed at 16 (i.e., for each reformulation sample N/K reasoning paths). This isolates the effect of diversifying problem surface forms versus just increasing the number of sampled reasoning traces. They compare naive reformulation prompts vs in-context reformulation examples, and compare RM-PoT to baselines CoT, SC, and PoT.",
            "task_or_benchmark": "GSM8K (grade-school math problems, ~8.5K), AQuA (algebraic word problems, multiple-choice), SVAMP (1K arithmetic word problems focused on basic ops).",
            "performance_results": "Reported final-voted accuracies (after voting) on GLM-4-9B with total N=16 reasoning paths: CoT: GSM8K 75.6%, AQuA 63.2%, SVAMP 86.3%; SC: GSM8K 77.3%, AQuA 64.9%, SVAMP 87.6%; PoT: GSM8K 78.9%, AQuA 65.6%, SVAMP 87.1%; RM-PoT: GSM8K 80.4%, AQuA 69.1%, SVAMP 88.0%. Ablation: increasing K (more reformulations) generally improves performance up to a point (they report K∈{1,2,4} with improvements as K increases; SVAMP showed best performance at K=2 in one result). Exact table values are provided in the paper's Table 1 and Table 2 (numerics above extracted from those tables).",
            "qualitative_findings": "Reformulating problems into diverse surface forms often increases solve rate and consistency even when some reformulations individually have lower solve rates; exposing the model to varied formulations helps avoid misinterpretations tied to a specific phrasing. PoT (code generation) disentangles computation from reasoning and reduces numerical/arithmetic mistakes, but PoT alone can still suffer from misinterpretation of ambiguous phrasing; combining reformulation with PoT corrects some of these interpretation errors. In-context reformulation examples produce better reformulations than naive paraphrasing prompts. The authors note some randomness and diminishing returns on very simple datasets (e.g., SVAMP).",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Combining diverse problem reformulations with Program-of-Thoughts and domain-aware few-shot exemplars (RM-PoT) yields consistent and statistically significant accuracy gains over Chain-of-Thought, Self-Consistency, and vanilla PoT baselines across GSM8K, AQuA, and SVAMP; fixing total sampled reasoning paths while varying the number of distinct reformulations shows that reformulation-driven diversity (not merely more samples) contributes to gains. In-context reformulation examples further improve performance over naive reformulation prompts.",
            "uuid": "e8274.0",
            "source_info": {
                "paper_title": "RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "PoT",
            "name_full": "Program of Thoughts",
            "brief_description": "A prompting method that directs the LLM to produce intermediate reasoning as executable program code (here, Python), separating symbolic computation from natural-language reasoning and enabling execution of computations.",
            "citation_title": "Program of thoughts prompting: A framework for large language models to reason through programs.",
            "mention_or_use": "use",
            "model_name": "GLM-4-9B",
            "model_description": "Same GLM-4-9B base model used as the PoT baseline; code-generation prompts used to produce python code and store result in a fixed variable for extraction/execution.",
            "reasoning_methods": [
                "Program-of-Thoughts (intermediate code generation)"
            ],
            "reasoning_methods_description": "Implemented by prompting the LLM to 'think step by step and write python code to solve the problem' and to store the final numeric result in a fixed variable (e.g., 'ans'). When multiple outputs are sampled, outputs are executed and voted over (self-consistency). For multiple-choice tasks, an extra prompt identifies the closest option.",
            "reasoning_diversity": "similar (single primary method: code-based reasoning), but often combined with sampling/self-consistency to produce multiple code variants",
            "reasoning_diversity_experimental_setup": "PoT baseline sampled N=16 reasoning/code outputs (matching other baselines) and used voting; compared directly against CoT, SC, and the RM-PoT variant that adds reformulation and few-shot retrieval.",
            "task_or_benchmark": "GSM8K, AQuA, SVAMP (same benchmarks as RM-PoT evaluation).",
            "performance_results": "PoT baseline (N=16) reported accuracies: GSM8K 78.9%, AQuA 65.6%, SVAMP 87.1% (as reported in the paper's Table 1).",
            "qualitative_findings": "PoT reduces arithmetic/computation errors relative to pure natural-language CoT by delegating computation to code execution; however, PoT can still fail due to misunderstanding of problem semantics/phrasing (mis-parsing the meaning of 'original price' in an example). Combining PoT with reformulation mitigates such misinterpretations.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "PoT outperforms CoT and SC baselines in these experiments, but adding problem reformulation (RM) and domain-aware few-shot retrieval on top of PoT (RM-PoT) produces further improvements, indicating complementary benefits of reformulation and code-aided reasoning.",
            "uuid": "e8274.1",
            "source_info": {
                "paper_title": "RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits step-by-step natural-language reasoning traces from LLMs to improve multi-step problem solving.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "GLM-4-9B",
            "model_description": "GLM-4-9B prompted with CoT-style prompts (e.g., 'Let's think step by step') as a baseline; sampling N=16 reasoning traces for comparison.",
            "reasoning_methods": [
                "Chain-of-Thought (natural-language stepwise reasoning)"
            ],
            "reasoning_methods_description": "LLM prompted to produce intermediate natural-language reasoning steps leading to the answer. In experiments CoT is run with sampling (N=16) and voting for comparison with self-consistency and PoT.",
            "reasoning_diversity": "similar (single reasoning style: natural-language stepwise chains), though multiple sampled chains are used in Self-Consistency variants.",
            "reasoning_diversity_experimental_setup": "CoT baseline compared to SC (self-consistency over CoT), PoT, and RM-PoT; all compared with total sampled traces N=16 to maintain parity.",
            "task_or_benchmark": "GSM8K, AQuA, SVAMP",
            "performance_results": "CoT (N=16) reported accuracies: GSM8K 75.6%, AQuA 63.2%, SVAMP 86.3% (paper Table 1).",
            "qualitative_findings": "CoT sometimes produces arithmetic or algebraic rearrangement mistakes in its natural-language steps (examples in paper); these computational errors are reduced when reasoning is expressed as executable code (PoT). CoT also appears more sensitive to surface-form phrasing in some examples.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "CoT improves multi-step reasoning but is outperformed by PoT and by RM-PoT; CoT is susceptible to arithmetic mistakes and misinterpretations tied to specific surface forms, motivating reformulation and code-based approaches.",
            "uuid": "e8274.2",
            "source_info": {
                "paper_title": "RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts",
                "publication_date_yy_mm": "2025-02"
            }
        },
        {
            "name_short": "SC",
            "name_full": "Self-Consistency",
            "brief_description": "A method that samples multiple reasoning paths (from the same prompting method) and selects the most consistent answer via majority voting to improve robustness of model outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GLM-4-9B",
            "model_description": "GLM-4-9B used with self-consistency: sample multiple chains (total N=16) and vote among answers; applied to CoT and PoT baselines and also integrated into RM-PoT voting procedure.",
            "reasoning_methods": [
                "Self-Consistency (sample multiple reasoning traces and majority-vote the final answer)"
            ],
            "reasoning_methods_description": "Multiple reasoning paths are generated per problem; answers from these multiple samples are aggregated by voting to pick the final answer. In RM-PoT the authors sample K reformulations and for each generate N/K reasoning paths, keeping total N fixed, and then vote over all produced answers.",
            "reasoning_diversity": "diverse (creates multiple varied reasoning paths from the same prompting method); used to compare sampling-based diversity vs reformulation-driven diversity.",
            "reasoning_diversity_experimental_setup": "SC baseline used N=16 sampled paths for voting. The RM-PoT experimental setup keeps N fixed and varies K (number of reformulations) to separate the influence of problem-form diversity from the sheer number of sampled traces used in self-consistency.",
            "task_or_benchmark": "GSM8K, AQuA, SVAMP",
            "performance_results": "SC baseline (N=16) reported accuracies: GSM8K 77.3%, AQuA 64.9%, SVAMP 87.6% (paper Table 1).",
            "qualitative_findings": "Self-consistency improves over single-run CoT by reducing variance and stabilizing answers; however, diversifying the problem surface form (RM) while keeping total sampled traces fixed produced additional gains beyond SC alone, indicating that reformulation provides complementary diversity not captured by sampling alone.",
            "explicit_comparison": true,
            "key_claims_or_conclusions": "Self-consistency (sampling multiple traces and voting) improves robustness over single-chain CoT, but RM-driven diversity (paraphrasing the problem into multiple forms and then applying PoT + voting) yields further improvements even when the total number of traces is held constant, demonstrating that diversity in problem presentation is distinct and beneficial relative to diversity in sampled reasoning traces.",
            "uuid": "e8274.3",
            "source_info": {
                "paper_title": "RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts",
                "publication_date_yy_mm": "2025-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Program of thoughts prompting: A framework for large language models to reason through programs.",
            "rating": 2,
            "sanitized_title": "program_of_thoughts_prompting_a_framework_for_large_language_models_to_reason_through_programs"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems.",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset.",
            "rating": 1,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        }
    ],
    "cost": 0.013449,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts</p>
<p>Yu Zhang 
△University of Electronic Science and Technology
China</p>
<p>Shujun Peng 
△University of Electronic Science and Technology
China</p>
<p>Nengwu Wu 
△University of Electronic Science and Technology
China</p>
<p>Xinhan Lin 
△University of Electronic Science and Technology
China</p>
<p>Yang Hu 
△University of Electronic Science and Technology
China</p>
<p>RM-PoT: Reformulating Mathematical Problems and Solving via Program of Thoughts
C3DC8D0FA50FC276F240E16CC2926306
Recently, substantial advancements have been made in training language models to carry out step-by-step reasoning for solving intricate numerical reasoning tasks.Beyond the methods used to solve these problems, the structure and formulation of the problems themselves also play a crucial role in determining the performance of large language models.We observe that even small changes in the surface form of mathematical problems can have a profound impact on both the answer distribution and solve rate.This highlights the vulnerability of LLMs to surface-level variations, revealing its limited robustness when reasoning through complex problems.In this paper, we propose RM-PoT, a three-stage framework that integrates problem reformulation (RM), code-aided reasoning (PoT), and domain-aware few-shot learning to address these limitations.Our approach first reformulates the input problem into diverse surface forms to reduce structural bias, then retrieves five semantically aligned examples from a pre-constructed domain-specific question bank to provide contextual guidance, and finally generates executable Python code for precise computation.</p>
<p>INTRODUCTION</p>
<p>Mathematical reasoning is a cornerstone of problem-solving, with applications spanning diverse fields such as physics, engineering, economics, and computer science.However, despite their success in general natural language processing tasks, existing Large Language Models (LLMs) such as GPT-4 struggle with mathematical problems that demand precision, logical reasoning, and step-by-step computation Hendrycks et al. (2021).This gap arises because LLMs rely heavily on statistical patterns in natural language, which often fail to capture the formal structure and symbolic complexity of mathematical problems.Cobbe et al. (2021).</p>
<p>Current advancements in mathematical reasoning with LLMs have primarily focused on methods like Chain-of-Thought (CoT) prompting Wei et al. (2022); Diao et al. (2023); Zhang et al. (2022), which encourages models to break problems into reasoning steps.Self-Consistency (SC) methods further refine CoT by introducing multiple reasoning paths to identify consistent answers.While effective, these approaches are still limited when faced with problems requiring complex computation or diverse logical forms.As shown in Fig 1(a), the LLM fails to provide the correct answer directly when posed with a complex calculation problem.This limitation is handled by another notable approach, Program of Thoughts (PoT), which introduces intermediate code generation to represent reasoning steps explicitly, improving the interpretability of solutions and disentagle computation from the reasoning process.However, PoT alone cannot resolve inconsistencies arising from ambiguous structured problem forms.As shown in Fig 1(b), we can observe that small changes in the surface form of the mathematical problem can lead to significantly different outcomes.</p>
<p>To address these challenges, we propose RM-PoT, a novel two-stage framework that integrates Reformulation of Mathematical Problems (RM) and Program of Thoughts (PoT).The RM stage prompts the LLM to generate multiple paraphrased versions of the same problem, thereby mitigating the adverse effects of poorly structured problem formulations.By generating multiple reformulations of a given problem and exposing the model to various formulations of the same task, LLMs can uncover the core mathematical structure underlying the problem zho.This diversity in problem presentation improves the LLM's accuracy and consistency in solving the problem.In the PoT stage, , Jie Tang § §Tsinghua University, China; ♭ Shanghai Artificial Intelligence Laboratory, China;
124712948×91274182 = 11383729597806936
Lauren is saving 20% of every paycheck.How many more years does Lauren need to work if she plans to save for a retirement period of 20 years, live with 40% of her current annual salary, and her current salary is $100,000?</p>
<p>If Lauren is saving 20% of her current salary of $100,000, how many additional years does she need to work if she intends to save for a retirement period of 20 years and live off 40% of her current annual salary?</p>
<p>Lauren needs to work an additional 17 years.</p>
<p>Lauren needs to work for 40 years.During the deduction phase, we introduce a novel integration of Few-Shot Learning module to further improve the accuracy and robustness of LLMs in mathematical problem-solving which operates as follows : Given a problem p, we (1) classify its mathematical domain D (e.g., algebra, arithmetic) using a lightweight embedding model, (2) retrieve the top-5 question,solution pairs from D's bank via cosine similarity over sentence-BERT embeddings, and (3) inject these examples into the prompt to prime the LLM with domain-specific reasoning patterns.This process ensures semantic alignment between the input problem and the few-shot exemplars, enhancing both reformulation diversity and solution accuracy.Additionally, we analyze the performance gains from the inclusion of few-shot examples, showing that incorporating these examples allows for more accurate model predictions, even when faced with unfamiliar or highly variable problem structures.</p>
<p>The main contributions of this paper are as follows:</p>
<p>• We introduce RM-PoT, a novel framework that combines surface-level problem reformulation and explicit intermediate code generation to improve mathematical reasoning in LLMs.</p>
<p>• We conduct comprehensive experiments on widely-used benchmarks, including GSM8K, AQuA, and MATH, demonstrating that RM-PoT outperforms baseline approaches across different datasets.</p>
<p>• We provide analysis of the effectiveness of RM and PoT stages, highlighting their contributions to solving mathematical problems.</p>
<p>SPECIALIZED MODELS FOR MATHEMATICAL TASKS</p>
<p>To address the limitations of general-purpose LLMs, specialized approaches have been developed.For instance, MathGPT ?integrates symbolic computation engines to solve algebraic problems, combining language modeling with symbolic reasoning.However, this hybrid approach still depends heavily on the capabilities of external tools.Other models, such as GeoSolver ? and MathQA ?, focus on specific domains like geometry or math question-answering, using domain-specific datasets and tailored architectures.While these models perform well in their respective areas, their narrow focus limits generalization to broader mathematical tasks.</p>
<p>PROBLEM REFORMULATION IN MATHEMATICAL PROBLEM SOLVING</p>
<p>The role of problem reformulation in enhancing the performance of language learning models (LLMs) in mathematical problem solving has gained increasing attention in recent years.Early work by ?? demonstrated that altering the structure and phrasing of a problem can lead to improved reasoning outcomes in LLMs.This line of research has been extended by ?, who explored how different surface forms of mathematical problems can significantly influence the solve rate of LLMs.Further studies have investigated the impact of reformulation strategies such as paraphrasing ?and introducing problem variants ?, suggesting that diverse representations help models better understand and process complex tasks.Building on these insights, recent works have also delved into the combination of reformulation with in-context learning techniques to optimize model performance through adaptive problem presentations.</p>
<p>METHOD</p>
<p>We propose RM-POT, a framework that leverages the potential of large language models (LLMs) to solve mathematical problems.The overall architecture of RM-POT is illustrated in Fig. 2, consisting of two stages.</p>
<p>• Stage I: We reformulate the given mathematical problems into diverse surface forms, enabling the LLM to better grasp the underlying structure of the problems.generates Python code to solve the reformulated problems.This approach not only reveals the reasoning process of the LLM but also separates the computational steps from the reasoning process.</p>
<p>The details of these two stages are described in the following subsections.</p>
<p>REFORMULATE MATHEMATICAL PROBLEMS(RM)</p>
<p>As demonstrated in Section 1, the surface form of a problem significantly influences the performance of LLMs zho.Therefore, we prompt the LLM to generate K different surface forms of the original problem and use a voting mechanism to determine the answer.The intuition behind this approach is that if a problem exhibits a low solve rate and ineffective reasoning paths due to its original surface form, introducing diversity in its surface forms can enhance the likelihood of finding a correct solution.</p>
<p>N</p>
<p>RM PoT&amp;SC Vote</p>
<p>Figure 2: Overview of our proposed RM-PoT framework.It operates in two stages.First, the RM method is employed to reformulate the original problem into various forms using the LLM.Next, the PoT approach is utilized to guide the LLM in generating intermediate code, which not only illustrates the reasoning process but also computes the result.The final answer is then determined through a voting mechanism.</p>
<p>It is important to note that the original problem is reformulated into different forms using the same LLM that is used to solve the problem.This ensures that the improvement in model performance is due to the diversity of the reformulated problem forms, rather than the sharing of knowledge with other LLMs.</p>
<p>In this paper, we explore two ways of prompting LLM to generate reformulated problems.The naive prompt template is "Reformulate the following math problem, try to change the sentence structure of the problem:{input problem}" .The In-Context method begins by identifying effective examples: We conduct experiments to determine which pairs of (Original Problem, Reformulated Problem) exhibit the largest margin in solve rates.Then, we use these examples to enable in-context learning for the LLM, as depicted in Fig. 3.</p>
<p>PROGRAM OF THOUGHTS(POT)</p>
<p>Program of Thoughts (PoT) is a method aimed at enhancing the reasoning capabilities of large language models (LLMs) Chen et al. (2022).It works by decomposing complex tasks into a series of intermediate steps, or "thoughts", which guide the model through a structured reasoning process.Each thought represents a logical step that incrementally leads to the final answer, enabling the model to tackle intricate problems using a step-by-step approach.</p>
<p>This method improves the model's ability to solve tasks that require multi-step reasoning, such as mathematical or logical problems, by fostering transparency in the reasoning process and increasing accuracy in the final result.PoT has proven effective in scenarios where direct answers are difficult, allowing LLMs to perform more reliably in problem-solving tasks.</p>
<p>In our proposed RM-PoT, we aim to instruct the LLM to generate intermediate Python code to solve mathematical problems.This approach can clearly show the reasoning process of LLM and improve accuracy.</p>
<p>SELF-CONSISTENCY(SC) AND VOTING</p>
<p>In our proposed RM-PoT framework, we reformulate the original problems into K different surface forms.Morever, we utilize the Self-Consistency(SC) ?method.Specifically, for each formulated problem, we let LLM generate K reasoning paths, and thus the total number of generated answer is N .We then vote for the final answer.By increasing K, we maintain a fixed total number of reasoning paths N .This approach effectively isolates the effect of diversifying the reasoning paths from merely increasing their number.It also ensures a fair comparison with the self-consistency baselines.</p>
<p>EXPERIMENTS</p>
<p>EXPERIMENTAL SETTINGS</p>
<p>Datasets We evaluate our approach on the following public mathematics reasoning benchmarks:</p>
<p>• GSM8k Cobbe et al. ( 2021) contains 8.5K linguistically diverse grade school-level math questions with moderate difficulties.</p>
<p>• AQuA ?consists of 100K algebraic word problems, including the questions,the possible multiple-choice options, and natural language answer rationales from GMAT and GRE.</p>
<p>• SVAMP ?contains 1K arithmetic word problems.It focuses on basic arithmetic operations such as addition, subtraction, multiplication, and division.</p>
<p>Large Language Model</p>
<p>We use GLM-4-9B ? as our base model.All experiments are conducted in zero-shot or few-shot settings, without training or fine-tuning it.For generation configs, We set the temperature T = 0.7, Top-p= 0.8 and Top-k= 3. The total number of reasoning paths N we sample for each problem is 16.</p>
<p>Implementation Details For problem solving, the PoT process consists of two steps, as illustrated in Fig. 4. First, we prompt the LLM to generate Python code and store the result in a variable with a fixed name, allowing for convenient extraction.If the problem includes options, we instruct the LLM to identify the closest match among the provided options.</p>
<p>EFFECTIVENESS OF REFORMULATING</p>
<p>To verify the effectiveness of RM, we reformulate the selected problems from the AQuA dataset and calculate the solve rate difference between the original problems and their reformulated versions.An example is shown in Fig. 5 to provide readers with an intuitive understanding of the RM process.When the surface form of the original problem is altered, the solve rate increases from 43.8% to reformulate</p>
<p>MAIN RESULTS</p>
<p>In this subsection, we compare the performance of RM-PoT with Chain of Thoughts (CoT), vanilla self-consistency (SC), and vanilla Program of Thoughts(PoT).All results are presented in Table 1.In this evaluation, we apply the naive reformulation of the original problems.the number of reformulated</p>
<p>PoT Prompting</p>
<p>Let's think step by step and write python code to solve the following problem.Store your result as a variable named 'ans':{input problem} Prompt for Choice Options: {options} Among them, find the closest option to {Answer}.</p>
<p>The original price of an item is discounted 22%.A customer buys the item at this discounted price using a $20-off coupon.There is no tax on the item, and this was the only item the customer bought.If the customer paid $1.90 more than half the original price of the item, what was the original price of the item?</p>
<p>Solve Rate:43.8%An item's initial cost has been reduced by 22%.Upon applying a $20 deduction, a customer acquires the item at this revised price.No additional tax is applied to the purchase, which consists solely of this single item.The customer's total payment exceeded half of the item's original price by $1.90.Determine the original cost of the item.</p>
<p>Solve Rate:81.3% problems is set to K = 4, and the total number of reasoning paths is N = 16.For SC, we also maintain N = 16.</p>
<p>The performance metric presented in the table is the accuracy of answers after voting.We can observe that RM-PoT outperforms the other three baselines across all datasets, despite considerable likelihood of generating reformulation with a lower solve rate (see Fig. 6).We assume that this may be because the model demonstrates more consistent understanding of the problems when exposed to various reformulations, thereby avoiding misinterpretations of the original problem in certain specific cases.We will further discuss it in section 5. We observe that as K increases, the performance of the LLM improves as well.This further validates our assumption that exposing the LLM to different surface forms of a problem allows it to better grasp the underlying structure.Notably, on the SVAMP dataset, the LLM performs better when K = 2 than when K = 4.This may be because the problems in this dataset are relatively simple, and the process of reformulating and solving the problems involves some degree of randomness.By comparing the naive reformulation with the In-Context reformulation, we can conclude that the LLM learns the reformulation method more effectively when provided with good examples.</p>
<p>DISCUSSION</p>
<p>In this section, we discuss how RM-PoT helps the LLM solve math problems through the example shown in Fig. 7.When applying CoT, the LLM makes two mistakes.First, there is an error in rearranging the terms of the equation in the second step.econd, the result of 21.90/1.60 is actually 20.660, not 20.741.This demonstrates that the LLMs has certain shortcomings in computation and reasoning.</p>
<p>To disentangle computaion from reasoning process, we then apply the PoT method.We find that the LLM's answer is generally correct, but it misinterprets the original price as the discounted price during the understanding of the problem.However, after reformulating the original problem, the LLM correctly understands the question and provides the correct answer.Although both versions of the problem contain the term 'original price,' the LLM does not fully grasp this in the original formulation.The deeper reasons behind this remain unclear and will be explored in future work.</p>
<p>XFigure 1 :
1
Figure 1: Limitations for GPT-4 solving MWPs.(a) When GPT-4 is asked to directly provide the result of a complex arithmetic problem, the answer is incorrect.(b)When the original problem is reformulated, answers are inconsistent.</p>
<p>Figure 3 :
3
Figure 3: Naive and In-Context Reformulating prompts for LLM.</p>
<p>Figure 4 :
4
Figure 4: Prompts for solving the problem and selecting the correct choice when options are provided.</p>
<p>Figure 5 :
5
Figure 5: An example from AQuA dataset.When the original problem is reformulated, the solve rate improves significantly.</p>
<p>Figure 6 :
6
Figure 6: The solve rate difference between the original problems and the reformulated versions.</p>
<p>Chen et al. (2022)mework, chain-ofthought(CoT) is proposed byWei et al. (2022).Rather than generate the answer directly, it prompts the LLMs to produce a sequence of intermediate reasoning steps.?further extended CoT by self-consistency.They generate multiple reasoning steps from different angles and potentially lead to the same answer.Chen et al. (2022)proposes program-of-thoughts, using intermediate codes to represent the reasoning process.</p>
<p>Wei et al. (2022)48<em>91274182 Calculate 124712948</em>91274182 by Python code The answer is 11383072313508536 by Python dode.√#perform the multiplication using Python Ans = 124712948 * 912741822 RELATED WORK2.1 MATHEMATICAL REASONING IN LLMSIn recent years, numerous studies have explored using the System-2 reasoning approach to solve mathematical problems with LLMsWei et al. (2022); ?); ?.</p>
<p>•</p>
<p>Stage II: We employ the Program of Thoughts Chen et al. (2022) (PoT) approach, which</p>
<p>Table 1 :
1
Comparison of accuracy between different methods.In this subsection, we vary the number of reformulated problems K across {1,2,4}, while keeping the total reasoning paths fixed at N = 16.Additionally, we compare the naive reformulation with the In-Context reformulation, as outlined in Fig 3.The results are shown in Table2.
GSM8K AQuA SVAMPCoT75.663.286.3SC77.364.987.6PoT78.965.687.1RM-PoT80.469.188.04.4 ABLATION STUDY</p>
<p>Table 2 :
2
Comparison of accuracy between different settings of K and reformulation.
K GSM8K AQuA SVAMP178.266.086.9Naive279.867.788.4480.469.188.0178.467.687.1In-Context280.169.489.0480.972.289.6
CONCLUSIONIn this paper, we present RM-PoT++, a framework that advances mathematical reasoning in LLMs through domain-aware few-shot learning, problem reformulation, and Program of Thoughts.By retrieving semantically aligned examples from a pre-constructed question bank, our method primes LLMs to interpret problems consistently, while reformulation and code generation mitigate structural ambiguities and computational errors.Experiments across datasets demonstrate statistically significant accuracy gains and robustness to linguistic variations.However, the underlying reasons why naive reformulation can enhance performance are still unclear.In some cases, the reformulated problem even exhibits a lower solve rate than the original.Future work could explore more effective ways to reformulate problems and integrate fine-tuning methods to further improve the performance of LLMs.At the same time, this method can be improved in the following ways: We can set up the equation:(1-0.22)<em>x -20 = 0.5</em>x + 1.90 1.56<em>x -0.5</em>x = 20 + 1.90 1.06*x = 21.90 x = 21.90/1.06So the answer of this question is 21.90/1.06=20.741X An item's initial cost has been reduced by 22%.Upon applying a $20 deduction, a customer acquires the item at this revised price.No additional tax is applied to the purchase, which consists solely of this single item.The customer's total payment exceeded half of the item's original price by $1.90.Determine the original cost of the item.The original price of an item is discounted 22%.A customer buys the item at this discounted price using a $20-off coupon.There is no tax on the item, and this was the only item the customer bought.If the customer paid $1.90 more than half the original price of the item, what was the original price of the item?The original price of an item is discounted 22%.A customer buys the item at this discounted price using a $20-off coupon.There is no tax on the item, and this was the only item the customer bought.If the customer paid $1.90 more than half the original price of the item, what was the original price of the item?
Self-consistency over paraphrases: Improving the robustness of mathematical reasoning. </p>
<p>Program of thoughts prompting: A framework for large language models to reason through programs. Shunyu Chen, Linjun Zhou, Tianyi Wang, arXiv:2208.148592022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Active prompting with chain-of-thought for large language models. Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang, arXiv:2302.122462023arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Steven Basart, Spencer Zou, Mantas Mazeika, Advances in Neural Information Processing Systems (NeurIPS). 2021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Denny Quoc V Le, Zhou, Advances in neural information processing systems. 202235</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola, arXiv:2210.034932022arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-7432 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-7432</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-7432</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-140.html">extraction-schema-140</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <p><strong>Paper ID:</strong> paper-263828962</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.06680v1.pdf" target="_blank">Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach</a></p>
                <p><strong>Paper Abstract:</strong> While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed. This has been a particular concern in the era of large language models (LLMs)- based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code. Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency. Inspired by the recent progress in causality analysis and its application in software engineering, this paper launches a causality analysis-based approach to systematically analyze the causal relations between the LLM input prompts and the generated code. To handle various technical challenges in this study, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts. The formed causal graph is then used to identify the causal relations between the prompt and the derived code. We illustrate the insights that our framework can provide by studying over 3 popular LLMs with over 12 prompt adjustment strategies. The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness, and aid end-users in understanding predictions. Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e7432.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e7432.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fig1_motivating_example</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Motivating example: semantically-equivalent prompt wording causing different code outputs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concrete example showing that two semantically equivalent natural-language problem statements (one ambiguous/poorly phrased, one explicit) produce different programs: one that capitalizes initial letters vs one that returns first letter of each word, demonstrating that subtle prompt wording/format can change LLM code-generation behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>unspecified LLM (motivating example)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Not specified in the figure; used to illustrate sensitivity to wording/format of prompts rather than to benchmark a particular model.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>unknown</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-to-code / small programming problem (string manipulation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate a Python program implementing the described string-processing function (return first letter of each word).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language programming prompt (two variants: ambiguous/unintelligible vs explicit instruction)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / wording</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Two semantically equivalent prompts differing in clarity/explicitness; no few-shot examples; single natural-language description fed to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>qualitative correctness of generated program (functional behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>ambiguous prompt -> incorrect program (capitalizes first letters); explicit prompt -> correct program (returns first letter of each word)</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>qualitative change from incorrect to correct when prompt made explicit (no numeric value reported)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>motivating illustrative example presented in paper (no model/hyperparameter details provided)</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7432.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e7432.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pilot_rephrase_Table1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pilot study on rephrasing instructions (Short / Long / Formal / Control)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Small-scale pilot where prompts were rephrased by ChatGPT according to simple instructions (make shorter, longer, more formal, or control) and the generated code metrics (pass rate, syntax error rate, stability) were compared to the original prompt, showing rephrasing affects generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>rephrasing: GPT-3.5-Turbo (ChatGPT); code generation model not explicitly named in pilot table</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Rephraser is GPT-3.5-Turbo; pilot uses rephrased prompts to elicit code from an LLM (not explicitly identified in table).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-3.5-Turbo: model family (size not stated)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Text-to-code for small set of programming problems (pilot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Measure how simple rephrasing intentions applied to programming problem prompts change code-generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language programming prompt rephrased under instruction labels (Short, Long, Formal, Control)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / rephrase instruction</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Rephrasing instructions applied via meta-prompt to rephraser LLM (make it shorter/longer/formal); then feed rephrased prompts into code-generation LLM; metrics compared to original prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Pass rate, Syntax error rate, Stability (fraction of identical outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported deltas vs original: Short: pass rate -0.0029, syntax error +0.0090, stability +0.0534; Long: pass rate +0.0095, syntax error -0.0013, stability -0.0184; Formal: pass rate +0.0061, syntax error -0.0067, stability +0.0011; Control (N/A): 0.0000, +0.0026, +0.0290</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original prompt (baseline) used as reference; pilot reports deltas relative to original</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Small absolute changes reported (see performance_value); directions vary by instruction (Short increased stability notably; Long decreased stability)</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Pilot using ChatGPT to rephrase prompts; small-scale — table shows averaged deltas; no sample size explicitly stated for pilot</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7432.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e7432.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Instruction_group_effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of instruction-type rephrasing (Short / Long / Formal / Fluent / Technical / Logical) on code metrics</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Systematic causal analysis across three models found consistent directional effects for instruction-type rephrasings: Long and Formal tend to decrease similarity to ground-truth code while Short and Fluent tend to increase it; different instruction tags affect different code metrics per model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based LLMs evaluated for text-to-code on APPS: GPT-Neo (open pretrained), GPT-3.5-Turbo and GPT-4 (ChatGPT series via Azure API); rephrasing performed by GPT-3.5-Turbo.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: not specified in paper (GPT family, larger)</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APPS Python text-to-code generation (6K sampled problems)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate Python3 code from natural-language problem statements and measure multiple code-quality metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language programming prompt; rephrased according to instruction labels (Short, Long, Formal, Fluent, Technical, Logical, etc.) via meta-prompt</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / instruction-type prompt</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Meta-prompt template combining instruction flags; rephraser (GPT-3.5-Turbo) generates rephrased prompts; models generate 3 solutions each (n=3); max output length set to 2000 tokens; default other API parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Similarity metrics (CodeBLEU, BLEU), pass rate, syntax error rate, timeout rate, readability, security counts, diversity</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Qualitative trends: Long and Formal generally correlated with decreased similarity to ground truth; Short and Fluent correlated with increased similarity and sometimes higher pass rates. Exact numeric effects vary by model and code metric (reported in causal graphs and Fig.7).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original prompt (baseline) used for comparison in causal ATE computations; paper reports ATE directions per meta-prompt variable rather than single-number baselines for all metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Directional: e.g., Long and Formal → negative effect on similarity; Short and Fluent → positive effect; no universal absolute percentage provided across all metrics (ATE computed per metric in analyses).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>Dataset: APPS; 6K data points sampled; rephrasing by GPT-3.5-Turbo; code generation by each model separately; causal discovery (DiBS) and causal inference (DML via EconML) to compute ATEs.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7432.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e7432.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Role_scenario_effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Effects of role-playing and scenario-setting rephrasing on model performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Causal analysis found that role and scenario rephrasing intentions (e.g., 'as a student', 'in a programming competition') tend to have broadly positive effects on code-generation metrics for the larger GPT-series models (GPT-3.5-Turbo and GPT-4), whereas the tiny GPT-Neo does not benefit and may perform poorly.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same set of transformer LLMs evaluated on APPS; role/scenario instructions provided via meta-prompt to rephraser.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: sizes not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APPS Python text-to-code generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Assess whether adding role or scenario context in prompt rephrasing changes code-generation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language programming prompt with role and scenario tags injected during rephrasing (role: Student/Programmer/Competitor/Researcher/Teacher/Engineer; scenario: e.g., clearer/specify/partner)</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt style / contextual framing</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Meta-prompt includes role and scenario options which are combined with instruction flags; rephraser (GPT-3.5-Turbo) generates variants; causal ATE computed per role/scenario variable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Multiple code metrics (similarity, pass rate, etc.) aggregated in causal analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported counts: role+scenario groups achieved positive effects in 13/18 cases for GPT-3.5-Turbo and 15/18 for GPT-4 (i.e., majority positive), while GPT-Neo showed poor responsiveness to these groups.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original prompt baseline; effects reported as positive/negative ATE per code metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Majority positive for GPT-3.5 and GPT-4 (as above); no absolute percentage change universally reported in text.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>APPS dataset, 6K samples, rephrasing by GPT-3.5-Turbo, causal discovery/inference pipeline (DiBS + DML).</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7432.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e7432.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model_sensitivity_overfitting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model sensitivity differences and apparent overfitting of GPT-Neo versus GPT-3.5/GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed that GPT-Neo shows 'stickiness' to ground-truth solutions and low variance under prompt changes (interpreted as overfitting to APPS), whereas GPT-3.5-Turbo and GPT-4 are more sensitive to prompt rephrasings; evidence includes high similarity metrics and predictive R^2 patterns on learned causal graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GPT-Neo (open pre-trained 2.7B) appears to memorize training snippets from APPS; GPT-3.5/GPT-4 trained with RLHF and more robust few-shot behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: unspecified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APPS Python text-to-code generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Compare how prompt rephrasings propagate to code metrics across models; check variance and responsiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language programming prompt; rephrased variants generated by GPT-3.5-Turbo; models produce outputs for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>model sensitivity to prompt presentation</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Measured using causal graph predictive evaluation (R^2, MSE) and by inspecting variance of similarity-based metrics; rephrasings applied across instruction/role/scenario groups.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>R^2 and MSE of causal-graph-based predictions; similarity metrics (CodeBLEU, BLEU); semgrep count (security issues) variance noted</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GPT-Neo: very high R^2 on similarity metrics (e.g., gold sim CodeBLEU R^2=0.9221) and semgrep count R^2=1.0 with MSE=0.0 (zero detected security issues) — interpreted as low variance/stickiness; GPT-3.5/GPT-4 show different R^2 patterns and are more affected by prompt changes.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Descriptive: GPT-Neo changes less in response to prompt rephrasings compared to GPT-3.5/GPT-4; exact ATEs vary per metric.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>APPS dataset; causal graph verification reported in Table 4 (R^2 and MSE per metric); 6K data points.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e7432.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e7432.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of a problem/prompt influences the performance of large language models, including details of the format, the task, the model, and reported performance differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Genetic_prompt_optimization_Table5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompt optimization via genetic algorithm guided by causal-surrogate fitness (Table 5 results)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A downstream application: use the learned causal graphs as a surrogate fitness function in a genetic search over rephrasing-instruction bitstrings; evaluated against the best single rephrasing instruction and the original prompt, showing improved BLEU/CodeBLEU-like similarity in most cases (except GPT-Neo).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same code-generation models as above; rephrasing instructions encoded as binary vectors (genetic algorithm operates in instruction space), surrogate fitness estimated from causal graph.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: unspecified</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>APPS Python text-to-code generation (optimization objective: BLEU / Code similarity)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Search for rephrasing-intent combinations that maximize an objective metric (BLEU/CodeBLEU) using a causal-graph-derived surrogate to avoid expensive code generation per candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Natural-language programming prompt rephrased by selected instruction combinations (binary vector representation); surrogate evaluation instead of full code generation during search.</td>
                        </tr>
                        <tr>
                            <td><strong>format_category</strong></td>
                            <td>prompt optimization / prompt style selection</td>
                        </tr>
                        <tr>
                            <td><strong>format_details</strong></td>
                            <td>Genetic algorithm over binary vectors representing instruction selection (e.g., 'make it short' bit); fitness computed by expected metric from causal graph; final candidates evaluated with actual code generation and BLEU/CodeBLEU.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>gold sim (CodeBLEU-like), mut sim (mutual similarity), BLEU surrogate -> final BLEU/CodeBLEU</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported (Table 5):
- GPT-Neo: gold sim baseline 0.7071; Single rephrase delta -0.1367; Ours delta -0.0965 (both reduced similarity, Ours less negative); mut sim baseline 0.8030; Single +0.0338; Ours +0.0704.
- GPT-3.5-Turbo: gold sim baseline 0.5014; Single +0.0239; Ours +0.0531. mut sim baseline 0.5072; Single +0.0165; Ours +0.0297.
- GPT-4: gold sim baseline 0.4867; Single +0.0454; Ours +0.0753. mut sim baseline 0.4971; Single +0.0132; Ours +0.0362.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Original prompt baseline values shown per model (see performance_value); Single = best single rephrasing instruction; Ours = genetic search with causal surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_change</strong></td>
                            <td>Reported deltas: Ours typically outperforms Single and Original for GPT-3.5-Turbo and GPT-4 (absolute improvements in gold sim and mut sim); GPT-Neo shows mixed results (overfitting suspected). See performance_value for numeric deltas.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_setting</strong></td>
                            <td>APPS; rephrasing intentions refined by GPT-4; genetic algorithm over instruction-bit vectors; surrogate fitness from causal graph; final evaluation using BLEU/CodeBLEU; generation: 3 solutions per prompt, max length 2000 tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Language models are few-shot learners <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code <em>(Rating: 2)</em></li>
                <li>Towards robust nlg bias evaluation with syntactically-diverse prompts <em>(Rating: 2)</em></li>
                <li>Evaluating the robustness of discrete prompts <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-7432",
    "paper_id": "paper-263828962",
    "extraction_schema_id": "extraction-schema-140",
    "extracted_data": [
        {
            "name_short": "Fig1_motivating_example",
            "name_full": "Motivating example: semantically-equivalent prompt wording causing different code outputs",
            "brief_description": "A concrete example showing that two semantically equivalent natural-language problem statements (one ambiguous/poorly phrased, one explicit) produce different programs: one that capitalizes initial letters vs one that returns first letter of each word, demonstrating that subtle prompt wording/format can change LLM code-generation behavior.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "unspecified LLM (motivating example)",
            "model_description": "Not specified in the figure; used to illustrate sensitivity to wording/format of prompts rather than to benchmark a particular model.",
            "model_size": "unknown",
            "task_name": "Text-to-code / small programming problem (string manipulation)",
            "task_description": "Generate a Python program implementing the described string-processing function (return first letter of each word).",
            "problem_format": "Natural-language programming prompt (two variants: ambiguous/unintelligible vs explicit instruction)",
            "format_category": "prompt style / wording",
            "format_details": "Two semantically equivalent prompts differing in clarity/explicitness; no few-shot examples; single natural-language description fed to the model.",
            "performance_metric": "qualitative correctness of generated program (functional behavior)",
            "performance_value": "ambiguous prompt -&gt; incorrect program (capitalizes first letters); explicit prompt -&gt; correct program (returns first letter of each word)",
            "baseline_performance": null,
            "performance_change": "qualitative change from incorrect to correct when prompt made explicit (no numeric value reported)",
            "experimental_setting": "motivating illustrative example presented in paper (no model/hyperparameter details provided)",
            "statistical_significance": null,
            "uuid": "e7432.0"
        },
        {
            "name_short": "Pilot_rephrase_Table1",
            "name_full": "Pilot study on rephrasing instructions (Short / Long / Formal / Control)",
            "brief_description": "Small-scale pilot where prompts were rephrased by ChatGPT according to simple instructions (make shorter, longer, more formal, or control) and the generated code metrics (pass rate, syntax error rate, stability) were compared to the original prompt, showing rephrasing affects generated code.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "rephrasing: GPT-3.5-Turbo (ChatGPT); code generation model not explicitly named in pilot table",
            "model_description": "Rephraser is GPT-3.5-Turbo; pilot uses rephrased prompts to elicit code from an LLM (not explicitly identified in table).",
            "model_size": "GPT-3.5-Turbo: model family (size not stated)",
            "task_name": "Text-to-code for small set of programming problems (pilot)",
            "task_description": "Measure how simple rephrasing intentions applied to programming problem prompts change code-generation metrics.",
            "problem_format": "Natural-language programming prompt rephrased under instruction labels (Short, Long, Formal, Control)",
            "format_category": "prompt style / rephrase instruction",
            "format_details": "Rephrasing instructions applied via meta-prompt to rephraser LLM (make it shorter/longer/formal); then feed rephrased prompts into code-generation LLM; metrics compared to original prompt.",
            "performance_metric": "Pass rate, Syntax error rate, Stability (fraction of identical outputs)",
            "performance_value": "Reported deltas vs original: Short: pass rate -0.0029, syntax error +0.0090, stability +0.0534; Long: pass rate +0.0095, syntax error -0.0013, stability -0.0184; Formal: pass rate +0.0061, syntax error -0.0067, stability +0.0011; Control (N/A): 0.0000, +0.0026, +0.0290",
            "baseline_performance": "Original prompt (baseline) used as reference; pilot reports deltas relative to original",
            "performance_change": "Small absolute changes reported (see performance_value); directions vary by instruction (Short increased stability notably; Long decreased stability)",
            "experimental_setting": "Pilot using ChatGPT to rephrase prompts; small-scale — table shows averaged deltas; no sample size explicitly stated for pilot",
            "statistical_significance": null,
            "uuid": "e7432.1"
        },
        {
            "name_short": "Instruction_group_effects",
            "name_full": "Effects of instruction-type rephrasing (Short / Long / Formal / Fluent / Technical / Logical) on code metrics",
            "brief_description": "Systematic causal analysis across three models found consistent directional effects for instruction-type rephrasings: Long and Formal tend to decrease similarity to ground-truth code while Short and Fluent tend to increase it; different instruction tags affect different code metrics per model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4",
            "model_description": "Transformer-based LLMs evaluated for text-to-code on APPS: GPT-Neo (open pretrained), GPT-3.5-Turbo and GPT-4 (ChatGPT series via Azure API); rephrasing performed by GPT-3.5-Turbo.",
            "model_size": "GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: not specified in paper (GPT family, larger)",
            "task_name": "APPS Python text-to-code generation (6K sampled problems)",
            "task_description": "Generate Python3 code from natural-language problem statements and measure multiple code-quality metrics.",
            "problem_format": "Natural-language programming prompt; rephrased according to instruction labels (Short, Long, Formal, Fluent, Technical, Logical, etc.) via meta-prompt",
            "format_category": "prompt style / instruction-type prompt",
            "format_details": "Meta-prompt template combining instruction flags; rephraser (GPT-3.5-Turbo) generates rephrased prompts; models generate 3 solutions each (n=3); max output length set to 2000 tokens; default other API parameters.",
            "performance_metric": "Similarity metrics (CodeBLEU, BLEU), pass rate, syntax error rate, timeout rate, readability, security counts, diversity",
            "performance_value": "Qualitative trends: Long and Formal generally correlated with decreased similarity to ground truth; Short and Fluent correlated with increased similarity and sometimes higher pass rates. Exact numeric effects vary by model and code metric (reported in causal graphs and Fig.7).",
            "baseline_performance": "Original prompt (baseline) used for comparison in causal ATE computations; paper reports ATE directions per meta-prompt variable rather than single-number baselines for all metrics.",
            "performance_change": "Directional: e.g., Long and Formal → negative effect on similarity; Short and Fluent → positive effect; no universal absolute percentage provided across all metrics (ATE computed per metric in analyses).",
            "experimental_setting": "Dataset: APPS; 6K data points sampled; rephrasing by GPT-3.5-Turbo; code generation by each model separately; causal discovery (DiBS) and causal inference (DML via EconML) to compute ATEs.",
            "statistical_significance": null,
            "uuid": "e7432.2"
        },
        {
            "name_short": "Role_scenario_effects",
            "name_full": "Effects of role-playing and scenario-setting rephrasing on model performance",
            "brief_description": "Causal analysis found that role and scenario rephrasing intentions (e.g., 'as a student', 'in a programming competition') tend to have broadly positive effects on code-generation metrics for the larger GPT-series models (GPT-3.5-Turbo and GPT-4), whereas the tiny GPT-Neo does not benefit and may perform poorly.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4",
            "model_description": "Same set of transformer LLMs evaluated on APPS; role/scenario instructions provided via meta-prompt to rephraser.",
            "model_size": "GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: sizes not specified in paper",
            "task_name": "APPS Python text-to-code generation",
            "task_description": "Assess whether adding role or scenario context in prompt rephrasing changes code-generation outcomes.",
            "problem_format": "Natural-language programming prompt with role and scenario tags injected during rephrasing (role: Student/Programmer/Competitor/Researcher/Teacher/Engineer; scenario: e.g., clearer/specify/partner)",
            "format_category": "prompt style / contextual framing",
            "format_details": "Meta-prompt includes role and scenario options which are combined with instruction flags; rephraser (GPT-3.5-Turbo) generates variants; causal ATE computed per role/scenario variable.",
            "performance_metric": "Multiple code metrics (similarity, pass rate, etc.) aggregated in causal analysis",
            "performance_value": "Reported counts: role+scenario groups achieved positive effects in 13/18 cases for GPT-3.5-Turbo and 15/18 for GPT-4 (i.e., majority positive), while GPT-Neo showed poor responsiveness to these groups.",
            "baseline_performance": "Original prompt baseline; effects reported as positive/negative ATE per code metric.",
            "performance_change": "Majority positive for GPT-3.5 and GPT-4 (as above); no absolute percentage change universally reported in text.",
            "experimental_setting": "APPS dataset, 6K samples, rephrasing by GPT-3.5-Turbo, causal discovery/inference pipeline (DiBS + DML).",
            "statistical_significance": null,
            "uuid": "e7432.3"
        },
        {
            "name_short": "Model_sensitivity_overfitting",
            "name_full": "Model sensitivity differences and apparent overfitting of GPT-Neo versus GPT-3.5/GPT-4",
            "brief_description": "Observed that GPT-Neo shows 'stickiness' to ground-truth solutions and low variance under prompt changes (interpreted as overfitting to APPS), whereas GPT-3.5-Turbo and GPT-4 are more sensitive to prompt rephrasings; evidence includes high similarity metrics and predictive R^2 patterns on learned causal graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4",
            "model_description": "GPT-Neo (open pre-trained 2.7B) appears to memorize training snippets from APPS; GPT-3.5/GPT-4 trained with RLHF and more robust few-shot behavior.",
            "model_size": "GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: unspecified in paper",
            "task_name": "APPS Python text-to-code generation",
            "task_description": "Compare how prompt rephrasings propagate to code metrics across models; check variance and responsiveness.",
            "problem_format": "Natural-language programming prompt; rephrased variants generated by GPT-3.5-Turbo; models produce outputs for comparison.",
            "format_category": "model sensitivity to prompt presentation",
            "format_details": "Measured using causal graph predictive evaluation (R^2, MSE) and by inspecting variance of similarity-based metrics; rephrasings applied across instruction/role/scenario groups.",
            "performance_metric": "R^2 and MSE of causal-graph-based predictions; similarity metrics (CodeBLEU, BLEU); semgrep count (security issues) variance noted",
            "performance_value": "GPT-Neo: very high R^2 on similarity metrics (e.g., gold sim CodeBLEU R^2=0.9221) and semgrep count R^2=1.0 with MSE=0.0 (zero detected security issues) — interpreted as low variance/stickiness; GPT-3.5/GPT-4 show different R^2 patterns and are more affected by prompt changes.",
            "baseline_performance": null,
            "performance_change": "Descriptive: GPT-Neo changes less in response to prompt rephrasings compared to GPT-3.5/GPT-4; exact ATEs vary per metric.",
            "experimental_setting": "APPS dataset; causal graph verification reported in Table 4 (R^2 and MSE per metric); 6K data points.",
            "statistical_significance": null,
            "uuid": "e7432.4"
        },
        {
            "name_short": "Genetic_prompt_optimization_Table5",
            "name_full": "Prompt optimization via genetic algorithm guided by causal-surrogate fitness (Table 5 results)",
            "brief_description": "A downstream application: use the learned causal graphs as a surrogate fitness function in a genetic search over rephrasing-instruction bitstrings; evaluated against the best single rephrasing instruction and the original prompt, showing improved BLEU/CodeBLEU-like similarity in most cases (except GPT-Neo).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo (2.7B), GPT-3.5-Turbo (ChatGPT), GPT-4",
            "model_description": "Same code-generation models as above; rephrasing instructions encoded as binary vectors (genetic algorithm operates in instruction space), surrogate fitness estimated from causal graph.",
            "model_size": "GPT-Neo: 2.7B; GPT-3.5-Turbo / GPT-4: unspecified",
            "task_name": "APPS Python text-to-code generation (optimization objective: BLEU / Code similarity)",
            "task_description": "Search for rephrasing-intent combinations that maximize an objective metric (BLEU/CodeBLEU) using a causal-graph-derived surrogate to avoid expensive code generation per candidate.",
            "problem_format": "Natural-language programming prompt rephrased by selected instruction combinations (binary vector representation); surrogate evaluation instead of full code generation during search.",
            "format_category": "prompt optimization / prompt style selection",
            "format_details": "Genetic algorithm over binary vectors representing instruction selection (e.g., 'make it short' bit); fitness computed by expected metric from causal graph; final candidates evaluated with actual code generation and BLEU/CodeBLEU.",
            "performance_metric": "gold sim (CodeBLEU-like), mut sim (mutual similarity), BLEU surrogate -&gt; final BLEU/CodeBLEU",
            "performance_value": "Reported (Table 5):\n- GPT-Neo: gold sim baseline 0.7071; Single rephrase delta -0.1367; Ours delta -0.0965 (both reduced similarity, Ours less negative); mut sim baseline 0.8030; Single +0.0338; Ours +0.0704.\n- GPT-3.5-Turbo: gold sim baseline 0.5014; Single +0.0239; Ours +0.0531. mut sim baseline 0.5072; Single +0.0165; Ours +0.0297.\n- GPT-4: gold sim baseline 0.4867; Single +0.0454; Ours +0.0753. mut sim baseline 0.4971; Single +0.0132; Ours +0.0362.",
            "baseline_performance": "Original prompt baseline values shown per model (see performance_value); Single = best single rephrasing instruction; Ours = genetic search with causal surrogate.",
            "performance_change": "Reported deltas: Ours typically outperforms Single and Original for GPT-3.5-Turbo and GPT-4 (absolute improvements in gold sim and mut sim); GPT-Neo shows mixed results (overfitting suspected). See performance_value for numeric deltas.",
            "experimental_setting": "APPS; rephrasing intentions refined by GPT-4; genetic algorithm over instruction-bit vectors; surrogate fitness from causal graph; final evaluation using BLEU/CodeBLEU; generation: 3 solutions per prompt, max length 2000 tokens.",
            "statistical_significance": null,
            "uuid": "e7432.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 2,
            "sanitized_title": "language_models_are_fewshot_learners"
        },
        {
            "paper_title": "Evaluating large language models trained on code",
            "rating": 2,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "Towards robust nlg bias evaluation with syntactically-diverse prompts",
            "rating": 2,
            "sanitized_title": "towards_robust_nlg_bias_evaluation_with_syntacticallydiverse_prompts"
        },
        {
            "paper_title": "Evaluating the robustness of discrete prompts",
            "rating": 2,
            "sanitized_title": "evaluating_the_robustness_of_discrete_prompts"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 1,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        }
    ],
    "cost": 0.016961999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach
10 Oct 2023</p>
<p>Zhenlan Ji 
Hong Kong University of Science and Technology Hong Kong
China</p>
<p>Pingchuan Ma 
Hong Kong University of Science and Technology Hong Kong
China</p>
<p>Zongjie Li 
Hong Kong University of Science and Technology Hong Kong
China</p>
<p>Shuai Wang shuaiw@cse.ust.hk 
Hong Kong University of Science and Technology Hong Kong
China</p>
<p>Benchmarking and Explaining Large Language Model-based Code Generation: A Causality-Centric Approach
10 Oct 2023E2FAC4CA0E528FE0F091B5BF0C4FE174arXiv:2310.06680v1[cs.SE]CausalityLarge Language ModelsCode GenerationExplainability
While code generation has been widely used in various software development scenarios, the quality of the generated code is not guaranteed.This has been a particular concern in the era of large language models (LLMs)-based code generation, where LLMs, deemed a complex and powerful black-box model, is instructed by a high-level natural language specification, namely a prompt, to generate code.Nevertheless, effectively evaluating and explaining the code generation capability of LLMs is inherently challenging, given the complexity of LLMs and the lack of transparency.Inspired by the recent progress in causality analysis and its application in software engineering, this paper launches a causality analysis-based approach to systematically analyze the causal relations between the LLM input prompts and the generated code.To handle various technical challenges in this study, we first propose a novel causal graph-based representation of the prompt and the generated code, which is established over the fine-grained, human-understandable concepts in the input prompts.The formed causal graph is then used to identify the causal relations between the prompt and the derived code.We illustrate the insights that our framework can provide by studying over 3 popular LLMs with over 12 prompt adjustment strategies.The results of these studies illustrate the potential of our technique to provide insights into LLM effectiveness, and aid end-users in understanding predictions.Additionally, we demonstrate that our approach provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.</p>
<p>Introduction</p>
<p>Code generation serves as one of the most central problems in automated software engineering.It enables machines to program automatically to satisfy human intent expressed in the form of some specification, usually in the form of natural language.The problem of code generation has been studied for decades, and has been applied as the basis of many different important domains, such as program synthesis, program repair, and fuzz testing.In recent years, code generation has achieved great progress in both academia and industry.In particular, we observe that large language models (LLMs) have been applied to support code generation, and have achieved impressive results.For example, GPT-4 has been applied to generate code from natural language, whose coding ability is comparable to that of humans [49].To date, many LLMs are already seamlessly integrated into the developer's IDE for commercial usage, like GitHub Copilot, Amazon CodeWhisperer, and Tabnine.</p>
<p>Despite the great progress, we notice that the quality of the generated code, especially in the era of LLMs-based code generation, fluctuates under the variation of natural language specifications.For instance, as will be shown shortly, the way of expressing the same intent in natural language (i.e., prompt) can lead to significantly different code outputs.As a result, the behavior of LLMs-based code generation is opaque and uninterruptible, which hinders the broader adoption of LLMs-based code generation in practice.For instance, it has been reported that GitHub Copilot generates code is highly unstable [40] or even generates code containing security vulnerabilities [52].</p>
<p>To gauge the effectiveness of our approach, we conduct extensive experiments using three representative LLMs, GPT-Neo [28], GPT-3.5-Turbo(ChatGPT) [5], and GPT-4.This empirical study enables a comprehensive analysis of the trade-offs and leads to a number of intriguing findings over LLM prompts and code outputs.First, as expected, the usage of certain keywords/concepts may notablely affect the pattern of produced code output, highlighting the need for a systematic and automated approach to deciding optimal concept/keyword usage for particular scenarios.Second, certain keywords, such as Fluent and Formal, deliver visible "tradeoffs" in the generated code, which may become a practical obstacle for developers to tune the prompt.Moreover, the reaction of LLMs to prompts with certain properties may be unreasonable, revealing some severe problems in the LLMs (e.g., overfitting to the training data).This observation highlights the importance of taking both human-understandable and other subtle factors into account when calibrating the LLM prompt.Last, we demonstrate a versatile and important application of our framework to improve prompts.With empirical assessment, we show the selected prompts offer superior code generation quality compared to the original prompt, illustrating the potential of our approach in assisting developers in refining the prompt.To conclude, this paper makes the following contributions:</p>
<p>• Given LLM-based code generation, a cornerstone and vastly-used task in software engineering, this paper for the first time uses causality analysis as a principled approach to analyzing how prompts affect the LLM-based code generation process.• Technically, we propose a set of domain-specific design considerations to enable accurate and comprehensive causality analysis in the context of LLM-based code generation.We also design a novel approach to identify optimal prompts that facilitate instructing LLMs to generate high-quality code.• We conduct extensive evaluations over datasets and different LLMs, and we obtain actionable suggestions for users to understand and enhance the quality of the LLM-based generated code.</p>
<p>Code and Data.We provide the code, data, and other supplementary materials of this research at [7].We will maintain the released artifacts to ensure reproducibility and facilitate future research.</p>
<p>Preliminary and Motivation</p>
<p>2.1 LLM and Prompt Engineering LLM.In recent years, LLMs have experienced a surge in popularity and adoption across various scenarios owing to their promising performance and high flexiblity on a diverse range of tasks.LLMs are typically trained on large corpora of text data using self-supervised learning, and can be fine-tuned on specific downstream tasks with only a few examples (see below).To date, LLMs like ChatGPT [5], which contain over 100 billion parameters, have demonstrated strong capabilities on tasks such as language translation [33], neuron explanation [12], and even clinical decision [62].</p>
<p>Prompt Engineering.One of the key factors that contribute to the success of LLMs is the input prompt, which is a text or template that provides task-specific knowledge to the model.The prompt is typically concatenated with the input text to form the final input to the model.In the task of code generation, the prompt usually refers to a textual or templated representation that encompasses the high-level specifications of the desired code.Conventional ML models are mainly employed following the "pre-train &amp; fine-tune" paradigm which entails training models on general tasks and subsequently fine-tuning them for specific downstream tasks.In contrast, scaled LLMs exhibit properties amenable to few-shot learning, where prompts can strongly steer model towards producing answers for desired tasks.This facilitates an era of "pre-train &amp; prompt," with properly designed prompts becoming the key to elicit the full potential of LLMs.To date, various methods that improve LLM performance via prompts have been proposed, including few-shot learning [13,59], chain-of-thought [71,75], tree-of-thought [79], and debate LLM [19].Consequently, the design and selection of prompts have rapidly gained importance, spearheading a new research area dubbed "prompt engineering" in LLM research [41].</p>
<p>Code Generation</p>
<p>Code generation, also known as program synthesis, is a fundamental software engineering technique [26].Early code generation task was typically formulated as a search-based problem [25,46,63], with the search space constructed by constraints.These constraints are derived from the high-level specification and the target language grammar.Although these approaches can handle simple tasks, their low generalizability prevents them from being applicable in real-world circumstances.Deep learning has led to novel neural network-based code generation methods.The use of various neural architectures such as recurrent neural networks (RNNs) [80], convolutional neural networks (CNNs) [66], and transformers [28] has achieved promising results in code generation tasks, including text-to-code and code-to-code [44].This paper focuses on generating code from natural language text [28,39], with Python3 as the target programming language for its high popularity [73,36,21].Our approach, however, is generalizable to other settings as shown in Sec. 8.</p>
<p>Recent years have witnessed a rising interest in applying LLMs to code generation [74,40,9].Although code generation with LLMs also benefits from prompt engineering, the output quality is prone to being compromised by the unstable nature of prompts (see example in Fig. 1).Moreover, unlike humans who are capable of correctly comprehending statements with subtle errors, the low tolerance of compilers and operating systems for bugs and performance issues further exacerbates the problem of prompt instability.In addition, the complexity of the natural language text that constitutes the prompt makes it inherently difficult to understand which characteristics (e.g., the average length of sentences in the prompt) of the prompt contribute to the output quality.All of these issues motivate us to investigate the relations between the prompt and the output quality of LLMs for code generation.</p>
<p>Causality Analysis</p>
<p>As a canonical technique that is extensively applied to analyze complex software systems [65,82,20], causality analysis is proficient at disentangling the complex relations between various factors into an intuitive causal graph with high interpretability.Typically, causality analysis comprises two phases: causal discovery and analysis based on the formed causal graph.</p>
<p>Causal Discovery.Causal discovery seeks to infer causal relations between variables from observational data.These inferred causal relations are often represented as a directed acyclic graph (DAG), referred to as Causal Graph.Formally, Definition 1 (Causal Graph).A causal graph is a DAG with nodes V and edges E, i.e., G = (V, E), where each node X (X ∈ V) represents a random variable and each edge X → Y (X, Y ∈ V) represents a directed causal relation from X to Y. Pa G (X) denotes the parent nodes of X in G, i.e., Pa G (X) = {N|N → X ∈ E}.</p>
<p>Definition 2 (Endogenous and Exogenous Nodes).Nodes in the causal graph can be categorized into two distinct groups based on the presence or absence of their parent nodes: endogenous nodes, whose values are determined by other nodes (i.e., parent nodes) in the graph, and exogenous nodes, which derive their values from external factors.</p>
<p>The edges of the causal graph represent causal relations, which are distinct from the commonly known correlations.The latter indicates merely that two variables are statistically correlated, whereas the former indicates that one variable causes the other.Suppose, for example, that there are three variables, X, Y, and Z, and that the corresponding causal graph is X ← Z → Y. Here, Z is the common cause of X and Y, also known as a confounder.In this case, X and Y are correlated because they are simultaneously affected by Z. X and Y are not causally related, however, because X is not the cause of Y and vice versa.This disparity between causal relations and correlation necessitates the use of causal discovery algorithms.These algorithms can precisely infer the causal relations between variables from observational data and then construct the causal graph.</p>
<p>Analysis based on Causal Graphs.After obtaining the causal graph, further analysis can be conducted.The prerequisite for the analysis, however, is that all conclusions derived from the graph's properties must be consistent with the real world.global Markov assumption [53,48], which is widely assumed in the field of causal analysis [64,53], resolves this issue.</p>
<p>Effect of Prompt in Code Generation</p>
<p>As reviewed in Sec. 2, while recent work has shown promising results in prompt engineering, the design of prompts is still largely a manual process that relies on human expertise.Thus, evaluating the quality of prompts remains an open problem [70,8], let alone providing a systematic guideline for prompt design.Overall, considering the high flexibility of prompts in natural language, we see that the effects of different types of prompts on LLM performance are not well understood.Specifically, the criteria for assessing prompt quality are unclear, making it challenging to determine whether one prompt is superior to another before actually executing it.Moreover, the effects of prompts are often non-linear and non-monotonic [31], making it difficult to predict the performance of a prompt solely based on its design.</p>
<p>Fig. 1 presents an example of how the performance of a prompt can be affected by subtle modifications in its design, leading to highly confusing and non-monotonic results.In this case, LLM is instructed to generate a program that returns the first letter of each word in a given string.The relatively unintelligible prompt in Fig. 1(a) misleads the LLM into generating a program that capitalizes the initial letter of each word.In contrast, the more explicit instructions provided by the prompt in Fig. 1(b) direct the LLM to generate the correct program.Evidently, this distinction was not anticipated by the designer, as the generated code for two semantically equivalent prompts should be identical.This example illustrates the importance of comprehending the mechanism of prompts' effect on code generation.Consequently, this work aims to answer this research question:</p>
<p>How to systematically establish the prompt's influence on LLM-based code generation?</p>
<p>Prompt Adjustment via LLM-Based Rephrasing</p>
<p>Following the observation in Fig. 1, we interpret that "prompt adjustment" is critical, as a minor change in the prompt may substantially improve the performance of a given LLM from generating incorrect code (Fig. 1(a)) to correct code (Fig. 1(b)).Thus, the intuition is that by properly adjusting the prompts, we expect to observe and characterize the relations between the prompt and the generated code in depth.Nonetheless, prompt adjustment is itself challenging [85,56,78].Generally, a desirable prompt adjustment method shall notably alter one or more properties of the prompt while preserving its semantics.In natural language processing, a straightforward method to achieve this goal is to replace words with their synonyms.However, we argue that this is not an optimal way in our research context.Indeed, the frequent occurrence of lengthy text in real-world prompts and the abundance of synonyms in natural language jointly construct an enormous search space, making the exhaustive search for the expected prompts impractical.In this work, we propose adjusting the prompt through rephrasing.</p>
<p>By explicitly asking an LLM to rephrase the prompt, often depicting a programming problem, in different manners, we can systematically and smoothly modify the style of the prompt while keeping its semantics unchanged.Moreover, the results of a small-scale pilot study, as shown in Table 1, indicate that rephrasing can effectively affect the generated code.In this pilot study, we ask ChatGPT [5] to rephrase the given programming problem by following three simple instructions: make the prompt shorter, longer, or more formal.As a baseline, we also set up a control group that receives no instructions.Then, all rephrased programming problems are fed into LLM to generate code, which is then compared to the code generated from the original prompt.The results show that rephrasing instructions can indeed affect the generated code and, more importantly, convey different intentions to the LLM, which are reflected in the varied change trends of code metrics. 2 Thus, this work adopts LLM-based rephrasing as the basic operator and aims to answer the following research challenge: How to effectively explore the prompt space and systematically adjust the prompt to achieve the desired effect on the generated code?A straightforward approach to analyzing the relationship between multiple variables is to identify and calculate the correlation between them.We argue, however, that correlation is insufficient for analyzing complex relationships.Consider an extremely simplified example in Fig. 2, which consists of only three variables.Here, X represents the prompt's quality and Y represents the generated code's quality.Besides, variable Z represents one specific property of the programming question, such as its difficulty level.Suppose, for instance, these three variables follow the quantitative relations presented on the right side of Fig. 2. X's value is determined by Z, and Y's value is determined by both X and Z.</p>
<p>Analysis for Complex Relationships in Code Generation</p>
<p>In this case, Z is a confounder of the relation between X and Y because it affects both X and Y. Besides, b 1 and b 2 are two constants, and ϵ 1 and ϵ 2 denote random noise with zero mean.The standard procedure for predicting the value of Y is to train a regression model taking X and Z as inputs.This trained model is capable of achieving a high level of accuracy, but it may not capture the correct quantitative relation between X and Y.For example, Y = 21Z + 10b 1 + b2 is a "perfect" model to offer 100% accuracy in estimating. 3However, it is evidently not the appropriate model for examining the effect of the prompt (X) on the generated code (Y).Any downstream analysis based on this model will inevitably lead to an incorrect conclusion that X has no effect on Y.This problem will be further exacerbated in the real-world scenario of LLM-based code generation, where hundreds of variables, which reflect the properties of the prompt, the generated code, and the programming question, are involved.</p>
<p>Additionally, the uncertainty of the LLM's output further complicates the trial-and-error process of empirical analysis [35,77,50].In particular, the generated code for a given prompt is stochastic rather than deterministic.Hence, it is contended that the assessment of the generated code's quality should be based on the distribution rather than the outcomes of singular or a few executions.To conclude, we attempt to address the following issue in this paper: How to disentangle the intricate relations between numerous variables in a nondeterministic system -LLM-based code generation -and to identify the correct quantitative relations between prompts and the generated code?Fig. 3 presents the overview of our study, where two primary phases are involved: data collection and analysis.In the data collection phase, we begin by rephrasing prompts (the green blocks in Fig. 3), which represent questions of programming tasks.Then, both the rephrased and the original question prompts are fed into two subtasks.The first subtask is to quantify the characteristics of prompts, which is accomplished by extracting linguistic features from the natural language text in these questions.In another subtask, questions are fed into an LLM to generate code.We further collect a variety of performance metrics from the generated code, including the code's correctness, readability, etc. Together, the linguistic features of the prompt and the performance metrics of the generated code (two blue blocks in Fig. 3) constitute the data for the analysis phase.</p>
<p>During the analysis phase, we first use the causal discovery algorithm to construct a causal graph that represents the causal relations between all the variables that we collected.A series of further analyses are conducted on the causal graph in an effort to identify general principles for code generation prompt design.Last, a prompt optimization algorithm is employed based on the causal graph to determine the optimal prompt setting that will generate highquality code with the highest probability.</p>
<p>Overall, the workflow in Fig. 3 is designed to answer the research questions raised in Sec.3.1 and address subsequent technical challenges.Specifically, Sec.4.1 proposes a systematic approach for quantifying the characteristics of prompts.Then, Sec.4.2 addresses the challenge of prompt adjustment raised in Sec.3.2.Afterwards, Sec.4.3 adopt causality analysis to disentangle the complex relations between the numerous variables involved in this study.Finally, Sec.4.4 proposes an algorithm for learning and explaining prompts' effect on code generation, thereby addressing the issue discussed in Sec.3.3.</p>
<p>Prompt Quantification</p>
<p>As discussed in Sec. 3, the criteria for assessing prompt quality are generally unclear, making it impractical to compare two prompts prior to feeding them to LLMs.To address this, we propose to quantify the characteristics of prompts using linguistic features.Linguistic features are the building blocks of natural language.They are the elements that make up the structure of a language, such as morphology, syntax, and semantics.In order to quantify the prompt for large language models, it is necessary to extract these linguistic features from the natural language text of the prompts.This can be done using various techniques, such as part-of-speech tagging [11,76], named entity recognition [23], and dependency parsing [47].</p>
<p>In this paper, we follow Lee et al. [37,38], one state-of-the-art study on linguistic feature in natural language processing, to extract linguistic features from prompts.Specifically, a total of 255 linguistic features are extracted from prompts, including lexical features (e.g., count of nouns, verbs, adjectives, etc.), syntactic features (e.g., phrasal features [45], average height of parsed dependency tree, etc.), and semantic features (e.g., semantic richness [37], word familiarity features [17], etc.).The complete list of linguistic features can be found in the Appendix of Lee et al. [37].</p>
<p>By quantifying the linguistic features of a given prompt, we can better understand the underlying structure and meaning of the text, allowing us to more accurately capture the nuances and complexities of natural language and providing a more systematic guideline for prompt design.Further, we presume that these vast amounts of linguistic features are capable of sufficiently capturing all interactions between prompts and other variables (including rephrase instructions and generated code), thereby resolving the unmeasurability issue of prompt discussed in Sec. 3.</p>
<p>Rephrase Generation</p>
<p>To explore the impact of prompt design on code generation, we propose rephrasing the prompt using LLM.In comparison to rule-based mutation, rephrase serves a more flexible and holistic mutation primitive; it can generate a greater variety of prompts and substantially improve the prompt diversity.A greater variety of prompts indicates that the value space for each linguistic feature has been thoroughly explored.Hence, we presume that the diversity of prompts is beneficial to the observability of the linguistic features, which is crucial to the success of subsequent analysis.</p>
<p>Inspired by mature rephrasing tools, such as QuillBot [6], we designed a prompt template for programming question rephrasing, as shown in Fig. 4. Note that, for simplicity, we use meta-prompt to refer to the prompt used for programming question rephrasing.All of these meta-prompts are refined and optimized by the state-of-the-art LLM, GPT-4.0, in an effort to enhance the quality of the rephrased prompts and subsequently escalate the quality of the generated code.</p>
<p>This template also includes a set of rephrasing intentions that can direct the LLM to rephrase the provided programming question in a particular manner.These rephrasing intentions are classified as instruction, role, and scenario.Instruction provides the LLM with explicit instructions, such as make it short and make it fluent.For role and scenario, they are designed to provide context for the LLM, like as a student and in a programming competition to unleash the LLM's creativity.Furthermore, pre-defined intentions from various categories can be combined to form a more complex rephrasing intention.These rephrasing intentions are intended to exhaustively cover applicable rephrasing preferences for the programming question rephrasing task.</p>
<p>We designed a total of 22 rephrasing intentions, which can be combined to form over one hundred filled-in content for the meta-prompt template; our tentative study shows that exploring all these combinations results in excessive cost in the experiment.Therefore, we select six instructions, three roles, and three scenarios based on the expert's experience and the results of the pilot study.The selected rephrasing intentions are marked in grey in Fig. 4(b).It is worth noting that the design rephrasing intention is not limited to those shown in Fig. 4(b).Additional rephrasing intention can be added to the template to expand the diversity of the prompts.This will be discussed in Sec. 8.</p>
<p>Causal Analysis</p>
<p>Causal analysis is a powerful tool for comprehending the intricate relations between variables.This section describes the methods we employed to construct a causal graph (causal discovery) and conduct quantitative analysis on the graph (causal inference).</p>
<p>Causal Discovery.This study involves a variety of variables, which can be categorized into three groups: meta-prompt variables (0/1 variables flagging whether users select a particular rephrasing intention listed in Fig. 4(b) or not), linguistic feature variables, and code metric variables.By employing causal discovery, it is possible to disentangle the complex relations between variables and learn a causal graph, where the nodes represent the variables and the edges represent the causal relations between nodes.In this study, we employ a state-of-the-art causal discovery algorithm, DiBS [43].As a gradient-based causal discovery algorithm, DiBS transforms the causal graph construction procedure into a differentiable optimization problem, thereby substantially enhancing the efficacy of causal discovery and ensuring the plausibility of the learned causal graph [43].</p>
<p>However, the large number of variables involved in this study makes constructing causal graphs exceedingly difficult.</p>
<p>To address this challenge, we propose a two-step causal discovery approach, as shown in Fig. 5. Holistically, the three aforementioned groups of variables propagate in the following order: The meta-prompt determines the output of the rephrase generation task, affecting the linguistic features of the rephrased programming question, which, in turn, affects the generated code's metrics.Consequently, the causal graph can be split into two subgraphs, which can be The following programming question is not clear enough, so you need to rephrase it.Clearer</p>
<p>Scenario</p>
<p>Someone wrote the following programming question, and asked you to improve it to make it clearer.Improve You need to rephrase programming questions to make them more suitable for python3.Specify Rephrase questions from an online programming practice platform to help you better understand the question.Yourself You need to rephrase a programming question to explain it to your partner.Partner (b) rephrase intention Figure 4: Meta-prompt design.Red text indicates the programming question that is filled in by the user, and blue text indicates the pre-defined rephrasing intention that is selected by the user.learned separately.After learning the two subgraphs independently, the complete causal graph can be obtained by combining them directly.</p>
<p>Causal Inference.After learning the causal graph, we conduct quantitative analysis on the graph to identify the causal effect of one variable on another.Taking Fig. 2 as an example, the quantitative analysis seeks to know the magnitude of the change in Y (the output variable) when X (the treatment variable) changes from x 1 to x 2 while blocking the effect of confounding variable Z. Here, x 1 and x 2 are two specific values of X.One or both of them may not be observed in the data.Therefore, they are counterfactual values, denoted as doX = x 1 and doX = x 2 , respectively.The magnitude of Y's average change reflects the causal effect of X on Y.This is referred to as the average treatment effect (ATE) within the context of causality analysis [53,48].Below, we present the formal definition of ATE: Definition 4 (ATE).ATE of treatment variable X on output variable Y is defined as:
ATE = E[Y | do(X = x 1 )] − E<a href="1">Y | do(X = x 2 )</a>
where terms containing do(•) operator are known as causal estimand, which cannot be estimated from observational data directly.</p>
<p>We employ double machine learning (DML) [15] to estimate causal estimands in Eq. 1. DML is a state-of-the-art method and has been extensively applied in the causal analysis literature [4].It first uses two machine learning models to estimate treatment variables and output variables, respectively, from confounders.The causal effect of the treatment variables on the output variables can then be determined by training another machine learning model that predicts the relations between the residuals of the two models.In this study, we employ the DML implementation in the EconML package [4].</p>
<p>Establishing Prompt Effect on Code Generation</p>
<p>Algorithm 1: Analysis
Input: Causal Graph G, Meta-prompt Variable M, Linguistic Features L l = (L 1 , • • • , L n ), Code Metrics C l = (C 1 , • • • , C n ) Output: Dictionary D of Influenced Code Metrics with Explanation for the Effect 1 D ← {}; 2 foreach C ∈ C l do 3 ATE C M ← E[C | do(M = 1)] − E[C | do(M = 0)] ; // Computing ATE of M over C 4 end
/<em> Find the top 3 code metrics that are most affected by M </em>/</p>
<p>5 Sort C l in descending order according to abs(ATE C M ); Alg. 1 presents the prompt effect analysis procedure.The inputs for this algorithm are a causal graph G, a meta-prompt variable M, linguistic features L l , and code metrics C l .Typically, the meta-prompt variable M refers to a binary (0/1) variable flagging if a particular rephrasing intention is selected or not ("1" denotes selection).Moreover, a set of binary variables indicating if multiple rephrasing intentions are employed or not can also be supported by this algorithm (by invoking Alg. 1 for multiple times).Here, the set of binary variables simulates the situation where a user combines multiple rephrasing intentions to rephrase a question, as described in Sec.4.2.
6 C S ← Top3Affected(C l ); 7 foreach C ∈ C l do 8 L P ← IdentifyAncestors(G, C, L l ); 9 foreach L ∈ L P do 10 l M=0 ← E[L|M = 0]; l M=1 ← E[L|M = 1]; 11 ATE C L ← E[C | do(L = l M=1 )] − E[C | do(L = l M=0 )];
Holistically, Alg. 1 comprises two steps, learning the effect and explaining the effect.In the first step, we compute the average treatment effect (ATE) of the meta-prompt variable M on each code metric C (lines 2-4) and then select the top three code metrics that are most affected by M (lines 5-6).This step is intended to quantify the extent of M's effect on code generation.In the second step, we attempt to explain how the changes in meta-prompt M are propagated to the selected code metrics C S (lines 7-13).Specifically, we first query the causal graph to find all the ancestor linguistic features of the chosen code metric C (line 8), as they are potential mediators of the effect of M on C.Then, we intervene on each ancestor individually to calculate their ATE on C (lines 10-12).Based on the calculated ATE, we determine the linguistic features primarily responsible for the effect of M on C (lines 13).Finally, we return a dictionary D containing the top three influenced code metrics and their corresponding explanations (line 15).</p>
<p>Experiment Setup</p>
<p>Scripts required to conduct the entire study is written in Python3 with about 2.4K lines of code.We run all experiments on a server with AMD Ryzen 3970X CPU and one NVIDIA RTX 3090 GPU.</p>
<p>Datasets &amp; Model</p>
<p>Datasets.Our study is conducted on one of the most popular text-to-code datasets, APPS [28].It is exclusively designed for Python code generation and contains 10K Python problems with difficulty annotations.Each problem is associated with a few correct solutions and a set of test cases.To learn high-quality causal graphs, we sample and generate 6K data points from this dataset. 4Note that CodeContests [39] and other datasets are also compatible with our framework.The promisingly high generalizability of our framework to other datasets will be discussed in Sec. 8.</p>
<p>Models.</p>
<p>Three LLMs are used in our experiments: a pre-trained GPT-Neo (2.7B) model by Hendrycks et al. [28], GPT-3.5-Turbo(ChatGPT), and GPT-4.These three models denote the relatively tiny but powerful LLM, the most widely-used LLM, and the state-of-the-art LLM, respectively.For ChatGPT series models (GPT-3.5-Turboand GPT-4), we use the official API supported by Azure.Except for the number of generated solutions, which is set to 3, and the maximum length of generated solutions, which is set to 2,000 to handle long solutions, all parameters are set to their default values.In this study, each model generates code for the 6K data points and GPT-3.5-Turborephrases the prompt.The reason for this setup is twofold.First, compared to code generation, prompt rephrasing is a relatively simple task for for which GPT-3.5-Turbo is sufficient.Our preliminary observation shows that the more powerful but expensive GPT-4 is an "overkill" in this task, while the tiny GPT-Neo cannot handle it plausibly.Second, in this study, we focus on the code generation task and the rephrasing itself is quite standard and not our primary focus.</p>
<p>Code Metrics</p>
<p>Code generation is naturally multi-faceted.For example, generated code should be correct but fail to adhere to the coding style (e.g., PEP8 for Python).To comprehensively evaluate the code generation capability of LLMs, we employ a set of code metrics to measure the quality of generated code.These metrics are categorized into five categories: correctness, diversity, overhead, readability, and security, as reported in Table 2.The rationale behind this categorization is that these metrics are orthogonal to each other and can be used to evaluate the code generation capability of LLMs from different perspectives.For these metrics, we use the implementation provided by [55,72,44,40].The number of syntax errors revealed by tree sitter [3].gold sim CB</p>
<p>The similarity (in CodeBLEU [58]) between the generated and the ground truth.gold sim B</p>
<p>The similarity (in BLEU [51]) between the generated and the ground truth.</p>
<p>Diversity (2) mut sim CB</p>
<p>The mutual similarity (in CodeBLEU) among the generated solutions.mut sim B</p>
<p>The mutual similarity (in BLEU) among the generated solutions.Overhead (1) timeout rate The timeout rate of test cases.</p>
<p>Readability (1) black count</p>
<p>The number of places reported by black [1] where PEP8 is violated.Security (1) semgrep count The number of potential security bugs revealed by Semgrep [2].</p>
<p>Evaluation</p>
<p>In this section, we evaluate the efficacy and value of our analysis framework, as described in Sec. 4. To assure the correctness of subsequent analysis, we first evaluate the accuracy of the learned causal graphs, which serve as the foundation of our framework (RQ1).Then, by querying the learned causal graphs, we obtain several valuable and indepth insights into LLM-based code generation (RQ2).Finally, we adopt the learned graphs to guide the adjustment of the prompt, thereby further demonstrating the value of our framework (RQ3).As the cornerstone of our analysis framework, the accuracy of the learned causal graphs is critical.In this section, we endeavor to verify the accuracy of the learned causal graphs.However, the lack of a ground-truth causal graph poses an obstacle to verification.To overcome it, we propose a comprehensive and efficient verification strategy that uses the learned causal graphs to predict each downstream variable (code metrics listed in Table 2 in our case) and then compares the predictions to the actual values.Idealistically, the learned graph would depict the true causal relationships between each pair of variables, thereby precisely depicting the propagation of changes from meta-prompt variables to code metrics.Consequently, the predictive potential of the learned graph should correctly reflect its accuracy.For each code metric, we report the R 2 score and mean squared error (MSE).The R 2 score quantifies the proportion of the variance in the target variables (code metrics) that can be predicted from the input variables (meta-prompt variables and the linguistic properties of the original prompt), with a higher R 2 score indicating a better predictive capability.</p>
<p>For MSE, it measures the average of the squares of the errors; the lower the value, the better.[2] in the generated code.On this metric, the graphs for GPT-3.5-Turbo and GPT-4 perform poorly, whereas the graph for GPT-Neo attains a perfect score (R 2 =1.0 and MSE=0.0).We investigate this phenomenon and discover that the semgrep count has a low variance.Specifically, zero security issues are detected for code generated by GPT-Neo, while 13 security issues are detected on GPT-3.5-Turbo and four on GPT-4.We presume that GPT-Neo has been fine-tuned to the point of relative overfitting on APPS, a clean dataset, resulting in no detected security flaws.For ChatGPT series models, we attribute the impressive performance to its innovative training paradigm, reinforcement learning from human feedback (RLHF) [86].In the subsequent analysis, the semgrep count metric will be omitted as no meaningful insights can be obtained from it.</p>
<p>Prompt: Your task is simply to count the total number of lowercase letters in a string.</p>
<p>Output:</p>
<p>def solution(strng): count = 0 for x in strng: if x.islower(): count += 1 return count Prompt: Your task is simply to count the total number of lowercase uppercase letters in a string.There are several other interesting observations when comparing the performance of the graphs for different models.In general, the graphs for ChatGPT series models have a similar performance pattern, as the differences between their R 2 scores and MSE values are typically small (less than 0.05).Given that GPT-4.0 is derived from GPT-3.5-Turbo, this is not surprising.In contrast, the graphs for GPT-Neo display a substantially distinct performance pattern.In particular, the graph performs outstandingly on similarity-based metrics (e.g., gold sim CodeBLEU).This phenomenon is interpreted as a consequence of the overfitting of GPT-Neo on APPS.Instead of "thinking" and then writing code like human programmers, we interpret that GPT-Neo is more likely to memorize a large number of tiny code snippets from training data and then combine them according to the requirements.Such characteristics render GPT-Neo insensitive to changes introduced by meta-prompt variables, preserving the stability and similarity of the generated code to the ground truth.Fig. 6 illustrates this phenomenon.In this example, altering the count target from lowercase letters to uppercase letters has no effect on the generated code.This stickiness to the original ground truth is also reflected in the low variance of similarity-based metrics, thereby reducing the predictive difficulty for the learned graph.</p>
<p>RQ2: Analysis based on the Causal Graphs</p>
<p>In this section, we assess our analysis framework by employing it to learn the extent of the impact of meta-prompt variables on code metrics; this reflects how code metrics can be affected by prompts.Alg. 1 is adopted to achieve this objective.Since we divide the meta-prompt variables into three groupsinstruction, role, and scenario -we report the results for each group separately.Specifically, Fig. 7 presents results for the instruction group, and Fig. 8 shows results for the role and scenario groups.Each row in these figures represents a meta-prompt variable (e.g., Long; see Fig. 4 for details), with three sub-rows representing the three code metrics that are mostly affected by the meta-prompt variable.Code metrics are highlighted in red and blue colors.The former implies that the meta-prompt variable has a negative effect on the code metric, whereas the latter indicates a positive effect.Besides, cells marked in gray contain the primarily responsible linguistic variables for their left-hand side code metrics.For simplicity, we attach the primarily responsible linguistic variables for each code metric only in Fig. 7, while omitting them in Fig. 8. Readers are encouraged to refer to the repository[7] for the complete results.</p>
<p>The straightforward conclusion from Fig. 7 is that each meta-prompt variable tends to influence distinct code metrics in different causal graphs.For example, the mostly affected code metric for meta-prompt variable Short is syn err in GPT-Neo, while it is gold sim B in GPT-3.5-Turbo, and pass rate in GPT-4.We attribute this notable difference to the distinction between the three learned graphs, as mentioned in Table 3's discussion.However, there are also some common patterns, particularly in the graphs belonging to the GPT series models.Typically, the meta-prompt variables Long and Formal have a negative effect on the generated code, decreasing its similarity to the ground truth.In contrast, Short and Fluent are beneficial.In fact, these four meta-prompt variables constitute two pairs of opposites: Long vs. Short and Formal vs. Fluent.For Long vs. Short, we attribute this phenomenon to the fact that the former tends to load rephrasings with unnecessary and even misleading information, whereas the latter tends to summarize and then extracts the most essential information.For Formal vs. Fluent, we presume that Formal complicates the programming logic while Fluent simplifies it.</p>
<p>From the perspective of the linguistic variables, we observe that, in the majority of instances, certain linguistic variables serve as the primary responsible variables.root det var occurs 12 times in the case of GPT-Neo.This variable represents the value of the total number of unique determiners divided by the square root of the total number of determiners.The unexpectedly high frequency of this variable is counterintuitive, as one may not expect it to have a substantial impact on the generated code.In line with our discussion in Sec. Figure 7: Prompt effect analysis for the instruction group.For code metrics, Red indicates a negative effect received from the meta-prompt variable, while blue indicates a positive effect.The right gray cells record the two primarily responsible linguistic variables for the meta-prompt effect.</p>
<p>occurrence is consistent with the overfitting characteristics of GPT-Neo.In contrast, simp ttr and features related to named entities occur most frequently for GPT series models.The former indicates the degree of lexical variation of the text, and the latter determines the complexity of the information contained in the text; both are intuitive and reasonable.Some additional interesting conclusions can be derived from Fig. 8. Compared to the instruction group, the effect of the role and scenario groups on the generated code in the cases of GPT series models tends to be more positive.In particular, these two groups of variables achieve 13 out of 18 positive effects on GPT-3.5-turbo and 15 out of 18 on GPT-4.This result is consistent with previous research on prompt engineering [75,19,83], indicating that role-playing and scenario setting are efficient ways to enhance the performance of LLM-based code generation.GPT-Neo, on the other hand, performs poorly on these two categories of variables, indicating that role-playing and scenario setting are inappropriate for models of its size.</p>
<h1>1 #2 #3 #1 #2 #3 #1 #2</h1>
<p>Findings: There are several interesting findings from the analysis that are worth mentioning.</p>
<ol>
<li>Treat the LLM as an "ignorant idler."When you try to improve the performance of LLM-based task by rephrasing the prompt, you should avoid extending it or stating it in a formal way.Instead, you should remove redundant information and make it easier to understand. 2. Take advantage of the analysis pipeline illustrated in this work, and watch out for the responsible linguistic variables.When some unreasonable linguistic variables frequently work as the primarily responsible variables for prompt effect, be careful.This phenomenon indicates that the LLM is failing to react reasonably to the prompt according to their linguistic properties.In other words, it may be a sign of overfitting.3.In general, using role-playing and scenario setting denotes a universal and effective way to improve the performance of LLM for models with sufficient capacity.However, for tiny models, using simple rephrasing instructions like "make it more concise" can often be more effective.</li>
</ol>
<p>Downstream Application</p>
<p>In this section, present a prospective downstream application on the basis of our approach in improving the prompt to generate high-quality code.Specifically, the procedure is based on the genetic algorithm which is widely used in the literature of search-based software engineering, such as test case generation [69].The genetic algorithm is a metaheuristic search algorithm that mimics the process of natural selection.It operates on a population of candidate solutions and iteratively evolves the population to find the optimal solution.Below, we describe how these procedures are concretized in our context.</p>
<p>Fitness Function.The goal of the procedure is to find the optimal rephrasing instructions that maximize the probability of generating high-quality code.First, as a straightforward solution, we may directly use the objective metric (e.g., BLEU score) as the fitness function and search for the optimal rephrasing instructions that maximize the objective metric.However, this solution is infeasible in our context.The reason is that the objective metric relies on the ground truth to evaluate the quality of generated code.Even for the metrics that do not require the ground truth (e.g., black count), the solution is still highly costly as it requires the generation of code for each candidate solution.</p>
<p>In contrast, our causality analysis-based approach actually constitutes a lightweight surrogate for the objective metric and is free from the ground truth.Specifically, based on the causal graph, we can estimate the expected value of the objective metric of a candidate solution without involving the actual code generation process.This surrogate is much more efficient than the objective metric and can be used as the fitness function in the genetic algorithm.More importantly, the surrogate is also expected to be more stable than actual code generation, as it is asymptotically unbiased and free from the infamous non-determinism issues in LLM [50].</p>
<p>Genetic Representation, Modification and Selection.We use a genetic algorithm to find the rephrasing instructions that can maximize a given objective metric.The instructions are represented as a binary string, with each bit indicating the selection status of the corresponding instruction.For example, "make it short" is represented as 100000, and combining it with "make it fluent" is 110000.The algorithm operates on these binary strings to find the best instructions.</p>
<p>In each generation, each vector is evaluated using a predefined fitness function.Only the top-N candidates, based on fitness scores, move to the next generation (i.e., "survive").The new generation is formed using these top-N vectors.Two standard operators in the genetic algorithm, Crossover and Mutation, are applied to create new offspring vectors from any two parent vectors.Specifically, two new vectors are generated using a two-point crossover.In addition to Crossover, Mutation in our context is implemented to randomly alter bits in a vector to enhance the population's genetic diversity.</p>
<p>Result.We apply the genetic algorithm to the APPS dataset and use the BLEU score as the objective metric.Because the rephrasing intentions have been refined and optimized by GPT-4, we consider single rephrasing instructions to be an adequate baseline.We evaluate our approach against the best single rephrasing-intent instruction and the original prompt.The results are shown in Table 5.We can see that our approach outperforms the single rephrasing instruction and the original prompt in most cases.The only exception is in the case of GPT-Neo, where the overfitting problem may be the cause (see related discussion in Sec. 6).Overall, we interpret the results as highly encouraging, suggesting that our algorithm for improving the prompt to generate high-quality code is effective.</p>
<p>Discussion</p>
<p>Extensibility.While the current technical pipeline is primarily applied over the task of generating Python3 code, it is extensible to other languages.Overall, our approach is generally language agnostic; while a few prompts are language specific, it is easy to see that such prompts can be constructed when considering other languages.For example, the prompt def foo(): can be replaced with function foo() { for JavaScript, or int foo() { for C. Besides, our approach is also extensible to other rephrasing instructions.For example, we can add the rephrasing instruction ''make it more object-oriented'' to the current set of rephrasing instructions.The only requirement is that the rephrasing instruction should be of reasonable difficulty for LLMs to understand and implement.</p>
<p>Threats to Validity.Our study is subject to the following threats to validity.First, our study primarily focuses on the GPT-family models, namely, GPT-Neo (2.7B), GPT-3.5-Turbo, and GPT-4.While these models are among the most popular LLMs, there are however other LLMs that are not investigated in this study such as the T5 model [57].</p>
<p>Second, our study is conducted on APPS [28], which is currently one of the most popular datasets for code generation.However, there are other datasets that are not investigated in this study such as the HumanEval [14] dataset.Third, our study is conducted on Python3 code generation and focuses on a limited number of rephrasing instructions.However, given the popularity of Python3 and the extensibility of our approach, we believe our findings are representative for common usage scenarios and are generalizable to other languages and rephrasing instructions, as discussed above in "Extensibility."</p>
<p>9 Related Work Theoretical Causality Analysis.Causality analysis has been studied in statistics and machine learning for decades [53].In the past few decades, there has been a surge of interest in learning causal relations from observational data [54].Algorithms from various categories typically employ different strategies for learning the causal graphs.Constraint-based algorithms are the most classical algorithms in this area.For instance, the Peter and Clark (PC) algorithm begins with a complete graph and deletes edges using conditional independence from hypothesis tests [64].Score-based algorithms recover causal graphs by maximizing a pre-defined scoring criterion, such as BDe(u) [27].Representative score-based algorithms includes HC [67], GES [16], BLIP [60] and GOBNILP [18].Recently, gradient-based algorithms have been proposed.For example, NOTEARS [84] and DAG-GNN [81] seek to recover the data generation process while adhering to acyclicity constraints.These advancements in the theoretical aspect of causality analysis are orthogonal to our work.In this paper, we focus on the application of it in evaluating and explaining LLM-based code generation.</p>
<p>Application of Causality Analysis in Software Engineering.Recently, it has been shown that causality analysis can be applied to various software engineering tasks [61], including debugging [22,20], root cause analysis [34], data race detection [29], and also deep neural network (DNN) testing and repairing [65,82,32].Besides, causality analysis has been used for understanding empirical software engineering data and uncover the underlying causal relations.For example, causality analysis has been used to understand the impact of programming language on code competitions [24] or various factors that affect software productivity [68].In this paper, we advocate the application of causality analysis for understanding the role of distinct prompts in LLM-based code generation.</p>
<p>Conclusion</p>
<p>In line with the prosperous adoption of LLMs in code generation, this paper proposes a novel causality analysis-based approach to systematically analyze the relations between the LLM input prompts and the generated code.Our solution facilitates the evaluation and explanation of LLM-based code generation, and provides actionable insights to improve the quality of the LLM-generated code by properly calibrating the prompt.Our work can serve as a roadmap for users to comprehend and improve the quality of LLM-generated code.</p>
<p>Figure 1 :
1
Figure 1: Motivating example of prompt engineering for code generation.Red boxes indicate the error in the generated code.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the difference between correlation and causation.</p>
<p>Figure 3 :
3
Figure 3: Study overview.</p>
<p>. Please rephrase the programming problem in XML tags while keeping its original meaning and structure, to help enhance the understanding of the problem's intent and facilitate better code generation.Instruction.You should keep all mathematical symbols in latex format.Here's the original problem: <text> Question </text> (a) prompt template for programming question rephrasing Filled-in Sentence Name Expand the problem in a more detailed and thorough manner.Make the explanation longer and clearer.Long Instruction Condense the problem in a more concise and clear manner.Make the explanation shorter while maintaining clarity.Short Rewrite the problem in a more formal and clear manner.Formal Rewrite the problem in a more fluent and clear manner.Fluent Rewrite the problem in a more technical and detailed manner.Technical Rewrite the problem to make it more logical and clear.Logical Simplify the problem in a more straightforward and easy-to-understand manner.Easy Rewrite the problem in a more creative and engaging manner, while ensuring clarity of the question.Creative Rewrite the problem with more precision and clarity.Precise Rewrite the problem to make it more objective and clear.Objective You are a student majoring in software engineering.Student Role You are a senior python programmer.Programmer You are a competitor in a programming competition.Competitor You are a researcher in the field of large language model.Researcher You are a teacher teaching python programming.Teacher You are a software engineer.Engineering</p>
<p>Figure 5 :
5
Figure 5: Two-step causal discovery.</p>
<p>12 end/ 13 D
1213
* Find the linguistic features that are mainly responsible for the effect of M on C */ [C] ← SortSelect(L P ); 14 end 15 return D</p>
<p>Figure 6 :
6
Figure 6: An illustrative example of GPT-Neo's overfitting.</p>
<p>Figure 8 :
8
Figure 8: Prompt effect analysis for the role and scenario groups.</p>
<p>[53]Z are three disjoint sets of variables, which also represent three nodes/node sets in the causal graph.X P Y | Z signifies that X and Y are conditionally independent given Z in P, reflecting the real-world causal relations.And X G Y | Z denotes that X and Y are separated by Z in G, also known as d-separation[53].On causal graphs, both qualitative and quantitative analysis can be conducted.Qualitative analysis typically leverages structural information contained within the causal graph.By traversing the graph, all factors that have a causal effect on the target variable can be identified.Furthermore, quantitative analysis, also known as causal inference, aims to measure the causal effect of one variable X on another variable Y, by answering counterfactual questions such as "what would happen to Y if X were set to a different value?".In this paper, both qualitative and quantitative analysis are performed on the causal graph for different purposes, which will be detailed in Sec. 4.
Definition 3 (Global Markov Assumption). Given a distribution P and its corresponding causal graph G, P and Gsatisfy the global Markov assumption if X G Y | Z =⇒ X P Y | Z.X, Y, 3 Research Motivation and Pilot StudyPrompt: In this exercise, a string is passed to aPrompt: In this task, you need to create a function thatmethod and a new string has to be returned with thetakes a string input, and returns a new string containingfirst character of each word in the string.the first letter of each word in the input string.Output:def capitalize_words(string):words = string.split()return " ".join([word[0].upper() +word[1:] for word in words])</p>
<p>Table 1 :
1
Pilot study on the effectiveness of rephrasing in prompt adjustment.
Instruction Pass Rate Syntax Error Rate StabilityShort-0.0029+0.0090+0.0534Long+0.0095-0.0013-0.0184Formal+0.0061-0.0067+0.0011N/A0.0000+0.0026+0.0290</p>
<p>Table 2 :
2
Code metrics employed in this study.
CategoryNameDescriptionpass rateThe pass rate of test cases.run err rateThe runtime error rate of test cases.Correctness (5)syn err</p>
<p>Table 3 :
3
Statistics of the learned causal graphs.
nodes edgesGPT-Neo45192GPT-3.5-Turbo 50177GPT-445180</p>
<p>Table 3
3
[10]rts the statistics of the learned causal graphs.For causal graph learning, we launch the causal discovery algorithm separately for each model, since Baluta et al.[10]have demonstrated that the causal graphs for distinct models can vary significantly.For each model, we first collect values for 12 meta-prompt variables (see Sec. 4.2), 255 linguistic variables (see Sec. 4.1), and 10 code metrics (see Sec. 2).overwhelming number of linguistic variables is then reduced by removing non-correlated variables, as non-correlation typically indicates d-separation in the causal graph (refer to Sec. 2.3).After launching the causal discovery algorithm, nodes with no edges are also removed for the same reason.
6.1 RQ1: Verification of the Causal Graphs</p>
<p>Table 4 :
4
Predictive capability of the learned causal graphs.
GPT-NeoGPT-3.5-TurboGPT-4R 2MSER 2MSER 2MSEpass rate0.84310.15270.76000.24190.82060.1817run err rate0.84920.15100.79780.20160.84740.1538syn err0.71960.28060.78120.21560.83630.1642gold sim CB0.92210.07830.84320.15610.87080.1297gold sim B0.90260.09780.86570.13580.87850.1227mut sim CB0.90680.09230.86540.13240.88780.1117mut sim B0.91200.08770.88050.11940.89280.1069timeout rate0.61570.39300.71010.29160.66860.3315black count0.78540.20800.80990.19130.86380.1356semgrep count1.00000.00000.18120.66170.14480.9503</p>
<p>Table 4
4
reports the verification results.From a holistic perspective, The high R2 scores and low MSE values indicate that the learned causal graphs have a strong predictive capability.In most cases, the R 2 scores are above 0.8, and the MSE values are below 0.2.The only exception is the semgrep count metric, which represents the number of security issues detected by Semgrep</p>
<p>6.1, we assume that this unusual
GPT-NeoGPT-3.5-TurboGPT-4#1syn_errroot_det_var rt_fastgold_sim_Bsimp_ttr a_det_pwgold_sim_Ba_n_ent_money_ps bilog_ttr_no_lemLong#2black_countroot_ttr WClar20_Sgold_sim_CBa_kup_pw simp_ttrmut_sim_Ba_n_ent_money_ps simp_ttr#3 run_err_rateroot_noun_var root_det_varsyn_errbilog_ttr_no_lem simp_ttrsyn_erra_n_ent_money_ps a_n_ent_percent_ps#1syn_errroot_det_var a_syll_pwgold_sim_Bsimp_ttr a_n_ent_loc_pwpass_ratesimp_ttr a_n_ent_money_psShort #2 gold_sim_CBroot_det_var root_noun_varrun_err_ratet_n_ent_fac simp_ttr_no_lemsyn_erra_n_ent_money_ps simp_ttr#3 black_countroot_det_var root_noun_varsyn_errt_n_ent_money bilog_ttr_no_lemblack_countbilog_ttr_no_lem simp_ttr#1 gold_sim_Broot_det_var simp_det_varmut_sim_CBsimp_ttr bilog_ttr_no_lemgold_sim_Ba_n_ent_money_ps bilog_ttr_no_lemFormal#2 gold_sim_CBcorr_ttr root_det_vargold_sim_CBt_n_ent_money bilog_ttr_no_lemmut_sim_Ba_n_ent_money_ps simp_ttr#3 black_countroot_det_var rt_fastrun_err_ratet_n_ent_fac simp_ttr_no_lemmut_sim_CBa_n_ent_money_ps bilog_ttr_no_lem#1 black_countroot_det_var corr_ttrgold_sim_CBt_n_ent_money bilog_ttr_no_lemgold_sim_CBbilog_ttr_no_lem a_cconj_pwFluent #2syn_errrt_fast rt_slowgold_sim_Bsimp_ttr a_n_ent_money_psgold_sim_Ba_n_ent_money_ps bilog_ttr_no_lem#3 gold_sim_Broot_ttr rt_slowtimeout_ratesimp_ttr a_kup_pwrun_err_ratea_n_ent_money_ps bilog_ttr_no_lem#1 gold_sim_CBcorr_ttr root_det_vargold_sim_Bsimp_ttr a_n_ent_money_psgold_sim_Bsimp_ttr a_n_ent_ordinal_pwTechnical#2mut_sim_CBa_noun_pw root_ttrgold_sim_CBt_n_ent_money bilog_ttr_no_lemrun_err_ratea_n_ent_money_ps bilog_ttr_no_lem#3 mut_sim_Broot_det_var rt_slowtimeout_ratea_n_ent_money_ps simp_ttrmut_sim_Bsimp_ttr root_punct_var#1pass_ratert_fast root_ttrgold_sim_Bsimp_ttr a_n_ent_money_psmut_sim_CBa_n_ent_percent_ps root_det_varLogical#2mut_sim_Broot_det_var rt_slowsyn_errbilog_ttr_no_lem simp_ttrpass_ratesimp_ttr a_n_ent_money_ps#3 mut_sim_CBa_noun_pw root_ttrgold_sim_CBt_n_ent_money bilog_ttr_no_lemmut_sim_Bsimp_ttr a_n_ent_ordinal_pw</p>
<p>Table 5 :
5
Evaluation of the rephrasing prompt generated by our algorithm.we highlight the winner , whose BLEU score is the highest and positive.
GPT-NeoGPT-3.5-TurboGPT-4OriginalSingleOursOriginalSingleOursOriginalSingleOursgold sim0.7071-0.1367-0.09650.5014+0.0239+0.05310.4867+0.0454+0.0753mut sim0.8030+0.0338+0.07040.5072+0.0165+0.02970.4971+0.0132+0.0362
In short, the pass rate is the percentage of generated code that passes the test cases, the syntax error rate is the percentage of generated code that contains syntax errors, and the stability is the percentage of generated code that is identical to other code generated from the same prompt. To clarify, we aim to demonstrate that rephrasing instructions can affect the generated code, as reflected in the change trends of these metrics. "Rephrasing" however does not necessarily mean "improving" the generated code.
All noise terms are omitted here because their mean is zero.
Indeed, more data points can be sampled, but we find that 6K data points are sufficient for our experiments to form causal graphs with reasonable quantity and acceptable cost.</p>
<p>Black -the uncompromising code formatter. </p>
<p>Semgrep -find bugs and enforce code standards. </p>
<p>. Tree-Sitter, </p>
<p>Econml: A python package for ml-based heterogeneous treatment effects estimation. 2022</p>
<p>ChatGPT. 2023</p>
<p>. Rephrasing Tool -QuillBot AI. 2023</p>
<p>Towards robust nlg bias evaluation with syntactically-diverse prompts. A Aggarwal, J Sun, N Peng, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Program synthesis with large language models. J Austin, A Odena, M Nye, M Bosma, H Michalewski, D Dohan, E Jiang, C Cai, M Terry, Q Le, arXiv:2108.077322021arXiv preprint</p>
<p>Membership inference attacks and generalization: A causal perspective. T Baluta, S Shen, S Hitarth, S Tople, P Saxena, arXiv:2209.086152022arXiv preprint</p>
<p>Weakly supervised part-of-speech tagging using eyetracking data. M Barrett, J Bingel, F Keller, A Søgaard, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Short Papers. the 54th Annual Meeting of the Association for Computational Linguistics20162</p>
<p>Language models can explain neurons in language models. S Bills, N Cammarata, D Mossing, H Tillman, L Gao, G Goh, I Sutskever, J Leike, J Wu, W Saunders, 2023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in NeurIPS. M Larochelle, R Ranzato, M Hadsell, H Balcan, Lin, Curran Associates, Inc202033</p>
<p>M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Double/debiased machine learning for treatment and causal parameters. V Chernozhukov, D Chetverikov, M Demirer, E Duflo, C Hansen, W Newey, J Robins, arXiv:1608.000602016arXiv preprint</p>
<p>Learning equivalence classes of bayesian-network structures. D M Chickering, Journal of Machine Learning Research. 22002</p>
<p>Computational assessment of text readability: A survey of current and future research. K Collins-Thompson, ITL-International Journal of Applied Linguistics. 1652014</p>
<p>Bayesian network structure learning with integer programming: Polytopes, facets and complexity. J Cussens, M Järvisalo, J H Korhonen, M Bartlett, JAIR. 582017</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, arXiv:2305.143252023arXiv preprint</p>
<p>Causality in configurable software systems. C Dubslaff, K Weis, C Baier, S Apel, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Automated repair of programs from large language models. Z Fan, X Gao, M Mirchev, A Roychoudhury, S H Tan, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Causality-guided adaptive interventional debugging. A Fariha, S Nath, A Meliou, Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data. the 2020 ACM SIGMOD International Conference on Management of Data2020</p>
<p>Cognitively motivated features for readability assessment. L Feng, N Elhadad, M Huenerfauth, Proceedings of the 12th Conference of the European Chapter of the ACL. the 12th Conference of the European Chapter of the ACL2009. 2009</p>
<p>Towards causal analysis of empirical software engineering data: The impact of programming languages on coding competitions. C A Furia, R Torkar, R Feldt, arXiv:2301.075242023arXiv preprint</p>
<p>Application of theorem proving to problem solving. C Green, Readings in Artificial Intelligence. Elsevier1981</p>
<p>Program synthesis. S Gulwani, O Polozov, R Singh, Foundations and Trends® in Programming Languages. 42017</p>
<p>Learning bayesian networks: The combination of knowledge and statistical data. D Heckerman, D Geiger, D M Chickering, Machine Learning. 1995</p>
<p>. D Hendrycks, S Basart, S Kadavath, M Mazeika, A Arora, E Guo, C Burns, S Puranik, H He, D Song, Steinhardt, J. Measuring coding challenge competence with apps. NeurIPS. 2021</p>
<p>Scalable inference of asynchronous event causality. C.-H Hsiao, S Narayanasamy, E M I Khan, C L Pereira, G A Pokam, Asyncclock, ACM SIGPLAN Notices. 522017</p>
<p>Codesearchnet challenge: Evaluating the state of semantic code search. H Husain, H.-H Wu, T Gazit, M Allamanis, M Brockschmidt, arXiv:1909.094362019arXiv preprint</p>
<p>Evaluating the robustness of discrete prompts. Y Ishibashi, D Bollegala, K Sudoh, S Nakamura, Proceedings of the 17th Conference of the European Chapter. the 17th Conference of the European Chapterthe Association for Computational Linguistics2023</p>
<p>Cc: Causality-aware coverage criterion for deep neural networks. Z Ji, P Ma, Y Yuan, S Wang, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>Is chatgpt a good translator? a preliminary study. W Jiao, W Wang, J.-T Huang, X Wang, Z Tu, arXiv:2301.087452023arXiv preprint</p>
<p>Causal testing: understanding defects' root causes. B Johnson, Y Brun, A Meliou, Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. the ACM/IEEE 42nd International Conference on Software Engineering2020</p>
<p>Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. L Kuhn, Y Gal, S Farquhar, arXiv:2302.096642023arXiv preprint</p>
<p>Mastering code generation through pretrained models and deep reinforcement learning. H Le, Y Wang, A D Gotmare, S Savarese, S C H Hoi, Coderl, Advances in Neural Information Processing Systems. 352022</p>
<p>Pushing on text readability assessment: A transformer meets handcrafted linguistic features. B W Lee, Y S Jang, J Lee, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>LFTK: Handcrafted features in computational linguistics. B W Lee, J Lee, Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023. the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023July 2023</p>
<p>Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Dal Lago, T Hubert, P Choy, C De Masson D'autume, I Babuschkin, X Chen, P.-S Huang, J Welbl, S Gowal, A Cherepanov, J Molloy, D Mankowitz, E Sutherland Robson, P Kohli, N De Freitas, K Kavukcuoglu, Vinyals, arXiv:2203.07814Competition-level code generation with alphacode. 2022arXiv preprint</p>
<p>Cctest: Testing and repairing code completion systems. Z Li, C Wang, Z Liu, H Wang, S Wang, C Gao, 2023</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. P Liu, W Yuan, J Fu, Z Jiang, H Hayashi, G Neubig, ACM Comput. Surv. 2023</p>
<p>On the reliability and explainability of automated code generation approaches. Y Liu, C Tantithamthavorn, Y Liu, L Li, arXiv:2302.095872023arXiv preprint</p>
<p>L Lorch, J Rothfuss, B Schölkopf, A Krause, Dibs, Differentiable bayesian structure learning. Advances in Neural Information Processing Systems. 202134</p>
<p>Codexglue: A machine learning benchmark dataset for code understanding and generation. S Lu, D Guo, S Ren, J Huang, A Svyatkovskiy, A Blanco, C Clement, D Drain, D Jiang, D Tang, arXiv:2102.046642021arXiv preprint</p>
<p>Automatic analysis of syntactic complexity in second language writing. X Lu, International journal of corpus linguistics. 152010</p>
<p>Toward automatic program synthesis. Z Manna, R J Waldinger, Communications of the ACM. 141971</p>
<p>The stanford corenlp natural language processing toolkit. C D Manning, M Surdeanu, J Bauer, J R Finkel, S Bethard, D Mcclosky, Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations. 52nd annual meeting of the association for computational linguistics: system demonstrations2014</p>
<p>Introduction to causal inference. B Neal, </p>
<p>R Openai, arXivGpt-4 technical report. 2023</p>
<p>Llm is like a box of chocolates: the non-determinism of chatgpt in code generation. S Ouyang, J M Zhang, M Harman, M Wang, arXiv:2308.028282023arXiv preprint</p>
<p>Bleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Asleep at the keyboard? assessing the security of github copilot's code contributions. H Pearce, B Ahmad, B Tan, B Dolan-Gavitt, R Karri, 2022 IEEE Symposium on Security and Privacy (SP). IEEE2022</p>
<p>. J Pearl, Causality, 2009Cambridge university press</p>
<p>Elements of causal inference: foundations and learning algorithms. J Peters, D Janzing, B Schölkopf, 2017The MIT Press</p>
<p>A call for clarity in reporting BLEU scores. M Post, Proceedings of the Third Conference on Machine Translation: Research Papers. the Third Conference on Machine Translation: Research PapersBelgium, BrusselsAssociation for Computational LinguisticsOct. 2018</p>
<p>Automatic prompt optimization with" gradient descent" and beam search. R Pryzant, D Iter, J Li, Y T Lee, C Zhu, M Zeng, arXiv:2305.034952023arXiv preprint</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Codebleu: a method for automatic evaluation of code synthesis. S Ren, D Guo, S Lu, L Zhou, S Liu, D Tang, N Sundaresan, M Zhou, A Blanco, S Ma, arXiv:2009.102972020arXiv preprint</p>
<p>Multitask prompted training enables zero-shot task generalization. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, A Raja, M Dey, M S Bari, C Xu, U Thakker, S S Sharma, E Szczechla, T Kim, G Chhablani, N V Nayak, D Datta, J Chang, M T Jiang, H Wang, M Manica, S Shen, Z X Yong, H Pandey, R Bawden, T Wang, T Neeraj, J Rozen, A Sharma, A Santilli, T Févry, J A Fries, R Teehan, T L Scao, S Biderman, L Gao, T Wolf, A M Rush, ICLR 2022Virtual Event. April 25-29, 2022. 2022</p>
<p>Learning bayesian networks with thousands of variables. M Scanagatta, C P De Campos, G Corani, M Zaffalon, NIPS. 2015</p>
<p>Applications of statistical causal inference in software engineering. J Siebert, Information and Software Technology. 1071982023</p>
<p>Large language models encode clinical knowledge. K Singhal, S Azizi, T Tu, S S Mahdavi, J Wei, H W Chung, N Scales, A Tanwani, H Cole-Lewis, S Pfohl, Nature. 2023</p>
<p>Program synthesis by sketching. A Solar-Lezama, 2008University of California, Berkeley</p>
<p>Causation, prediction, and search. P Spirtes, C N Glymour, R Scheines, D Heckerman, 2000</p>
<p>Causality-based neural network repair. B Sun, J Sun, L H Pham, J Shi, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>A grammar-based structural cnn decoder for code generation. Z Sun, Q Zhu, L Mou, Y Xiong, G Li, L Zhang, Proceedings of the AAAI conference on artificial intelligence. the AAAI conference on artificial intelligence201933</p>
<p>The max-min hill-climbing bayesian network structure learning algorithm. I Tsamardinos, L E Brown, C F Aliferis, Machine learning. 6512006</p>
<p>On software productivity analysis with propensity score matching. M Tsunoda, S Amasaki, ACM/IEEE International Symposium on Empirical Software Engineering and Measurement. 2017. 2017IEEE</p>
<p>Search based software test data generation for structural testing: a perspective. S Varshney, M Mehrotra, ACM SIGSOFT Software Engineering Notes. 382013</p>
<p>No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence. C Wang, Y Yang, C Gao, Y Peng, H Zhang, M R Lyu, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Self-consistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q V Le, E H Chi, D Zhou, CoRR abs/2203.111712022</p>
<p>Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. Y Wang, W Wang, S Joty, S C Hoi, arXiv:2109.008592021arXiv preprint</p>
<p>Code generation as a dual task of code summarization. B Wei, G Li, X Xia, Z Fu, Jin , Z , Advances in neural information processing systems. 322019</p>
<p>J Wei, Y Tay, R Bommasani, C Raffel, B Zoph, S Borgeaud, D Yogatama, M Bosma, D Zhou, D Metzler, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903Chain of thought prompting elicits reasoning in large language models. 2022arXiv preprint</p>
<p>M Xia, E Kochmar, T Briscoe, arXiv:1906.07580Text readability assessment for second language learners. 2019arXiv preprint</p>
<p>Uncertainty quantification with pre-trained language models: A large-scale empirical analysis. Y Xiao, P P Liang, U Bhatt, W Neiswanger, R Salakhutdinov, L.-P Morency, arXiv:2210.047142022arXiv preprint</p>
<p>C Yang, X Wang, Y Lu, H Liu, Q V Le, D Zhou, X Chen, arXiv:2309.03409Large language models as optimizers. 2023arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T L Griffiths, Y Cao, K Narasimhan, arXiv:2305.106012023arXiv preprint</p>
<p>A syntactic neural model for general-purpose code generation. P Yin, G Neubig, arXiv:1704.016962017arXiv preprint</p>
<p>Dag-gnn: Dag structure learning with graph neural networks. Y Yu, J Chen, T Gao, M Yu, International Conference on Machine Learning. PMLR2019</p>
<p>Adaptive fairness improvement based on causality analysis. M Zhang, J Sun, Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering2022</p>
<p>Y Zhang, J Yang, Y Yuan, A C Yao, .-C, arXiv:2308.04371Cumulative reasoning with large language models. 2023arXiv preprint</p>
<p>Dags with no tears: Continuous optimization for structure learning. X Zheng, B Aragam, P K Ravikumar, E P Xing, Advances in Neural Information Processing Systems. 312018</p>
<p>Large language models are human-level prompt engineers. Y Zhou, A I Muresanu, Z Han, K Paster, S Pitis, H Chan, J Ba, arXiv:2211.019102022arXiv preprint</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
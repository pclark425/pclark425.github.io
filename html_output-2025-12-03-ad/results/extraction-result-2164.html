<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2164 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2164</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2164</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-277999781</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2504.16728v2.pdf" target="_blank">IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery</a></p>
                <p><strong>Paper Abstract:</strong> The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery? This work tackles the crucial first stage of research, generating novel hypotheses. While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach. To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation. IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and query-based literature synthesis. Designed to empower researchers with greater control and insight throughout the ideation process. We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation. We open-source our code at https://github.com/Anikethh/IRIS-Interactive-Research-Ideation-System</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2164.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2164.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive Research Ideation System (IRIS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A human-in-the-loop platform that combines LLM-driven ideation, an LLM-based review agent, and targeted retrieval (ScholarQA) within a Monte Carlo Tree Search (MCTS) loop to generate, refine, and rank research briefs and hypotheses with fine-grained human steering.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>IRIS</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>human-in-the-loop hybrid system (LLM-based + retrieval + MCTS)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific ideation / research hypothesis generation (multi-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generates research briefs comprising Title, Proposed Methodology, and Experiment Plan (scientific hypotheses and research ideas)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Multi-tiered: (1) LLM-based Review Agent provides hierarchical, aspect-level scores and actionable feedback; (2) LLM-as-a-judge absolute scoring (1-10) and pairwise comparisons aggregated into ELO ratings; (3) human researcher verifies/filters fine-grained feedback and acts as final judge; (4) retrieval-grounding using Ai2 ScholarQA to cite and surface supporting literature used to contextualize/ground ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Operationalized via MCTS exploration (exploration constant c) promoting diverse nodes; evaluation proxies: LLM-as-a-judge absolute scores (1-10) and pairwise ELO ratings; qualitative human judgments about novelty (user reports of novelty in study). No explicit distance-from-training-data or automated novelty-to-corpus metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>User study/automated eval: interaction within IRIS improved hypothesis quality — average absolute score increased by 0.5 points and ELO ratings rose by 12 points for tree depth = 3; qualitatively 25% of users found substantial improvement, 50% marginal improvement, 25% similar. No explicit breakdown for novel vs familiar tasks provided.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>LLM-as-a-judge alignment with humans: ELO-based rankings correlated moderately with human baseline (Pearson r = 0.60); absolute LLM scores correlated weakly (Pearson r = 0.45). No precision/recall/FP/FN rates reported for validation decisions; human verification used to filter/accept feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Paper reports that pushing for novelty can evoke 'reward-hacking' in LLM evaluation (models produce superficially novel-sounding or plagiarized outputs); thus validation reliability of LLM-only judges decreases for aggressively novel outputs unless human verification is applied. No quantitative curve reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — evidence of asymmetry: LLMs can generate fluent, apparently novel hypotheses but these can be fabricated or skillfully plagiarized; LLM-based validation only moderately aligns with humans and can be gamed, so generation often outpaces reliable automated validation. IRIS mitigates this via fine-grained human-verified feedback and retrieval grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Qualitative: retrieval grounding quality varied by domain (e.g., lower quality for chemistry and physics due to corpus coverage), suggesting worse performance when topic literature is sparse (OOD relative to Semantic Scholar corpus). No numeric OOD vs ID metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>LLM-as-a-judge absolute scores had weaker correlation with human judgments (r=0.45) compared to ELO (r=0.60), indicating calibration issues in raw scoring; no confidence calibration metrics reported and likely degrade for novel/gamed outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Validation via MCTS + repeated Review Agent calls is computationally intensive; IRIS exposes budget controls and adjusts exploration constant c to trade exploration vs. computation. No wall-clock or FLOP costs provided.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human-in-the-loop verification of fine-grained feedback, hierarchical review taxonomy, retrieval-grounding (ScholarQA), MCTS exploration/exploitation control, and researcher steerability over tree and feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>IRIS demonstrates that combining MCTS-driven LLM ideation with fine-grained LLM reviews and human verification improves perceived hypothesis quality (absolute score +0.5, ELO +12 at depth 3). The paper also documents validation weaknesses of pure LLM judges (moderate alignment with humans and susceptibility to reward-hacking), arguing for human oversight and retrieval grounding to mitigate fabricated or gamed outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2164.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2164.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Review Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Review Agent (hierarchical, fine-grained evaluator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM agent within IRIS that evaluates generated research briefs using a hierarchical taxonomy of scientific critique, returns aspect-level scores and actionable feedback, and supplies rewards used in MCTS. It can run a coarse whole-brief evaluation automatically and a researcher-triggered fine-grained segment-level review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Review Agent (LLM-based)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model used as automated evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>evaluation of generated research ideas (scientific ideation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>not for primary generation — it generates evaluative feedback, aspect-level critiques, and numerical scores used as rewards</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Computes averaged rewards across hierarchical aspects (originality, feasibility, clarity, etc.); provides fine-grained actionable feedback per segment when researcher triggers; final reward for MCTS computed from researcher-verified aspects (human can omit irrelevant pieces).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Assesses 'Originality' as an aspect in the hierarchical taxonomy but does not implement an automated dataset-distance novelty metric; novelty judgments are subjective scores produced by the LLM and subject to human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A (not a generator of hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Proxy evidence: LLM-based judgments (as used by IRIS) show moderate alignment with humans when aggregated into ELO (r=0.60) and weaker alignment for absolute scores (r=0.45); the paper reports cases of reward-hacking when systems are pushed for novelty, indicating potential false validations in high-novelty settings.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Reliability drops for aggressively novel outputs because LLM judges can be gamed (models produce superficially novel or jargon-laden outputs that receive high automated scores). Human verification of aspect-level feedback is used to mitigate this.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — the Review Agent can rate or justify novel-sounding ideas that may be fabricated; authors report this asymmetry and implement human verification to reduce it.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified; likely degrades for highly novel or domain-specific content where the LLM judge lacks specialized grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Imperfect — absolute scores less aligned with human judgments than ELO aggregated pairwise comparisons, implying calibration issues in single-shot scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Each evaluation invocation imposes LLM call cost; within MCTS repeated evaluations increase compute substantially. No numeric cost given.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Human verification of feedback, fine-grained aspect-level taxonomy to localize critiques, and use of verified aspects to compute rewards for MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The Review Agent enables automated, fine-grained evaluation but is susceptible to reward-hacking and imperfect alignment with human judgments; researcher verification of feedback is necessary to produce reliable rewards for search and to mitigate fabricated or gamified evaluations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2164.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2164.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-judge evaluation paradigm</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using large language models to automatically score or compare generated outputs (absolute scoring and pairwise comparisons aggregated to ELO) as a proxy for human evaluation of hypothesis quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-as-a-judge</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>evaluation methodology using large language models</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>evaluation of generated scientific hypotheses/ideas</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>N/A (evaluation-only role)</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Absolute scoring on a 1-10 scale and head-to-head pairwise preference elicitation aggregated into ELO ratings; used both to evaluate baselines and as the Review Agent's proxy reward for MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>None intrinsic — novelty judged subjectively by LLM scorers as part of overall quality assessment; no explicit novelty-distance metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>N/A</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Reported alignment with humans: ELO aggregated LLM judgments correlated moderately with human rankings (Pearson r = 0.60), absolute LLM scores correlated less (r = 0.45). Authors plan to prefer ELO over absolute scores due to better alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Paper documents that LLM-as-a-judge can be gamed when optimizing for novelty (reward-hacking), resulting in inflated scores for low-value or plagiarized outputs; reliability declines for high-novelty, adversarially-optimized outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — the evaluation LLMs can be tricked into overrating novel-appearing but low-quality or plagiarized outputs, creating a mismatch between generation fluency and true scientific validity.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not quantified specifically; implied degradation for domains poorly covered by retrieval corpora or when judging very novel ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Partial: ELO pairwise aggregation yields better correlation with humans than single absolute scores, indicating calibration improvements via pairwise methodology but overall imperfect calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Requires additional LLM calls per evaluation and more for pairwise comparisons (large if many candidates); no numeric cost reported.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Switching to ELO-based pairwise comparisons, combining LLM judgments with human verification, and using fine-grained aspect-level feedback to reduce gaming.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mixed</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-as-a-judge provides a scalable automated evaluation signal and ELO aggregation aligns moderately with human preferences (r=0.60), but absolute automated scores are less reliable and LLM judges can be gamed, especially when novelty is explicitly optimized for.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2164.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2164.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini-2.0-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini-2.0-Flash (DeepMind)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large language model (used as the core LLM in the IRIS implementation) with built-in safety filters; it powers ideation, review, and other LLM-based agents in the system.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gemini-2.0-Flash</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general language understanding and generation applied to scientific ideation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>natural-language generation for research briefs, feedback, and retrieval-based summarization</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Within IRIS, Gemini is used for generation and (when used as evaluator) for scoring; system also relies on Gemini's built-in safety filters. Final validation depends on Review Agent processes and human verification.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not provided by model; novelty assessed externally by review and MCTS mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used as the default LLM backend; no standalone performance metrics reported in paper for Gemini beyond being compared with other baseline models in LLM-as-a-judge experiments (figure referenced but numeric baseline breakdown not presented in-text).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Not separately measured for this model; general caveat applies that base LLMs can produce fabricated or plagiarized content that must be validated by retrieval and human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Not separately quantified; IRIS-level observations about generation/validation asymmetry apply when Gemini is used as generator or judge.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not reported specifically for Gemini in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; paper notes limitations in model choice and that frontier models were not evaluated due to budget constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>No per-model cost reported; the system-level MCTS and repeated LLM calls are described as computationally intensive.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Used in combination with Review Agent, retrieval grounding, and human verification within IRIS to mitigate hallucination and misalignment.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Gemini-2.0-Flash served as the working LLM backend for IRIS; the paper does not make claims about Gemini-specific validation strengths or weaknesses beyond system-level observations about LLM judges and the need for human oversight.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2164.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2164.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MCTS for Ideation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Monte Carlo Tree Search adaptation for research idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An adaptation of Monte Carlo Tree Search (MCTS) to explore and exploit the space of research ideas using the Review Agent as a proxy reward function; actions include generate, refine via retrieval, refine via review, and refine via user feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MCTS (adapted)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>search/planning algorithm integrated with LLM agents</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>search over research idea/hypothesis space (scientific ideation)</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>systematically samples and refines candidate research briefs by applying generation/refinement actions to nodes in a search tree</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Node evaluations (rewards) derived from the Review Agent (LLM-as-a-judge) and, when applicable, researcher-verified feedback; best child selected by highest average reward (Q/N).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Exploration controlled via UCT/ exploration constant c — higher c encourages exploring novel branches; novelty is operationally induced by exploration rather than measured directly.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Using MCTS with depth = 3 and user interaction produced mean improvements reported (absolute +0.5, ELO +12). MCTS allowed trade-offs between producing fewer high-quality outputs (lower c) vs more diverse outputs (higher c).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Dependent on Review Agent quality; if the judge is imperfect (susceptible to reward-hacking), MCTS can amplify gaming; IRIS mitigates by requiring researcher verification of feedback used to compute rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Higher exploration (novelty) risks misleading rewards if the judge is gamed; the paper documents reward-hacking where novelty optimization led to vacuous or jargon-filled outputs scoring highly under naive LLM judges.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — MCTS can produce novel candidates that outstrip the reliability of automated validation if the Review Agent is not robust; human verification is used to close this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not numerically characterized; authors note the need for budget controls since deeper/wider search increases chance of producing low-quality or ungrounded novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported quantitatively; quality of node evaluation depends on judge calibration (see Review Agent / LLM-as-a-judge).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>MCTS is computationally intensive because each node evaluation requires Review Agent calls; IRIS exposes budget controls to limit iterations N, max depth, and exploration constant c to control cost.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Combines LLM-based review with human-verified feedback, retrieval grounding, and adjustable exploration/exploitation to reduce reward-hacking and improve meaningful novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>MCTS effectively extends test-time compute to explore idea spaces and, when paired with human-verified LLM reviews and retrieval grounding, yields measurable improvements in evaluated hypothesis quality; however, it can amplify validation weaknesses if the automated judge is imperfect.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2164.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2164.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval Agent (Ai2 ScholarQA)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval Agent using Ai2 ScholarQA + re-ranking and summarization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Component that synthesizes targeted queries for literature retrieval, runs a two-stage retrieval pipeline using Ai2 ScholarQA (Semantic Scholar snippet search), re-ranks passages, clusters and generates cited section-wise summaries to ground ideation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval Agent (Ai2 ScholarQA backed)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented LLM pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>literature grounding for research ideation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>does not produce hypotheses primarily; generates retrieval queries, extracted quotes, clustered passages, and cited summaries to support ideation</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Provides evidence and citations to support claims in generated briefs; grounding acts as an external check against hallucination but not a formal validation — human checks the cited literature. Also allows user-uploaded PDFs parsed via Grobid to augment retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not used as a novelty metric; is used to assess whether an idea is grounded in existing literature (helping detect non-novel or plagiarized content).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Users found retrieval useful for grounding but variable across domains; literature summaries rated lower for chemistry/physics (mean 3.7/5) likely due to corpus coverage gaps. No numeric recall/precision figures reported.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Acts as grounding support rather than binary validator; quality varies with corpus coverage. No numeric validation metrics reported.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Retrieval can expose lack of novelty (if prior art exists) and improves trustworthiness of ideas, but limited corpus coverage reduces effectiveness for some OOD domains.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Retrieval helps reduce generation-validation asymmetry by providing citations and evidence, but cannot fully substitute domain expert judgement.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Degraded for domains with sparse coverage in Semantic Scholar (e.g., some chemistry/physics topics), per user feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Not reported; retrieval quality varies with query generation and corpus coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Two-stage retrieval + re-ranking + summarization has non-trivial cost; exact cost not provided. Users can upload PDFs to mitigate missed literature.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Retrieval-grounding using ScholarQA, re-ranking, clustering and cited summaries; user-supplied PDFs; integration of retrieval results into ideation and review workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Targeted retrieval and cited summaries help ground generated hypotheses and mitigate hallucination, but effectiveness depends on corpus coverage and cannot replace expert validation; users reported variable satisfaction across domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2164.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2164.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Agentic LLM systems (examples)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Agentic LLM-based scientific discovery systems (e.g., AI-Researcher, Researchagent, Acceleron)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A class of systems that orchestrate one or more LLM agents to perform end-to-end research tasks such as hypothesis generation, literature synthesis, experimental planning, or other short-horizon scientific tasks; often evaluated via automated judgments or human validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Agentic LLM systems (AI-Researcher, Researchagent, Acceleron, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent LLM frameworks / agentic systems</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>automated scientific discovery and short-horizon research tasks</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>generate hypotheses, syntheses, experimental designs, literature summaries, and other research artifacts</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Typically human validation and/or LLM-as-a-judge evaluations; many existing systems use post-hoc LLM evaluation or human inspection rather than integrated human-in-the-loop steering during generation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Some systems optimize for novelty via search or iterative planning; metrics vary by work (novelty heuristics, automated scoring, or human judgment). The paper cautions that novelty optimization can lead to superficial or plagiarized outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Prior work reports promising results on short-horizon tasks (paper cites agentic systems outperforming PhD researchers/postdocs on tasks like QA, summarization, contradiction detection in some studies), but performance claims vary across studies and tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation often relies on human judgment or LLM-as-a-judge; authors highlight cases of fabrication, reward-hacking, and skillful plagiarism in prior automated hypothesis generation systems, meaning validation performance is fragile for novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Reported vulnerability: when systems are pushed to be more novel, naive automated evaluation loops can be gamed, increasing false acceptance of low-value novel outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Yes — prior agentic systems have demonstrated a gap where generated outputs can be fluent/novel-seeming but not scientifically valid, and automated validation approaches have failed to reliably detect these issues.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Not systematically reported in this paper; prior systems may struggle outside the domains and distributions they were validated on.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td>Varies by system; several cited works and the present paper flag alignment, fabrication, and calibration concerns with agentic LLM validation.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td>Agentic multi-agent frameworks can be computationally expensive, especially when incorporating repeated evaluation or search; specifics are system-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Proposed/used mechanisms across works include human-in-the-loop designs, fine-grained feedback, retrieval-grounding, multi-agent debate, and search/optimization (MCTS, bandit planning); IRIS emphasizes researcher steering and fine-grained verification.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The literature and this paper document both the potential of agentic LLM systems to aid or outperform humans on short-horizon tasks and the serious validation gaps (fabrication, reward-hacking, plagiarism). The authors argue for integrated human-in-the-loop controls and retrieval grounding to reduce erroneous acceptance of novel but invalid outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers <em>(Rating: 2)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models <em>(Rating: 2)</em></li>
                <li>All that glitters is not novel: Plagiarism in ai generated research <em>(Rating: 2)</em></li>
                <li>Automated hypothesis generation based on mining scientific literature <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery <em>(Rating: 2)</em></li>
                <li>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2164",
    "paper_id": "paper-277999781",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "IRIS",
            "name_full": "Interactive Research Ideation System (IRIS)",
            "brief_description": "A human-in-the-loop platform that combines LLM-driven ideation, an LLM-based review agent, and targeted retrieval (ScholarQA) within a Monte Carlo Tree Search (MCTS) loop to generate, refine, and rank research briefs and hypotheses with fine-grained human steering.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "IRIS",
            "system_type": "human-in-the-loop hybrid system (LLM-based + retrieval + MCTS)",
            "domain": "general scientific ideation / research hypothesis generation (multi-domain)",
            "generation_capability": "generates research briefs comprising Title, Proposed Methodology, and Experiment Plan (scientific hypotheses and research ideas)",
            "validation_method": "Multi-tiered: (1) LLM-based Review Agent provides hierarchical, aspect-level scores and actionable feedback; (2) LLM-as-a-judge absolute scoring (1-10) and pairwise comparisons aggregated into ELO ratings; (3) human researcher verifies/filters fine-grained feedback and acts as final judge; (4) retrieval-grounding using Ai2 ScholarQA to cite and surface supporting literature used to contextualize/ground ideas.",
            "novelty_measure": "Operationalized via MCTS exploration (exploration constant c) promoting diverse nodes; evaluation proxies: LLM-as-a-judge absolute scores (1-10) and pairwise ELO ratings; qualitative human judgments about novelty (user reports of novelty in study). No explicit distance-from-training-data or automated novelty-to-corpus metric reported.",
            "generation_performance": "User study/automated eval: interaction within IRIS improved hypothesis quality — average absolute score increased by 0.5 points and ELO ratings rose by 12 points for tree depth = 3; qualitatively 25% of users found substantial improvement, 50% marginal improvement, 25% similar. No explicit breakdown for novel vs familiar tasks provided.",
            "validation_performance": "LLM-as-a-judge alignment with humans: ELO-based rankings correlated moderately with human baseline (Pearson r = 0.60); absolute LLM scores correlated weakly (Pearson r = 0.45). No precision/recall/FP/FN rates reported for validation decisions; human verification used to filter/accept feedback.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Paper reports that pushing for novelty can evoke 'reward-hacking' in LLM evaluation (models produce superficially novel-sounding or plagiarized outputs); thus validation reliability of LLM-only judges decreases for aggressively novel outputs unless human verification is applied. No quantitative curve reported.",
            "generation_validation_asymmetry": "Yes — evidence of asymmetry: LLMs can generate fluent, apparently novel hypotheses but these can be fabricated or skillfully plagiarized; LLM-based validation only moderately aligns with humans and can be gamed, so generation often outpaces reliable automated validation. IRIS mitigates this via fine-grained human-verified feedback and retrieval grounding.",
            "out_of_distribution_performance": "Qualitative: retrieval grounding quality varied by domain (e.g., lower quality for chemistry and physics due to corpus coverage), suggesting worse performance when topic literature is sparse (OOD relative to Semantic Scholar corpus). No numeric OOD vs ID metrics provided.",
            "calibration_quality": "LLM-as-a-judge absolute scores had weaker correlation with human judgments (r=0.45) compared to ELO (r=0.60), indicating calibration issues in raw scoring; no confidence calibration metrics reported and likely degrade for novel/gamed outputs.",
            "validation_computational_cost": "Validation via MCTS + repeated Review Agent calls is computationally intensive; IRIS exposes budget controls and adjusts exploration constant c to trade exploration vs. computation. No wall-clock or FLOP costs provided.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human-in-the-loop verification of fine-grained feedback, hierarchical review taxonomy, retrieval-grounding (ScholarQA), MCTS exploration/exploitation control, and researcher steerability over tree and feedback.",
            "evidence_type": "supports",
            "key_findings": "IRIS demonstrates that combining MCTS-driven LLM ideation with fine-grained LLM reviews and human verification improves perceived hypothesis quality (absolute score +0.5, ELO +12 at depth 3). The paper also documents validation weaknesses of pure LLM judges (moderate alignment with humans and susceptibility to reward-hacking), arguing for human oversight and retrieval grounding to mitigate fabricated or gamed outputs.",
            "uuid": "e2164.0"
        },
        {
            "name_short": "Review Agent",
            "name_full": "LLM-based Review Agent (hierarchical, fine-grained evaluator)",
            "brief_description": "An LLM agent within IRIS that evaluates generated research briefs using a hierarchical taxonomy of scientific critique, returns aspect-level scores and actionable feedback, and supplies rewards used in MCTS. It can run a coarse whole-brief evaluation automatically and a researcher-triggered fine-grained segment-level review.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Review Agent (LLM-based)",
            "system_type": "large language model used as automated evaluator",
            "domain": "evaluation of generated research ideas (scientific ideation)",
            "generation_capability": "not for primary generation — it generates evaluative feedback, aspect-level critiques, and numerical scores used as rewards",
            "validation_method": "Computes averaged rewards across hierarchical aspects (originality, feasibility, clarity, etc.); provides fine-grained actionable feedback per segment when researcher triggers; final reward for MCTS computed from researcher-verified aspects (human can omit irrelevant pieces).",
            "novelty_measure": "Assesses 'Originality' as an aspect in the hierarchical taxonomy but does not implement an automated dataset-distance novelty metric; novelty judgments are subjective scores produced by the LLM and subject to human verification.",
            "generation_performance": "N/A (not a generator of hypotheses).",
            "validation_performance": "Proxy evidence: LLM-based judgments (as used by IRIS) show moderate alignment with humans when aggregated into ELO (r=0.60) and weaker alignment for absolute scores (r=0.45); the paper reports cases of reward-hacking when systems are pushed for novelty, indicating potential false validations in high-novelty settings.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Reliability drops for aggressively novel outputs because LLM judges can be gamed (models produce superficially novel or jargon-laden outputs that receive high automated scores). Human verification of aspect-level feedback is used to mitigate this.",
            "generation_validation_asymmetry": "Yes — the Review Agent can rate or justify novel-sounding ideas that may be fabricated; authors report this asymmetry and implement human verification to reduce it.",
            "out_of_distribution_performance": "Not quantified; likely degrades for highly novel or domain-specific content where the LLM judge lacks specialized grounding.",
            "calibration_quality": "Imperfect — absolute scores less aligned with human judgments than ELO aggregated pairwise comparisons, implying calibration issues in single-shot scoring.",
            "validation_computational_cost": "Each evaluation invocation imposes LLM call cost; within MCTS repeated evaluations increase compute substantially. No numeric cost given.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Human verification of feedback, fine-grained aspect-level taxonomy to localize critiques, and use of verified aspects to compute rewards for MCTS.",
            "evidence_type": "supports",
            "key_findings": "The Review Agent enables automated, fine-grained evaluation but is susceptible to reward-hacking and imperfect alignment with human judgments; researcher verification of feedback is necessary to produce reliable rewards for search and to mitigate fabricated or gamified evaluations.",
            "uuid": "e2164.1"
        },
        {
            "name_short": "LLM-as-a-judge",
            "name_full": "LLM-as-a-judge evaluation paradigm",
            "brief_description": "Using large language models to automatically score or compare generated outputs (absolute scoring and pairwise comparisons aggregated to ELO) as a proxy for human evaluation of hypothesis quality.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM-as-a-judge",
            "system_type": "evaluation methodology using large language models",
            "domain": "evaluation of generated scientific hypotheses/ideas",
            "generation_capability": "N/A (evaluation-only role)",
            "validation_method": "Absolute scoring on a 1-10 scale and head-to-head pairwise preference elicitation aggregated into ELO ratings; used both to evaluate baselines and as the Review Agent's proxy reward for MCTS.",
            "novelty_measure": "None intrinsic — novelty judged subjectively by LLM scorers as part of overall quality assessment; no explicit novelty-distance metrics reported.",
            "generation_performance": "N/A",
            "validation_performance": "Reported alignment with humans: ELO aggregated LLM judgments correlated moderately with human rankings (Pearson r = 0.60), absolute LLM scores correlated less (r = 0.45). Authors plan to prefer ELO over absolute scores due to better alignment.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Paper documents that LLM-as-a-judge can be gamed when optimizing for novelty (reward-hacking), resulting in inflated scores for low-value or plagiarized outputs; reliability declines for high-novelty, adversarially-optimized outputs.",
            "generation_validation_asymmetry": "Yes — the evaluation LLMs can be tricked into overrating novel-appearing but low-quality or plagiarized outputs, creating a mismatch between generation fluency and true scientific validity.",
            "out_of_distribution_performance": "Not quantified specifically; implied degradation for domains poorly covered by retrieval corpora or when judging very novel ideas.",
            "calibration_quality": "Partial: ELO pairwise aggregation yields better correlation with humans than single absolute scores, indicating calibration improvements via pairwise methodology but overall imperfect calibration.",
            "validation_computational_cost": "Requires additional LLM calls per evaluation and more for pairwise comparisons (large if many candidates); no numeric cost reported.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Switching to ELO-based pairwise comparisons, combining LLM judgments with human verification, and using fine-grained aspect-level feedback to reduce gaming.",
            "evidence_type": "mixed",
            "key_findings": "LLM-as-a-judge provides a scalable automated evaluation signal and ELO aggregation aligns moderately with human preferences (r=0.60), but absolute automated scores are less reliable and LLM judges can be gamed, especially when novelty is explicitly optimized for.",
            "uuid": "e2164.2"
        },
        {
            "name_short": "Gemini-2.0-Flash",
            "name_full": "Gemini-2.0-Flash (DeepMind)",
            "brief_description": "A large language model (used as the core LLM in the IRIS implementation) with built-in safety filters; it powers ideation, review, and other LLM-based agents in the system.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Gemini-2.0-Flash",
            "system_type": "large language model",
            "domain": "general language understanding and generation applied to scientific ideation",
            "generation_capability": "natural-language generation for research briefs, feedback, and retrieval-based summarization",
            "validation_method": "Within IRIS, Gemini is used for generation and (when used as evaluator) for scoring; system also relies on Gemini's built-in safety filters. Final validation depends on Review Agent processes and human verification.",
            "novelty_measure": "Not provided by model; novelty assessed externally by review and MCTS mechanisms.",
            "generation_performance": "Used as the default LLM backend; no standalone performance metrics reported in paper for Gemini beyond being compared with other baseline models in LLM-as-a-judge experiments (figure referenced but numeric baseline breakdown not presented in-text).",
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Not separately measured for this model; general caveat applies that base LLMs can produce fabricated or plagiarized content that must be validated by retrieval and human oversight.",
            "generation_validation_asymmetry": "Not separately quantified; IRIS-level observations about generation/validation asymmetry apply when Gemini is used as generator or judge.",
            "out_of_distribution_performance": "Not reported specifically for Gemini in this paper.",
            "calibration_quality": "Not reported; paper notes limitations in model choice and that frontier models were not evaluated due to budget constraints.",
            "validation_computational_cost": "No per-model cost reported; the system-level MCTS and repeated LLM calls are described as computationally intensive.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Used in combination with Review Agent, retrieval grounding, and human verification within IRIS to mitigate hallucination and misalignment.",
            "evidence_type": "neutral",
            "key_findings": "Gemini-2.0-Flash served as the working LLM backend for IRIS; the paper does not make claims about Gemini-specific validation strengths or weaknesses beyond system-level observations about LLM judges and the need for human oversight.",
            "uuid": "e2164.3"
        },
        {
            "name_short": "MCTS for Ideation",
            "name_full": "Monte Carlo Tree Search adaptation for research idea generation",
            "brief_description": "An adaptation of Monte Carlo Tree Search (MCTS) to explore and exploit the space of research ideas using the Review Agent as a proxy reward function; actions include generate, refine via retrieval, refine via review, and refine via user feedback.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MCTS (adapted)",
            "system_type": "search/planning algorithm integrated with LLM agents",
            "domain": "search over research idea/hypothesis space (scientific ideation)",
            "generation_capability": "systematically samples and refines candidate research briefs by applying generation/refinement actions to nodes in a search tree",
            "validation_method": "Node evaluations (rewards) derived from the Review Agent (LLM-as-a-judge) and, when applicable, researcher-verified feedback; best child selected by highest average reward (Q/N).",
            "novelty_measure": "Exploration controlled via UCT/ exploration constant c — higher c encourages exploring novel branches; novelty is operationally induced by exploration rather than measured directly.",
            "generation_performance": "Using MCTS with depth = 3 and user interaction produced mean improvements reported (absolute +0.5, ELO +12). MCTS allowed trade-offs between producing fewer high-quality outputs (lower c) vs more diverse outputs (higher c).",
            "validation_performance": "Dependent on Review Agent quality; if the judge is imperfect (susceptible to reward-hacking), MCTS can amplify gaming; IRIS mitigates by requiring researcher verification of feedback used to compute rewards.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Higher exploration (novelty) risks misleading rewards if the judge is gamed; the paper documents reward-hacking where novelty optimization led to vacuous or jargon-filled outputs scoring highly under naive LLM judges.",
            "generation_validation_asymmetry": "Yes — MCTS can produce novel candidates that outstrip the reliability of automated validation if the Review Agent is not robust; human verification is used to close this gap.",
            "out_of_distribution_performance": "Not numerically characterized; authors note the need for budget controls since deeper/wider search increases chance of producing low-quality or ungrounded novel outputs.",
            "calibration_quality": "Not reported quantitatively; quality of node evaluation depends on judge calibration (see Review Agent / LLM-as-a-judge).",
            "validation_computational_cost": "MCTS is computationally intensive because each node evaluation requires Review Agent calls; IRIS exposes budget controls to limit iterations N, max depth, and exploration constant c to control cost.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Combines LLM-based review with human-verified feedback, retrieval grounding, and adjustable exploration/exploitation to reduce reward-hacking and improve meaningful novelty.",
            "evidence_type": "supports",
            "key_findings": "MCTS effectively extends test-time compute to explore idea spaces and, when paired with human-verified LLM reviews and retrieval grounding, yields measurable improvements in evaluated hypothesis quality; however, it can amplify validation weaknesses if the automated judge is imperfect.",
            "uuid": "e2164.4"
        },
        {
            "name_short": "Retrieval Agent (Ai2 ScholarQA)",
            "name_full": "Retrieval Agent using Ai2 ScholarQA + re-ranking and summarization",
            "brief_description": "Component that synthesizes targeted queries for literature retrieval, runs a two-stage retrieval pipeline using Ai2 ScholarQA (Semantic Scholar snippet search), re-ranks passages, clusters and generates cited section-wise summaries to ground ideation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Retrieval Agent (Ai2 ScholarQA backed)",
            "system_type": "retrieval-augmented LLM pipeline",
            "domain": "literature grounding for research ideation",
            "generation_capability": "does not produce hypotheses primarily; generates retrieval queries, extracted quotes, clustered passages, and cited summaries to support ideation",
            "validation_method": "Provides evidence and citations to support claims in generated briefs; grounding acts as an external check against hallucination but not a formal validation — human checks the cited literature. Also allows user-uploaded PDFs parsed via Grobid to augment retrieval.",
            "novelty_measure": "Not used as a novelty metric; is used to assess whether an idea is grounded in existing literature (helping detect non-novel or plagiarized content).",
            "generation_performance": "Users found retrieval useful for grounding but variable across domains; literature summaries rated lower for chemistry/physics (mean 3.7/5) likely due to corpus coverage gaps. No numeric recall/precision figures reported.",
            "validation_performance": "Acts as grounding support rather than binary validator; quality varies with corpus coverage. No numeric validation metrics reported.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Retrieval can expose lack of novelty (if prior art exists) and improves trustworthiness of ideas, but limited corpus coverage reduces effectiveness for some OOD domains.",
            "generation_validation_asymmetry": "Retrieval helps reduce generation-validation asymmetry by providing citations and evidence, but cannot fully substitute domain expert judgement.",
            "out_of_distribution_performance": "Degraded for domains with sparse coverage in Semantic Scholar (e.g., some chemistry/physics topics), per user feedback.",
            "calibration_quality": "Not reported; retrieval quality varies with query generation and corpus coverage.",
            "validation_computational_cost": "Two-stage retrieval + re-ranking + summarization has non-trivial cost; exact cost not provided. Users can upload PDFs to mitigate missed literature.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Retrieval-grounding using ScholarQA, re-ranking, clustering and cited summaries; user-supplied PDFs; integration of retrieval results into ideation and review workflows.",
            "evidence_type": "supports",
            "key_findings": "Targeted retrieval and cited summaries help ground generated hypotheses and mitigate hallucination, but effectiveness depends on corpus coverage and cannot replace expert validation; users reported variable satisfaction across domains.",
            "uuid": "e2164.5"
        },
        {
            "name_short": "Agentic LLM systems (examples)",
            "name_full": "Agentic LLM-based scientific discovery systems (e.g., AI-Researcher, Researchagent, Acceleron)",
            "brief_description": "A class of systems that orchestrate one or more LLM agents to perform end-to-end research tasks such as hypothesis generation, literature synthesis, experimental planning, or other short-horizon scientific tasks; often evaluated via automated judgments or human validation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Agentic LLM systems (AI-Researcher, Researchagent, Acceleron, etc.)",
            "system_type": "multi-agent LLM frameworks / agentic systems",
            "domain": "automated scientific discovery and short-horizon research tasks",
            "generation_capability": "generate hypotheses, syntheses, experimental designs, literature summaries, and other research artifacts",
            "validation_method": "Typically human validation and/or LLM-as-a-judge evaluations; many existing systems use post-hoc LLM evaluation or human inspection rather than integrated human-in-the-loop steering during generation.",
            "novelty_measure": "Some systems optimize for novelty via search or iterative planning; metrics vary by work (novelty heuristics, automated scoring, or human judgment). The paper cautions that novelty optimization can lead to superficial or plagiarized outputs.",
            "generation_performance": "Prior work reports promising results on short-horizon tasks (paper cites agentic systems outperforming PhD researchers/postdocs on tasks like QA, summarization, contradiction detection in some studies), but performance claims vary across studies and tasks.",
            "validation_performance": "Validation often relies on human judgment or LLM-as-a-judge; authors highlight cases of fabrication, reward-hacking, and skillful plagiarism in prior automated hypothesis generation systems, meaning validation performance is fragile for novel outputs.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Reported vulnerability: when systems are pushed to be more novel, naive automated evaluation loops can be gamed, increasing false acceptance of low-value novel outputs.",
            "generation_validation_asymmetry": "Yes — prior agentic systems have demonstrated a gap where generated outputs can be fluent/novel-seeming but not scientifically valid, and automated validation approaches have failed to reliably detect these issues.",
            "out_of_distribution_performance": "Not systematically reported in this paper; prior systems may struggle outside the domains and distributions they were validated on.",
            "calibration_quality": "Varies by system; several cited works and the present paper flag alignment, fabrication, and calibration concerns with agentic LLM validation.",
            "validation_computational_cost": "Agentic multi-agent frameworks can be computationally expensive, especially when incorporating repeated evaluation or search; specifics are system-dependent.",
            "human_validation_required": true,
            "gap_closing_mechanisms": "Proposed/used mechanisms across works include human-in-the-loop designs, fine-grained feedback, retrieval-grounding, multi-agent debate, and search/optimization (MCTS, bandit planning); IRIS emphasizes researcher steering and fine-grained verification.",
            "evidence_type": "supports",
            "key_findings": "The literature and this paper document both the potential of agentic LLM systems to aid or outperform humans on short-horizon tasks and the serious validation gaps (fabrication, reward-hacking, plagiarism). The authors argue for integrated human-in-the-loop controls and retrieval grounding to reduce erroneous acceptance of novel but invalid outputs.",
            "uuid": "e2164.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers",
            "rating": 2
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models",
            "rating": 2
        },
        {
            "paper_title": "All that glitters is not novel: Plagiarism in ai generated research",
            "rating": 2
        },
        {
            "paper_title": "Automated hypothesis generation based on mining scientific literature",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Agentic ai for scientific discovery: A survey of progress, challenges, and future directions",
            "rating": 1
        }
    ],
    "cost": 0.01938475,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery
24 May 2025</p>
<p>Aniketh Garikaparthi aniketh.g@tcs.com 
TCS Research</p>
<p>Manasi Patwardhan manasi.patwardhan@tcs.com 
TCS Research</p>
<p>Lovekesh Vig lovekesh.vig@tcs.com 
TCS Research</p>
<p>Arman Cohan arman.cohan@yale.edu 
Yale University</p>
<p>IRIS: Interactive Research Ideation System for Accelerating Scientific Discovery
24 May 2025F0B18A475921D7444EF898C6C10441D6arXiv:2504.16728v2[cs.AI]
The rapid advancement in capabilities of large language models (LLMs) raises a pivotal question: How can LLMs accelerate scientific discovery?This work tackles the crucial first stage of research, generating novel hypotheses.While recent work on automated hypothesis generation focuses on multi-agent frameworks and extending test-time compute, none of the approaches effectively incorporate transparency and steerability through a synergistic Human-in-the-loop (HITL) approach.To address this gap, we introduce IRIS: Interactive Research Ideation System, an open-source platform designed for researchers to leverage LLM-assisted scientific ideation.IRIS incorporates innovative features to enhance ideation, including adaptive test-time compute expansion via Monte Carlo Tree Search (MCTS), fine-grained feedback mechanism, and querybased literature synthesis.Designed to empower researchers with greater control and insight throughout the ideation process.We additionally conduct a user study with researchers across diverse disciplines, validating the effectiveness of our system in enhancing ideation.We open-source our code here.</p>
<p>Introduction</p>
<p>With the growing capabilities of large language models (LLMs), the automation of scientific discovery has captured a lot of attention (Gridach et al., 2025).Agentic LLM based systems have shown potential of outperforming PhD researchers and postdocs on short-horizon scientific tasks like question answering, summarization and contradiction detection in various domains (Skarlinski et al., 2024;Asai et al., 2024).These advancements have spurred new opportunities of LLMs accelerating scientific discovery, which is essential given the exponential growth of scientific publications (Landhuis, 2016;Fire and Guestrin, 2019).</p>
<p>Current solutions that leverage LLMs in scientific ideation primarily remain hinged on multiagent frameworks or extending test-time compute (Si et al., 2024;Hu et al., 2024;Gottweis, 2025), and aim to validate the quality of the final ideas through human validation or LLM-as-a-judge evaluations (Wang et al., 2024;Li et al., 2024;Baek et al., 2025).However, these approaches often fail to integrate human supervision during generation in a truly complementary manner, neglecting the nuanced expectations and goals of the user.Consequently, despite investing significant computational resources to develop objectively "novel" ideas, they might not align with the user's research goals, inevitably leading to dissatisfaction (Ou et al., 2022;Kim et al., 2024).</p>
<p>Moreover, the importance of meaningful human intervention in the research process cannot be overstated.Notably, AI models have been known to fabricate convincing yet fraudulent scientific information (Májovský et al., 2023).More troubling are cases of deceptive and misaligned AI behaviors (Ryan Greenblatt, 2025;Booth, 2025;Betley et al., 2025;Baker et al., 2025).Recent developments of more capable Agentic LLMs have shown difficulties in transparently delegating sub-tasks, leading to "reward hacking" behaviors (Anthropic, 2025).In the context of idea generation, we find signs of similar "reward hacking" where LLMs adopt fancy terminology e.g."Prompt Learning and Optimization Nexus" for building a library of prompts, or often proposing the use of "graphs" without any clear motivation or description behind the design choice.We observe that naive recursive feedback loops (Baek et al., 2025) forcing the LLM to be more novel inevitably lead to gamifying LLM-as-a-judge metrics without adding actual value.Gupta and Pruthi (2025) carefully study the results of AI-Researcher (Si et al., 2024) and advise careful assessment of LLM generated hypotheses due to signs of skillful plagiarism.These examples Despite the recent innovations made in LLMbased scientific ideation, several key limitations persist.These include (1) generating hypotheses in a single pass (Si et al., 2024) , which overlooks the iterative nature of the ideation process.In contrast, Pu et al. (2024) find that researchers typically seek to refine their hypotheses into concrete research briefs.(2) Optimization through feedback on coarse-grained criteria like rigorousness, originality, generalizability etc. (Baek et al., 2025), while often critiquing entire ideas rather than specific components.(3) Simplistic retrieval augmentation such as appending keywords or abstracts of previous papers in context (Wang et al., 2024;Si et al., 2024), whereas effective ideation demands a deeper, more holistic understanding of the domain literature.(4) Unstructured and sub-optimal search of the idea space through either refinement of a generated base-idea (exploitation) (Wang et al., 2024;Baek et al., 2025), or through initial search and plan (exploration) without subsequent refinement of promising ideas (Hu et al., 2024).Finally, there is a lack of open-source implementations that would encourage broader adoption.In light of these challenges, we propose IRIS, tackling each of these limitations while enabling human intervention at every stage of the ideation process.Specifically, we make the following contributions:</p>
<p>• HITL Framework: A user-centered design balancing human control with automation instead of entirely delegating the process of ideation to AI</p>
<p>• Monte Carlo Tree Search: A systematic method to iteratively explore the idea space and extend test time compute via alternating phases of exploration and exploitation ( §3.2)</p>
<p>• Fine-grained Review based Refinement: An exhaustive taxonomy (Table 2) with finegrained actionable feedback for improving hypotheses (Figure 2) ( §3.1)</p>
<p>• Query-based Retrieval: Generating targeted queries for retrieving relevant literature, with re-ranking, clustering and summarization to produce comprehensive, technical and cited responses ( §3.1)</p>
<p>• Open Source: Publicly available platform for AI-Assisted scientific ideation Finally, we conduct a user study with researchers from diverse disciplines validating the effectiveness of our designed system ( §4).The integration of (AI) into scientific research has evolved from early concept-linking tools (Swanson, 1986;Sybrandt et al., 2020;Nadkarni et al., 2021) to sophisticated systems that enhance various research stages.In recent years, LLMs have significantly transformed research life-cycles by assisting in literature searches (Zheng et al., 2024;Ajith et al., 2024;Asai et al., 2024), citation recommendations (Pillai and R, 2022;Zhang and Zhu, 2022;Press et al., 2024), review of scientific documents (Zhou et al., 2024), experimental design (Huang et al., 2024;Schmidgall et al., 2025), scientific claim verification (Lu et al., 2024), theorem proving (Song et al., 2025), manuscript writing (Weng et al., 2025), and reading assistants1 .</p>
<p>Human-AI Co-creation Systems</p>
<p>The emergence of Gen AI has introduced a new dimension to Co-creation systems, setting them apart from previous ones where machines primarily served as supportive tools for human users (Davis et al., 2015;Muller et al., 2020;Weisz et al., 2024).Recent studies, such as those by Kantosalo and Jordanous (2021); Liu et al. (2024), demonstrate the effectiveness of Gen AI tools in creative tasks, particularly through their steerability and explain-ability.This has led to growing emphasis among researchers to develop design guidelines for integrating Gen AI into existing frameworks (Amershi et al., 2019;Shneiderman, 2020).We build IRIS for researcher-in-the-loop ideation while incorporating design principles from prior work, such as minimizing opacity, adopting granular feedback, encouraging AI processing delays (Amershi et al., 2019;Liu et al., 2024), and replacing rigid post-hoc analysis with oversight across planning, generation, and retrospection stages (Shneiderman, 2020).Spangler et al. (2014) demonstrate the first proof of principle for automated hypothesis generation through text mining of scientific literature, leveraging techniques such as entity detection and graphbased diffusion of information.Rising capabilities of text completion models has driven significant advancements in this field (Wang et al., 2024;Lu et al., 2024;Li et al., 2024;Hu et al., 2024;Si et al., 2024;Kumar et al., 2024;Baek et al., 2025;Gottweis, 2025).However, current efforts focus on fully automated systems, often overlooking the critical role of human involvement.Acceleron demonstrates one of the first human-in-the-loop (HITL) framework assisting researchers in validation of motivation behind a research problem and synthesizing a method for the same (Nigam et al., 2024), followed by Pu et al. (2024) making an attempt to develop an interactive idea generation system.These approaches remain limited, allowing idea exploration only within a predefined framework, restricting flexibility and adaptability.Furthermore, their system lacks sophisticated components like automated fine-grained feedback, literature retrieval targeted to the research goal and scaling test-time compute.</p>
<p>Automated Hypothesis Generation</p>
<p>IRIS</p>
<p>Broadly, the system expects as input a research goal G consisting of a research problem and it's motivation, and outputs a research brief B consisting of a Title, Proposed Methodology and Experiment Plan, while improving it's quality; either in semi-automatic manner through directions from the researcher or autonomously exploiting Monte Carlo Tree Search (MCTS).We provide detailed overview of our system including the implementation of agents ( §3.1) and MCTS adaptation for hypothesis generation ( §3.2).</p>
<p>Agent Architecture</p>
<p>IRIS employs a three-agent architecture consisting of an ideation agent, a review agent, and a retrieval agent.The ideation agent navigates the search space of possible research ideas, while the review and retrieval agents provide feedback and relevant scientific context respectively.</p>
<p>Ideation Agent generates and iteratively improves the research brief.It can toggle between a semi-automatic mode, to receive guidance from a researcher to refine research briefs through steering reviews, retrievals or employing custom feedback, and a completely autonomous mode to explore and exploit the idea space by leveraging actions which support iterative refinement of the research briefs through MCTS.</p>
<p>Review Agent is accountable for two tasks namely providing reward and feedback.For evaluation of an idea, we have defined a hierarchical taxonomy of aspects grounded in real-world scientific critique (For example, (Ghosal et al., 2022), (Kennard et al., 2022), (Dycke et al., 2023)), detailed in Table 2. Review Agent is auto-triggered after each new generation of the research brief to provide a reward averaged over the scores assigned to distinct aspects, based on the evaluation provided for the complete research brief.</p>
<p>As opposed to the parallel works (Wang et al., 2024;Baek et al., 2025) that focus on coarse-level criteria and provide broad evaluation of the entire generated research brief, usually, a feedback with respect to an aspect is applicable to only specific parts of the research brief.For example, only some component of the brief can be infeasible or some other component requires more clarity.Addressing this need, when explicitly triggered by the researcher, the review agent switches to a finegrained evaluation, delivering targeted, actionable feedback on each aspect of the taxonomy for distinct segments of the current research-brief (Figures 1 and 2 (R) ).This fine-grained feedback is verified by the researcher and omitted if deemed irrelevant.Then the review agent computes reward based on the scores of the verified aspects of the feedback.This adept human intervention coupled with granular feedback, successfully mitigates "reward hacking" behavior of LLMs.</p>
<p>Retrieval Agent: For the input research goal, the retrieval agent synthesizes queries targeted to retrieve literature relevant to the research goal.For answering each query, it adopts Ai2 Scholar QA API2 .The pipeline consists of two-stage retrieval followed by three-stage generation.The Semantic Scholar API's (storing over 200M open access papers) snippet search endpoint (Kinney et al., 2023) extracts relevant passages, which are re-ranked to retain top-k passages and aggregated at the paper level.With the finalized set of passages, the retrieval agent (i) extracts quotes from the passages relevant to the query, (ii) generates a plan to produce an organized report with sections, and clusters the top-k passages accordingly, and (iii) generates cited sections-wise reports along with summaries (Figure 2 (L)).Our motivation for adopting Schol-arQA stems from the limitations of naive RAG failing to appropriately answer global questions targeted at a corpus as opposed to a single document (Edge et al., 2025).We also provide the ability for the researcher to upload papers in the form of PDF documents, which they think to be relevant but have been missed out as the part of the retrieval.The retrieval agent parses the PDF through Grobid based doc2json tool3 and appends the most relevant chunks to the context for the ideation agent to refine the research brief.</p>
<p>Monte Carlo Tree Search Framework</p>
<p>To systematically explore the vast space of potential research ideas, IRIS employs Monte Carlo Tree Search (MCTS) (Kocsis and Szepesvári, 2006).MCTS allows the system to effectively extend testtime compute similar to recent work in augmenting LLM reasoning (Qi et al., 2024;Guan et al., 2025).Unlike applications with objective rewards (e.g., mathematics, code generation), scientific ideation quality is subjective.We adapt MCTS by using the LLM-based Review Agent as a proxy judge to estimate the quality (reward) of generated hypotheses.</p>
<p>Formally, given a research goal G, our system constructs a search tree T rooted with G.A state s encapsulates the current {research brief b, reward estimate r, latest review feedback f (if applicable, else ϕ), and retrieved knowledge k (if applicable, else ϕ)}.Edges represent actions a taken by the Ideation Agent to transition between states.We define a comprehensive action space A = {a 1 : generate, a 2 : refine w/ retrieval, a 3 : refine w/ review, a 4 : refine w/ user feedback}.The MCTS process iteratively builds the tree over N iterations, guided by the Upper Confidence Bound for Trees (UCT) algorithm (Coquelin and Munos, 2007).UCT of a node n is defined by:
UCT(n) = Q(n) N (n) + c ln N (n p ) N (n)(1)
where Q(n) is the total reward at child node n accumulated from its children, N (n) is its visit count, N (n p ) is the visit count of the parent node of n , and c is the exploration constant.Algorithm 1 outlines the MCTS process.Each node n stores its state s n as defined above, Q(n) and N (n).</p>
<p>Algorithm 1 MCTS for Research Idea Generation</p>
<p>Require: Research goal G, iterations N , max depth d max , actions A, constant c 1: Initialize tree T with root n 0 (state
s 0 = G, Q(n 0 ) = 0, N (n 0 ) = 0). 2: for i = 1 to N do 3: n leaf ← SELECT(n 0 , c) 4: r ← EVALUATE(n leaf ) 5: if depth &lt; d max then 6: EXPAND(n leaf , A) 7:
end if 8: BACKPROPAGATE(n leaf , r) 9: end for 10: return BESTCHILD(n 0 ) Each iteration involves four phases: SELECT(n root , c): Traverse the tree from the root n 0 to select a leaf node n leaf .At each node n during traversal, if n has any unvisited children (Q(n) = 0), one such child is randomly selected.If all children of n have been visited, the next node is chosen by: arg max n ′ ∈children(n) (UCT(n ′ )).</p>
<p>EVALUATE(n leaf ): Obtain reward r for the state s leaf of n leaf via the Review Agent.</p>
<p>EXPAND(n leaf , A): If n leaf is non-terminal and below d max , create child nodes n ′ for each applicable action a ∈ A, with Q(n ′ ) = 0, N (n ′ ) = 0. BACKPROPAGATE(n leaf , r): Update Q and N values for n leaf and its ancestors with reward r.</p>
<p>BESTCHILD(n 0 ): After N iterations, select the child of n 0 with the highest average reward Q/N .</p>
<p>Memory: Agents maintain trajectory-level memory.For instance, the Ideation Agent recalls generated briefs, the Retrieval Agent remembers past queries, and the Review Agent tracks prior feedback.This helps steer the generation towards nonredundant refinements.Cost: MCTS can be computationally intensive.IRIS incorporates budget controls, allowing users to set limits.For tighter budgets, the system prioritizes exploitation by lowering the exploration constant c, ensuring delivery of few refined outputs rather than numerous low-quality ones.</p>
<p>Evaluation</p>
<p>To assess the effectiveness and usability of IRIS, we conduct automated evaluations and user studies.</p>
<p>Experiment Setup</p>
<p>System Implementation: IRIS's user interface is developed using HTML, CSS, JavaScript.The core LLM functionalities are powered by Gemini-2.0-Flash(DeepMind, 2024) accessed via LiteLLM4 , which allows users to substitute other LLMs of their choice.We utilize Gemini's built-in safety filters to mitigate harmful or inappropriate queries.</p>
<p>Metrics: We employ LLM-as-a-judge, popularly adopted in parallel literature (Baek et al., 2025;Gottweis, 2025).We use two methods guided by our pre-defined criteria (Table 2).absolute score: each generated hypothesis (1-10), and relative score: aggregating head-to-head comparisons and preferences to compute ELO ratings.</p>
<p>To contextualize the alignment of LLM-as-ajudge with human preferences in the context of scientific ideation, we prompt baselines Gemini-2.0-Flash,ChatGPT, ChatGPT w/ search and Claude 3.5 Haiku to generate novel research briefs.Then ask users and LLMs to rate the generations in the order of their preference.</p>
<p>User Study</p>
<p>We conducted a user study with 8 researchers (N=8) from diverse fields (AI/NLP, Chem, Physics, HCI) and experience levels.Two users voluntarily participated twice (10 total case studies).Each ∼60 min session involved: 1) Defining a research goal, 2) Blindly ranking initial set of hypotheses, 3) Interacting with IRIS, 4) Completing a post-task survey.</p>
<p>Results and Analysis</p>
<p>Metric Validation: Human baseline rankings correlated moderately with LLM based ELO scores (Pearson's r=0.60) but weakly with LLM based absolute scores (r=0.45).With this learning we plan to replace the LLM-as-the-judge scores, displayed to showcase the quality of the idea, with the ELO ratings.</p>
<p>Automated Evaluation: LLM-as-a-judge evaluations (Figure 3) showed that user interaction within IRIS consistently improved hypothesis quality, increasing average absolute scores by 0.5 points and ELO ratings by 12 points for a tree depth of 3.  User Study Feedback: Quantitative ratings (Table 1) show users found the fine-grained feedback highly insightful and unpromptedly mentioned better usability and control over other reading assistant interfaces mentioned in §2.</p>
<p>Feature / Aspect</p>
<p>Mean Rating (± Std Dev) Additionally, through qualitative feedback we arrived at the following insights:</p>
<p>• Steerability: All users valued the MCTS tree for control and transparency over ideation.</p>
<p>• Feedback: Critiques often reflected user's own concerns (87.5% users) and sometimes sparked novel insights (50% cases).</p>
<p>• Retrieval: Found to be facilitating grounding of ideas, but quality varied with domains such as chemistry and physics research, matching the lower rating (3.7/5).We attribute this to reduced availability of relevant literature in the semantic scholar corpus.</p>
<p>• Relevance: hypotheses often shared similarities with or extended users' ongoing work (62.5% users).</p>
<p>Overall Improvement: Post-interaction, 25% (2/8) found the hypothesis substantially better, 50% (4/8) marginally better, and 25% (2/8) similar quality.Crucially, all users reported enhanced understanding of the proposed methodology, and considered it to be promising.</p>
<p>Conclusion</p>
<p>We introduce IRIS, an Interactive Research Ideation System, to augment automated scientific hypothesis generation with human expertise.We apply MCTS to iteratively explore the idea space, refine ideas with fine-grained segment level reviews and targeted query based multi-document retrieval; offering a steerable environment for researchers during LLM-driven scientific ideation.Our user study validates the usability and effectiveness of our system, demonstrating consistent improvement in hypothesis quality increasing average absolute scores by 0.5 points and ELO ratings by 12 points for a tree depth of 3. Crucially, users frequently considered the generated hypotheses plausible and worthy of further investigation.We position that the potential of LLMs, particularly within human-AI collaborative frameworks, for developing novel scientific hypothesis remains a heavily underexplored avenue.We present IRIS as a concrete step towards realizing this untapped potential.</p>
<p>Limitations</p>
<p>Currently the system relies on the researcher as the judge to verify the quality of the emerging idea at each iteration, augmented by LLM-as-thejudge.This reliance is based on the assumption of sufficient domain expertise of the researcher.As opposed to this in future we aim for a true Human AI Co-creation System, where more foundational LLMs with scientific expertise, questions researchers for the choices he or she has made leading to a two way socratic review and refinement communication, simulating a more realistic scenario of brain-storming between colleagues or a mentor and a mentee.</p>
<p>Due to budget constraints, we have not explored frontier LLMs such as Claude 3.7 Sonnet, Grok-3 or reasoning models like Gemini-2.5-Pro,o1 etc.The quality of produced hypothesis in terms of novelty and effectiveness would likely benefit from stronger base models.</p>
<p>A Review Taxonomy</p>
<p>Aspect Sub-aspect Definition Originality Lack of Novelty The idea does not introduce a significant or meaningful advancement over existing work, lacking originality or innovation.</p>
<p>Assumptions</p>
<p>The idea relies on untested or unrealistic assumptions that may weaken its validity or applicability.</p>
<p>Clarity</p>
<p>Vagueness</p>
<p>The idea is presented in an unclear or ambiguous manner, making it difficult to understand its core components or contributions.</p>
<p>Contradictory Statements</p>
<p>The idea contains internal inconsistencies or conflicts in its assumptions, methods, or conclusions.</p>
<p>Alignment</p>
<p>The idea is not aligned with the problem statement and its objectives.</p>
<p>Feasibility</p>
<p>Feasibility and Practicality</p>
<p>The idea is not practical or achievable given current technological, theoretical, or resource constraints.</p>
<p>Justification for Methods</p>
<p>The idea does not provide sufficient reasoning or evidence to explain why specific methods, techniques, or approaches were chosen.</p>
<p>Effectiveness</p>
<p>Evaluation and Validation Issues The idea lacks rigorous evaluation methods, such as insufficient benchmarks, inadequate baselines, or poorly defined success metrics.Reproducibility and Robustness The idea does not provide sufficient detail or transparency to allow others to replicate or verify its findings, and is not resilient to variations in input data, assumptions, or environmental conditions.The degree to which the solution consistently produces accurate and dependable results is low, making it less reliable.</p>
<p>Impact</p>
<p>Overgeneralization and Overstatement The idea extends its conclusions or applicability beyond the scope of the context provided or exaggerates its claims, significance, or potential impact beyond what is supported by evidence or reasoning.</p>
<p>Impact</p>
<p>The idea is not impactful or significant.It does not solve a real problem.It does not create value by solving a significant problem or fulfilling a need for individuals, organizations, or society.Ethical and Social Considerations</p>
<p>The idea does not adhere to ethical standards and is harmful to individuals, communities, or the environment.</p>
<p>Figure 1 :
1
Figure 1: Human-in-the-loop Idea Generation with Monte-Carlo-Tree-Search. G: Research Goal, B: Research Brief</p>
<p>Figure 2 :
2
Figure 2: IRIS Platform Interface with (L) Retrieval Panel, (C) Chat Overview Panel, (R) Research Brief Panel</p>
<p>Figure 3 :
3
Figure 3: Iterative improvement in hypothesis quality within IRIS over interaction depth (up to depth 3).Interaction enhances both absolute scores and ELO ratings.</p>
<p>Figure 4 :
4
Figure 4: Top: Comparison of hypothesis quality generated by baseline methods (ChatGPT, ChatGPT+Search, Claude 3.5 Haiku, Gemini-2.0-Flash)using LLM-as-a-judge absolute scores and ELO ratings.Bottom: User Survey Feedback Form Questions.</p>
<p>Table 1 :
1
User ratings (1-5 Likert scale) for key IRIS features and overall satisfaction (N=10).
Usefulness of Fine-grained Feedback4.3 ± 0.7MCTS Tree Interface (Steerability)4.2 ± 0.6Quality of Lit. Summaries3.7 ± 0.8Usability and control4.5 ± 0.7Overall Satisfaction (Final Research Brief)3.9 ± 0.7</p>
<p>Table 2 :
2
Hierarchical Review Taxonomy</p>
<p>JenniAI, SciSpace, ScholarAI <br />
https://allenai.org/blog/ai2-scholarqa
https://github.com/allenai/s2orc-doc2json
https://docs.litellm.ai/docs/</p>
<p>Litsearch: A retrieval benchmark for scientific literature search. Anirudh Ajith, Mengzhou Xia, Alexis Chevalier, Tanya Goyal, Danqi Chen, Tianyu Gao, arXiv:2407.189402024Preprint</p>
<p>Guidelines for human-ai interaction. Saleema Amershi, Dan Weld, Mihaela Vorvoreanu, Adam Fourney, Besmira Nushi, Penny Collisson, Jina Suh, Shamsi Iqbal, Paul N Bennett, Kori Inkpen, Jaime Teevan, Ruth Kikin-Gil, Eric Horvitz, 10.1145/3290605.3300233Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI '19. the 2019 CHI Conference on Human Factors in Computing Systems, CHI '19New York, NY, USAAssociation for Computing Machinery2019</p>
<p>Claude 3.7 sonnet system card. Accessed. Anthropic, 2025</p>
<p>Openscholar: Synthesizing scientific literature with retrieval-augmented lms. Akari Asai, Jacqueline He, Rulin Shao, Weijia Shi, Amanpreet Singh, Joseph Chee Chang, Kyle Lo, Luca Soldaini, Sergey Feldman, Mike D , David Wadden, Matt Latzke, Minyang Tian, Pan Ji, Shengyan Liu, Hao Tong, Bohao Wu, Yanyu Xiong, Luke Zettlemoyer, Graham Neubig, Dan Weld, Doug Downey, Wen Tau Yih, Pang Wei Koh, Hannaneh Hajishirzi, arXiv:2411.141992024Preprint</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382025Preprint</p>
<p>Monitoring reasoning models for misbehavior and the risks of promoting obfuscation. Bowen Baker, Joost Huizinga, Leo Gao, Zehao Dou, Melody Y Guan, Aleksander Madry, Wojciech Zaremba, Jakub Pachocki, David Farhi, 2025</p>
<p>Emergent misalignment: Narrow finetuning can produce broadly misaligned llms. Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans, arXiv:2502.174242025Preprint</p>
<p>Accessed: 2025-02-25. Pierre-Arnaud Coquelin and Rémi Munos. Harry Booth, arXiv:cs/0703062Time. 2025. 2007PreprintBandit algorithms for tree search</p>
<p>An Enactive Model of Creativity for Computational Collaboration and Co-creation. Nicholas Davis, Chih-Pin Hsiao, Yanna Popova, Brian Magerko, 10.1007/978-1-4471-6681-8_72015SpringerLondon, London</p>
<p>Google gemini ai update. Google Deepmind, 2024. December 2024</p>
<p>. Accessed, </p>
<p>NLPeer: A unified resource for the computational study of peer review. Nils Dycke, Ilia Kuznetsov, Iryna Gurevych, 10.18653/v1/2023.acl-long.277Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>
<p>From local to global: A graph rag approach to query-focused summarization. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, Jonathan Larson, arXiv:2404.161302025Preprint</p>
<p>Overoptimization of academic publishing metrics: observing goodhart's law in action. Michael Fire, Carlos Guestrin, 10.1093/gigascience/giz0532019GigaScience853</p>
<p>Peer review analyze: A novel benchmark resource for computational analysis of peer reviews. Tirthankar Ghosal, Sandeep Kumar, Prabhat Kumar Bharti, Asif Ekbal, 10.1371/journal.pone.0259238PLOS ONE. 1712022</p>
<p>Towards an ai co-scientist. Juraj Gottweis, 2025</p>
<p>Agentic ai for scientific discovery: A survey of progress, challenges, and future directions. Mourad Gridach, Jay Nanavati, Khaldoun Zine El Abidine, Lenon Mendes, Christina Mack, arXiv:2503.089792025Preprint</p>
<p>rstar-math: Small llms can master math reasoning with self-evolved deep thinking. Xinyu Guan, Li Lyna Zhang, Yifei Liu, Ning Shang, Youran Sun, Yi Zhu, Fan Yang, Mao Yang, arXiv:2501.045192025Preprint</p>
<p>All that glitters is not novel: Plagiarism in ai generated research. Tarun Gupta, Danish Pruthi, arXiv:2502.164872025Preprint</p>
<p>An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan, arXiv:2410.142552024NovaPreprint</p>
<p>Mlagentbench: Evaluating language agents on machine learning experimentation. Qian Huang, Jian Vora, Percy Liang, Jure Leskovec, arXiv:2310.033022024Preprint</p>
<p>Rolebased perceptions of computer participants in humancomputer co-creativity. Anna Kantosalo, Anna Jordanous, 7th Computational Creativity Symposium at AISB 2021. London, UK. AISB2021</p>
<p>DISAPERE: A dataset for discourse structure in peer review discussions. Nayak Neha, Tim O Kennard, Rajarshi 'gorman, Akshay Das, Chhandak Sharma, Matthew Bagchi, Pranay Clinton, Hamed Kumar Yelugam, Andrew Zamani, Mccallum, 10.18653/v1/2022.naacl-main.89Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Understanding users' dissatisfaction with chatgpt responses: Types, resolving tactics, and the effect of knowledge level. Yoonsu Kim, Jueon Lee, Seoyoung Kim, Jaehyuk Park, Juho Kim, 10.1145/3640543.3645148Proceedings of the 29th International Conference on Intelligent User Interfaces, IUI '24. the 29th International Conference on Intelligent User Interfaces, IUI '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>The semantic scholar open data platform. Rodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey Macmillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D Wade, Linda Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, arXiv:2301.101402023Madeleine Van Zuylen, and Daniel S. WeldPreprint</p>
<p>Bandit based monte-carlo planning. Levente Kocsis, Csaba Szepesvári, Machine Learning: ECML 2006. Berlin, Heidelberg; Berlin HeidelbergSpringer2006</p>
<p>Can large language models unlock novel scientific research ideas?. Sandeep Kumar, Tirthankar Ghosal, Vinayak Goyal, Asif Ekbal, arXiv:2409.061852024Preprint</p>
<p>Scientific literature: Information overload. Esther Landhuis, Nature. 5352016</p>
<p>Chain of ideas: Revolutionizing research via novel idea development with llm agents. Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, Tian Feng, Lidong Bing, arXiv:2410.131852024Preprint</p>
<p>How ai processing delays foster creativity: Exploring research question co-creation with an llm-based agent. Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, Yun Huang, 10.1145/3613904.3642698Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24. the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024Preprint</p>
<p>Artificial intelligence can generate fraudulent but authentic-looking scientific medical articles: Pandora's box has been opened. Martin Májovský, Martin Černý, Matěj Kasal, Martin Komarc, David Netuka, 10.2196/46924J Med Internet Res. 25e469242023</p>
<p>Mixed initiative generative ai interfaces: An analytic framework for generative ai applications. Michael Muller, Justin D Weisz, Werner Geyer, Proceedings of the Workshop The Future of Co-Creative Systems-A Workshop on Human-Computer Co-Creativity of the 11th International Conference on Computational Creativity. the Workshop The Future of Co-Creative Systems-A Workshop on Human-Computer Co-Creativity of the 11th International Conference on Computational Creativity2020ICCC 2020</p>
<p>Scientific language models for biomedical knowledge base completion: An empirical study. Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, Tom Hope, arXiv:2106.097002021Preprint</p>
<p>An interactive co-pilot for accelerated research ideation. Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff, 10.18653/v1/2024.hcinlp-1.6Proceedings of the Third Workshop on Bridging Human-Computer Interaction and Natural Language Processing. the Third Workshop on Bridging Human-Computer Interaction and Natural Language ProcessingMexico City, MexicoAssociation for Computational Linguistics2024</p>
<p>The human in the infinite loop: A case study on revealing and explaining human-ai interaction loop failures. Changkun Ou, Daniel Buschek, Sven Mayer, Andreas Butz, 10.1145/3543758.3543761Proceedings of Mensch Und Computer 2022, MuC '22. Mensch Und Computer 2022, MuC '22New York, NY, USA2022Association for Computing Machinery</p>
<p>A survey on citation recommendation system. S Reshma, Pillai, L R Deepthi, 10.1109/ICICICT54557.2022.99178872022 Third International Conference on Intelligent Computing Instrumentation and Control Technologies (ICICICT). 2022</p>
<p>Citeme: Can language models accurately cite scientific claims?. Ori Press, Andreas Hochlehnert, Ameya Prabhu, arXiv:2407.128612024PreprintVishaal Udandarao, Ofir Press, and Matthias Bethge</p>
<p>Ideasynth: Iterative research idea development through evolving and composing idea facets with literature-grounded feedback. Kevin Pu, K J Kevin Feng, Tovi Grossman, Tom Hope, Bhavana Dalvi Mishra, Matt Latzke, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue, arXiv:2410.040252024Preprint</p>
<p>Mutual reasoning makes smaller llms stronger problem-solvers. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, Mao Yang, arXiv:2408.061952024Preprint</p>
<p>Alignment faking in large language models. et.al Ryan Greenblatt2025</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025Preprint</p>
<p>Bridging the gap between ethics and practice: Guidelines for reliable, safe, and trustworthy human-centered ai systems. Ben Shneiderman, 10.1145/3419764ACM Trans. Interact. Intell. Syst. 1042020</p>
<p>Can llms generate novel research ideas? a large-scale human study with 100+ nlp researchers. Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.041092024Preprint</p>
<p>Language agents achieve superhuman synthesis of scientific knowledge. D Michael, Sam Skarlinski, Jon M Cox, James D Laurent, Michaela Braza, Michael J Hinks, Manvitha Hammerling, Ponnapati, G Samuel, Andrew D Rodriques, White, arXiv:2409.137402024Preprint</p>
<p>Lean copilot: Large language models as copilots for theorem proving in lean. Peiyang Song, Kaiyu Yang, Anima Anandkumar, arXiv:2404.125342025Preprint</p>
<p>Automated hypothesis generation based on mining scientific literature. Scott Spangler, Angela D Wilkins, Benjamin J Bachman, Meena Nagarajan, Tajhal Dayaram, Peter Haas, Sam Regenbogen, Curtis R Pickering, Austin Comer, Jeffrey N Myers, Ioana Stanoi, Linda Kato, Ana Lelescu, Jacques J Labrie, Neha Parikh, Andreas Martin Lisewski, Lawrence Donehower, Ying Chen, Olivier Lichtarge, 10.1145/2623330.2623667Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14. the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '14New York, NY, USA2014Association for Computing Machinery</p>
<p>Undiscovered public knowledge. Don R Swanson, The Library Quarterly: Information, Community. 198656</p>
<p>Agatha: Automatic graph mining and transformer based hypothesis generation approach. Justin Sybrandt, Ilya Tyagin, Michael Shtutman, Ilya Safro, 10.1145/3340531.3412684Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM '20. the 29th ACM International Conference on Information &amp; Knowledge Management, CIKM '20New York, NY, USAAssociation for Computing Machinery2020</p>
<p>SciMON: Scientific inspiration machines optimized for novelty. Qingyun Wang, Doug Downey, Heng Ji, Tom Hope, 10.18653/v1/2024.acl-long.18Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. Long Papers. the 62nd Annual Meeting of the Association for Computational LinguisticsBangkok, ThailandAssociation for Computational Linguistics20241</p>
<p>Design principles for generative ai applications. Justin D Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel Miles, Werner Geyer, 10.1145/3613904.3642466Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24. the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24New York, NY, USAAssociation for Computing Machinery2024</p>
<p>Cycleresearcher: Improving automated research via automated review. Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang, arXiv:2411.008162025Preprint</p>
<p>Citation recommendation using semantic representation of cited papers' relations and content. Jinzhu Zhang, Lipeng Zhu, 10.1016/j.eswa.2021.115826Expert Systems with Applications. 1871158262022</p>
<p>Openresearcher: Unleashing ai for accelerated scientific research. Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, Pengfei Liu, arXiv:2408.069412024Preprint</p>
<p>Is LLM a reliable reviewer? a comprehensive evaluation of LLM on automatic paper reviewing tasks. Ruiyang Zhou, Lu Chen, Kai Yu, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCL2024</p>            </div>
        </div>

    </div>
</body>
</html>
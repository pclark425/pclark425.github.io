<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4702 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4702</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4702</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-102.html">extraction-schema-102</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <p><strong>Paper ID:</strong> paper-28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d" target="_blank">Quantifying Memorization Across Neural Language Models</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> On the whole, this work finds that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4702.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4702.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Numeric-sequence completion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Completion of numeric and arithmetic-like sequences</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper repeatedly notes that language models often complete numeric sequences (e.g., number sequences, phone numbers, price lists) by memorization or by exploiting local statistical/pattern regularities rather than executing symbolic arithmetic; such completions are more likely when sequences are duplicated in training or when long context is available.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Various (GPT-Neo family, GPT-2, GPT-J, GPT2-XL, T5, OPT)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal and masked transformer language models evaluated in the paper: GPT-Neo (125M, 1.3B, 2.7B, 6B; trained on The Pile), GPT-2 family (baseline, trained on WebText / GPT2-XL), GPT-J 6B, T5 (masked encoder–decoder models from 77M to 11B trained on C4), and OPT family (125M–66B). Evaluations probe model next-token prediction / fill-in tasks using training-data prefixes and checking exact suffix reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Completion of numeric sequences and other arithmetic-like token sequences (e.g., phone numbers, numeric lists, price lists, repeated numeric patterns) — single-step continuation / sequence completion rather than symbolic multi-step arithmetic</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Pattern memorization and statistical pattern-matching: models reproduce numeric sequences primarily by memorizing training examples or learning frequent local token patterns; not demonstrated to perform symbolic arithmetic or multi-step algorithmic calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Qualitative examples show models reproducing phone numbers, price lists, and simple numeric sequences exactly when those sequences or similar patterns appeared in training (Figures and examples). GPT-2's smaller fraction of completions (≈6%) corresponds to 'uninteresting' easy-to-predict sequences (number sequences, repetitions). Larger GPT-Neo/GPT-J models reproduce many more exact continuations, consistent with memorization scaling with model size, duplication, and context length.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No experiments show models performing algorithmic arithmetic (e.g., addition/subtraction) or generalizing arithmetic rules; masked models (T5) memorize far less even at comparable sizes, suggesting objective/data preprocessing affect numeric completion. Some highly-repeated numeric-like fragments are not memorized when they contain semantically meaningless variation, arguing that mere numeric content isn't sufficient — duplication and exact matches matter.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported as fraction of exact suffix completions (not arithmetic accuracy). Examples include: GPT-2 baseline ≈6% of evaluation examples completed exactly; GPT-Neo 1.3B ≈40% on same Pile-derived prompts; GPT-J / 6B model: ~33% of training sequences extractable at 50-token context and ~65% at 450-token context (overall, across content). For particular comparisons: T5-XL (3B) memorizes ~3.5% of sequences repeated 100 times, whereas GPT-Neo 2.7B memorizes 53.6% for sequences repeated 100 times (with 150 tokens context) — indicating large differences in memorization propensity. Beam search (b=100) increases extracted memorization by under ~2 percentage points on average (max 5.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions included: varying context length (longer context increases chance of exact numeric completion), sampling strategies (duplicate-normalized vs uniform), deduplication of training data (reduces memorization), and decoding strategies (greedy vs beam search — beam search slightly increases exact matches). Also an alternate, more permissive metric checked whether generated [prefix||f(prefix)] appears anywhere in dataset (finds more matches). No direct mechanistic probes (e.g., neuron-level circuits) for arithmetic were run.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Failure modes reported: smaller models often produce locally plausible but semantically incorrect numeric completions (syntactically sound but wrong); models memorize duplicated numeric strings but fail on near-duplicates with minor variations; memorization is 'discoverable' only under favorable conditions (long context), making some memorized numeric strings hidden unless prompted with specific prefixes; deduplication mitigates but does not eliminate numeric memorization especially for very high repetition counts; no evidence models compute arithmetic — instead they rely on memorized or high-probability continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Comparisons show: GPT-Neo/GPT-J (causal) memorize far more numeric-like continuations than GPT-2 baseline on the same prompts (e.g., 40% vs 6% for similarly sized models on Pile-derived prompts). T5 masked models memorize an order-of-magnitude less than comparable causal models. OPT family (trained on modified/deduplicated Pile) shows much smaller memorization fractions even at large scale, suggesting dataset curation and objective affect numeric-pattern memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4702.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4702.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phone/identifier memorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memorization of phone numbers, URLs, identifiers and similar short numeric/alphanumeric tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper cites concrete examples (phone numbers, URLs, code identifiers) where LMs output exact memorized tokens from training; these are a special case of numeric/arithmetic-like content where exact reproduction can leak private data or boilerplate strings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo family, GPT-2, GPT-J, GPT2-XL (examples drawn from models evaluated)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same model families as above; evaluated by prompting with training prefixes and checking exact 50-token suffix match or dataset-wide containment.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Exact reproduction of short numeric/alphanumeric tokens (phone numbers, URLs, identifiers) — single-token or short-token sequence continuation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Memorization of full training sequences (or frequent substrings) rather than algorithmic inference; local pattern statistics and duplication increase likelihood of exact reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Examples and prior extraction work cited (Carlini et al. 2020) show extraction of phone numbers and URLs. In this paper, qualitative examples include phone-number-like and price/number lists being exactly produced by larger models (Figures 8, 13–15). The paper also notes that GPT-2's memorized examples are largely number sequences and repetitive tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Not all repeated numeric-like strings are memorized — e.g., some highly duplicated strings with minor variations are not memorized. Deduplication of training data reduces these exact reproductions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No arithmetic-specific accuracy reported; overall extractability metrics cover these examples implicitly (see numeric-sequence completion entry). Prior work (cited) extracted a small number of such items from GPT-2; this paper finds much larger fractions for larger models trained on The Pile.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Interventions: dataset deduplication drastically reduces such memorization for low-to-moderate repetition counts; checking generation containment anywhere in dataset (rather than only the ground-truth suffix) increases the detected amount of memorized tokens (e.g., for examples with 100 repetitions, 32.6% contained somewhere vs 15.8% exact ground-truth match).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Memorization poses privacy risks because exact short numeric tokens (phone numbers, URLs) can be leaked; discoverability requires specific prompts (so not always trivially exploitable), but adversarial discovery remains a threat; deduplication reduces but does not completely eliminate risk.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>GPT-Neo / GPT-J show higher propensity to reproduce such tokens than GPT-2; T5 and OPT families (esp. trained on deduplicated data or different objectives) show markedly lower reproduction rates.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4702.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4702.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, representations, performance, and failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Discoverability effect on numeric completion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Context-dependent discoverability of memorized numeric continuations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents that the ability to elicit exact numeric or other memorized continuations rises strongly with available context length: some numeric memorization is only discoverable when the prompt is long and specific.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-Neo family (mainly 6B), comparisons to GPT-2 and GPT-J</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Causal transformer LMs evaluated by providing varying-length prefixes (context) and checking for exact 50-token suffix reproduction.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Context-conditioned continuation of numeric/arithmetic-like sequences (single-step continuation dependent on long prefix).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_hypothesis</strong></td>
                            <td>Longer context increases conditional probability mass on memorized training continuations, revealing memorized numeric sequences that are otherwise not the most likely outputs with shorter context; this suggests memorization is stored as conditional associations rather than globally active 'always-on' patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical trend: for the 6B model, fraction extractable rose from ~33% at 50-token context to ~65% at 450-token context (Figure 1c); similar log-linear relationships for other sizes. The paper coins this the 'discoverability phenomenon'.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct counterevidence in paper; but the fact that some duplicates are still not extractable even with large context suggests other factors (token variability, noise) can prevent discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Example numbers: 6B model extractability ~33% @ 50 tokens, ~65% @ 450 tokens (aggregated across content); in random-sample experiments for GPT-J 6B, extracting last 50 tokens of a 1000-token sequence: 7% probability for 6B model vs 4% for 125M model; baseline GPT2-XL ~2%. These metrics are across dataset content, not arithmetic-only items, but include numeric continuations.</td>
                        </tr>
                        <tr>
                            <td><strong>probing_or_intervention_results</strong></td>
                            <td>Controlled experiments varying prefix length; also considered sampling strategies and deduplication. No neuron-level probing performed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_and_failure_modes</strong></td>
                            <td>Makes auditing and detection of memorized numeric data harder because many memorized items only appear with long, specific context — audits must therefore probe with training-text prefixes to find them. For defenders, restricting prompt length may reduce practical extraction risk but doesn't change the underlying memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>All causal models show the discoverability trend, but magnitude varies: larger causal models show stronger increase with context; GPT-2 baseline shows much lower extractability overall and thus smaller absolute discoverability effect; T5 masked models show overall far less memorization even when allowed more context (given the different objective).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Quantifying Memorization Across Neural Language Models', 'publication_date_yy_mm': '2022-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Extracting training data from large language models <em>(Rating: 2)</em></li>
                <li>Deduplicating training data makes language models better <em>(Rating: 2)</em></li>
                <li>Deduplicating training data mitigates privacy risks in language models <em>(Rating: 2)</em></li>
                <li>How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4702",
    "paper_id": "paper-28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d",
    "extraction_schema_id": "extraction-schema-102",
    "extracted_data": [
        {
            "name_short": "Numeric-sequence completion",
            "name_full": "Completion of numeric and arithmetic-like sequences",
            "brief_description": "The paper repeatedly notes that language models often complete numeric sequences (e.g., number sequences, phone numbers, price lists) by memorization or by exploiting local statistical/pattern regularities rather than executing symbolic arithmetic; such completions are more likely when sequences are duplicated in training or when long context is available.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Various (GPT-Neo family, GPT-2, GPT-J, GPT2-XL, T5, OPT)",
            "model_description": "Causal and masked transformer language models evaluated in the paper: GPT-Neo (125M, 1.3B, 2.7B, 6B; trained on The Pile), GPT-2 family (baseline, trained on WebText / GPT2-XL), GPT-J 6B, T5 (masked encoder–decoder models from 77M to 11B trained on C4), and OPT family (125M–66B). Evaluations probe model next-token prediction / fill-in tasks using training-data prefixes and checking exact suffix reproduction.",
            "arithmetic_task_type": "Completion of numeric sequences and other arithmetic-like token sequences (e.g., phone numbers, numeric lists, price lists, repeated numeric patterns) — single-step continuation / sequence completion rather than symbolic multi-step arithmetic",
            "mechanism_hypothesis": "Pattern memorization and statistical pattern-matching: models reproduce numeric sequences primarily by memorizing training examples or learning frequent local token patterns; not demonstrated to perform symbolic arithmetic or multi-step algorithmic calculation.",
            "evidence_for_mechanism": "Qualitative examples show models reproducing phone numbers, price lists, and simple numeric sequences exactly when those sequences or similar patterns appeared in training (Figures and examples). GPT-2's smaller fraction of completions (≈6%) corresponds to 'uninteresting' easy-to-predict sequences (number sequences, repetitions). Larger GPT-Neo/GPT-J models reproduce many more exact continuations, consistent with memorization scaling with model size, duplication, and context length.",
            "evidence_against_mechanism": "No experiments show models performing algorithmic arithmetic (e.g., addition/subtraction) or generalizing arithmetic rules; masked models (T5) memorize far less even at comparable sizes, suggesting objective/data preprocessing affect numeric completion. Some highly-repeated numeric-like fragments are not memorized when they contain semantically meaningless variation, arguing that mere numeric content isn't sufficient — duplication and exact matches matter.",
            "performance_metrics": "Reported as fraction of exact suffix completions (not arithmetic accuracy). Examples include: GPT-2 baseline ≈6% of evaluation examples completed exactly; GPT-Neo 1.3B ≈40% on same Pile-derived prompts; GPT-J / 6B model: ~33% of training sequences extractable at 50-token context and ~65% at 450-token context (overall, across content). For particular comparisons: T5-XL (3B) memorizes ~3.5% of sequences repeated 100 times, whereas GPT-Neo 2.7B memorizes 53.6% for sequences repeated 100 times (with 150 tokens context) — indicating large differences in memorization propensity. Beam search (b=100) increases extracted memorization by under ~2 percentage points on average (max 5.6%).",
            "probing_or_intervention_results": "Interventions included: varying context length (longer context increases chance of exact numeric completion), sampling strategies (duplicate-normalized vs uniform), deduplication of training data (reduces memorization), and decoding strategies (greedy vs beam search — beam search slightly increases exact matches). Also an alternate, more permissive metric checked whether generated [prefix||f(prefix)] appears anywhere in dataset (finds more matches). No direct mechanistic probes (e.g., neuron-level circuits) for arithmetic were run.",
            "limitations_and_failure_modes": "Failure modes reported: smaller models often produce locally plausible but semantically incorrect numeric completions (syntactically sound but wrong); models memorize duplicated numeric strings but fail on near-duplicates with minor variations; memorization is 'discoverable' only under favorable conditions (long context), making some memorized numeric strings hidden unless prompted with specific prefixes; deduplication mitigates but does not eliminate numeric memorization especially for very high repetition counts; no evidence models compute arithmetic — instead they rely on memorized or high-probability continuations.",
            "comparison_to_other_models": "Comparisons show: GPT-Neo/GPT-J (causal) memorize far more numeric-like continuations than GPT-2 baseline on the same prompts (e.g., 40% vs 6% for similarly sized models on Pile-derived prompts). T5 masked models memorize an order-of-magnitude less than comparable causal models. OPT family (trained on modified/deduplicated Pile) shows much smaller memorization fractions even at large scale, suggesting dataset curation and objective affect numeric-pattern memorization.",
            "uuid": "e4702.0",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Phone/identifier memorization",
            "name_full": "Memorization of phone numbers, URLs, identifiers and similar short numeric/alphanumeric tokens",
            "brief_description": "The paper cites concrete examples (phone numbers, URLs, code identifiers) where LMs output exact memorized tokens from training; these are a special case of numeric/arithmetic-like content where exact reproduction can leak private data or boilerplate strings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo family, GPT-2, GPT-J, GPT2-XL (examples drawn from models evaluated)",
            "model_description": "Same model families as above; evaluated by prompting with training prefixes and checking exact 50-token suffix match or dataset-wide containment.",
            "arithmetic_task_type": "Exact reproduction of short numeric/alphanumeric tokens (phone numbers, URLs, identifiers) — single-token or short-token sequence continuation.",
            "mechanism_hypothesis": "Memorization of full training sequences (or frequent substrings) rather than algorithmic inference; local pattern statistics and duplication increase likelihood of exact reproduction.",
            "evidence_for_mechanism": "Examples and prior extraction work cited (Carlini et al. 2020) show extraction of phone numbers and URLs. In this paper, qualitative examples include phone-number-like and price/number lists being exactly produced by larger models (Figures 8, 13–15). The paper also notes that GPT-2's memorized examples are largely number sequences and repetitive tokens.",
            "evidence_against_mechanism": "Not all repeated numeric-like strings are memorized — e.g., some highly duplicated strings with minor variations are not memorized. Deduplication of training data reduces these exact reproductions.",
            "performance_metrics": "No arithmetic-specific accuracy reported; overall extractability metrics cover these examples implicitly (see numeric-sequence completion entry). Prior work (cited) extracted a small number of such items from GPT-2; this paper finds much larger fractions for larger models trained on The Pile.",
            "probing_or_intervention_results": "Interventions: dataset deduplication drastically reduces such memorization for low-to-moderate repetition counts; checking generation containment anywhere in dataset (rather than only the ground-truth suffix) increases the detected amount of memorized tokens (e.g., for examples with 100 repetitions, 32.6% contained somewhere vs 15.8% exact ground-truth match).",
            "limitations_and_failure_modes": "Memorization poses privacy risks because exact short numeric tokens (phone numbers, URLs) can be leaked; discoverability requires specific prompts (so not always trivially exploitable), but adversarial discovery remains a threat; deduplication reduces but does not completely eliminate risk.",
            "comparison_to_other_models": "GPT-Neo / GPT-J show higher propensity to reproduce such tokens than GPT-2; T5 and OPT families (esp. trained on deduplicated data or different objectives) show markedly lower reproduction rates.",
            "uuid": "e4702.1",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        },
        {
            "name_short": "Discoverability effect on numeric completion",
            "name_full": "Context-dependent discoverability of memorized numeric continuations",
            "brief_description": "The paper documents that the ability to elicit exact numeric or other memorized continuations rises strongly with available context length: some numeric memorization is only discoverable when the prompt is long and specific.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-Neo family (mainly 6B), comparisons to GPT-2 and GPT-J",
            "model_description": "Causal transformer LMs evaluated by providing varying-length prefixes (context) and checking for exact 50-token suffix reproduction.",
            "arithmetic_task_type": "Context-conditioned continuation of numeric/arithmetic-like sequences (single-step continuation dependent on long prefix).",
            "mechanism_hypothesis": "Longer context increases conditional probability mass on memorized training continuations, revealing memorized numeric sequences that are otherwise not the most likely outputs with shorter context; this suggests memorization is stored as conditional associations rather than globally active 'always-on' patterns.",
            "evidence_for_mechanism": "Empirical trend: for the 6B model, fraction extractable rose from ~33% at 50-token context to ~65% at 450-token context (Figure 1c); similar log-linear relationships for other sizes. The paper coins this the 'discoverability phenomenon'.",
            "evidence_against_mechanism": "No direct counterevidence in paper; but the fact that some duplicates are still not extractable even with large context suggests other factors (token variability, noise) can prevent discovery.",
            "performance_metrics": "Example numbers: 6B model extractability ~33% @ 50 tokens, ~65% @ 450 tokens (aggregated across content); in random-sample experiments for GPT-J 6B, extracting last 50 tokens of a 1000-token sequence: 7% probability for 6B model vs 4% for 125M model; baseline GPT2-XL ~2%. These metrics are across dataset content, not arithmetic-only items, but include numeric continuations.",
            "probing_or_intervention_results": "Controlled experiments varying prefix length; also considered sampling strategies and deduplication. No neuron-level probing performed.",
            "limitations_and_failure_modes": "Makes auditing and detection of memorized numeric data harder because many memorized items only appear with long, specific context — audits must therefore probe with training-text prefixes to find them. For defenders, restricting prompt length may reduce practical extraction risk but doesn't change the underlying memorization.",
            "comparison_to_other_models": "All causal models show the discoverability trend, but magnitude varies: larger causal models show stronger increase with context; GPT-2 baseline shows much lower extractability overall and thus smaller absolute discoverability effect; T5 masked models show overall far less memorization even when allowed more context (given the different objective).",
            "uuid": "e4702.2",
            "source_info": {
                "paper_title": "Quantifying Memorization Across Neural Language Models",
                "publication_date_yy_mm": "2022-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Extracting training data from large language models",
            "rating": 2
        },
        {
            "paper_title": "Deduplicating training data makes language models better",
            "rating": 2
        },
        {
            "paper_title": "Deduplicating training data mitigates privacy risks in language models",
            "rating": 2
        },
        {
            "paper_title": "How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN",
            "rating": 1
        }
    ],
    "cost": 0.01526825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>QUANTIFYING MEMORIZATION ACROSS NEURAL LANGUAGE MODELS</h1>
<p>Nicholas Carlini ${ }^{1}$ Daphne Ippolito ${ }^{1,2}$ Matthew Jagielski ${ }^{1}$<br>Katherine Lee ${ }^{1,3}$ Florian Tramèr ${ }^{1}$ Chiyuan Zhang ${ }^{1}$<br>${ }^{1}$ Google Research<br>${ }^{2}$ University of Pennsylvania<br>${ }^{3}$ Cornell University</p>
<h2>ABSTRACT</h2>
<h4>Abstract</h4>
<p>Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LM emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LM is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.</p>
<h2>1 INTRODUCTION</h2>
<p>The performance of neural language models has continuously improved as these models have grown from millions to trillions of parameters (Fedus et al., 2021), with their training sets similarly growing from millions to trillions of tokens. In anticipation of future, even larger models trained on minimally curated datasets, it is important to quantify factors that lead to increased memorization of a model's training set. Indeed, recent work has shown that training data extraction attacks are a practical threat for current language models (Carlini et al., 2020); an adversary interacting with a pretrained model can extract individual sequences that were used to train the model.</p>
<p>While current attacks are effective, they only represent a lower bound on how much memorization occurs in existing models. For example, by querying the GPT-2 language model, Carlini et al. (2020) (manually) identified just 600 memorized training examples out of a 40GB training dataset. This attack establishes a (loose) lower bound that at least $0.00000015 \%$ of the dataset is memorized. In contrast, we are able to show that the 6 billion parameter GPT-J model (Black et al., 2021; Wang and Komatsuzaki, 2021) memorizes at least $1 \%$ of its training dataset: The Pile (Gao et al., 2020).</p>
<p>In addition to prior work's loose estimates of models' memorization capabilities, there is a limited understanding of how memorization varies across different neural language models and datasets of different scales. Prior studies of memorization in language models either focus on models or datasets of a fixed size (Carlini et al., 2019; Zhang et al., 2021; Thakkar et al., 2020) or identify a narrow memorization-versus-scale relationship (Carlini et al., 2020; Lee et al., 2021). While McCoy et al. (2021) broadly study the extent to which language models memorize, their focus is on how to avoid the problem and ensure novelty of model outputs, rather than on studying model risk through identifying the maximal amount of data memorization.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>This paper addresses both of the above open questions by comprehensively quantifying memorization across three families of neural language models and their associated datasets. We leverage access to each model's original training set to provide order-of-magnitude more precise bounds on the amount of extractable data that an adversary could recover than in prior works.</p>
<p>We first construct a set of prompts from the model's training set. By feeding prefixes of these prompts into the trained model, we check whether the model has the ability to complete the rest of the example verbatim. This allows us to measure memorization across models, datasets, and prompts of varying sizes. We identify three properties that significantly impact memorization:</p>
<ol>
<li>Model scale: Within a model family, larger models memorize $2-5 \times$ more than smaller models.</li>
<li>Data duplication: Examples repeated more often are more likely to be extractable.</li>
<li>Context: It is orders of magnitude easier to extract sequences when given a longer context.</li>
</ol>
<p>Our analysis suggests that future research on neural language modeling will need to take steps to prevent future (larger) models from memorizing their training datasets.</p>
<h1>2 Related Work</h1>
<p>There is extensive prior work that qualitatively studies memorization in neural language models. Prior work has demonstrated extraction attacks that recover memorized data including URLs, phone numbers, and other personal information (Carlini et al., 2020; Ziegler, 2021)—or synthetically injected "canaries" (Carlini et al., 2019; Henderson et al., 2018; Thakkar et al., 2020; Thomas et al., 2020). However most of these works are qualitative and aim to demonstrate the existence of extractable data, rather than precisely quantifying how much models memorize. For example, the unprompted memorization evaluation of Carlini et al. (2020) found just 600 examples of memorization in GPT-2. Our paper aims to establish tighter bounds on the fraction of a dataset that is memorized.</p>
<p>Our analysis is relevant to the broad literature on privacy attacks on machine learning. For example, membership inference attacks (Shokri et al., 2017; Yeom et al., 2018) let an adversary detect the presence of a given example in a model's training set; other forms of data leakage let an adversary learn dataset properties (Ganju et al., 2018; Fredrikson et al., 2015). We focus on extraction attacks due to their relevance for language modeling-extraction implies significant leakage from a model, and grows with data duplication (Lee et al., 2021), a common feature of large-scale text datasets.</p>
<p>Various definitions of memorization in deep neural networks have been studied in prior work (Carlini et al., 2019; 2020; Feldman and Zhang, 2020; Zhang et al., 2021). A detailed comparison with those existing formulations is presented in Section 3.1. One leading general memorization definition is differential privacy (Dwork et al., 2006), which formalizes the idea that removing any one example from the training set should not change the trained model. However, while differential privacy protects a single user's private information, it is ineffective for preventing memorization of highly duplicated data, and does not capture the complexity of social, linguistic data (Brown et al., 2022). Also, differentially private learning algorithms (Abadi et al., 2016) generally suffer from expensive computation, slow convergence, and poor model utility, despite recent advances (Anil et al., 2021).</p>
<p>In concurrent work, Kandpal et al. (2022) study how often models emit memorized data as a function of data duplication. Their analysis focuses on evaluating why training data extraction attacks succeed. In contrast, we explicitly prompt models with training data prefixes in order to measure memorization in the worst case, something that a practical attack cannot necessarily do.</p>
<p>Prior scaling hypotheses. Our motivation to study scaling phenomena stems from anecdotal evidence in prior work that memorization ability relates to various aspects of scale. In particular, our analysis on model scale is informed by preliminary experiments in (Zhang et al., 2017; Carlini et al., 2020), our data duplication experiments follow in the line of Lee et al. (2021), and our context length experiments build on hypotheses by Carlini et al. (2020); Ziegler (2021).</p>
<h2>3 Methodology</h2>
<h3>3.1 Definition of Memorization</h3>
<p>To begin, we first select a precise definition for memorization:</p>
<p>Definition 3.1. A string $s$ is extractable with $k$ tokens of context from a model $f$ if there exists a (length- $k$ ) string $p$, such that the concatenation $[p | s]$ is contained in the training data for $f$, and $f$ produces $s$ when prompted with $p$ using greedy decoding.</p>
<p>For example, if a model’s training dataset contains the sequence "My phone number is 555-6789", and given the length $k=4$ prefix "My phone number is", the most likely output is "555-6789", then this sequence is extractable (with 4 words of context). We focus on greedy sampling in this paper, and verify in Section 4.1 that our choice of decoding strategy does not significantly impact our results.</p>
<p>While prior work proposed other definitions, we prefer ours in this paper as it is more actionable. Some memorization definitions, including lower-bounds on differential privacy (Dwork et al., 2006; Jagielski et al., 2020; Nasr et al., 2021) or counterfactual memorization (Feldman and Zhang, 2020; Zhang et al., 2021), require training hundreds or thousands of models, which is impractical for large language models. Alternatively, computing exposure (Carlini et al., 2019) requires thousands of generations per sequence, and is only designed for carefully crafted training examples.Finally, $k$-eidetic memorization (Carlini et al., 2020), is a useful definition for unprompted memorization, but less useful for tightly bounding memorization by prompting with training data (as we will do). Future work might explore how our three scaling observations apply to other definitions of memorization.</p>
<h1>3.2 SELECTION OF EVALUATION DATA</h1>
<p>Having chosen a definition, we next describe our evaluation procedure. Ideally, we would consider every sequence $x=[p | s]$ in the model's training dataset (where $x$ has been split into a length- $k$ prefix $p$ and a suffix $s$ ). For each sequence, we would report if the model exactly reproduces $s$ when prompted with $p$, following Definition 3.1. Unfortunately, performing this test on every sequence in the training data would be prohibitively expensive. For example, the largest 6 billion parameter GPT-Neo model has a throughput of roughly one 100-token generation per second on a V100 GPU. Extrapolating to the 800GB training dataset, this would require over 30 GPU-years of compute.</p>
<p>Instead, we query on a smaller subset of the training data, that still produces statistically confident estimates. In this paper we randomly choose subsets of roughly 50,000 sequences, allowing us to efficiently run inference in just a few hours. The primary criteria when choosing a subset of the training data is to obtain a representative sample that allows us to draw meaningful conclusions from the data. We consider two approaches to constructing a subset of the data.</p>
<p>Our first subset is a uniformly random sample of 50,000 sequences, drawn from the training dataset without repetition. While a uniform sample is useful to estimate the absolute amount of memorization in a model, it is poorly suited for studying how memorization scales with data properties that are not uniformly represented in the training set. For example, prior work has identified that data duplication (i.e., how often the same sequence is repeated either exactly or approximately) is an important factor for memorization. Yet, because the frequency of training data duplication decays extremely quickly (Lee et al., 2021), a uniformly random sample of 50,000 sequences (accounting for $\leq 0.02 \%$ of the dataset) is unlikely to contain any signal that would allow us to accurately measure the tail of this repeated data distribution. A similar concern arises for measuring how memorization scales with prompt length, since very long sentences account for only a small fraction of the training set.</p>
<p>Therefore, our second subset is a random sample normalized by both sequence lengths and duplication counts, which allows us to accurately measure memorization of large language models in the worst-case, on highly duplicated data with long prompts. For each sequence length $\ell \in{50,100,150, \ldots, 500}$, and integer $n$, we select 1,000 sequences of length $\ell$ that are contained in the training dataset between $2^{n / 4}$ and $2^{(n+1) / 4}$ times. We do this until we reach an $n$ for which 1,000 sequences are not available. This gives us 1,000 sequences that repeat between 6 and 8 times $\left(\approx 2^{11 / 4}\right.$ and $\left.\approx 2^{12 / 4}\right)$ and also 1,000 sequences that repeat between 724 and 861 times $\left(\approx 2^{38 / 4}\right.$ and $\left.\approx 2^{39 / 4}\right)$. This biased sampling allows us to more accurately measure memorization as a function of a sample's duplication factor and prompt length, without querying the entire dataset. Note that constructing this duplicate-normalized data subset requires some work, as efficiently identifying duplicate substrings in an 800GB training dataset is computationally challenging. We make use of the suffix array construction from Lee et al. (2021) (see Appendix).</p>
<p>For each length from 50 to 500 tokens, we collect 50,000 examples duplicated varying numbers of times, totaling roughly 500,000 sequences. For each sequence of length $\ell$, we prompt the model with</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: We prompt various sizes of GPT-Neo models (green) with data sampled from their training set—The Pile, and normalized by sequence lengths and duplication counts. As a baseline (yellow), we also prompt the GPT-2 family of models with the same Pile-derived prompts, even though these models were trained on WebText, a different training dataset. (a) Larger models memorize a larger fraction of their training dataset, following a log-linear relationship. This is not just a result of better generalization, as shown by the lack of growth for the GPT-2 baseline models. (b) Examples that are repeated more often in the training set are more likely to be extractable, again following a log-linear trend (baseline is GPT-2 XL). (c) As the number of tokens of context available increases, so does our ability to extract memorized text (baseline is GPT-2 XL).
the first $\ell-50$ tokens and report the sequence as "extractable" if the model exactly emits the next 50 token suffix of this sequence. Fifty tokens corresponds to an average of 127 characters or 25 wordsin the GPT-Neo training set, well over the length of a typical English sentence. Finally, we compute the average probability that a sequence is extractable by averaging over all lengths $\ell$.</p>
<h1>4 EXPERIMENTS</h1>
<p>We primarily study the GPT-Neo model family (Black et al., 2021; Wang and Komatsuzaki, 2021) trained on the Pile dataset (Gao et al., 2020). The GPT-Neo models are causal language models trained with the objective of predicting the next token in a sequence given the previous ones. They come in four sizes: 125 million, 1.3 billion, 2.7 billion and 6 billion parameters. ${ }^{1}$ The Pile is a dataset of 825 GB of text collected from various sources (e.g., books, Web scrapes, open source code). Prior to the recent release of OPT (Zhang et al., 2022), the GPT-Neo models were the largest language models available for public download, and The Pile is the largest public text dataset available.</p>
<h3>4.1 Bigger Models Memorize More</h3>
<p>We begin by considering the impact of model size on memorization, expanding on prior studies which qualitatively established a relationship between the size of GPT-2 models and their ability to memorize $&lt;30$ URLs (Carlini et al., 2020). In contrast, we study a million model generations in order to describe how model scale relates to memorization.</p>
<p>Results. We first study our biased random data sample normalized by duplication count and sequence lengths. The results of this experiment are given in Figure 1a. The y-axis reports the fraction of generations which exactly reproduce the true suffix for their prompt, averaged over all prompt and sequence lengths in our evaluation set. Because our biased sampling over-represents duplicated strings, the absolute degree of memorization in Figure 1a is not particularly important here-rather, we are interested in how memorization varies with scale. ${ }^{2}$ We find that larger models memorize significantly more than smaller models do, with a near-perfect log-linear fit ( $R^{2}$ of $99.8 \%$ ): a ten fold increase in model size corresponds to an increase in memorization of 19 percentage points.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>To confirm that larger models are indeed memorizing more data, and not simply generalizing better, we repeat the analysis with the GPT-2 model family as a baseline. The GPT-2 models are similarly sized, and also trained on Internet-scraped data. If our "larger models memorize more" result was due to the predictive strength of larger models, and not the memorization of specific training data, we would expect a similar relationship between comparably sized GPT-2 models trained on similar data. Put differently, this baseline allows to establish what fraction of the training data is sufficiently "easy" that any language model can correctly predict the 50 -token suffix, even if the example has not been seen during training. For example, a language model trained on multiple examples of number sequences can likely correctly complete some other unseen number sequences. We find that GPT-2 correctly completes approximately $6 \%$ of the examples in our evaluation set, compared to $40 \%$ for the similarly sized 1.3B parameter GPT-Neo model. A qualitative analysis (see examples in Appendix Figure 15) suggests that examples "memorized" by GPT-2 are largely uninteresting sequences (e.g., number sequences, repetitions of the same few tokens, or common phrases). Therefore, we conclude that larger models have a higher fraction of extractable training data because they have actually memorized the data; it is not simply that the larger models are more accurate.</p>
<h1>4.2 Repeated Strings are Memorized More</h1>
<p>Prior work provides preliminary evidence that memorization in language models increases with the number of times sequences are repeated in the training set (Carlini et al., 2020; Lee et al., 2021). We expand on this observation and quantitatively measure the effect of data duplication on memorization. Using our duplication-normalized data sample, we measure the fraction of sequences which are extractable, for buckets of sequences duplicated between 2 and 900 times. Each bucket consists of 1,000 distinct sentences, and we compute the average amount of memorization for each bucket.
Results. Figure 1b shows our results, aggregated over all sequence lengths. We observe a clear log-linear trend in memorization. While models rarely regurgitate strings that are repeated only a few times, this probability increases severely for highly duplicated strings. The small memorization values at low numbers of repetitions corroborates the positive impact of training dataset deduplication on memorization observed by Lee et al. (2021). However, we find that memorization does still happen, even with just a few duplicates-thus, deduplication will not perfectly prevent leakage. While this relationship is perhaps obvious, and has been corroborated for specific training examples in prior work (Carlini et al., 2019; 2020), our results show that it holds across the entire training set.</p>
<h3>4.3 LONGER CONTEXT DISCOVERS MORE MEMORIZATION</h3>
<p>The previous two questions evaluated how data collection and model training decisions impact the leakage of a model's training data when it is provided a fixed number of tokens from a sequence as context. As a result, those experiments suggest particular actions that could be taken to mitigate memorization (by reducing model size, or limiting the number of duplicate examples).</p>
<p>However, even when the model is fixed, it is possible to vary the amount of extractable training data by controlling the length of the prefix passed to the model. By studying how the number of tokens of context impacts extractability, we demonstrate the difficulty of discovering memorization-language models may only exhibit their memorization under favorable conditions.</p>
<p>Results. In Figure 1c, we observe that the fraction of extractable sequences increases log-linearly with the number of tokens of context. For example, $33 \%$ of training sequences in our evaluation set are extractable from the 6 B model at 50 tokens of context, compared to $65 \%$ with 450 tokens of context. We call this the discoverability phenomenon: some memorization only becomes apparent under certain conditions, such as when the model is prompted with a sufficiently long context.</p>
<p>The discoverability phenomenon may seem natural: conditioning a model on 100 tokens of context is more specific than conditioning the model on 50 tokens of context, and it is natural that the model would estimate the probability of the training data as higher in this situation. However, the result is that some strings are "hidden" in the model and require more knowledge than others to be extractable.</p>
<p>From one point of view, it is good that some memorization is difficult to discover. This makes it harder for attackers to perform training data extraction attacks (Carlini et al., 2020), or otherwise exploit memorization. Indeed, if an exact 100 token prompt is required to make the model output a given string, then, in practice, an adversary will likely be unable to perform the attack. The difficulty</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: (a) Fraction of sequences extracted as a function of model scale where we sample uniformly from the training set. (b) Fraction of sequences extracted as we vary the length of the prompt. For each sequence length $n$, $n-50$ tokens are used as the prefix, and we check for extraction of the remaining 50 tokens. (c-left) Using beam search with $b=100$ slightly increases the data extracted. (c-right) We observe considerably more memorization when checking whether the generated sequence occurs anywhere in the entire training set (Section C). However, this approach is very computationally expensive so we do not use it for our other experiments.
in discovering memorization also reduces the likelihood of non-adversarial training data regurgitation. For example, the GitHub Copilot model (Chen et al., 2021) reportedly rarely emits memorized code in benign situations, and most memorization occurs only when the model has been prompted with long code excerpts that are very similar to the training data (Ziegler, 2021). Practitioners building language generation APIs could (until stronger attacks are developed) significantly reduce extraction risk by restricting the maximum prompt length available to users.</p>
<p>Viewed differently, however, the difficulty of discovering memorization can also harm our ability to audit privacy in machine learning models. Because provably-correct approaches for privacypreserving training of machine learning models are applied only rarely in practice (Abadi et al., 2016; Thakkar et al., 2020; Ramaswamy et al., 2020), it is common to attempt post-hoc privacy auditing (Jayaraman and Evans, 2019; Jagielski et al., 2020; Nasr et al., 2021). Our results suggest that correctly auditing large language models likely requires prompting the model with training data, as there are no known techniques to identify the tail of memorized data without conditioning the model with a large context. Improving upon this limitation is an interesting problem for future work.</p>
<h1>4.4 Alternate Experimental Settings</h1>
<p>In this section, we briefly review other strategies that we could have used to quantify memorization.
Random dataset sampling. The majority of this paper uses subsets of the training data that were explicitly sampled according to training data duplication frequency. Now, we consider how our results would differ if we chose a truly random subset of the training data, where each sequence is sampled uniformly, instead of sampling a duplicate-normalized dataset. Specifically, we randomly sample 100,000 sequences of varying lengths from The Pile dataset, then prompt the model and test for memorization as before (more details in Appendix C).</p>
<p>Figure 2a and Figure 2b present the results. We observe similar qualitative trends with model scale and context length as in Figure 1. Larger models memorize more training examples than smaller models-and much more than the GPT-2 models that were not trained on The Pile. Similarly, providing more context to a model increases the likelihood we discover memorization. We can extract the last 50 tokens of a length-1000 sequence with $7 \%$ probability for the largest GPT-J 6B model, compared to $4 \%$ probability for the smallest 125 M GPT-Neo model. (And both of these are much larger than the $2 \%$ probability of extraction for the 1.5B parameter GPT2-XL model.) These results, taken together, allow us to estimate a lower bound that there is at least $1 \%$ of The Pile dataset that is extractable by the 6B GPT-J model, but not by GPT-2 XL.</p>
<p>Alternate decoding strategies. We have defined memorization as a model's ability to generate the true continuation when choosing the most likely token at every step of decoding. Yet, this greedy decoding strategy does not produce the overall most likely sequence. Many language model</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Text examples that are memorized by the 6B model, but not by smaller models. Green highlighted text matches the ground truth continuation, while red text indicates incorrect generation.
applications use other decoding strategies, such as beam search to find the generation with highest likelihood. To understand how our choice of decoding strategy affects the amount of memorization we measure, we compare greedy decoding with beam search in Figure 2(c). We find that using beam search with 100 beams results in marginally more extracted memorization. The difference in extractable memorization is just under 2 percentage points on average, with a maximum of 5.6\%. Interestingly, beam search and greedy decoding generated the same output $45 \%$ of the time.
The most common decoding strategy employed by modern LMs is random sampling, where the next token is selected at random according to a probability distribution derived from the model's predictions. McCoy et al. (2021) found that random sampling resulted in generated text with a greater number of novel $n$-grams. Since the goal of our study is to maximize discoverability-an antithetical goal to maximizing linguistic novelty-we do not present experiments that use random sampling.
Alternate definition of extractability. Our main experiments report a sequence as "extractable" if the model's generation is identical to the true suffix of the considered training example. However it is possible this suffix is still present (elsewhere) in the dataset. We now consider a loose lower bound on memorization that considers a sequence memorized if the generation $[p | f(p)]$ from a prompt $p$ is contained anywhere in the training dataset. Searching within the entire dataset finds more memorized content than comparing with the ground truth (Figure 2c). For examples at 100 repetitions, $32.6 \%$ of outputs are contained somewhere in the dataset but just $15.8 \%$ match the ground truth continuation.</p>
<h1>4.5 Qualitative Examples of Memorization</h1>
<p>In Figure 3, we present qualitative examples that are only memorized by the largest (6B) model, but not the smaller ones. We highlight some interesting patterns in these sequences: while the generations from the smaller models do not match the training data, they are generally thematically-relevant and locally consistent. However, a closer inspection reveals that those generations are only syntactically sound, but semantically incorrect. Appendix Figure 8 shows further examples of sequences that are memorized by all the models. We found most of these universally-memorized sequences to be "unconventional" texts such as code snippets or highly duplicated texts such as open source licenses. Figure 13 shows sequences which are memorized by the 6B parameter model despite being infrequent in the training set. These tend to be easily completed text- Figure 14 shows sequences which are repeated thousands of times but are surprisingly not memorized by the 6B parameter model. Many of these are mostly correctly completed, only differing on semantically unimportant characters.</p>
<h2>5 Replication Study</h2>
<p>The above analysis provides evidence that memorization scales log-linearly with model size, data duplicates, and context length. We now replicate this analysis for other language models trained with different datasets and training objectives, namely: (1) the T5 family of models trained on the C4 dataset (Raffel et al., 2020), (2) models from Lee et al. (2021), trained on a deduplicated version of C4, and (3) the OPT family of models (Zhang et al., 2022), also trained on the Pile. We expected our results to cleanly generalize across settings, and this is indeed true for model scale. Yet, the situation is more complicated when considering data duplication, due to training set idiosyncrasies.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: (a) Masked language model objective: Larger models have a higher fraction of sequences extractable on T5. (b) Masked language model objective: Relationship between number of repetitions and extractable tokens on T5. (c) Causal language model objective: Relationship between number of repetitions and memorization on language models trained with deduplicated data.</p>
<h1>5.1 T5 MASKED LANGUAGE MODELING</h1>
<p>Model and dataset. The T5 v1.1 models are masked encoder-decoder models trained to reproduce randomly deleted spans from an input sequence. The models vary in size from 77 M to 11 B billion parameters, and are trained on C4-a 806 GB curated version of English web pages from the Common Crawl. The largest T5 model (11B parameters) is the largest publicly available masked language model. T5 models are thus good candidates for studying how memorization scales with model size.</p>
<p>We must first define what is meant by "extractable data" for the masked language modeling task. T5 models are trained by removing a random $15 \%$ of tokens from each training sequence (i.i.d), and the model must then "fill in the blanks" to restore the tokens that were dropped from the input. As a result of this different training objective, Definition 3.1 is not directly applicable: the model does not operate on a prefix and output a suffix. We instead call a sequence memorized if the model perfectly solves the masked language modeling task on that sequence. For example, we call a 200-token sequence memorized if the model can use the $170(=200 \cdot 0.85))$ tokens of context to perfectly predict the remaining 30 tokens $(=200 \cdot 0.15)$. Because this token-dropping procedure is stochastic, it is possible that one set of dropped tokens might yield an output of "memorized" and another might not. For simplicity, we inspect only one set of masked tokens per sequence; because we are already averaging over 50,000 sequences this additional randomness does not harm the results of our analysis.</p>
<p>Results. In Figure 4a, we reproduce the model scaling effect (from Figure 1a) for T5 models. Larger models similarly have an increased ability to perfectly solve the masked prediction task. Surprisingly, while a scaling trend does hold here as well, the absolute memorization in masked models is an order of magnitude lower than for comparably sized causal language models. For example, the 3B parameter T5-XL model memorizes $3.5 \%$ of sequences repeated 100 times, whereas the GPT-Neo 2.7B model memorizes $53.6 \%$ of sequences repeated 100 times (with 150 tokens of context).</p>
<p>Next, we turn to reproducing the analysis of how memorization scales with data duplication. The situation here becomes significantly less clear. As shown in Figure 4b, sequences duplicated more often tend to be easier to memorize, but there is no monotonic scaling relationship. Compared to the case of the GPT-Neo models trained on The Pile, the relation between data duplication counts and memorization for T5 models trained on C4 exhibits large variance. This variance is statistically significant: sequences repeated 159 to 196 times are memorized with probability less than $5.1 \%$ with $99.7 \%$ confidence (three standard deviations from the mean), however sequences repeated 138 to 158 times (that is, less often) are memorized with probability at least $6.2 \%$ (also with $99.7 \%$ confidence). That is, for some reason, sequences that occur $\sim 140$ times are more likely to be memorized, despite occurring less often, even if we assume a three-sigma error in both measurements simultaneously.</p>
<p>In order to explain this counter-intuitive phenomenon, we qualitatively study each of these two buckets of examples to understand this difference. We find that most of the duplicate examples repeated 138-158 times consist mainly of whitespace tokens. These sequences are thus much easier to predict correctly than other sequences, even if they are repeated more often. This effect, to a lesser extent, can be found in other buckets which contain many approximately near duplicates.</p>
<h1>5.2 Language Models Trained on Deduplicated Data</h1>
<p>Model and dataset. The models used in Lee et al. (2021) are 1.5B parameter causal language models. This model family consists of one model trained on C 4 (the same dataset as T5), one model trained on a version of C 4 that was deduplicated by removing all documents which were near-duplicates of other documents, and one model trained on a version of C 4 that was deduplicated by deleting any string of length-50 tokens that occurred more than once. Lee et al. (2021) found that both types of deduplication reduced the likelihood of memorization.</p>
<p>Results. We were most interested in whether models trained on deduplicated data would still exhibit increased memorization of examples which were repeated frequently in the original, non-deduplicated C4 dataset (e.g., because the deduplication missed some near-duplicates). Figure 4c plots the fraction of sequences memorized by these three models. We draw two interesting conclusions from this data.</p>
<p>First, we confirm that models trained on deduplicated datasets memorize less data than models trained without deduplication. For example, for sequences repeated below 35 times, the exact deduplicated model memorizes an average of $1.2 \%$ of sequences, compared to $3.6 \%$ without deduplication, a statistically significant $\left(p&lt;10^{-15}\right)$ decrease by a factor of $3 \times$. Second, while deduplication does help for sequences repeated up to $\sim 100$ times, it does not help for sequences repeated more often! The extractability of examples repeated at least 408 times is statistically significantly higher than any other number of repeats before this. We hypothesize that this is due to the fact that any deduplication strategy is necessarily imperfect in order to efficiently scale to hundreds of gigabytes of training data. Thus, while it may be possible to remove most instances of duplicate data, different and valid definitions of duplicates can mean deduplication is not exhaustive.</p>
<h3>5.3 Language Models Trained on a Modified Version of the Pile</h3>
<p>Model and dataset. We finally study the OPT family of models (Zhang et al., 2022), that vary from 125 million to 175 billion parameters. ${ }^{3}$ These models were trained on a 800 GB dataset that overlaps with The Pile but is not identical and contains data from many new sources, while also removing some data from the Pile. This dataset was also deduplicated prior to training, and so we do not expect to see duplicate sequences memorized (much) more than sequences repeated only a few times.</p>
<p>Results. Overall, we find that while there are nearly identical scaling trends to those we found for GPT-Neo's model family, the effect size is orders-of-magnitude smaller (figure 7). Even the 66 billion parameter model memorizes a smaller fraction of The Pile than the smallest 125 million parameter GPT Neo model. This suggests two possible conclusions: (a) careful data curation and training can mitigate memorization, or (b) even slight shifts in data distribution can significantly alter what content gets memorized. Without direct access to the original training dataset, we can not distinguish between these two conclusions and hope future work will be able to resolve this question.</p>
<h2>6 CONCLUSION</h2>
<p>Our paper presents the first comprehensive quantitative analysis of memorization in large language models, by re-processing the training set to find memorized data. Our work has two broad conclusions.</p>
<p>For the study of generalization, we have shown that while current LMs do accurately model the statistics of their training data, this need not imply that they faithfully model the desired underlying data distribution. In particular, when the training data distribution is skewed (e.g., by containing many duplicates of some sequences) larger models are likely to learn these unintended dataset peculiarities. It is therefore important to carefully analyze the datasets used to train ever larger models, as future (larger) models are likely to remember even more training details than current (smaller) models.</p>
<p>For the study of privacy, our work indicates that current large language models memorize a significant fraction of their training datasets. Memorization scales log-linear with model size-by doubling the number of parameters in a model we can extract a significantly larger fraction of the dataset. Given that current state-of-the-art models contain more than $200 \times$ as many parameters as the largest 6B parameter model we analyze, it is likely that these even larger models memorize many sequences</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>that are repeated just a handful of times. At the same time, we have shown that this memorization is often hard to discover, and for an attack to actually extract this data it will be necessary to develop qualitatively new attack strategies. Fortunately, it appears that (for the comparatively small models we study) training data inserted just once is rarely memorized, and so deduplicating training datasets (Lee et al., 2021) is likely a practical technique to mitigate the harms of memorization.</p>
<h1>REFERENCES</h1>
<p>Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pages 308-318, 2016.</p>
<p>Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differentially private BERT. arXiv preprint arXiv:2108.01624, 2021.</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. URL https://doi.org/ 10.5281/zenodo. 5297715 . If you use this software, please cite it using these metadata.</p>
<p>Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and Florian Tramèr. What does it mean for a language model to preserve privacy?, 2022.</p>
<p>Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer: Evaluating and testing unintended memorization in neural networks. In USENIX Security Symposium, 2019 .</p>
<p>Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data from large language models. arXiv preprint arXiv:2012.07805, 2020.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.</p>
<p>Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In Theory of cryptography conference, pages 265-284. Springer, 2006.</p>
<p>William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961, 2021.</p>
<p>Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the long tail via influence estimation. In Advances in Neural Information Processing Systems, 2020.</p>
<p>Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confidence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security, pages 1322-1333, 2015.</p>
<p>Karan Ganju, Qi Wang, Wei Yang, Carl A Gunter, and Nikita Borisov. Property inference attacks on fully connected neural networks using permutation invariant representations. In Proceedings of the 2018 ACM SIGSAC conference on computer and communications security, pages 619-633, 2018.</p>
<p>Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.</p>
<p>Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. Ethical challenges in data-driven dialogue systems. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pages 123-129, 2018.</p>
<p>Matthew Jagielski, Jonathan Ullman, and Alina Oprea. Auditing differentially private machine learning: How private is private SGD? arXiv preprint arXiv:2006.07709, 2020.</p>
<p>Bargav Jayaraman and David Evans. Evaluating differentially private machine learning in practice. In 28th ${$ USENIX $}$ Security Symposium ( ${$ USENIX $}$ Security 19), pages 1895-1912, 2019.</p>
<p>Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. arXiv preprint arXiv:2202.06539, 2022.</p>
<p>Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris CallisonBurch, and Nicholas Carlini. Deduplicating training data makes language models better. CoRR, abs/2107.06499, 2021. URL https://arxiv.org/abs/2107.06499.
R. Thomas McCoy, Paul Smolensky, Tal Linzen, Jianfeng Gao, and Asli Celikyilmaz. How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN. CoRR, abs/2111.09509, 2021. URL https://arxiv.org/abs/2111.09509.</p>
<p>Milad Nasr, Shuang Song, Abhradeep Thakurta, Nicolas Papernot, and Nicholas Carlini. Adversary instantiation: Lower bounds for differentially private machine learning. arXiv preprint arXiv:2101.04535, 2021.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr.org/papers/v21/20-074.html.</p>
<p>Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and Françoise Beaufays. Training production language models without memorizing user data. arXiv preprint arXiv:2009.10031, 2020.</p>
<p>Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3-18. IEEE, 2017.</p>
<p>Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, and Françoise Beaufays. Understanding unintended memorization in federated learning, 2020.</p>
<p>Aleena Thomas, David Ifeoluwa Adelani, Ali Davody, Aditya Mogadala, and Dietrich Klakow. Investigating the impact of pre-trained word embeddings on memorization in neural networks. In International Conference on Text, Speech, and Dialogue, pages 273-281. Springer, 2020.</p>
<p>Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.</p>
<p>Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pages 268-282. IEEE, 2018.</p>
<p>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. $I C L R, 2017$.</p>
<p>Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini. Counterfactual memorization in neural language models. arXiv preprint arXiv:2112.12938, 2021.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.</p>
<p>Albert Ziegler. GitHub Copilot: Parrot or crow? https://docs.github.com/en/github/copilot/researchrecitation, 2021.</p>
<h1>A IMPLEMENTATION DETAILS FOR DATASET CREATION</h1>
<p>Intuitively speaking, it is straightforward to construct a dataset containing specifiable proportions of documents at various frequencies. We need only enumerate all sequences repeated various numbers of times, and then sample uniformly at random from each of these subsets. However in practice this is difficult to do, given the scale of these datasets: even asking the question "how many times is this sequence present in the training dataset" requires linear work for each query, and so repeating this thousands of times for an 800 GB dataset would be infeasible.</p>
<p>To do this efficiently, we build on the work of Lee et al. (2021) and construct a suffix array over the training dataset. Such a data structure allows efficient queries to enumerate all sequences of length $k$ that are repeated between $N$ and $M$ times for any $N, M$. This can be accomplished by a linear scan of the suffix array. As notation, write $i$ as the pointer into the dataset at a certain position $j$ of the suffix array (i.e., $A[j]=i$ ), $i^{\prime}$ as the index at position $j+N$ (so that $A[j+N]=i^{\prime}$ ), and $i^{\prime \prime}$ as the index at position $j+M$ (so that $A[j+M]=i^{\prime \prime}$. Then, if $D[i: i+k]=D\left[i^{\prime}: i^{\prime \prime}+k\right]$ but $D[i: i+k] \neq D\left[i^{\prime \prime}: i^{\prime \prime}+k\right]$, the sequence $D[k: i+k]$ is guaranteed to appear between $N$ and $M$ times in the dataset. As a result, we can scan linearly through the suffix array and enumerate all values of $\mathrm{j} j$ to efficiently find all potential sequences repeated between N and M times. From here, we then randomly sample 1,000 indices within these buckets to construct all of our sequences.</p>
<h2>B LONGER DOCUMENTS ARE NOT EASIER TO MEMORIZE THAN SHORTER DOCUMENTS</h2>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Longer sequences are not easier to extract. We compute the probability that an adversary can extract a sequence as a function of the number of tokens of context available, when varying the length of the sequences. All sequences are repeated the same number of times, and evaluated with the same 6B parameter model. Each line represents the fraction extractable in sequences of increasing lengths. Because all lines nearly perfectly overlap, longer sequences are not fundamentally "easier" to extract than shorter sequences.</p>
<p>Intuitively, one might think that longer sequences are more likely in the tail of the distribution, and if the model is trained to a low perplexity, then the tail of the distribution may be more likely to be memorized. This could lead our context length results to be exaggerated (as it would be difficult to untangle the tail effect of memorization from the context length effect). To check if sequence length plays a role in the amount of memorization we can extract with this method, we generated the next 50 tokens after the prompt for various sequence lengths and various prompt lengths. Figure 5 shows the fraction of extractable tokens in the next 50 tokens after the prompt. Each line on the figure represents a set of sequences with sequence lengths between 100 and 500 tokens. For each sequence length, we looked at prompt lengths from 50 tokens to ${$ sequence length -50$}$ tokens. We do not see significant differences between the fraction of extractable tokens with varying prompt lengths across various sequence lengths.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Text examples that are memorized by the 6B model, but not by smaller models. Text highlighted in green matches the ground truth continuation, while text in red indicates incorrect (novel) generation.</p>
<h1>C Alternate Experimental Settings</h1>
<p>In this section, we study other strategies that we could have used to quantify memorization.
Random dataset sampling. In Section 4.4, we explored what would happen if we instead chose a truly random subset of the training data, where each sequence is sampled uniformly. Specifically, we randomly sample 100,000 sequences from The Pile dataset of length 100, 200, 500, and 1,000; prompt the model with the first $N-50$ tokens; and then test for memorization by verifying if the model can emit the remaining 50 tokens perfectly. In our analysis in Figures 2a and 2b, we vary the size of the trained model and the context length we provide it to understand how these factors impact memorization-but this time through prompting the models with randomly sampled training sequences. As expected, the absolute probability of memorization is much lower than in Figure 1 where we prompted models with training data from the sampled duplication-normalized subset.</p>
<p>We observe similar trends with model scale and context length as in our other results. Larger models memorize more training examples than smaller models-and much more than the baseline GPT-2 model that was not trained on The Pile. Similarly, providing more context to a model increases the likelihood we can discover memorization. In Figure 2b, we prompt models with: prompt length $=$ sequence length -50 . We see that the longer prompts are easier to predict correctly than shorter prompts. The baseline GPT-2 model is nearly twice as accurate on sequences of length 1,000 (prompt length $=950$ ) compared to sequences of length 100 (prompt length $=50$ ).</p>
<p>Alternate definition of extractability. Our main experiments report a sequence as "extractable" if the model's generated continuation is identical to the true suffix within that training example. This method is a loose lower bound on memorization. Consider two sequences $x_{1}, x_{2}$ both contained in the training dataset. Suppose these two sequences share the same prefix, and differ only in the final suffix; that is, $x_{1}=[p | s_{1}]$ and $x_{2}=[p | s_{2}]$. When we select $x_{1}$ and prompt the model on the prefix $p$, we will report "success" only if the output equals $s_{1}$, but not if the output is $s_{2}$, even though this is also a form of memorization.</p>
<p>We now consider how our results would change if we instead checked that the generation $[p | f(p)]$ from a prompt $p$ was contained anywhere in the training dataset. This gives a strictly larger measurement of memorization. By comparing these two methods (checking for memorization within the ground truth continuation, and within the entire dataset), we can understand how the choice of measurement affects the results in our experiments.</p>
<p>Searching within the entire dataset finds more memorized content than comparing with the ground truth (Figure 2c). For examples at 100 repetitions $32.6 \%$ of outputs are contained somewhere in the dataset but just $15.8 \%$ match the ground truth continuation. This difference becomes more pronounced as the number of repetitions increases. The maximum difference between these approaches is 28.4\%, at 2,200 repetitions.</p>
<p>We refrain from using this approach for our main experiments, because this definition requires vastly larger computation resources; it requires querying whether hundreds of thousands of sequences are contained in an 800GB training dataset. Therefore, to promote reproducability, the remainder of this paper continues with testing the generated suffix against the single expected training suffix.</p>
<h1>D Text Memorized by Only Some Models</h1>
<p>Table 1: The number of sequences memorized by one model, and not memorized by another.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;">Not Memorized By</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Model</td>
<td style="text-align: right;">Memorized</td>
<td style="text-align: right;">125 M</td>
<td style="text-align: right;">1.3 B</td>
<td style="text-align: right;">2.7 B</td>
<td style="text-align: right;">6 B</td>
</tr>
<tr>
<td style="text-align: left;">125 M</td>
<td style="text-align: right;">4,812</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">328</td>
<td style="text-align: right;">295</td>
<td style="text-align: right;">293</td>
</tr>
<tr>
<td style="text-align: left;">1.3 B</td>
<td style="text-align: right;">10,391</td>
<td style="text-align: right;">5,907</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,205</td>
<td style="text-align: right;">1,001</td>
</tr>
<tr>
<td style="text-align: left;">2.7 B</td>
<td style="text-align: right;">12,148</td>
<td style="text-align: right;">7,631</td>
<td style="text-align: right;">2,962</td>
<td style="text-align: right;">-</td>
<td style="text-align: right;">1,426</td>
</tr>
<tr>
<td style="text-align: left;">6 B</td>
<td style="text-align: right;">14,792</td>
<td style="text-align: right;">10,273</td>
<td style="text-align: right;">5,402</td>
<td style="text-align: right;">4,070</td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 1 shows the total number of sequences that are memorized by one model but not another. Larger models have more uniquely memorized sequences, although every model has some memorization not shared by any other model. (Even the 125 M model memorizes a few sequences that the 6 B model does not.)</p>
<h2>E Memorization in OPT Models</h2>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: We prompt OPT models with data sampled from their training set. We use a prompt length of 100 here. (a) Fraction of sequences extracted as a function of model scale. (b) Fraction of sequences extracted as the number of repetitions of that sequence in the training set increases.</p>
<h2>F Examples of Memorized Texts</h2>
<p>We show examples of texts that are memorized by different models. We consider the case of 50 -token prompts and 50 -token generation. We sample texts with various number of repetitions in the training data. It is impossible to inspect all the generated examples, so we random sample examples satisfying a certain criterion and show a few interesting ones in the paper. Figure 8 lists examples that are memorized by models of all sizes, in the sense that the 50 -token generations match the groundtruth continuations of the prompts.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $=6 B=2.7 B=1.3 B=125 M$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">use this file except in compliance with the License. * You may obtain a copy of the License at * http:// www.apache.org/licenses/LEENSE-2.0 * Unless required by applicable law or agreed</td>
<td style="text-align: center;">to in writing, software * distributed under the License is distributed on an "AS IS" BASIS, * WITHOUT WARRANTIES OF CONDITIONS OF ANY KIND, either express or implied. * See the License for the specific language</td>
</tr>
<tr>
<td style="text-align: center;">* * This program is free software; you can redistribute it and/or modify * it under the terms of the GNU General Public License version 2 and * only version 2 as published by the Free Software Foundation. * *</td>
<td style="text-align: center;">This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY, without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the *</td>
</tr>
<tr>
<td style="text-align: center;">Privacy \&amp; Cookies Policy Privacy Overview This website uses cookies to improve your experience while you navigate through the website. Out of these cookies, the cookies that are categorized as necessary are stored on your browser as they are essential for the working</td>
<td style="text-align: center;">of basic functionalities of the website. We also use thirdparty cookies that help us analyze and understand how you use this website. These cookies will be stored in your browser only with your consent. You also have the option to opt-out of</td>
</tr>
<tr>
<td style="text-align: center;">\end{document[5 in front of \$\documentclass[12pt] {minimal}\usepackage[amsmath] }</td>
<td style="text-align: center;">\usepackage[wasysym]\usepackage[amsfonts]</td>
</tr>
<tr>
<td style="text-align: center;">Len int for shift $&gt;$ uint8); ; shift $=&gt;7$ [if shift $=&gt;64$ ] return ErrIntOverflowRaft</td>
<td style="text-align: center;">if iNdEx $=&gt;1$ [ return in.ErrUnexpectedEOF] b $=&gt;$ dAtA</td>
</tr>
<tr>
<td style="text-align: center;"></object> &lt;oil key="sourceID"</td>
<td style="text-align: center;">&gt;<int key="maxID">10</int> </object> &lt;object class="IBClassDescriber" key="</td>
</tr>
</tbody>
</table>
<p>Figure 8: Text examples that are memorized by all the models: given 50-token prompts on the left, the next 50 tokens generated by all the models match the groundtruth continuation.</p>
<p>Figure 9 lists examples that are memorized by the 6B model but not by smaller ones. Specifically, the 50 -token generations of the 6 B model match the groundtruth continuations exactly, but the generations from the smaller models match neither the groundtruth continuations of the prompted examples nor any other training examples with the same prompts. We find that when smaller models do not get the groundtruth continuation right, they are generally still able to stick to similar topics. However, in many cases, the texts generated by the smaller models are only syntactically sound, but semantically incorrect. Figure 10 and Figure 11 show more examples.</p>
<p>In Figure 12 we show examples that are only memorized by the smallest model, using similar criterion as when we filter examples that are only memorized by the largest model. There are significantly fewer number of examples that are only memorized by the smallest model (35) than that of the largest model (2860). One of those examples (the first row of Figure 12) is particularly interesting: the groundtruth continuation contains a typo due to formatting cutoff. While the smallest model memorized the typo, larger models try to fix the typo.</p>
<p>In Figure 13 and Figure 14 we show examples that are memorized but not heavily duplicated in the training set, and examples that are heavily duplicated but not memorized, respectively. Finally, we show examples that are memorized by GPT2-XL in Figure 15.</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Text examples that are memorized by the 6B model (according to true-continuation match), but not memorized by smaller models (the generated texts do not match the true continuation, nor any other training examples). The first column shows the prompt. The second column shows the prediction from the 6B model, which matches the groundtruth continuation exactly. The remaining columns shows predictions from smaller models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $=6 B$ )</th>
<th style="text-align: center;">2.7B</th>
<th style="text-align: center;">1.3B</th>
<th style="text-align: center;">125M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">disabled. BCL.com will work better for you if you enable JavaScript or switch to a JavaScript supported browser. Restore Consulting Group is an Equal Opportunity Employer, All qualified applicants will receive consideration for employment without regard to race, color,</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity / expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity/ expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable. This website uses cookies to improve your experience. We'll</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity/ expression, national origin, protected veteran status, or any other characteristic protected - See. BCL.com cookies to improve the functionality, performance, and effectiveness of our communications. Detailed</td>
<td style="text-align: center;">religion, sex, national origin, gender identity, sexual orientation, gender identity expression, protected veteran status, or any other characteristic protected - See. BCL.com cookies to improve the functionality, performance, and effectiveness of our communications. Detailed</td>
</tr>
</tbody>
</table>
<p>Figure 10: Continuation of Figure 9.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $=68$ )</th>
<th style="text-align: center;">2.7B</th>
<th style="text-align: center;">1.3B</th>
<th style="text-align: center;">125M</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">_bPLS crypto_unregister_alg); int crypto_register_template\&amp;struct crypto_template<em>#ep21; struct crypto_template </em>\%; int err = #10455;</td>
<td style="text-align: center;">down_write\&amp;crypto_alg_same; list_for_each_entry\&g; \&amp;crypto_template [list_10455 if (a, $=$ bnp)]</td>
<td style="text-align: center;">list_for_each_entry\&g; \&amp;crypto_+_list_10455 if (bns#-mame\&amp;&amp; bnp#-map\&amp;a $=a=a g \cdot c o \cdot a \cdot a m)$</td>
<td style="text-align: center;">$a=a g$ \&amp;c02amd1*g); $\mathrm{dr} f \mathrm{f} \mathrm{pr} \mathrm{d} \mathrm{ir} \mathrm{kt} \mathrm{~g} \mathrm{~d} \mathrm{~g} \mathrm{~d} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{~g} \mathrm{</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">Frequency</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation ( $==68$ )</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">L_LONG_LONG */__STL_TEMPLATE_NULL struct __type_traits=float</td>
<td style="text-align: center;">constructor; typedef _true_type has_trivial_copy_constructor; typedef __true_type has_trivial_assignment_operator;</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">"groupby4_map", "groupby4_map_skew", <br> "groupby4_noskew", "groupby5",</td>
<td style="text-align: center;">"groupby5_map", "groupby5_map_skew", "groupby5_noskew", <br> "groupby6",</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">pair=K12=[_12], fusion::make_pair=K13=[_13], <br> fusion::make_pair=K14=[_14], fusion::make_pair=K15=[_15]);</td>
<td style="text-align: center;">I namespace result_of [ template = typename KO, typename K1, typename K]</td>
</tr>
<tr>
<td style="text-align: center;">2</td>
<td style="text-align: center;">_GLSL_400]) list == QLatin1String("dmat2"); if (variant \&amp; [Lexer::Variant_GLSL_400]) list == QLatin</td>
<td style="text-align: center;">1String("dmat3"); if (variant \&amp; [Lexer::Variant_GLSL_400]) list == <br> QLatin1String("dmat4"); if (variant \&amp; [</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">disabled. BCG.com will work better for you if you enable JavaScript or switch to a JavaScript supported browser. Boston Consulting Group is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color,</td>
<td style="text-align: center;">age, religion, sex, sexual orientation, gender identity / expression, national origin, protected veteran status, or any other characteristic protected under federal, state or local law, where applicable, and those with criminal histories will be considered in a manner consistent with</td>
</tr>
</tbody>
</table>
<p>Figure 13: Text examples that are memorized but are not heavily duplicated in the training set. Many of these have a simple sequential structure (the middle three), may be boilerplate code (the first), or starts out with unique text, and completes with frequently repeated text (the last example). Overall, these are easily completed sequences.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Frequency</th>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation</th>
<th style="text-align: center;">68</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">5526</td>
<td style="text-align: center;">contributors. All rights reserved. // // Licensed under the Apache License, Version 2.0 (the "License"); // you may not use this file except in compliance with the License. // You may obtain a copy of the</td>
<td style="text-align: center;">License at // http://www.apache.org/licenses/ licensee/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an "AS</td>
<td style="text-align: center;">License at // // http://www.apache.org/licenses/ <br> LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License</td>
</tr>
<tr>
<td style="text-align: center;">5652</td>
<td style="text-align: center;">alors are Standing By Our Dumpster Specialists are waiting by the phone to help you get started. However, there is some important information we need before we can give you a price quote and delivery time. Some things we need to know</td>
<td style="text-align: center;">What size dumpster you will need What you plan on putting in your dumpster When you need us to drop your roll off container off How long you plan on keeping your rolloff City \&amp;</td>
<td style="text-align: center;">Include: What size dumpster -is you need? What type of waste do you - is - how much waste do you have? What is the delivery time? What's the drop/time?What</td>
</tr>
<tr>
<td style="text-align: center;">4323</td>
<td style="text-align: center;">a copy of the License at # # http:// www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law</td>
<td style="text-align: center;">or agreed to in writing, software # distributed under the License is distributed on an "AS IS" BASIS, WITHOUT # WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See</td>
<td style="text-align: center;">or agreed to in writing, software # distributed under the License is distributed on an "AS IS" <br> BASIS, + WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</td>
</tr>
<tr>
<td style="text-align: center;">3556</td>
<td style="text-align: center;">date_default_timezone_set() function. In case you used any of those methods and you are still getting this warning, you most likely misspelled the timezone identifier. We selected the timezone 'UTC' for now, but please</td>
<td style="text-align: center;">set date.timezone to select your timezone. in /home/erlypro/public_html/matural/datas/ persobanner_center.php on line 17 <br> Deprecated: Function eregi_replace</td>
<td style="text-align: center;">set date.timezone to select your timezone. in /home/ erlyp /public_html/wp-d炎/2017/07/07/07.php on line 2017/07/07/07.php on line 2017/07/07/07.php on line 2017/07/07/07.07.07 is not safe to say.</td>
</tr>
<tr>
<td style="text-align: center;">3920</td>
<td style="text-align: center;">1\&nbsp;\&nbsp;【手把手翻墙教程】https:// github.com/gfw-breaker/gsides/wiki/ \&nbsp;</td>
<td style="text-align: center;">nbsp;l\&nbsp;\&nbsp;【禁闹聚合安卓版】 (https://github.com/gfw-breaker/bn-android) \&nbsp;\&amp;n</td>
<td style="text-align: center;">nbsp;[/4]「安卓版】https://github.com/gfw- breaker/wdbpack-interpreter/ \&nbsp;\&nbsp;【移向】2.2.2</td>
</tr>
<tr>
<td style="text-align: center;">3675</td>
<td style="text-align: center;">* / _|</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">3030</td>
<td style="text-align: center;">1775, "date": "1775\u20131851", "displayOrder": 1, "fc": "Joseph Mallord William</td>
<td style="text-align: center;">Turner", "gender": "Male", "id": 558, "mda", "Turner, Joseph Mallord William",</td>
<td style="text-align: center;">Turner", "gender": "Male", "id": 1010, "mda", "Turner, Joseph Mallord William",</td>
</tr>
</tbody>
</table>
<p>Figure 14: Text examples that are heavily replicated in the training set but not memorized. We find many examples which have slight differences with no semantic (English) meaning. This includes comment characters in code, non-English characters, template values, error messages, and meaningless symbols. We also surprisingly find a large number of slightly different but heavily repeated documents about dumpsters.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Prompt</th>
<th style="text-align: center;">Continuation (== GPT2-XL)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">165.00 \$175.00 \$180.00 \$185.00 \$190.00 \$195.00 \$200.00 \$210.00 \$220.00 \$230.00 \$240.00 \$250.00 \$260.</td>
<td style="text-align: center;">00 \$270.00 \$280.00 \$290.00 \$300.00 \$310.00 \$320.00 \$330.00 \$340.00 \$350.00 \$360.00 \$370.00 \$380.00 \$</td>
</tr>
<tr>
<td style="text-align: center;">ISRC3DEC2, ARIZONA_ISRC3DEC2MIX_INPUT_1_SOURCE); ARIZONA_MUX_ENUMS(ISRC3DEC3, ARIZONA_</td>
<td style="text-align: center;">ISRC3DEC3MIX_INPUT_1_SOURCE); ARIZONA_MUX_ENUMS(ISRC3DEC4, ARIZONA_ISRC3DEC4MIX_INPUT_</td>
</tr>
<tr>
<td style="text-align: center;">the councils of government, we must guard against the acquisition of unwarranted influence, whether sought or unsought, by the militaryindustrial complex. The potential for the disastrous rise of misplaced power exists and will persist. We must never let the weight</td>
<td style="text-align: center;">of this combination endanger our liberties or democratic processes. We should take nothing for granted. Only an alert and knowledgeable citizenry can compel the proper meshing of the huge industrial and military machinery of defense with our peaceful methods and goals, so that security and</td>
</tr>
<tr>
<td style="text-align: center;">full spreadsheet NEW! Buy our book $\cdot$ Facts are Sacred: the power of data (on Kindle) More open data Data journalism and data visualisations from the Guardian World government data $\cdot$ Search the</td>
<td style="text-align: center;">world's government data with our gateway Development and aid data $\cdot$ Search the world's global development data with our gateway Can you do something with this data? $\cdot$ Flickr Please post your visualisations and mash-ups on</td>
</tr>
<tr>
<td style="text-align: center;">Original press release Get ahead of the crowd by signing up for 420 Investor, the largest \&amp; most comprehensive premium subscription service for cannabis traders and investors since 2013. Published by NCV Newswire The NCV Newswire</td>
<td style="text-align: center;">by New Cannabis Ventures aims to curate high quality content and information about leading cannabis companies to help our readers filter out the noise and to stay on top of the most important cannabis business news. The NCV Newswire is hand-curated by</td>
</tr>
<tr>
<td style="text-align: center;">of sole responsibility of the author(s). The Centre for Research on Globalization will not be responsible for any inaccurate or incorrect statement in this article. The Centre of Research on Globalization grants permission to crosspost Global Research articles on community internet sites</td>
<td style="text-align: center;">as long the source and copyright are acknowledged together with a hyperlink to the original Global Research article. For publication of Global Research articles in print or other forms including commercial internet sites, contact: [email protected] www.globalresearch.ca</td>
</tr>
</tbody>
</table>
<p>Figure 15: Text examples that are from The Pile and memorized by GPT2-XL. The first two examples have a natural sequential structure, while the others appear to represent an overlap in GPT2-XL's training set and The Pile.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ We were unable to access the 175 billion parameter model; we run OPT models up to 66 billion parameters.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6421 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6421</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6421</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-126.html">extraction-schema-126</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <p><strong>Paper ID:</strong> paper-276079350</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2501.18858v2.pdf" target="_blank">BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Within this framework, we introduce the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in two steps. First, it generates high-quality rationales by approximating the optimal thinking process through reinforcement learning, using a novel reward shaping mechanism. Second, it enhances the base LLM by maximizing the joint probability of rationale generation with respect to the model's parameters. Theoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$ representing the number of iterations. Empirical evaluations on math and coding benchmarks demonstrate that our approach consistently improves performance across different base models without requiring human-annotated thinking processes. In addition, BRiTE demonstrates superior performance compared to existing algorithms that bootstrap thinking processes use alternative methods such as rejection sampling, and can even match or exceed the results achieved through supervised fine-tuning with human-annotated data.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6421.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6421.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BRiTE (Gemma-1.1 eval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bootstrapping Reinforced Thinking Process (BRiTE) — evaluation on Gemma-1.1-7B-it</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BRiTE is an RL-based two-stage algorithm that (1) uses reinforcement learning with reward shaping to learn a generator Q of high-quality chain-of-thought (CoT) rationales and (2) fine-tunes the base LLM by maximizing joint rationale+answer probability; applied to Gemma-1.1-7B-it for arithmetic/multi-step math.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemma-1.1-7B-it</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (instruction-tuned variant)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Paper does not provide pretraining corpora details for the base model; models listed are open-source instruction-tuned variants (Gemma family); evaluation/fine-tuning used math datasets such as GSM8K and MATH.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and grade-school math word problems (multi-step reasoning, elementary algebra)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems with free-text answers; model prompted to produce chain-of-thought rationales + final answer</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>grade-school to early middle-school for GSM8K; MATH contains harder competition-level problems (higher difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>BRiTE uses RL to generate CoT rationales (no human-annotated CoT required); evaluation-stage uses SFT on (x, z_Q, y*) tuples; baseline prompting includes CoT prompting and rejection-sampling CoT</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer exact-match / verified correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>GSM8K: BRiTE reports an absolute accuracy ~59.2% (paper notes a ~10-point improvement on GSM8K when applied to Gemma-1.1-7B-it versus the RS baseline); MATH: BRiTE reported 23.7% on MATH for this model (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Paper provides theoretical analysis tying the ψ-update (rationale generator) to a posterior via an entropy-regularized MDP; no layerwise/intervention-based mechanistic probes of numeric computation (no attention or activation probes reported). Token-level reward shaping maps total reward log P(z,y,o|x,θ) to per-token rewards log P(token | prefix), which the authors use to train Q via PPO/GRPO.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper reports that generated rationales can initially lack logical completeness or validity (a general CoT limitation); quality depends on verifier/reward; iterative training may plateau (authors note iteration-1 plateau limiting further gains). No low-level arithmetic error taxonomy (e.g., off-by-one) is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>BRiTE showed improvements across multiple base models (Gemma, Mistral, Llama). The paper highlights a notable gain (~+10 pp on GSM8K) for Gemma-1.1-7B-it, and reports larger gains when scaling dataset + model (see Qwen2.5 experiments). Theoretical convergence of BRiTE is proven at rate 1/T (T = iterations).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6421.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6421.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BRiTE (Qwen2.5 scaling)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BRiTE — scaled training on Qwen2.5-7B with ~40K mixed reasoning dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Scaled BRiTE run on Qwen2.5-7B with a larger mixed math dataset (~40K) showing large absolute gains on multiple harder math benchmarks when RL-bootstrapped rationales and verifier-based feedback are used.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Qwen2.5-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (Qwen family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>BRiTE scaling experiment used a mixed dataset (≈40K) composed of RUC-AIBOX/STILL-3-Preview-RL-Data, MATH, and historical AIME problems (paper-specified mixture); base Qwen pretraining data not detailed in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>MATH500, Minerva Math, OlympiadBench, AIME24, AMC23, GPQA Diamond</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>competition-level math and science multi-step problems (algebra, number theory, olympiad-style problems, contest problems)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language competition-style questions with free-text solutions and verifier/unit-test evaluation (for code tasks separate dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>competition/advanced (undergraduate-level for Minerva; olympiad-level for OlympiadBench; AIME/AMC contest difficulty)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>BRiTE ψ-update via RL (GRPO/PPO) to learn CoT generator Q; ψ-generated rationales used for SFT (θ-update); an external verifier was used in some runs to label correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (pass@1 / verified-correct final-answer rate), reported as percent correct</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>BRiTE (ψ-update) accuracies reported in Table 2: MATH500 79.1%, Minerva Math 35.0%, OlympiadBench 35.7%, AIME24 14.3%, AMC23 57.7%, GPQA Diamond 28.5%. For the same benchmarks, the RS baseline gave MATH500 54.3%, Minerva 21.0%, OlympiadBench 23.1%, AIME24 5.6%, AMC23 31.6%, GPQA 26.9% (showing large absolute gains).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Authors report BRiTE yields faster and more stable training-loss reduction than rejection sampling, and produces higher mean accuracy across training; primary internal-method analysis is theoretical (probabilistic graphical model, EM-like ψ/θ updates, reward shaping proposition). No mechanistic neural-level probes (e.g., attention maps or neuron activations) reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Paper notes dependence on external verifiers for labeling correctness; where verifier quality or coverage is limited (e.g., diagram-based problems), gains may be constrained. Authors also observed training plateau effects after initial iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Substantial improvements when combining BRiTE with larger/more diverse training data and a stronger base model (Qwen2.5-7B + ~40K data). Reported gains vs RS exceed +15 percentage points on several benchmarks (MATH500, Minerva Math, AMC23), indicating favorable scaling with dataset/model capability in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6421.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6421.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Rejection Sampling (RS) baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Rejection sampling EM-type fine-tuning (RS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Rejection-sampling EM-style baseline: generate N candidate rationales/responses per prompt, select those whose final answer passes a verifier, and fine-tune the model on the selected (prompt, rationale, correct-answer) tuples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (Gemma, Mistral, Llama, Qwen experiments used RS as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (instruction-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B / 9B models reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>RS uses model-generated candidates sampled from the base instruction-tuned models; does not require human-annotated rationales but does require a verifier (e.g., answer checker or unit tests for code).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH, MATH500, Minerva Math, OlympiadBench, AIME24, AMC23, GPQA Diamond</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic and math word problems; code generation uses unit-test verification</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language word problems (CoT candidates generated), code prompts with unit tests for correctness</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>ranging from grade-school (GSM8K) to competition-level (MATH500, AIME, AMC, OlympiadBench)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Generate N=30 candidate CoT + final answers (paper's math experiments commonly use N=30), filter selections with verifiers (correct final answer), then SFT on filtered tuples</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Table 2 RS results (paper): MATH500 54.3%, Minerva Math 21.0%, OlympiadBench 23.1%, AIME24 5.6%, AMC23 31.6%, GPQA Diamond 26.9%. On GSM8K/MATH per-model RS numbers are reported in Table 1 (example: for some models RS ~47.7% GSM8K / 10.3% MATH).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>RS is framed in the paper as an EM-type E-step that approximates posterior over rationales by rejection selection; authors analyze RS within their unified probabilistic graphical model and show BRiTE's RL-based E-step generalizes and often outperforms RS.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Requires a reliable verifier: for code tasks, unit tests are needed; for many math problems, constructing a verifier may be non-trivial. RS quality is limited by sampling budget (N) and can select spurious/fragile rationales if the verifier is imperfect.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>RS improvements saturate with limited sampling and modest fine-tuning; BRiTE demonstrates larger gains when replaced by RL-based rationale generation, especially with larger datasets and stronger base models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6421.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6421.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT with human rationales</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Supervised Fine-Tuning (SFT) using human-annotated Chain-of-Thought</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard supervised fine-tuning on datasets that include human-authored rationales (z*) paired with questions and final answers; treated as a strong baseline for reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (Gemma, Mistral, Llama reported as baseline SFT targets)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer (instruction-tuned variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (7B–9B models reported)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>SFT uses human-annotated datasets such as original GSM8K and MATH (paper uses provided human rationales z* when available for SFT baseline runs).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH (and others depending on dataset availability)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step arithmetic / math word problems (chain-of-thought supervision)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language problems with human-annotated step-by-step solutions and final answers</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>GSM8K (grade-school), MATH (competition-level)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Supervised training on (x, z*, y*) tuples; at inference optionally CoT prompting or greedy decoding used</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer correctness)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper reports that BRiTE matches or exceeds SFT performance in multiple settings; exact SFT values vary by base model (Table 1 shows SFT performance per model — e.g., SFT improved base accuracies substantially in many cases).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>SFT is interpreted within the paper's probabilistic framework as maximizing P(z,y|x) when human z* is available (conditional SFT). No mechanistic probes of how human rationales change numeric processing are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Human-annotation is costly and inconsistent; SFT performance depends on quality/coverage of human rationales. Paper emphasizes BRiTE can match/exceed SFT without requiring human rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>SFT improves performance but requires human data; BRiTE achieves comparable or superior gains without human annotations, and thus can scale more cheaply according to the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6421.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e6421.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BRiTE internal/reward-shaping</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BRiTE reward-shaping and MDP interpretation (Proposition 3.7)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper's internal-mechanism analysis: maps the posterior over (z,y,o) needed for the ψ-update to an entropy-regularized deterministic MDP whose optimal policy π* ∝ exp(sum rewards/β). Setting β=1 and token-level rewards equal to log P(token|prefix, x, θ_t) makes the RL optimal policy recover the desired posterior for rationale generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>applies to any autoregressive LLM used as policy (paper used PPO/GRPO to learn Q)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>autoregressive decoder-only transformer (policy interpretation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (applies across reported sizes)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not a dataset entry — describes method to convert sampling/posterior inference into token-level reward signals derived from the current model logits.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>applies to math/code benchmarks used elsewhere in paper (GSM8K, MATH, HumanEval, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>multi-step reasoning / arithmetic / code generation where latent CoT z is to be sampled from a posterior conditioned on correct y</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>natural-language prompts with autoregressive token generation; (z,y,o) represented as token sequences</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>applies generically</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>No external prompting change — training a policy Q via RL where per-token rewards are set to log-probabilities under the reference model (log P(a_j | prefix, x)), optionally augmented by verifier reward (log P(o|x,z,y)).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect — enables learning of higher-quality rationales used downstream to improve accuracy; theoretical guarantee: ψ/θ updates converge at rate 1/T under assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not a single accuracy number; used as the method to obtain Q which enabled the empirical BRiTE gains reported (see Tables 1 & 2).</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>Proposition 3.7 and Section 3.4 show that under deterministic transitions and entropy regularization the optimal trajectory distribution is proportional to exp(sum token rewards/β); authors explicitly use tokenwise log-probabilities as shaped rewards to make the RL objective recover the desired posterior. This is the main internal-mechanistic argument in the paper (no neuron-level interpretability experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Reward shaping requires careful design: if the shaped reward does not match the desired posterior (e.g., missing verifier signal), the learned Q can produce low-quality rationales; computational cost and hyperparameter sensitivity of RL (PPO/GRPO) are implicit limitations discussed.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>Authors used PPO / GRPO with LoRA and different batch/learning-rate regimes; reward-shaping method is presented as scalable and was applied across small (7B) to larger-scale experiments (Qwen2.5 training on 8 H100s) with observed empirical gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6421.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e6421.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including model details, task details, prompting methods, performance results, and any analysis of internal mechanisms or failure modes.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BRiTE-DPO / iterative DPO comparison</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BRiTE-DPO: BRiTE generated rationales used to produce preference pairs for DPO (latent DPO); comparison to iterative DPO baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>BRiTE used to generate CoT+answers which are ranked/categorized to form preference tuples (best/worst) for DPO training; BRiTE-DPO consistently outperformed iterative DPO in the RLHF/preference-learning stage in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various instruction-tuned models (Gemma, Mistral, Llama family used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_family</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B–9B variants in experiments</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Preference data created from model-generated (z,y) pairs; iterative DPO baseline used same generation procedure (30 candidates per prompt) and 3 training iterations.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_name</strong></td>
                            <td>GSM8K, MATH and RLHF4MATH-derived data used for preference training; results aggregated across math benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>preference-based fine-tuning for improved final-answer correctness on multi-step math problems</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>generate N=30 CoT+answers per prompt; select best/worst responses based on final-answer correctness to form preference pairs for DPO</td>
                        </tr>
                        <tr>
                            <td><strong>difficulty_level</strong></td>
                            <td>multi-step math problems (GSM8K/MATH-level and upward depending on dataset used)</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>BRiTE used to learn Q (ψ-update) and then to sample (z,y) for preference labeling; DPO applied to preference pairs (latent-aware DPO objective in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (final-answer correctness) aggregated over benchmarks; qualitative improvement vs iterative DPO shown in Figure 3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Paper states BRiTE consistently outperforms iterative DPO across multiple benchmarks (Figure 3) but does not report a single consolidated numeric delta in the main text for iterative DPO vs BRiTE-DPO.</td>
                        </tr>
                        <tr>
                            <td><strong>internal_analysis</strong></td>
                            <td>BRiTE improves the quality and structure of sampled preference data, which yields higher-quality DPO updates; the latent-variable formulation (incorporating z into DPO objective) is provided analytically (Equation 3.6).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Iterative DPO and BRiTE-DPO both rely on correct preference labeling (here via answer correctness); if sampling or verification is noisy, preference signals degrade. Authors note BRiTE's generated rationales produce more structured preference data, but do not provide failure-mode breakdowns.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_trend</strong></td>
                            <td>BRiTE used across 3 DPO iterations and improved over iterative DPO initialized from instruction-tuned models; improvement trend appears consistent but training may plateau after early iterations per authors' observation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning', 'publication_date_yy_mm': '2025-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Training verifiers to solve math word problems <em>(Rating: 2)</em></li>
                <li>Quiet-star: Language models can teach themselves to think before speaking <em>(Rating: 2)</em></li>
                <li>Star: Bootstrapping reasoning with reasoning <em>(Rating: 2)</em></li>
                <li>Reinforced self-training (rest) for language modeling <em>(Rating: 1)</em></li>
                <li>Direct preference optimization: Your language model is secretly a reward model <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6421",
    "paper_id": "paper-276079350",
    "extraction_schema_id": "extraction-schema-126",
    "extracted_data": [
        {
            "name_short": "BRiTE (Gemma-1.1 eval)",
            "name_full": "Bootstrapping Reinforced Thinking Process (BRiTE) — evaluation on Gemma-1.1-7B-it",
            "brief_description": "BRiTE is an RL-based two-stage algorithm that (1) uses reinforcement learning with reward shaping to learn a generator Q of high-quality chain-of-thought (CoT) rationales and (2) fine-tunes the base LLM by maximizing joint rationale+answer probability; applied to Gemma-1.1-7B-it for arithmetic/multi-step math.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Gemma-1.1-7B-it",
            "model_family": "decoder-only transformer (instruction-tuned variant)",
            "model_size": "7B",
            "training_data_description": "Paper does not provide pretraining corpora details for the base model; models listed are open-source instruction-tuned variants (Gemma family); evaluation/fine-tuning used math datasets such as GSM8K and MATH.",
            "benchmark_name": "GSM8K and MATH",
            "task_type": "multi-step arithmetic and grade-school math word problems (multi-step reasoning, elementary algebra)",
            "problem_format": "natural-language word problems with free-text answers; model prompted to produce chain-of-thought rationales + final answer",
            "difficulty_level": "grade-school to early middle-school for GSM8K; MATH contains harder competition-level problems (higher difficulty)",
            "prompting_method": "BRiTE uses RL to generate CoT rationales (no human-annotated CoT required); evaluation-stage uses SFT on (x, z_Q, y*) tuples; baseline prompting includes CoT prompting and rejection-sampling CoT",
            "performance_metric": "accuracy (final-answer exact-match / verified correctness)",
            "performance_value": "GSM8K: BRiTE reports an absolute accuracy ~59.2% (paper notes a ~10-point improvement on GSM8K when applied to Gemma-1.1-7B-it versus the RS baseline); MATH: BRiTE reported 23.7% on MATH for this model (Table 1).",
            "internal_analysis": "Paper provides theoretical analysis tying the ψ-update (rationale generator) to a posterior via an entropy-regularized MDP; no layerwise/intervention-based mechanistic probes of numeric computation (no attention or activation probes reported). Token-level reward shaping maps total reward log P(z,y,o|x,θ) to per-token rewards log P(token | prefix), which the authors use to train Q via PPO/GRPO.",
            "failure_modes": "Paper reports that generated rationales can initially lack logical completeness or validity (a general CoT limitation); quality depends on verifier/reward; iterative training may plateau (authors note iteration-1 plateau limiting further gains). No low-level arithmetic error taxonomy (e.g., off-by-one) is provided.",
            "scaling_trend": "BRiTE showed improvements across multiple base models (Gemma, Mistral, Llama). The paper highlights a notable gain (~+10 pp on GSM8K) for Gemma-1.1-7B-it, and reports larger gains when scaling dataset + model (see Qwen2.5 experiments). Theoretical convergence of BRiTE is proven at rate 1/T (T = iterations).",
            "uuid": "e6421.0",
            "source_info": {
                "paper_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BRiTE (Qwen2.5 scaling)",
            "name_full": "BRiTE — scaled training on Qwen2.5-7B with ~40K mixed reasoning dataset",
            "brief_description": "Scaled BRiTE run on Qwen2.5-7B with a larger mixed math dataset (~40K) showing large absolute gains on multiple harder math benchmarks when RL-bootstrapped rationales and verifier-based feedback are used.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Qwen2.5-7B",
            "model_family": "decoder-only transformer (Qwen family)",
            "model_size": "7B",
            "training_data_description": "BRiTE scaling experiment used a mixed dataset (≈40K) composed of RUC-AIBOX/STILL-3-Preview-RL-Data, MATH, and historical AIME problems (paper-specified mixture); base Qwen pretraining data not detailed in paper.",
            "benchmark_name": "MATH500, Minerva Math, OlympiadBench, AIME24, AMC23, GPQA Diamond",
            "task_type": "competition-level math and science multi-step problems (algebra, number theory, olympiad-style problems, contest problems)",
            "problem_format": "natural-language competition-style questions with free-text solutions and verifier/unit-test evaluation (for code tasks separate dataset)",
            "difficulty_level": "competition/advanced (undergraduate-level for Minerva; olympiad-level for OlympiadBench; AIME/AMC contest difficulty)",
            "prompting_method": "BRiTE ψ-update via RL (GRPO/PPO) to learn CoT generator Q; ψ-generated rationales used for SFT (θ-update); an external verifier was used in some runs to label correctness.",
            "performance_metric": "accuracy (pass@1 / verified-correct final-answer rate), reported as percent correct",
            "performance_value": "BRiTE (ψ-update) accuracies reported in Table 2: MATH500 79.1%, Minerva Math 35.0%, OlympiadBench 35.7%, AIME24 14.3%, AMC23 57.7%, GPQA Diamond 28.5%. For the same benchmarks, the RS baseline gave MATH500 54.3%, Minerva 21.0%, OlympiadBench 23.1%, AIME24 5.6%, AMC23 31.6%, GPQA 26.9% (showing large absolute gains).",
            "internal_analysis": "Authors report BRiTE yields faster and more stable training-loss reduction than rejection sampling, and produces higher mean accuracy across training; primary internal-method analysis is theoretical (probabilistic graphical model, EM-like ψ/θ updates, reward shaping proposition). No mechanistic neural-level probes (e.g., attention maps or neuron activations) reported.",
            "failure_modes": "Paper notes dependence on external verifiers for labeling correctness; where verifier quality or coverage is limited (e.g., diagram-based problems), gains may be constrained. Authors also observed training plateau effects after initial iterations.",
            "scaling_trend": "Substantial improvements when combining BRiTE with larger/more diverse training data and a stronger base model (Qwen2.5-7B + ~40K data). Reported gains vs RS exceed +15 percentage points on several benchmarks (MATH500, Minerva Math, AMC23), indicating favorable scaling with dataset/model capability in this paper's experiments.",
            "uuid": "e6421.1",
            "source_info": {
                "paper_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "Rejection Sampling (RS) baseline",
            "name_full": "Rejection sampling EM-type fine-tuning (RS)",
            "brief_description": "Rejection-sampling EM-style baseline: generate N candidate rationales/responses per prompt, select those whose final answer passes a verifier, and fine-tune the model on the selected (prompt, rationale, correct-answer) tuples.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various (Gemma, Mistral, Llama, Qwen experiments used RS as baseline)",
            "model_family": "decoder-only transformer (instruction-tuned variants)",
            "model_size": "various (7B / 9B models reported)",
            "training_data_description": "RS uses model-generated candidates sampled from the base instruction-tuned models; does not require human-annotated rationales but does require a verifier (e.g., answer checker or unit tests for code).",
            "benchmark_name": "GSM8K, MATH, MATH500, Minerva Math, OlympiadBench, AIME24, AMC23, GPQA Diamond",
            "task_type": "multi-step arithmetic and math word problems; code generation uses unit-test verification",
            "problem_format": "natural-language word problems (CoT candidates generated), code prompts with unit tests for correctness",
            "difficulty_level": "ranging from grade-school (GSM8K) to competition-level (MATH500, AIME, AMC, OlympiadBench)",
            "prompting_method": "Generate N=30 candidate CoT + final answers (paper's math experiments commonly use N=30), filter selections with verifiers (correct final answer), then SFT on filtered tuples",
            "performance_metric": "accuracy (final-answer correctness)",
            "performance_value": "Table 2 RS results (paper): MATH500 54.3%, Minerva Math 21.0%, OlympiadBench 23.1%, AIME24 5.6%, AMC23 31.6%, GPQA Diamond 26.9%. On GSM8K/MATH per-model RS numbers are reported in Table 1 (example: for some models RS ~47.7% GSM8K / 10.3% MATH).",
            "internal_analysis": "RS is framed in the paper as an EM-type E-step that approximates posterior over rationales by rejection selection; authors analyze RS within their unified probabilistic graphical model and show BRiTE's RL-based E-step generalizes and often outperforms RS.",
            "failure_modes": "Requires a reliable verifier: for code tasks, unit tests are needed; for many math problems, constructing a verifier may be non-trivial. RS quality is limited by sampling budget (N) and can select spurious/fragile rationales if the verifier is imperfect.",
            "scaling_trend": "RS improvements saturate with limited sampling and modest fine-tuning; BRiTE demonstrates larger gains when replaced by RL-based rationale generation, especially with larger datasets and stronger base models.",
            "uuid": "e6421.2",
            "source_info": {
                "paper_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "SFT with human rationales",
            "name_full": "Supervised Fine-Tuning (SFT) using human-annotated Chain-of-Thought",
            "brief_description": "Standard supervised fine-tuning on datasets that include human-authored rationales (z*) paired with questions and final answers; treated as a strong baseline for reasoning tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "various (Gemma, Mistral, Llama reported as baseline SFT targets)",
            "model_family": "decoder-only transformer (instruction-tuned variants)",
            "model_size": "various (7B–9B models reported)",
            "training_data_description": "SFT uses human-annotated datasets such as original GSM8K and MATH (paper uses provided human rationales z* when available for SFT baseline runs).",
            "benchmark_name": "GSM8K, MATH (and others depending on dataset availability)",
            "task_type": "multi-step arithmetic / math word problems (chain-of-thought supervision)",
            "problem_format": "natural-language problems with human-annotated step-by-step solutions and final answers",
            "difficulty_level": "GSM8K (grade-school), MATH (competition-level)",
            "prompting_method": "Supervised training on (x, z*, y*) tuples; at inference optionally CoT prompting or greedy decoding used",
            "performance_metric": "accuracy (final-answer correctness)",
            "performance_value": "Paper reports that BRiTE matches or exceeds SFT performance in multiple settings; exact SFT values vary by base model (Table 1 shows SFT performance per model — e.g., SFT improved base accuracies substantially in many cases).",
            "internal_analysis": "SFT is interpreted within the paper's probabilistic framework as maximizing P(z,y|x) when human z* is available (conditional SFT). No mechanistic probes of how human rationales change numeric processing are reported.",
            "failure_modes": "Human-annotation is costly and inconsistent; SFT performance depends on quality/coverage of human rationales. Paper emphasizes BRiTE can match/exceed SFT without requiring human rationales.",
            "scaling_trend": "SFT improves performance but requires human data; BRiTE achieves comparable or superior gains without human annotations, and thus can scale more cheaply according to the paper.",
            "uuid": "e6421.3",
            "source_info": {
                "paper_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BRiTE internal/reward-shaping",
            "name_full": "BRiTE reward-shaping and MDP interpretation (Proposition 3.7)",
            "brief_description": "Paper's internal-mechanism analysis: maps the posterior over (z,y,o) needed for the ψ-update to an entropy-regularized deterministic MDP whose optimal policy π* ∝ exp(sum rewards/β). Setting β=1 and token-level rewards equal to log P(token|prefix, x, θ_t) makes the RL optimal policy recover the desired posterior for rationale generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "applies to any autoregressive LLM used as policy (paper used PPO/GRPO to learn Q)",
            "model_family": "autoregressive decoder-only transformer (policy interpretation)",
            "model_size": "various (applies across reported sizes)",
            "training_data_description": "Not a dataset entry — describes method to convert sampling/posterior inference into token-level reward signals derived from the current model logits.",
            "benchmark_name": "applies to math/code benchmarks used elsewhere in paper (GSM8K, MATH, HumanEval, etc.)",
            "task_type": "multi-step reasoning / arithmetic / code generation where latent CoT z is to be sampled from a posterior conditioned on correct y",
            "problem_format": "natural-language prompts with autoregressive token generation; (z,y,o) represented as token sequences",
            "difficulty_level": "applies generically",
            "prompting_method": "No external prompting change — training a policy Q via RL where per-token rewards are set to log-probabilities under the reference model (log P(a_j | prefix, x)), optionally augmented by verifier reward (log P(o|x,z,y)).",
            "performance_metric": "indirect — enables learning of higher-quality rationales used downstream to improve accuracy; theoretical guarantee: ψ/θ updates converge at rate 1/T under assumptions.",
            "performance_value": "Not a single accuracy number; used as the method to obtain Q which enabled the empirical BRiTE gains reported (see Tables 1 & 2).",
            "internal_analysis": "Proposition 3.7 and Section 3.4 show that under deterministic transitions and entropy regularization the optimal trajectory distribution is proportional to exp(sum token rewards/β); authors explicitly use tokenwise log-probabilities as shaped rewards to make the RL objective recover the desired posterior. This is the main internal-mechanistic argument in the paper (no neuron-level interpretability experiments).",
            "failure_modes": "Reward shaping requires careful design: if the shaped reward does not match the desired posterior (e.g., missing verifier signal), the learned Q can produce low-quality rationales; computational cost and hyperparameter sensitivity of RL (PPO/GRPO) are implicit limitations discussed.",
            "scaling_trend": "Authors used PPO / GRPO with LoRA and different batch/learning-rate regimes; reward-shaping method is presented as scalable and was applied across small (7B) to larger-scale experiments (Qwen2.5 training on 8 H100s) with observed empirical gains.",
            "uuid": "e6421.4",
            "source_info": {
                "paper_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        },
        {
            "name_short": "BRiTE-DPO / iterative DPO comparison",
            "name_full": "BRiTE-DPO: BRiTE generated rationales used to produce preference pairs for DPO (latent DPO); comparison to iterative DPO baseline",
            "brief_description": "BRiTE used to generate CoT+answers which are ranked/categorized to form preference tuples (best/worst) for DPO training; BRiTE-DPO consistently outperformed iterative DPO in the RLHF/preference-learning stage in the authors' experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various instruction-tuned models (Gemma, Mistral, Llama family used in experiments)",
            "model_family": "decoder-only transformer",
            "model_size": "7B–9B variants in experiments",
            "training_data_description": "Preference data created from model-generated (z,y) pairs; iterative DPO baseline used same generation procedure (30 candidates per prompt) and 3 training iterations.",
            "benchmark_name": "GSM8K, MATH and RLHF4MATH-derived data used for preference training; results aggregated across math benchmarks",
            "task_type": "preference-based fine-tuning for improved final-answer correctness on multi-step math problems",
            "problem_format": "generate N=30 CoT+answers per prompt; select best/worst responses based on final-answer correctness to form preference pairs for DPO",
            "difficulty_level": "multi-step math problems (GSM8K/MATH-level and upward depending on dataset used)",
            "prompting_method": "BRiTE used to learn Q (ψ-update) and then to sample (z,y) for preference labeling; DPO applied to preference pairs (latent-aware DPO objective in paper).",
            "performance_metric": "accuracy (final-answer correctness) aggregated over benchmarks; qualitative improvement vs iterative DPO shown in Figure 3",
            "performance_value": "Paper states BRiTE consistently outperforms iterative DPO across multiple benchmarks (Figure 3) but does not report a single consolidated numeric delta in the main text for iterative DPO vs BRiTE-DPO.",
            "internal_analysis": "BRiTE improves the quality and structure of sampled preference data, which yields higher-quality DPO updates; the latent-variable formulation (incorporating z into DPO objective) is provided analytically (Equation 3.6).",
            "failure_modes": "Iterative DPO and BRiTE-DPO both rely on correct preference labeling (here via answer correctness); if sampling or verification is noisy, preference signals degrade. Authors note BRiTE's generated rationales produce more structured preference data, but do not provide failure-mode breakdowns.",
            "scaling_trend": "BRiTE used across 3 DPO iterations and improved over iterative DPO initialized from instruction-tuned models; improvement trend appears consistent but training may plateau after early iterations per authors' observation.",
            "uuid": "e6421.5",
            "source_info": {
                "paper_title": "BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning",
                "publication_date_yy_mm": "2025-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Training verifiers to solve math word problems",
            "rating": 2,
            "sanitized_title": "training_verifiers_to_solve_math_word_problems"
        },
        {
            "paper_title": "Quiet-star: Language models can teach themselves to think before speaking",
            "rating": 2,
            "sanitized_title": "quietstar_language_models_can_teach_themselves_to_think_before_speaking"
        },
        {
            "paper_title": "Star: Bootstrapping reasoning with reasoning",
            "rating": 2,
            "sanitized_title": "star_bootstrapping_reasoning_with_reasoning"
        },
        {
            "paper_title": "Reinforced self-training (rest) for language modeling",
            "rating": 1,
            "sanitized_title": "reinforced_selftraining_rest_for_language_modeling"
        },
        {
            "paper_title": "Direct preference optimization: Your language model is secretly a reward model",
            "rating": 2,
            "sanitized_title": "direct_preference_optimization_your_language_model_is_secretly_a_reward_model"
        }
    ],
    "cost": 0.02136525,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning
6 Jun 2025</p>
<p>Han Zhong hanzhong@stu.pku.edu.cn 
Peking University</p>
<p>Yutong Yin yutongyin2028@u.northwestern.edu 
Northwestern University</p>
<p>Shenao Zhang shenaozhang2028@u.northwestern.edu 
Northwestern University</p>
<p>Xiaojun Xu xiaojun.xu@bytedance.com 
Bytedance Inc</p>
<p>Yuanxin Liu yuanxinliu2029@u.northwestern.edu 
Northwestern University</p>
<p>Yifei Zuo yifeizuo2029@u.northwestern.edu 
Northwestern University</p>
<p>Zhihan Liu zhihanliu2027@u.northwestern.edu 
Boyi Liu boyi.liu01@bytedance.com 
Northwestern University</p>
<p>Bytedance Inc</p>
<p>Sirui Zheng siruizheng2025@u.northwestern.edu 
Northwestern University</p>
<p>Hongyi Guo hongyiguo2025@u.northwestern.edu 
Northwestern University</p>
<p>Liwei Wang wanglw@cis.pku.edu.cn 
Peking University</p>
<p>Zhaoran Wang zhaoranwang@gmail.com 
Northwestern University</p>
<p>University of Minnesota</p>
<p>BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning
6 Jun 20259EC2968058247636CE2D8FCE9DEFF01DarXiv:2501.18858v2[cs.LG]
Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge.We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals.Within this framework, we introduce the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in two steps.First, it generates high-quality rationales by approximating the optimal thinking process through reinforcement learning, using a novel reward shaping mechanism.Second, it enhances the base LLM by maximizing the joint probability of rationale generation with respect to the model's parameters.Theoretically, we demonstrate BRiTE's convergence at a rate of 1/T with T representing the number of iterations.Empirical evaluations on math and coding benchmarks demonstrate that our approach consistently improves performance across different base models without requiring human-annotated thinking processes.In addition, BRiTE demonstrates superior performance compared to existing algorithms that bootstrap thinking processes use alternative methods such as rejection sampling, and can even match or exceed the results achieved through supervised fine-tuning with human-annotated data.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs;OpenAI, 2023;Anthropic, 2023;Team et al., 2024a), have emerged as a breakthrough in artificial intelligence, demonstrating unprecedented capabilities in natural language processing and generation.The training pipeline of these state-of-the-art models consists of two critical phases: pre-training and post-training.During the pre-training phase, LLMs learn from vast datasets to predict subsequent tokens in sequences, enabling them to learn extensive linguistic patterns, contextual understanding, and general world knowledge.The post-training phase further refines these models through two stages: Supervised Fine-tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017;Ziegler et al., 2019;Ouyang et al., 2022).Recent research (OpenAI, 2024) has shown that by scaling the inference time, these models demonstrate sophisticated reasoning capabilities, particularly in domains such as mathematics and programming.</p>
<p>Unlocking LLM reasoning abilities typically relies on structured prompting methods that break down problems into step-by-step solutions, known as chain-of-thought (CoT) reasoning (Wei et al., 2022).While this approach has shown promise and inspired various extensions (Wang et al., 2022;Yao et al., 2024), the fundamental challenge of reasoning reliability remains unresolved.Generated rationales often lack logical completeness or validity, with their quality heavily dependent on task-specific prompting strategies.Recent developments in inference-time scaling techniques (Snell et al., 2024) have shown potential improvements.</p>
<p>However, these approaches primarily address surface-level symptoms rather than the core challenge of generating high-quality reasoning processes.Furthermore, the field increasingly seeks automated improvements to reduce reliance on manual prompt engineering.This context motivates our designing mechanism for highquality thinking process generation.Meanwhile, prior research (e.g., Zelikman et al., 2022;Yuan et al., 2023) indicates that reasoning processes, when properly selected via verifiers, can enhance an LLM's reasoning capabilities during post-training.This leads to our research objective: developing a framework for the automated generation of high-quality (correct) reasoning processes and incorporating them into the post-training stage to improve existing algorithms.</p>
<p>To this end, we propose a unified probabilistic framework that formalizes the reasoning process accompanying evaluation signals, followed by a generic algorithmic framework.Specifically, our work has three key contributions:</p>
<p>• We formulate the problem as a probabilistic graphical model (Figure 1), characterizing the generation flow from prompt X to latent rationales Z to answer Y , along with their corresponding evaluation signal O.This explicit mathematical characterization serves two essential purposes: first, introducing Z breaks down the complex distribution P(Y | X) into more tractable marginal distributions P(Z | X) and P(Y | X, Z), which aligns with Chain-of-Thought (CoT) methods (Wei et al., 2022); second, introducing O provides crucial rationale-answer quality signal, making the generation of desired (correct) rationales more achievable.</p>
<p>• Under this framework, our learning objective is to maximize the probability of generating high-quality rationales and answers that yield optimal evaluation signals.To achieve this, we propose the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm consisting of two stages: first generating high-quality rationales by training an LLM whose output approximates the desired posterior of thought given the question-answer pair; and then fine-tuning the seed LLM by maximizing the joint probability of rationale generation with respect to the LLM's parameters.Theoretically, we prove our algorithm converges at a rate of 1/T , where T is the number of iterations.Regarding the practical implementation, to address the challenging Bayesian inference problem in the former step, we develop a novel reward shaping mechanism that converts it into a reinforcement learning optimization problem.</p>
<p>• Our empirical evaluations of math and code generation benchmarks show consistent improvements across multiple LLM series, including Gemma, Llama, and Mistral.Our experimental results demonstrate BRiTE's ability to enhance existing post-training algorithms (rejection sampling type methods and iterative DPO) through RL-based rationale generation with consistent improvements.Notably, BRiTE achieves a 10-point improvement on GSM8K benchmarks when applied to the Gemma-1.1-7B-itbase model.Notably, our algorithm matches or even exceeds the performance of supervised fine-tuning methods that use human-labeled thinking processes, despite not requiring any human-annotated data.</p>
<p>In summary, we present a provable and practical framework for automated thinking process generation that can be seamlessly integrated into the training stage.The thinking processes generated by our framework are of high quality, surpassing not only those produced through CoT prompting, but also outperforming human-annotated thinking processes when applied to fine-tuning.Our framework represents a significant advancement in improving LLM reasoning capacity through the creation of synthetic data that incorporates detailed thinking processes (CoT data).</p>
<p>Related Works</p>
<p>Reasoning in LLMs.Prior work has explored various prompting techniques to enhance language model reasoning capabilities.CoT prompting (Wei et al., 2022) has emerged as a particularly effective approach, encouraging models to break down complex problems into intermediate steps by demonstrating step-by-step reasoning paths.The following works, such as Zhou et al. (2022); Yao et al. (2024), design more prompting techniques to enhance the model's capacity.However, these methods typically rely on manually crafted (CoT) prompts to elicit reasoning processes, which are then used to guide the model's generation process.While such approaches have shown promising results in improving model performance across various reasoning tasks, they remain dependent on human-designed prompting templates and may not fully capture the natural reasoning patterns that emerge during model inference.To this end, a line of works aims to boost the latent reasoning process quality or even achieve automatic reasoning process generation, where the latter one is our focus but our method is based on reinforcement learning and thus is different from previous works.Previous methods can be roughly regarded as an EM-type algorithm, and detailed comparisons are presented below.</p>
<p>EM-type Methods.Our algorithmic framework builds upon the Expectation-Maximization (EM) algorithm (Dempster et al., 1977) and its variant, rejection sampling EM (Neal and Hinton, 1998;Rush and Ritter, 2024).In standard EM, the E-step learns a posterior over latent variables while the M-step maximizes the expected log-likelihood; rejection sampling EM approximates complex posteriors through rejection sampling in the E-step before proceeding with the standard M-step.This motivates various rejection sampling fine-tuning methods employed in modern LLM training (e.g., Cobbe et al., 2021;Zelikman et al., 2022;Dong et al., 2023a;Yuan et al., 2023;Gulcehre et al., 2023;Singh et al., 2023;Zelikman et al., 2024;Yuan et al., 2024;Chen et al., 2024).These algorithms filter outputs or/and latent reasoning process through reward functions or verifiers (E-step), then perform fine-tuning on the selected samples (M-step).Our algorithm generalizes these approaches (Example 3.6) and introduces a novel E-step based on reinforcement learning (Section 3.4).While Hu et al. (2023); Hoffman et al. (2024) also conceptualize LLM generation as latent variable models and propose EM-type algorithms based on Markov chain Monte Carlo (MCMC) or generative flow networks, our work distinctively focuses on enhancing the reasoning capabilities of LLMs through automated reasoning processes using reinforcement learning (PPO).Notably, in contrast to these prior works, we provide a more general and rigorous mathematical framework fro LLM reason and unified theoretical guarantees.Furthermore, our framework extends beyond supervised and rejection sampling finetuning to advance iterative direct preference learning (Xiong et al., 2024), contributing to improved LLM reasoning in the RLHF paradigm.Finally, two recent works by Liu et al. (2024a) and Wang et al. (2024) also apply RL techniques to enhance LLM reasoning.However, these works do not focus on improving the thinking process generation, making them not directly comparable to our approach.RLHF.RLHF (Christiano et al., 2017;Ziegler et al., 2019), also known as the dueling RL (Yue et al., 2012;Pacchiano et al., 2021) or preference-based RL (Wirth et al., 2017;Chen et al., 2022), has emerged as a breakthrough technique in language model development, playing a pivotal role in the success of ChatGPT (Ouyang et al., 2022) and subsequent large language models by effectively aligning model outputs with human preferences.In the standard RLHF pipeline, the first stage involves learning a reward function from human preference data, followed by optimization using proximal policy optimization (Schulman et al., 2017).However, this approach demands substantial computational resources and requires careful tuning of numerous hyperparameters.Direct preference learning (e.g., Zhao et al., 2023;Rafailov et al., 2024;Azar et al., 2024;Tang et al., 2024;Meng et al., 2024;Liu et al., 2024b;Cen et al., 2024) offers an alternative by learning the desired policy directly, circumventing the need for explicit reward learning and optimization.Recent work by Xiong et al. (2024); Xie et al. (2024); Cen et al. (2024); Zhang et al. (2024) extends these offline approaches through online iterative learning to gather on-policy data for enhanced performance.However, these algorithms mainly focus on the chat task and do not explicitly model the latent reasoning process, thus not fully extracting the reasoning power of LLMs.While Pang et al. (2024); Wu et al. (2024) incorporate reasoning processes into DPO training using CoT prompting, our approach distinctively generates reasoning processes automatically through RL, demonstrating significant improvements in model reasoning capacity.</p>
<p>Notations</p>
<p>For any space X , we denote ∆(X ) as the set of distributions over X .For any positive integer h, we denote the sequence {a 1 , • • • , a h } by a 1:h .We use 1{•} to denote the indicator function.</p>
<p>Preliminaries</p>
<p>In this section, we present the single-step bandit formulation and the multi-step Markov decision process (MDP) formulation for LLMs.</p>
<p>Bandit Formulation of LLMs.A simple way to understand LLMs is through the bandit formulation.In this context, the prompt and the response are represented as x ∈ X and y ∈ Y, respectively.Here, X refers to the set of prompts, while Y represents the set of responses.The LLM corresponds to the policy π in this bandit framework, where π(y | x) indicates the probability of generating the response y given the prompt x.</p>
<p>MDP Formulation of LLMs.Following the notations in Zhong et al. (2024), we consider an MDP M = (S, A, P, r, ρ, H).In this framework, S and A represent the state and action spaces, respectively.The transition kernel is denoted by P : S × A → ∆(S), while r indicates the reward function.The initial distribution is defined by ρ ∈ ∆(S), and H specifies the horizon length.A policy π : S → ∆(A) maps a state to a distribution over the action space.Initially, an initial state is sampled using s 1 ∼ ρ.At the h-th step, the agent receives the state s h and chooses the action a h ∼ π(• | s h ).This interaction continues until a specified ending condition is met, which will occur within H steps.</p>
<p>In the context of generating large language models (LLMs), let s 1 ∼ ρ represent the prompt x ∼ ρ.At each step h, the state s h = (s 1 , a 1:h−1 ) consists of the prompt x and all tokens generated up to that point.The LLM acts as a policy π that maps s h to a distribution over the action a h ∼ π(• | s h ), where the action signifies a token (or a series of consecutive tokens).The transition process is deterministic; it simply concatenates s h = (s 1 , a 1:h−1 ) and a h to create a new state s h+1 = (s 1 , a 1:h ).The generation process concludes with a special end-of-sentence token EoS, which will be generated within H steps.For simplicity, we consider the length-H trajectories {(s h , a h )} H h=1 , noting that this does not lose generality since we can pad the EoS token to the text to reach length H.With this notation and recognizing the autoregressive nature of LLMs, for any realizable trajectory {(s h , a h )} H h=1 , the generation probability is given by π(a
1:H | s 1 ) = H h=1 π(a h | s 1 , a 1:h−1 ).
Regularized Value Functions.For a policy π, its entropy-regularized value function is defined as
V π (s; r) = E π H h=1 r(s h , a h ) − β • log π(a h | s h ) s 1 = s , (2.1)
where β &gt; 0 is a regularization parameter.The regularized Q-function Q π of a policy π is related to the regularized value function V π as
Q π (s, a; r) = r(s, a) + E s ′ ∼P(• | s,a) [V π (s ′ ; r)], V π (s; r) = E a∼π(• | s) [−β log π(a | s) + Q π (s, a; r)], (2.2)
The regularized optimal policy π * is the policy that maximizes the regularized value function defined in (2.1), and its corresponding optimal Q-function and value function are denoted as Q * and V * , respectively.By (2.2), it can be shown that
V * (s; r) = log a∈A exp Q * (s, a; r) , π * (a | s) = exp{(Q * (s, a; r) − V * (s; r))/β} ∝ exp Q * (s, a; r) . (2.3)
3 Unified Framework and Generic Algorithm</p>
<p>In this section, we present a new framework for LLM reasoning and our generic algorithm within this framework.</p>
<p>LLM as A Probabilistic Graphical Model</p>
<p>We consider four different spaces: X represents the prompt space, Z denotes the latent space that captures the intrinsic thought process (CoT), Y signifies the response space, and O stands for the evaluation signal space, which reflects the optimality of the prompt-latent-response tuple.Furthermore, for any (x, z, y, o) ∈ X × Z × Y × O, we describe the generation process using the probabilistic graphical model illustrated in Figure 1.This indicates that
P(z, y, o | x, θ) = P(z, y | x, θ) • P(o | x, z, y) = P(z | x, θ) • P(y | x, z, θ) • P(o | x, z, y), (3.1)
where θ is the parameter of the LLM that guides the generation process.First, a latent variable z is generated from the distribution P(• | x, θ), and then the response y ∼ P(• | x, z, θ) is produced based on both the prompt x and the latent variable z.Importantly, the probability P(o | x, z, y) is independent of the LLM parameterized by θ, as we assume there exists a ground-truth judgment for the triplet (x, z, y), such as a ground-truth/human reward function.Unlike traditional LLM frameworks that only consider the prompt space X and output space Y, our framework incorporates both a latent thinking process space and an observation space.These additional components are crucial for mathematically understanding how to improve the quality of thinking processes using evaluation signals.</p>
<p>Under this probabilistic graphical modeling of LLMs, our learning objective is to maximize
L(θ) = log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ),(3.2)
where Z ⊆ Z, Y ⊆ Y, and O ⊆ O denote the subsets of spaces representing the latent thinking process, response, and evaluation signals, respectively.In Section 3.2, we develop a general optimization algorithm that works with any choice of these spaces (Z , Y , O).Subsequently, in Section 3.3, we show how this framework unifies existing learning approaches by demonstrating how different choices of these spaces (Z , Y , O) correspond to various established learning paradigms and algorithms.</p>
<p>Bootstrapping Reinforced Thinking Process</p>
<p>We propose the algorithm, Bootstrapping Reinforced Thinking Process (BRiTE), to maximize the objective (3.2) within the framework proposed in the previous subsection.Since this objective may be difficult to optimize directly, we rewrite it as
L(θ) = log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ) = log (z,y,o)∈Z ×Y ×O P(z, y, o | x, θ) = max Q(•,•,•|x,ψ) ∈∆(Z ×Y ×O) (z,y,o) ∈Z ×Y ×O log P(z, y, o | x, θ) • Q(z, y, o | x, ψ) − (z,y,o) ∈Z ×Y ×O log Q(z, y, o | x, ψ) • Q(z, y, o | x, ψ) L ψ (θ) , (3.3) where Q(•, •, • | x, ψ
) can be regarded as another LM parametrized by ψ and the last equality follows the following lemma: Lemma 3.1.For any set W and non-negative numbers {P w ≥ 0} w∈W , it holds
log w∈W P w = max q∈∆(W) E w∼q(•) [log P w − log q(w)].
The maximum is achieved when q(w) = P w /( w ′ ∈W P w ′ ).</p>
<p>Proof of Lemma 3.1.This lemma is equivalent to the non-negativity of the KL divergence.For a detailed proof, please refer to Appendix B.1.</p>
<p>We have transformed the problem of maximizing L(θ) into maximizing its lower bound L ψ .This shares the same spirit with the Expectation-Maximization (EM) algorithm (Dempster et al., 1977) and Variational Autoencoders (VAE) (Kingma, 2013).To solve this optimization problem, we iteratively update the parameters ψ and θ to maximize (3.3).Given ψ t and θ t , the updating rules of ψ t+1 and θ t+1 are specified as
• choosing ψ t+1 such that Q(z, y, o | x, ψ t+1 ) = argmax Q(•,•,• | x,ψ) L ψ (θ t ) = P(z, y, o | x, θ t ) (z,y,o)∈Z ×Y ×O P(z, y, o | x, θ t ) ∝ P(z, y, o | x, θ t ). (3.4)
This is implied by the optimality condition in Lemma 3.1.</p>
<p>• choosing θ t+1 such that
θ t+1 = argmax θ L ψt+1 (θ) = argmax θ (z,y,o)∈Z ×Y ×O log P(z, y, o | x, θ) • Q(z, y, o | x, ψ t+1 ) = argmax θ (z,y,o)∈Z ×Y ×O log P(z, y | x, θ) + log P(o | x, z, y) • Q(z, y, o | x, ψ t+1 ) = argmax θ (z,y,o)∈Z ×Y ×O log P(z, y | x, θ) • Q(z, y, o | x, ψ t+1 ) ,(3.5)
where the second equality is implied by (3.1).</p>
<p>Intuitively, during the ψ-updating step in (3.4), the proposed algorithm learns to generate high-quality thinking processes by focusing on verified correct responses.Specifically, when we define O = {O * }, where O * represents the correct evaluation signal in mathematical tasks, P(z, y, O * | x) represents the probability distribution of generating both a high-quality thinking process z and correct final answer y.Following this, in the following θ-updating step in (3.5), the algorithm fine-tunes the Large Language Model by maximizing the joint distribution between the learned thinking process generator and the LLM's output with respect to parameter θ.This fine-tuning approach encompasses several learning paradigms and algorithms, including SFT, RLHF, and rejection sampling-based algorithms, as detailed in Examples 3.4, 3.5, and 3.6.The twostage BRiTE algorithm draws inspiration from the classical EM algorithm.While the probabilistic graphical model used in BRiTE differs from traditional latent variable models, the ψ-updating and θ-updating steps correspond to the E-step and M-step in the EM algorithm, respectively.</p>
<p>Before presenting the theoretical results for BRiTE, we make the following assumption about the generation probability of parametrized LLMs.Assumption 3.2.Assume that LLM is parameterized by θ, we denote that the logits of generating (z, y) conditioned on x by f θ (x, z, y), then the probability of P(z, y | x, θ) takes the form
P(z, y | x, θ) = exp f θ (x, z, y) − A(x, θ) ∝ exp f θ (x, z, y) ,
where A θ (x, θ) is the normalization factor.Moreover, we assume that f θ ∈ H for some reproducing kernel Hilbert space (RKHS)1 associated with the kernel K :
(X × Z × Y) × (X × Z × Y) → R.
The f θ (x, z, y) in Assumption 3.2 represents the logits for predicting (y, z) based on the prompt x, thereby capturing the current generation method of modern transformer-based LLMs.With this assumption, we establish the convergence result of our algorithm in the following theorem.</p>
<p>Theorem 3.3.Suppose Assumption 3.2 holds.Given that L is concave and θ * = argmax θ L(θ), we have
min 1≤t≤T {log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ * ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t )} ≤ KL P(•, • | x, θ 1 )∥P(•, • | x, θ * ) T ,
Proof of Theorem 3.3.See Appendix C.1 for a detailed proof.</p>
<p>Theorem 3.3 establishes a convergence rate of 1/T , where T represents the number of iteration steps.In our analysis, we link our algorithm to classical mirror descent algorithms (Nemirovskij and Yudin, 1983;Bubeck et al., 2015), which have also been utilized in recent studies of policy optimization algorithms (Agarwal et al., 2021;Cai et al., 2020;Zhong and Zhang, 2024).Additionally, our results rely on a concave assumption.This assumption is crucial for proving global convergence results, and we can still obtain a weaker guarantee for stationary points even without this concave condition; see Appendix C.2 for details.</p>
<p>Having established a theoretical guarantee for our proposed generic algorithm, we will demonstrate how our algorithm can incorporate many existing learning algorithms, highlighting the broad applicability of our algorithmic framework and its theoretical guarantee.</p>
<p>Connections to Existing Learning Paradigms and Algorithms</p>
<p>Example 3.4 (Pre-training, SFT and Conditional SFT).We first set aside the latent space and observation space, meaning that Z = O = ∅.In this case, our learning objective (3.2) encompasses two key processes: (I) pre-training, where we let x represent the prompt and Y the next token; and (II) supervised fine-tuning (SFT), where we define Y = {y * (x)} as the expert response corresponding to the prompt x.Moreover, for the prompt-response pair (x, y) and singleton space Y = {y}, we consider Z = R + and Z = {R(x, y)} be the reward corresponding to the prompt-response pair, then our objective recovers the conditional SFT (Lu et al., 2022;Dong et al., 2023b;Yang et al., 2024).</p>
<p>Example 3.5 (RLHF: PPO and DPO).We choose Y = Y and Z = Z as the complete response space and latent space, respectively.Additionally, we set O = {0, 1}, where 1 indicates optimality and 0 indicates nonoptimality, respectively.Since our goal is to maximize the probability of observing the signal of optimality, we focus on O = {1}.We also assume that P(o = 1 | x, z, y) = exp(R(x, z, y)/β) for some reward function R and β &gt; 0. With these choices, we have
L ψ (θ) = (z,y)∈Z×Y log P(z, y, 1 | x, θ) • Q(z, y, 1 | x, ψ) − (z,y)∈Z×Y log Q(z, y, 1 | x, ψ) • Q(z, y, 1 | x, ψ) = E (z,y)∼Q(•,• | x,ψ) R(x, z, y) − β log Q(z, y | x, ψ) P(z, y | x, θ)
,
where Q(z, y | x, ψ) = o∈O Q(z, y, o | x, ψ) = Q(z, y, 1 | x, ψ
).This expression recovers the proximal policy optimization (PPO; Schulman et al., 2017) for RLHF (Christiano et al., 2017;Ouyang et al., 2022), where ψ represents the LLM being optimized, and θ stands for the reference policy.Assuming that the preference data {(x, z + , y + , z − , y − )} is drawn from the Bradley-Terry (BT) model (Bradley and Terry, 1952), with (z + , y + ) denoting preferred data and (z − , y − ) indicating dispreferred data, one can follow Rafailov et al. (2024) to derive the latent direct preference optimization (latent DPO) objective:
L latent−DPO = σ β log Q(z + , y + | x, ψ) P(z + , y + | x, θ) − β log Q(z − , y − | x, ψ) P(z − , y − | x, θ) , (3.6)
where σ is the sigmoid function.In contrast to the standard DPO objective in Rafailov et al. (2024), the objective in (3.6) additionally incorporates the latent variable z.</p>
<p>Example 3.6 (Rejection Sampling EM Methods).Let Y = Y represent the complete response space, and define O = {0, 1}, where 1 indicates the optimal outcome and 0 indicates otherwise.We focus on the optimal signal, denoted as O = {1}.Additionally, we denote Z = Z as the complete thinking process space.The updating rule in (3.4) is given by
Q(z, y, 1 | x, ψ t+1 ) ∝ P(z, y, 1 | x, θ t ) = P(z, y | x, θ t ) • P(o = 1 | x, z, y).
Consequently, the update in (3.5) can be expressed as
θ t+1 = argmax θ (z,y)∈Z×Y log P(z, y | x, θ) • Q(z, y, 1 | x, θ t ) = argmax θ E (z,y)∼P(•,• | x,θt) log P(z, y | x, θ) • P(1 | x, z, y) .
(3.7)</p>
<p>1.If we simplify the scenario by omitting the latent space (i.e., Z = ∅) and assume that
P(o = 1 | x, z, y) =
1{y is the correct answer}, this leads to the STaR algorithm (Zelikman et al., 2022) or rejection sampling fine-tuning (Dong et al., 2023a;Yuan et al., 2023Yuan et al., , 2024)), which performs supervised fine-tuning after rejection sampling based on verifier results.</p>
<ol>
<li>If we simplify the scenario by omitting the latent space (i.e., Z = ∅) and assuming P(o = 1 | x, y) = exp(R(x, y)/β) for some true reward function R, the updating rule in (3.7) simplifies to
θ t+1 = argmax θ E y∼P(• | x,θt) log P(y | x, θ) • exp R(x, y)/β . (3.8)
This recovers the ReST EM algorithm presented in (Singh et al., 2023).</li>
</ol>
<p>Finally, we note that Neal and Hinton (1998) first introduced the unified view of rejection sampling EM, which Rush and Ritter (2024) later expanded upon.</p>
<p>By combining Theorem 3.3 with the examples in this section, we provide theoretical guarantees for these learning algorithms.As a result, we establish theoretical foundations for two approaches: (1) PPO, connecting to the work of (Schulman et al., 2017;Cai et al., 2020;Zhong and Zhang, 2024), and (2) Generalized Rest EM in Example 3.6 (which includes vanilla Rest EM (Singh et al., 2023) as a special case), presenting the first theoretical analysis of its kind.</p>
<p>Practical Implementation: The Power of Reinforcement Learning</p>
<p>We have demonstrated that our generic algorithm encompasses a wide range of existing learning paradigms and algorithms, with a provable convergence guarantee.In this section, we carefully examine the practicality of our algorithm.</p>
<p>First, the θ-updating step is relatively straightforward to implement, as this step simply maximizes the predicted probability of the next tokens after we sample (z, y, o) from Q(z, y, o | x, ψ t+1 ).In contrast, the ψ-updating step can be challenging in certain contexts.For instance, when Y = {y} represents the response corresponding to x, O = {1} indicates the optimality signal of interest, and Z = Z, we have Q(z, y, 1 | x, θ) ∝ P(z, y, 1 | x, θ) = P(z | x, y, 1, θ)-the posterior of the latent variable z.Intuitively, obtaining this distribution requires us to identify the ideal latent (CoT) based on the pair (x, y), which represents an intractable posterior.</p>
<p>To achieve this goal, we aim to use RL to train an LLM that characterizes the distribution Q(z, y, o | x, ψ t+1 ) or P(z, y, o | x, θ t ) in (3.4).Since an LLM acts as the policy of an MDP, our approach involves two main steps: (i) constructing an MDP whose optimal policy matches the distribution Q that we need to learn; and (ii) applying RL algorithms to solve this MDP and identify the optimal policy to learn the desired distribution.These two steps convert the challenging sampling problem to a more amenable RL optimization problem.Notably, the main challenge we face is reward shaping, which involves designing appropriate reward functions to ensure that the optimal policy aligns with the intended LLM that accurately represents the posterior.Our approach to reward shaping is based on the following proposition, which characterizes the optimal policy for deterministic entropy-regularized MDPs.</p>
<p>Proposition 3.7.Assuming the transition dynamic of entropy-regularized MDP is deterministic, then for any trajectories {(s i , a i )} H i=h satisfying a H = EOS, we have
π * (a h ∪ {(s i , a i )} H i=h+1 | s h ) = H i=h π * (a i | s i ) ∝ exp 1 β H i=h r(s i , a i ) .
Proof of Proposition 3.7.See Appendix B.2 for a detailed proof.</p>
<p>By Proposition 3.7, if we select β = 1 and use the total reward as log P(z, y, o | x, θ t ), the resulting optimal policy recovers Q defined in (3.4).This choice of total reward function can naturally be assigned to each token as the token-reward function, due to the autoregressive nature of LLM generation.Specifically, when (z, y, o) is represented by a token sequence a 1:τ , the total reward log P(z, y, o | x, θ t ) can be expressed as τ j=1 log P(a j | a 1:j−1 , x), where log P(a j | a 1:j−1 , x) serves as the reward for the j-th token in RL training.Finally, we remark that the application of RL to text generation has been explored in previous studies, such as Guo et al. (2021).However, there appears to be no work applying RL to generate the thinking process to enhance reasoning capabilities within the context of LLMs.Additionally, existing studies do not address the issue of reward shaping, which is a crucial problem in our scenario and is tackled by Proposition 3.7.</p>
<p>Experiments</p>
<p>In this section, we systematically demonstrate that our unified algorithm enhances the reasoning capability of LLMs.</p>
<p>Experimental Setups</p>
<p>Tasks and Datasets.To evaluate mathematical reasoning capabilities, we conduct experiments on two prominent benchmarks: GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021).GSM8K contains 1,319 high-quality grade school math problems requiring multi-step reasoning, ranging from basic arithmetic to elementary algebra.The MATH dataset comprises 5,000 competition-level problems covering advanced topics such as algebra, geometry, number theory, probability, and calculus.These problems require more sophisticated problem-solving strategies and formal mathematical reasoning.The training sets of GSM8K and MATH datasets each contain approximately 7,500 data points.Each data point includes a question x, along with human-annotated rationale z * and the correct final answer y * .</p>
<p>Base Models.We use several open-source instruction-tuned LLMs as base models, including Gemma-2-9b-it (Team et al., 2024b), Gemma-1.1-7B-it(Team et al., 2024a), Mixtral-7B-Instruct-v0.2 (Jiang et al., 2023) and Llama-3-8B-Instruct (Touvron et al., 2023).</p>
<p>Baselines.Our first baseline is rejection sampling (RS) methods (Neal and Hinton, 1998;Dong et al., 2023a;Yuan et al., 2023;Zelikman et al., 2022).For each problem x, we generate N = 30 candidate rationales and select one (z rs ) containing the correct answer y * .The model is fine-tuned on these problemrationale-answer tuples {(x, z rs , y * )}.Importantly, this method does not use human-annotated rationales z * , making it directly comparable to our approach.We also test the performance of supervised fine-tuning (SFT) on datasets with human-annotated rationales {(x, z * , y * )}.As a baseline learning from preference data, we implement the iterative DPO (Xiong et al., 2024;Pang et al., 2024), initialized with instructiontuned models.The training process consists of 3 iterations.In each iteration, we: (1) use the current model to generate 30 CoT and responses {(z, y)} per prompt; (2) select the best and worst response pairs (z + , y + , z − , y − ) based on the correctness of the y; and (3) apply DPO training (see Rafailov et al. (2024) or (3.6)) on these preference tuples {(x, z + , y + , z − , y − )}.</p>
<p>Implementations of BRiTE.</p>
<p>To systematically demonstrate the effectiveness of our algorithm, we implemented BRiTE in three distinct configurations:</p>
<ol>
<li>
<p>We implement the ψ-updating step of BRiTE to obtain Q using (3.4), examining how the generated thinking process aligns with the desired reasoning path to enhance LLM reasoning capabilities.Following our theoretical framework in Section 3.4, we optimize two types of reward functions in the entropy-regularized MDP:  et al., 2017) or GRPO (Shao et al., 2024) to optimize this MDP.2. BRiTE first obtains Q in (3.4) through RL, then updates θ in (3.5) using SFT.The SFT data consists of three components: problems x, thinking processes z Q (x) generated by Q, along with the ground truth answers y * .</p>
</li>
<li>
<p>We implement BRiTE-DPO, which consists of 3 iterations.During the t-th iteration, we first learn the distribution Q according to (3.4).Then for each prompt x, we leverage Q to generate N = 30 reasoning process and output pairs {(z Q , y Q )}.Subsequently, we evaluate the correctness of each y Q to identify the best and worst latent reasoning processes and construct response pairs (z</p>
</li>
<li>Q , y + Q , z − Q , y − Q ) for use in DPO training (3.6).</li>
</ol>
<p>Experimental Result and Analysis</p>
<ol>
<li>BRiTE Significantly Improves Existing Rejection Sampling Fine-Tuning Algorithms.We begin by comparing BRiTE-SFT with rejection sampling EM-type algorithms, both of which aim to enhance the reasoning capabilities of LLMs by bootstrapping the thinking process.The key distinction is that BRiTE-SFT uses RL for this bootstrapping, while rejection sampling methods rely on sampling techniques.From Table 1, we observe that BRiTE-SFT consistently outperforms rejection sampling-based algorithms across all models, achieving a concrete accuracy improvement of 1-10 points.These improvements are primarily attributed to BRiTE-SFT's ability to generate higher-quality thinking processes compared to rejection sampling, highlighting the potential of RL-driven approaches to advance LLM reasoning through more effective bootstrapping of the thinking process.</li>
</ol>
<p>BRiTE Matches or Even</p>
<p>Enhances the Performance of SFT with Human-Annotated Thinking Process.To further validate the effectiveness of BRiTE's RL-based bootstrapping mechanism, we compare its performance against SFT using human-annotated thinking process data.Human annotations are widely regarded as the gold standard for training LLMs, as they include explicitly crafted reasoning pathways to ensure high-quality outputs.However, as shown in Table 1, BRiTE achieves performance on par with, and in some cases surpasses, that of human-annotated reasoning-based fine-tuning in downstream tasks.This highlights a remarkable outcome: RL-generated thinking processes can achieve a quality comparable to or even superior to human-derived reasoning.This somewhat surprising result underscores the value of BRiTE as a cost-effective alternative to labor-intensive and time-consuming human annotation processes.By reducing reliance on manual annotation, BRiTE sheds light on mitigating the bottleneck associated with creating high-quality datasets, particularly for complex reasoning tasks where human annotations can be prohibitively expensive or inconsistent.</p>
<ol>
<li>BRiTE Further Enhances the Reasoning Capacity in RLHF Stage.In addition to advancing reasoning algorithms using question-(rational)-answer data, BRiTE demonstrates the potential to enhance RLHF algorithms that rely on preference data.As shown by the experimental results in Figure 3, BRiTE consistently outperforms iterative DPO across multiple benchmarks, highlighting its effectiveness in the RLHF stage.This superior performance is attributed to BRiTE's ability to facilitate more structured and contextually nuanced reasoning processes, which, in turn, produce higher-quality preference data and enable more robust policy refinement.These findings not only underscore the versatility of BRiTE but also affirm its value in optimizing RLHF-based fine-tuning pipelines, further cementing its role as a generic algorithm for enhancing LLM reasoning in the post-training stage.Figure 2: Results of BRiTE on coding generation task using the deepseek-coder-6.7b-instructmodel.
Mistral-7B- Instruct-v0.2 Gemma-1.1- 7B-it Gemma-2- 9B-it Llama-3-8B- Instruct</li>
</ol>
<p>BRiTE Can Also Improve Code</p>
<p>Generation Ability.Finally, we extend the evaluation of BRiTE beyond mathematics tasks to assess its performance on code generation.The results, presented in Table 2, demonstrate a consistent trend observed in mathematics tasks: BRiTE outperforms rejection sampling-based algorithms and even surpasses SFT using human-annotated answers.This highlights BRiTE's versatility and effectiveness across diverse problem domains, showcasing its ability to generalize to a wide range of reasoning tasks.We also remark that rejection sampling-based algorithms require the code datasets to be equipped with correct unit tests, which are unnecessary for BRiTE.For a detailed description of the experimental setup and configuration, refer to Appendix D.2.</p>
<p>Enhanced Math Reasoning via Expanded Dataset and Advanced Base Model</p>
<p>In the previous subsection, BRiTE's performance did not demonstrate significant advantages over other algorithms like rejection sampling.This was primarily due to two factors: the instruct models had already undergone post-training, and the dataset size was limited.To overcome these constraints, we have implemented a larger training dataset based on the more advanced Qwen base model.Specifically, we perform BRiTE on the model Qwen2.5-7B(Team, 2024) with a mixed dataset (the size is around 40K) of RUC-AIBOX/STILL-3-Preview-RL-Data2 , MATH, and historical AIME problems (excluding AIME 2024).We evaluated models trained using the RS baseline, the ψ-update and θ-update of BRiTE on a diverse set of reasoning benchmarks, including six challenging math and science reasoning benchmarks: Math-500, Minerva Math, Olympiad Bench, AIME24, AMC23, and GPQA Diamond.Here MATH500 contains 500 competition-level math problems drawn from the MATH dataset (Hendrycks et al., 2021), spanning subjects from algebra to precalculus and accompanied by solutions .Minerva Math comprises 272 undergraduate-level quantitative problems in physics, chemistry, biology, economics, and mathematics, designed to test advanced multi-step reasoning (Lewkowycz et al., 2022).OlympiadBench (He et al., 2024) is a collection of 8,952 Olympiad-caliber problems in math and physics (with 57% including diagrams) provided in both English and Chinese, each with a detailed solution .Drawing from its extensive entries, we selected 674 open-ended, competition-style, text-only problems from OlympiadBench.AMC23 (40 problems) and AIME24 (30 problems) are benchmarks based on recent AMC 12 (2023) and AIME (2024) contests (Mathematical Association of America, 2023, 2024), representing high school-level competition questions of moderate and high difficulty, respectively .Finally, GPQA Diamond (Rein et al., 2024) consists of 198 graduate-level scientific questions verified by experts.Each of these datasets challenges models with a different profile of difficulty and domain coverage, collectively evaluating a broad spectrum of mathematical reasoning capabilities.We evaluate the  pass@1 accuracy (64 sampling times for AIME24 and AMC23 and 8 sampling times for the others) for each benchmark.</p>
<p>As shown in Table 2, BRiTE with an external verifier can improve reject sampling (RS) significantly.Specifically, on MATH500, Minerva Math and AMC23, BRiTE improve upon RS by over 15 points in accuracy, showcasing the strong impact of RL-bootstrapped rationales.Moreover, we also show that the iterative training version of BRiTE leads to some improvements, though the training in iteration 1 reaches a plateau that may limit further gains.Figure 4 (left) shows that BRiTE exhibits faster and more stable training loss reduction compared to rejection sampling, indicating more effective use of feedback during optimization.Figure 4 (right) further confirms this trend across benchmarks: BRiTE consistently achieves higher mean accuracy throughout training.These results indicate that BRiTE not only improves final performance, but also accelerates the acquisition of reasoning capabilities during training.</p>
<p>Conclusion</p>
<p>In this work, we explore methods for enhancing language model reasoning through the automated generation of high-quality thinking processes.We present a unified probabilistic framework that characterizes both the reasoning process and evaluation signals.Within this framework, we develop the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which advances automated reasoning generation through reinforcement learning during inference and incorporates improved reasoning processes into post-training phases.Furthermore, we demonstrate that BRiTE possesses a provable convergence property and unifies various existing learning paradigms and algorithms.Extensive empirical results on mathematics tasks show that our approach surpasses traditional chain-of-thought prompting while enhancing existing supervised/rejection sampling fine-tuning and reinforcement learning from human feedback methods.Our work opens several promising research directions, including the application of our framework to broader reasoning tasks and the investigation of its potential for developing more robust and reliable AI systems.</p>
<p>A Author Contributions</p>
<p>This work stems from the valuable contributions and close collaboration of all authors.The first seven authors and ZW are the core contributors to this project, all participating in discussions of problem formulation and theory, as well as experimental implementation.In particular, HZ and ZW lead the project, primarily propose the methodology, derive the theoretical results, guide experimental progress, contribute to early baselines, and write the paper.YY, SZ, and XX mainly contribute to the initial implementation of BRiTE without verifier reward for Llama, Gemma, and Mistral models, while YL, YZ, and ZL mainly contribute to BRiTE with verifier reward for larger datasets and Qwen base models.In addition, YL, YZ, and ZL make key contributions to direct preference learning, rejection sampling, and code generation, respectively.Other authors also make significant contributions to this work, providing computational resources and offering suggestions for theoretical analysis, experiment design, and paper writing.</p>
<p>B Missing Proofs in the Main Paper</p>
<p>B.1 Proof of Lemma 3.1</p>
<p>Proof of Lemma 3.1.For any q(•) ∈ ∆(W), we have
E w∼q(•) [log P w − log q(w)] − log w∈W P w = E w∼q(•) log P w w ′ ∈W P w ′ − log q(w) = −KL(q∥ p) ≤ 0,
where p is the distribution defined as p(w) = P w /( w ′ ∈W P w ′ ), and the equality is achieved when q = p.Hence, we have finished the proof of Lemma 3.1.</p>
<p>B.2 Proof of Proposition 3.7</p>
<p>Proof of Proposition 3.7.The first equation follows from the deterministic transition.We will now focus on proving the second propositional relationship.According to equation (2.3), we have:
π * (a i | s i ) = exp{(Q * (s i , a i ) − V * (s i ))/β}, ∀h ≤ i ≤ H, which implies that H i=h β log π * (a i | s i ) = H i=h Q * (s i , a i ) − V * (s i ) = H−1 i=h r(s i , a i ) + V * (s i+1 ) − V * (s i ) + r(s H , a H ) − V * (s H ) = H i=h r(s i , a i ) − V * (s h ), (B.1)
where the second equality uses the fact that a H = EOS.Hence, we have
H i=h π * (a i | s i ) = exp 1 β H i=h r(s i , a i ) exp(V * (s h )/β) ∝ exp 1 β H i=h r(s i , a i ) ,
where the last step is obtained by the fact that s h is a fixed state, independent of a h ∪ {(s i , a i )} H i=h+1 .</p>
<p>C Convergence Results</p>
<p>C.1 Proof of Theorem 3.3</p>
<p>Before starting the proof of Theorem 3.3, we present two technical lemmas.</p>
<p>Combining (C.3) and (C.4), we have
∇ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ) = (z,y,o)∈Z ×Y ×O P(o | x, z, y) • ∇P(z, y | x, θ) (z,y,o)∈Z ×Y ×O P(z, y, o | x, θ) = (z,y)∈Z ×Y o∈O P(z, y, o | x, θ) (z,y,o)∈Z ×Y ×O P(z, y, o | x, θ) • [K((x, z, y), •) − ∇A(x, θ)] = E (z,y)∼Q(•,• | x,θ) [K((x, z, y), •)] − ∇A(x, θ). (C.5)
This finishes the proof of Lemma C.2.Now we start the proof of Theorem 3.3.</p>
<p>Proof of Theorem 3.3.By the updating rule of θ t+1 in (3.5), we have
θ t+1 = argmax θ (z,y,o)∈Z ×Y log P(z, y | x, θ) • Q(z, y, o | x, θ t ) = argmax θ (z,y)∈Z ×Y log P(z, y | x, θ) • Q(z, y | x, θ t ) ,
where we use the notation
Q(z, y | x, θ t ) = o∈O Q(z, y, o | x, θ t ).</p>
<p>C.2 Additional Theoretical Results</p>
<p>Theorem C.3.Suppose Assumption 3.2 holds.Then we have</p>
<p>Figure 1 :
1
Figure 1: LLM as a probabilistic graphical model.X and Y represent prompt and response, respectively.The latent variable Z indicates the intrinsic thinking process behind generation.Evaluation signal O is influenced by X, Z, and Y .</p>
<p>(i) log(z, y, o | x) = log P(z, y * (x) | x), where y * represents the correct answer for question x.This equality holds because we focus solely on correct questions, using proper choices of O = {answer verified to be correct} and Y = {y * }. (ii) log(z, y, o | x) = log P(z, y | x) + R(x, z, y)/β, where we assume that P(o = 1 | x, z, y) = exp(R(x, z, y)/β) with O = {1}.We employ PPO (Schulman</p>
<p>Figure 3 :
3
Figure 3: Comparison between BRiTE and iterative DPO in the RLHF stage.</p>
<p>Figure 4 :
4
Figure 4: Left: Training dynamics of BRiTE with an external verifier and rejection sampling.Right: Mean accuracy of benchmark scores of models trained by BRiTE with an external verifier and reject sampling during the training process.</p>
<p>Furthermore, we haveθ t+1 = argmax θ (z,y)∈Z ×Y log P(z, y | x, θ) • Q(z, y | x, θ t ) = argmax θ (z,y)∈Z ×Y log P(z, y | x, θ) P(z, y | x, θ t ) • Q(z, y | x, θ t ) = argmax θ E (z,y)∼Q(•,• | x,θt) [K((x, z, y), •)], f θ − f θt + A(x, θ t ) − A(x, θ) , (C.6)where the last equality is implied bylog P(z, y | x, θ) P(z, y | x, θ t ) = f θ (x, z, y) − f θt (x, z, y) + A(x, θ t ) − A(x, θ) = ⟨K((x, z, y), •), f θ − f θt ⟩ + A(x, θ t ) − A(x, θ).Combining (C.6) and Lemma C.2, we haveθ t+1 = argmax θ ⟨∇ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) + ∇A(x, θ t ), f θ − f θt ⟩ + A(x, θ t ) − A(x, θ) .(C.7)By the optimality condition, this implies that∇A(θ t+1 ) − ∇A(θ t ) = ∇ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ).(C.8)We have∇A(x, θ t+1 ) − ∇A(x, θ t ), f θ * − f θt+1 = ⟨∇A(x, θ t ), f θt+1 − f θt ⟩ − A(x, θ t+1 ) + A(x, θ t ) − ⟨∇A(x, θ t ), f θ * − f θt ⟩ + A(x, θ * ) − A(x, θ t ) + ⟨∇A(x, θ t+1 ), f θ * − f θt+1 ⟩ − A(x, θ * ) + A(x, θ t+1 ) = −KL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) + KL P(•, • | x, θ t )∥P(•, • | x, θ * ) − KL P(•, • | x, θ t+1 )∥P(•, • | x, θ * ) ,where the last equality follows from Lemma C.1.This is equivalent toKL P(•, • | x, θ t )∥P(•, • | x, θ * ) − KL P(•, • | x, θ t+1 )∥P(•, • | x, θ * ) = ⟨∇A(θ t+1 ) − ∇A(θ t ), f θ * − f θt+1 ⟩ + KL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) = ⟨∇ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ), f θ * − f θt+1 ⟩ (⋆) +KL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) , (C.9)where the third line is implied by Lemma C.1, and the last equality follows from (C.5).Plugging (C.16) into (C.15),we haveTerm (II) in (C.10) = ∇ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ), f θt − f θt+1 ≥ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ) − KL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) .(C.17)Combining (C.10), (C.11), and (C.17), we have (⋆) ≥ Term (I) + Term (II)≥ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ * ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) + log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ) − KL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) = log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ * ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ) − KL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) .(C.18)Putting (C.9) and (C.18) together, we obtainKL P(•, • | x, θ t )∥P(•, • | x, θ * ) − KL P(•, • | x, θ t+1 )∥P(•, • | x, θ * ) ≥ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ * ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ).Telescoping this inequality from 0 to T − 1, we obtain that which implies that min 1≤t≤T {log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ * ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t )} ≤ KL P(•, • | x, θ 1 )∥P(•, • | x, θ * ) T , which finishes the proof of Theorem 3.3.</p>
<p>•, • | x, θ t+1 )∥P(•, • | x, θ t ) ≤ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ T +1 ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ 1 )T .Proof of Theorem C.3.By the same derivation of (C.17), we haveKL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) ≥ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ) − ∇ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ), f θt − f θt+1 = log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ) − ∇A(θ t+1 ) − ∇A(θ t ), f θt − f θt+1 , (C.19)where the last equality is implied by the optimality condition in (C.8).By Lemma C.1, we haveKL P(•, • | x, θ t )∥P(•, • | x, θ t+1 ) = A(x, θ t+1 ) − A(x, θ t ) + ⟨∇A(x, θ t ), f θt − f θt+1 ⟩. (C.20) Combining (C.19) and (C.20), we haveA(x, θ t+1 ) − A(x, θ t ) + ∇A(θ t+1 ), f θt − f θt+1 ≥ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ).Together with Lemma C.1, we havelog P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t+1 ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ t ) ≥ A(x, θ t ) − A(x, θ t+1 ) + ∇A(θ t+1 ), f θt+1 − f θt = KL P(•, • | x, θ t+1 )∥P(•, • | x, θ t ) .Telescoping this inequality across t ∈ [T ], we have mint∈[T ] KL P(•, • | x, θ t+1 )∥P(•, • | x, θ t ) ≤ T t=1 KL P(•, • | x, θ t+1 )∥P(•, • | x, θ t ) T ≤ log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ T +1 ) − log P(z ∈ Z , y ∈ Y , o ∈ O | x, θ 1 ) T ,which finishes the proof of Theorem C.3.</p>
<p>Table 1 :
1
A comparison of the performance of three algorithms: BRiTE, rejection sampling (RS) type algorithms, and SFT using human-annotated data.
AlgorithmMistral-7B-Instruct-v0.2 Gemma-1.1-7B-itGemma-2-9B-itLlama-3-8B-InstructGSM8KMATHGSM8K MATH GSM8K MATH GSM8KMATH-41.89.849.018.881.337.379.228.3SFT52.813.657.519.680.141.572.627.1RS47.710.358.418.787.647.579.528.9BRiTE52.211.259.223.789.750.581.030.0</p>
<p>Table 2 :
2
Performance comparison across different reasoning benchmarks.
MethodMATH500 Minerva Math OlympiadBench AIME24 AMC23 GPQA Diamond-44.112.916.10.910.125.9RS54.321.023.15.631.626.9BRiTE (ψ-update)79.135.035.714.357.728.5BRiTE (θ-update)76.940.637.014.457.129.8BRiTE-iter-2 (ψ-update)80.641.337.314.357.929.9BRiTE-iter-2 (θ-update)78.239.837.915.356.430.1Accuracy over Epochs4035Accuracy30Reject Sampling BRiTE ( -update) BRiTE ( -update) Base2520BaseEpoch1Epoch2 Training EpochEpoch3Epoch4
We say H is a RKHS on the set W with the reproducing kernelK : W × W → R if the inner product ⟨•, •⟩ satisfies f (w) = ⟨K(w, •), f ⟩, for any (f, w) ∈ H × W.
https://huggingface.co/datasets/RUC-AIBOX/STILL-3-Preview-RL-Data
https://huggingface.co/RLHF4MATH
https://github.com/QwenLM/Qwen2.5-Math
https://huggingface.co/datasets/OpenCoder-LLM/opc-sft-stage2/tree/main
https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct
AcknowledgmentsWe would like to thank Wei Xiong and Hang Li for valuable discussions regarding experiments and related works.Lemma C.1.For any (θ, θ ′ ) and fixed x ∈ X , we havewhere ∇ is a functional gradient.Combining (C.1) and (C.2), we havewhich finishes the proof of Lemma C.1.Lemma C.2.It holds that (z,y,o | x,θ) .Proof.By definition, we havewhere the last equality follows from (3.1).Meanwhile, by Assumption 3.2, we further havewhere the last equality follows from (C.8).For the Term (⋆), we have.(C.10)Term (I).By the local concave assumption, we haveTerm (II).By Lemma 3.1, we havewherewhere the last equality is implied by the definition of Q(z, y, o | x, θ) in (C.13).Combining (C.12) and (C.14), we havewhere the third equality follows from the non-negativity of KL-divergence, and the last equality is implied by (3.1).By Assumption 3.2, we can rewrite the right-handside of (C.15) asD Implementation Details D.1 Implementation Details for Math TaskImplementation of BRiTE.The BRiTE algorithm is run on 4 NVIDIA H100 during training.We leverage the PPO pipeline(Schulman et al., 2017)to learn the sampling policy Q (3.4) with a learning rate of 5e − 7 and a batch size of 1.For the subsequent SFT on rationales sampled by Q, we set the learning rate to 5e − 5 and the batch size to 2. We adopt the LoRA(Hu et al., 2021)training for both steps, where the r is set to 32 and lora alpha is set to 128.Implementation of RS.For rejection sampling, we set the temperature of the model's generation to 1.0, sample N = 30 candidate rationales for each problem x, and select the best rationales z rs .we filter generations by selecting those that produce the correct final answer.The model is then fine-tuned on these problem-rationale-answer tuples {x, z rs , y * } using the same learning rate and LoRA parameters as in BRiTE.Implementation of SFT.For SFT on original datasets with human-annotated rationales, we use the same LoRA parameters as in BRiTE.The learning rates for Mistral-7B-Instruct-v0.2 and Gemma-1.1-7B-it are set to 5e−5.For Gemma-2-9b-it, the learning rate is 5e−6, and for Llama-3-8B-Instruct, it is 8e−7.These smaller learning rates are chosen to mitigate overfitting and ensure optimal performance.Implementation of RLHF Algorithms.For both BRiTE DPO and iterative DPO, we select 6k data points per iteration from the RLHF4MATH/prompt iter1,2,3 dataset 3 over three iterations.The learning rates for DPO training are configured as follows: Mistral-7B-Instruct-v0.2 uses a learning rate of 2e − 7, Gemma-1.1-7B-itemploys 4e − 7, while Gemma-2-9B-it and Llama-3-8B-Instruct are set to 5e − 7. The generation temperature is set to 1.0, and each prompt sample N = 30 responses.Evaluation.We utilize the evaluation function provided in the Qwen2.5-Mathrepository 4 to assess the models' performance on the test sets of GSM8K and MATH.D.2 Implementation Details for Coding Generation TasksDatasets.For the code generation task, We choose the first 4000 rows of the educational instruct split of the dataset OpenCoder-LLM/opc-sft-stage2 5(Huang et al., 2024)as the training dataset, which comprises (instruction, code, test case) tuples.The entire dataset is generated with an algorithmic corpus as a seed and validated through a Python compiler.Models.We choose the language model deepseek-ai/deepseek-coder-6.7b-instruct 6with around seven billion active parameters as the base model, which is especially pretrained and fined-tuned on the code corpus for better code generation performance.Baselines.Similar to the mathematics tasks, we use Reject Sampling (RS) and SFT as the baselines.We follow a rejection sampling approach similar to prior math section.The model generates candidate rationales with a temperature of 1.0, sample N = 30 rationales for problem x.The best rationales, z rs , is then selected.The model is then fine-tuned on these rationale-answer tuples {x, z rs , y}.Notably, unlike the mathematical setting, filtering correct generations in the code generation task requires candidates to pass all unit tests and receive positive compiler feedback.Benchmarks.We evaluate the trained models on the HumanEval task from Evalplus(Liu et al., 2023)and the instruct split of the BigCodeBench(Zhuo et al., 2024).These popular benchmarks evaluate how the language models complete the partial code snippet and generate a full code snippet according to natural language instructions.Training Details.We use 4 NVIDIA A100 GPUs for all the training.We leverage the PPO pipeline(Schulman et al., 2017)to learn the sampling policy Q (3.4) with a learning rate of 5e − 7 and a batch size of 1.With the rationales sampled from Q of BRiTE, we set the learning rate as 5e − 5 and the batch size as 2 in the SFT stage of BRiTE.For the baselines, we also set the learning rate as 5e − 5 and the batch size as 2 in the SFT algorithm.For the fine-tune stage of reject sampling, we use the same learning rate as in BRiTE.To save the computation budget, we adopt the LoRA(Hu et al., 2021)training for all the optimization phases, where the r is set to 32 and lora alpha is set to 128.Evaluation Details and Results.We report the pass@1 scores (the success rate of the first attempt) with the greedy decoding for the base model, SFT algorithm, Reject Sampling (RS) algorithm, and our proposed algorithm BRiTE-SFT in Table2. Results show that our proposed algorithm BRiTE can help LLMs improve their code generation ability and demonstrate the performance gain of BRiTE compared with the baselines.We also remark that rejection sampling-based algorithms require the code datasets to be equipped with correct unit tests, which are unnecessary for BRiTE.D.3 Implentation Details for the scaling-up ExperimentsImplementation of BRiTE.We train the BRiTE algorithm on eight NVIDIA H100 GPUs using the GRPO pipeline(Shao et al., 2024)in the verl codebase(Sheng et al., 2024), learning the sampling policy Q (3.4) with a learning rate of 1e − 6 and a batch size of 512 × 8 (8 rollouts per prompt).For the subsequent SFT ψ-update on rationales sampled by Q, we set the learning rate to 1e − 6 and bath size to be 8.We also adapt LoRA and set r to be 32 and lora alpha to be 128.Implementation of RS.For rejection sampling, we set the temperature of the model's generation to 1.0, sample N = 2 candidate rationales for each problem x, and select the best rationales z rs .we filter generations by selecting those that produce the correct final answer.The model is then fine-tuned on these problem-rationale-answer tuples {x, z rs , y * } using the same learning rate and LoRA parameters as in BRiTE.
On the theory of policy gradient methods: Optimality, approximation, and distribution shift. A Agarwal, S M Kakade, J D Lee, G Mahajan, Journal of Machine Learning Research. 222021</p>
<p>. Anthropic, 2023Introducing claude</p>
<p>A general theoretical paradigm to understand learning from human preferences. M G Azar, Z D Guo, B Piot, R Munos, M Rowland, M Valko, D Calandriello, International Conference on Artificial Intelligence and Statistics. PMLR2024</p>
<p>Rank analysis of incomplete block designs: I. the method of paired comparisons. R A Bradley, M E Terry, Biometrika. 391952</p>
<p>S Bubeck, Convex optimization: Algorithms and complexity. Foundations and Trends® in Machine Learning. 20158</p>
<p>Provably efficient exploration in policy optimization. Q Cai, Z Yang, C Jin, Z Wang, International Conference on Machine Learning. PMLR2020</p>
<p>Value-incentivized preference optimization: A unified approach to online and offline rlhf. S Cen, J Mei, K Goshvadi, H Dai, T Yang, S Yang, D Schuurmans, Y Chi, B Dai, arXiv:2405.193202024arXiv preprint</p>
<p>Language models are hidden reasoners: Unlocking latent reasoning capabilities via selfrewarding. H Chen, Y Feng, Z Liu, W Yao, A Prabhakar, S Heinecke, R Ho, P Mui, S Savarese, C Xiong, arXiv:2411.042822024arXiv preprint</p>
<p>Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation. X Chen, H Zhong, Z Yang, Z Wang, L Wang, International Conference on Machine Learning. PMLR2022</p>
<p>Deep reinforcement learning from human preferences. P F Christiano, J Leike, T Brown, M Martic, S Legg, D Amodei, Advances in neural information processing systems. 201730</p>
<p>K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Maximum likelihood from incomplete data via the em algorithm. A P Dempster, N M Laird, D B Rubin, Journal of the royal statistical society: series B. 1977</p>
<p>H Dong, W Xiong, D Goyal, Y Zhang, W Chow, R Pan, S Diao, J Zhang, K Shum, T Zhang, arXiv:2304.06767Raft: Reward ranked finetuning for generative foundation model alignment. 2023aarXiv preprint</p>
<p>Y Dong, Z Wang, M N Sreedhar, X Wu, O Kuchaiev, arXiv:2310.05344Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf. 2023barXiv preprint</p>
<p>Reinforced self-training (rest) for language modeling. C Gulcehre, T L Paine, S Srinivasan, K Konyushkova, L Weerts, A Sharma, A Siddhant, A Ahern, M Wang, C Gu, arXiv:2308.089982023arXiv preprint</p>
<p>Efficient (soft) q-learning for text generation with limited good data. H Guo, B Tan, Z Liu, E P Xing, Z Hu, arXiv:2106.077042021arXiv preprint</p>
<p>Olympiadbench: A challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. C He, R Luo, Y Bai, S Hu, Z L Thai, J Shen, J Hu, X Han, Y Huang, Y Zhang, J Liu, L Qi, Z Liu, M Sun, 2024</p>
<p>D Hendrycks, C Burns, S Kadavath, A Arora, S Basart, E Tang, D Song, J Steinhardt, arXiv:2103.03874Measuring mathematical problem solving with the math dataset. 2021arXiv preprint</p>
<p>Training chain-of-thought via latent-variable inference. M D Hoffman, D Phan, D Dohan, S Douglas, T A Le, A Parisi, P Sountsov, C Sutton, S Vikram, R Saurous, Advances in Neural Information Processing Systems. 202436</p>
<p>E J Hu, M Jain, E Elmoznino, Y Kaddar, G Lajoie, Y Bengio, N Malkin, arXiv:2310.04363Amortizing intractable inference in large language models. 2023arXiv preprint</p>
<p>E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.09685Lora: Lowrank adaptation of large language models. 2021arXiv preprint</p>
<p>S Huang, T Cheng, J K Liu, J Hao, L Song, Y Xu, J Yang, J Liu, C Zhang, L Chai, arXiv:2411.04905Opencoder: The open cookbook for top-tier code large language models. 2024arXiv preprint</p>
<p>A Q Jiang, A Sablayrolles, A Mensch, C Bamford, D S Chaplot, D Casas, F Bressand, G Lengyel, G Lample, L Saulnier, arXiv:2310.06825Mistral 7b. 2023arXiv preprint</p>
<p>D P Kingma, arXiv:1312.6114Auto-encoding variational bayes. 2013arXiv preprint</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, Advances in Neural Information Processing Systems. 202235</p>
<p>Enhancing multi-step reasoning abilities of language models through direct q-function optimization. G Liu, K Ji, R Zheng, Z Wu, C Dun, Q Gu, L Yan, arXiv:2410.093022024aarXiv preprint</p>
<p>Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. J Liu, C S Xia, Y Wang, L Zhang, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>Z Liu, M Lu, S Zhang, B Liu, H Guo, Y Yang, J Blanchet, Z Wang, arXiv:2405.16436Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. 2024barXiv preprint</p>
<p>Quark: Controllable text generation with reinforced unlearning. X Lu, S Welleck, J Hessel, L Jiang, L Qin, P West, P Ammanabrolu, Y Choi, Advances in neural information processing systems. 202235</p>
<p>American mathematics competitions amc 12. 2023. 2023Mathematical Association of America</p>
<p>American invitational mathematics examination aime i &amp; ii. 2024. 2024Mathematical Association of America</p>
<p>Y Meng, M Xia, D Chen, arXiv:2405.14734Simpo: Simple preference optimization with a reference-free reward. 2024arXiv preprint</p>
<p>A view of the em algorithm that justifies incremental, sparse, and other variants. R M Neal, G E Hinton, Learning in graphical models. Springer1998</p>
<p>Problem complexity and method efficiency in optimization. A S Nemirovskij, D B Yudin, arXiv:2303.08774.OpenAIGpt-4 technical report. 1983. 2023. 2024arXiv preprintIntroducing openai o1</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in neural information processing systems. 202235</p>
<p>Dueling rl: reinforcement learning with trajectory preferences. A Pacchiano, A Saha, J Lee, arXiv:2111.048502021arXiv preprint</p>
<p>R Y Pang, W Yuan, K Cho, H He, S Sukhbaatar, J Weston, arXiv:2404.19733Iterative reasoning preference optimization. 2024arXiv preprint</p>
<p>Direct preference optimization: Your language model is secretly a reward model. R Rafailov, A Sharma, E Mitchell, C D Manning, S Ermon, C Finn, Advances in Neural Information Processing Systems. 202436</p>
<p>Gpqa: A graduate-level google-proof q&amp;a benchmark. D Rein, B L Hou, A C Stickland, J Petty, R Y Pang, J Dirani, J Michael, S R Bowman, First Conference on Language Modeling. 2024</p>
<p>S Rush, D Ritter, Speculations on test-time scaling. 2024</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Z Shao, P Wang, Q Zhu, R Xu, J Song, X Bi, H Zhang, M Zhang, Y Li, Y Wu, arXiv:2402.03300Deepseekmath: Pushing the limits of mathematical reasoning in open language models. 2024arXiv preprint</p>
<p>G Sheng, C Zhang, Z Ye, X Wu, W Zhang, R Zhang, Y Peng, H Lin, C Wu, arXiv:2409.19256Hybridflow: A flexible and efficient rlhf framework. 2024arXiv preprint</p>
<p>Beyond human data: Scaling self-training for problem-solving with language models. A Singh, J D Co-Reyes, R Agarwal, A Anand, P Patil, X Garcia, P J Liu, J Harrison, J Lee, K Xu, arXiv:2312.065852023arXiv preprint</p>
<p>Scaling llm test-time compute optimally can be more effective than scaling model parameters. C Snell, J Lee, K Xu, A Kumar, arXiv:2408.033142024arXiv preprint</p>
<p>Y Tang, Z D Guo, Z Zheng, D Calandriello, R Munos, M Rowland, P H Richemond, M Valko, B Á Pires, B Piot, arXiv:2402.05749Generalized preference optimization: A unified approach to offline alignment. 2024arXiv preprint</p>
<p>G Team, T Mesnard, C Hardin, R Dadashi, S Bhupatiraju, S Pathak, L Sifre, M Rivière, M S Kale, J Love, arXiv:2403.08295Gemma: Open models based on gemini research and technology. 2024aarXiv preprint</p>
<p>G Team, M Riviere, S Pathak, P G Sessa, C Hardin, S Bhupatiraju, L Hussenot, T Mesnard, B Shahriari, A Ramé, arXiv:2408.00118Gemma 2: Improving open language models at a practical size. 2024barXiv preprint</p>
<p>Q Team, Qwen2.5: A party of foundation models. 2024</p>
<p>H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>H Wang, S Hao, H Dong, S Zhang, Y Bao, Z Yang, Y Wu, arXiv:2412.16145Offline reinforcement learning for llm multi-step reasoning. 2024arXiv preprint</p>
<p>Selfconsistency improves chain of thought reasoning in language models. X Wang, J Wei, D Schuurmans, Q Le, E Chi, S Narang, A Chowdhery, D Zhou, arXiv:2203.111712022arXiv preprint</p>
<p>Chain-ofthought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, F Xia, E Chi, Q V Le, D Zhou, Advances in neural information processing systems. 202235</p>
<p>A survey of preference-based reinforcement learning methods. C Wirth, R Akrour, G Neumann, J Fürnkranz, Journal of Machine Learning Research. 182017</p>
<p>Thinking llms: General instruction following with thought generation. T Wu, J Lan, W Yuan, J Jiao, J Weston, S Sukhbaatar, arXiv:2410.106302024arXiv preprint</p>
<p>Exploratory preference optimization: Harnessing implicit q*-approximation for sample-efficient rlhf. T Xie, D J Foster, A Krishnamurthy, C Rosset, A Awadallah, A Rakhlin, arXiv:2405.210462024arXiv preprint</p>
<p>Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. W Xiong, H Dong, C Ye, Z Wang, H Zhong, H Ji, N Jiang, T Zhang, Forty-first International Conference on Machine Learning. 2024</p>
<p>. R Yang, X Pan, F Luo, S Qiu, H Zhong, D Yu, J Chen, 2024Rewards-in-context</p>
<p>Multi-objective alignment of foundation models with dynamic preference adjustment. arXiv:2402.10207arXiv preprint</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. S Yao, D Yu, J Zhao, I Shafran, T Griffiths, Y Cao, K Narasimhan, Advances in Neural Information Processing Systems. 202436</p>
<p>Rrhf: Rank responses to align language models with human feedback. H Yuan, Z Yuan, C Tan, W Wang, S Huang, F Huang, Advances in Neural Information Processing Systems. 202436</p>
<p>Z Yuan, H Yuan, C Li, G Dong, K Lu, C Tan, C Zhou, J Zhou, arXiv:2308.01825Scaling relationship on learning mathematical reasoning with large language models. 2023arXiv preprint</p>
<p>The k-armed dueling bandits problem. Y Yue, J Broder, R Kleinberg, T Joachims, Journal of Computer and System Sciences. 782012</p>
<p>Quiet-star: Language models can teach themselves to think before speaking. E Zelikman, G Harik, Y Shao, V Jayasiri, N Haber, N D Goodman, arXiv:2403.096292024arXiv preprint</p>
<p>Star: Bootstrapping reasoning with reasoning. E Zelikman, Y Wu, J Mu, N Goodman, Advances in Neural Information Processing Systems. 202235</p>
<p>Self-exploring language models: Active preference elicitation for online alignment. S Zhang, D Yu, H Sharma, H Zhong, Z Liu, Z Yang, S Wang, H Hassan, Z Wang, arXiv:2405.193322024arXiv preprint</p>
<p>Y Zhao, R Joshi, T Liu, M Khalman, M Saleh, P J Liu, arXiv:2305.10425Slic-hf: Sequence likelihood calibration with human feedback. 2023arXiv preprint</p>
<p>H Zhong, G Feng, W Xiong, L Zhao, D He, J Bian, L Wang, arXiv:2404.18922Dpo meets ppo: Reinforced token optimization for rlhf. 2024arXiv preprint</p>
<p>A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes. H Zhong, T Zhang, Advances in Neural Information Processing Systems. 202436</p>
<p>D Zhou, N Schärli, L Hou, J Wei, N Scales, X Wang, D Schuurmans, C Cui, O Bousquet, Q Le, arXiv:2205.10625Least-to-most prompting enables complex reasoning in large language models. 2022arXiv preprint</p>
<p>T Y Zhuo, M C Vu, J Chim, H Hu, W Yu, R Widyasari, I N B Yusuf, H Zhan, J He, I Paul, arXiv:2406.15877Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. 2024arXiv preprint</p>
<p>D M Ziegler, N Stiennon, J Wu, T B Brown, A Radford, D Amodei, P Christiano, G Irving, arXiv:1909.08593Fine-tuning language models from human preferences. 2019arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
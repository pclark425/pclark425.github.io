<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-8989 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-8989</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-8989</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-158.html">extraction-schema-158</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b2075a1ce0e9bea9a6b6c0448c67de067471e885</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b2075a1ce0e9bea9a6b6c0448c67de067471e885" target="_blank">AMR Parsing via Graph-Sequence Iterative Inference</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> A new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy.</p>
                <p><strong>Paper Abstract:</strong> We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e8989.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e8989.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2Seq linearization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sequence-to-sequence linearization of AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Representation that serializes an AMR graph into a token sequence (a linearized string) so that standard sequence-to-sequence models can be trained to map between sentences and AMR linearizations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural AMR: Sequence-to-sequence models for parsing and generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>linearization / serialization</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Serialize the graph (nodes and relations) into a flat token sequence using some graph traversal or custom linearization scheme; the sequence contains tokens encoding concepts and relation markers and is treated as the target/source of a seq2seq model.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (rooted, directed, labeled graphs)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Traversal-based serialization (e.g., depth-first or other linearizations) or hand-designed token encodings of nodes and edges to produce a single token sequence representing the graph structure.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (graph generation from text); AMR-to-text or text-to-AMR transduction</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported example: van Noord and Bos (2017) (character-based seq2seq style) reported SMATCH 71.0 (%) on AMR 2.0 (reported in Table 1 of this paper). More recent seq/seq or transduction variants achieved higher scores (see Zhang et al. entries), but basic seq2seq linearization approaches were reported with lower SMATCH than best graph-aware methods.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper states that seq2seq linearization (one-pass prediction) methods tend to have lower accuracy than models that better model interactions between concept and relation prediction; e.g., older seq2seq-style results (van Noord 71.0) are below recent graph-aware transduction methods (Zhang et al. ~76-77%) and below the iterative inference model in this paper (up to 80.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Simple to implement; leverages mature seq2seq modeling and decoding infrastructures; treats concepts and relations with unified target vocabulary.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>One-pass prediction lacks explicit modeling of reciprocal causation between concept and relation decisions; can be weak on long or highly informative sentences and complex graph phenomena (reentrancies).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Reported to perform worse on long, information-dense sentences and in cases where modeling interaction between relation prediction and concept prediction is critical; overall lower SMATCH compared to methods that incorporate graph structure or iterative inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8989.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e8989.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Graph recategorization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph re-categorization / compact-format compression of AMR subgraphs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing representation that replaces frequently recurring AMR subgraphs by single compound-category nodes (and applies anonymization/removal steps) to reduce sparsity and complexity before modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AMR parsing as sequence-to-graph transduction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>subgraph compression / recategorization (compact format)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Rule-based grouping of specific subgraphs into single compound node categories, anonymizing entities, removing wiki links and polarity attributes, and converting graphs into a compact serialized format for parsing and later expansion in post-processing.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Apply hand-crafted rules to (1) anonymize named entities, (2) remove wiki/polarity attributes if desired, and (3) compress certain recurrent subgraphs into single nodes with compound categories; postprocessing recovers original AMR forms (restore wiki links, attributes).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (preprocessing step to simplify graphs for training and decoding)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation figures in this paper: On AMR 2.0, the authors' model without graph recategorization and without BERT achieved SMATCH 74.5%, while adding graph recategorization (no BERT) raised it to 77.3% (≈ +2.8 points). On AMR 1.0 the model moved from 68.8% (no recat, no BERT) to 71.2% (recat only) (≈ +2.4 points).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Many prior state-of-the-art systems rely heavily on graph recategorization; the authors note that while recategorization improves scores, their model without recategorization already outperforms earlier SOTA models, indicating recategorization is helpful but not the sole cause of large gains.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Reduces output vocabulary sparsity and structural complexity; empirically increases SMATCH by a few points in this study; enables simpler modeling and decoding over a more compact output space.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Requires substantial manual, expert-crafted rules for grouping subgraphs; rule-based routines (e.g., for NER) can fail to generalize.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>The paper reports notably lower NER subtask scores when using the recategorization pipeline because the rule-based NER used for recategorization does not generalize well to unseen entities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8989.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e8989.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chronological node-sequence encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph-as-sequence (chronological node order) encoding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encode an incrementally constructed AMR graph as a sequence of nodes ordered by their insertion time, and represent that sequence with a Transformer (masked self-attention + source-attention) to enable efficient incremental graph encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core semantic first: A top-down approach for AMR parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-as-node-sequence (chronological ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Treat the current graph as a sequence c1..cm of concepts in the chronological order they were inserted (prefix-masked Transformer with source-attention to text memories). Insert a dummy BOG token as position 0; each node is represented by concatenated char-CNN and learned embeddings; rely on masked self-attention so positions attend only to previous nodes plus source-attention to input text.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs (incrementally constructed graphs with reentrancies)</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>No explicit graph-to-text linearization of edges — the graph is represented by a sequential ordering of nodes (chronological), encoded via a Transformer decoder-style network; edges are not directly serialized (edge information is neglected by default).</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (incremental sequence-to-graph transduction) and relation reasoning within the parser's relation solver.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This encoding is central to the authors' parser. Reported end-to-end results (using this representation and the full iterative inference model): best SMATCH on AMR 2.0 = 80.2% (with graph recategorization + BERT); without recategorization & no BERT = 74.5%; with recategorization only = 77.3%; with BERT only = 78.7% (see Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Compared to methods that explicitly encode edges (e.g., graph neural nets) or convert graphs into trees, this representation permits more parallelization and incremental computation but naturally ignores explicit edge encodings; the authors attempted auxiliary attention supervision to inject parent information but saw no improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Enables significant parallelization during training and computation-saving incrementality during testing; computationally efficient and cognitively plausible for an incremental node-by-node construction process.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Inherently neglects explicit edge information in the representation (edges are not serialized), which can limit relation-aware reasoning; attempts to inject edge supervision into attention heads did not improve results and may disturb training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Explicit auxiliary supervision (encouraging attention heads to attend to graph parents) failed to improve and was attributed to disturbing the learned attention; potential errors arise when edge information is critical and not captured implicitly by the node sequence.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8989.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e8989.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BFS generation ordering</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Breadth-first traversal (BFS) generation order for AMR nodes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use a breadth-first traversal over the gold AMR graph to define a canonical node generation order for training an incremental node-by-node AMR parser.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Core semantic first: A top-down approach for AMR parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>BFS linearization for training order</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Create a reference generation order by running breadth-first traversal over target AMR graphs (core-semantic-first). For sibling nodes the authors use a mixed strategy (initially random ordering, later deterministic sorted-by-frequency) to stabilize and improve training.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR semantic graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Breadth-first traversal of the target graph to produce an ordered sequence of nodes for supervised incremental generation; sibling ordering handled by randomized vs deterministic strategies across training steps.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>Supervised training of incremental AMR parsers (determining generation order for node insertion decisions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>No direct stand-alone metric for BFS ordering; authors report training regime using BFS ordering and that a deterministic-after-random sibling-order strategy slightly improves final performance (they use 50K steps with random sibling order then 10K steps deterministic).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Authors note BFS (core-semantic-first) is cognitively appealing and effective; they also mention Zhang et al. (2019a) empirically verified effectiveness of pre-order (depth-first) traversal in another setting.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Cognitively plausible generation order (core semantics first); empirically effective as a training target sequence; easy to compute.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Sibling ordering can affect learning; requires heuristic choices (random->deterministic schedule) to stabilize training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>None explicitly reported; authors only note that ordering choices (sibling order) matter and require careful scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8989.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e8989.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree conversion via duplication</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Convert AMR graph to tree by duplicating reentrant nodes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversion that turns AMR graphs into trees by duplicating nodes that have reentrancies so that tree-based models can be applied to the resulting structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Broad-coverage semantic parsing as transduction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>graph-to-tree conversion (node duplication)</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Duplicate nodes that participate in multiple incoming edges (reentrancies) to remove reentrancy and obtain a tree; the transformed tree can then be serialized and consumed by models expecting trees/sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs with reentrancies</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Identify nodes with reentrant references and duplicate them to produce a tree; serialize the tree as needed for transduction models.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (enable tree-based transduction methods to operate on originally graph-structured AMR)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Zhang et al. (2019b), using such transformations in their pipeline, reported SMATCH 77.0% on AMR 2.0 (with graph recategorization + BERT) as reported in Table 1.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>The paper cites this as an alternative to multi-head simultaneous relation prediction, noting it is a different strategy (duplicating nodes) to handle reentrancies.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Allows tree-based architectures and simple serializations to be applied to originally graph-structured data.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>(Paper does not detail disadvantages beyond noting the method is different); transformation changes the original graph representation and may alter how reentrancies are modeled in downstream training.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not explicitly reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8989.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e8989.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Synchronous HRG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Synchronous hyperedge replacement grammar approach for AMR parsing</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A grammar-based approach that uses synchronous hyperedge replacement grammars to map between sentence strings and AMR graph fragments during parsing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A synchronous hyperedge replacement grammar based approach for amr parsing</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>synchronous HRG-based graph-to-string mapping</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Use synchronous hyperedge replacement grammar rules that generate graph fragments in lock-step with corresponding string spans, enabling structured grammar-based parsing/transduction between text and graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Extract and apply synchronous hyperedge replacement grammar rules that relate graph hyperedges/subgraphs to corresponding text spans; parsing is performed via grammar derivations.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (structured grammar-based parsing)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>This paper mentions Peng et al. (2015) but does not report their numeric metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Listed as an alternative approach beyond the primary categories (transition-based, seq2seq, sequence-to-graph); no direct empirical comparison provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Principled, linguistically/structurally grounded grammar approach for aligning graph structure to text.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Not discussed in detail in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e8989.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e8989.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods for representing or converting graphs into text for language model training, including details of the representation, the type of graph, the conversion process, downstream tasks, performance metrics, comparisons to other methods, and any reported advantages, disadvantages, or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MT-style conversion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parsing English into Abstract Meaning Representation using syntax-based machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Treat AMR parsing as a machine-translation-like problem by converting AMR graphs into a string representation suitable for use with syntax-based SMT/MT pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Parsing english into abstract meaning representation using syntaxbased machine translation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>representation_name</strong></td>
                            <td>MT-style linearization / translation-compatible encoding</td>
                        </tr>
                        <tr>
                            <td><strong>representation_description</strong></td>
                            <td>Linearize/encode AMR graphs into a string format compatible with syntax-based machine translation systems and then apply translation/parsing machinery to map between English and AMR string forms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_type</strong></td>
                            <td>AMR graphs</td>
                        </tr>
                        <tr>
                            <td><strong>conversion_method</strong></td>
                            <td>Convert graphs into linearized forms amenable to SMT-style models; specifics not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>downstream_task</strong></td>
                            <td>AMR parsing (treat as translation from English to AMR-string)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Pust et al. (2015) are reported in Table 2 with SMATCH 67.1% (AMR 1.0) in this paper's comparative table.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_others</strong></td>
                            <td>Reportedly lower SMATCH than more recent neural or graph-aware systems; older MT-style pipelines underperform modern neural transduction approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>advantages</strong></td>
                            <td>Leverages established MT tools and syntactic translation machinery.</td>
                        </tr>
                        <tr>
                            <td><strong>disadvantages</strong></td>
                            <td>Lower parsing performance in the neural-era comparisons presented in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Lower SMATCH on AMR 1.0 in the comparisons shown (67.1%), indicating limitations vs modern neural approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'AMR Parsing via Graph-Sequence Iterative Inference', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural AMR: Sequence-to-sequence models for parsing and generation <em>(Rating: 2)</em></li>
                <li>Neural semantic parsing by character-based translation: Experiments with abstract meaning representations <em>(Rating: 2)</em></li>
                <li>AMR parsing as sequence-to-graph transduction <em>(Rating: 2)</em></li>
                <li>Broad-coverage semantic parsing as transduction <em>(Rating: 2)</em></li>
                <li>Core semantic first: A top-down approach for AMR parsing <em>(Rating: 2)</em></li>
                <li>A synchronous hyperedge replacement grammar based approach for amr parsing <em>(Rating: 2)</em></li>
                <li>Parsing english into abstract meaning representation using syntaxbased machine translation <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-8989",
    "paper_id": "paper-b2075a1ce0e9bea9a6b6c0448c67de067471e885",
    "extraction_schema_id": "extraction-schema-158",
    "extracted_data": [
        {
            "name_short": "Seq2Seq linearization",
            "name_full": "Sequence-to-sequence linearization of AMR graphs",
            "brief_description": "Representation that serializes an AMR graph into a token sequence (a linearized string) so that standard sequence-to-sequence models can be trained to map between sentences and AMR linearizations.",
            "citation_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "mention_or_use": "mention",
            "representation_name": "linearization / serialization",
            "representation_description": "Serialize the graph (nodes and relations) into a flat token sequence using some graph traversal or custom linearization scheme; the sequence contains tokens encoding concepts and relation markers and is treated as the target/source of a seq2seq model.",
            "graph_type": "AMR semantic graphs (rooted, directed, labeled graphs)",
            "conversion_method": "Traversal-based serialization (e.g., depth-first or other linearizations) or hand-designed token encodings of nodes and edges to produce a single token sequence representing the graph structure.",
            "downstream_task": "AMR parsing (graph generation from text); AMR-to-text or text-to-AMR transduction",
            "performance_metrics": "Reported example: van Noord and Bos (2017) (character-based seq2seq style) reported SMATCH 71.0 (%) on AMR 2.0 (reported in Table 1 of this paper). More recent seq/seq or transduction variants achieved higher scores (see Zhang et al. entries), but basic seq2seq linearization approaches were reported with lower SMATCH than best graph-aware methods.",
            "comparison_to_others": "The paper states that seq2seq linearization (one-pass prediction) methods tend to have lower accuracy than models that better model interactions between concept and relation prediction; e.g., older seq2seq-style results (van Noord 71.0) are below recent graph-aware transduction methods (Zhang et al. ~76-77%) and below the iterative inference model in this paper (up to 80.2%).",
            "advantages": "Simple to implement; leverages mature seq2seq modeling and decoding infrastructures; treats concepts and relations with unified target vocabulary.",
            "disadvantages": "One-pass prediction lacks explicit modeling of reciprocal causation between concept and relation decisions; can be weak on long or highly informative sentences and complex graph phenomena (reentrancies).",
            "failure_cases": "Reported to perform worse on long, information-dense sentences and in cases where modeling interaction between relation prediction and concept prediction is critical; overall lower SMATCH compared to methods that incorporate graph structure or iterative inference.",
            "uuid": "e8989.0",
            "source_info": {
                "paper_title": "AMR Parsing via Graph-Sequence Iterative Inference",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Graph recategorization",
            "name_full": "Graph re-categorization / compact-format compression of AMR subgraphs",
            "brief_description": "A preprocessing representation that replaces frequently recurring AMR subgraphs by single compound-category nodes (and applies anonymization/removal steps) to reduce sparsity and complexity before modeling.",
            "citation_title": "AMR parsing as sequence-to-graph transduction",
            "mention_or_use": "use",
            "representation_name": "subgraph compression / recategorization (compact format)",
            "representation_description": "Rule-based grouping of specific subgraphs into single compound node categories, anonymizing entities, removing wiki links and polarity attributes, and converting graphs into a compact serialized format for parsing and later expansion in post-processing.",
            "graph_type": "AMR semantic graphs",
            "conversion_method": "Apply hand-crafted rules to (1) anonymize named entities, (2) remove wiki/polarity attributes if desired, and (3) compress certain recurrent subgraphs into single nodes with compound categories; postprocessing recovers original AMR forms (restore wiki links, attributes).",
            "downstream_task": "AMR parsing (preprocessing step to simplify graphs for training and decoding)",
            "performance_metrics": "Ablation figures in this paper: On AMR 2.0, the authors' model without graph recategorization and without BERT achieved SMATCH 74.5%, while adding graph recategorization (no BERT) raised it to 77.3% (≈ +2.8 points). On AMR 1.0 the model moved from 68.8% (no recat, no BERT) to 71.2% (recat only) (≈ +2.4 points).",
            "comparison_to_others": "Many prior state-of-the-art systems rely heavily on graph recategorization; the authors note that while recategorization improves scores, their model without recategorization already outperforms earlier SOTA models, indicating recategorization is helpful but not the sole cause of large gains.",
            "advantages": "Reduces output vocabulary sparsity and structural complexity; empirically increases SMATCH by a few points in this study; enables simpler modeling and decoding over a more compact output space.",
            "disadvantages": "Requires substantial manual, expert-crafted rules for grouping subgraphs; rule-based routines (e.g., for NER) can fail to generalize.",
            "failure_cases": "The paper reports notably lower NER subtask scores when using the recategorization pipeline because the rule-based NER used for recategorization does not generalize well to unseen entities.",
            "uuid": "e8989.1",
            "source_info": {
                "paper_title": "AMR Parsing via Graph-Sequence Iterative Inference",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Chronological node-sequence encoding",
            "name_full": "Graph-as-sequence (chronological node order) encoding",
            "brief_description": "Encode an incrementally constructed AMR graph as a sequence of nodes ordered by their insertion time, and represent that sequence with a Transformer (masked self-attention + source-attention) to enable efficient incremental graph encoding.",
            "citation_title": "Core semantic first: A top-down approach for AMR parsing",
            "mention_or_use": "use",
            "representation_name": "graph-as-node-sequence (chronological ordering)",
            "representation_description": "Treat the current graph as a sequence c1..cm of concepts in the chronological order they were inserted (prefix-masked Transformer with source-attention to text memories). Insert a dummy BOG token as position 0; each node is represented by concatenated char-CNN and learned embeddings; rely on masked self-attention so positions attend only to previous nodes plus source-attention to input text.",
            "graph_type": "AMR semantic graphs (incrementally constructed graphs with reentrancies)",
            "conversion_method": "No explicit graph-to-text linearization of edges — the graph is represented by a sequential ordering of nodes (chronological), encoded via a Transformer decoder-style network; edges are not directly serialized (edge information is neglected by default).",
            "downstream_task": "AMR parsing (incremental sequence-to-graph transduction) and relation reasoning within the parser's relation solver.",
            "performance_metrics": "This encoding is central to the authors' parser. Reported end-to-end results (using this representation and the full iterative inference model): best SMATCH on AMR 2.0 = 80.2% (with graph recategorization + BERT); without recategorization & no BERT = 74.5%; with recategorization only = 77.3%; with BERT only = 78.7% (see Table 1).",
            "comparison_to_others": "Compared to methods that explicitly encode edges (e.g., graph neural nets) or convert graphs into trees, this representation permits more parallelization and incremental computation but naturally ignores explicit edge encodings; the authors attempted auxiliary attention supervision to inject parent information but saw no improvement.",
            "advantages": "Enables significant parallelization during training and computation-saving incrementality during testing; computationally efficient and cognitively plausible for an incremental node-by-node construction process.",
            "disadvantages": "Inherently neglects explicit edge information in the representation (edges are not serialized), which can limit relation-aware reasoning; attempts to inject edge supervision into attention heads did not improve results and may disturb training.",
            "failure_cases": "Explicit auxiliary supervision (encouraging attention heads to attend to graph parents) failed to improve and was attributed to disturbing the learned attention; potential errors arise when edge information is critical and not captured implicitly by the node sequence.",
            "uuid": "e8989.2",
            "source_info": {
                "paper_title": "AMR Parsing via Graph-Sequence Iterative Inference",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "BFS generation ordering",
            "name_full": "Breadth-first traversal (BFS) generation order for AMR nodes",
            "brief_description": "Use a breadth-first traversal over the gold AMR graph to define a canonical node generation order for training an incremental node-by-node AMR parser.",
            "citation_title": "Core semantic first: A top-down approach for AMR parsing",
            "mention_or_use": "use",
            "representation_name": "BFS linearization for training order",
            "representation_description": "Create a reference generation order by running breadth-first traversal over target AMR graphs (core-semantic-first). For sibling nodes the authors use a mixed strategy (initially random ordering, later deterministic sorted-by-frequency) to stabilize and improve training.",
            "graph_type": "AMR semantic graphs",
            "conversion_method": "Breadth-first traversal of the target graph to produce an ordered sequence of nodes for supervised incremental generation; sibling ordering handled by randomized vs deterministic strategies across training steps.",
            "downstream_task": "Supervised training of incremental AMR parsers (determining generation order for node insertion decisions)",
            "performance_metrics": "No direct stand-alone metric for BFS ordering; authors report training regime using BFS ordering and that a deterministic-after-random sibling-order strategy slightly improves final performance (they use 50K steps with random sibling order then 10K steps deterministic).",
            "comparison_to_others": "Authors note BFS (core-semantic-first) is cognitively appealing and effective; they also mention Zhang et al. (2019a) empirically verified effectiveness of pre-order (depth-first) traversal in another setting.",
            "advantages": "Cognitively plausible generation order (core semantics first); empirically effective as a training target sequence; easy to compute.",
            "disadvantages": "Sibling ordering can affect learning; requires heuristic choices (random-&gt;deterministic schedule) to stabilize training.",
            "failure_cases": "None explicitly reported; authors only note that ordering choices (sibling order) matter and require careful scheduling.",
            "uuid": "e8989.3",
            "source_info": {
                "paper_title": "AMR Parsing via Graph-Sequence Iterative Inference",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Tree conversion via duplication",
            "name_full": "Convert AMR graph to tree by duplicating reentrant nodes",
            "brief_description": "A conversion that turns AMR graphs into trees by duplicating nodes that have reentrancies so that tree-based models can be applied to the resulting structure.",
            "citation_title": "Broad-coverage semantic parsing as transduction",
            "mention_or_use": "mention",
            "representation_name": "graph-to-tree conversion (node duplication)",
            "representation_description": "Duplicate nodes that participate in multiple incoming edges (reentrancies) to remove reentrancy and obtain a tree; the transformed tree can then be serialized and consumed by models expecting trees/sequences.",
            "graph_type": "AMR graphs with reentrancies",
            "conversion_method": "Identify nodes with reentrant references and duplicate them to produce a tree; serialize the tree as needed for transduction models.",
            "downstream_task": "AMR parsing (enable tree-based transduction methods to operate on originally graph-structured AMR)",
            "performance_metrics": "Zhang et al. (2019b), using such transformations in their pipeline, reported SMATCH 77.0% on AMR 2.0 (with graph recategorization + BERT) as reported in Table 1.",
            "comparison_to_others": "The paper cites this as an alternative to multi-head simultaneous relation prediction, noting it is a different strategy (duplicating nodes) to handle reentrancies.",
            "advantages": "Allows tree-based architectures and simple serializations to be applied to originally graph-structured data.",
            "disadvantages": "(Paper does not detail disadvantages beyond noting the method is different); transformation changes the original graph representation and may alter how reentrancies are modeled in downstream training.",
            "failure_cases": "Not explicitly reported in this paper.",
            "uuid": "e8989.4",
            "source_info": {
                "paper_title": "AMR Parsing via Graph-Sequence Iterative Inference",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Synchronous HRG",
            "name_full": "Synchronous hyperedge replacement grammar approach for AMR parsing",
            "brief_description": "A grammar-based approach that uses synchronous hyperedge replacement grammars to map between sentence strings and AMR graph fragments during parsing.",
            "citation_title": "A synchronous hyperedge replacement grammar based approach for amr parsing",
            "mention_or_use": "mention",
            "representation_name": "synchronous HRG-based graph-to-string mapping",
            "representation_description": "Use synchronous hyperedge replacement grammar rules that generate graph fragments in lock-step with corresponding string spans, enabling structured grammar-based parsing/transduction between text and graphs.",
            "graph_type": "AMR graphs",
            "conversion_method": "Extract and apply synchronous hyperedge replacement grammar rules that relate graph hyperedges/subgraphs to corresponding text spans; parsing is performed via grammar derivations.",
            "downstream_task": "AMR parsing (structured grammar-based parsing)",
            "performance_metrics": "This paper mentions Peng et al. (2015) but does not report their numeric metrics.",
            "comparison_to_others": "Listed as an alternative approach beyond the primary categories (transition-based, seq2seq, sequence-to-graph); no direct empirical comparison provided in this paper.",
            "advantages": "Principled, linguistically/structurally grounded grammar approach for aligning graph structure to text.",
            "disadvantages": "Not discussed in detail in this paper.",
            "failure_cases": "Not reported in this paper.",
            "uuid": "e8989.5",
            "source_info": {
                "paper_title": "AMR Parsing via Graph-Sequence Iterative Inference",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "MT-style conversion",
            "name_full": "Parsing English into Abstract Meaning Representation using syntax-based machine translation",
            "brief_description": "Treat AMR parsing as a machine-translation-like problem by converting AMR graphs into a string representation suitable for use with syntax-based SMT/MT pipelines.",
            "citation_title": "Parsing english into abstract meaning representation using syntaxbased machine translation",
            "mention_or_use": "mention",
            "representation_name": "MT-style linearization / translation-compatible encoding",
            "representation_description": "Linearize/encode AMR graphs into a string format compatible with syntax-based machine translation systems and then apply translation/parsing machinery to map between English and AMR string forms.",
            "graph_type": "AMR graphs",
            "conversion_method": "Convert graphs into linearized forms amenable to SMT-style models; specifics not detailed in this paper.",
            "downstream_task": "AMR parsing (treat as translation from English to AMR-string)",
            "performance_metrics": "Pust et al. (2015) are reported in Table 2 with SMATCH 67.1% (AMR 1.0) in this paper's comparative table.",
            "comparison_to_others": "Reportedly lower SMATCH than more recent neural or graph-aware systems; older MT-style pipelines underperform modern neural transduction approaches.",
            "advantages": "Leverages established MT tools and syntactic translation machinery.",
            "disadvantages": "Lower parsing performance in the neural-era comparisons presented in this paper.",
            "failure_cases": "Lower SMATCH on AMR 1.0 in the comparisons shown (67.1%), indicating limitations vs modern neural approaches.",
            "uuid": "e8989.6",
            "source_info": {
                "paper_title": "AMR Parsing via Graph-Sequence Iterative Inference",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural AMR: Sequence-to-sequence models for parsing and generation",
            "rating": 2
        },
        {
            "paper_title": "Neural semantic parsing by character-based translation: Experiments with abstract meaning representations",
            "rating": 2
        },
        {
            "paper_title": "AMR parsing as sequence-to-graph transduction",
            "rating": 2
        },
        {
            "paper_title": "Broad-coverage semantic parsing as transduction",
            "rating": 2
        },
        {
            "paper_title": "Core semantic first: A top-down approach for AMR parsing",
            "rating": 2
        },
        {
            "paper_title": "A synchronous hyperedge replacement grammar based approach for amr parsing",
            "rating": 2
        },
        {
            "paper_title": "Parsing english into abstract meaning representation using syntaxbased machine translation",
            "rating": 2
        }
    ],
    "cost": 0.02327675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>AMR Parsing via Graph $\approx$ Sequence Iterative Inference*</h1>
<p>Deng Cai<br>The Chinese University of Hong Kong<br>thisisjcykcd@gmail.com</p>
<p>Wai Lam<br>The Chinese University of Hong Kong<br>wlam@se.cuhk.edu.hk</p>
<h4>Abstract</h4>
<p>We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported SMATCH scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to $80.2 \%$ on LDC2017T10 (AMR 2.0) and $75.4 \%$ on LDC2014T12 (AMR 1.0).</p>
<h2>1 Introduction</h2>
<p>Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a broad-coverage semantic formalism that encodes the meaning of a sentence as a rooted, directed, and labeled graph, where nodes represent concepts and edges represent relations (See an example in Figure 1). AMR parsing is the task of transforming natural language text into AMR. One biggest challenge of AMR parsing is the lack of explicit alignments between nodes (concepts) in the graph and words in the text. This characteristic not only poses great difficulty in concept</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>prediction but also brings a close tie for concept prediction and relation prediction.</p>
<p>While most previous works rely on a pre-trained aligner to train a parser, some recent attempts include: modeling the alignments as latent variables (Lyu and Titov, 2018), attention-based sequence-to-sequence transduction models (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017), and attention-based sequence-to-graph transduction models (Cai and Lam, 2019; Zhang et al., 2019b). Sequence-to-graph transduction models build a semantic graph incrementally via spanning one node at every step. This property is appealing in terms of both computational efficiency and cognitive modeling since it mimics what human experts usually do, i.e., first grasping the core ideas then digging into more details (Banarescu et al., 2013; Cai and Lam, 2019).</p>
<p>Unfortunately, the parsing accuracy of existing works including recent state-of-the-arts (Zhang et al., 2019a,b) remain unsatisfactory compared to human-level performance, ${ }^{1}$ especially in cases where the sentences are rather long and informative, which indicates substantial room for improvement. One possible reason for the deficiency is the inherent defect of one-pass prediction process; that is, the lack of the modeling capability of the interactions between concept prediction and relation prediction, which is critical to achieving fullyinformed and unambiguous decisions.</p>
<p>We introduce a new approach tackling AMR parsing, following the incremental sequence-tograph transduction paradigm. We explicitly characterize each spanning step as the efforts for finding which part to abstract with respect to the input sequence, and where to construct with respect to the partially constructed output graph. Equivalently,</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>we treat AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. Intuitively, the answer of what concept to abstract decides where to construct (i.e., the relations to existing concepts), while the answer of where to construct determines what concept to abstract. Our proposed model, supported by neural networks with explicit structure for attention, reasoning, and composition, integrated with an iterative inference algorithm. It iterates between finding supporting text pieces and reading the partially constructed semantic graph, inferring more accurate and harmonious expansion decisions progressively. Our model is aligner-free and can be effectively trained with limited amount of labeled data. Experiments on two AMR benchmarks demonstrate that our parser outperforms the previous best parsers on both benchmarks. It achieves the best-reported SMATCH scores (F1): 80.2\% on LDC2017T10 and $75.4 \%$ on LDC2014T12, surpassing the previous state-of-the-art models by large margins.</p>
<h2>2 Related Work \&amp; Background</h2>
<p>On a coarse-grained level, we can categorize existing AMR parsing approaches into two main classes: Two-stage parsing (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) uses a pipeline design for concept identification and relation prediction, where the concept decisions precede all relation decisions; One-stage parsing constructs a parse graph incrementally. For more fine-grained analysis, those one-stage parsing methods can be further categorized into three types: Transitionbased parsing (Wang et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Peng et al., 2017; Guo and Lu, 2018; Liu et al., 2018; Wang and Xue, 2017; Naseem et al., 2019) processes a sentence from left-to-right and constructs the graph incrementally by alternately inserting a new node or building a new edge. Seq2seq-based parsing (Barzdins and Gosko, 2016; Konstas et al., 2017; van Noord and Bos, 2017; Peng et al., 2018) views parsing as sequence-to-sequence transduction by some linearization of the AMR graph. The concept and relation prediction are then treated equally with a shared vocabulary. The third class is graph-based parsing (Cai and Lam, 2019; Zhang et al., 2019b), where at each time step, a new node along with its connections to existing nodes are jointly decided, either in order (Cai and Lam, 2019) or in parallel (Zhang et al., 2019b). So far, the recip-
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: AMR graph construction given the partially constructed graph: (a) one possible expansion resulting in the boy concept. (b) another possible expansion resulting in the - (negation) concept.
rocal causation of relation prediction and concept prediction has not been closely-studied and wellutilized.</p>
<p>There are also some exceptions staying beyond the above categorization. Peng et al. (2015) introduce a synchronous hyperedge replacement grammar solution. Pust et al. (2015) regard the task as a machine translation problem, while Artzi et al. (2015) adapt combinatory categorical grammar. Groschwitz et al. (2018); Lindemann et al. (2019) view AMR graphs as the structure AM algebra.</p>
<h2>3 Motivation</h2>
<p>Our approach is inspired by the deliberation process when a human expert is deducing a semantic graph from a sentence. The output graph starts from an empty graph and spans incrementally in a node-by-node manner. At any time step of this process, we are distilling the information for the next expansion. We call it expansion because the new node, as an abstract concept of some specific text fragments in the input sentence, is derived to complete some missing elements in the current semantic graph. Specifically, given the input sentence and the current partially constructed graph, we are answering two critical questions: which part of the input sequence to abstract, and where in the output graph to construct the new concept. For instance, Figure 1(a) and (b) show two possible choices for the next expansion. In Figure 1(a), the word "boy" is abstracted to the concept boy to complement the subject information of the event go-02. On the</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of the dual graph-sequence iterative inference for AMR parsing. Given the current graph $G^{i}$ and input sequence $W$. The inference starts with an initial concept decision $x_{0}$ and follows the inference chain $x_{0} \rightarrow f\left(G^{i}, x_{0}\right) \rightarrow y_{1} \rightarrow g\left(W, y_{1}\right) \rightarrow x_{1} \rightarrow f\left(G^{i}, x_{1}\right) \rightarrow y_{2} \rightarrow g\left(W, y_{2}\right) \rightarrow \cdots$. The details of $f$ and $g$ are shown in red and blue boxes, where nodes in graph and tokens in sequence are selected via attention mechanisms.
other hand, in Figure 1(b), a polarity attribute of the event go-2 is constructed, which is triggered by the word "not" in the sentence.</p>
<p>We note that the answer to one of the questions can help answer the other. For instance, if we have decided to render the word "not" to the graph, then we will consider adding an edge labeled as polarity, and finally determine its attachment to the existing event go-2 (rather than an edge labeled ARGO to the same event go-2, though it is also present in the golden graph). On the other hand, if we have decided to find the subject (ARGO relation) of the action go-02, we are confident to locate the word "boy" instead of function words like "not" or "must", thus unambiguously predict the right concept boy. Another possible circumstance is that we may make a mistake trying to ask something that is not present in the sentence (e.g., the destination of the go-02 action). This attempt will be rejected by a review of the sentence. The rationale is that literally we cannot find the destination information in the sentence. Similarly, if we mistakenly propose to abstract some parts of the sentence that are not ready for construction yet, the proposal will be rejected by another inspection on the graph since that there is nowhere to place such a new concept.</p>
<p>We believe the mutual causalities, as described above, are useful for action disambiguation and harmonious decision making, which eventually result in more accurate parses. We formulate AMR parsing as a series of dual graph-sequence decisions and design an iterative inference approach
to tackle each of them. It is sort of analogous to the cognition procedure of a person, who might first notice part of the important information in one side (graph or sequence), then try to confirm her decision at the other side, which could just refute her former hypothesis and propose a new one, and finally converge to a conclusion after multiple rounds of reasoning.</p>
<h2>4 Proposed Model</h2>
<h3>4.1 Overview</h3>
<p>Formally, the parsing model consists of a series of graph expansion procedures $\left{G^{0} \rightarrow \ldots \rightarrow G^{i} \rightarrow\right.$ $\left.\ldots\right}$, starting from an empty graph $G^{0}$. In each turn of expansion, the following iterative inference process is performed:</p>
<p>$$
\begin{aligned}
y_{t}^{i} &amp; =g\left(G^{i}, x_{t}^{i}\right) \
x_{t+1}^{i} &amp; =f\left(W, y_{t}^{i}\right)
\end{aligned}
$$</p>
<p>where $W, G^{i}$ are the input sequence and the current semantic graph respectively. $g(\cdot), f(\cdot)$ seek where to construct (edge prediction) and what to abstract (node prediction) respectively, and $x_{t}^{i}, y_{t}^{i}$ are the $t$-th graph hypothesis (where to construct) and $t$-th sequence hypothesis (what to abstract) for the $i$-th expansion step respectively. For clarity, we may drop the superscript $i$ in the following descriptions.</p>
<p>Figure 2 depicts an overview of the graphsequence iterative inference process. Our model has four main components: (1) Sequence Encoder, which generates a set of text memories (per token)</p>
<p>to provide grounding for concept alignment and abstraction; (2) Graph Encoder, which generates a set of graph memories (per node) to provide grounding for relation reasoning; (3) Concept Solver, where a previous graph hypothesis is used for concept prediction; and (4) Graph Solver, where a previous concept hypothesis is used for relation prediction. The last two components correspond to the reasoning functions $g(\cdot)$ and $f(\cdot)$ respectively.</p>
<p>The text memories can be computed by Sentence Encoder at the beginning of the whole parsing while the graph memories are constructed by Graph Encoder incrementally as the parsing progresses. During the iterative inference, a semantic representation of current state is used to attend to both graph and text memories (blue and red arrows) in order to locate the new concept and obtain its relations to the existing graph, both of which subsequently refine each other. Intuitively, after a first glimpse of the input sentence and the current graph, specific sub-areas of both sequence and graph are revisited to obtain a better understanding of the current situation. Later steps typically read the text in detail with specific learning aims, either confirming or overturning a previous hypothesis. Finally, after several iterations of reasoning steps, the refined sequence/graph decisions are used for graph expansion.</p>
<h3>4.2 Sequence Encoder</h3>
<p>As mentioned above, we employ a sequence encoder to convert the input sentence into vector representations. The sequence encoder follows the multi-layer Transformer architecture described in Vaswani et al. (2017). At the bottom layer, each token is firstly transformed into the concatenation of features learned by a character-level convolutional neural network (charCNN, Kim et al., 2016) and randomly initialized embeddings for its lemma, part-of-speech tag, and named entity tag. Additionally, we also include features learned by pre-trained language model BERT (Devlin et al., 2019). ${ }^{2}$</p>
<p>Formally, for an input sequence $w_{1}, w_{2}, \ldots, w_{n}$ with length $n$, we insert a special token BOS at the beginning of the sequence. For clarity, we omit the detailed transformations (Vaswani et al., 2017) and denote the final output from our sequence encoder as $\left{h_{0}, h_{1}, \ldots, h_{n}\right} \in \mathbb{R}^{d}$, where $h_{0}$ corresponds the special token BOS and serves as an overall rep-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>resentation while others are considered as contextualized word representations. Note that the sequence encoder only needs to be invoked once, and the produced text memories are used for the whole parsing procedure.</p>
<h3>4.3 Graph Encoder</h3>
<p>We use a similar idea in Cai and Lam (2019) to encode the incrementally expanding graph. Specifically, a graph is simply treated as a sequence of nodes (concepts) in the chronological order of when they are inserted into the graph. We employ multi-layer Transformer architecture with masked self-attention and source-attention, which only allows each position in the node sequence to attend to all positions up to and including that position, and every position in the node sequence to attend over all positions in the input sequence. ${ }^{3}$ While this design allows for significantly more parallelization during training and computation-saving incrementality during testing, ${ }^{4}$ it inherently neglects the edge information. We attempted to alleviate this problem by incorporating the idea of Strubell et al. (2018) that applies auxiliary supervision at attention heads to encourage them to attend to each node's parents in the AMR graph. However, we did not see performance improvement. We attribute the failure to the fact that the neural attention mechanisms on their own are already capable of learning to attend to useful graph elements, and the auxiliary supervision is likely to disturb the ultimate parsing goal.</p>
<p>Consequently, for the current graph $G$ with $m$ nodes, we take its output concept sequence $c_{1}, c_{2}, \ldots, c_{m}$ as input. Similar to the sequence encoder, we insert a special token BOG at the beginning of the concept sequence. Each concept is firstly transformed into the concatenation of feature vector learned by a char-CNN and randomly initialized embedding. Then, a multi-layer Transformer encoder with masked self-attention and sourceattention is applied, resulting in vector representations $\left{s_{0}, s_{1}, \ldots, s_{m}\right} \in \mathbb{R}^{d}$, where $s_{0}$ represents the special concept BOG and serves as a dummy node while others are considered as contextualized node representations.</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>4.4 Concept Solver</h3>
<p>At each sequence reasoning step $t$, the concept solver receives a state vector $y_{t}$ that carries the latest graph decision and the input sequence memories $h_{1}, \ldots, h_{n}$ from the sequence encoder, and aims to locate the proper parts in the input sequence to abstract and generate a new concept. We employ the scaled dot-product attention proposed in Vaswani et al. (2017) to solve this problem. Concretely, we first calculate an attention distribution over all input tokens:</p>
<p>$$
\alpha_{t}=\operatorname{softmax}\left(\frac{\left(W^{Q} y_{t}\right)^{\mathrm{T}} W^{K} h_{1: n}}{\sqrt{d_{k}}}\right)
$$</p>
<p>where $\left{W^{Q}, W^{K}\right} \in \mathbb{R}^{d_{k} \times d}$ denote learnable linear projections that transform the input vectors into the query and key subspace respectively, and $d_{k}$ represents the dimensionality of the subspace.</p>
<p>The attention weights $\alpha_{t} \in \mathbb{R}^{n}$ provide a soft alignment between the new concept and the tokens in the input sequence. We then compute the probability distribution of the new concept label through a hybrid of three channels. First, $\alpha_{t}$ is fed through an MLP and softmax to obtain a probability distribution over a pre-defined vocabulary:</p>
<p>$$
\begin{array}{r}
\operatorname{MLP}\left(\alpha_{t}\right)=\left(W^{V} h_{1: n}\right) \alpha_{t}+y_{t} \
P^{(\text {vocab })}=\operatorname{softmax}\left(W^{(\text {vocab })} \operatorname{MLP}\left(\alpha_{t}\right)+b^{(\text {vocab })}\right)
\end{array}
$$</p>
<p>where $W^{V} \in \mathbb{R}^{d \times d}$ denotes the learnable linear projection that transforms the text memories into the value subspace, and the value vectors are averaged according to $\alpha_{t}$ for concept label prediction. Second, the attention weights $\alpha_{t}$ directly serve as a copy mechanism (Gu et al., 2016; See et al., 2017), i,e., the probabilities of copying a token lemma from the input text as a node label. Third, to address the attribute values such as person names or numerical strings, we also use $\alpha_{t}$ for another copy mechanism that directly copies the original strings of input tokens. The above three channels are combined via a soft switch to control the production of the concept label from different sources:</p>
<p>$$
\left[p_{0}, p_{1}, p_{2}\right]=\operatorname{softmax}\left(W^{(\text {switch })} \operatorname{MLP}\left(\alpha_{t}\right)\right)
$$</p>
<p>where MLP is the same as in Eq. 1, and $p_{0}, p_{1}$ and $p_{2}$ are the probabilities of three prediction channels respectively. Hence, the final prediction probability
of a concept $c$ is given by:</p>
<p>$$
\begin{aligned}
P(c)= &amp; p_{0} \cdot P^{(\text {vocab })}(c) \
+ &amp; p_{1} \cdot\left(\sum_{i \in L(c)} \alpha_{t}[i]\right)+p_{2} \cdot\left(\sum_{i \in T(c)} \alpha_{t}[i]\right)
\end{aligned}
$$</p>
<p>where $[i]$ indexes the $i$-th element and $L(c)$ and $T(c)$ are index sets of lemmas and tokens respectively that have the surface form as $c$.</p>
<h3>4.5 Relation Solver</h3>
<p>At each graph reasoning step $t$, the relation solver receives a state vector $x_{t}$ that carries the latest concept decision and the output graph memories $s_{0}, s_{1}, \ldots, s_{m}$ from the graph encoder, and aims to point out the nodes in the current graph that have an immediate relation to the new concept (source nodes) and generate corresponding edges. Similar to Cai and Lam (2019); Zhang et al. (2019b), we factorize the task as two stages: First, a relation identification module points to some preceding nodes as source nodes; Then, the relation classification module predicts the relation type between the new concept and predicted source nodes. We leave the latter to be determined after iterative inference.</p>
<p>AMR is a rooted, directed, and acyclic graph. The reason for AMR being a graph instead of a tree is that it allows reentrancies where a concept participates in multiple semantic relations with different semantic roles. Following Cai and Lam (2019), we use multi-head attention for a more compact parsing procedure where multiple source nodes are simultaneously determined. ${ }^{5}$ Formally, our relation identification module employs $H$ different attention heads, for each head $h$, we calculate an attention distribution over all existing node (including the dummy node $s_{0}$ ):</p>
<p>$$
\beta_{t}^{h}=\operatorname{softmax}\left(\frac{\left(W_{h}^{Q} x_{t}\right)^{\mathrm{T}} W_{h}^{K} s_{0: m}}{\sqrt{d_{k}}}\right)
$$</p>
<p>Then, we take the maximum over different heads as the final edge probabilities:</p>
<p>$$
\beta_{t}[i]=\max <em t="t">{h=1}^{H} \beta</em>[i]
$$}^{h</p>
<p>Therefore, different heads may points to different nodes at the same time. Intuitively, each head represents a distinct relation detector for a particular</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Multi-head attention for relation identification. At left is the attention matrix, where each column corresponds to a unique attention head, and each row corresponds to an existing node.
set of relation types. For each attention head, it will point to a source node if certain relations exist between the new node and the existing graph, otherwise it will point to the dummy node. An example with four attention heads and three existing nodes (excluding the dummy node) is illustrated in Figure 3.</p>
<h3>4.6 Iterative Inference</h3>
<p>As described above, the concept solver and the relation solver are conceptually two attention mechanisms over the sequence and graph respectively, addressing the concept prediction and relation prediction separately. The key is to pass the decisions between the solvers so that they can examine each other's answer and make harmonious decisions. Specifically, at each spanning step $i$, we start the iterative inference by setting $x_{0}=h_{0}$ and solving $f\left(G^{i}, x_{0}\right)$. After the $t$-th graph reasoning, we compute the state vector $y_{t}$, which will be handed over to the concept solver as $g\left(W, y_{t}\right)$, as:</p>
<p>$$
y_{t}=\operatorname{FFN}^{(y)}\left(x_{t}+\left(W^{V} h_{1: n}\right) \alpha_{t}\right)
$$</p>
<p>where $\operatorname{FFN}^{(y)}$ is a feed-forward network and $W^{V}$ projects text memories into a value space. Similarly, after the $t$-th sequence reasoning, we update the state vector from $y_{t}$ to $x_{t+1}$ as:</p>
<p>$$
x_{t+1}=\operatorname{FFN}^{(x)}\left(y_{t}+\sum_{h=1}^{H}\left(W_{h}^{V} s_{0: n}\right) \beta_{t}^{h}\right)
$$</p>
<p>where $\operatorname{FFN}^{(x)}$ is a feed-forward network and $W_{h}^{V}$ projects graph memories into a value space for each head $h$. After $N$ steps of iterative inference, i,e.,</p>
<p>$$
\begin{aligned}
&amp; x_{0} \rightarrow f\left(G^{i}, x_{0}\right) \rightarrow y_{1} \rightarrow g\left(W, y_{1}\right) \rightarrow x_{1} \rightarrow \cdots \
&amp; \rightarrow f\left(G^{i}, x_{N-1}\right) \rightarrow y_{N} \rightarrow g\left(W, y_{N}\right) \rightarrow x_{N}
\end{aligned}
$$</p>
<p>we finally employ a deep biaffine classifier (Dozat and Manning, 2016) for edge label prediction. The</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1 AMR Parsing via Graph \(\approx\) Sequence
Iterative Inference
</code></pre></div>

<p>Input: the input sentence $W=\left(w_{1}, w_{2}, \ldots, w_{n}\right)$
Output: the corresponding AMR graph $G$
// compute text memories
1: $h_{0}, h_{1}, \ldots, h_{n}=$ SequenceEncoder((BOS, $\left.\left.w_{1}, \ldots, w_{n}\right)\right)$
// initialize graph
2: $G^{0}=$ (nodes $={$ BOG $}$, edges $=\emptyset$ )
// start graph expansions
3: $i=0$
4: while True do
5: $s_{0}, \ldots, s_{i}=$ GraphEncoder $\left(G^{i}\right)$
// the graph memories can be
computed <em>incrementally</em>
6: $x_{0}=h_{0}$
// iterative inference
7: for $t \leftarrow 1$ to $N$ do
8: $\quad y_{t}=f\left(G^{i}, x_{t-1}\right) / /$ Seq. $\rightarrow$ Graph
9: $\quad x_{t}=g\left(W, y_{t}\right) / /$ Graph $\rightarrow$ Seq.
10: end for
11: if concept prediction is EOG then
12: break
13: end if
14: update $G^{i+1}$ based on $G^{i}, x_{N}$ and $y_{N}$
15: $i=i+1$
16: end while
17: return $G^{i}$
classifier uses a biaffine function to score each label, given the final concept representation $x_{N}$ and the node vector $s_{1: m}$ as input. The resulted concept, edge, and edge label predictions will added to the new graph $G^{i+1}$ if the concept prediction is not EOG, a special concept that we add for indicating termination. Otherwise, the whole parsing process is terminated and the current graph is returned as final result. The complete parsing process adopting the iterative inference is described in Algorithm 1.</p>
<h2>5 Training \&amp; Prediction</h2>
<p>Our model is trained with the standard maximum likelihood estimate. The optimization objective is to maximize the sum of the decomposed step-wise log-likelihood, where each is the sum of concept, edge, and edge label probabilities. To facilitate training, we create a reference generation order of nodes by running a breadth-first-traversal over target AMR graphs, as it is cognitively appealing (core-semantic-first principle, Cai and Lam, 2019) and the effectiveness of pre-order traversal is also</p>
<p>empirically verified by Zhang et al. (2019a) in a depth-first setting. For the generation order for sibling nodes, we adopt the uniformly random order and the deterministic order sorted by the relation frequency in a $1: 1$ ratio at first then change to the deterministic order only in the final training steps. We empirically find that the deterministic-afterrandom strategy slightly improves performance.</p>
<p>During testing, our model searches for the best output graph through beam search based on the log-likelihood at each spanning step. The time complexity of our model is $O(k|V|)$, where $k$ is the beam size, and $|V|$ is the number of nodes.</p>
<h2>6 Experiments</h2>
<h3>6.1 Experimental Setup</h3>
<p>Datasets Our evaluation is conducted on two AMR public releases: AMR 2.0 (LDC0217T10) and AMR 1.0 (LDC2014T12). AMR 2.0 is the latest and largest AMR sembank that was extensively used in recent works. AMR 1.0 shares the same development and test set with AMR, while the size of its training set is only about one-third of AMR 2.0, making it a good testbed to evaluate our model's sensitivity for data size. ${ }^{6}$</p>
<p>Implementation Details We use Stanford CoreNLP (Manning et al., 2014) for tokenization, lemmatization, part-of-speech, and named entity tagging. The hyper-parameters of our models are chosen on the development set of AMR 2.0. Without explicit specification, we perform $N=4$ steps of iterative inference. Other hyper-parameter settings can be found in the Appendix. Our models are trained using ADAM (Kingma and Ba, 2014) for up to 60 K steps (first 50 K with the random sibling order and last 10 K with deterministic order), with early stopping based on development set performance. We fix BERT parameters similar to Zhang et al. (2019a,b) due to the GPU memory limit. During testing, we use a beam size of 8 for the highest-scored graph approximation. ${ }^{7}$</p>
<p>AMR Pre- and Post-processing We remove senses as done in Lyu and Titov (2018); Zhang et al. (2019a,b) and simply assign the most frequent sense for nodes in post-processing. Notably,</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>most existing methods including the state-the-ofart parsers (Zhang et al., 2019a,b; Lyu and Titov, 2018; Guo and Lu, 2018, inter alia) often rely on heavy graph re-categorization for reducing the complexity and sparsity of the original AMR graphs. For graph re-categorization, specific subgraphs of AMR are grouped together and assigned to a single node with a new compound category, which usually involves non-trivial expert-level manual efforts for hand-crafting rules. We follow the exactly same pre- and post-processing steps of those of Zhang et al. (2019a,b) for graph re-categorization. More details can be found in the Appendix.</p>
<p>Ablated Models As pointed out by Cai and Lam (2019), the precise set of graph re-categorization rules differs among different works, making it difficult to distinguish the performance improvement from model optimization and carefully designed rules. In addition, only recent works (Zhang et al., 2019a,b; Lindemann et al., 2019; Naseem et al., 2019) have started to utilize the large-scale pretrained language model, BERT (Devlin et al., 2019; Wolf et al., 2019). Therefore, we also include ablated models for addressing two questions: (1) How dependent is our model on performance from handcrafted graph re-categorization rules? (2) How much does BERT help? We accordingly implement three ablated models by removing either one of them or removing both. The ablation study not only reveals the individual effect of two model components but also helps facilitate fair comparisons with prior works.</p>
<h3>6.2 Experimental Results</h3>
<p>Main Results The performance of AMR parsing is conventionally evaluated by SMATCH (F1) metric (Cai and Knight, 2013). The left block of Table 1 shows the SMATCH scores on the AMR 2.0 test set of our models against the previous best approaches and recent competitors. On AMR 2.0, we outperform the latest push from Zhang et al. (2019b) by $3.2 \%$ and, for the first time, obtain a parser with over $80 \%$ SMATCH score. Note that even without BERT, our model still outperforms the previous state-of-the-art approaches using BERT (Zhang et al., 2019b,a) with $77.3 \%$. This is particularly remarkable since running BERT is computationally expensive. As shown in Table 2, on AMR 1.0 where the training instances are only around 10 K , we improve the best-reported results by $4.1 \%$ and reach at $75.4 \%$, which is already higher than</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">G. R.</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">SMATCH</th>
<th style="text-align: center;">fine-grained evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Unlabeled</td>
<td style="text-align: center;">No WSD</td>
<td style="text-align: center;">Concept</td>
<td style="text-align: center;">SRL</td>
<td style="text-align: center;">Reent.</td>
<td style="text-align: center;">Neg.</td>
<td style="text-align: center;">NER</td>
<td style="text-align: center;">Wiki</td>
</tr>
<tr>
<td style="text-align: center;">van Noord and Bos (2017)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">62</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">65</td>
</tr>
<tr>
<td style="text-align: center;">Groschwitz et al. (2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">71.0</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">71</td>
</tr>
<tr>
<td style="text-align: center;">Lyu and Titov (2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">74.4</td>
<td style="text-align: center;">77.1</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">69.8</td>
<td style="text-align: center;">52.3</td>
<td style="text-align: center;">58.4</td>
<td style="text-align: center;">86.0</td>
<td style="text-align: center;">75.7</td>
</tr>
<tr>
<td style="text-align: center;">Cai and Lam (2019)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">73.2</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">84.4</td>
<td style="text-align: center;">66.7</td>
<td style="text-align: center;">55.3</td>
<td style="text-align: center;">62.9</td>
<td style="text-align: center;">82.0</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: center;">Lindemann et al. (2019)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">75.3</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Naseem et al. (2019)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">75.5</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">Zhang et al. (2019a)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">74.6</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Zhang et al. (2019a)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">76.3</td>
<td style="text-align: center;">79.0</td>
<td style="text-align: center;">76.8</td>
<td style="text-align: center;">84.8</td>
<td style="text-align: center;">69.7</td>
<td style="text-align: center;">60.0</td>
<td style="text-align: center;">75.2</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">85.8</td>
</tr>
<tr>
<td style="text-align: center;">Zhang et al. (2019b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">77.0</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">79</td>
<td style="text-align: center;">86</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">77.8</td>
<td style="text-align: center;">75.1</td>
<td style="text-align: center;">85.9</td>
<td style="text-align: center;">68.5</td>
<td style="text-align: center;">57.7</td>
<td style="text-align: center;">65.0</td>
<td style="text-align: center;">82.9</td>
<td style="text-align: center;">81.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">77.3</td>
<td style="text-align: center;">80.1</td>
<td style="text-align: center;">77.9</td>
<td style="text-align: center;">86.4</td>
<td style="text-align: center;">69.4</td>
<td style="text-align: center;">58.5</td>
<td style="text-align: center;">75.6</td>
<td style="text-align: center;">78.4</td>
<td style="text-align: center;">86.1</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">78.7</td>
<td style="text-align: center;">81.5</td>
<td style="text-align: center;">79.2</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">74.5</td>
<td style="text-align: center;">63.8</td>
<td style="text-align: center;">66.1</td>
<td style="text-align: center;">87.1</td>
<td style="text-align: center;">81.3</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">80.2</td>
<td style="text-align: center;">82.8</td>
<td style="text-align: center;">80.8</td>
<td style="text-align: center;">88.1</td>
<td style="text-align: center;">74.2</td>
<td style="text-align: center;">64.6</td>
<td style="text-align: center;">78.9</td>
<td style="text-align: center;">81.1</td>
<td style="text-align: center;">86.3</td>
</tr>
</tbody>
</table>
<p>Table 1: SMATCH scores (\%) (left) and fine-grained evaluations (\%) (right) on the test set of AMR 2.0. G. R./BERT indicates whether or not the results use Graph Re-categorization/BERT respectively.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">G. R.</th>
<th style="text-align: center;">BERT</th>
<th style="text-align: center;">SMATCH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Flanigan et al. (2016)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">66.0</td>
</tr>
<tr>
<td style="text-align: left;">Pust et al. (2015)</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">67.1</td>
</tr>
<tr>
<td style="text-align: left;">Wang and Xue (2017)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">68.1</td>
</tr>
<tr>
<td style="text-align: left;">Guo and Lu (2018)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">68.3</td>
</tr>
<tr>
<td style="text-align: left;">Zhang et al. (2019a)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">70.2</td>
</tr>
<tr>
<td style="text-align: left;">Zhang et al. (2019b)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">71.3</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">68.8</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">71.2</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\times$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">74.0</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\mathbf{7 5 . 4}$</td>
</tr>
</tbody>
</table>
<p>Table 2: SMATCH scores on the test set of AMR 1.0.
most models trained on AMR 2.0. The even more substantial performance gain on the smaller dataset suggests that our method is both effective and dataefficient. Besides, again, our model without BERT already surpasses previous state-of-the-art results using BERT. For ablated models, it can be observed that our models yield the best results in all settings if there are any competitors, indicating BERT and graph re-categorization are not the exclusive key for our superior performance.</p>
<p>Fine-grained Results In order to investigate how our parser performs on individual sub-tasks, we also use the fine-grained evaluation tool (Damonte et al., 2017) and compare to systems which reported these scores. ${ }^{8}$ As shown in the right block of Table 1, our best model obtains the highest scores on almost all sub-tasks. The improvements in all sub-tasks are consistent and uniform (around $2 \% \sim 3 \%$ ) compared to the previous state-of-the-art performance (Zhang et al., 2019b), partly confirming that our model boosts performance via consolidated and harmonious decisions rather than fixing particular phenomena. By our ablation study,</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: SMATCH scores with different numbers of inference steps. Sentences are grouped by length.
it is worth noting that the NER scores are much lower when using graph re-categorization. This is because the rule-based system for NER in graph recategorization does not generalize well to unseen entities, which suggest a potential improvement by adapting better NER taggers.</p>
<h3>6.3 More Analysis</h3>
<p>Effect of Iterative Inference We then turn to study the effect of our key idea, namely, the iterative inference design. To this end, we run a set of experiments with different values of the number of the inference steps $N$. The results on AMR 2.0 are shown in Figure 4 (solid line). As seen, the performance generally goes up when the number of inference steps increases. The difference is most noticeable between 1 (no iterative reasoning is performed) and 2 , while later improvements gradually diminish. One important point here is that the model size in terms of the number of parameters is constant regardless of the number of inference steps, making it different from general over-parameterized problems.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Case study (viewed in color). Color shading intensity represents the value of the attention score.</p>
<p>For a closer study on the effect of the inference steps with respect to the lengths of input sentences, we group sentences into three classes by length and also show the individual results in Figure 4 (dashed lines). As seen, the iterative inference helps more for longer sentences, which confirms our intuition that longer and more complex input needs more reasoning. Another interesting observation is that the performance on shorter sentences reaches the peaks earlier. This observation suggests that the number of inference steps can be adjusted according to the input sentence, which we leave as future work.</p>
<p>Effect of Beam Size We are also interested in the effect of beam size during testing. Ideally, if a model is able to make accurate predictions in the first place, it should rely less on the search algorithm. We vary the beam size and plot the curve in Figure 6. The results show that the performance generally gets better with larger beam sizes. However, a small beam size of 2 already gets the most of the credits, which suggests that our model is robust enough for time-stressing environments.</p>
<p>Visualization We visualize the iterative reasoning process with a case study in Figure 5. We illustrate the values of $\alpha_{t}, \beta_{t}$ as the iterative inference progresses. As seen, the parser makes mistakes in the first step, but gradually corrects its decisions and finally makes the right predictions. Later reasoning steps typically provide a sharper attention distribution than earlier steps, narrowing down the most likely answer with more confidence.</p>
<p>Speed We also report the parsing speed of our non-optimized code: With BERT, the parsing speed of our system is about 300 tokens/s, while without BERT, it is about 330 tokens/s on a single Nvidia P4 GPU. The absolute speed depends on various implementation choices and hardware performance.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: SMATCH scores with different beam sizes.</p>
<p>In theory, the time complexity of our parsing algorithm is $O(k b n)$, where $k$ is the number of iterative steps, $b$ is beam size, and $n$ is the graph size (number of nodes) respectively. It is important to note that our algorithm is linear in the graph size.</p>
<h2>7 Conclusion</h2>
<p>We presented the dual graph-sequence iterative inference method for AMR Parsing. Our method constructs an AMR graph incrementally in a node-by-node fashion. Each spanning step is explicitly characterized as answering two questions: which parts of the sequence to abstract, and where in the graph to construct. We leverage the mutual causalities between the two and design an iterative inference algorithm. Our model significantly advances the state-of-the-art results on two AMR corpora. An interesting future work is to make the number of inference steps adaptive to input sentences. Also, the idea proposed in this paper may be applied to a broad range of structured prediction tasks (not only restricted to other semantic parsing tasks) where the complex output space can be divided into two interdependent parts with a similar iterative inference process to achieve harmonious predictions and better performance.</p>
<h2>References</h2>
<p>Yoav Artzi, Kenton Lee, and Luke Zettlemoyer. 2015. Broad-coverage ccg semantic parsing with amr. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages $1699-1710$.</p>
<p>Miguel Ballesteros and Yaser Al-Onaizan. 2017. AMR parsing using stack-LSTMs. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1269-1275.</p>
<p>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178-186.</p>
<p>Guntis Barzdins and Didzis Gosko. 2016. RIGA at SemEval-2016 task 8: Impact of Smatch extensions and character-level neural translation on AMR parsing accuracy. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval2016), pages 1143-1147.</p>
<p>Deng Cai and Wai Lam. 2019. Core semantic first: A top-down approach for AMR parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3797-3807.</p>
<p>Shu Cai and Kevin Knight. 2013. Smatch: an evaluation metric for semantic feature structures. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pages 748-752.</p>
<p>Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo N Mendes. 2013. Improving efficiency and accuracy in multilingual entity extraction. In Proceedings of the 9th International Conference on Semantic Systems, pages 121-124.</p>
<p>Marco Damonte, Shay B. Cohen, and Giorgio Satta. 2017. An incremental parser for abstract meaning representation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 536-546.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Timothy Dozat and Christopher D Manning. 2016. Deep biaffine attention for neural dependency parsing. arXiv preprint arXiv:1611.01734.</p>
<p>Jeffrey Flanigan, Chris Dyer, Noah A Smith, and Jaime Carbonell. 2016. Cmu at semeval-2016 task 8: Graph-based amr parsing with infinite ramp loss. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 12021206.</p>
<p>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages $1426-1436$.</p>
<p>Jonas Groschwitz, Matthias Lindemann, Meaghan Fowlie, Mark Johnson, and Alexander Koller. 2018. AMR dependency parsing with a typed semantic algebra. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1831-1841.</p>
<p>Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $1631-1640$.</p>
<p>Zhijiang Guo and Wei Lu. 2018. Better transitionbased amr parsing with refined search space. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 17121722.</p>
<p>Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. Character-aware neural language models. In Thirtieth AAAI Conference on Artificial Intelligence, pages 2741-2749.</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.</p>
<p>Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 146-157.</p>
<p>Matthias Lindemann, Jonas Groschwitz, and Alexander Koller. 2019. Compositional semantic parsing across graphbanks. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4576-4585.</p>
<p>Yijia Liu, Wanxiang Che, Bo Zheng, Bing Qin, and Ting Liu. 2018. An AMR aligner tuned by transition-based parser. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2422-2430.</p>
<p>Chunchuan Lyu and Ivan Titov. 2018. AMR parsing as graph prediction with latent alignment. In Proceedings of the 56th Annual Meeting of the Association</p>
<p>for Computational Linguistics (Volume 1: Long Papers), pages 397-407.</p>
<p>Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David McClosky. 2014. The stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55-60.</p>
<p>Tahira Naseem, Abhishek Shah, Hui Wan, Radu Florian, Salim Roukos, and Miguel Ballesteros. 2019. Rewarding Smatch: Transition-based AMR parsing with reinforcement learning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4586-4592.</p>
<p>Rik van Noord and Johan Bos. 2017. Neural semantic parsing by character-based translation: Experiments with abstract meaning representations. arXiv preprint arXiv:1705.09980.</p>
<p>Xiaochang Peng, Linfeng Song, and Daniel Gildea. 2015. A synchronous hyperedge replacement grammar based approach for amr parsing. In Proceedings of the Nineteenth Conference on Computational Natural Language Learning, pages 32-41.</p>
<p>Xiaochang Peng, Linfeng Song, Daniel Gildea, and Giorgio Satta. 2018. Sequence-to-sequence models for cache transition systems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1842-1852.</p>
<p>Xiaochang Peng, Chuan Wang, Daniel Gildea, and Nianwen Xue. 2017. Addressing the data sparsity issue in neural AMR parsing. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 366-375.</p>
<p>Michael Pust, Ulf Hermjakob, Kevin Knight, Daniel Marcu, and Jonathan May. 2015. Parsing english into abstract meaning representation using syntaxbased machine translation. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1143-1154.</p>
<p>Abigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10731083.</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958.</p>
<p>Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-informed self-attention for semantic
role labeling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5027-5038.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008.</p>
<p>Chuan Wang, Sameer Pradhan, Xiaoman Pan, Heng Ji, and Nianwen Xue. 2016. Camr at semeval-2016 task 8: An extended transition-based amr parser. In Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016), pages 11731178.</p>
<p>Chuan Wang and Nianwen Xue. 2017. Getting the most out of amr parsing. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1257-1268.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771.</p>
<p>Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019a. AMR parsing as sequence-tograph transduction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 80-94.</p>
<p>Sheng Zhang, Xutai Ma, Kevin Duh, and Benjamin Van Durme. 2019b. Broad-coverage semantic parsing as transduction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 3784-3796.</p>
<h1>A Hyper-parameter Settings</h1>
<p>Table 3 lists the hyper-parameters used in our full models. Char-level CNNs and Transformer layers in the sentence encoder and the graph encoder share the same hyper-parameter settings. The BERT model (Devlin et al., 2019) we used is the Huggingface's implementation (Wolf et al., 2019) (bert-base-cased). To mitigate overfitting, we apply dropout (Srivastava et al., 2014) with the drop rate 0.2 between different layers. We randomly mask (replacing inputs with a special UNK token) the input lemmas, POS tags, and NER tags with a rate of 0.33 . Parameter optimization is performed with the ADAM optimizer (Kingma and Ba, 2014) with $\beta_{1}=0.9$ and $\beta_{2}=0.999$. The learning rate schedule is similar to that in Vaswani et al. (2017), with warm-up steps being set to 2 K . We use early stopping on the development set for choosing the best model.</p>
<h2>B AMR Pre- and Post-processing</h2>
<p>We follow exactly the same pre- and postprocessing steps of those of Zhang et al. (2019a,b) for graph re-categorization. In preprocessing, we anonymize entities, remove wiki links and polarity attributes, and convert the resultant AMR graphs into a compact format by compressing certain subgraphs. In post-processing, we recover the original AMR format from the compact format, restore Wikipedia links using the DBpedia Spotlight API (Daiber et al., 2013), add polarity attributes based on rules observed from the training data. More details can be found in Zhang et al. (2019a).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Embeddings</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">lemma</td>
<td style="text-align: right;">300</td>
</tr>
<tr>
<td style="text-align: left;">POS tag</td>
<td style="text-align: right;">32</td>
</tr>
<tr>
<td style="text-align: left;">NER tag</td>
<td style="text-align: right;">16</td>
</tr>
<tr>
<td style="text-align: left;">concept</td>
<td style="text-align: right;">300</td>
</tr>
<tr>
<td style="text-align: left;">char</td>
<td style="text-align: right;">32</td>
</tr>
<tr>
<td style="text-align: left;">Char-level CNN</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">#filters</td>
<td style="text-align: right;">256</td>
</tr>
<tr>
<td style="text-align: left;">ngram filter size</td>
<td style="text-align: right;">$[3]$</td>
</tr>
<tr>
<td style="text-align: left;">output size</td>
<td style="text-align: right;">128</td>
</tr>
<tr>
<td style="text-align: left;">Sentence Encoder</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">#transformer layers</td>
<td style="text-align: right;">4</td>
</tr>
<tr>
<td style="text-align: left;">Graph Encoder</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">#transformer layers</td>
<td style="text-align: right;">2</td>
</tr>
<tr>
<td style="text-align: left;">Transformer Layer</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">#heads</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">hidden size</td>
<td style="text-align: right;">512</td>
</tr>
<tr>
<td style="text-align: left;">feed-forward hidden size</td>
<td style="text-align: right;">1024</td>
</tr>
<tr>
<td style="text-align: left;">Concept Solver</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">feed-forward hidden size</td>
<td style="text-align: right;">1024</td>
</tr>
<tr>
<td style="text-align: left;">Relation Solver</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">#heads</td>
<td style="text-align: right;">8</td>
</tr>
<tr>
<td style="text-align: left;">feed-forward hidden size</td>
<td style="text-align: right;">1024</td>
</tr>
<tr>
<td style="text-align: left;">Deep biaffine classifier</td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">hidden size</td>
<td style="text-align: right;">100</td>
</tr>
</tbody>
</table>
<p>Table 3: Hyper-parameters settings.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ We only list the results on AMR 2.0 since there are few results on AMR 1.0 to compare.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{3}$ It is analogous to a standard Transformer decoder (Vaswani et al., 2017) for sequence-to-sequence learning.
${ }^{4}$ Trivially employing a graph neural network here can be computationally expensive and intractable since it needs to re-compute all graph representations after every expansion.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
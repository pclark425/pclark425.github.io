<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-9026 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-9026</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-9026</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-159.html">extraction-schema-159</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <p><strong>Paper ID:</strong> paper-257205751</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2302.12313v4.pdf" target="_blank">Testing AI on language comprehension tasks reveals insensitivity to underlying meaning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) are recruited in applications that span from clinical assistance and legal support to question answering and education. Their success in specialized tasks has led to the claim that they possess human-like linguistic capabilities related to compositional understanding and reasoning. Yet, reverse-engineering is bound by Moravec’s Paradox, according to which easy skills are hard. We systematically assess 7 state-of-the-art models on a novel benchmark. Models answered a series of comprehension questions, each prompted multiple times in two settings, permitting one-word or open-length replies. Each question targets a short text featuring high-frequency linguistic constructions. To establish a baseline for achieving human-like performance, we tested 400 humans on the same prompts. Based on a dataset of n = 26,680 datapoints, we discovered that LLMs perform at chance accuracy and waver considerably in their answers. Quantitatively, the tested models are outperformed by humans, and qualitatively their answers showcase distinctly non-human errors in language understanding. We interpret this evidence as suggesting that, despite their usefulness in various tasks, current AI models fall short of understanding language in a way that matches humans, and we argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e9026.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e9026.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An OpenAI conversational large language model (GPT-3.5 family) with RLHF used via OpenAI's interface; evaluated here on sentence-level comprehension and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's conversational GPT-3.5 family model; conversational interface tested via OpenAI; includes Reinforcement Learning from Human Feedback (RLHF) in training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sentence comprehension yes/no task (40 items), repeated-response stability test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>A psycholinguistic-style comprehension battery of 40 short affirmative sentences with high-frequency verbs and simple coordination; each item followed by a yes/no comprehension question. Each prompt was presented three times in two settings: open-length (free response) and one-word (instruction: 'Answer using just one word'). Measures: accuracy (correct yes/no) and stability (whether the three responses to the same prompt were identical).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported only as part of the LLM pool; not singled out with a numeric mean accuracy in text. Group-level finding: as a group LLMs were at chance; some models outperformed chance (see Falcon, Llama2, ChatGPT-4). ChatGPT-3.5 included in experiments and benefitted from RLHF but did not reach ChatGPT-4 performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans (n=400) provided mostly accurate answers and near-ceiling performance; 24,000 human replies collected; 51 human subjects performed at ceiling. Exact percent accuracy not reported numerically for humans in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below typical human baseline — as a group LLMs (including ChatGPT-3.5) performed significantly worse and were less stable than humans; ChatGPT-3.5 was outperformed by the best humans and by ChatGPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Each of 40 prompts presented 3 times in open-length then 3 times in one-word setting; total 240 prompts per LLM (120 per setting), 240 replies per model; ChatGPT-3.5 prompted via OpenAI interface in Dec 2023. Responses coded leniently (open-length replies that contained a correct answer but additional text could be coded as accurate; in one-word setting only first relevant piece of information coded).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Paper does not provide model-specific numeric accuracy for ChatGPT-3.5; scoring was intentionally lenient to favor models (open-length non-explicit but non-erroneous answers counted as correct; one-word replies coded by first relevant token), which biases results toward LLMs; models differ in training (RLHF for some) and were tested on different dates (Gemini later). The task is a constrained psycholinguistic comprehension test rather than a broad cognitive battery.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing AI on language comprehension tasks reveals insensitivity to underlying meaning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9026.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e9026.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI's GPT-4 conversational model; highest-performing model in this study on sentence comprehension and response stability but still inferior to the best human performers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 model accessed via OpenAI's interface; conversational agent with RLHF and instruction-tuning aimed at improving alignment and instruction following.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sentence comprehension yes/no task (40 items), repeated-response stability test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same 40-item comprehension battery (affirmative, coordinated sentences with high-frequency verbs) with two response settings (open-length, one-word) and triplicate prompting to assess stability and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Mean accuracy reported as M = 0.83 (83%) across conditions (paper states ChatGPT-4 had significantly higher accuracy than all other tested LLMs). Stability less than ceiling — ChatGPT-4 did not achieve perfect stability.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants (n=400) showed near-ceiling accuracy and very high stability; 51 human subjects performed at ceiling. Exact aggregate percent accuracy for humans not given numerically in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below best humans: ChatGPT-4 is the best-tested LLM but still performs significantly worse than the best-performing humans (and worse overall than human baseline on both accuracy and stability).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompting: 40 items × 3 repeats × 2 settings = 240 prompts per LLM; ChatGPT-4 tested via OpenAI interface in Dec 2023; open-length responses coded leniently, one-word responses coded by first relevant token when models produced longer replies. GLMM analyses compared model accuracies and stability; ChatGPT-4 parameter estimates significantly above other models.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although ChatGPT-4 outperformed other LLMs in this task, it still failed to reach human ceiling; scoring favored LLMs (lenient coding), so true gap might be larger. Paper notes qualitative non-human errors (redundancy, irrelevant reasoning) even when the final token was correct. Model size and training details not enumerated in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing AI on language comprehension tasks reveals insensitivity to underlying meaning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9026.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e9026.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-70B-chat-hf (Llama2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Meta's Llama-2 chat model (70B parameter chat variant) included here as a state-of-the-art open model; showed above-chance accuracy in the comprehension battery but below ChatGPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-70B-chat-hf (Llama2)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source chat-oriented Llama-2 family model (70B parameter chat variant referenced by name in Methods), tested via HuggingFace interface.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B (as indicated by the model name Llama-2-70B-chat-hf)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sentence comprehension yes/no task (40 items), repeated-response stability test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same 40-item comprehension battery assessing sentence-level parsing and simple semantic role inference via yes/no questions; also measures response stability across three repeats in two instruction settings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Mean accuracy reported as M = 0.66 (66%) — above chance for this model but well below human performance and below ChatGPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans (n=400) showed near-ceiling accuracy and stability; exact percent not numerically reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline: Llama2 (≈66% accuracy) is above chance but substantially worse than humans and inferior to ChatGPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tested in Dec 2023 via HuggingFace interface. Each prompt shown 3× in open-length and 3× in one-word settings; replies coded leniently (open-length acceptable if contained correct answer; in one-word only first relevant piece coded).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Model-level numeric performance provided, but stability and qualitative error patterns show non-human mistakes; scoring favored models, so real-world deficits may be larger. Llama2's accuracy varied by setting (one-word setting improved performance relative to open-length).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing AI on language comprehension tasks reveals insensitivity to underlying meaning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9026.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e9026.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Google Bard (LaMDA-based)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's conversational LLM (Bard) tested on the comprehension battery; showed some of the lowest stability among tested models and generally underperformed humans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Bard</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's conversational dialogue model (LaMDA-family/Google's Bard as used through Google's interface); includes instruction-tuning and safety/alignment interventions (RLHF-like procedures in deployment).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sentence comprehension yes/no task (40 items), repeated-response stability test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>40 simple affirmative sentence items with yes/no comprehension questions; assessed accuracy and response stability across three repeats in two instruction settings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Stability mean reported as M = 0.66 (66%) — identified as the LLM with the lowest stability. The paper does not list a per-model numeric accuracy for Bard, but reports Bard performed poorly relative to ChatGPT-4 and humans.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants (n=400) achieved near-ceiling accuracy and extremely high stability; precise percent not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline: Bard produced less stable responses and lower accuracy than humans; lower stability than several other LLMs (significantly less stable than Falcon).</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Prompts administered via Google's interface in Dec 2023; same 40 items × 3 repeats × 2 settings; coding lenient toward LLMs' responses.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Per-model accuracy numeric value not explicitly reported in the paper; instability may have been influenced by interface/session factors; scoring leniently favored LLMs, so Bard's true deficits could be larger.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing AI on language comprehension tasks reveals insensitivity to underlying meaning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9026.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e9026.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Falcon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>falcon-180B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The Falcon 180B chat model (an open large model) showed intermediate-to-strong stability and above-chance accuracy in this comprehension battery but still below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>falcon-180B-chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Falcon series open language model (180B parameter chat variant) tested via HuggingFace interface; used here as a representative large open model with strong stability.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>180B (as indicated by model name falcon-180B-chat)</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sentence comprehension yes/no task (40 items), repeated-response stability test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>40 short affirmative sentences with high-frequency verbs; yes/no comprehension questions; repeated prompts to assess consistency across responses and two instruction settings (open-length and one-word).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Mean accuracy reported as M = 0.66 (66%). Stability reported as M = 0.83 (83%) — Falcon was the most stable LLM in the study and provided partially consistent accurate replies.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants (n=400) had near-ceiling accuracy and very high stability; exact percent values not supplied in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline: Falcon performed above chance and showed high stability but was still significantly worse than humans in accuracy and stability measures.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tested via HuggingFace in Dec 2023. Each prompt presented three times in open-length then three times in one-word setting. Responses coded for accuracy/stability with lenient rules favoring models when ambiguous.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Although Falcon had high stability relative to other LLMs, its accuracy remained far from human-level; model tested as a single representative of the Falcon family and may not generalize across variants. Lenient scoring may understate the gap to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing AI on language comprehension tasks reveals insensitivity to underlying meaning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9026.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e9026.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mixtral</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mixtral-8x7B-Instruct-v0.1 (Mixtral)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mixture-of-experts instruct-tuned model (Mixtral) included as a state-of-the-art model; in this study its performance did not exceed chance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Mixtral-8x7B-Instruct-v0.1</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Mixtral mixture-of-experts instruct-tuned model (denoted 8x7B in name suggesting experts configuration) tested via HuggingFace; included as a recent SOTA variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8x7B Mixture-of-experts configuration (as indicated by model name); exact effective parameter count not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sentence comprehension yes/no task (40 items), repeated-response stability test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>Same 40-item comprehension task testing simple sentence meaning comprehension and response stability across three repeats and two instruction settings.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Performance characterized as at chance. Statistical analysis reported Mixtral as the first LLM to perform at chance (parameter β = 0.103, z = 0.524, P = .600), interpreted as not exceeding chance-level accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Humans (n=400) performed mostly accurately and stably (near-ceiling). Exact numeric human accuracy not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Well below human baseline: Mixtral did not exceed chance and thus performed substantially worse than human participants.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Tested via HuggingFace in Dec 2023; 40 prompts × 3 repeats × 2 settings; lenient coding applied to LLM outputs (open-length coded as correct if contained correct response; one-word coded by first relevant token).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>The paper reports Mixtral as at chance but does not provide an explicit percentage accuracy; mixture-of-experts configuration complicates direct parameter comparison to dense models. Scoring rules were lenient toward models, so failing to exceed chance despite leniency is notable but still limited by small item count (40).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing AI on language comprehension tasks reveals insensitivity to underlying meaning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e9026.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e9026.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being evaluated on cognitive psychology tests, including details of the models, the tests, LLM performance, human baseline performance, and any comparisons or notable findings.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gemini (Google)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Google's Gemini conversational model evaluated here; showed relatively high stability but often provided systematically incorrect or overly verbose answers, indicating non-human error patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Gemini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Google's large multimodal/LLM family (Gemini) accessed via Google's interface; includes alignment and instruction-following training (tested Feb 2024 per paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>not reported in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>test_battery_name</strong></td>
                            <td>Sentence comprehension yes/no task (40 items), repeated-response stability test</td>
                        </tr>
                        <tr>
                            <td><strong>test_description</strong></td>
                            <td>40-item sentence-level comprehension battery (affirmative, coordinated sentences), two instruction settings (open-length, one-word), triplicate prompting to measure stability and accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Stability reported as M = 0.80 (80%). Accuracy not given as an explicit per-model mean in text, but qualitative examples show Gemini frequently produced incorrect or ambiguous explanations and failed to provide one-word direct answers; characterized as 'partially consistent in providing inaccurate replies.'</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Human participants (n=400) performed near-ceiling and with very high stability; precise numeric accuracy not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Below human baseline: Gemini's stability is relatively high compared to other LLMs, but its accuracy and qualitative answer quality are worse than humans; Gemini often produced verbose, incorrect explanations and failed to match human consistency.</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_details</strong></td>
                            <td>Gemini was tested in Feb 2024 via Google's interface (other models in Dec 2023). Each prompt was repeated 3× in open-length then 3× in one-word setting. Paper provides verbatim examples showing Gemini giving lengthy, incorrect, and contextually irrelevant rationales.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_caveats</strong></td>
                            <td>Accuracy number not explicitly provided for Gemini; testing occurred later (Feb 2024) possibly after other models' tests, which may affect comparability. The study employed lenient scoring to favor LLMs; even so Gemini produced many non-human errors and verbose irrelevancies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Testing AI on language comprehension tasks reveals insensitivity to underlying meaning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models <em>(Rating: 2)</em></li>
                <li>Large linguistic models: Analyzing theoretical linguistic abilities of LLMs <em>(Rating: 2)</em></li>
                <li>A sentence is worth a thousand pictures: Can large language models understand human language? <em>(Rating: 2)</em></li>
                <li>Language models align with human judgments on key grammatical constructions <em>(Rating: 1)</em></li>
                <li>On the dangers of stochastic parrots: Can Language Models be too big? <em>(Rating: 1)</em></li>
                <li>DALL-E 2 fails to reliably capture common syntactic processes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-9026",
    "paper_id": "paper-257205751",
    "extraction_schema_id": "extraction-schema-159",
    "extracted_data": [
        {
            "name_short": "ChatGPT-3.5",
            "name_full": "ChatGPT (GPT-3.5)",
            "brief_description": "An OpenAI conversational large language model (GPT-3.5 family) with RLHF used via OpenAI's interface; evaluated here on sentence-level comprehension and stability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT-3.5",
            "model_description": "OpenAI's conversational GPT-3.5 family model; conversational interface tested via OpenAI; includes Reinforcement Learning from Human Feedback (RLHF) in training.",
            "model_size": "not reported in this paper",
            "test_battery_name": "Sentence comprehension yes/no task (40 items), repeated-response stability test",
            "test_description": "A psycholinguistic-style comprehension battery of 40 short affirmative sentences with high-frequency verbs and simple coordination; each item followed by a yes/no comprehension question. Each prompt was presented three times in two settings: open-length (free response) and one-word (instruction: 'Answer using just one word'). Measures: accuracy (correct yes/no) and stability (whether the three responses to the same prompt were identical).",
            "llm_performance": "Reported only as part of the LLM pool; not singled out with a numeric mean accuracy in text. Group-level finding: as a group LLMs were at chance; some models outperformed chance (see Falcon, Llama2, ChatGPT-4). ChatGPT-3.5 included in experiments and benefitted from RLHF but did not reach ChatGPT-4 performance.",
            "human_baseline_performance": "Humans (n=400) provided mostly accurate answers and near-ceiling performance; 24,000 human replies collected; 51 human subjects performed at ceiling. Exact percent accuracy not reported numerically for humans in the paper.",
            "performance_comparison": "Below typical human baseline — as a group LLMs (including ChatGPT-3.5) performed significantly worse and were less stable than humans; ChatGPT-3.5 was outperformed by the best humans and by ChatGPT-4.",
            "experimental_details": "Each of 40 prompts presented 3 times in open-length then 3 times in one-word setting; total 240 prompts per LLM (120 per setting), 240 replies per model; ChatGPT-3.5 prompted via OpenAI interface in Dec 2023. Responses coded leniently (open-length replies that contained a correct answer but additional text could be coded as accurate; in one-word setting only first relevant piece of information coded).",
            "limitations_or_caveats": "Paper does not provide model-specific numeric accuracy for ChatGPT-3.5; scoring was intentionally lenient to favor models (open-length non-explicit but non-erroneous answers counted as correct; one-word replies coded by first relevant token), which biases results toward LLMs; models differ in training (RLHF for some) and were tested on different dates (Gemini later). The task is a constrained psycholinguistic comprehension test rather than a broad cognitive battery.",
            "uuid": "e9026.0",
            "source_info": {
                "paper_title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "ChatGPT-4",
            "name_full": "ChatGPT (GPT-4)",
            "brief_description": "OpenAI's GPT-4 conversational model; highest-performing model in this study on sentence comprehension and response stability but still inferior to the best human performers.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT-4",
            "model_description": "OpenAI's GPT-4 model accessed via OpenAI's interface; conversational agent with RLHF and instruction-tuning aimed at improving alignment and instruction following.",
            "model_size": "not reported in this paper",
            "test_battery_name": "Sentence comprehension yes/no task (40 items), repeated-response stability test",
            "test_description": "Same 40-item comprehension battery (affirmative, coordinated sentences with high-frequency verbs) with two response settings (open-length, one-word) and triplicate prompting to assess stability and accuracy.",
            "llm_performance": "Mean accuracy reported as M = 0.83 (83%) across conditions (paper states ChatGPT-4 had significantly higher accuracy than all other tested LLMs). Stability less than ceiling — ChatGPT-4 did not achieve perfect stability.",
            "human_baseline_performance": "Human participants (n=400) showed near-ceiling accuracy and very high stability; 51 human subjects performed at ceiling. Exact aggregate percent accuracy for humans not given numerically in the paper.",
            "performance_comparison": "Below best humans: ChatGPT-4 is the best-tested LLM but still performs significantly worse than the best-performing humans (and worse overall than human baseline on both accuracy and stability).",
            "experimental_details": "Prompting: 40 items × 3 repeats × 2 settings = 240 prompts per LLM; ChatGPT-4 tested via OpenAI interface in Dec 2023; open-length responses coded leniently, one-word responses coded by first relevant token when models produced longer replies. GLMM analyses compared model accuracies and stability; ChatGPT-4 parameter estimates significantly above other models.",
            "limitations_or_caveats": "Although ChatGPT-4 outperformed other LLMs in this task, it still failed to reach human ceiling; scoring favored LLMs (lenient coding), so true gap might be larger. Paper notes qualitative non-human errors (redundancy, irrelevant reasoning) even when the final token was correct. Model size and training details not enumerated in paper.",
            "uuid": "e9026.1",
            "source_info": {
                "paper_title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Llama2",
            "name_full": "Llama-2-70B-chat-hf (Llama2)",
            "brief_description": "Meta's Llama-2 chat model (70B parameter chat variant) included here as a state-of-the-art open model; showed above-chance accuracy in the comprehension battery but below ChatGPT-4.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-70B-chat-hf (Llama2)",
            "model_description": "Open-source chat-oriented Llama-2 family model (70B parameter chat variant referenced by name in Methods), tested via HuggingFace interface.",
            "model_size": "70B (as indicated by the model name Llama-2-70B-chat-hf)",
            "test_battery_name": "Sentence comprehension yes/no task (40 items), repeated-response stability test",
            "test_description": "Same 40-item comprehension battery assessing sentence-level parsing and simple semantic role inference via yes/no questions; also measures response stability across three repeats in two instruction settings.",
            "llm_performance": "Mean accuracy reported as M = 0.66 (66%) — above chance for this model but well below human performance and below ChatGPT-4.",
            "human_baseline_performance": "Humans (n=400) showed near-ceiling accuracy and stability; exact percent not numerically reported in the paper.",
            "performance_comparison": "Below human baseline: Llama2 (≈66% accuracy) is above chance but substantially worse than humans and inferior to ChatGPT-4.",
            "experimental_details": "Tested in Dec 2023 via HuggingFace interface. Each prompt shown 3× in open-length and 3× in one-word settings; replies coded leniently (open-length acceptable if contained correct answer; in one-word only first relevant piece coded).",
            "limitations_or_caveats": "Model-level numeric performance provided, but stability and qualitative error patterns show non-human mistakes; scoring favored models, so real-world deficits may be larger. Llama2's accuracy varied by setting (one-word setting improved performance relative to open-length).",
            "uuid": "e9026.2",
            "source_info": {
                "paper_title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Bard",
            "name_full": "Google Bard (LaMDA-based)",
            "brief_description": "Google's conversational LLM (Bard) tested on the comprehension battery; showed some of the lowest stability among tested models and generally underperformed humans.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Bard",
            "model_description": "Google's conversational dialogue model (LaMDA-family/Google's Bard as used through Google's interface); includes instruction-tuning and safety/alignment interventions (RLHF-like procedures in deployment).",
            "model_size": "not reported in this paper",
            "test_battery_name": "Sentence comprehension yes/no task (40 items), repeated-response stability test",
            "test_description": "40 simple affirmative sentence items with yes/no comprehension questions; assessed accuracy and response stability across three repeats in two instruction settings.",
            "llm_performance": "Stability mean reported as M = 0.66 (66%) — identified as the LLM with the lowest stability. The paper does not list a per-model numeric accuracy for Bard, but reports Bard performed poorly relative to ChatGPT-4 and humans.",
            "human_baseline_performance": "Human participants (n=400) achieved near-ceiling accuracy and extremely high stability; precise percent not reported.",
            "performance_comparison": "Below human baseline: Bard produced less stable responses and lower accuracy than humans; lower stability than several other LLMs (significantly less stable than Falcon).",
            "experimental_details": "Prompts administered via Google's interface in Dec 2023; same 40 items × 3 repeats × 2 settings; coding lenient toward LLMs' responses.",
            "limitations_or_caveats": "Per-model accuracy numeric value not explicitly reported in the paper; instability may have been influenced by interface/session factors; scoring leniently favored LLMs, so Bard's true deficits could be larger.",
            "uuid": "e9026.3",
            "source_info": {
                "paper_title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Falcon",
            "name_full": "falcon-180B-chat",
            "brief_description": "The Falcon 180B chat model (an open large model) showed intermediate-to-strong stability and above-chance accuracy in this comprehension battery but still below human performance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "falcon-180B-chat",
            "model_description": "Falcon series open language model (180B parameter chat variant) tested via HuggingFace interface; used here as a representative large open model with strong stability.",
            "model_size": "180B (as indicated by model name falcon-180B-chat)",
            "test_battery_name": "Sentence comprehension yes/no task (40 items), repeated-response stability test",
            "test_description": "40 short affirmative sentences with high-frequency verbs; yes/no comprehension questions; repeated prompts to assess consistency across responses and two instruction settings (open-length and one-word).",
            "llm_performance": "Mean accuracy reported as M = 0.66 (66%). Stability reported as M = 0.83 (83%) — Falcon was the most stable LLM in the study and provided partially consistent accurate replies.",
            "human_baseline_performance": "Human participants (n=400) had near-ceiling accuracy and very high stability; exact percent values not supplied in the paper.",
            "performance_comparison": "Below human baseline: Falcon performed above chance and showed high stability but was still significantly worse than humans in accuracy and stability measures.",
            "experimental_details": "Tested via HuggingFace in Dec 2023. Each prompt presented three times in open-length then three times in one-word setting. Responses coded for accuracy/stability with lenient rules favoring models when ambiguous.",
            "limitations_or_caveats": "Although Falcon had high stability relative to other LLMs, its accuracy remained far from human-level; model tested as a single representative of the Falcon family and may not generalize across variants. Lenient scoring may understate the gap to humans.",
            "uuid": "e9026.4",
            "source_info": {
                "paper_title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Mixtral",
            "name_full": "Mixtral-8x7B-Instruct-v0.1 (Mixtral)",
            "brief_description": "A mixture-of-experts instruct-tuned model (Mixtral) included as a state-of-the-art model; in this study its performance did not exceed chance.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Mixtral-8x7B-Instruct-v0.1",
            "model_description": "Mixtral mixture-of-experts instruct-tuned model (denoted 8x7B in name suggesting experts configuration) tested via HuggingFace; included as a recent SOTA variant.",
            "model_size": "8x7B Mixture-of-experts configuration (as indicated by model name); exact effective parameter count not specified in paper",
            "test_battery_name": "Sentence comprehension yes/no task (40 items), repeated-response stability test",
            "test_description": "Same 40-item comprehension task testing simple sentence meaning comprehension and response stability across three repeats and two instruction settings.",
            "llm_performance": "Performance characterized as at chance. Statistical analysis reported Mixtral as the first LLM to perform at chance (parameter β = 0.103, z = 0.524, P = .600), interpreted as not exceeding chance-level accuracy.",
            "human_baseline_performance": "Humans (n=400) performed mostly accurately and stably (near-ceiling). Exact numeric human accuracy not reported.",
            "performance_comparison": "Well below human baseline: Mixtral did not exceed chance and thus performed substantially worse than human participants.",
            "experimental_details": "Tested via HuggingFace in Dec 2023; 40 prompts × 3 repeats × 2 settings; lenient coding applied to LLM outputs (open-length coded as correct if contained correct response; one-word coded by first relevant token).",
            "limitations_or_caveats": "The paper reports Mixtral as at chance but does not provide an explicit percentage accuracy; mixture-of-experts configuration complicates direct parameter comparison to dense models. Scoring rules were lenient toward models, so failing to exceed chance despite leniency is notable but still limited by small item count (40).",
            "uuid": "e9026.5",
            "source_info": {
                "paper_title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Gemini",
            "name_full": "Gemini (Google)",
            "brief_description": "Google's Gemini conversational model evaluated here; showed relatively high stability but often provided systematically incorrect or overly verbose answers, indicating non-human error patterns.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Gemini",
            "model_description": "Google's large multimodal/LLM family (Gemini) accessed via Google's interface; includes alignment and instruction-following training (tested Feb 2024 per paper).",
            "model_size": "not reported in this paper",
            "test_battery_name": "Sentence comprehension yes/no task (40 items), repeated-response stability test",
            "test_description": "40-item sentence-level comprehension battery (affirmative, coordinated sentences), two instruction settings (open-length, one-word), triplicate prompting to measure stability and accuracy.",
            "llm_performance": "Stability reported as M = 0.80 (80%). Accuracy not given as an explicit per-model mean in text, but qualitative examples show Gemini frequently produced incorrect or ambiguous explanations and failed to provide one-word direct answers; characterized as 'partially consistent in providing inaccurate replies.'",
            "human_baseline_performance": "Human participants (n=400) performed near-ceiling and with very high stability; precise numeric accuracy not provided.",
            "performance_comparison": "Below human baseline: Gemini's stability is relatively high compared to other LLMs, but its accuracy and qualitative answer quality are worse than humans; Gemini often produced verbose, incorrect explanations and failed to match human consistency.",
            "experimental_details": "Gemini was tested in Feb 2024 via Google's interface (other models in Dec 2023). Each prompt was repeated 3× in open-length then 3× in one-word setting. Paper provides verbatim examples showing Gemini giving lengthy, incorrect, and contextually irrelevant rationales.",
            "limitations_or_caveats": "Accuracy number not explicitly provided for Gemini; testing occurred later (Feb 2024) possibly after other models' tests, which may affect comparability. The study employed lenient scoring to favor LLMs; even so Gemini produced many non-human errors and verbose irrelevancies.",
            "uuid": "e9026.6",
            "source_info": {
                "paper_title": "Testing AI on language comprehension tasks reveals insensitivity to underlying meaning",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models",
            "rating": 2,
            "sanitized_title": "what_bert_is_not_lessons_from_a_new_suite_of_psycholinguistic_diagnostics_for_language_models"
        },
        {
            "paper_title": "Large linguistic models: Analyzing theoretical linguistic abilities of LLMs",
            "rating": 2,
            "sanitized_title": "large_linguistic_models_analyzing_theoretical_linguistic_abilities_of_llms"
        },
        {
            "paper_title": "A sentence is worth a thousand pictures: Can large language models understand human language?",
            "rating": 2,
            "sanitized_title": "a_sentence_is_worth_a_thousand_pictures_can_large_language_models_understand_human_language"
        },
        {
            "paper_title": "Language models align with human judgments on key grammatical constructions",
            "rating": 1,
            "sanitized_title": "language_models_align_with_human_judgments_on_key_grammatical_constructions"
        },
        {
            "paper_title": "On the dangers of stochastic parrots: Can Language Models be too big?",
            "rating": 1,
            "sanitized_title": "on_the_dangers_of_stochastic_parrots_can_language_models_be_too_big"
        },
        {
            "paper_title": "DALL-E 2 fails to reliably capture common syntactic processes",
            "rating": 1,
            "sanitized_title": "dalle_2_fails_to_reliably_capture_common_syntactic_processes"
        }
    ],
    "cost": 0.014852999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Testing AI on language comprehension tasks reveals insensitivity to underlying meaning</p>
<p>Vittoria Dentella 
Departament d'Estudis Anglesos i Alemanys
Universitat Rovira i Virgili
TarragonaSpain</p>
<p>Fritz Günther 
Institut für Psychologie
Humboldt-Universitat zu Berlin
BerlinGermany</p>
<p>Elliot Murphy 
Department of Neurosurgery
UTHealth
HoustonTexasUSA</p>
<p>Gary Marcus 
Department of Psychology
New York University
New YorkUSA</p>
<p>Evelina Leivada 
Departament de Filologia Catalana
Universitat Autònoma de Barcelona
BarcelonaSpain</p>
<p>Institució Catalana de Recerca i Estudis Avançats (ICREA)
BarcelonaSpain</p>
<p>Testing AI on language comprehension tasks reveals insensitivity to underlying meaning
E2A862B44F01C55CF82DDFDEB02E3C1E
Large Language Models (LLMs) are recruited in applications that span from clinical assistance and legal support to question answering and education.Their success in specialized tasks has led to the claim that they possess human-like linguistic capabilities related to compositional understanding and reasoning.Yet, reverse-engineering is bound by Moravec's Paradox, according to which easy skills are hard.We systematically assess 7 state-of-the-art models on a novel benchmark.Models answered a series of comprehension questions, each prompted multiple times in two settings, permitting one-word or open-length replies.Each question targets a short text featuring high-frequency linguistic constructions.To establish a baseline for achieving human-like performance, we tested 400 humans on the same prompts.Based on a dataset of n=26,680 datapoints, we discovered that LLMs perform at chance accuracy and waver considerably in their answers.Quantitatively, the tested models are outperformed by humans, and qualitatively their answers showcase distinctly non-human errors in language understanding.We interpret this evidence as suggesting that, despite their usefulness in various tasks, current AI models fall short of understanding language in a way that matches humans, and we argue that this may be due to their lack of a compositional operator for regulating grammatical and semantic information.</p>
<p>Introduction</p>
<p>Large Language Models (LLMs) are neural networks trained on generating probability distributions over natural language data.Through interfaces that allow direct interaction with users, LLMs (i.e., the underlying models together with their respective interface setup) perform tasks that span from translation to answering a wide range of general queries (Bubeck et al., 2024), spanning domains as diverse as law (Nay et al., 2024), medicine (Singhal et al., 2023;Sandmann et al., 2024), and chemistry (Jablonka et al., 2024).Yet, good performance in tasks that require memorizing specialized knowledge is not necessarily grounded in a solid understanding of language, such that LLMs may fail at comparatively easier tasks (Moravec's paradox;Moravec, 1988Moravec, , 1999;;Pinker, 1994).Reverse-engineering is harder for simple, effortless tasks that our minds do best (Minsky, 1986); and understanding language is easy for humans, with even eighteen-month-olds demonstrating an understanding of complex grammatical relations (Perkins &amp; Lidz, 2021).Our species is characterized by an irrepressible predisposition to acquire language, to seek meaning beneath the surface of words, and to impose on linear sequences a surprisingly rich array of hierarchical structure and relations (Dehaene et al., 2022).</p>
<p>Based on their success in various applications, LLMs have been linked with human-like capabilities such as advanced reasoning (OpenAI on ChatGTP and GPT-4;OpenAI, 2024), understanding across modalities (Google on Gemini; Pichai &amp; Hassabis, 2023), and common sense (Bubeck et al., 2024).Some scholars have claimed that LLMs approximate human cognition, understand language to the point that their performance aligns with or even surpasses that of humans, and are good models of human language (Piantadosi &amp; Hill, 2022;Mahowald et al., 2024;Hu et al., in press).However, much evidence exists pointing to possible inconsistencies in model performance.Models can both produce highly fluent, semantically coherent outputs (Radford et al., 2019;Brown et al., 2020;Bojic et al., 2024;Bubeck et al., 2024) whilst also struggling with fundamental syntactic or semantic properties of natural language (Bender &amp; Koller, 2020;Ettinger, 2020;Rassin et al., 2022;Dentella et al., 2023;Leivada et al., 2023;Moro et al., 2023;Murphy et al., 2024).Observing errors in the linguistic performance of LLMs begs the question of how their application in various tasks, such as replying to a medical or legal question, is affected by possible language limitations, and may in fact be achieved through some series of computational steps entirely remote from a cognitively plausible architecture for human language (Collins, 2024).</p>
<p>While shortcomings in LLM performance do not negate their usefulness as tools for specific applied uses, they do invite serious concerns over what guides output generation: is it (i) context-sensitive text parsing (i.e., the ability to pair specific linguistic forms with their corresponding meanings) in a way that generalizes across contexts (Marcus &amp; Davis, 2019;Sinha et al., 2023), or (ii) the mechanistic leveraging of artifacts present in the training data (Kandpal et al., 2022), hence giving merely the illusion of competence?At present, the dominant means of evaluating LLMs consists in employing their (well-formed) outputs as a basis to infer human-like language skills (modus ponens, for Guest &amp; Martin, 2023).For instance, accurate performance in language-oriented tasks and benchmarks (e.g., Dua et al., 2019;Wang et al., 2019;Srivastava et al., 2023) is used to conclude that the LLMs have succeeded not only at the specific task they performed, but also at learning the general knowledge necessary in order to carry out that task (cf.Yogatama et al., 2019).This line of reasoning lies at the basis for holding LLMs as cognitive theories (Piantadosi, 2023;Mahowald et al., 2024).</p>
<p>On the other hand, assuming competence for LLMs over all morphosyntactic, semantic and pragmatic processes involved in language comprehension clashes with their inability to consistently deploy the knowledge that is attributed to them (Dentella et al., 2023;Leivada et al., 2023;Marcus &amp; Davis, 2023).We list the grammaticality judgments (i.e., judgments about whether a prompt complies with or deviates from the language patterns that the model has internalized) of two frontier LLMs on linguistic prompts that are less frequent in day-to-day language and, consequently, are considerably less likely to form part of the training data (Figure 1).While in humans the influence of cognitive factors (e.g., working memory limitations or distraction) can affect language processing in ways that may result in non-target grammaticality judgments, humans are able to correctly process the stimuli upon reflection; that is, we can engage in 'deep' processing after an initial 'shallow' parse (Karimi &amp; Ferreira, 2016).For LLMs, however, there is no analogous straightforward explanation of their systematic language errors.</p>
<p>Figure 1: GPT-3 (text-davinci-002) and ChatGPT-3.5 performance on a grammaticality judgment task that involves low-frequency constructions.Inaccurate responses are marked in red.Accurate responses are marked in green.Full model replies and detailed linguistic analyses are available (osf.io/dfgmr/?view_only=063bcf2314c547199063e87da79c32b3).</p>
<p>While it is incontestable that LLMs offer a potentially useful interactive tool that mimics human conversation, it remains to be established whether their ability to understand language is on a par with that of humans.To answer, we investigate the ability of 7 state-of-the-art LLMs in a comprehension task that features prompts whose linguistic complexity is purposely kept at a minimum, e.g. ( 1).</p>
<p>(1) John deceived Mary and Lucy was deceived by Mary.In this context, did Mary deceive Lucy?</p>
<p>The benefit of this methodology is that it tests LLMs in the very task for which they have been trained: respond to questions based on the input.While previous research testing LLMs on grammaticality judgments has found that their performance is not comparable to that of humans (Dentella et al., 2023), the results of this reported method are not unequivocally convincing.Some have argued that judgment prompting is not a suitable methodology for assessing LLMs, because this is (putatively) a metalinguistic skill that LLMs do not necessarily possess (Hu &amp; Levy, 2023;Hu et al., in press).Thus, in the present work we test LLMs in their default ability to answer questions based on input.Given their use in a range of meaning-sensitive tasks, from giving medical diagnoses to travel advice and to being "thought partners" to students (Extance, 2023;Singhal et al., 2023), it is expected that LLMs should be able to accurately parse prompts featuring high-frequency verbs and constructions.</p>
<p>To this end, we asked whether LLMs' language performance is on a par with that of humans.Two research questions (RQs) emerge from this general aim: RQ1: Do LLMs reply to comprehension questions accurately?RQ2: When the same question is asked multiple times, do LLMs reply consistently?</p>
<p>Results</p>
<p>Results are reported separately for accuracy (RQ1) and stability (RQ2).First, LLMs are compared between each other on these two variables.Then, their performance is contrasted with that of humans.(Generalized) Linear Mixed Effect Models ((G)LMMs) are used to analyze the data, using the lme4 (Bates et al., 2015) and lmerTest (Kuznetsova et al., 2017) packages in R. LLM and human datasets, together with the annotated code employed in the analyses are available (osf.io/dfgmr/?view_only=063bcf2314c547199063e87da79c32b3).</p>
<p>LLMs: Accuracy</p>
<p>Accuracy by model and setting Accuracy rates by model and setting (open-length vs. oneword) are displayed in Figure 2A.The intercept in an intercept-only GLMM (accuracy ~ 1 + (1 | test_item)) is not significantly different from zero (β = 0.224, z = 1.71,P = .087),indicating atchance performance when all LLMs are taken together.A first chi-square test between a GLMM with a main effect of model (accuracy ~ model + (1 | test_item)) and this intercept-only GLMM however finds significant differences in the performance of the 7 LLMs, justifying the introduction of a model parameter in the GLMM (X2(6) = 202.8,P &lt; .001).The intercept of a GLMM with Bard set as baseline category is negative (β = -0.688,z = -3.433,P = .001),indicating that the performance of at least one LLM is below chance.By systematically changing the baseline category, we find that Mixtral is the first LLM to perform at chance (β = 0.103, z = 0.524, P = .600).With a mean accuracy of .66,Llama2 is the first LLM to perform above chance (β = 0.752, z = 3.720, P &lt; .001),alongside Falcon (M = .66)and ChatGPT-4 (M = .83).Further, we find that ChatGPT-4 has a significantly higher accuracy than all other LLMs: When setting ChatGPT-4 as the baseline category, all parameters for all other models are significant (Ps &lt; .001)and negative (βs &lt; -1.01, zs &lt; -4.31).</p>
<p>A second chi-square test between a GLMM with a main effect of setting (accuracy ~ model + setting + (1 | test_item)) and a GLMM without it (accuracy ~ model + (1 | test_item)) finds a significant main effect of setting (X2(1) = 82.587,P &lt; .001),with more accurate responses in the one-word setting than in the open-length setting (β = 1.037, z = 8.872 , P &lt; .001).We additionally find an interaction between model and setting (X2(6) = 43.374,P &lt; .001),indicating that most but not all LLMs perform more poorly in the open-length setting.</p>
<p>LLMs: Stability</p>
<p>In order to establish the extent to which the LLMs provide stable responses, we performed an analysis at the item level (as opposed to the individual trial level for the accuracy data).Here, the responses to an item were coded as stable (= 1) if all three responses to a prompt were the same, and as unstable (= 0) if one response was different than the other two.</p>
<p>Stability by model and setting</p>
<p>Stability rates by model and setting are displayed in Figure 2B.As was the case for accuracy, a GLMM including a parameter for model significantly outperforms a GLMM without it (X2(6) = 21.438,P = .002).In a GLMM with Bard (the LLM with the lowest stability, M = .66)as baseline category, the intercept is positive (β = 0.610, z = 2.810, P = .005),indicating that even for the most unstable model, the responses are not fully random (i.e., there is a higher likelihood of stable than unstable response patterns).Additionally, the model parameters show that stability is significantly higher for Falcon (β = 0.901, z = 2.357, P = .018;M = .83)and Gemini (β = 0.733, z = 1.976,P = .048;M = .80).Taken together with accuracy results, significantly higher stability for Falcon and Gemini respectively translates into Falcon being partially consistent in providing accurate replies, and Gemini being partially consistent in providing inaccurate replies.Employing Falcon (the most stable LLM) as baseline category for the GLMM, we find that Bard (β = -0.901,z = -2.357,P = .018)and Mixtral (β = -1.340,z = -3.559,P &lt; .001)display less stable response patterns than Falcon, while all the other LLMs perform comparably to Falcon.</p>
<p>Similar to accuracy, a second chi-square test between a GLMM with a main effect of setting (stability ~ model + setting + (1 | test_item)) and a GLMM without it (stability ~ model + (1 | test_item)), finds a significant main effect of setting on stability (X2(1) = 5.160, P = .023),with answers being more stable in the one-word setting (β = 0.450, z = 2.262, P = .024).Lastly, we find no significant interaction between model and setting, indicating that the effect of setting does not differ between the different LLMs (X2(6) = 4.241, P = .644).</p>
<p>LLMs vs. Humans</p>
<p>Comparative analyses showed major differences in the performance of the two responding agents (humans vs. LLMs) in terms of both accuracy and stability (Figure 3).</p>
<p>Accuracy by agent and setting Accuracy rates by responding agent and setting are displayed in Figure 3A.A first chi-square test finds that a GLMM with a main effect of agent (accuracy ~ setting + agent + (1 | item) + (1 | unit)) significantly outperforms a GLMM without it (accuracy ~ setting + (1 | item) + (1 | unit)), justifying the introduction of an agent parameter in the GLMM (X2(1) = 23.356,P &lt; .001).Additionally, a GLMM with an interaction between agent and setting (accuracy ~ setting * agent + (1 | item) + (1 | unit)) outperforms the GLMM that only includes them as main effects (X2(1) = 45.265,P &lt; .001).In the GLMM including the interaction, with humans and the open-length setting as baseline category (β = 3.270, z = 16.190,P &lt; .001for the intercept), we find that LLMs perform significantly worse than humans in the open-length setting (β = -3.508,z = -6.103,P &lt; .001),that humans do not perform better in the one-word setting as opposed to the open-length setting (β = -0.228,z = -1.561,P = .118),and that the gap between humans and LLMs is significantly smaller for responses in the one-word setting, indicating a discrepancy between the response settings for LLMs that is not observed for humans (β = 1.276, z = 6.833,P &lt; .001;see Figure 3).</p>
<p>Stability by agent and setting</p>
<p>Stability rates by responding agent and setting are displayed in Figure 3B.As was the case for accuracy, a first chi-square test finds that a GLMM with a main effect of agent (stability ~ setting + agent + (1 | item) + (1 | unit)) outperforms a GLMM without it (X2(1) = 10.515,P = .001).A second chi-square test additionally finds that a GLMM with an interaction between agent and setting (stability ~ setting * agent + (1 | item) + (1 | unit)) significantly outperforms a GLMM which includes agent and setting as main effects (X2(1) = 4.903, P = .027).In the GLMM including the interaction with humans and the open-length setting again as baseline category (β = 2.286, z = 17.368,P &lt; .001for the intercept), we find that LLMs are less stable than humans in the open-length setting (β = -1.385,z = -3.800,P &lt; .001),that humans are not less stable in the open-length setting compared to the one-word setting (β = -0.038,z = -0.362,P = .717),and that in the one-word setting, the stability gap of LLMs with humans is smaller than in the open-length setting (again indicating a discrepancy between the settings for LLMs that is not observed for humans; β = 0.507, z = 2.234, P = 0.026; see Figure 3).Lastly, anticipating possible arguments that LLMs as a category should be represented by their best performing representative, ChatGPT-4 (the best performing LLM among those tested) can be compared to the best performing humans (as opposed to the averaged performance of all humans).Given that n = 51 human subjects perform at ceiling in terms of accuracy and therefore stability, while ChatGPT-4 does not perform at ceiling on either variable, it follows that even the best performing LLM performs significantly worse than the best performing humans.</p>
<p>Discussion</p>
<p>This work sheds light on what guides output generation in LLMs: (i) the human-like ability to parse and understand written text, or (ii) the leveraging of artifacts in the training data.To do so, we test seven state-of-the-art LLMs on comprehension questions targeting sentences that feature high-frequency constructions and lexical items, and whose linguistic complexity is kept at a minimum.Particularly, we aim at determining whether LLMs generate answers that are both accurate (RQ1) and stable over repeated trials (RQ2).Systematic testing shows that the average performance of LLMs as a group is at chance for accuracy, and that LLMs are relatively unstable in their answers.On the other hand, humans tested on the same comprehension questions provide mostly accurate answers (RQ1) which almost never change when a question is repeatedly prompted (RQ2).Importantly, these differences between LLMs and humans are pronounced even though the models' replies are favorably scored, ignoring any possible inconsistencies they contain (i.e., in the one-word setting; see section 'Methods').</p>
<p>Language parsing, referring to the ability to comprehend and produce language by assigning strings of symbols with meaning, is a distinctively human ability (Hauser et al., 2002).This explains why humans in our experiment responded accurately and without alternating answers when a question is asked multiple times, or with different instructions.For humans, failure to reach at-ceiling performance is explained by the interplay of processing factors (Karimi &amp; Ferreira, 2016) and interspeaker variation which affects interpretative choices.For instance, given the prompt Steve hugged Molly and Donna was kissed by Molly.In this context, was Molly kissed? the answer is "no", though some might assign the verb to kiss a reciprocal reading consequently replying "yes".Nonetheless, despite processing limitations and the variability induced by subjective choices, humans cohesively outperform LLMs both in terms of accuracy and stability, thus confirming that testing LLMs on foundational tasks can reveal shortcomings which remain concealed in more sophisticated settings (Pinker, 1994).</p>
<p>The LLM outputs differ from human answers both quantitatively and qualitatively.Quantitatively, the averaged accuracy of LLMs as a group is at chance, and the models which manage to surpass the chance threshold (i.e., Falcon, Llama2, and ChatGPT-4) reach accuracy rates that are far from perfect.The best performing LLM, ChatGPT-4, performs worse than the best performing humans.Secondly, while all LLMs perform above chance in terms of stability, none of them succeeds in consistently providing the same answer to a question.Taken together, this evidence translates into an overall inability on the LLMs' part to tackle simple comprehension questions in a way that can be reasonably called 'human-like'.</p>
<p>Building on how their outputs show surface similarities with natural language, LLMs have been endowed with human-like language skills both in public discourse, where they are described as "understanding", "reasoning", or "intelligent", and in science, where their adoption as cognitive models, and even theories, is increasingly called for (Piantadosi, 2023;Mahowald et al., 2024).Our results suggest that such a framing may not be justified at present.Importantly, once a LLM has been invested with competence over a language phenomenon, without theoretically informed hypotheses at hand (Guest &amp; Martin, 2020), the causes of its failures to perform at ceiling leave room for speculation.We thus interpret the LLMs' failure in providing accurate and stable answers in a simple comprehension task as suggesting that the models lack a real understanding of language: they produce words that are like semantic black-boxes, approximating the surface statistics of language and the more 'automatic' aspects of parsing (Mitchell &amp; Krakauer, 2022).In addition, LLMs are not suitable for serving as theories of language since their representational capacities are virtually unbounded, rendering their representations arbitrary and not explanatorily grounded, belonging as they do to the class of Universal Functional Approximators (Yun et al., 2019) which have been shown to be capable of approximating any mathematical function (Ismailov, 2023).Consequently, LLMs bear a closer resemblance to tools such as generalized derivatives than scientific theories (see also Evanson et al., 2023).</p>
<p>Additionally, the differences between LLM and human performance are also qualitative in nature.First, all human participants with no exception provide one-word replies regardless of the setting.This shows alignment between subjects on the amount of information that is necessary in order to satisfactorily reply to a question (Grice, 1975), and even in this space of pragmatics humans seem to comply with principles of computational efficiency and relevance that are built out of the path that syntactic inferences carve out for them (Carston, 2022).This also marks a stark difference from LLMs which typically provide lengthy explanations, even in the one-word setting, thus defying the task instructions.In these explanations, their reasoning is broken down into logical steps which often irrelevant or contradicting information.For example, to the prompt "Franck read to himself and John read to himself, Anthony and Franck.In this context, was Franck read to?", Gemini replies (2):</p>
<p>(2) "In this context, it's impossible to say for sure if Franck was read to.Here's why:</p>
<ol>
<li>
<p>Limited information: While the sentence "John read to himself, Anthony and Franck" implies John read aloud to others, it doesn't clarify if Franck was one of the recipients or just present during the reading.</p>
</li>
<li>
<p>Separate actions: The sentence about Franck reading to himself is separate and doesn't necessarily connect to John's reading session.</p>
</li>
<li>
<p>Ambiguity: The phrase "read to" can have different interpretations depending on the context.It could mean reading aloud for someone or simply reading in the presence of someone.</p>
</li>
</ol>
<p>Without additional information about the specific situation, such as John's reading material, the purpose of his reading, or the relationship between the characters, it's impossible to determine if Franck was actually read to by John.</p>
<p>Here are some possibilities based on the limited information: * Franck was read to: John read aloud to both Anthony and Franck, and Franck listened.</p>
<ul>
<li>
<p>Franck wasn't read to: John read aloud only to Anthony, and Franck was simply present during the reading.</p>
</li>
<li>
<p>Franck listened to himself: Both sentences describe independent reading activities, with Franck reading silently to himself and John reading aloud to others.</p>
</li>
</ul>
<p>Therefore, the answer remains uncertain due to the lack of sufficient context."</p>
<p>10/18</p>
<p>The correct answer to the question is "yes".Gemini not only gives the wrong answer, but it also answers redundantly and mixes unrelated matters about the nature of the characters and the type of reading material, none of which plays a role in answering the question.</p>
<p>The problem of redundant, semantically irrelevant information is not a trivial one.Our testing showed that obtaining one-word replies from the tested LLMs was possible only in a reduced number of cases (mostly in .While this issue was ignored for the purposes of coding in the one-word setting, because we wanted to give the models the benefit of the doubt, it further showcases how LLMs' outputs are sensitive to the meaning of prompts only in a superficial manner (Du et al., 2024).While pre-trained LLMs might not have the skills necessary for following specific instructions such as the one we provided (i.e., reply using only one word), it should be noted that ChatGPT-3.5,ChatGPT-4, Bard and Gemini include RLHF in their training, which means that their performance in this experiment benefitted from previous targeted interventions aimed at fixing incongruent outputs and maximizing compliance to user requests; yet, these models' replies fail to conform to task demands.</p>
<p>If the LLMs' fluent outputs are taken to entail human-like linguistic skills (i.e., from the production of a correct sentence such as "The scientist is happy", we infer that the model has knowledge of the relevant rules that put the sentence together), then it follows that LLMs should possess mastery over all the words, constructions, and rules of use, that figure in and govern their outputs.Our results from grammaticality judgments (Figure 1) and language comprehension tasks (Figures 2-3) suggest that this is not the case: distinctly non-human errors are made.To discuss an example, when asked whether "The patient the nurse the clinic had hired admitted met Jack" is grammatically correct, GPT-3 said no, providing the wrong answer (Figure 1).This is not as surprising as is the correction the model volunteered: "The nurse had hired at the clinic a patient who admitted met Jack".This answer contains a semantic anomaly that marks an important deviation from any answer a human would produce: a nurse hiring a patient.Even if we change the methodology from judgment prompting to simple question answering, we get similar errors that suggest that the tested models do not really understand the meaning of words.Recall that Gemini could not answer as to whether Franck has been read to (after being told that "John read to himself, Anthony and Franck"; example 2), because it lacks information "such as John's reading material".Overall, if we attempt to map the encountered LLM errors across levels of linguistic analysis, it seems that their incidence becomes greater as we move from basic phonological form to the more complex outer layers of linguistic organization (Figure 4).While LLMs are often portrayed through wishful mnemonics that hint to human-like abilities (Mitchell, 2021), seemingly sound performance may hide flaws that are inherent to language modelling as a method: intelligence cannot in fact emerge as a side product of statistical inference (van Rooij et al., 2023), nor can the ability to understand meaning (Bender &amp; Koller, 2020).Our results offer an argument against employing LLMs as cognitive theories (Piantadosi, 2023;Mahowald et al., 2024): the fact that LLMs are trained on natural language data does not entail that LLMs understand language; it only means that LLMs can predict some of the fossilized patterns found in training texts.Claiming a model's mastery over language because of its ability to reproduce it, is analogous to claiming that a painter knows a person because they can reproduce this person's face on canvas, just by looking at her pictures.Unjustly equating LLMs' abilities to those of humans in order to justify their mass-scale deployment in applied areas, in turn, results in the biases present in LLMs' training data (Birhane et al., 2023) being allowed one more display venue.</p>
<p>With respect to practical applications, in AI reproducing high-level reasoning is overall less demanding and needs less computational power than comparatively easier tasks (Moravec, 1999).At present, LLMs are employed in settings which require high-level reasoning.For instance, LLMs can reach high accuracy in answering medical questions (Singhal et al., 2023) and are believed to be ready for testing in real-world simulations (Mehandru et al., 2024).While LLMs in a white coat strike many as an implausible scenario, in reality LLM-based chatbots already reply to any question that any human user in real-life situations may pose, possibly 12/18 including doctors.Except for warnings typically issued when ethically problematic words or politically incorrect concepts figure in prompts (Ghafouri et al., 2023), often chatbots reply without flagging any hesitations or lack of knowledge.As a result, the message conveyed is that whatever content LLMs output, it is indeed the product of true understanding and reasoning.However, as the results obtained in this work demonstrate alongside a consistent body of evidence (Rassin et al., 2022;Dentella et al., 2023;Leivada et al., 2023;Kim et al., 2024;Murphy et al., 2024), coherent, sophisticated, and refined outputs amount to patchwork in disguise.</p>
<p>To conclude, LLMs' fluent outputs have given rise to claims that models possess humanlike language abilities, which would in turn allow them to act as cognitive theories able to provide mechanistic explanations of language processing in humans (Piantadosi, 2023;Mahowald et al., 2024).Following evidence that LLMs often produce non-human outputs (Marcus et al., 2023), in this work we tested 7 state-of-the-art LLMs on simple comprehension questions targeting short sentences, purposefully setting an extremely low bar for the evaluation of the models.Systematic testing showed that the performance of these LLMs lags behind that of humans both quantitatively and qualitatively, providing further confirmation that the easiest tasks for humans are the hardest to develop for AI (Pinker, 1994).We argue that these results invite further reflection about the standards of evaluation we adopt for claiming human-likeness in AI.Both in society and science, the role of LLMs is currently defined according to inflated humanlike linguistic abilities (Leivada et al., in press), as opposed to their real capacities.If the application of internalized linguistic rules is absent and LLMs do not go beyond reproducing input patterns (Udandarao et al., 2024), all that is presented as a danger induced by the stochastic nature of LLMs can be anticipated (Bender et al., 2021).If LLMs are useful tools, understanding their limitations in a realistic way paves the way for them to be better and safer.</p>
<p>Methods</p>
<p>The language understanding abilities of 7 LLMs are tested on a set of comprehension questions (n=40).Specifically, the employed prompts have the following characteristics: (a) sentences are affirmative (i.e., contain no negation, which is known to negatively impact LLM performance; Ettinger, 2020), (b) they are linked through coordination, (c) either 3 or 5 human entities are featured in each prompt, all identified by proper names (i.e., no pronouns), and (d) highfrequency verbs are used (e.g., to cook, to hug, to kiss), so as to maximize the chances of LLMs replying correctly.Following previous research that has suggested that LLMs fail to be stable in their answers to a question prompted repeatedly (Dentella et al., 2023), we prompted each comprehension question multiple times to assess the stability of answers.To obtain the right level of comparisons for determining human-likeness, the performance of the LLMs is compared to that of human subjects (n=400) who were tested on the same stimuli.Table 1 provides an overview of the prompts, with one sample sentence and question per condition.The full list of prompts alongside the obtained replies by both humans and LLMs is available at</p>
<p>Figure 2 :
2
Figure 2: (A) Mean accuracy by model and setting.(B) Mean stability by model and setting.</p>
<p>Figure 3 :
3
Figure 3: (A) Mean accuracy by responding agent and setting.(B) Mean stability by responding agent and setting.</p>
<p>Figure 4 :
4
Figure 4: Mapping of the types of errors presented in Figure 1 onto levels of linguistic analysis.The observations for phonology are based on the results of Beguš et al. (2023).</p>
<p>LLMsAll prompts were presented to the LLMs in two settings: (i) open-length setting and (ii) oneword setting.In each setting, every prompt was repeated three times.In (i), LLMs were allowed to produce answers of any length.In (ii), the LLMs were prompted using the same materials as in (i), but the instruction "Answer using just one word" was added after every question (e.g., "John deceived Mary and Lucy was deceived by Mary.In this context, did Mary deceive Lucy?Answer using just one word").The use of the one-word setting was intended to facilitate the LLMs, as it was noted that lengthy LLM replies often contained contradictory, irrelevant, and eventually incorrect statements.In total, each of the tested LLMs was prompted 240 times (120 times per setting).The LLMs' pool of data thus consists of n=1.680 replies (240 replies per each of the 7 LLMs).In both settings, all replies were coded for accuracy (1 for accurate answers, 0 for inaccurate answers) and stability (1 if the three responses were either coded all as yes or all as no, 0 if at least one response was coded differently than the others).In an attempt to give the LLMs the benefit of the doubt and obtain from them the best possible result, their responses in the open-length setting which contained no clear error, but also failed to directly provide the correct answer, were coded as accurate.Additionally, due to the difficulty of obtaining one-word replies from LLMs, the decision was made to only code the first relevant piece of information provided in a reply in the one-word setting.Put another way, if a LLM correctly replied 'Yes' but this was followed by additional content that was incorrect, contradictory, or irrelevant, this was coded as accurate.From this it follows, that a less lenient coding would further amplify the differences between humans and LLMs.All prompts were merged in a unified pool, randomized and administered in random order, first in the open-length setting, and subsequently in the oneword setting.The prompts were given to 7 LLMs, which were chosen as representative of the state of the art: ChatGPT-3.5(Ouyang et al., 2022),ChatGPT-4 (OpenAI, 2023), Llama2 (Llama-2-70Bchat-hf;Touvron et al., 2023), Bard(Thoppilan et al., 2022), Falcon (falcon-180B-chat;Almazrouei et al., 2023), Mixtral (Mixtral-8x7B-Instruct-v0.1;Jiang et al., 2024), and Gemini(Pichai &amp; Hassabis, 2023).ChatGPT-3.5,ChatGPT-4, Bard and Gemini include Reinforcement Learning from Human Feedback (RLHF) in their training regime.All LLMs were tested in December 2023, with the exception of Gemini which was tested in February 2024.ChatGPT-3.5 and ChatGPT-4 were prompted through OpenAI's interface; Bard and Gemini through Google's interface; and Llama2, Falcon and Mixtral through HuggingFace's interface.Human dataFor comparison, we tested human participants (n=400; 200 F, 200 M) on the same prompts.Subjects were recruited through the crowdsourcing platform Prolific, and all participants selfidentified as native speakers of English with no history of neurocognitive impairments.The experiment was carried out in accordance with the Declaration of Helsinki and was approved by the ethics committee of the Department of Psychology at Humboldt-Universität zu Berlin (application 2020-47).Participants were divided into two groups: 200 subjects (100 F, 100 M) were tested in the open-length setting, and 200 subjects (100 F, 100 M) in the one-word setting.The prompts (n=40) originally administered to the LLMs were split into two lists (each presented to half of the subjects in a condition), so that each human subject was prompted with half of the questions (n=20).Similarly to the LLMs, each participant saw each question 3 times, providing a total of n=60 replies.Overall, the human pool of data consists of n=24,000 replies.Human replies were coded for accuracy and stability.To ensure compliance, two attention checks were added that are not part of the analyses (i.e., "The door is red.In this context, is the door red?" and "The door is green.In this context, is the door blue?").All prompts, including attention checks, were administered to each participant in random order.Data from 2 additional participants who did not provide correct answers to all attention checks were excluded from the analyses.Testing took place after participants had given informed consent to take part in the experiment.For each trial, the screen displayed the prompt (i.e., the sentence, its related question and, in the one-word setting, the additional instruction).The experiment was carried out with the jsPsych toolkit(de Leeuw et al., 2023), and had a median completion time of 13.4 minutes.Full participant data can be found at https://osf.io/dfgmr/?view_only=063bcf2314c547199063e87da79c32b3.
The Falcon series of open language models. E Almazrouei, arXiv:2311.16867v215/182023</p>
<p>Fitting linear mixed-effects models using lme4. D Bates, M Maechler, B Bolker, S Walker, Journal of Statistical Software. 6712015</p>
<p>Large linguistic models: Analyzing theoretical linguistic abilities of LLMs. G Beguš, M Dąbkowski, R Rhodes, arXiv:2305.009482023</p>
<p>On the dangers of stochastic parrots: Can Language Models be too big?. E M Bender, T Gebru, A Mcmillan-Major, S Shmitchell, Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency. the 2021 ACM Conference on Fairness, Accountability, and Transparency2021</p>
<p>Climbing towards NLU: On meaning, form, and understanding in the age of data. E M Bender, A Koller, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational Linguistics2020</p>
<p>On hate scaling laws for data-swamps. A Birhane, V Prabhu, S Han, V N Boddeti, arXiv:2306.131412023</p>
<p>L Bojic, P Kovacevic, M Cabarkapa, arXiv:2312.09545GPT-4 surpassing human performance in linguistic pragmatics. 2024</p>
<p>Language models are few-shot learners. T B Brown, arXiv:2005.141652020</p>
<p>S Bubeck, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with GPT-4. 2024</p>
<p>Words: Syntactic structures and pragmatic meanings. Synthese. R Carston, 2022. 200430</p>
<p>The simple reason LLMs are not scientific models (and what the alternative is for linguistics. J Collins, lingbuzz/0080262024</p>
<p>Symbols and mental programs: A hypothesis about human singularity. S Dehaene, F Al Roumi, Y Lakretz, S Planton, M Sablé-Meyer, Trends in Cognitive Sciences. 2692022</p>
<p>Systematic testing of three Language Models reveals low language accuracy, absence of response stability, and a yes-response bias. J R De Leeuw, R A Gilbert, B Luchterhandt, V Dentella, F Günther, E Leivada, Journal of Open Source Software. 851e23095831202023. 2023PNAS</p>
<p>Shortcut learning of large language models in natural language understanding. M Du, F He, N Zou, D Tao, X Hu, Communications of the ACM. 6712024</p>
<p>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. D Dua, Y Wang, P Dasigi, G Stanovsky, S Singh, M Gardner, Proceedings of the 2019 Conference of the North American Chapter. the 2019 Conference of the North American ChapterAssociation for Computational Linguistics2019</p>
<p>What BERT is not: Lessons from a new suite of psycholinguistic diagnostics for language models. A Ettinger, Transactions of the Association for Computational Linguistics. 82020MIT Press</p>
<p>Language acquisition: Do children and language models follow similar learning stages?. L Evanson, Y Lakretz, J R King, arXiv:2306.035862023</p>
<p>ChatGPT has entered the classroom: How LLMs could transform education. A Extance, Nature. 6232023</p>
<p>Logic and conversation. P Grice, Syntax and Semantics 3: Speech Acts. P Cole, Academic Press1975</p>
<p>AI in the gray: Exploring moderation policies in dialogic large language models vs. human answers in controversial topics. V Ghafouri, V Agarwal, Y Zhang, N Sastry, J Such, G Suarez-Tangil, Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. the 32nd ACM International Conference on Information and Knowledge Management2023</p>
<p>How computational modeling can force theory building in science. O Guest, A E Martin, Perspectives on Psychological Science. 1642020</p>
<p>On logical inference over brains, behaviour, and artificial neural networks. O Guest, A Martin, Computational Brain &amp; Behavior. 62023</p>
<p>Faculty of language: What is it, who has it, and how did it evolve?. M D Hauser, N Chomsky, W T Fitch, Science. 29855982002</p>
<p>Prompting is not a substitute for probability measurements in large language models. J Hu, R Levy, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingAssociation for Computational Linguistics2023</p>
<p>Language models align with human judgments on key grammatical constructions. J Hu, K Mahowald, G Lupyan, A Ivanova, R Levy, PNASin press</p>
<p>A three layer neural network can represent any multivariate function. V E Ismailov, Journal of Mathematical Analysis and Applications. 52311270962023</p>
<p>Leveraging large language models for predictive chemistry. K M Jablonka, P Schwaller, A Ortega-Guerrero, B Smit, Nature Machine Intelligence. 62024</p>
<p>A Q Jiang, arXiv:2401.04088v1Mixtral of experts. 2024</p>
<p>Large Language Models struggle to learn long-tail knowledge. N Kandpal, H Deng, A Roberts, E Wallace, C Raffel, Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023</p>
<p>Good-enough linguistic representations and online cognitive equilibrium in language processing. H Karimi, F Ferreira, The Quarterly Journal of Experimental Psychology. 692016</p>
<p>Fables: Evaluating faithfulness and content selection in book-length summarization. Y Kim, arXiv:2404.01261v12024</p>
<p>lmerTest package: Tests in linear mixed effects models. A Kuznetsova, P B Brockhoff, R H B Christensen, Journal of Statistical Software. 82132017</p>
<p>Reply to Hu et al: Applying different evaluation standards to humans vs. Large Language Models overestimates AI performance. E Leivada, F Günther, V Dentella, PNASin press</p>
<p>DALL-E 2 fails to reliably capture common syntactic processes. E Leivada, E Murphy, G Marcus, Social Sciences &amp; Humanities Open. 82023. 100648</p>
<p>Dissociating language and thought in large language models: A cognitive perspective. K Mahowald, A I Ivanova, I A Blank, N Kanwisher, J B Tenenbaum, E Fedorenko, Trends in Cognitive Sciences. 19242024</p>
<p>G Marcus, E Davis, Rebooting AI: Building Artificial Intelligence We Can Trust. Pantheon Books2019</p>
<p>Hello, multimodal hallucinations. G Marcus, E Davis, Substack. 2023</p>
<p>A sentence is worth a thousand pictures: Can large language models understand human language?. G Marcus, E Leivada, E Murphy, arXiv:2308.001092023</p>
<p>Evaluating large language models as agents in the clinic. N Mehandru, B Y Miao, E R Almaraz, Npj Digital Medicine. 7842024</p>
<p>The Society of Mind. M Minsky, 1986Simon and Schuster</p>
<p>Why AI is harder than we think. M Mitchell, Proceedings of the Genetic and Evolutionary Computation Conference. the Genetic and Evolutionary Computation Conference2021</p>
<p>The debate over understanding in AI's large language models. M Mitchell, D C Krakauer, PNAS. 12013e22159071202022</p>
<p>Rise of the robots. H Moravec, Mind Children. Moravec, HHarvard University Press1988. 1999281</p>
<p>Large languages, impossible languages and human brains. A Moro, M Greco, S Cappa, Cortex. 1672023</p>
<p>E Murphy, J De Villiers, S Lucero Morales, arXiv:2403.12294A comparative investigation of compositional syntax and semantics in DALL-E 2. 2024</p>
<p>Large language models as tax attorneys: A case study in legal capabilities emergence. J J Nay, Philosophical Transactions of the Royal Society A. 3822024. 20230159</p>
<p>Openai, arXiv:2303.08774GPT-4 technical report. 2023</p>
<p>. Openai, 2024. April 30</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, arXiv:2203.021552022</p>
<p>Eighteen-month-old infants represent nonlocal syntactic dependencies. L Perkins, J Lidz, PNAS. 11841e20264691182021</p>
<p>Modern language models refute Chomsky's approach to language. S Piantadosi, lingbuzz/0071802023</p>
<p>Meaning without reference in large language models. S T Piantadosi, F Hill, Proceedings of the 36th Conference on Neural Information Processing Systems. the 36th Conference on Neural Information Processing Systems2022</p>
<p>Introducing Gemini: Our largest and most capable AI model. S Pichai, S.D Hassabis, S.2023. 1994William Morrow and CompanyThe Language Instinct</p>
<p>Language models are unsupervised multitask learners. A Radford, J Wu, R Child, D Luan, D Amodei, I Sutskever, 2019</p>
<p>DALLE-2 is seeing double: Flaws in word-toconcept mapping in Text2Image models. R Rassin, S Ravfogel, Y Goldberg, Proceedings of the Fifth BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. the Fifth BlackboxNLP: Analyzing and Interpreting Neural Networks for NLPAssociation for Computational Linguistics2022</p>
<p>Systematic analysis of ChatGPT, Google search and Llama 2 for clinical decision support tasks. S Sandmann, S Riepenhausen, L Plagwitz, J Varghese, Nature Communications. 152024. 2050</p>
<p>Large language models encode clinical knowledge. K Singhal, Nature. 6202023</p>
<p>Language model acceptability judgements are not always robust to context. K Sinha, J Gauthier, A Mueller, K Misra, K Fuentes, R Levy, A Williams, Proceedings of the 61st Annual Meeting of Association for Computational Linguistics. the 61st Annual Meeting of Association for Computational LinguisticsAssociation for Computational Linguistics2023</p>
<p>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. A Srivastava, Transactions on Machine Learning Research. 2023</p>
<p>LaMDA: Language models for dialog applications. R Thoppilan, arXiv:2201.082392022</p>
<p>LLaMA: Open and efficient foundation language models. H Touvron, arXiv:2302.139712023</p>
<p>No "zero-shot" without exponential data: Pretraining concept frequency determines multimodal model performance. V Udandarao, arXiv:2404.04125v12024</p>
<p>Reclaiming AI as a theoretical tool for cognitive science. I Van Rooij, O Guest, F Adolfi, R De Haan, A Kolokolova, P Rich, psyarXiv:10.31234/osf.io/4cbuv2023</p>
<p>SuperGLUE: A stickier benchmark for general-purpose language understanding systems. A Wang, Y Pruksachatkun, N Nangia, A Singh, J Michael, F Hill, O Levy, S Bowman, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Buc, E Fox, R Garnett, Curran Associates Inc201932</p>
<p>Learning and evaluating general linguistic intelligence. D Yogatama, arXiv:1901.113732019</p>
<p>Are transformers universal approximators of sequence-to-sequence functions?. C Yun, S Bhojanapalli, A S Rawat, S J Reddi, S Kumar, arXiv:1912.100772019</p>            </div>
        </div>

    </div>
</body>
</html>